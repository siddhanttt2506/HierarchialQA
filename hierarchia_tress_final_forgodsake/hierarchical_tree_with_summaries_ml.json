{
    "id": "root",
    "title": "Textbook",
    "content": null,
    "summary": null,
    "children": [
        {
            "id": "chapter-1",
            "title": "1. Introduction",
            "content": "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nmore complex models. However, simply duplicating the same data points or collect‐\ning very similar data will not help.\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\nand all of them complied with the rule “If the customer is older than 45, and has less\nthan 3 children or is not divorced, then they want to buy a boat,” we would be much\nmore likely to believe this to be a good rule than when it was developed using only\nthe 12 rows in Table 2-1.\nHaving more data and building appropriately more complex models can often work\nwonders for supervised learning tasks. In this book, we will focus on working with\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\nmuch data to collect, which might be more beneficial than tweaking and tuning your\nmodel. Never underestimate the power of more data.\nSupervised Machine Learning Algorithms\nWe will now review the most popular machine learning algorithms and explain how\nthey learn from data and how they make predictions. We will also discuss how the\nconcept of model complexity plays out for each of these models, and provide an over‐\nSupervised Machine Learning Algorithms \n| \n29\n4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\nfor more details.\nview of how each algorithm builds a model. We will examine the strengths and weak‐\nnesses of each algorithm, and what kind of data they can best be applied to. We will\nalso explain the meaning of the most important parameters and options.4 Many algo‐\nrithms have a classification and a regression variant, and we will describe both.\nIt is not necessary to read through the descriptions of each algorithm in detail, but\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\n• The logic required to make a decision is specific to a single domain and task.\nChanging the task even slightly might require a rewrite of the whole system.\n• Designing rules requires a deep understanding of how a decision should be made\nby a human expert.\nOne example of where this handcoded approach will fail is in detecting faces in\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\ntion was an unsolved problem until as recently as 2001. The main problem is that the\nway in which pixels (which make up an image in a computer) are “perceived” by the\ncomputer is very different from how humans perceive a face. This difference in repre‐\nsentation makes it basically impossible for a human to come up with a good set of\nrules to describe what constitutes a face in a digital image.\nUsing machine learning, however, simply presenting a program with a large collec‐\ntion of images of faces is enough for an algorithm to determine what characteristics\nare needed to identify a face.\nProblems Machine Learning Can Solve\nThe most successful kinds of machine learning algorithms are those that automate\ndecision-making processes by generalizing from known examples. In this setting,\nwhich is known as supervised learning, the user provides the algorithm with pairs of\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\nput given an input. In particular, the algorithm is able to create an output for an input\nit has never seen before without any help from a human. Going back to our example\nof spam classification, using machine learning, the user provides the algorithm with a\nlarge number of emails (which are the input), together with information about particular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n188 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-37. Cluster assignments found by DBSCAN with varying settings for the\nmin_samples and eps parameters\nIn this plot, points that belong to clusters are solid, while the noise points are shown\nin white. Core samples are shown as large markers, while boundary points are dis‐\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\nmeans that more points will be included in a cluster. This makes clusters grow, but\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\nfrom top to bottom in the figure) means that fewer points will be core points, and\nmore points will be labeled as noise.\nThe parameter eps is somewhat more important, as it determines what it means for\npoints to be “close.” Setting eps to be very small will mean that no points are core\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\nwill result in all points forming a single cluster.\nThe min_samples setting mostly determines whether points in less dense regions will\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\nthat would have been a cluster with less than min_samples many samples will now be\nlabeled as noise. min_samples therefore determines the minimum cluster size. You\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\nClustering \n| \n189\npoints, one of five points, and one of three points. Using min_samples=5, the two print(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\n['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n '01', '01pm', '02']\nFeatures 20010 to 20030:\n['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\nEvery 2000th feature:\n['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery',\n 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer',\n 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful',\n 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher',\n 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse',\n 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\ning—apart from \"007\", which in the particular context of movies is likely to refer to\nthe James Bond character.5 Weeding out the meaningful from the nonmeaningful\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\nlection of English words starting with “dra”. You might notice that for \"draught\",\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\nvocabulary as distinct words. These words have very closely related semantic mean‐\nings, and counting them as different words, corresponding to different features,\nmight not be ideal. packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of would never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nIn[32]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[32]:\nTest set score: 0.97\nThis snippet contains the core code for applying any machine learning algorithm\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\nface to supervised models in scikit-learn, and with the concepts introduced in this\nchapter, you can apply these models to many machine learning tasks. In the next\nchapter, we will go into more depth about the different kinds of supervised models in\nscikit-learn and how to apply them successfully.\n24 \n| \nChapter 1: Introduction\nCHAPTER 2\nSupervised Learning\nAs we mentioned earlier, supervised machine learning is one of the most commonly\nused and successful types of machine learning. In this chapter, we will describe super‐\nvised learning in more detail and explain several popular supervised learning algo‐\nrithms. We already saw an application of supervised machine learning in Chapter 1:\nclassifying iris flowers into several species using physical measurements of the\nflowers.\nRemember that supervised learning is used whenever we want to predict a certain\noutcome from a given input, and we have examples of input/output pairs. We build a\nmachine learning model from these input/output pairs, which comprise our training\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\nvised learning often requires human effort to build the training set, but afterward\nautomates and often speeds up an otherwise laborious or infeasible task.\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\ninformation about Safari Books Online, please visit us online.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://bit.ly/intro-machine-learning-python.\nTo comment or ask technical questions about this book, send email to bookques‐\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our web‐\nsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nFrom Andreas\nWithout the help and support of a large group of people, this book would never have\nexisted.\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\nand John Myles White, who took the time to read the early versions of this book and\nprovided me with invaluable feedback—in addition to being some of the corner‐\nstones of the scientific open source ecosystem.\nPreface \n| \nxi\nI am forever thankful for the welcoming open source scientific Python community,\nespecially the contributors to scikit-learn. Without the support and help from this\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package. packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nndarray class as “NumPy arrays” or just “arrays.”\nSciPy\nSciPy is a collection of functions for scientific computing in Python. It provides,\namong other functionality, advanced linear algebra routines, mathematical function\noptimization, signal processing, special mathematical functions, and statistical distri‐\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\nsparse matrices, which are another representation that is used for data in scikit-\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\nmostly zeros:\nIn[3]:\nfrom scipy import sparse\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\neye = np.eye(4)\nprint(\"NumPy array:\\n{}\".format(eye))\nOut[3]:\nNumPy array:\n[[ 1.  0.  0.  0.]\n [ 0.  1.  0.  0.]\n [ 0.  0.  1.  0.]\n [ 0.  0.  0.  1.]]\nIn[4]:\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\n# Only the nonzero entries are stored\nsparse_matrix = sparse.csr_matrix(eye)\nprint(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\nOut[4]:\nSciPy sparse CSR matrix:\n  (0, 0)    1.0\n  (1, 1)    1.0\n  (2, 2)    1.0\n  (3, 3)    1.0\n8 \n| \nChapter 1: Introduction\nUsually it is not possible to create dense representations of sparse data (as they would\nnot fit into memory), so we need to create sparse representations directly. Here is a\nway to create the same sparse matrix as before, using the COO format:\nIn[5]:\ndata = np.ones(4)\nrow_indices = np.arange(4)\ncol_indices = np.arange(4)\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"COO representation:\\n{}\".format(eye_coo))\nOut[5]:\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with accompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nusing the sepal and petal measurements. This means that a machine learning model\nwill likely be able to learn to separate them.\nBuilding Your First Model: k-Nearest Neighbors\nNow we can start building the actual machine learning model. There are many classi‐\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\nneighbors classifier, which is easy to understand. Building this model only consists of\nstoring the training set. To make a prediction for a new data point, the algorithm\nfinds the point in the training set that is closest to the new point. Then it assigns the\nlabel of this training point to the new data point.\n20 \n| \nChapter 1: Introduction\nThe k in k-nearest neighbors signifies that instead of using only the closest neighbor\nto the new data point, we can consider any fixed number k of neighbors in the train‐\ning (for example, the closest three or five neighbors). Then, we can make a prediction\nusing the majority class among these neighbors. We will go into more detail about\nthis in Chapter 2; for now, we’ll use only a single neighbor.\nAll machine learning models in scikit-learn are implemented in their own classes,\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\nwe can use the model, we need to instantiate the class into an object. This is when we\nwill set any parameters of the model. The most important parameter of KNeighbor\nsClassifier is the number of neighbors, which we will set to 1:\nIn[25]:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nThe knn object encapsulates the algorithm that will be used to build the model from\nthe training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nEvaluating the Model                                                                                                   22\nSummary and Outlook                                                                                                   23\niii\n2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\nClassification and Regression                                                                                         25\nGeneralization, Overfitting, and Underfitting                                                             26\nRelation of Model Complexity to Dataset Size                                                         29\nSupervised Machine Learning Algorithms                                                                  29\nSome Sample Datasets                                                                                                 30\nk-Nearest Neighbors                                                                                                    35\nLinear Models                                                                                                               45\nNaive Bayes Classifiers                                                                                                 68\nDecision Trees                                                                                                               70\nEnsembles of Decision Trees                                                                                      83\nKernelized Support Vector Machines                                                                        92\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\n# build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nThe plot shows the training and test set accuracy on the y-axis against the setting of\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\nrecognize some of the characteristics of overfitting and underfitting (note that\nbecause considering fewer neighbors corresponds to a more complex model, the plot\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\nbors are considered, the model becomes simpler and the training accuracy drops. The\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\nbors, indicating that using the single nearest neighbor leads to a model that is too\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\nand performance is even worse. The best performance is somewhere in the middle,\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\nworst performance is around 88% accuracy, which might still be acceptable.\nSupervised Machine Learning Algorithms \n| \n39\nFigure 2-7. Comparison of training and test accuracy as a function of n_neighbors\nk-neighbors regression\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\nadded three test data points as green stars on the x-axis. The prediction using a single might be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\ncommon application is prediction of time series (like stock prices), which also has a\nwhole body of literature devoted to it. There are many more machine learning tasks\nout there—much more than we can list here—and we encourage you to seek out\ninformation from books, research papers, and online communities to find the para‐\ndigms that best apply to your situation.\nProbabilistic Modeling, Inference, and Probabilistic Programming\nMost machine learning packages provide predefined machine learning models that\napply one particular algorithm. However, many real-world problems have a particular\nstructure that, when properly incorporated into the model, can yield much better-\nperforming predictions. Often, the structure of a particular problem can be expressed\nusing the language of probability theory. Such structure commonly arises from hav‐\ning a mathematical model of the situation for which you want to predict. To under‐\nstand what we mean by a structured problem, consider the following example.\nLet’s say you want to build a mobile application that provides a very detailed position\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\nprovides many sensors to help you get precise location measurements, like the GPS,\naccelerometer, and compass. You also have an exact map of the area. This problem is\nhighly structured. You know where the paths and points of interest are from your\nmap. You also have rough positions from the GPS, and the accelerometer and com‐\npass in the user’s device provide you with very precise relative measurements. But\nthrowing these all together into a black-box machine learning system to predict posi‐ CHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and",
            "summary": "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the mos",
            "children": [
                {
                    "id": "chapter-1-section-1",
                    "title": "Why Machine Learning?",
                    "content": "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
                    "summary": "We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. As mathematics (probabilitytheory, in particular) is the foundation upon which machine learning is built, we won’t go into the analysis of the algorithms in great detail. We recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. There are many books on machine learning and AI. However, all of them are meant for graduate students or PhD students in computer science. This is in stark contrast with how machine learning is being used in research and commercial applications. We will not describe how to write machine learning algorithms from scratch. Instead, we will focus on how to use the large array of models already implemented in scikit-learn and other libraries. We hope this book will help you understand machine learning more effectively and more easily in the real world. Back to Mail Online home.back to the page you came from. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes how to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is published by Piatkus, a division of Penguin Books, and is available in hardback and e-book form. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here. For support in the U.S., call the National O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://safaribooksonline.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. All rights reserved.Printed in the United States of America.Published by O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. The publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate. They disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of this work. Use of the information or instructions in this book is at your own peril. Introduction to Machine Learning withPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. The book is published by O'Reilly, which is a division of Penguin Random House, LLC. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org for details. I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at theintersection of statistics, artificial intelligence, and computer science. The application of machinelearning methods has in recent years become ubiquitous in everyday life. And finally, thanks to DTS, for your everlasting and endless support. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning. Chapters 2 and 3 describe the actual machine learning algorithms that are most commonly used in practice. Chapter 5 covers advanced methods for model evaluation and parameter tuning. Chapter 6 explains the concept of pipelines for chaining models and encapsulatly encapsulating your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advanced topics such as probability theory and calculus. The book is written in the style of a computer science textbook, with a focus on machine learning and data analysis. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 to evaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” which is free to download. You are not responsible for errors or omissions, including without limitation responsibility for damages resulting from the use of this work or reliance on this work. For more in-depth documentation of the classes and functions, and many examples, visit the scik it-learn website. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open sourcelicenses or the intellectual property rights of others, it is your responsibility to ensure that your use of such licenses and/or rights complies with such licenses. For more information on how to use this book, please visit the book’s website. The book is available in English, Spanish, Portuguese, and French. The Spanish version has been translated from the Spanish to the English version of this article. The English version includes the Spanish version of the book, as well as the Portuguese version. The French version includes a Spanish version, which is available from the book Machine Learning methods has become ubiquitous in recent years. Machine Learning can help you solve problems and improve your life. Use these questions to help you understand Machine Learning and how to use it in your daily life. Many modern websites and devices have machine learning algorithms at theircore. Machine learning has had a tremendous influence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn’t need to be as large-scale or world-changing as these exam‐ples in order to benefit from machine learning, though. For more information on how to use machine learning in your everyday life, visit the Machine Learning Project website or go to www.machinelearningproject.org. To learn more about the book and its features, go to In the early days of “intelligent” applications, many systems used handcoded rules to process data or adjust to user input. Manually crafting decision rules is feasible for some applications, particularly those in which humans have a good understanding of the process. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. We will show you how to build your first machine learning model, introducing important concepts along the way. And we will conclude with a look at how to use machine learning to improve your current applications and develop new ones. We hope this chapter will help you understand how machine learning works and how to apply it to your own applications. We hope we have convinced you of the usefulness of machine learning in a wide range of applications. Using handcoded rules to make decisions has two major disad‐                vantages. Defining the problem and collecting the data are also important aspects of real-world problems. Representing the problem in the right way might be much moreimportant than squeezing the last percent of accuracy out of a classifier.Keep digging into the data, and don’t lose sight of the larger picture. We’ll be back in a few weeks with a new chapter on how to use Python to build your own machine learning systems. We hope you’ve learned a lot from this first chapter. Bayes classifiers cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐                sions. Others might not need immediate responses, and so it can be possible to have humans confirm uncertain decisions. Medical applications, for example, might need very high levels of precision that possibly cannot be achieved by a machine learning algorithm alone. Bayes classifier cesses can be used to make decisions about movie reviews, medical applications, and other applications. For more information on how to use Bayes, visit the Bayes Institute for Machine Learning at the University of California, San Diego, or go to: www.bayes.org. The tools we’ve discussed in this book are great for many machine learning applications. Python and scikit-learn are also used in production systems in many organizations. However, many companies have complex infrastructure, and it is not always easy to include Python in these systems. That is not necessarily a problem, however, as Python can be used in a variety of ways to make complex decisions more easily. The book concludes with a look at how to use these tools in your own production systems. For more information, visit the book’s website or go to: http://www.pklearn.org/book/machine-learning-and- Many companies use languages like Go, Scala, C++, and Java to build robust, scalable systems. Vowpal wabbit is a highly optimized machine learning package written in C++ with a command-line interface. This can be easier than using a whole library or programming language and converting from and to the different data formats. It's a good way to test and test data before using it in a production system. It can also be used to test data that is too big or too small to fit in a single application. It is a great way to get the most out of a large data set, but it can be difficult to test in a small amount of time. This book focuses on the most common machine learning tasks. There are many more kinds of machine learning out there, with many important applications. For running machine learning algorithms distributed on a cluster, mllib is a Scala library built on top of the spark distributed computing environment. vw is particularly useful for large datasets and for streaming data. The book also covers ranking, Recommender Systems, and Other Kinds of Learning. For more information, visit the book’s website or read the chapters in the book by clicking the link below. The final chapter will be published in the spring of 2014. The full book will be available in the summer of 2014, with a price tag of $39.99. You input a search query and obtain a sorted list of answers, ranked by howrelevant they are. A great introduction to ranking is provided in Manning, Raghavan, and Schütze’s book Introduction to Information Retrieval. The second topic is recom‐mender systems, which provide suggestions to users based on their preferences. You’ve probably encountered recommender systems under headings like “People YouMay Know,” “Customers Who Bought This Item Also Bought,’ or “Top Picks for you.” There is plenty of literature on the topic, and if you want This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapters 2 and 3 describe the actual machine learning algorithms that are most widely used in practice. Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐ing your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advanced computer sciencetopics. The book is available in English and Spanish, with a particular focus on cross-validation and grid search. For more information, or to order your copy, visit the book's website. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 toevaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” that includes some of the material in this book. For more in-depth documentation of the classes and functions, and many examples, go to the scik it-learn website. The book is available in English, German, French and Spanish. When going deep into the technical aspects of machine learning, it is easy to lose sight of the ultimate goals. Many people spend a lot of time building complex machine learning solutions, only to find out they don’t solve the right problem. In a larger context, the algorithms and methods in machine learning are only onepart of a greater process to solve a particular problem, and it is good to keep the big picture in mind at all times. In this book, we will look at how you can use machine learning to help you solve problems in the real world. We will also look at some of the challenges you may face when using machine learning in your own research or business. We hope this book will help you overcome some of Python has become the lingua franca for many data science applications. It combines the power of general-purpose programming languages with the ease of use of domain-specific scripting languages like MATLAB or R. Python has libraries for dataloading, visualization, statistics, natural language processing, image processing, and more. One of the main advantages of using Python is the ability to interact directly with the code, using a terminal or other tools like the JupyterNotebook, which we’ll look at shortly. It is essential to keep in mind all the assumptions that you might be making, explicitly or implicitly, when you start building machine learning models. The most important thing to remember is that it is never too early to start The machine learning algorithm is usually only a small part of a larger dataanalysis and decision-making process. To make effective use of machine learning, we need to take a step back and consider the problem at large. First, you should think about what kind of question you want to answer. Then, think about how you would like to use machine learning to solve your data-related problem. Finally, take a look at some additional resources that you can use to help you improve your machine learning and data science skills. For more information on how to get started with machine learning in the U.S., visit the Machine Learning Project website or go to the Machine learning Project’s official website. If you have a goal, you should first think about how to achieve it. For example, how do you measure success of an algorithm? If you want to find out if an algorithm is working, you need to know how to measure success. You can then use that information to make a decision about whether or not to use the algorithm in the future. For more information on how to use an algorithm, go to: http://www.cnn.com/2013/01/29/technology/how-to-use-an algorithm.html?pagewinners=1&cPath=http://www cni.org/2013-01-29/tech/news/topics/fraud Machine learning methods have become ubiquitous in everyday life. Many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ­ence on the way data-driven research is done today. For more information on how to use machine learning in your everyday life, visit the Machine Learning Institute at the University of Toronto. Machine learning has been applied to diverse scientific problems such as understanding stars and finding distant planets. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. We will also show you how to build your first machine learning model, introducing important concepts along the way. The book is published by Simon & Schuster, a subsidiary of Penguin Random House, and is available in hardback and e-book. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255. The book is a guide to how to use the Python programming language to make better use of data. The book is written in the style of an open-source book, but with an emphasis on making use of Python's built-in features. It is intended to be a guide for people who want to learn more about the use of the programming language, but also for those who just want to make use of some of the tools available in the toolkit. For more information on the book, visit: http://www.cnn.com/2013/01/29/science/features/topical/topics/top-10-most-important-newspapers/top 10-most We will not describe how to write machine learning algorithms from scratch. Instead, we will focus on how to use the large array of models already implemented in scikit-learn and other libraries. We recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, which is available for free at the authors’ website. Today, applying machine learning does not require a PhD. We hope this book will help people understand how machine learning is being used in research and commercial applications. Back to Mail Online home. Back To the page you came from. Machine learning is a rapidly evolving area of machine learning. There are few resources out there that fully cover all the important aspects. This book will help you apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machinelearning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 7 describes the subject of neural networks. Chapter 8 describes the application of neural Networks to a variety of different situations. Chapter 9 describes the use of a number of different types of networks. Recent breakthroughs in machine learning and artificial intelligence have all been driven by these advances. While the progress in this field is so fast-paced that current reference to the state of the art will soon be outdated, the recent book Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press) is a comprehensive introduction into the subject. Even though modern servers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restriction on the size of data you can work with. Not everybody can afford to buy such a large machine, or even to rent one from a cloud provider.2                Scaling to Larger Datasets. In most applications, the data that is used to build a machine learning system is relatively small. This makes expanding your RAM or renting a machine from a cloud provider a viable option. If you need to work with terabytes of data, however, or you need. to process large amounts of data on a budget, there are two basic strategies: out-of-core learning and parallelization over a cluster. The first strategy involves learning from data that cannot be stored in mainmemory, but where the learning takes place on a single computer (or even a single. cluster)",
                    "children": [
                        {
                            "id": "chapter-1-section-1-subsection-1",
                            "title": "Introduction to Machine Learning",
                            "content": "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers",
                            "summary": "We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. As mathematics (probabilitytheory, in particular) is the foundation upon which machine learning is built, we won’t go into the analysis of the algorithms in great detail. We recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman. There are many books on machine learning and AI. However, all of them are meant for graduate students or PhD students in computer science. This is in stark contrast with how machine learning is being used in research and commercial applications. We will not describe how to write machine learning algorithms from scratch. Instead, we will focus on how to use the large array of models already implemented in scikit-learn and other libraries. We hope this book will help you understand machine learning more effectively and more easily in the real world. Back to Mail Online home.back to the page you came from. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes how to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is published by Piatkus, a division of Penguin Books, and is available in hardback and e-book form. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch or click here. For support in the U.S., call the National O’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are also available for most titles (http://safaribooksonline.com). For more information, contact our corporate/institutional sales department: 800-998-9938 or corporate@oreilly.com. All rights reserved.Printed in the United States of America.Published by O'Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. The publisher and the authors have used good faith efforts to ensure that the information and instructions contained in this work are accurate. They disclaim all responsibility for errors or omissions, including without limitation responsibility for damages resulting from the use of this work. Use of the information or instructions in this book is at your own peril. Introduction to Machine Learning withPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. The book is published by O'Reilly, which is a division of Penguin Random House, LLC. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org for details. I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at theintersection of statistics, artificial intelligence, and computer science. The application of machinelearning methods has in recent years become ubiquitous in everyday life. And finally, thanks to DTS, for your everlasting and endless support. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning. Chapters 2 and 3 describe the actual machine learning algorithms that are most commonly used in practice. Chapter 5 covers advanced methods for model evaluation and parameter tuning. Chapter 6 explains the concept of pipelines for chaining models and encapsulatly encapsulating your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advanced topics such as probability theory and calculus. The book is written in the style of a computer science textbook, with a focus on machine learning and data analysis. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 to evaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” which is free to download. You are not responsible for errors or omissions, including without limitation responsibility for damages resulting from the use of this work or reliance on this work. For more in-depth documentation of the classes and functions, and many examples, visit the scik it-learn website. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open sourcelicenses or the intellectual property rights of others, it is your responsibility to ensure that your use of such licenses and/or rights complies with such licenses. For more information on how to use this book, please visit the book’s website. The book is available in English, Spanish, Portuguese, and French. The Spanish version has been translated from the Spanish to the English version of this article. The English version includes the Spanish version of the book, as well as the Portuguese version. The French version includes a Spanish version, which is available from the book Machine Learning methods has become ubiquitous in recent years. Machine Learning can help you solve problems and improve your life. Use these questions to help you understand Machine Learning and how to use it in your daily life. Many modern websites and devices have machine learning algorithms at theircore. Machine learning has had a tremendous influence on the way data-driven research is done today. The tools introduced in this book have been applied to diverse scientific problems such as understanding stars, finding distant planets, analyzing DNA sequences, and providing personalized cancer treatments. Your application doesn’t need to be as large-scale or world-changing as these exam‐ples in order to benefit from machine learning, though. For more information on how to use machine learning in your everyday life, visit the Machine Learning Project website or go to www.machinelearningproject.org. To learn more about the book and its features, go to In the early days of “intelligent” applications, many systems used handcoded rules to process data or adjust to user input. Manually crafting decision rules is feasible for some applications, particularly those in which humans have a good understanding of the process. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. We will show you how to build your first machine learning model, introducing important concepts along the way. And we will conclude with a look at how to use machine learning to improve your current applications and develop new ones. We hope this chapter will help you understand how machine learning works and how to apply it to your own applications. We hope we have convinced you of the usefulness of machine learning in a wide range of applications. Using handcoded rules to make decisions has two major disad‐                vantages. Defining the problem and collecting the data are also important aspects of real-world problems. Representing the problem in the right way might be much moreimportant than squeezing the last percent of accuracy out of a classifier.Keep digging into the data, and don’t lose sight of the larger picture. We’ll be back in a few weeks with a new chapter on how to use Python to build your own machine learning systems. We hope you’ve learned a lot from this first chapter. The study includes data representation choices on, 211evaluation and improvement, 251-252                evaluation metrics and scoring, 275-302iris classification application, 13-23                overfitting vs. underfitting, 28                pipeline preprocessing and 317                selecting with grid search, 319                theory behind, 361                tuning parameters with gridsearch, 260-275                movie reviews.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-1-subsection-2",
                            "title": "Importance of ML",
                            "content": "cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is",
                            "summary": "The tools we’ve discussed in this book are great for many machine learning applications. They allow very quick analysis and prototyping. Many applications are dominated by “simple cases,” for which an algo can make a decision. There are relatively few “complicated cases” which can be confidentlyouted to a human. We’ll end the book with a look at the next step in the development of machine learning tools for self-driving cars, drones, and other applications. The next chapter will look at how we can use machine learning to improve the quality of our products and services. The book will be available in hardback and e-book. Python and scikit-learn are also used in production systems in many organizations. Data analysis has different requirements from building live services, so using different languages for these tasks makes sense. A relatively common solution is to reimplement the solution that was found by the analytics team inside the larger framework, using a high-performance language. In many companies, the data analytics teamswork with languages like Python and R that allow the quick testing of ideas, while the production teams work with Go, Scala, C++, and Java to build robust, Scalable systems. For more information on how to use Python in production, visit the Python Regardless of whether you can use scikit-learn in a production system or not, it is. easier thanembedding a whole library or programming language and converting",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-1-subsection-3",
                            "title": "ML Applications",
                            "content": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
                            "summary": "Vowpal wabbit is a highly optimized machine learning package written in C++ with a command-line interface. For running machine learning algorithms distributed on a cluster, mllib is a Scala library built on top of the spark distributed computing environment. There are many more kinds of machine learning out there, with many important applications. We focused on the most common machine learning tasks: classification and regression in supervised learning, and clustering and signaling decomposition in unsupervised learning. We did not cover two partic‐                ularly important topics that we didn't cover in this book. We hope that this book has helped you understand some of the most important topics in machine learning. Back to the page you came from. You’ve probably already used a ranking system today. You input a search query and obtain a sorted list of answers, ranked by howrelevant they are. The second topic is recom­mender systems, which provide suggestions to users based on their preferences. There is plenty of literature on the topic, and if you want to dive right in you                might be interested in the now classic “Netflix prize challenge’.  The Netflix prize challenge offered a prize of $1 million to the team that could provide the best recommendations. It was won by a team that was able to find the perfect movie based on a large dataset of movie preferences. The winner of the prize was awarded a $2 million prize. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapters 2 and 3 describe the actual machine learning algorithms that are most widely used in practice. Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐ing your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advanced computer sciencetopics. The book is available in English and Spanish, with a particular focus on cross-validation and grid search. For more information, or to order your copy, visit the book's website. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 toevaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” that includes some of the material in this book. For more in-depth documentation of the classes and functions, and many examples, go to the scik it-learn website. The book is available in English, German, French and Spanish. When going deep into the technical aspects of machine learning, it is easy to lose sight of the ultimate goals. Many people spend a lot of time building complex machine learning solutions, only to find out they don’t solve the right problem. In a larger context, the algorithms and methods in machine learning are only onepart of a greater process to solve a particular problem, and it is good to keep the big picture in mind at all times. In this book, we will look at how you can use machine learning to help you solve problems in the real world. We will also look at some of the challenges you may face when using machine learning in your own research or business. We hope this book will help you overcome some of Python has become the lingua franca for many data science applications. It combines the power of general-purpose programming languages with the ease of use of domain-specific scripting languages like MATLAB or R. Python has libraries for dataloading, visualization, statistics, natural language processing, image processing, and more. One of the main advantages of using Python is the ability to interact directly with the code, using a terminal or other tools like the JupyterNotebook, which we’ll look at shortly. It is essential to keep in mind all the assumptions that you might be making, explicitly or implicitly, when you start building machine learning models. The most important thing to remember is that it is never too early to start The machine learning algorithm is usually only a small part of a larger dataanalysis and decision-making process. To make effective use of machine learning, we need to take a step back and consider the problem at large. First, you should think about what kind of question you want to answer. Then, think about how you would like to use machine learning to solve your data-related problem. Finally, take a look at some additional resources that you can use to help you improve your machine learning and data science skills. For more information on how to get started with machine learning in the U.S., visit the Machine Learning Project website or go to the Machine learning Project’s official website. If you have a goal, you should first think about how to achieve it. For example, how do you measure success of an algorithm? If you want to find out if an algorithm is working, you need to know how to measure success. You can then use that information to make a decision about whether or not to use the algorithm in the future. For more information on how to use an algorithm, go to: http://www.cnn.com/2013/01/29/technology/how-to-use-an algorithm.html?pagewinners=1&cPath=http://www cni.org/2013-01-29/tech/news/topics/fraud Machine learning methods have become ubiquitous in everyday life. Many modern websites and devices have machine learning algorithms at their core. When you look at a complex website like Facebook, Amazon, or Netflix, it is very likely that every part of the site contains multiple machine learning models. Outside of commercial applications, machine learning has had a tremendous influ­ence on the way data-driven research is done today. For more information on how to use machine learning in your everyday life, visit the Machine Learning Institute at the University of Toronto. Machine learning has been applied to diverse scientific problems such as understanding stars and finding distant planets. In this chapter, we will explain why machine learning has become so popular and discuss what kinds of problems can be solved using machine learning. We will also show you how to build your first machine learning model, introducing important concepts along the way. The book is published by Simon & Schuster, a subsidiary of Penguin Random House, and is available in hardback and e-book. For confidential support call the Samaritans on 08457 90 90 90, visit a local Samaritans branch, or see www.samaritans.org. In the U.S. call the National Suicide Prevention Line on 1-800-273-8255. The book is a guide to how to use the Python programming language to make better use of data. The book is written in the style of an open-source book, but with an emphasis on making use of Python's built-in features. It is intended to be a guide for people who want to learn more about the use of the programming language, but also for those who just want to make use of some of the tools available in the toolkit. For more information on the book, visit: http://www.cnn.com/2013/01/29/science/features/topical/topics/top-10-most-important-newspapers/top 10-most We will not describe how to write machine learning algorithms from scratch. Instead, we will focus on how to use the large array of models already implemented in scikit-learn and other libraries. We recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, which is available for free at the authors’ website. Today, applying machine learning does not require a PhD. We hope this book will help people understand how machine learning is being used in research and commercial applications. Back to Mail Online home. Back To the page you came from. Machine learning is a rapidly evolving area of machine learning. There are few resources out there that fully cover all the important aspects. This book will help you apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machinelearning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 7 describes the subject of neural networks. Chapter 8 describes the application of neural Networks to a variety of different situations. Chapter 9 describes the use of a number of different types of networks. Recent breakthroughs in machine learning and artificial intelligence have all been driven by these advances. While the progress in this field is so fast-paced that current reference to the state of the art will soon be outdated, the recent book Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press) is a comprehensive introduction into the subject. Even though modern servers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restriction on the size of data you can work with. Not everybody can afford to buy such a large machine, or even to rent one from a cloud provider.2                Scaling to Larger Datasets. In most applications, the data that is used to build a machine learning system is relatively small. This makes expanding your RAM or renting a machine from a cloud provider a viable option. If you need to work with terabytes of data, however, or you need. to process large amounts of data on a budget, there are two basic strategies: out-of-core learning and parallelization over a cluster. The first strategy involves learning from data that cannot be stored in mainmemory, but where the learning takes place on a single computer (or even a single. cluster)",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-2",
                    "title": "Problems Machine Learning Can Solve",
                    "content": "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nmore complex models. However, simply duplicating the same data points or collect‐\ning very similar data will not help.\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\nand all of them complied with the rule “If the customer is older than 45, and has less\nthan 3 children or is not divorced, then they want to buy a boat,” we would be much\nmore likely to believe this to be a good rule than when it was developed using only\nthe 12 rows in Table 2-1.\nHaving more data and building appropriately more complex models can often work\nwonders for supervised learning tasks. In this book, we will focus on working with\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\nmuch data to collect, which might be more beneficial than tweaking and tuning your\nmodel. Never underestimate the power of more data.\nSupervised Machine Learning Algorithms\nWe will now review the most popular machine learning algorithms and explain how\nthey learn from data and how they make predictions. We will also discuss how the\nconcept of model complexity plays out for each of these models, and provide an over‐\nSupervised Machine Learning Algorithms \n| \n29\n4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\nfor more details.\nview of how each algorithm builds a model. We will examine the strengths and weak‐\nnesses of each algorithm, and what kind of data they can best be applied to. We will\nalso explain the meaning of the most important parameters and options.4 Many algo‐\nrithms have a classification and a regression variant, and we will describe both.\nIt is not necessary to read through the descriptions of each algorithm in detail, but\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\n• The logic required to make a decision is specific to a single domain and task.\nChanging the task even slightly might require a rewrite of the whole system.\n• Designing rules requires a deep understanding of how a decision should be made\nby a human expert.\nOne example of where this handcoded approach will fail is in detecting faces in\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\ntion was an unsolved problem until as recently as 2001. The main problem is that the\nway in which pixels (which make up an image in a computer) are “perceived” by the\ncomputer is very different from how humans perceive a face. This difference in repre‐\nsentation makes it basically impossible for a human to come up with a good set of\nrules to describe what constitutes a face in a digital image.\nUsing machine learning, however, simply presenting a program with a large collec‐\ntion of images of faces is enough for an algorithm to determine what characteristics\nare needed to identify a face.\nProblems Machine Learning Can Solve\nThe most successful kinds of machine learning algorithms are those that automate\ndecision-making processes by generalizing from known examples. In this setting,\nwhich is known as supervised learning, the user provides the algorithm with pairs of\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\nput given an input. In particular, the algorithm is able to create an output for an input\nit has never seen before without any help from a human. Going back to our example\nof spam classification, using machine learning, the user provides the algorithm with a\nlarge number of emails (which are the input), together with information about particular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n188 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-37. Cluster assignments found by DBSCAN with varying settings for the\nmin_samples and eps parameters\nIn this plot, points that belong to clusters are solid, while the noise points are shown\nin white. Core samples are shown as large markers, while boundary points are dis‐\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\nmeans that more points will be included in a cluster. This makes clusters grow, but\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\nfrom top to bottom in the figure) means that fewer points will be core points, and\nmore points will be labeled as noise.\nThe parameter eps is somewhat more important, as it determines what it means for\npoints to be “close.” Setting eps to be very small will mean that no points are core\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\nwill result in all points forming a single cluster.\nThe min_samples setting mostly determines whether points in less dense regions will\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\nthat would have been a cluster with less than min_samples many samples will now be\nlabeled as noise. min_samples therefore determines the minimum cluster size. You\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\nClustering \n| \n189\npoints, one of five points, and one of three points. Using min_samples=5, the two",
                    "summary": "There are two major types of supervised machine learning problems. Classification and Regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. Regression is the process of learning to accept a label from a list of possible labels, rather than choosing one from a set of choices. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter, we will look at the problem of binary classification, which tries to answer a yes/no When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                Out[120]:                unique classes in training data: ['setosa' 'versicolor' 'virginica']                predictions: ['versicolors' 'setosa', 'v Virginica', 'versColors'  'versColours'  'vers Colours' }. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. You should now be in a position where you have some idea of how to apply, tune, and meticulouslyanalyze the models we discussed here. In this chapter, we focused on the binary clas ­-ification case, as this is usually easiest to understand. For small datasets, good as a baseline, easy to explain. In the next two chapters, we will discuss how to use the models described here in a specific situa­tion. We will conclude with a quick summary of when to use each model and how to tune the models to suit your needs. We hope that this chapter has given you a better understanding of the algorithms we discussed in this chapter and will help you apply machine learning in practice. Most of the algorithms in scikit-learn have classification and regression variants. All of the classification algorithms support both binary and multiclass classification. Try applying any of these algorithms to the built-in datasets in sckit- learn. Playing around with the algorithms on different datasets will give you a better feel for how to use them in the real world. For more information on how to apply these algorithms, visit the Scikit Learning website. For a more detailed look at how the algorithms work, go to: http://www.scikitlearn.com/algorithms/supervised-learning. Unsupervised learning subsumes all kinds of machine learning algorithms where there is no known output. In unsupervisedlearning, the learning algorithm is just shown the input data and asked to extract knowledge from this data. We will see how to properly adjust parameters                and how to find good parameters automatically in Chapter 6. First, though, we will dive in more detail into unsuper supervised learning and preprocess‐                ing in the next chapter.Summary and Outlook |                 129                            Chapters 3 and 4: Unsupervised Learning and Preprocessing, Machine Learning and Clustering, and Machine learning and Preprocessing. The machine learning algorithm is usually only a small part of a larger dataanalysis and decision-making process. To make effective use of machine learning, we need to take a step back and consider the problem at large. First, you should think about what kind of question you want to answer. Then, think about how you would like to use machine learning to solve your data-related problem. Finally, take a look at some additional resources that you can use to help you improve your machine learning and data science skills. For more information on how to get started with machine learning in the U.S., visit the Machine Learning Project website or go to the Machine learning Project’s official website. If you have a goal, you should first think about how to achieve it. For example, how do you measure success of an algorithm? If you want to find out if an algorithm is working, you need to know how to measure success. You can then use that information to make a decision about whether or not to use the algorithm in the future. For more information on how to use an algorithm, go to: http://www.cnn.com/2013/01/29/technology/how-to-use-an algorithm.html?pagewinners=1&cPath=http://www cni.org/2013-01-29/tech/news/topics/fraud Metrics for Binary Classification are arguably the most common and conceptually simple applica­tion of machine learning in practice. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it it pays off to find the closest metric to the original business goal that is feasible to evaluate. The closest metric should be used whenever possible. The number of errors we make does not contain all the information we are interested in. We often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. We would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. The number of mistakes we make is not a good measure of predictive performance, as the number of Errors is not the same as Accuracy. Having more data and building appropriately more complex models can often work with supervised learning tasks. We can’t assume that our model will always work perfectly, and it will make mistakes. In the real world, you often have the ability to decide how much data to collect, which might be more beneficial than tweaking and tuning your model. In this book, we will focus on working withdatasets of fixed sizes. We will also focus on how to work with data points that are of fixed size. For more information, or to get help with your own supervised learning task, visit the supervised learning website, or go to www.supervisedlearning.org. Back to the page you came from. Supervised Machine Learning Algorithms. Never underestimate the power of more data. Review the most popular machine learning algorithms and explain how they learn from data and how they make predictions. We will also discuss how the concept of model complexity plays out for each of these models, and provide an over-view of how each algorithm builds a model. Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation for more details. The book is available in English and Spanish. 4 Many algo‐rithms have a classification and a regression variant, and we will describe both. It is not necessary to read through the descriptions of each algorithm in detail, but understanding the models will give you a better feeling for the different waysmachine learning algorithms can work. We hope we have convinced you of the usefulness of machine learning in a wide range of applications. This chapter can also be used as a referenceguide, and you can come back to it when you are unsure about the workings of any of the algorithms. Keep digging into the data, and don’t lose sight of the larger picture. We hope this chapter has convinced you that machine learning can be implemented in practice in a variety of ways, and that it can be used to help you solve real-world problems. Back to the page you came from. The next chapter is titled “Wrapping Up” and includes the index, A/B testing, clustering, regression and Bayes. Manually crafting decision rules is feasible for some applications. However, using handcoded rules to make decisions has two major disad­vantages. The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. The main problem is that the way in which pixels (which make up an image in a computer) are “perceived” by the computer is very different from how humans perceive a face. Today, every smartphone can detect a face in an image. But this was an unsolved problem until as recently as 2001. Using machine learning, a program can determine what characteristics are needed to identify a face. The algorithm is able to create an output for an input it has never seen before without any help from a human. This makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out‐                put given an input. The goal of this article is to show how machine learning can be used to solve problems This is not surprising, given the results of DBSCAN, which tried to cluster all points together. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that there is no notion of cluster center in agglomerative clustering, and we simply show the first couple of points in each cluster. We could have used a different clustering method to get the mean. In[82] we show the number of points in each cluster to the left of the first image. Some of the clusters seem to have a semantic theme, many of them are too large to be actually homogeneous. We show the results of unsupervised learning and preprocessing in Figure 3-46. In[82) we show random images from the clusters generated by In. Each row corre‐porre‐sponds to one cluster; the number to the right lists number of images in that cluster. We also show the result of Unsupervised Learning and In[86]:# extract clusters with ward agglomerative clustering. n_clusters = 40; for cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters. mask = labels_agg == cluster; print(np.bincount(labels_agg), cluster)Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5. Figure 3-39. Cluster assignments found by the same algorithm with eps set to 0.5; cluster size set to 1.0. There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. The clustering algorithm k-means is based on the idea that each point is its own cluster. The Agglomerative Clustering algorithm merges the two most similar clusters until some stopping criterion is satis‐ophobicfied. Next, we will look at two more clustering algorithms that improve upon these proper‐ autoimmuneties in some ways. We also include what it looks like when we randomly assign points to two Carbuncleclusters for comparison (see Figure 3-39), and how the clustering results look when we plot the data on a plot. We will end with a look at the results of our clustering experiment. Back to the page you came from.. There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐insureding. All three have a way of controlling the granularity of clustering. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. The following plot (Figure 3-33) illustrates the progression of agglomersative cluster-centricing on a two-dimensional dataset, looking for three clusters. In the next chapter, we will look at unsupervised K-means allows for a char‐acterization of the clusters using the cluster means. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. All three methods can be used on large, real-world datasets, and are rel‐phthalatively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths and weaknesses, but they all have a common goal: clustering large data points into many small clusters. The three methods are: k-mean, DBS CAN, and DBSCLIM. This chapter introduced a range of unsupervised learning algorithms that can be applied for exploratory data analysis and preprocessing. Agglomerative clustering can provide a whole hierarchy of possible configurations of the data, which can be easily inspected via dendrograms. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data in the absence of supervision information. Having the right representa­ntion of theData is often crucial for supervised or unsuper supervised learning to succeed. In[55]: # generate some random cluster data and transform the data to be stretched. Even in a supervised setting, exploratory algorithms can be used. In Chapter 3: Unsupervised Learning and Preprocessing, we plot the data into three clusters and plot the cluster assignments and cluster centers. In Figure 3-28, we show the results of the exploratory algorithm in the form of a figure. K-means fails to identify nonspherical clusters. It also performs poorly if the clusters have more complex shapes. Here, we would hope that the clustering algorithm can discover the two half-moonshapes. The algorithm was used to generate synthetic two_moons data (with less noise this time) The results are shown in Figure 3-29 of Chapter 2 of the book, \"K-Means and the Two-Moons Algorithm,\" which is available on Amazon.com for $99.99. For confidential support, call the Samaritans on 08 K-means is a clustering algorithm, but there are parallels between it and decomposition methods like PCA and NMF. Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the min_samples and eps parameters. Core samples are shown as large markers, while boundary points are dis‐played as smaller markers. In this plot, points that belong to clusters are solid, while the noise points are shown in white.Figure 3-38. Unsupervised Learning and Preprocessing with k-Means algorithm. The algorithm is based on the Kaggle clustering method. It is not possible to use this method with the k- means algorithm, however. The min_samples setting mostly determines whether points in less dense regions will be labeled as outliers or as their own clusters. Increasing eps (going from left to right in the figure) means that more points will be included in a cluster. This makes clusters grow, but might also lead to multiple clusters joining into one. Setting eps to be very small will mean that no points are core points, and may lead to all points being labeled as noise. The min_Samples setting is somewhat more important, as it determines what it means for points to be “close” to each other. If you decrease min-samples, anything that would have been a cluster with less than min-Samples will now With min_samples=3, there are three clusters: one of four. points, one of five points, and one of three points. Using min_samples=5, the two.",
                    "children": [
                        {
                            "id": "chapter-1-section-2-subsection-1",
                            "title": "Classification Problems",
                            "content": "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nmore complex models. However, simply duplicating the same data points or collect‐\ning very similar data will not help.\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\nand all of them complied with the rule “If the customer is older than 45, and has less\nthan 3 children or is not divorced, then they want to buy a boat,” we would be much\nmore likely to believe this to be a good rule than when it was developed using only\nthe 12 rows in Table 2-1.\nHaving more data and building appropriately more complex models can often work\nwonders for supervised learning tasks. In this book, we will focus on working with\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\nmuch data to collect, which might be more beneficial than tweaking and tuning your\nmodel. Never underestimate the power of more data.\nSupervised Machine Learning Algorithms\nWe will now review the most popular machine learning algorithms and explain how\nthey learn from data and how they make predictions. We will also discuss how the\nconcept of model complexity plays out for each of these models, and provide an over‐\nSupervised Machine Learning Algorithms \n| \n29\n4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\nfor more details.\nview of how each algorithm builds a model. We will examine the strengths and weak‐\nnesses of each algorithm, and what kind of data they can best be applied to. We will\nalso explain the meaning of the most important parameters and options.4 Many algo‐\nrithms have a classification and a regression variant, and we will describe both.\nIt is not necessary to read through the descriptions of each algorithm in detail, but\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\n• The logic required to make a decision is specific to a single domain and task.\nChanging the task even slightly might require a rewrite of the whole system.\n• Designing rules requires a deep understanding of how a decision should be made\nby a human expert.\nOne example of where this handcoded approach will fail is in detecting faces in\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\ntion was an unsolved problem until as recently as 2001. The main problem is that the\nway in which pixels (which make up an image in a computer) are “perceived” by the\ncomputer is very different from how humans perceive a face. This difference in repre‐\nsentation makes it basically impossible for a human to come up with a good set of\nrules to describe what constitutes a face in a digital image.\nUsing machine learning, however, simply presenting a program with a large collec‐\ntion of images of faces is enough for an algorithm to determine what characteristics\nare needed to identify a face.\nProblems Machine Learning Can Solve\nThe most successful kinds of machine learning algorithms are those that automate\ndecision-making processes by generalizing from known examples. In this setting,\nwhich is known as supervised learning, the user provides the algorithm with pairs of\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\nput given an input. In particular, the algorithm is able to create an output for an input\nit has never seen before without any help from a human. Going back to our example\nof spam classification, using machine learning, the user provides the algorithm with a\nlarge number of emails (which are the input), together with information about",
                            "summary": "There are two major types of supervised machine learning problems. Classification and Regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. Regression is the process of learning to accept a label from a list of possible labels, rather than choosing one from a set of choices. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter, we will look at the problem of binary classification, which tries to answer a yes/no When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                Out[120]:                unique classes in training data: ['setosa' 'versicolor' 'virginica']                predictions: ['versicolors' 'setosa', 'v Virginica', 'versColors'  'versColours'  'vers Colours' }. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. You should now be in a position where you have some idea of how to apply, tune, and meticulouslyanalyze the models we discussed here. In this chapter, we focused on the binary clas ­-ification case, as this is usually easiest to understand. For small datasets, good as a baseline, easy to explain. In the next two chapters, we will discuss how to use the models described here in a specific situa­tion. We will conclude with a quick summary of when to use each model and how to tune the models to suit your needs. We hope that this chapter has given you a better understanding of the algorithms we discussed in this chapter and will help you apply machine learning in practice. Most of the algorithms in scikit-learn have classification and regression variants. All of the classification algorithms support both binary and multiclass classification. Try applying any of these algorithms to the built-in datasets in sckit- learn. Playing around with the algorithms on different datasets will give you a better feel for how to use them in the real world. For more information on how to apply these algorithms, visit the Scikit Learning website. For a more detailed look at how the algorithms work, go to: http://www.scikitlearn.com/algorithms/supervised-learning. Unsupervised learning subsumes all kinds of machine learning algorithms where there is no known output. In unsupervisedlearning, the learning algorithm is just shown the input data and asked to extract knowledge from this data. We will see how to properly adjust parameters                and how to find good parameters automatically in Chapter 6. First, though, we will dive in more detail into unsuper supervised learning and preprocess‐                ing in the next chapter.Summary and Outlook |                 129                            Chapters 3 and 4: Unsupervised Learning and Preprocessing, Machine Learning and Clustering, and Machine learning and Preprocessing. The machine learning algorithm is usually only a small part of a larger dataanalysis and decision-making process. To make effective use of machine learning, we need to take a step back and consider the problem at large. First, you should think about what kind of question you want to answer. Then, think about how you would like to use machine learning to solve your data-related problem. Finally, take a look at some additional resources that you can use to help you improve your machine learning and data science skills. For more information on how to get started with machine learning in the U.S., visit the Machine Learning Project website or go to the Machine learning Project’s official website. If you have a goal, you should first think about how to achieve it. For example, how do you measure success of an algorithm? If you want to find out if an algorithm is working, you need to know how to measure success. You can then use that information to make a decision about whether or not to use the algorithm in the future. For more information on how to use an algorithm, go to: http://www.cnn.com/2013/01/29/technology/how-to-use-an algorithm.html?pagewinners=1&cPath=http://www cni.org/2013-01-29/tech/news/topics/fraud Metrics for Binary Classification are arguably the most common and conceptually simple applica­tion of machine learning in practice. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it it pays off to find the closest metric to the original business goal that is feasible to evaluate. The closest metric should be used whenever possible. The number of errors we make does not contain all the information we are interested in. We often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. We would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. The number of mistakes we make is not a good measure of predictive performance, as the number of Errors is not the same as Accuracy. Having more data and building appropriately more complex models can often work with supervised learning tasks. We can’t assume that our model will always work perfectly, and it will make mistakes. In the real world, you often have the ability to decide how much data to collect, which might be more beneficial than tweaking and tuning your model. In this book, we will focus on working withdatasets of fixed sizes. We will also focus on how to work with data points that are of fixed size. For more information, or to get help with your own supervised learning task, visit the supervised learning website, or go to www.supervisedlearning.org. Back to the page you came from. Supervised Machine Learning Algorithms. Never underestimate the power of more data. Review the most popular machine learning algorithms and explain how they learn from data and how they make predictions. We will also discuss how the concept of model complexity plays out for each of these models, and provide an over-view of how each algorithm builds a model. Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation for more details. The book is available in English and Spanish. 4 Many algo‐rithms have a classification and a regression variant, and we will describe both. It is not necessary to read through the descriptions of each algorithm in detail, but understanding the models will give you a better feeling for the different waysmachine learning algorithms can work. We hope we have convinced you of the usefulness of machine learning in a wide range of applications. This chapter can also be used as a referenceguide, and you can come back to it when you are unsure about the workings of any of the algorithms. Keep digging into the data, and don’t lose sight of the larger picture. We hope this chapter has convinced you that machine learning can be implemented in practice in a variety of ways, and that it can be used to help you solve real-world problems. Back to the page you came from. The next chapter is titled “Wrapping Up” and includes the index, A/B testing, clustering, regression and Bayes. Manually crafting decision rules is feasible for some applications. However, using handcoded rules to make decisions has two major disad­vantages. The logic required to make a decision is specific to a single domain and task. Changing the task even slightly might require a rewrite of the whole system. Designing rules requires a deep understanding of how a decision should be made by a human expert. The main problem is that the way in which pixels (which make up an image in a computer) are “perceived” by the computer is very different from how humans perceive a face. Today, every smartphone can detect a face in an image. But this was an unsolved problem until as recently as 2001. Using machine learning, a program can determine what characteristics are needed to identify a face. The algorithm is able to create an output for an input it has never seen before without any help from a human. This makes it basically impossible for a human to come up with a good set of rules to describe what constitutes a face in a digital image. The most successful kinds of machine learning algorithms are those that automate decision-making processes by generalizing from known examples. In this setting, the user provides the algorithm with pairs of inputs and desired outputs, and the algorithm finds a way to produce the desired out‐                put given an input. The goal of this article is to show how machine learning can be used to solve problems Using machine learning, the user provides the algorithm with alarge number of emails (which are the input), together with information about them.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-2-subsection-2",
                            "title": "Regression Problems",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-2-subsection-3",
                            "title": "Clustering Problems",
                            "content": "particular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n188 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-37. Cluster assignments found by DBSCAN with varying settings for the\nmin_samples and eps parameters\nIn this plot, points that belong to clusters are solid, while the noise points are shown\nin white. Core samples are shown as large markers, while boundary points are dis‐\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\nmeans that more points will be included in a cluster. This makes clusters grow, but\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\nfrom top to bottom in the figure) means that fewer points will be core points, and\nmore points will be labeled as noise.\nThe parameter eps is somewhat more important, as it determines what it means for\npoints to be “close.” Setting eps to be very small will mean that no points are core\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\nwill result in all points forming a single cluster.\nThe min_samples setting mostly determines whether points in less dense regions will\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\nthat would have been a cluster with less than min_samples many samples will now be\nlabeled as noise. min_samples therefore determines the minimum cluster size. You\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\nClustering \n| \n189\npoints, one of five points, and one of three points. Using min_samples=5, the two",
                            "summary": "Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note thatthere is no notion of cluster center in agglomerative clustering. This is not surprising, given the results of DBSCAN, which tried to cluster all points together. In[82] we show the number of points in each cluster to the left of the first image. Some of the clusters seem to have a semantic theme, many of them are too large to be actually homogeneous. We show the results of unsupervised learning and preprocessing in Figure 3-46. In[82) we show random images from the clusters generated by In. Each row corre‐porre‐sponds to one cluster; the number to the right lists number of images in that cluster. We also show the result of Unsupervised Learning and In[86]:# extract clusters with ward agglomerative clustering. n_clusters = 40; for cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters. mask = labels_agg == cluster; print(np.bincount(labels_agg), cluster)Figure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5. Figure 3-39. Cluster assignments found by the same algorithm with eps set to 0.5; cluster size set to 1.0. There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. The clustering algorithm k-means is based on the idea that each point is its own cluster. The Agglomerative Clustering algorithm merges the two most similar clusters until some stopping criterion is satis‐ophobicfied. Next, we will look at two more clustering algorithms that improve upon these proper‐ autoimmuneties in some ways. We also include what it looks like when we randomly assign points to two Carbuncleclusters for comparison (see Figure 3-39), and how the clustering results look when we plot the data on a plot. We will end with a look at the results of our clustering experiment. Back to the page you came from.. There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐insureding. All three have a way of controlling the granularity of clustering. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. The following plot (Figure 3-33) illustrates the progression of agglomersative cluster-centricing on a two-dimensional dataset, looking for three clusters. In the next chapter, we will look at unsupervised K-means allows for a char‐acterization of the clusters using the cluster means. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. All three methods can be used on large, real-world datasets, and are rel‐phthalatively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths and weaknesses, but they all have a common goal: clustering large data points into many small clusters. The three methods are: k-mean, DBS CAN, and DBSCLIM. This chapter introduced a range of unsupervised learning algorithms that can be applied for exploratory data analysis and preprocessing. Agglomerative clustering can provide a whole hierarchy of possible configurations of the data, which can be easily inspected via dendrograms. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data in the absence of supervision information. Having the right representa­ntion of theData is often crucial for supervised or unsuper supervised learning to succeed. In[55]: # generate some random cluster data and transform the data to be stretched. Even in a supervised setting, exploratory algorithms can be used. In Chapter 3: Unsupervised Learning and Preprocessing, we plot the data into three clusters and plot the cluster assignments and cluster centers. In Figure 3-28, we show the results of the exploratory algorithm in the form of a figure. K-means fails to identify nonspherical clusters. It also performs poorly if the clusters have more complex shapes. Here, we would hope that the clustering algorithm can discover the two half-moonshapes. The algorithm was used to generate synthetic two_moons data (with less noise this time) The results are shown in Figure 3-29 of Chapter 2 of the book, \"K-Means and the Two-Moons Algorithm,\" which is available on Amazon.com for $99.99. For confidential support, call the Samaritans on 08 K-means is a clustering algorithm, but there are parallels between it and decomposition methods like PCA and NMF. Figure 3-37. Cluster assignments found by DBSCAN with varying settings for the min_samples and eps parameters. Core samples are shown as large markers, while boundary points are dis‐played as smaller markers. In this plot, points that belong to clusters are solid, while the noise points are shown in white.Figure 3-38. Unsupervised Learning and Preprocessing with k-Means algorithm. The algorithm is based on the Kaggle clustering method. It is not possible to use this method with the k- means algorithm, however. The min_samples setting mostly determines whether points in less dense regions will be labeled as outliers or as their own clusters. Increasing eps (going from left to right in the figure) means that more points will be included in a cluster. This makes clusters grow, but might also lead to multiple clusters joining into one. Setting eps to be very small will mean that no points are core points, and may lead to all points being labeled as noise. The min_Samples setting is somewhat more important, as it determines what it means for points to be “close” to each other. If you decrease min-samples, anything that would have been a cluster with less than min-Samples will now With min_samples=3, there are three clusters: one of four. points, one of five points, and one of three points. Using min_samples=5, the two.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-2-subsection-4",
                            "title": "Common Use Cases",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-3",
                    "title": "Knowing Your Task and Knowing Your Data",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-1-section-3-subsection-1",
                            "title": "Task Definition",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-2",
                            "title": "Data Understanding",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-3",
                            "title": "Data Requirements",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-4",
                    "title": "Why Python?",
                    "content": "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\n['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n '01', '01pm', '02']\nFeatures 20010 to 20030:\n['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\nEvery 2000th feature:\n['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery',\n 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer',\n 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful',\n 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher',\n 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse',\n 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\ning—apart from \"007\", which in the particular context of movies is likely to refer to\nthe James Bond character.5 Weeding out the meaningful from the nonmeaningful\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\nlection of English words starting with “dra”. You might notice that for \"draught\",\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\nvocabulary as distinct words. These words have very closely related semantic mean‐\nings, and counting them as different words, corresponding to different features,\nmight not be ideal. packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of",
                    "summary": "Number of features: 74849. First 20 features: '000', '00001', '00015', '000s', '001', '003830' Every 2000th feature: 'aesir', 'aquarian', 'barking', 'blustering', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz \"007\", which in the particular context of movies is likely to refer to the James Bond character. All these numbers appear somewhere in the reviews, and are thereforeextracted as words. You might notice that for \"draught\", \"drawback\", and \"drawer\" both the singular and plural forms are contained in the vocabulary as distinct words. These words have very closely related semantic mean‐phthalings, and counting them as different words, corresponding to different features, might not be ideal. There are afew other libraries that will enhance your experience with scikit-learn and pandas. For more information on how to use these libraries, visit the scik it-learn website. Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. All of the code examples we include can be downloaded from GitHub. We only need the Python support for scikitlearn to work with NumPy, SciPy, pandas and matplotlib. The book was in fact written as a JupYter Notebooks. It is a great tool for exploratory data analysis and is widely used by data scientists. It also makes it easy to incorporate code, text and images, and all of the examples are available on GitHub. In scikit-learn, the NumPy array is the fundamental data structure. The core functionality of NumPy is the ndarray class, a n-dimensional array. All elements of the array must be of the same type. We will be using NumPy a lot in this book, and we will refer to objects of the numPy array. You should also install matplotlib, IPython, and the Jupyter Notebook. The book is written in Python, with an emphasis on interactive development. It is intended to be a quick and easy way to get started with programming in Python. The most important things to know about NumPy are: its type system, and how to use it with Anaconda comes with NumPy, SciPy, matplotlib, IPython, Jupyter Notebook, and scikit-learn. Anaconda now also includes the commercial Intel MKL library for free. Using MKL can give significant speed improvements for many algorithms in scikitslearn. Another Python distribution for scientific computing is Enthought Canopy, which comes with IPython and pandas but not ScikitLearn. The free version does not come with the free version of SciktLearn. For people without an existing installation of the scientific Python packages, we suggest using one of the prepack Python(x,y) is a free Python distribution for scientific computing. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. Enthought Canopy is available for Python 2.7.x, andworks on Mac OS, Windows, and Linux. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription ver‐sion of En thought Canopy. For more information on how to use this guide, visit the official website or read the book's Introduction to SciPy and Scikit-learn chapters 1 and 2. The book is also available as a free download.",
                    "children": [
                        {
                            "id": "chapter-1-section-4-subsection-1",
                            "title": "Python Features",
                            "content": "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\n['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n '01', '01pm', '02']\nFeatures 20010 to 20030:\n['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\nEvery 2000th feature:\n['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery',\n 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer',\n 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful',\n 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher',\n 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse',\n 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\ning—apart from \"007\", which in the particular context of movies is likely to refer to\nthe James Bond character.5 Weeding out the meaningful from the nonmeaningful\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\nlection of English words starting with “dra”. You might notice that for \"draught\",\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\nvocabulary as distinct words. These words have very closely related semantic mean‐\nings, and counting them as different words, corresponding to different features,\nmight not be ideal.",
                            "summary": "Number of features: 74849. First 20 features: '000', '00001', '00015', '000s', '001', '003830' Every 2000th feature: 'aesir', 'aquarian', 'barking', 'blustering', 'chicanery', 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer', 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz \"007\", which in the particular context of movies is likely to refer to the James Bond character. All these numbers appear somewhere in the reviews, and are thereforeextracted as words. Most of these numbers don’t have any immediate semantic mean‐ing. Looking further along in the vocabulary, we find a collection of English words starting with “dra”. You might notice that for \"draught\",\"drawback\", and \"drawer\" both the singular and plural forms are contained in the vocabulary as distinct words.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-2",
                            "title": "Python Libraries",
                            "content": "packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of",
                            "summary": "Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. pandas and matplotlib will also be used for exploratory data analysis and are widely used by data scientists. We only need the Python support for these tools. The scikit.learn. packages: $ pip install numpy. scipy matplot lib ipython. scik it-learn pandas pandas. mat plotlib. ipython-scikitlearn.ipython. python.py. python2. python3. python4. python This book was written as a Jupyter Notebook. All of the code examples can be downloaded from GitHub. All data in scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐phthalverted to a NumPy array. The core functionality of Num Py is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. The book includes code, text, and images, and all of the examples are available on GitHub. We will be using NumPy a lot in this book, and we will refer to objects of the NumPyting and interactive development. You should also install matplotlib, IPython, and the Jupyter Notebook. We recommend using one of the following prepackagedPython distributions, which will provide the necessary packages. Anaconda comes with NumPy, SciPy, matplot lib, IP python, JupYter, scikit-learn, and more. Ana­conda now also includes the commercial Intel MKL library for free.    The book is available on Mac OS, Windows, and Linux, and is the one we suggest for people without an existing installation of the scientific Python Using MKL can give significant speed improvements for many algorithms in scikit-learn. This comes with NumPy, matplotlib, pandas, and IPython. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. If you already have a Python installation set up, you can use pip to install all of these packages. It is built on top of. scikit-learn, but there are afew other libraries that will enhance your experience. It's free for Windows and Mac OS and comes with a 30-day trial.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-3",
                            "title": "Python Community",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-5",
                    "title": "scikit-learn",
                    "content": "would never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nIn[32]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[32]:\nTest set score: 0.97\nThis snippet contains the core code for applying any machine learning algorithm\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\nface to supervised models in scikit-learn, and with the concepts introduced in this\nchapter, you can apply these models to many machine learning tasks. In the next\nchapter, we will go into more depth about the different kinds of supervised models in\nscikit-learn and how to apply them successfully.\n24 \n| \nChapter 1: Introduction\nCHAPTER 2\nSupervised Learning\nAs we mentioned earlier, supervised machine learning is one of the most commonly\nused and successful types of machine learning. In this chapter, we will describe super‐\nvised learning in more detail and explain several popular supervised learning algo‐\nrithms. We already saw an application of supervised machine learning in Chapter 1:\nclassifying iris flowers into several species using physical measurements of the\nflowers.\nRemember that supervised learning is used whenever we want to predict a certain\noutcome from a given input, and we have examples of input/output pairs. We build a\nmachine learning model from these input/output pairs, which comprise our training\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\nvised learning often requires human effort to build the training set, but afterward\nautomates and often speeds up an otherwise laborious or infeasible task.\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\ninformation about Safari Books Online, please visit us online.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://bit.ly/intro-machine-learning-python.\nTo comment or ask technical questions about this book, send email to bookques‐\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our web‐\nsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nFrom Andreas\nWithout the help and support of a large group of people, this book would never have\nexisted.\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\nand John Myles White, who took the time to read the early versions of this book and\nprovided me with invaluable feedback—in addition to being some of the corner‐\nstones of the scientific open source ecosystem.\nPreface \n| \nxi\nI am forever thankful for the welcoming open source scientific Python community,\nespecially the contributors to scikit-learn. Without the support and help from this\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.",
                    "summary": "I want to thank my parents, Harald and Margot, and my sister,Miriam, for their continuing support and encouragement. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning. My thanks also go out to all the other contrib‐utors who donate their time to improve and maintain this package. I would never have become a core contributor to scikit-learn or learned to under‐stand this package as well as I do now. I want to say thank you to Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Sven Kreis, Alice Zheng, Kyungh Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. The project would not have even existed without Meg Blanchette's help and guidance. Thanks to Celia La and Brian Carlson for reading in the early days of the project and the O’Reilly folks for their patience. And finally, thanks to DTS, for your everlasting and endless support. For more information on machine learning, visit the Machine Learning Project website or visit the DTS website. As a general-purpose programming language, Python also allows for the creation of complex graphical user interfaces (GUIs) and web services. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. The scikit-learn project is constantly being developed and improved, and it has a very active user community. From auto-matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your                ity to interact directly with the code, using a terminal or other tools like the JupyterNotebook, we’ll look at shortly. Scikit-learn is the most prominent Python library for machine learning. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive documentation about each algorithm. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐phthalms and code snippets are available online. It works well with a number. of other scientific Python tools, which we will discuss later in this chapter. For plot.lyting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open sourcelicenses or the intellectual property rights of others, it is your responsibility to ensure that your use complies with such licenses and/or rights. We recommend using one of the following prepackagedPython distributions, which will provide the necessary packages: Anaconda or Python 2.5 or later. We are not responsible for damages resulting from the use of this work or reliance on this work. We apologise for any inconvenience caused by any errors or omissions in this material. We also apologise for the inconvenience caused to anyone who has used this information or relied on it. Machine Learning will be used by scientists and researchers, as well as data scientists. It will be a tool for developers, developers, and data scientists to help them understand and solve problems. It can be used to develop new applications, tools, and libraries. It We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. If you are interested in the mathematics of machine learning, we recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and JeromeFriedman. We will also not describe how to write machinelearning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries. The book is available for free at the authors’ website. Machine learning is being used as a commodity tool in research and commercial applications. There are few resources out there that fully cover all the important aspects of implementing machine learning. This book will help people who want to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes how to use Python to implement machine learning in the real world. Chapter 4 explains how to test machine learning on a real-world computer. Chapter 5 explains the various types of data that machine learning can be used to analyze. Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido. A Guide for Data Scientists. O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. Online editions are also available for most titles (http://safaribooksonline.com). The book is published in the United States of America and can be purchased for educational or business use. The O’Reilly logo is a registered trademark of O'Reilly Media, Inc. See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details. For more information, contact our corporate/institutional sales department: 800-998- Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information andinstructions contained in this work are accurate, they disclaim all responsibility for errors or omissions. The author is not responsible for damages resulting from the This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning. Chapters 2 and 3 describe the actual machine learning algorithms that are most widely used in practice. Chapter 6 explains the concept of pipelines for chaining models and encapsuling your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advancedtopics. Use of the information and instructions contained in this work is at your own peril. The book is written in the style of calculus, linear algebra, and probability theory. For more information, visit the book's website. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 toevaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” that includes some of the material in this book. For more in-depth documentation of the classes and functions, and many examples, go to the scik it-learn website. The book is available in English, German, French and Spanish. This snippet contains the core code for applying any machine learning algorithm. The fit, predict, and score methods are the common inter‐face to supervised models in scikit-learn. With the concepts introduced in this chapters, you can apply these models to many machine learning tasks. In the next chapters, we will go into more depth about the different kinds of supervised models and how to apply them successfully. We will end the chapter with the code for using the KNeighbors classifier to predict a person’s location in a neighborhood. The code for this section can be downloaded from the GitHub repository. Supervised machine learning is used to predict a certain outcome from a given input. In this chapter, we will describe super‐                vised learning in more detail and explain several popular supervised learning algo-rithms. Our goal is to make accurate predictions for new, never-before-seen data. We build a machine learning model from these input/output pairs, which comprise our training set. We end the chapter with an example of how we can use Supervised learning often requires human effort to build the training set, but afterwardautomates and often speeds up an otherwise laborious or infeasible task. There are two major types of supervised machine learning problems, called classifica­tion and regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of labels. For more information about Safari Books Online, please visit us online. For information about O’Reilly Media, Inc., please visit the publisher’s website, or call the company at 800-998-9938. This page includes the intro to machine-learning-python. Use this page to help people with reading comprehension and vocabulary. At the bottom of the page, please share your feedback about the book. I would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐ticular Dawn Schanafelt, for helping Sarah and me make this book a reality. I am forever thankful for the welcoming open source scientific Python community,especially the contributors to scikit-learn. I want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der der Leer, John Myles White, and Thomas Cas well. I also want to thanks the contributors who took the time to read \"Without the support and help from this community, I would never have become a core contributor to scikit-learn\" \"My thanks also go out to all the other contrib‐utors who donate their time to improve and maintain this package\"",
                    "children": [
                        {
                            "id": "chapter-1-section-5-subsection-1",
                            "title": "Introduction to scikit-learn",
                            "content": "would never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nIn[32]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[32]:\nTest set score: 0.97\nThis snippet contains the core code for applying any machine learning algorithm\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\nface to supervised models in scikit-learn, and with the concepts introduced in this\nchapter, you can apply these models to many machine learning tasks. In the next\nchapter, we will go into more depth about the different kinds of supervised models in\nscikit-learn and how to apply them successfully.\n24 \n| \nChapter 1: Introduction\nCHAPTER 2\nSupervised Learning\nAs we mentioned earlier, supervised machine learning is one of the most commonly\nused and successful types of machine learning. In this chapter, we will describe super‐\nvised learning in more detail and explain several popular supervised learning algo‐\nrithms. We already saw an application of supervised machine learning in Chapter 1:\nclassifying iris flowers into several species using physical measurements of the\nflowers.\nRemember that supervised learning is used whenever we want to predict a certain\noutcome from a given input, and we have examples of input/output pairs. We build a\nmachine learning model from these input/output pairs, which comprise our training\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\nvised learning often requires human effort to build the training set, but afterward\nautomates and often speeds up an otherwise laborious or infeasible task.\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\ninformation about Safari Books Online, please visit us online.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://bit.ly/intro-machine-learning-python.\nTo comment or ask technical questions about this book, send email to bookques‐\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our web‐\nsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nFrom Andreas\nWithout the help and support of a large group of people, this book would never have\nexisted.\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\nand John Myles White, who took the time to read the early versions of this book and\nprovided me with invaluable feedback—in addition to being some of the corner‐\nstones of the scientific open source ecosystem.\nPreface \n| \nxi\nI am forever thankful for the welcoming open source scientific Python community,\nespecially the contributors to scikit-learn. Without the support and help from this\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.",
                            "summary": "I want to thank my parents, Harald and Margot, and my sister,Miriam, for their continuing support and encouragement. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning. My thanks also go out to all the other contrib‐utors who donate their time to improve and maintain this package. I would never have become a core contributor to scikit-learn or learned to under‐stand this package as well as I do now. I want to say thank you to Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Sven Kreis, Alice Zheng, Kyungh Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. The project would not have even existed without Meg Blanchette's help and guidance. Thanks to Celia La and Brian Carlson for reading in the early days of the project and the O’Reilly folks for their patience. And finally, thanks to DTS, for your everlasting and endless support. For more information on machine learning, visit the Machine Learning Project website or visit the DTS website. As a general-purpose programming language, Python also allows for the creation of complex graphical user interfaces (GUIs) and web services. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. The scikit-learn project is constantly being developed and improved, and it has a very active user community. From auto-matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your                ity to interact directly with the code, using a terminal or other tools like the JupyterNotebook, we’ll look at shortly. Scikit-learn is the most prominent Python library for machine learning. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive documentation about each algorithm. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐phthalms and code snippets are available online. It works well with a number. of other scientific Python tools, which we will discuss later in this chapter. For plot.lyting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook. Use of the information and instructions contained in this work is at your own risk. If any code samples or other technology this work contains or describes is subject to open sourcelicenses or the intellectual property rights of others, it is your responsibility to ensure that your use complies with such licenses and/or rights. We recommend using one of the following prepackagedPython distributions, which will provide the necessary packages: Anaconda or Python 2.5 or later. We are not responsible for damages resulting from the use of this work or reliance on this work. We apologise for any inconvenience caused by any errors or omissions in this material. We also apologise for the inconvenience caused to anyone who has used this information or relied on it. Machine Learning will be used by scientists and researchers, as well as data scientists. It will be a tool for developers, developers, and data scientists to help them understand and solve problems. It can be used to develop new applications, tools, and libraries. It We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. If you are interested in the mathematics of machine learning, we recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and JeromeFriedman. We will also not describe how to write machinelearning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries. The book is available for free at the authors’ website. Machine learning is being used as a commodity tool in research and commercial applications. There are few resources out there that fully cover all the important aspects of implementing machine learning. This book will help people who want to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes how to use Python to implement machine learning in the real world. Chapter 4 explains how to test machine learning on a real-world computer. Chapter 5 explains the various types of data that machine learning can be used to analyze. Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido. A Guide for Data Scientists. O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. Online editions are also available for most titles (http://safaribooksonline.com). The book is published in the United States of America and can be purchased for educational or business use. The O’Reilly logo is a registered trademark of O'Reilly Media, Inc. See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details. For more information, contact our corporate/institutional sales department: 800-998- Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information andinstructions contained in this work are accurate, they disclaim all responsibility for errors or omissions. The author is not responsible for damages resulting from the This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning. Chapters 2 and 3 describe the actual machine learning algorithms that are most widely used in practice. Chapter 6 explains the concept of pipelines for chaining models and encapsuling your workflow. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advancedtopics. Use of the information and instructions contained in this work is at your own peril. The book is written in the style of calculus, linear algebra, and probability theory. For more information, visit the book's website. If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 toevaluate and tune your model. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikit-learn,” that includes some of the material in this book. For more in-depth documentation of the classes and functions, and many examples, go to the scik it-learn website. The book is available in English, German, French and Spanish. This snippet contains the core code for applying any machine learning algorithm. The fit, predict, and score methods are the common inter‐face to supervised models in scikit-learn. With the concepts introduced in this chapters, you can apply these models to many machine learning tasks. In the next chapters, we will go into more depth about the different kinds of supervised models and how to apply them successfully. We will end the chapter with the code for using the KNeighbors classifier to predict a person’s location in a neighborhood. The code for this section can be downloaded from the GitHub repository. Supervised machine learning is used to predict a certain outcome from a given input. In this chapter, we will describe super‐                vised learning in more detail and explain several popular supervised learning algo-rithms. Our goal is to make accurate predictions for new, never-before-seen data. We build a machine learning model from these input/output pairs, which comprise our training set. We end the chapter with an example of how we can use Supervised learning often requires human effort to build the training set, but afterwardautomates and often speeds up an otherwise laborious or infeasible task. There are two major types of supervised machine learning problems, called classifica­tion and regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of labels. For more information about Safari Books Online, please visit us online. For information about O’Reilly Media, Inc., please visit the publisher’s website, or call the company at 800-998-9938. This page includes the intro to machine-learning-python. Use this page to help people with reading comprehension and vocabulary. At the bottom of the page, please share your feedback about the book. I would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐ticular Dawn Schanafelt, for helping Sarah and me make this book a reality. I am forever thankful for the welcoming open source scientific Python community,especially the contributors to scikit-learn. I want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der der Leer, John Myles White, and Thomas Cas well. I also want to thanks the contributors who took the time to read \"Without the support and help from this community, I would never have become a core contributor to scikit-learn\" \"My thanks also go out to all the other contrib‐utors who donate their time to improve and maintain this package\"",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-5-subsection-2",
                            "title": "Key Features",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-5-subsection-3",
                            "title": "Basic Usage",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-6",
                    "title": "Installing scikit-learn",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-1-section-6-subsection-1",
                            "title": "Installation Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-2",
                            "title": "Dependencies",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-3",
                            "title": "Verification",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-7",
                    "title": "Essential Libraries and Tools",
                    "content": "packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nndarray class as “NumPy arrays” or just “arrays.”\nSciPy\nSciPy is a collection of functions for scientific computing in Python. It provides,\namong other functionality, advanced linear algebra routines, mathematical function\noptimization, signal processing, special mathematical functions, and statistical distri‐\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\nsparse matrices, which are another representation that is used for data in scikit-\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\nmostly zeros:\nIn[3]:\nfrom scipy import sparse\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\neye = np.eye(4)\nprint(\"NumPy array:\\n{}\".format(eye))\nOut[3]:\nNumPy array:\n[[ 1.  0.  0.  0.]\n [ 0.  1.  0.  0.]\n [ 0.  0.  1.  0.]\n [ 0.  0.  0.  1.]]\nIn[4]:\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\n# Only the nonzero entries are stored\nsparse_matrix = sparse.csr_matrix(eye)\nprint(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\nOut[4]:\nSciPy sparse CSR matrix:\n  (0, 0)    1.0\n  (1, 1)    1.0\n  (2, 2)    1.0\n  (3, 3)    1.0\n8 \n| \nChapter 1: Introduction\nUsually it is not possible to create dense representations of sparse data (as they would\nnot fit into memory), so we need to create sparse representations directly. Here is a\nway to create the same sparse matrix as before, using the COO format:\nIn[5]:\ndata = np.ones(4)\nrow_indices = np.arange(4)\ncol_indices = np.arange(4)\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"COO representation:\\n{}\".format(eye_coo))\nOut[5]:\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with accompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create",
                    "summary": "Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. pandas and matplotlib will also be used for exploratory data analysis and are widely used by data scientists. We only need the Python support for these tools. The scikit.learn. packages: $ pip install numpy. scipy matplot lib ipython. scik it-learn pandas pandas. mat plotlib. ipython-scikitlearn.ipython. python.py. python2. python3. python4. python This book was written as a Jupyter Notebook. All of the code examples can be downloaded from GitHub. All data in scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐phthalverted to a NumPy array. The core functionality of Num Py is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. The book includes code, text, and images, and all of the examples are available on GitHub. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy arrays class as “NumPy arrays” or just “arrays” A NumPy array looks like this: grotesquex = numpy.array([[1, 2, 3], [4, 5, 6],]).print(\"x:\\n{}\".format(x),.define('array', 'numpy', ' NumPyArray', true, false, false),. define('array,' 'array'', 'num', 'array', false, 'num'). print('array' });. We will also use SciPy, which is a collection of functions for scientific computing SciPy provides a sparse representation that is used for data in scikit-learn. Sparse matrices are used whenever we want to store a 2D array that contains mostly zeros. Usually it is not possible to create dense representations of sparse data, so we need to create sparse representations directly. The most important part of SciPy for us is scipy.sparse: this provides                sparse matrices, which are another representation that's used forData in Scikit. The code for this article is written in Python and can be downloaded from the project's README page. For more information on how to use SciPy, see http://www.scipyonline.org/blog/2013 You can write code that runs under Python 2 and Python 3.2 If you don’t have to interface with.legacy software, you should definitely use Python3.2. You should try to migrate to Python 3 assoon as possible. All the code in this book is written in a way that works for both versions. The code is written in the COO format, which is used for the sparse matrix. You can use this format to create the same sparse matrix as before. This book assumes that you have version 0.18 or later of scikit- autoimmunelearn. However, the exact output might differ slightly under Python 2.1. We are using the following versions of the previously mentioned libraries in this book: python, scipy, matplotlib, NumPy, and sklearn. Let’s dive into our first application of machine learning with the help of these libraries. We’ll start with the introduction to machine learning in Python 3.5.2 | Anaconda 4.4.7 20120313 (Red Hat 4.3.7-1) and work our way up to Python 5.1 or later. We recommend using one of the following prepackaged Python distributions. Anaconda comes with NumPy, SciPy, matplotlib, IPython, Jupyter Notebook, and scikit-learn. The model_selection module was added in 0.18, and if you                use an earlier version of scik it-learn, you will need to adjust the imports from this module. The first application is a simple machine learning application that classifies Iris species. We will go through a simpleMachine learning application and create an interactive version of it. The next section will show how to use this application to classify Iris species in the wild. Using MKL can give significant speed improvements for many algorithms in scikit-learn. This comes with NumPy, matplotlib, pandas, and IPython. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. If you already have a Python installation set up, you can use pip to install all of these packages. It is built on top of scikit-learn, which includes not only all the examples shown in this book, but also all of the mglearn library. It also includes a few other libraries that will enhance your experience. For more information on how to use this guide, visit the Enthought canopy website. The mglearn module is not really important to the material in this book. This is a library of utility functions we wrote for this book, so that we don’t clutter up our code listings with details of plotting and data loading. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data.Throughout the book we make ample use of NumPy, matplotlib and pandas. All the code will assume the following imports:import numpy as numpy, matPlotlib.pyplot as plt, pandas as pd, mglearn as mglearn. There are two major versions of Python that are widely used at the moment. Python 2 is no longer activelydeveloped, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. If your codebase is written for Python 2, you are likely to need to migrate to Python 3 as soon as possible. The six package can be very handy for that. The notebook and these magical commands can also be used to help you with reading comprehension and vocabulary. You will have to call plt.show to actually show                any of the figures. The scikit-learn project is constantly being developed and improved, and it has a very active user community. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. Python also allows for the creation of complex graphical user interfaces (GUIs) and web services, and for integration into existing systems. When writing any new code, it is for the most part quite easy to write code that runs under Python 2 and Python 3.2 If you don’t have to interface with the code, using a terminal or other tools like the JupyterNotebook, which we’ll look at shortly. Scikit-learn is the most prominent Python library for machine learning. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive documentation about each algorithm. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐phthalms and code snippets are available online. It works well with a number. of other scientific Python tools, which we will discuss later in this chapter. For plot.lyting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook. Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. We recommend using one of the following prepackaged Python distributions: Anaconda, PyPy, or Python 2.5 or PyPy 3.0. We are also using pandas and matplotlib for data analysis. We only need the Python support for these tools for the time being. We hope this guide will help you get the most out of the scikitlearn tool. It is free and open-source, so please share it with your friends and colleagues if you like it. This book was written as a Jupyter Notebook. All of the code examples can be downloaded from GitHub. All data in scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐phthalverted to a NumPy array. The core functionality of Num Py is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. The book includes code, text, and images, and all of the examples are available on GitHub. Python is a general-purpose programming language. It allows for the creation of complex graphical user interfaces (GUIs) and web services. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy array as numpy. The JupyterNotebook will be used to help you with reading comprehension and vocabulary. We’ll look at how to use NumPy in the next few chapters to learn more about the language and its tools. We'll also be looking at some of the tools and libraries used in the book, such as the Jup The scikit-learn project is constantly being developed and improved. It contains a number of state-of-the-art machine learning algorithms. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐insuredals and code snippets are available online. It works well with other scientific Python tools, which we will discuss later in this chapter. It depends on two other Python packages, NumPy and SciPy. The user guide and API documentation for each algorithm can be found at the bottom of the page. The book is available in English and French. Anaconda is a Python distribution made for large-scale data processing, predictive analytics, and scientific computing. Anaconda comes with NumPy, SciPy, matplotlib, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,Windows, and Linux, it is a very convenient solution and is the one we suggest for people without an existing installation of the scientific Python packages. Ana‐Conda now also includes the commercial Intel MKL library for free. For plot‐insuredting and interactive development, you should also install matplot lib, IP python, and JupY Using MKL can give significant speed improvements for many algorithms in scikit-learn. This comes with NumPy, matplotlib, pandas, and IPython. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. If you already have a Python installation set up, you can use pip to install all of these packages. It is built on top of scikit-learn, which includes not only all the examples shown in this book, but also all of the mglearn library. It also includes a few other libraries that will enhance your experience. For more information on how to use this guide, visit the Enthought canopy website. The mglearn module is not really important to the material in this book. This is a library of utility functions we wrote for this book, so that we don’t clutter up our code listings with details of plotting and data loading. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data.Throughout the book we make ample use of NumPy, matplotlib and pandas. All the code will assume the following imports:import numpy as numpy, matPlotlib.pyplot as plt, pandas as pd, mglearn as mglearn. There are two major versions of Python that are widely used at the moment. Python 2 is no longer activelydeveloped, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. If your codebase is written for Python 2, you are likely to need to migrate to Python 3 as soon as possible. The six package can be very handy for that. The notebook and these magical commands can also be used to help you with reading comprehension and vocabulary. You will have to call plt.show to actually show                any of the figures. The mglearn library is a library of utility functions we wrote for this book. It is for the most part quite easy to write code that runs under Python 2 and Python 3. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data. Throughout the book we make ample use of NumPy, matplotliband pandas. We hope that this book has been helpful to people who are interested in learning more about plotting and data analysis in Python 2.2 and 3.1. We also hope that it has been of some use to people using Python 3 and 4. We assume that you will run the code in a Jupyter Notebook. Python 2 is no longer actively developed, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. There are two major versions of Python that are widely used at the moment: Python 2(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of writing) The six package can be very handy for that. The code will assume the following imports:import numpy as numpy, pandas as pd, matplotlib.pyplot as plt, mglearn as mglearn. If you are not using the notebook or these directives, you will Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. When writing any new code, it is for the most part quite easy towrite code that runs under Python 2 and Python 3. We are also using pandas and matplotlib to help you with reading comprehension and vocabulary. We hope you will find this guide useful and useful. We would like to hear from you if you have any questions about the project. The Jupyter Notebook is an interactive environment for running code in the browser. It is a great tool for exploratory data analysis and is widely used by data scientists. All of the code examples we include can be downloaded from GitHub. The NumPy array is the fundamental data structure for scikit-learn. It takes in data in the form of NumPy arrays. It contains functionality for multidimensional arrays, high-level mathematical operations and the Fourier transform, and pseudorandom generators. The Python support makes it easy to incorporate code, text, and images into the JupYter Notebooks. The entire book was in fact written as a J upyter notebook. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy array class. We recommend using one of the following prepackaged Python distributions: Anaconda, IPython, matplotlib, scikit-learn, Jupyter Notebook, and SciPy. For interactive development, you should also install matplot lib, IPPython, and the J upyter notebook. The core functionality of NumPy is the ndarray class, a n-dimensional (n-dimensional) array. All elements of the array must be of the same type. Any data you’re using will have to be con‐phthalverted to a NumPy Anaconda now also includes the commercial Intel MKL library for free. Using MKL can give significant speed improvements for many algorithms in scikit-learn. Available on Mac OS,Windows, and Linux, it is a very convenient solution. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription of Enthought Canopy. This comes with NumPy, insanelySciPy, matplotlib, pandas, and IPython. Python(x,y) is a free Python distribution for scientific computing. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. If you already have a Python installation set up, you can use pip to install all of these packages. For now, you should try to migrate to Python 3 as soon as possible. For more information on how to use scikit-learn, please visit the project's website, or read the first chapter of the SciPy Lec­ture Notes. When writing any new code, it is for the most part quite easy to write code that runs under Python 2 and Python 3.2. All the code in this book is written in a way that works for both versions. If you don’t have This book assumes that you have version 0.18 or later of scikit- autoimmunelearn. However, the exact output might differ slightly under Python 2.1. We are using the following versions of the previously mentioned libraries in this book: python, scipy, matplotlib, NumPy, and sklearn. Let’s dive into our first application of machine learning with the help of these libraries. We’ll start with the introduction to machine learning in Python 3.5.2 | Anaconda 4.4.7 20120313 (Red Hat 4.3.7-1) and work our way up to Python 5.1 or later. In this section, we will go through a simple machine learning application and create. The model_selection module was added in 0.18, and if you use an earlier version of scikit-learn, you",
                    "children": [
                        {
                            "id": "chapter-1-section-7-subsection-1",
                            "title": "Jupyter Notebook",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-2",
                            "title": "NumPy",
                            "content": "packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nndarray class as “NumPy arrays” or just “arrays.”\nSciPy\nSciPy is a collection of functions for scientific computing in Python. It provides,\namong other functionality, advanced linear algebra routines, mathematical function\noptimization, signal processing, special mathematical functions, and statistical distri‐\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\nsparse matrices, which are another representation that is used for data in scikit-\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\nmostly zeros:\nIn[3]:\nfrom scipy import sparse\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\neye = np.eye(4)\nprint(\"NumPy array:\\n{}\".format(eye))\nOut[3]:\nNumPy array:\n[[ 1.  0.  0.  0.]\n [ 0.  1.  0.  0.]\n [ 0.  0.  1.  0.]\n [ 0.  0.  0.  1.]]\nIn[4]:\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\n# Only the nonzero entries are stored\nsparse_matrix = sparse.csr_matrix(eye)\nprint(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\nOut[4]:\nSciPy sparse CSR matrix:\n  (0, 0)    1.0\n  (1, 1)    1.0\n  (2, 2)    1.0\n  (3, 3)    1.0\n8 \n| \nChapter 1: Introduction\nUsually it is not possible to create dense representations of sparse data (as they would\nnot fit into memory), so we need to create sparse representations directly. Here is a\nway to create the same sparse matrix as before, using the COO format:\nIn[5]:\ndata = np.ones(4)\nrow_indices = np.arange(4)\ncol_indices = np.arange(4)\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"COO representation:\\n{}\".format(eye_coo))\nOut[5]:\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda",
                            "summary": "Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. pandas and matplotlib will also be used for exploratory data analysis and are widely used by data scientists. We only need the Python support for these tools. The scikit.learn. packages: $ pip install numpy. scipy matplot lib ipython. scik it-learn pandas pandas. mat plotlib. ipython-scikitlearn.ipython. python.py. python2. python3. python4. python This book was written as a Jupyter Notebook. All of the code examples can be downloaded from GitHub. All data in scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐phthalverted to a NumPy array. The core functionality of Num Py is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. The book includes code, text, and images, and all of the examples are available on GitHub. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy arrays class as “NumPy arrays” or just “arrays” A NumPy array looks like this: grotesquex = numpy.array([[1, 2, 3], [4, 5, 6],]).print(\"x:\\n{}\".format(x),.define('array', 'numpy', ' NumPyArray', true, false, false),. define('array,' 'array'', 'num', 'array', false, 'num'). print('array' });. We will also use SciPy, which is a collection of functions for scientific computing SciPy provides a sparse representation that is used for data in scikit-learn. Sparse matrices are used whenever we want to store a 2D array that contains mostly zeros. Usually it is not possible to create dense representations of sparse data, so we need to create sparse representations directly. The most important part of SciPy for us is scipy.sparse: this provides                sparse matrices, which are another representation that's used forData in Scikit. The code for this article is written in Python and can be downloaded from the project's README page. For more information on how to use SciPy, see http://www.scipyonline.org/blog/2013 You can write code that runs under Python 2 and Python 3.2 If you don’t have to interface with.legacy software, you should definitely use Python3.2. You should try to migrate to Python 3 assoon as possible. All the code in this book is written in a way that works for both versions. The code is written in the COO format, which is used for the sparse matrix. You can use this format to create the same sparse matrix as before. This book assumes that you have version 0.18 or later of scikit- autoimmunelearn. However, the exact output might differ slightly under Python 2.1. We are using the following versions of the previously mentioned libraries in this book: python, scipy, matplotlib, NumPy, and sklearn. Let’s dive into our first application of machine learning with the help of these libraries. We’ll start with the introduction to machine learning in Python 3.5.2 | Anaconda 4.4.7 20120313 (Red Hat 4.3.7-1) and work our way up to Python 5.1 or later. We recommend using one of the following prepackaged Python distributions. Anaconda comes with NumPy, SciPy, matplotlib, IPython, Jupyter Notebook, and scikit-learn. The model_selection module was added in 0.18, and if you                use an earlier version of scik it-learn, you will need to adjust the imports from this module. The first application is a simple machine learning application that classifies Iris species. We will go through a simpleMachine learning application and create an interactive version of it. The next section will show how to use this application to classify Iris species in the wild. Using MKL can give significant speed improvements for many algorithms in scikit-learn. This comes with NumPy, matplotlib, pandas, and IPython. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. If you already have a Python installation set up, you can use pip to install all of these packages. It is built on top of scikit-learn, which includes not only all the examples shown in this book, but also all of the mglearn library. It also includes a few other libraries that will enhance your experience. For more information on how to use this guide, visit the Enthought canopy website. The mglearn module is not really important to the material in this book. This is a library of utility functions we wrote for this book, so that we don’t clutter up our code listings with details of plotting and data loading. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data.Throughout the book we make ample use of NumPy, matplotlib and pandas. All the code will assume the following imports:import numpy as numpy, matPlotlib.pyplot as plt, pandas as pd, mglearn as mglearn. There are two major versions of Python that are widely used at the moment. Python 2 is no longer activelydeveloped, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. If your codebase is written for Python 2, you are likely to need to migrate to Python 3 as soon as possible. The six package can be very handy for that. The notebook and these magical commands can also be used to help you with reading comprehension and vocabulary. You will have to call plt.show to actually show                any of the figures. The scikit-learn project is constantly being developed and improved, and it has a very active user community. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. Python also allows for the creation of complex graphical user interfaces (GUIs) and web services, and for integration into existing systems. When writing any new code, it is for the most part quite easy to write code that runs under Python 2 and Python 3.2 If you don’t have to interface with the code, using a terminal or other tools like the JupyterNotebook, which we’ll look at shortly. Scikit-learn is the most prominent Python library for machine learning. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive documentation about each algorithm. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐phthalms and code snippets are available online. It works well with a number. of other scientific Python tools, which we will discuss later in this chapter. For plot.lyting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook. We recommend using one of the following prepackagedPython distributions, which",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-3",
                            "title": "SciPy",
                            "content": "packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with",
                            "summary": "Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. pandas and matplotlib will also be used for exploratory data analysis and are widely used by data scientists. We only need the Python support for these tools. The scikit.learn. packages: $ pip install numpy. scipy matplot lib ipython. scik it-learn pandas pandas. mat plotlib. ipython-scikitlearn.ipython. python.py. python2. python3. python4. python This book was written as a Jupyter Notebook. All of the code examples can be downloaded from GitHub. All data in scikit-learn takes in data in the form of NumPy arrays. Any data you’re using will have to be con‐phthalverted to a NumPy array. The core functionality of Num Py is the ndarray class, a multidimensional (n-dimensional) array. All elements of the array must be of the same type. The book includes code, text, and images, and all of the examples are available on GitHub. Python is a general-purpose programming language. It allows for the creation of complex graphical user interfaces (GUIs) and web services. It is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy array as numpy. The JupyterNotebook will be used to help you with reading comprehension and vocabulary. We’ll look at how to use NumPy in the next few chapters to learn more about the language and its tools. We'll also be looking at some of the tools and libraries used in the book, such as the Jup The scikit-learn project is constantly being developed and improved. It contains a number of state-of-the-art machine learning algorithms. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. It is widely used in industry and academia, and a wealth of tutori‐insuredals and code snippets are available online. It works well with other scientific Python tools, which we will discuss later in this chapter. It depends on two other Python packages, NumPy and SciPy. The user guide and API documentation for each algorithm can be found at the bottom of the page. The book is available in English and French. Anaconda is a Python distribution made for large-scale data processing, predictive analytics, and scientific computing. Anaconda comes with NumPy, SciPy, matplotlib, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,Windows, and Linux, it is a very convenient solution and is the one we suggest for people without an existing installation of the scientific Python packages. Ana‐Conda now also includes the commercial Intel MKL library for free. For plot‐insuredting and interactive development, you should also install matplot lib, IP python, and JupY Using MKL can give significant speed improvements for many algorithms in scikit-learn. This comes with NumPy, matplotlib, pandas, and IPython. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. If you already have a Python installation set up, you can use pip to install all of these packages. It is built on top of scikit-learn, which includes not only all the examples shown in this book, but also all of the mglearn library. It also includes a few other libraries that will enhance your experience. For more information on how to use this guide, visit the Enthought canopy website. The mglearn module is not really important to the material in this book. This is a library of utility functions we wrote for this book, so that we don’t clutter up our code listings with details of plotting and data loading. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data.Throughout the book we make ample use of NumPy, matplotlib and pandas. All the code will assume the following imports:import numpy as numpy, matPlotlib.pyplot as plt, pandas as pd, mglearn as mglearn. There are two major versions of Python that are widely used at the moment. Python 2 is no longer activelydeveloped, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. If your codebase is written for Python 2, you are likely to need to migrate to Python 3 as soon as possible. The six package can be very handy for that. The notebook and these magical commands can also be used to help you with reading comprehension and vocabulary. You will have to call plt.show to actually show                any of the figures. It is for the most part quite easy to write code that runs under Python 2 and Python 3.2 If you don’t have",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-4",
                            "title": "matplotlib",
                            "content": "accompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create",
                            "summary": "The mglearn library is a library of utility functions. The code will assume the following imports: matplotlib.pyplot as plt, pandas as pd, and numpy as numpy. If you see a call to mglearn in the code, it is usually a way to make a pretty picture quickly, or to get our hands on some interesting data. We also assume that you will run the code in a Jumper Notebook or %matplotlib inline magicenabled to show plots. We make ample use of NumPy, mat plotlib and pandas to make the code easier to read and understand. For more information, visit the mglearn website. The mglearn repository is available on GitHub. There are two major versions of Python that are widely used at the moment. Python 2 is no longer activelydeveloped, but because Python 3 contains major changes, Python 2 code usually does not run on Python 3. If you are new to Python, or are starting a new project from scratch, we highly recommend using the latest version of Python 3 without changes. If your codebase is written for Python 2, you are likely to need to migrate to Python 3 as soon as possible. The six package can be very handy for that. The notebook and these magical commands can also be used to help you with reading comprehension and vocabulary. You will have to call plt.show to actually show                any of the figures. Scikit-learn is built on top of the NumPy and SciPy scientific Python libraries. We will also introduce the Jupyter Notebook, which is a browser-based interactive programming environment. In addition to NumPy, we will be using pandas and matplotlib, as well as ipython and scipy. If you don’t have to interface withpackages: $ pip install numpy, matPlotlib, ipython, scikit, pandas. The Jupyter Notebook is an interactive environment for running code in the browser. It is a great tool for exploratory data analysis and is widely used by data scientists. All of the code examples we include can be downloaded from GitHub. The NumPy array is the fundamental data structure for scikit-learn. It takes in data in the form of NumPy arrays. It contains functionality for multidimensional arrays, high-level mathematical operations and the Fourier transform, and pseudorandom generators. The Python support makes it easy to incorporate code, text, and images into the JupYter Notebooks. The entire book was in fact written as a J upyter notebook. We will be using NumPy a lot in this book, and we will refer to objects of the NumPy array class. We recommend using one of the following prepackaged Python distributions: Anaconda, IPython, matplotlib, scikit-learn, Jupyter Notebook, and SciPy. For interactive development, you should also install matplot lib, IPPython, and the J upyter notebook. The core functionality of NumPy is the ndarray class, a n-dimensional (n-dimensional) array. All elements of the array must be of the same type. Any data you’re using will have to be con‐phthalverted to a NumPy Anaconda now also includes the commercial Intel MKL library for free. Using MKL can give significant speed improvements for many algorithms in scikit-learn. Available on Mac OS,Windows, and Linux, it is a very convenient solution. If you are part of an academic, degree-granting institution, you can request an academic license and get free access to the paid subscription of Enthought Canopy. This comes with NumPy, insanelySciPy, matplotlib, pandas, and IPython. Python(x,y) is a free Python distribution for scientific computing. It comes with NumPy, SciPy, matplotlib, pandas, IPython, andscikit-learn. Enthought Canopy is available for Python 2.7.x, and works on Mac OS, Windows, and Linux. If you already have a Python installation set up, you can use pip to install all of these packages. For now, you should try to migrate to Python 3 as soon as possible. For more information on how to use scikit-learn, please visit the project's website, or read the first chapter of the SciPy Lec­ture Notes. When writing any new code, it is for the most part quite easy to write code that runs under Python 2 and Python 3.2. All the code in this book is written in a way that works for both versions. If you don’t have This book assumes that you have version 0.18 or later of scikit- autoimmunelearn. However, the exact output might differ slightly under Python 2.1. We are using the following versions of the previously mentioned libraries in this book: python, scipy, matplotlib, NumPy, and sklearn. Let’s dive into our first application of machine learning with the help of these libraries. We’ll start with the introduction to machine learning in Python 3.5.2 | Anaconda 4.4.7 20120313 (Red Hat 4.3.7-1) and work our way up to Python 5.1 or later. In this section, we will go through a simple machine learning application and create. The model_selection module was added in 0.18, and if you use an earlier version of scikit-learn, you",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-5",
                            "title": "pandas",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-6",
                            "title": "mglearn",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-8",
                    "title": "Python 2 Versus Python 3",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-1-section-8-subsection-1",
                            "title": "Key Differences",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-2",
                            "title": "Migration Considerations",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-3",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-9",
                    "title": "Versions Used in this Book",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-1-section-9-subsection-1",
                            "title": "Package Versions",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-9-subsection-2",
                            "title": "Compatibility Notes",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-9-subsection-3",
                            "title": "Version Management",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-10",
                    "title": "A First Application: Classifying Iris Species",
                    "content": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nusing the sepal and petal measurements. This means that a machine learning model\nwill likely be able to learn to separate them.\nBuilding Your First Model: k-Nearest Neighbors\nNow we can start building the actual machine learning model. There are many classi‐\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\nneighbors classifier, which is easy to understand. Building this model only consists of\nstoring the training set. To make a prediction for a new data point, the algorithm\nfinds the point in the training set that is closest to the new point. Then it assigns the\nlabel of this training point to the new data point.\n20 \n| \nChapter 1: Introduction\nThe k in k-nearest neighbors signifies that instead of using only the closest neighbor\nto the new data point, we can consider any fixed number k of neighbors in the train‐\ning (for example, the closest three or five neighbors). Then, we can make a prediction\nusing the majority class among these neighbors. We will go into more detail about\nthis in Chapter 2; for now, we’ll use only a single neighbor.\nAll machine learning models in scikit-learn are implemented in their own classes,\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\nwe can use the model, we need to instantiate the class into an object. This is when we\nwill set any parameters of the model. The most important parameter of KNeighbor\nsClassifier is the number of neighbors, which we will set to 1:\nIn[25]:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nThe knn object encapsulates the algorithm that will be used to build the model from\nthe training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nEvaluating the Model                                                                                                   22\nSummary and Outlook                                                                                                   23\niii\n2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\nClassification and Regression                                                                                         25\nGeneralization, Overfitting, and Underfitting                                                             26\nRelation of Model Complexity to Dataset Size                                                         29\nSupervised Machine Learning Algorithms                                                                  29\nSome Sample Datasets                                                                                                 30\nk-Nearest Neighbors                                                                                                    35\nLinear Models                                                                                                               45\nNaive Bayes Classifiers                                                                                                 68\nDecision Trees                                                                                                               70\nEnsembles of Decision Trees                                                                                      83\nKernelized Support Vector Machines                                                                        92\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\n# build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nThe plot shows the training and test set accuracy on the y-axis against the setting of\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\nrecognize some of the characteristics of overfitting and underfitting (note that\nbecause considering fewer neighbors corresponds to a more complex model, the plot\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\nbors are considered, the model becomes simpler and the training accuracy drops. The\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\nbors, indicating that using the single nearest neighbor leads to a model that is too\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\nand performance is even worse. The best performance is somewhere in the middle,\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\nworst performance is around 88% accuracy, which might still be acceptable.\nSupervised Machine Learning Algorithms \n| \n39\nFigure 2-7. Comparison of training and test accuracy as a function of n_neighbors\nk-neighbors regression\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\nadded three test data points as green stars on the x-axis. The prediction using a single might be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\ncommon application is prediction of time series (like stock prices), which also has a\nwhole body of literature devoted to it. There are many more machine learning tasks\nout there—much more than we can list here—and we encourage you to seek out\ninformation from books, research papers, and online communities to find the para‐\ndigms that best apply to your situation.\nProbabilistic Modeling, Inference, and Probabilistic Programming\nMost machine learning packages provide predefined machine learning models that\napply one particular algorithm. However, many real-world problems have a particular\nstructure that, when properly incorporated into the model, can yield much better-\nperforming predictions. Often, the structure of a particular problem can be expressed\nusing the language of probability theory. Such structure commonly arises from hav‐\ning a mathematical model of the situation for which you want to predict. To under‐\nstand what we mean by a structured problem, consider the following example.\nLet’s say you want to build a mobile application that provides a very detailed position\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\nprovides many sensors to help you get precise location measurements, like the GPS,\naccelerometer, and compass. You also have an exact map of the area. This problem is\nhighly structured. You know where the paths and points of interest are from your\nmap. You also have rough positions from the GPS, and the accelerometer and com‐\npass in the user’s device provide you with very precise relative measurements. But\nthrowing these all together into a black-box machine learning system to predict posi‐ CHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and",
                    "summary": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machi",
                    "children": [
                        {
                            "id": "chapter-1-section-10-subsection-1",
                            "title": "Meet the Data",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-10-subsection-2",
                            "title": "Measuring Success: Training and Testing Data",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-10-subsection-3",
                            "title": "First Things First: Look at Your Data",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-10-subsection-4",
                            "title": "Building Your First Model: k-Nearest Neighbors",
                            "content": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nusing the sepal and petal measurements. This means that a machine learning model\nwill likely be able to learn to separate them.\nBuilding Your First Model: k-Nearest Neighbors\nNow we can start building the actual machine learning model. There are many classi‐\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\nneighbors classifier, which is easy to understand. Building this model only consists of\nstoring the training set. To make a prediction for a new data point, the algorithm\nfinds the point in the training set that is closest to the new point. Then it assigns the\nlabel of this training point to the new data point.\n20 \n| \nChapter 1: Introduction\nThe k in k-nearest neighbors signifies that instead of using only the closest neighbor\nto the new data point, we can consider any fixed number k of neighbors in the train‐\ning (for example, the closest three or five neighbors). Then, we can make a prediction\nusing the majority class among these neighbors. We will go into more detail about\nthis in Chapter 2; for now, we’ll use only a single neighbor.\nAll machine learning models in scikit-learn are implemented in their own classes,\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\nwe can use the model, we need to instantiate the class into an object. This is when we\nwill set any parameters of the model. The most important parameter of KNeighbor\nsClassifier is the number of neighbors, which we will set to 1:\nIn[25]:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nThe knn object encapsulates the algorithm that will be used to build the model from\nthe training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nEvaluating the Model                                                                                                   22\nSummary and Outlook                                                                                                   23\niii\n2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\nClassification and Regression                                                                                         25\nGeneralization, Overfitting, and Underfitting                                                             26\nRelation of Model Complexity to Dataset Size                                                         29\nSupervised Machine Learning Algorithms                                                                  29\nSome Sample Datasets                                                                                                 30\nk-Nearest Neighbors                                                                                                    35\nLinear Models                                                                                                               45\nNaive Bayes Classifiers                                                                                                 68\nDecision Trees                                                                                                               70\nEnsembles of Decision Trees                                                                                      83\nKernelized Support Vector Machines                                                                        92\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\n# build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nThe plot shows the training and test set accuracy on the y-axis against the setting of\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\nrecognize some of the characteristics of overfitting and underfitting (note that\nbecause considering fewer neighbors corresponds to a more complex model, the plot\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\nbors are considered, the model becomes simpler and the training accuracy drops. The\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\nbors, indicating that using the single nearest neighbor leads to a model that is too\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\nand performance is even worse. The best performance is somewhere in the middle,\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\nworst performance is around 88% accuracy, which might still be acceptable.\nSupervised Machine Learning Algorithms \n| \n39\nFigure 2-7. Comparison of training and test accuracy as a function of n_neighbors\nk-neighbors regression\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\nadded three test data points as green stars on the x-axis. The prediction using a single",
                            "summary": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machi",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-10-subsection-5",
                            "title": "Making Predictions",
                            "content": "might be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\ncommon application is prediction of time series (like stock prices), which also has a\nwhole body of literature devoted to it. There are many more machine learning tasks\nout there—much more than we can list here—and we encourage you to seek out\ninformation from books, research papers, and online communities to find the para‐\ndigms that best apply to your situation.\nProbabilistic Modeling, Inference, and Probabilistic Programming\nMost machine learning packages provide predefined machine learning models that\napply one particular algorithm. However, many real-world problems have a particular\nstructure that, when properly incorporated into the model, can yield much better-\nperforming predictions. Often, the structure of a particular problem can be expressed\nusing the language of probability theory. Such structure commonly arises from hav‐\ning a mathematical model of the situation for which you want to predict. To under‐\nstand what we mean by a structured problem, consider the following example.\nLet’s say you want to build a mobile application that provides a very detailed position\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\nprovides many sensors to help you get precise location measurements, like the GPS,\naccelerometer, and compass. You also have an exact map of the area. This problem is\nhighly structured. You know where the paths and points of interest are from your\nmap. You also have rough positions from the GPS, and the accelerometer and com‐\npass in the user’s device provide you with very precise relative measurements. But\nthrowing these all together into a black-box machine learning system to predict posi‐",
                            "summary": "There are many more machine learning tasks than we can list here. We encourage you to seek out information from books, research papers, and online communities to find the para‐digms that best apply to your situation. Many real-world problems have a particular structure that, when properly incorporated into the model, can yield much better-performing predictions. The most common application is prediction of time series (like stock prices), which also has a whole body of literature devoted to it. The “Netflix prize challenge” is a classic example of a machine learning task that can be applied to a variety of real-life situations. It offered a prize of $1 million to the team that could provide the The structure of a particular problem can be expressedusing the language of probability theory. To under‐stand what we mean by a structured problem, consider the following example. Let’s say you want to build a mobile application that provides a very detailed position                estimate in an outdoor space. You know where the paths and points of interest are from your map. You also have rough positions from the GPS. But throwing these all together into a black-box machine learning system to predict posi­tive positions is a highly structured problem. We call this type of problem a ‘structured problem’ and use the term ‘probability theory’ to describe it.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-10-subsection-6",
                            "title": "Evaluating the Model",
                            "content": "CHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and",
                            "summary": "We will focus on the supervised methods, regression and classification. We have split our dataset into a training set and a test set using the train_test_split function. We built a model on the training set by calling the fit method, and evaluated it on the test set by the score method. The score method for classification computes the fraction of correctly classified samples. We will then evaluate our supervised models using the fit and score methods. We hope this will help us understand how to improve our models in unsupervised learning. We'll end the chapter with a discussion of how we can improve our In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. Cross-validation is a more robust way to assess general",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-11",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-1-section-11-subsection-1",
                            "title": "Chapter Review",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-11-subsection-2",
                            "title": "Key Takeaways",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-11-subsection-3",
                            "title": "Next Steps",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-2",
            "title": "2. Supervised Learning",
            "content": "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nour first model. In the process, we will introduce some core concepts and terms.\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\niris flowers that she has found. She has collected some measurements associated with\neach iris: the length and width of the petals and the length and width of the sepals, all\nmeasured in centimeters (see Figure 1-2).\nShe also has the measurements of some irises that have been previously identified by\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\nthat these are the only species our hobby botanist will encounter in the wild.\nOur goal is to build a machine learning model that can learn from the measurements\nof these irises whose species is known, so that we can predict the species for a new\niris.\nA First Application: Classifying Iris Species \n| \n13\nFigure 1-2. Parts of the iris flower\nBecause we have measurements for which we know the correct species of iris, this is a\nsupervised learning problem. In this problem, we want to predict one of several\noptions (the species of iris). This is an example of a classification problem. The possi‐\nble outputs (different species of irises) are called classes. Every iris in the dataset\nbelongs to one of three classes, so this problem is a three-class classification problem.\nThe desired output for a single data point (an iris) is the species of this flower. For a\nparticular data point, the species it belongs to is called its label.\nMeet the Data\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\nthe algorithms.\nSome Sample Datasets\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\nwill be small and synthetic (meaning made-up), designed to highlight particular\naspects of the algorithms. Other datasets will be large, real-world examples.\nAn example of a synthetic two-class classification dataset is the forge dataset, which\nhas two features. The following code creates a scatter plot (Figure 2-2) visualizing all\nof the data points in this dataset. The plot has the first feature on the x-axis and the\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\nrepresented as one dot. The color and shape of the dot indicates its class:\nIn[2]:\n# generate dataset\nX, y = mglearn.datasets.make_forge()\n# plot dataset\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))\nOut[2]:\nX.shape: (26, 2)\n30 \n| \nChapter 2: Supervised Learning\nFigure 2-2. Scatter plot of the forge dataset\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\ndataset has a single input feature and a continuous target variable (or response) that\nwe want to model. The plot created here (Figure 2-3) shows the single feature on the\nx-axis and the regression target (the output) on the y-axis:\nIn[3]:\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")\nSupervised Machine Learning Algorithms \n| \n31\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nTo evaluate how well our model generalizes, we can call the score method with the\ntest data together with the test labels:\nIn[16]:\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\nOut[16]:\nTest set accuracy: 0.86\nWe see that our model is about 86% accurate, meaning the model predicted the class\ncorrectly for 86% of the samples in the test dataset.\nAnalyzing KNeighborsClassifier\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\npoints in the xy-plane. We color the plane according to the class that would be\nassigned to a point in this region. This lets us view the decision boundary, which is the\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\nSupervised Machine Learning Algorithms \n| \n37\nThe following code produces the visualizations of the decision boundaries for one,\nthree, and nine neighbors shown in Figure 2-6:\nIn[17]:\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # the fit method returns the object self, so we can instantiate\n    # and fit in one line\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\naxes[0].legend(loc=3)\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\nues of n_neighbors\nAs you can see on the left in the figure, using a single neighbor results in a decision\nboundary that follows the training data closely. Considering more and more neigh‐ number in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐ want to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\none in the future.\nUsually we build a model in such a way that it can make accurate predictions on the\ntraining set. If the training and test sets have enough in common, we expect the\nmodel to also be accurate on the test set. However, there are some cases where this\ncan go wrong. For example, if we allow ourselves to build very complex models, we\ncan always be as accurate as we like on the training set.\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\ntist wants to predict whether a customer will buy a boat, given records of previous\nboat buyers and customers who we know are not interested in buying a boat.2 The\ngoal is to send out promotional emails to people who are likely to actually make a\npurchase, but not bother those customers who won’t be interested.\nSuppose we have the customer records shown in Table 2-1.\nTable 2-1. Example data about customers\nAge\nNumber of \ncars owned\nOwns house\nNumber of children\nMarital status\nOwns a dog\nBought a boat\n66\n1\nyes\n2\nwidowed\nno\nyes\n52\n2\nyes\n3\nmarried\nno\nyes\n22\n0\nno\n0\nmarried\nyes\nno\n25\n1\nno\n1\nsingle\nno\nno\n44\n0\nno\n2\ndivorced\nyes\nno\n39\n1\nyes\n2\nmarried\nyes\nno\n26\n1\nno\n2\nsingle\nno\nno\n40\n3\nyes\n1\nmarried\nyes\nno\n53\n2\nyes\n2\ndivorced\nno\nyes\n64\n2\nyes\n3\ndivorced\nno\nno\n58\n2\nyes\n2\nmarried\nyes\nyes\n33\n1\nno\n1\nsingle\nno\nno\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is data, where all the features have similar meanings. For data that has very different\nkinds of features, tree-based models might work better. Tuning neural network\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\nface of possible ways to adjust neural network models and how to train them.\nEstimating complexity in neural networks.    The most important parameters are the num‐\nber of layers and the number of hidden units per layer. You should start with one or\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\nlayer is often similar to the number of input features, but rarely higher than in the low\nto mid-thousands.\nA helpful measure when thinking about the model complexity of a neural network is\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\n100 = 10,000 weights between the input and the first hidden layer. There are also\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\nhidden layers of size 100.\nA common way to adjust parameters in a neural network is to first create a network\nthat is large enough to overfit, making sure that the task can actually be learned by\nthe network. Then, once you know the training data can be learned, either shrink the scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very though. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\nprises, most user-facing services employ A/B testing, a form of blind user study. In\nFrom Prototype to Production \n| \n359\nA/B testing, without their knowledge a selected portion of users will be provided with\na website or service using algorithm A, while the rest of the users will be provided\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\nand a selection between the two approaches will be made according to these metrics.\nUsing A/B testing enables us to evaluate the algorithms “in the wild,” which might\nhelp us to discover unexpected consequences when users are interacting with our\nmodel. Often A is a new model, while B is the established system. There are more\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\nrithms for Website Optimization by John Myles White (O’Reilly). \nBuilding Your Own Estimator\nThis book has covered a variety of tools and algorithms implemented in scikit-\nlearn that can be used on a wide range of tasks. However, often there will be some\nparticular processing you need to do for your data that is not implemented in\nscikit-learn. It may be enough to just preprocess your data before passing it to your\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\nand you want to apply a grid search or cross-validation, things become trickier.\nIn Chapter 6 we discussed the importance of putting all data-dependent processing ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0] the training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nTo build the model on the training set, we call the fit method of the knn object,\nwhich takes as arguments the NumPy array X_train containing the training data and\nthe NumPy array y_train of the corresponding training labels:\nIn[26]:\nknn.fit(X_train, y_train)\nOut[26]:\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')\nThe fit method returns the knn object itself (and modifies it in place), so we get a\nstring representation of our classifier. The representation shows us which parameters\nwere used in creating the model. Nearly all of them are the default values, but you can\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\nscikit-learn have many parameters, but the majority of them are either speed opti‐\nmizations or for very special use cases. You don’t have to worry about the other\nparameters shown in this representation. Printing a scikit-learn model can yield\nvery long strings, but don’t be intimidated by these. We will cover all the important\nparameters in Chapter 2. In the remainder of this book, we will not show the output\nof fit because it doesn’t contain any new information.\nA First Application: Classifying Iris Species \n| \n21\nMaking Predictions\nWe can now make predictions using this model on new data for which we might not\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\nfeatures (4): and how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐ and >50k. It would also be possible to predict the exact income, and make this a\nregression task. However, that would be much more difficult, and the 50K division is\ninteresting to understand on its own.\nIn this dataset, age and hours-per-week are continuous features, which we know\nhow to treat. The workclass, education, sex, and occupation features are categori‐\ncal, however. All of them come from a fixed list of possible values, as opposed to a\nrange, and denote a qualitative property, as opposed to a quantity.\n212 \n| \nChapter 4: Representing Data and Engineering Features\nAs a starting point, let’s say we want to learn a logistic regression classifier on this\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\nthe following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\nway when applying logistic regression. The next section will explain how we can\novercome this problem.\nOne-Hot-Encoding (Dummy Variables)\nBy far the most common way to represent categorical variables is using the one-hot-\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\ndummy variables is to replace a categorical variable with one or more new features\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\nlinear binary classification (and for all other models in scikit-learn), and we can\nrepresent any number of categories by introducing one new feature per category, as\ndescribed here.\nLet’s say for the workclass feature we have possible values of \"Government\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\nrated\". To encode these four possible values, we create four new features, called \"Gov\ndifferent values of C\nSupervised Machine Learning Algorithms \n| \n61\nIf we desire a more interpretable model, using L1 regularization might help, as it lim‐\nits the model to using only a few features. Here is the coefficient plot and classifica‐\ntion accuracies for L1 regularization (Figure 2-18):\nIn[46]:\nfor C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_train, y_train)))\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_test, y_test)))\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)\nOut[46]:\nTraining accuracy of l1 logreg with C=0.001: 0.91\nTest accuracy of l1 logreg with C=0.001: 0.92\nTraining accuracy of l1 logreg with C=1.000: 0.96\nTest accuracy of l1 logreg with C=1.000: 0.96\nTraining accuracy of l1 logreg with C=100.000: 0.99\nTest accuracy of l1 logreg with C=100.000: 0.98\nAs you can see, there are many parallels between linear models for binary classifica‐\ntion and linear models for regression. As in regression, the main difference between\nthe models is the penalty parameter, which influences the regularization and\nwhether the model will use all available features or select only a subset.\n62 \n| \nChapter 2: Supervised Learning\nFigure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\n94 \n| \nChapter 2: Supervised Learning\nFigure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\nfeature derived from feature1\nIn the new representation of the data, it is now indeed possible to separate the two\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\nting a linear model to the augmented data (see Figure 2-39):\nIn[79]:\nlinear_svm_3d = LinearSVC().fit(X_new, y)\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n# show linear decision boundary\nfigure = plt.figure()\nax = Axes3D(figure, elev=-152, azim=-26)\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\nXX, YY = np.meshgrid(xx, yy)\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature0 ** 2\")\nSupervised Machine Learning Algorithms \n| \n95\nFigure 2-39. Decision boundary found by a linear SVM on the expanded three-\ndimensional dataset\nAs a function of the original features, the linear SVM model is not actually linear any‐\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\n(Figure 2-40):\nIn[80]:\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents). iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents). preprocessing, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\nkinds of, 133\nparameter selection with, 306\npipelines and, 317\npurpose of, 132\nscaling training and test data, 136\nprincipal component analysis (PCA)\ndrawbacks of, 146\nexample of, 140\nfeature extraction with, 147\nunsupervised nature of, 145\nvisualizations with, 142\nwhitening option, 150\nprobabilistic modeling, 363\n372 \n| \nIndex\nprobabilistic programming, 363\nproblem solving\nbuilding your own estimators, 360\nbusiness metrics and, 358\ninitial approach to, 357\nresources, 361-366\nsimple vs. complicated cases, 358\nsteps of, 358\ntesting your system, 359\ntool choice, 359\nproduction systems\ntesting, 359\ntool choice, 359\npruning for decision trees, 74\npseudorandom number generators, 18\npure leafs, 73\nPyMC language, 364\nPython\nbenefits of, 5\nprepackaged distributions, 6\nPython 2 vs. Python 3, 12\nPython(x,y), 6\nstatsmodel package, 362\nR\nR language, 362\nradial basis function (RBF) kernel, 97\nrandom forests\nanalyzing, 85\nbuilding, 84\ndata representation and, 220-224\nvs. decision trees, 83\nvs. gradient boosted regression trees, 88\nparameters, 88\npredictions with, 84\nrandomization in, 83\nstrengths and weaknesses, 87\nrandom_state parameter, 18\nranking, 363\nreal numbers, 26\nrecall, 282\nreceiver operating characteristics (ROC)\ncurves, 292-296\nrecommender systems, 363\nrectified linear unit (relu), 106\nrectifying nonlinearity, 106\nrecurrent neural networks (RNNs), 356\nrecursive feature elimination (RFE), 240\nregression\nf_regression, 236, 310\nLinearRegression, 47-56, 81, 247\nregression problems\nBoston Housing dataset, 34\nvs. classification problems, 26\nevaluation metrics and scoring, 299\nexamples of, 26\ngoals for, 26\nk-nearest neighbors, 40\nLasso, 53\nlinear models, 45\nridge regression, 49\nwave dataset illustration, 31\nregularization\nL1 regularization, 53\nL2 regularization, 49, 60\nrescaling\nexample of, 132-140\nkernel SVMs, 102\nresources, ix\nridge regression, 49\nrobustness-based clustering, 194\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\nrithm we described for the decision tree is slightly modified. Instead of looking for\nthe best test for each node, in each node the algorithm randomly selects a subset of\nthe features, and it looks for the best possible test involving one of these features. The\nnumber of features that are selected is controlled by the max_features parameter.\nThis selection of a subset of features is repeated separately in each node, so that each\nnode in a tree can make a decision using a different subset of the features.\nThe bootstrap sampling leads to each decision tree in the random forest being built\non a slightly different dataset. Because of the selection of features in each node, each\nsplit in each tree operates on a different subset of features. Together, these two mech‐\nanisms ensure that all the trees in the random forest are different.\nA critical parameter in this process is max_features. If we set max_features to n_fea\ntures, that means that each split can look at all features in the dataset, and no ran‐\ndomness will be injected in the feature selection (the randomness due to the\nbootstrapping remains, though). If we set max_features to 1, that means that the\nsplits have no choice at all on which feature to test, and can only search over different\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\ntures means that the trees in the random forest will be quite similar, and they will be\nable to fit the data easily, using the most distinctive features. A low max_features\nmeans that the trees in the random forest will be quite different, and that each tree\nmight need to be very deep in order to fit the data well.\nTo make a prediction using the random forest, the algorithm first makes a prediction\nfor every tree in the forest. For regression, we can average these results to get our final\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\ndata. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other ing completely ignored some of the features.\nAs both gradient boosting and random forests perform well on similar kinds of data,\na common approach is to first try random forests, which work quite robustly. If ran‐\ndom forests work well but prediction time is at a premium, or it is important to\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\ning to gradient boosting often helps.\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\nlooking into the xgboost package and its Python interface, which at the time of writ‐\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\ngradient boosting on many datasets.\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\nmost powerful and widely used models for supervised learning. Their main drawback\nis that they require careful tuning of the parameters and may take a long time to\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\nand on a mixture of binary and continuous features. As with other tree-based models,\nit also often does not work well on high-dimensional sparse data.\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\nmators, and the learning_rate, which controls the degree to which each tree is\nallowed to correct the mistakes of the previous trees. These two parameters are highly\nSupervised Machine Learning Algorithms \n| \n91\ninterconnected, as a lower learning_rate means that more trees are needed to build\na model of similar complexity. In contrast to random forests, where a higher n_esti\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\n# all will be split in a consistent manner\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n# build the gradient boosting model\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train_named)\nUncertainty Estimates from Classifiers \n| \n119\nThe Decision Function\nIn the binary classification case, the return value of decision_function is of shape\n(n_samples,), and it returns one floating-point number for each sample:\nIn[106]:\nprint(\"X_test.shape: {}\".format(X_test.shape))\nprint(\"Decision function shape: {}\".format(\n    gbrt.decision_function(X_test).shape))\nOut[106]:\nX_test.shape: (25, 2)\nDecision function shape: (25,)\nThis value encodes how strongly the model believes a data point to belong to the\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\ntive class, and negative values indicate a preference for the “negative” (other) class:\nIn[107]:\n# show the first few entries of decision_function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))\nOut[107]:\nDecision function:\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\nWe can recover the prediction by looking only at the sign of the decision function:\nIn[108]:\nprint(\"Thresholded decision function:\\n{}\".format(\n    gbrt.decision_function(X_test) > 0))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[108]:\nThresholded decision function:\n[ True False False False  True  True False  True  True  True False  True\n  True False  True False False False  True  True  True  True  True False\n  False]\nPredictions:\n['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\nFor binary classification, the “negative” class is always the first entry of the classes_\nattribute, and the “positive” class is the second entry of classes_. So if you want to\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\nboosted models, often not deeper than five splits.\nKernelized Support Vector Machines\nThe next type of supervised model we will discuss is kernelized support vector\nmachines. We explored the use of linear support vector machines for classification in\n“Linear models for classification” on page 56. Kernelized support vector machines\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\nport vector machines for classification and regression, we will restrict ourselves to the\nclassification case, as implemented in SVC. Similar concepts apply to support vector\nregression, as implemented in SVR.\nThe math behind kernelized support vector machines is a bit involved, and is beyond\nthe scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, and\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\nsense of the idea behind the method.\nLinear models and nonlinear features\nAs you saw in Figure 2-15, linear models can be quite limiting in low-dimensional\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\nmodel more flexible is by adding more features—for example, by adding interactions\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\npredictions on part of the data, and so more and more trees are added to iteratively\nimprove performance.\nGradient boosted trees are frequently the winning entries in machine learning com‐\npetitions, and are widely used in industry. They are generally a bit more sensitive to\nparameter settings than random forests, but can provide better accuracy if the param‐\neters are set correctly.\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\ntant parameter of gradient boosting is the learning_rate, which controls how\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\nrate means each tree can make stronger corrections, allowing for more complex mod‐\nels. Adding more trees to the ensemble, which can be accomplished by increasing\nn_estimators, also increases the model complexity, as the model has more chances\nto correct mistakes on the training set.\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\nIn[72]:\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[72]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.958\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\nlower the learning rate:\nSupervised Machine Learning Algorithms \n| \n89\nIn[73]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train))) data. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other dimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nX, y = mglearn.tools.make_handcrafted_dataset()\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n# plot support vectors\nsv = svm.support_vectors_\n# class labels of support vectors are given by the sign of the dual coefficients\nsv_labels = svm.dual_coef_.ravel() > 0\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n98 \n| \nChapter 2: Supervised Learning\nFigure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\nwhich we will now discuss in detail.\nTuning SVM parameters\nThe gamma parameter is the one shown in the formula given in the previous section,\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\nmeans for points to be close together. The C parameter is a regularization parameter,\nsimilar to that used in the linear models. It limits the importance of each point (or\nmore precisely, their dual_coef_).\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\nIn[82]:\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\nfor ax, C in zip(axes, [-1, 0, 3]):\n    for a, gamma in zip(ax, range(-1, 2)):\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n                  ncol=4, loc=(.9, 1.2))\nSupervised Machine Learning Algorithms \n| \n99\nFigure 2-42. Decision boundaries and support vectors for different settings of the param‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\nthe most commonly used ones.\nGrid Search\nNow that we know how to evaluate how well a model generalizes, we can take the\nnext step and improve the model’s generalization performance by tuning its parame‐\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\nin Chapters 2 and 3, and it is important to understand what the parameters mean\nbefore trying to adjust them. Finding the values of the important parameters of a\nmodel (the ones that provide the best generalization performance) is a tricky task, but\nnecessary for almost all models and datasets. Because it is such a common task, there\nare standard methods in scikit-learn to help you with it. The most commonly used\nmethod is grid search, which basically means trying all possible combinations of the\nparameters of interest.\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\nsame for gamma. Because we have six different settings for C and gamma that we want to\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\n260 \n| \nChapter 5: Model Evaluation and Improvement\nC = 0.001\nC = 0.01\n… C = 10\ngamma=0.001\nSVC(C=0.001, gamma=0.001)\nSVC(C=0.01, gamma=0.001)\n… SVC(C=10, gamma=0.001)\ngamma=0.01\nSVC(C=0.001, gamma=0.01)\nSVC(C=0.01, gamma=0.01)\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n# naive grid search implementation\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {}   size of test set: {}\".format(\n      X_train.shape[0], X_test.shape[0]))\nbest_score = 0\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters, train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_test, y_test)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_parameters))\nOut[18]:\nSize of training set: 112   size of test set: 38\nBest score: 0.97\nBest parameters: {'C': 100, 'gamma': 0.001}\nThe Danger of Overfitting the Parameters and the Validation Set\nGiven this result, we might be tempted to report that we found a model that performs\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\njust wrong), for the following reason: we tried many different parameters and\nGrid Search \n| \n261\nselected the one with best accuracy on the test set, but this accuracy won’t necessarily\ncarry over to new data. Because we used the test data to adjust the parameters, we can\nno longer use it to assess how good the model is. This is the same reason we needed\nto split the data into training and test sets in the first place; we need an independent\n2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\nSupervised Machine Learning Algorithms \n| \n97\n11 This follows from the Taylor expansion of the exponential map.\nit considers all possible polynomials of all degrees, but the importance of the features\ndecreases for higher degrees.11\nIn practice, the mathematical details behind the kernel SVM are not that important,\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\nquite easily—we’ll do so in the next section.\nUnderstanding SVMs\nDuring training, the SVM learns how important each of the training data points is to\nrepresent the decision boundary between the two classes. Typically only a subset of\nthe training points matter for defining the decision boundary: the ones that lie on the\nborder between the classes. These are called support vectors and give the support vec‐\ntor machine its name.\nTo make a prediction for a new point, the distance to each of the support vectors is\nmeasured. A classification decision is made based on the distances to the support vec‐\ntor, and the importance of the support vectors that was learned during training\n(stored in the dual_coef_ attribute of SVC).\nThe distance between data points is measured by the Gaussian kernel:\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\nis a parameter that controls the width of the Gaussian kernel.\nFigure 2-41 shows the result of training a support vector machine on a two-\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nrange_on_training = (X_train - min_on_training).max(axis=0)\n# subtract the min, and divide by range\n# afterward, min=0 and max=1 for each feature\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\nprint(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))\n102 \n| \nChapter 2: Supervised Learning\nOut[85]:\nMinimum for each feature\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\nMaximum for each feature\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\nIn[86]:\n# use THE SAME transformation on the test set,\n# using min and range of the training set (see Chapter 3 for details)\nX_test_scaled = (X_test - min_on_training) / range_on_training\nIn[87]:\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[87]:\nAccuracy on training set: 0.948\nAccuracy on test set: 0.951\nScaling the data made a huge difference! Now we are actually in an underfitting\nregime, where training and test set performance are quite similar but less close to\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\nplex model. For example:\nIn[88]:\nsvc = SVC(C=1000)\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[88]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\naccuracy.\nSupervised Machine Learning Algorithms \n| \n103\nStrengths, weaknesses, and parameters\nsvm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_valid, y_valid)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n262 \n| \nChapter 5: Model Evaluation and Improvement\n# rebuild a model on the combined training and validation set,\n# and evaluate it on the test set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\ntest_score = svm.score(X_test, y_test)\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\nprint(\"Best parameters: \", best_parameters)\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\nOut[20]:\nSize of training set: 84   size of validation set: 28   size of test set: 38\nBest score on validation set: 0.96\nBest parameters:  {'C': 10, 'gamma': 0.001}\nTest set score with best parameters: 0.92\nThe best score on the validation set is 96%: slightly lower than before, probably\nbecause we used less data to train the model (X_train is smaller now because we split\nour dataset twice). However, the score on the test set—the score that actually tells us\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\ndata 92% correctly, not 97% correctly as we thought before!\nThe distinction between the training set, validation set, and test set is fundamentally\nimportant to applying machine learning methods in practice. Any choices made\nbased on the test set accuracy “leak” information from the test set into the model.\nTherefore, it is important to keep a separate test set, which is only used for the final\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐ sian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nplt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\n106 \n| \nChapter 2: Supervised Learning\nFigure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\ntion function\nFor the small neural network pictured in Figure 2-45, the full formula for computing\nŷ in the case of regression would be (when using a tanh nonlinearity):\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\nHere, w are the weights between the input x and the hidden layer h, and v are the\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\ncomputations. An important parameter that needs to be set by the user is the number\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\nsets and as big as 10,000 for very complex data. It is also possible to add additional\nhidden layers, as shown in Figure 2-47:\nSupervised Machine Learning Algorithms \n| \n107\nIn[92]:\nmglearn.plots.plot_two_hidden_layer_graph()\nFigure 2-47. A multilayer perceptron with two hidden layers\nHaving large neural networks made up of many of these layers of computation is\nwhat inspired the term “deep learning.”\nTuning neural networks\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\ntwo_moons dataset we used earlier in this chapter. The results are shown in\nFigure 2-48:\nIn[93]:\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42) markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\nUncertainty Estimates from Classifiers \n| \n123\nFigure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\ning model shown in Figure 2-55\nThe boundaries in this plot are much more well-defined, and the small areas of\nuncertainty are clearly visible.\nThe scikit-learn website has a great comparison of many models and what their\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\nage you to go though the example there.\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\ncourtesy http://scikit-learn.org)\nUncertainty in Multiclass Classification\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\ndecision_function and predict_proba methods also work in the multiclass setting.\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\n124 \n| \nChapter 2: Supervised Learning\nIn[115]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\ngbrt.fit(X_train, y_train)\nIn[116]:\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n# plot the first few entries of the decision function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\n13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\never, if that happens, the prediction is made at random.\nOut[113]:\nPredicted probabilities:\n[[ 0.016  0.984]\n [ 0.843  0.157]\n [ 0.981  0.019]\n [ 0.974  0.026]\n [ 0.014  0.986]\n [ 0.025  0.975]]\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\nbe above 50% certainty. That class is the one that is predicted.13\nYou can see in the previous output that the classifier is relatively certain for most\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\nthe model and the parameters. A model that is more overfitted tends to make more\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\nally has more uncertainty in its predictions. A model is called calibrated if the\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\ndiction made with 70% certainty would be correct 70% of the time.\nIn the following example (Figure 2-56) we again show the decision boundary on the\ndataset, next to the class probabilities for the class 1:\nIn[114]:\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nmglearn.tools.plot_2d_separator(\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common\ntechnique to extend a binary classification algorithm to a multiclass classification\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\nlearned for each class that tries to separate that class from all of the other classes,\nresulting in as many binary models as there are classes. To make a prediction, all\nbinary classifiers are run on a test point. The classifier that has the highest score on its\nsingle class “wins,” and this class label is returned as the prediction.\nSupervised Machine Learning Algorithms \n| \n63\nHaving one binary classifier per class results in having one vector of coefficients (w)\nand one intercept (b) for each class. The class for which the result of the classification\nconfidence formula given here is highest is the assigned class label:\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\nand the same method of making a prediction is applied.\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\nuse a two-dimensional dataset, where each class is given by data sampled from a\nGaussian distribution (see Figure 2-19):\nIn[47]:\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\nFigure 2-19. Two-dimensional toy dataset containing three classes\n64 \n| \nChapter 2: Supervised Learning\nNow, we train a LinearSVC classifier on the dataset:\nIn[48]:\nlinear_svm = LinearSVC().fit(X, y)\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\nn_classes) and each column provides a “certainty score” for each class, where a large\nscore means that a class is more likely and a small score means the class is less likely.\nYou can recover the predictions from these scores by finding the maximum entry for\neach data point:\nIn[117]:\nprint(\"Argmax of decision function:\\n{}\".format(\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[117]:\nArgmax of decision function:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\nthe probabilities for the possible classes for each data point sum to 1:\nUncertainty Estimates from Classifiers \n| \n125\nIn[118]:\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n# show that sums across rows are one\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\nOut[118]:\nPredicted probabilities:\n[[ 0.107  0.784  0.109]\n [ 0.789  0.106  0.105]\n [ 0.102  0.108  0.789]\n [ 0.107  0.784  0.109]\n [ 0.108  0.663  0.228]\n [ 0.789  0.106  0.105]]\nSums: [ 1.  1.  1.  1.  1.  1.]\nWe can again recover the predictions by computing the argmax of predict_proba:\nIn[119]:\nprint(\"Argmax of predicted probabilities:\\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[119]:\nArgmax of predicted probabilities:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to",
            "summary": "There are two major types of supervised machine learning problems. Classification and Regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. Regression is the process of learning to accept a label from a list of possible labels, rather than choosing one from a set of choices. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter, we will look at the problem of binary classification, which tries to answer a yes/no When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                learn. The model_selection module was added in 0.18, and if you                use an earlier version of scikit-learn, you will need to adjust the imports from this module. A First Application: Classifying Iris Species. In this section, we will go through a simple machine learning application and create our first model. In the process, we'll introduce some core concepts and terms. We'll also go through some examples of how to use the model to identify iris species. The goal is to build a machine learning model that can learn from the measurements of irises whose species is known. The desired output for a single data point (an iris) is the species of this flower. In this problem, we want to predict one of several options (the species of iris). This is an example of a classification problem. Every iris in the dataset                belongs to one of three classes, so this problem is a three-class classification problem and is a supervised learning problem. The possi‐phthalble outputs (different species of Iris) are called classes. For more information on the project, visit the project's website or go to: http://iris.org/iris. We will use several datasets to illustrate the different algorithms. Some of the datasets will be small and synthetic (meaning made-up), designed to highlight particular aspects of the algorithms. The data we will use for this example is the Iris dataset, a classical dataset in machine learning.understanding the models will give you a better feeling for the different waysmachine learning algorithms can work. This chapter can also be used as a referenceguide, and you can come back to it when you are unsure about the workings of any of thegorithms. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available in paperback and Kindle versions for $39.99. The forge dataset consists of 26 data points, with 2 features. As is always the case in scatter plots, each data point isrepresented as one dot. The wave dataset has a single input feature and a continuous target variable (or response) that we want to model. To illustrate regression algorithms, we will use the synthetic wave dataset. For more information on the forge dataset, visit the forge website. For a more detailed look at the forge data set, see the forge site. For the rest of the article, please visit the Forge website. The plot created here (Figure 2-3) shows the single feature on the X-axis and the regression target (the output) on the y-axis. We started this chapter with a discussion of model complexity, then discussed gener‐ overseeing, or learning a model that is able to perform well on new, previously unseen data. In the next section, we will look at the impact of machine learning on the economy. We will end the chapter with an overview of the latest developments in machine learning. We hope to see you in the next chapter. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. This chapter contains a lot of information about the algorithms. Some knowledge of the models described here is important for successfully applying machine learning in practice. For small datasets, good as a baseline, easy to explain. In general, multiclass classifica­tion results are harder to understand than binary classification results. For example, what does being 85% accurate mean on this dataset? Here is a quick summary of when to use each model:. Nearest neighbors are defined as points belonging to class A, 10% belonging. to class B, and 5% to class C. For more information on how to use the models in this chapter, visit the Machine Learning Handbook. The Machine learning Handbook is published by Oxford University Press. Common tools are the confusion matrix and the classification report we saw in the previous section. The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. In Figure 5-18, each row corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 4-18. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate and score. For example, we could test classifying images of pedestrians against non-reviewedpedestrians and measure accuracy. The closest metric should be used whenever possible for model evaluation andselection. For more information, visit mglearn.tools. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any number in programming terms (or real number in mathematical terms), the error rate is a good measure of its predictive performance. The number of errors we make does not contain all the information we are interested in, however. An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting a person’s annual income from their education, their age, and where they live. When predicting income, the predicted value is an                amount, and can be any number in a given range. Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields,weather, and number of employees working on the farm. In supervised learning, we want to build a model on the training data and then make accurate predictions on new, unseen data. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French. We say a model is able to generalize from the training set to the test set. If our algorithm predicts $39,999 or $40,001 when it should have predicted $40,.000, we don’t mind that much. We call this generalization ‘overfitting’ or ‘underfitting.’ We call it ‘the ability to make predictions on unseen data’ We want to build a model that is able to generalize as accurately as possible. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                in the test set, this computes its nearest neighbors in the training set. We see that our model is about 86% accurate, meaning the model predicted the class                correctly for 86% of the samples in the test dataset. For two-dimensional datasets, we can also illustrate the prediction for all possible testpoints in the xy-plane. For three-dimensional dataset, we show how the prediction works in the 2D and 3D world. We can also show how our model works in 2D. Supervised Machine Learning Algorithms                 |                 37                . The following code produces the visualizations of the decision boundaries for one, three, and nine neighbors shown in Figure 2-6. We color the plane according to the class that would beassigned to a point in this region. This lets us view the decision boundary, which is the divide between where the algorithm assigns class 0 versus where it assigns class 1. For example, the boundary between class 0 and class 1 could be as small as 0.5 or as large as 1.5. For more details on the algorithm, see the Machine Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                amount, and can be any number in a given range. Another example is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an                arbitrary number. An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between                possible outcomes, then the problem is a regression problem. If the output is not consistent, then it is a classification problem. In supervised learning, we want to build a model on the training data and then be able to make accurate predictions on new, unseen data. If a model is able to generalize from the training set to the test set, we say it can generalize. In this article, we look at the difference between generalization, overfitting, and underfitting in a supervised learning model. We conclude that supervised learning can be used to improve the accuracy of computerized models of human behavior. The next step is to test the model’s ability to make predictions on unseen data that has the same characteris­tics as the trainingSet that we used. Back to the page you came from. There are two major types of supervised machine learning problems. The goal is to predict a class label, which is a choice from a predefined list of possibilities. In Chapter 1 we used the example of classifying irises into one of three possible species. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                Classification and Regression are two different types of machine learning. We want to build a model that is able to generalize as accurately as possible. We hope to show how this can be done in the next few chapters of the book. For more information, visit the book’s website or read the first chapter online. When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. The mean squared error is the sum of the squared differences between the predictions and the true values. When predicting income, the predicted value is an                and how model complexity can be controlled. We will now take a look at the most popular linear models for regression and how they can be used to predict income. Back to Mail Online home. back to the page you came from. The article was originally published in the online version of this article. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number, while the coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. That is to separate them from parameters that are set by the user. The training set and test set performance is shown in the plot below. We added a coordinate cross into the plot to make it easier to understand the line.Looking at w[0] we see that the slope should be around 0.4, which we can confirm visually. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. There are many different linear models for regression. The difference between thesemodels lies in how the model parameters w and b are learned from the training data, and how model complexity can be controlled. We will now take a look at the most popular linear models. We want to build a model that is able to generalize as accurately as possible. In the real world, this is actually a tricky problem. Let’s look at some of the best models for this purpose. We’ll start by looking at ordinary least squares (OLS) and then look at supervised learning. If the training and test sets have enough in common, we expect the model to be accurate on the test set. However, there are some cases where this can go wrong. For example, if we allow ourselves to build very complex models, we can’t always be as accurate as we like on the training set. Let’s take a look at a made-up example to illustrate this point. Say a novice data scien‐ mistakenlytist wants to predict whether a customer will buy a boat, given records of previous buyers and customers who we know are not interested in buying a boat. The goal is to send out promotional emails to people who are likely to actually make For data that has very different kinds of features, tree-based models might work better. Tuning neural networkparameters is also an art unto itself. For data that is data, where all the features have similar meanings, tree models might be better. For example, a data scientist might say that if a customer is older than 45, and has less than 3 children or is not divorcing, then they want to buy a boat. The rule is 100 percent accurate, says the data scientist. For more information, visit the Open Data Project. The OpenData Project is a project of the University of California, San Diego, and the California Institute of Technology. The project is funded by the California Endowment, the California State The number of nodes per hidden layer is often similar to the number of input features. You should start with one or two hidden layers, and possibly expand from there. A helpful measure when thinking about the model complexity of a neural network is the amount of weights or coefficients that are learned. In our experiments, we barely scratched the surveillanceface of possible ways to adjust neural network models and how to train them. The most important parameters are the num‐ grotesqueber of layers and the number-of-hidden units per layer. The number of features per layer is rarely higher than in the low to mid-thousands. It is difficult to estimate the complexity of neural networks, but it can be as high as 10,000. A common way to adjust parameters in a neural network is to first create a networkthat is large enough to overfit, making sure that the task can actually be learned by the network. If instead you use one layer with 1,000 hidden units, you are learning 100,000 weights from the input to the hidden layer. If you add a second hidden layer, you add 1,101,000—50 times larger than the model with two hidden layers of size 100. And if you add 100 hidden units to the first layer, there will be another 100 * 100 = 10,000weights from the first hidden layer to the secondhidden layer, resulting in a total of 20,100. Unsupervised learning algorithms don’t make use of the supervised information, making them unsupervised. Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. A common practice is to adjust the features so that the data representation is more suitable for these algorithms. The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of                the numbers are larger than x. Then, once you know the training data can be learned, either shrink the scaling are often used in tandem with supervised learning algorithms, or use a different method to learn the data in the first place. Four plots show four different ways to transform the data that yield more standard ranges. The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler works similarly to the Standard scaler in that it ensures statistical properties that guarantee that each feature is on the same scale. The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two feature levels of 10 and 15. The second plot shows a dataset with a feature level of 1 and a feature range of 9 and 10. The lower quartile is the number x such that one-fourth of the numbers are smaller than x, and the upper quartiles are the numbers that are The RobustScaler uses the median and quartiles,1 instead of mean and variance. This makes the Robust scaler ignore data points that are very though. The next step is usually online testing or live testing, where the consequences of employing the algorithm in the overall system are evaluated. Changing the recom‐                mendations or search results users are shown by a website can drastically change their behavior and lead to unexpected consequences. To protect against these sur‐                prises, most user-facing services employ A/B testing, a form of blind user study. For both groups, relevant success metrics will be recorded for a set period of time. Using A/B testing enables us to evaluate the algorithms “in the wild,” which mighthelp us to discover unexpected consequences when users are interacting with our model. Often A is a new model, while B is the established system. Then, the metrics of algorithm A and algorithm B will be compared, and a selection between the two approaches will be made according to these metrics. A great introduction to this subject can be found in the book Bandit Algo‐rithms for Website Optimization by John Myles White (O’Reilly).  Creating Your Own Estimator is a guide to building your own estimator in scikit-learn. The book has covered a variety of tools and algorithms implemented In Chapter 6 we discussed the importance of putting all data-dependent processing in one place. However, if your preprocessing is data dependent and you want to apply a grid search or cross-validation, things become trickier. As we can see from the plot, using only a single neighbor, each point in the training set has an obvious influence on the predictions, and the predicted values go through all of the data points. This leads to a very unsteady prediction. Using a small number of neighbors like three or five often works well. Choosing the right distance measure is somewhat beyond the scope of this book. By default, Euclidean distance is used, which works well in many settings. The model is very easy to understand, and often gives reasonable performance without a lot of adjustments. Using this algorithm is a good baseline method to try before considering more advanced techniques, such as convolutional networks or Bayes networks. For more information on the KNeighbors classifier, visit k-NN.org. For a more detailed look at the algorithm, visit the K-NN website. The k-nearest neighbors (k-NN) algorithm is arguably the simplest machine learning algorithm. Building the model consists only of storing the training dataset. When using the k-NN algorithm, it’s important to preprocess your data. This approach often does not perform well on datasets with many features. It does particularly badly with datasets where most features are 0 most of the time (so-called sparse datasets). So, while the nearest k-Nearest Neighbors algorithm is easy to understand, it is not often used in machine learning. But for now, let's get to the algorithms themselves. Back to the page you came from. The next chapter will focus on the supervised learning part of the algorithm. In its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐bors. This is the closest training data point to the point we want to make a prediction for. The prediction is then simply the known output for this training point. Figure 2-4 illustrates this for the case of classification on the forge dataset. For each of the three new data points, we marked the closest point in the training set for each prediction to be made. For example, for the one-nearest-neighbor model, the closest data point was the training point Instead of considering only the closest neighbor, we can also consider an arbitrarynumber, k, of neighbors. The prediction of the one-nearest-neighbor algo is the label of that point. For each test point, we count how many neighbors belong to 0 and how many belong to 1. We then assign the class that is more frequent: in other words, the majority class among the k-Nearest neighbors. For example, in Figure 2-5 we use the three closest neighbors: In[11]: grotesquemglearn.plots.plot_knn_classification(n_neighbors=3) grotesquemglearning.pl plots. plot_knN_ classification( n_ne The method can be applied to datasets with any number of classes. For more classes, we count how many k-nearest-neighbors belong to each class and again predict the most common class. The following example uses the three closest neighbors:In[11]: purposefullymglearn.plots.plot_knn_classification(n_neigh neighbors=3)Figure 2-5. Predictions made by the three-neapest-neirbors model on the forge dataset. The prediction is shown as the color of the cross. We split our data into a training and a test set so we can evaluate generalization performance. First, we import and instantiate the class. This is when we can set parameters, like the number of neighbors to use. Here, we set it to 3. Now, we fit the classifier using the training set. To make predictions on the test data, we call the predict method. For KNeighborsClassifier, this means storing the dataset, so that we can compute neighbors during prediction. We can use this method to predict neighbors on our test data. For each data point in the test set, this computes its nearest neighbors in the training set and finds the most common class. It willalso hold the information that the algorithm has extracted from the training data. The training data, as well the algorithm to make predictions on new data points. To build the model on the training set, we call the fit method of the knn object. Most models inscikit-learn have many parameters, but the majority of them are either speed opti‐mizations or for very special use cases. In the case of KNeighborsClassifier, it will just store the training data. The representation shows us which parameters were used in creating the model. Nearly all are the default values, but you can also find n_neighbors=1, which is the parameter that we passed. The fit method returns theknn object itself (and modifies it in place), so we get a string representation of our classifier. We can now make predictions using this model on new data for which we might not know the correct labels. Imagine we found an iris in the wild with a sepal length of 2.9 cm, a petal length of 1 cm, and aPetal width of 0.2 cm. What species of iris would this be? We can put this data into a NumPy array, again by calculating the shape. We will cover all the importantparameters in Chapter 2 of this book. We also show how model complexity can be controlled and how to print a scikit-learn model. We are now on to the next section: Classifying Iris Species and making predictions. Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. Linear regression finds the parameters w and b that mini‐mize the mean squared error between predictions and the true regression targets, y, on the training set. We will now take a look at the most popular linear models for regression in this article. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number. Coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. The ElasticNet class combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for L2 regularization. The prediction is made using the following formula: w[0] * x[0) + w[1]* x[1) + ...  ...         The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. For linear models for classification, the decision boundary is a linear function of the input. There are many algorithms for learning linear models. We will see examples of that in this section. For example, a (binary) lin‐ipientear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐phthalplane. The output, ŷ, is a Linear Regression Algorithm (LRA), which is similar to a linear regression algorithm but uses a different type of coefficients and a different cutoff point. Different algorithms choose different ways to measure what “fitting the training set                well’ means. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce. These algorithms all differ in the following two ways: The way in which they measure how well a particular combination of coefficients and intercept fits the training data. If and what kind of regularization they use to ensure that the model fits the data. We show the predictions of a linear model on the wave dataset in Figure 2-11. We added a coordinate cross into the plot to make it easier to understand the line. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. The difference between these models lies in how the model parameters w and b are learned from the training data. We will now take a look at the most popular linear models for regression. The most popular model is ordinary least squares (OLS), which is the simplest and most classic lin‐ and >50k. It would also be possible to predict the exact income, and make this a Regression task. However, that would be much more difficult, and the 50K division isinteresting to understand on its own. It is also possible to treat categori­cal features such as workclass, The most common way to represent categorical variables is using the one-hot-encoding or one-out-of-N encoding, also known as dummy variables. All of them come from a fixed list of possible values, and denote a qualitative property, as opposed to a quantity. The next section will explain how we can overcome the problem of how to represent data in this way. We will also look at how to use features to help us understand data and engineering features in a more general way. Back to the page you came from. Click here to read the next part of the series, \"How to Use Engineering Features in a Logistic Regression Classifier\", and to see the rest of the book. The idea behind dummy variables is to replace a categorical variable with one or more new features. The values 0 and 1 make sense in the formula for linear binary classification. We canrepresent any number of categories by introducing one new feature per category. L1 regularization might help, as it lim‐ishlyits the model to using only a few features. If we desire a more interpretable model, using L1Regularization might also help. We have four new features, called \"Gov.,\" \"Employee\", \"Private Employee\", \"Self Employed\", and \"Incorporated\". To encode these four possible values, we create four new L1 regularization is an example of a linear regression model. There are many parallels between linear models for binary classifica‐phthaltion and linear model for regression. Here is the coefficient plot and classific a‐reprehensive accuracy for L1 regularisation (Figure 2-18):. The coefficient plot shows the accuracy of the L1 logreg with C=0.001, C=1.000, and C=100.000. The test accuracy of l1 log Reg with C is 0.98. The regression accuracy of C is 1.0, which is the same as the test accuracy for C. Many linear classification models are for binary classification only, and don’t extendaturally to the multiclass case (with the exception of logistic regression). A common the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for L2 regularization. The main difference between the models is the penalty parameter, which influences the regularization and whether the model will use all available features or select only a subset. Let’s look at binary classifi‐urouscation first. First, we’re going to look at classification. The output, ŷ, is a linear function of the features: a line, plane, or hyperplane. If the function is smaller than zero, we predict the class –1; if it is larger thanzero, we prediction the class +1. This prediction rule is common to all linear models for classifica­tion. There are many different ways to find the coefficients (w) and the inter‐centriccept (b) for a classifier that separates two classes using a line or a hyper­plane. For linear models, the decision boundary is a Linear Decision Boundary (LDB), which is the same as the decision threshold for a regression model. The LDB is a type of linear decision model. There are many algorithms for learning linear models. These algorithms all differ in the way they measure how well a particular combination of coefficientsand intercept fits the training data. Different algorithms choose different ways to measure what “fitting the training set                well’ means. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce, as one might want. There are two major types of supervised machine learning problems, called classifica­tion and regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. We will see examples of that in this section.  Classification is sometimes separated into binary classification, which is distinguishing between exactly two classes, and multiclass classification. You can think of binary classification as trying to answer a yes/no question. Classifying emails as either spam or not spam is an example of a binary classification problem. We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities. Back to Mail Online home. Back To the page you came from.   Back to the pageyou came from,    The Mail Online page you were coming from, back to the pages you were Coming from,        Â  Â Â. The iris example, on the other hand, is an example of a multiclass classification prob‐lem. Another example is predicting what language a website is in from the text on the website. Which of the two classes is called positive is often asubjective matter, and specific to the domain. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms). Predicting aperson’s annual income from their education, When predicting income, the predicted value is an estimate of income. All three binary classifiers clas‐ encompass points there as “rest.” Which class would a point there be assigned to? The answer is the one with the highest value for the classification formula. The class of the closest line is the most likely to be the class of a point in the middle of the plot. For example, if points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as ‘rest’ Therefore, any point in this area will be classified as class 0 The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to. The new representation of the data. is now indeed possible to separate the two. classes using a linear model, a plane in three dimensions.    It is now. possible to split the two Classes using a. linear model.  The. new representation is shown in Figure 2-38. It is also possible to. separate the. twoclasses using a Linear model. We can fit a linear model to the augmented data (see Figure 2-39) We can confirm this by fit‐                ting a LinearSVC model to augmented data. A linear model is fitted to the data using the following algorithm. We can see the results of this fit in Figure 2 2-39 by using the Axes3D model. The model is then fitted to a set of augmented data to test the model's predictions. The results are shown in Figure 2 2.95 in the next section of this article. The linear SVM model is not actually linear any more. It is not a line, but more of an ellipse, as you can see from the plot created here. We started this chapter with a discussion of model complexity, then discussed gener‐ grotesquealization, or learning a model that is able to perform well on new, previously unseen data. The next section will focus on the decision boundary found by a linear S VM on the expanded three-dimensional dataset. The final section will look at the predictions made by the model on the new data. It will conclude with a look at how the model can be used to predict future events. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. Some knowledge of the models described here is important for successfully applying machine learning in practice. blindly applying an algorithm to a dataset without understandingthe assumptions the model makes and the meanings of the parameter settings will hardly lead to an accurate model. For small datasets, good as a baseline, easy to explain. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of nonzero features (i.e., large documents) The naive Bayes models share many of the strengths and weaknesses of the linear linear models. For large documents, use the naive linear linear Bayes model, which is more robust. For smaller documents, try using the simple linear linearBayes model. Decision trees are widely used models for classification and regression tasks. They learn a hierarchy of if/else questions, leading to a decision. Naive Bayes models are great baseline models andare often used on very large datasets. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. For more information on Decision Trees, visit the Decision Trees website. The decision trees website can be found at: http://www.decision-trees.org/. For more on the NaiveBayes model, visit: http:/www.naive-bayes.org. This series of questions can be expressed as a decision tree, as shown in Figure 2-22. In this illustration, each node in the tree either represents a question or a terminal training. For example, you could ask whether the animal can fly. If the animaldoesn’t have feathers, your possible animal choices are dolphins and bears, and you need to ask a question to distinguish between these two animals. If you want to know if the animal has fins, you can ask whether it has fins or not. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data and MultinomialNB assumes count data. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers likeLogisticRegression and LinearSVC. There are three kinds of naive Baye classifiers implemented in scikit-68. The BernoulliNB classifier counts how often every feature of each class is not zero. There are two classes, 0.0 and 1.0. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times. These same counts are then calculated for the data points in the second and third class. The classifier then counts how many times each feature is nonzero one time, and so on. This is most easily understood with an example of the classifier in action. For example, in the example above we have four data points The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. Counting the nonzero entries per class in essence looks like this: For each label in np.unique(y): # count (sum) entries of 1 per feature                    counts[label] = X[y == label].sum(axis=0) print(\"Feature counts:\\n{}\".format(counts) MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The naive Bayes models share many of the strengths and weaknesses of the linear models. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of nonzero features (i.e., large documents). iants of naiveBayes are widely used for sparse count data such as text. The naive models are based on the Bayes Bayes algorithm, which was developed in the 1960s by Richard Stallman and others. Decision trees are widely used models for classification and regression tasks. They learn a hierarchy of if/else questions, leading to a decision. Naive Bayes models are great baseline models andare often used on very large datasets. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. For more information on Decision Trees, visit the Decision Trees website. The decision trees website can be found at: http://www.decision-trees.org/. For more on the NaiveBayes model, visit: http:/www.naive-bayes.org. This series of questions can be expressed as a decision tree, as shown in Figure 2-22. In this illustration, each node in the tree either represents a question or a terminal training. For example, you could ask whether the animal can fly. If the animaldoesn’t have feathers, your possible animal choices are dolphins and bears, and you need to ask a question to distinguish between these two animals. If you want to know if the animal has fins, you can ask whether it has fins or not. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data and MultinomialNB assumes count data. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers likeLogisticRegression and LinearSVC. There are three kinds of naive Baye classifiers implemented in scikit-68. The BernoulliNB classifier counts how often every feature of each class is not zero. There are two classes, 0.0 and 1.0. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times. These same counts are then calculated for the data points in the second and third class. The classifier then counts how many times each feature is nonzero one time, and so on. This is most easily understood with an example of the classifier in action. For example, in the example above we have four data points The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. Counting the nonzero entries per class in essence looks like this: For each label in np.unique(y): # count (sum) entries of 1 per feature                    counts[label] = X[y == label].sum(axis=0) print(\"Feature counts:\\n{}\".format(counts) MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. GaussianNB is mostly used on very high-dimensional data, while the other two naive Bayes are widely used for sparse count data such as text. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of Python 2 vs. Python 2, 132-140 data transformation application, 134-140 supervised learning, 138-140 unsupervised learning, 133parameter selection with, 306-366 decision trees, 18 pure leafs, 73 PyMC language, 364 python distributions, 6 prepackaged distributions, and 6 pre-packaged python distributions. The Python programming language is based on the programming language Python 2.2, which was released in June 2011. It is available in Python 3, Python 4, Python 5, Python 6, and Python 3.0. It uses the Python 2 programming language and the Python Python 3, 12, 12-12-12, 6-6-6, 362R, 24-24-24, 240-240, 310-310, 47-56, 81-81, 247-247. Python 3 has 362 R, 24 R, 240 R, 236 R, 256 R, 310 R, 81 R, 85 R, 84 R, 88 R, 87 R, 96 R, 98 R, 97 R, 83 R, 86 R would never have become a core contributor to scikit-learn or learned to under‐stand this package as well as I do now. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning and gave me ideas for a textbook. My thanks also go out to all the other contribsiveutors who donate their time to improve and maintain this package. I would like to thank all of the contributors for their help with this article. I hope to see you in the next issue of the textbook. I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. The algorithm randomly selects a subset of features, and it looks for the best possible test involving one of these features. The decision tree is built based on this newly created dataset. And finally, thanks                to DTS, for your everlasting and endless support. Back to the page you came from.   Back to The page you were from.     The page that you were coming from was from  DTS.   Back to  the page that was from DTS. The bootstrap sampling leads to each decision tree in the random forest being built on a slightly different dataset. The number of features that are selected is controlled by the max_features parameter. This selection of a subset of features is repeated separately in each node, so that eachnode in a tree can make a decision using a different subset of the features. Together, these two mech‐phthalanisms ensure that all the trees in therandom forest are different. The result is that each split in each tree operates on a different set of features. If we set max_ features to n_fea                tures, that means that every split can look at all features in the dataset. To make a prediction using the random forest, the algorithm first makes a predictionfor every tree in the forest. If we set max_features to 1, that means that the algorithm has no choice at all on which feature to test, and can only search over differentresholds for the feature that was selected randomly. A high max_fea                tures means the trees in the random Forest will be quite similar, and they will be                able to fit the data easily, using the most distinctive features. A lowmax_features                means that the trees will be very We can average these results to get our final regression results. For regression, we can average 47, 224-232 supportive vector machines (SVMs), 56 linkage arrays, 185 live testing, 359 log functions. For machine learning, we average 32machine learning algorithms. For data representation, we use 211-250 examples of, 1, 13-23, and 5-13 supervised and unsupervised learning. We also use 163 mathematical functions for feature transforma‐tions. We use 314-321 pipelines and 314-315syntax for, 313-321manifold learning algorithms, and ix-366 for Scikit-learn. We can also use the Python programming language to test our results. Andreas C. Müller & Sarah Guido.Introduction to                  Machine Learning with Python. A GUIDE FOR DATA SCIENTISTS. A guide to Bayes classifiers and other Bayes-based data analysis tools. A toolkit for Bayes Bayes analysis with Python. An overview of the main features of BayesBayes. An introduction to the Python programming language and a guide to the programming tools used to use it. An explanation of some of the concepts used in the Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido. A Guide for Data Scientists. O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. Online editions are also available for most titles (http://safaribooksonline.com). The book is published in the United States of America and can be purchased for educational or business use. The O’Reilly logo is a registered trademark of O'Reilly Media, Inc. See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details. For more information, contact our corporate/institutional sales department: 800-998- Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information andinstructions contained in this work are accurate, they disclaim all responsibility for errors or omissions. The author is not responsible for damages resulting from the We hope we have convinced you of the usefulness of machine learning in a wide range of applications. Use of the information and instructions contained in this work is at your own peril. Keep digging into the data, and don’t lose sight of the larger picture. We hope that the information we introduce will be helpful for scientists and researchers, as well as data scien­tists working on commercial applications. For more information on machine learning, see the Machine Learning for Data: A/B Testing and Data Analysis section of this article. For information on how to use machine learning for data analysis, visit the Machine learning for Data analysis section of the same article, or contact us at:  http://www.scientist We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. If you are interested in the mathematics of machine learning, we recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and JeromeFriedman. We will also not describe how to write machinelearning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries. The book is available for free at the authors’ website. Machine learning is being used as a commodity tool in research and commercial applications. There are few resources out there that fully cover all the important aspects of implementing machine learning. We hope this book will help people who want to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its                applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes the tools and techniques we will use to implement machine learning in the real world. Chapter 4 explains how to use machine learning to analyze data. Ensembles of Decision Trees are methods that combine multiple machine learning models to create more powerful models. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. The main downside of decision trees is that even with the use of pre-pruning, they. tend to overfit and provide poor generalization performance. In most applications, the ensemble methods we discuss next are usually used in place of a sin‐giangle decision tree A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. The idea behind random forests is that each tree might do a relativelygood job of predicting, but will likely overfit on part of the data. If we build many decision trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. The results of this approach can be seen in the following image. The image is of a random forest with a gradient boos‐phthalted decision tree. The gradient boo­phthalted tree is a tree with a slightly different shape than the rest of the tree. To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other trees. This reduction in overfitting, while retaining the predictive power of the trees, can be shown using rigorous mathematics. For example, we can use gradient boosting and random forests, which perform well on similar kinds of data. Gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time totrain. If you want to apply gradient boosting to a large-scale problem, it might be worth looking into the xgboost package and its Python interface. The algorithm works well without scaling and on a mixture of binary and continuous features. It is faster (and sometimes easier to tune) than the scikit-learn implementation of gradient boosting on many datasets. For more information on gradient boosting, see gradientboost.org and gradientboosting.org. For a more detailed description of the algorithm, see the gradientboost The main parameters of gradient boosted tree models are the number of trees, n_estimators, and the learning_rate, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees. These two parameters are highly                Supervised Machine Learning Algorithms. As with other tree-based models, it also often does not work well on high-dimensional sparse data. In contrast to random forests, where a higher n_mostimators value is always better, increasing n_stimators in The decision_function is of shape of n_samples, and it returns one floating-point number for each sample. This value encodes how strongly the model believes a data point to belong to the “positive” class, in this case class 1. A common practice is to fitimizen_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. The decision function is called gbrt.decision_function(X_test).shape. It returns a shape of 25, which is the shape of a For binary classification, the “negative” class is always the first entry of the classes_attribute. The “positive’ class is the second entry of classes. Positive values indicate a preference for the posi‐ grotesquetive class, and negative values indicate the ‘negative’ (other) class. We can recover the prediction by looking only at the sign of the decision function. For example, we can see the first few entries of decision_function in the following example. The prediction is based on the thresholded decision function, which has a value of 4.2. We see the prediction in the next example, with the threshold level at 3.6. An important parameter is max_depth (or alternatively max_leaf_nodes), to reduce the complexity of each tree. Usually max_ depth is set very low for gradientboosted models, often not deeper than five splits. A common practice is to fit n_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. We explored the use of linear support vector machines for classification in “Linear models for classification” on page 56 of the book. The next type of supervised model we will discuss is kernelized support vectormachines. We will discuss kernelized Support Vector Machines in the next section. The math behind kernelized support vector machines is a bit involved, and is beyond the scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, andFriedman’s The Elements of Statistical Learning. However, we will try to give you some sense of the idea behind the method. We will restrict ourselves to the classification case, as implemented in SVC. Similar concepts apply to support vectorregression, as implement in SVR. We hope this will help you understand some of the One way to make a linear model more flexible is by adding more features. More and more trees are added to iterativelyimprove performance. Gradient boosted trees are frequently the winning entries in machine learning competitions. They are generally a bit more sensitive toparameter settings than random forests. But they can provide better accuracy if the param‐ worrisomeeters are set correctly. The learning_rate controls how strongly each tree tries to correct the mistakes of the previous trees. The number of trees in the ensemble also controls how quickly each tree corrects the previous tree’s mistakes. It is possible to add interactions between features and polynomials of the input features to improve the performance of the model. Using GradientBoostingClassifier on the Breast Cancerdataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used. A higher learning rate means each tree can make stronger corrections. Adding more trees to the ensemble also increases the model complexity, as the model has more chances to correct mistakes on the training set. The training set accuracy is 100%, we are likely to be overfitting.   The test set is 0.958, and the test set's accuracy is 1,000, so we don't need to overfit. The main downside of decision trees is that even with the use of pre-pruning, they they tend to overfit and provide poor generalization performance. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. In particular, decision trees work well when you have features that are on completely different scales, or a mix of binary and con‐tinuous features. To reduce overfit‐                ting, we could either apply stronger pre- Ensembles are methods that combine multiple machine learning models to create more powerful models. There are two ensemble models that have proven to                be effective on a wide range of datasets for classification and regression. Both of these models use decision trees as their building blocks. The idea behind random forests is that each tree might do a relatively                good job of predicting, but will likely overfit on part of the data. The ensemble methods we discuss next are usually used in place of a sin‐                gle decision tree in most applications. They are called random forests and gradient boos‐                ted decision trees. The methods are described in the next section of this article. The full article can be found at: http://www.rese To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other dimensional two-class dataset. If we build many trees that work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. This reduction in overfitting, while retaining the predictive power of the trees, can be shown using rigorous mathematics. In this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐ary. The following code creates a plot by training an SVM on the forge dataset. The code is based on sklearn.tools.make_handcrafted_dataset() and sklearnlearn.com/svm/sVC. It uses the SVC kernel with the C=10, gamma=0.1 and RBF kernel with C=5, gamma = 0.1. The gamma parameter controls the width of the Gaussian kernel. It determines the scale of what it means for points to be close together. The C parameter is a regularization parameter, similar to that used in the linear models. It limits the importance of each point (or more precisely, their dual_coef_).Let’s have a look at what happens when we vary these parameters (Figure 2-42): Figure 2- 42. Supervised Machine Learning Algorithms                 |                 99                99        ‘SVM’ is the name of the algorithm we are using to train the SVM. The SVM is a type of supervised machine learning. SVM will learn an SVM on the scaled training data. Decision boundaries and support vectors for different settings of the param will be used. We will use GridSearchCV to find better parameters for SVC using GridSearch CV, as dis‐cussed in Chapter 5. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, The standard KFold and StratifiedKFold are by far the most commonly used. For each split in the cross-validation, some part of the original training set will be declared the training part. The test part is used to measure what new data will look like to a model trained on the training set. We used the information from the entire training set to find the right scaling of the data. However, we already used the infor‐ipientmation contained in the test part. of the split, when scaling theData. Remember thatthe test part in each split is part ofthe training set, and weused the information. from the whole training set for this example. Finding the values of the important parameters of a model is a tricky task. Because it is such a common task, there are standard methods in scikit-learn to help you with it. The most commonly used method is grid search, which basically means trying all possible combinations of the parameters of interest. It is important to understand what the parameters mean before trying to adjust them. For example, the kernel bandwidth, gamma, and the regularization parameter, C, are important. The parameters are used in the SVC class of the scik it-learn algorithm. The kernel bandwidth and gamma are important for generalization. The regularizationparameters are also important for the generalization of the model. \"We have 36 combinations of parameters in total. Because we have six different settings for C and gamma We can implement a simple grid search just as for loops over the two parameters, evaluating a classifier for each combination. Looking at all possible combina­tions creates a table (or grid) of parameter settings for the SVM. We might be tempted to report that we found a model that performs.with 97% accuracy on our dataset. The Danger of Overfitting the Parameter Set and the Validation Set. The Model Evaluation and Improvement section of the book includes the model evaluation and improvement section. The book also includes the training and evaluation section, the test set section, and the validation set section. For more information on the book, visit: http://www.sklearn.com/svm/book/SVC We tried many different parameters and selected the one with best accuracy on the test set. This accuracy won’t necessarilycarry over to new data. Because we used the test data to adjust the parameters, we can't use it to assess how good the model is. This is the same reason we needed to split the data into training and test sets in the first place. We need an independent independent feature space. The radial basis function (RBF) kernel, also known as theGaussian kernel, is a bit harder to explain, as it corresponds to an infinite-dimensional featureSpace. During training, the SVM learns how important each of the training data points is to represent the decision boundary between the two classes. One way to explain the Gaussian kernel is that that it considers all possible polynomials of all degrees, but the importance of the features progressivelydecreases for higher degrees. In practice, the mathematical details behind the kernel SVM are not that important, though, and how an SVM with an RBF kernel makes a decision can be summarizedquite easily—we’ll do so in the next section. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. Figure 2-41 shows the result of training a support vector machine on a two-dimensional two-class dataset. The decision boundary is shown in black, and the sup‐port vectors are larger points with the wide outline. A classification decision is made based on the distances to the support vectors. The importance of the support vector that was learned during training is stored in the dual_coef_ attribute of SVC. The distance between data points is measured by the Gaussian kernel:                krbf(x1, x2) = exp (°x1 The main downside of the use of cross-validation is the time it takes to train all these models. To evaluate the accuracy of the SVM using a particular setting of C and gamma, we need to train 36 * 5 = 180 models. The following code creates a plot by training an SVM on the forge dataset. For each parameter setting (only a subset is shown), five accuracy values are compu‐phthalted, one for each split in the cross- validation. Then the mean validation accuracy iscomputed for eachparameter setting. The visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding code. For more information, visit sklearn.com. The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a The GridSearchCV class can be used to search over parameters. To use the class, you first need to specify theparameters you want to search. GridSearch CV will then per‐range_on_training = (X_train - min_on-training).max(axis=0) For each feature, min=0 and max=1 for each feature. For more information on how to use this class, visit the Grid searchCV website. The class can also be used as part of the Supervised Learning toolkit, which is available on the MIT Open Learning Project website and on the Google Play Store. The training and test set performance are quite similar but less close to 100% accuracy. Now we are actually in an underfitting regime. From here, we can try increasing either C or gamma to fit a more com‐ hypertrophicplex model.  1.] Use THE SAME transformation on the test set, and using min and range of the training set (see Chapter 3 for details) 2.] X_test_scaled = (X_test - min_on_training) / range_ on_training. 3 Supervised Machine Learning Algorithms                   103                Strengths, weaknesses, and parameters                svm = SVC(gamma=gamma, C=C) svm.fit(X_train, y_train) best_parameters = {'C': C, 'gamma': gamma} best_score = score.best_score (best_ score, C, gamma) best parameters = 'C': 10, 'Gamma': 0.001' best score on validation set: 0.96Best parameters: 'C' (C, gamma), 'Test set' (test_score), 'Best parameters' (worst_score, test_score) The distinction between the training set, validation set, and test set is fundamentallyimportant to applying machine learning methods in practice. It is good practice to do all exploratory analysis and model selection using the combination of a training and a validation set. Strictly speaking, evaluat‐ sian kernel. gamma and C both control the complexity of the model, with large values                in either resulting in a more complex model. The score on the test set—the score that actually tells us                how well we generalize—is even lower, at 92%. So we can only claim to classify new                data 92% correctly, not 97% correctly as we thought before! It is important to keep a separate test set, which is A family of algorithms known as neural networks has recently seen a revival under the name “deep learning.” While deep learning shows great promise in many machine learning applications, deep learning algorithms are often tailored very carefully to a specific use case. Here, we will only discuss some relatively simple methods, namelymultilayer perceptrons for classification and regression, that can serve as a starting point for more involved deep learning methods. Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neuralnetworks. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.Remember that the prediction by a linear regressor is given as: ŷ is a weighted sum of the input features x[0] to x[p], weighted by the learned coefficients w[0) to w[p]. We could visualize this graphically as shown in Figure 2-46. Visualization of logistic regression, where input features and predictions are shown as nodes, and the coefficients are connections between the nodes. In an MLP this process of computing weighted sums is repeated multiple times, first by computing hidden units that represent an intermediate processing step, which are then used to compute the final sum of all the input features. The result is a weighted sum of the inputs and the hidden units. For the small neural network pictured in Figure 2-45, the full formula for computing                ŷ in the case of regression would be (when using a tanh nonlinearity):. The weights v and w are learned from data, x are the input features, ŷ is the computed output, and h are intermediatecomputations. An important parameter that needs to be set by the user is the number of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐sets and as big as 10,000 for very complex data. The hyperbolic tangent activation function and the rectified linear activa‐paralleledtion function are used in the network. Having large neural networks made up of many of these layers of computation is what inspired the term “deep learning” It is also possible to add additional hidden layers, as shown in Figure 2-47. Let’s look into the workings of the MLP by applying it to the dataset we used earlier in this chapter. The results are shown in Figure 2-48. The boundaries in this plot are much more well-defined, and the small areas of                uncertainty are clearly visible. The scikit-learn website has a great comparison of many models and what their uncertainty estimates look like. The results are also shown in Figures 2-55 and 2-56. The decision boundary (left) and predicted probabilities for the gradient boost-boosting model shown inFigure 2-54. The boundary (right) and predictions for the Gradient Boosting Model are So far, we’ve only talked about uncertainty estimates in binary classification. Now, let’s talk about uncertainty in multiclass classification. We’re going to show you how this works. The decision_function and predict_proba methods also work in the multiclass setting. Let’s apply them on the Iris dataset, which is a three-class classification dataset. Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. The decision function has the shape (n_samples, n_probability) of a circle. We can plot the first few entries of the decision function to get the shape of the function. We’ll also use these methods to predict the outcome of a test. Predicted probabilities: 0.016  0.984, 0.843  0,157. The classifier is relatively certain for most points. How well the uncertainty actually reflects uncertainty in the data depends on the model and the parameters. A model that is more overfitted tends to make morecertain predictions, even if they might be wrong. The model with less complexity has more uncertainty in its predictions. The prediction is made at random if the probabilities for the two classes sum to 1, and exactly one of the classes will be predicted with above 50% certainty. A model is called calibrated if thereported uncertainty actually matches how correct it is. In a calibrated model, a pre‐paralleleddiction made with 70% certainty would be correct 70% of the time. In the following example we again show the decision boundary on the dataset. Next to the class probabilities for the class 1, we show the training and test points. We then plot the training points next to the test points in Figure 2-56. The training points are shown in the figure's right-hand corner. The test points are seen in the left-hand side of the figure. Binary classification is arguably the most common and conceptually simple applica‐tion of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. We will also look at the impact of choosing one model over another and how to use these metrics in the real world. We hope this article has helped you better understand machine learning and its potential applications to your business. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available on Amazon Kindle and the Mac. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. What does being 85% accurate mean on this dataset? In general, multiclass classifica­tion results are harder to understand than binary classification results. Common tools are the confusion matrix and the classification report we saw in the binary case in the previous section. For any point in the dataset, 10% of The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. Each row of the confusion matrix corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 5-18. A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs-rest approach. For the binary case, each row corresponds to the true label and the column to the predictedlabel. For multiclass case, the rows and columns are the same, but the labels are different in the multiclass class. For more information, see the Multiclass Classification Al Binary classifiers are run on a test point. The classifier that has the highest score “wins” The result of the classifier is then used to predict the outcome of the test. For example, the result of a test for a class is the result for the class for which the class is named. For a class that is unknown, the class will be called “unknown” by the binary classifier. For more information on the “one-to-one” method, see the ‘How to do it’ guide. The mathematics behind multiclass logistic regression differ somewhat from the one-vs.-rest approach, but they also result in one coefficient vector and intercept per class. The same method of making a prediction is applied. We use a two-dimensional dataset, where each class is given by data sampled from aGaussian distribution (see Figure 2-19). The results are published in the open-source version of the book, “Machine Learning for Dummies”. Two-dimensional toy dataset containing three classes. Each column provides a “certainty score” for each class. A large score means that a class is more likely and a small score means the class is less likely. The output of predict_proba has the same shape, (n_samples, n_classes) as the decision_function has the shape (n-1, n-2,n-3) We train a LinearSVC classifier on the dataset. We use the following code to train the classifier. The results are shown in the next section of the paper. The code is available on GitHub. It can be downloaded from the GitHub site here. The probabilities for the possible classes for each data point sum to 1 are shown in the following table. The probabilities for each class are given by the first few entries of predict_proba. For example, the probabilities are 0.107, 0.784 and 0.105 for the first and second classes respectively. Predictions: points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Any point in this area will be classified as class 0 by the final classifier. The class with the highest confidence formula is the class of the closest line. All three binary classifiers clas‐phthalsify points in the middle of the plot as ‘rest’ The classifier 0 is greater than zero, while it is smaller than zero for the other two classes. The result of the classifica­tion confidence formula for classifiers 0 and 1 is ‘0’. The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values foralpha or small values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to.",
            "children": [
                {
                    "id": "chapter-2-section-1",
                    "title": "Classification and Regression",
                    "content": "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nour first model. In the process, we will introduce some core concepts and terms.\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\niris flowers that she has found. She has collected some measurements associated with\neach iris: the length and width of the petals and the length and width of the sepals, all\nmeasured in centimeters (see Figure 1-2).\nShe also has the measurements of some irises that have been previously identified by\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\nthat these are the only species our hobby botanist will encounter in the wild.\nOur goal is to build a machine learning model that can learn from the measurements\nof these irises whose species is known, so that we can predict the species for a new\niris.\nA First Application: Classifying Iris Species \n| \n13\nFigure 1-2. Parts of the iris flower\nBecause we have measurements for which we know the correct species of iris, this is a\nsupervised learning problem. In this problem, we want to predict one of several\noptions (the species of iris). This is an example of a classification problem. The possi‐\nble outputs (different species of irises) are called classes. Every iris in the dataset\nbelongs to one of three classes, so this problem is a three-class classification problem.\nThe desired output for a single data point (an iris) is the species of this flower. For a\nparticular data point, the species it belongs to is called its label.\nMeet the Data\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\nthe algorithms.\nSome Sample Datasets\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\nwill be small and synthetic (meaning made-up), designed to highlight particular\naspects of the algorithms. Other datasets will be large, real-world examples.\nAn example of a synthetic two-class classification dataset is the forge dataset, which\nhas two features. The following code creates a scatter plot (Figure 2-2) visualizing all\nof the data points in this dataset. The plot has the first feature on the x-axis and the\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\nrepresented as one dot. The color and shape of the dot indicates its class:\nIn[2]:\n# generate dataset\nX, y = mglearn.datasets.make_forge()\n# plot dataset\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))\nOut[2]:\nX.shape: (26, 2)\n30 \n| \nChapter 2: Supervised Learning\nFigure 2-2. Scatter plot of the forge dataset\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\ndataset has a single input feature and a continuous target variable (or response) that\nwe want to model. The plot created here (Figure 2-3) shows the single feature on the\nx-axis and the regression target (the output) on the y-axis:\nIn[3]:\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")\nSupervised Machine Learning Algorithms \n| \n31\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nTo evaluate how well our model generalizes, we can call the score method with the\ntest data together with the test labels:\nIn[16]:\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\nOut[16]:\nTest set accuracy: 0.86\nWe see that our model is about 86% accurate, meaning the model predicted the class\ncorrectly for 86% of the samples in the test dataset.\nAnalyzing KNeighborsClassifier\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\npoints in the xy-plane. We color the plane according to the class that would be\nassigned to a point in this region. This lets us view the decision boundary, which is the\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\nSupervised Machine Learning Algorithms \n| \n37\nThe following code produces the visualizations of the decision boundaries for one,\nthree, and nine neighbors shown in Figure 2-6:\nIn[17]:\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # the fit method returns the object self, so we can instantiate\n    # and fit in one line\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\naxes[0].legend(loc=3)\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\nues of n_neighbors\nAs you can see on the left in the figure, using a single neighbor results in a decision\nboundary that follows the training data closely. Considering more and more neigh‐ number in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐",
                    "summary": "There are two major types of supervised machine learning problems. Classification and Regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. Regression is the process of learning to accept a label from a list of possible labels, rather than choosing one from a set of choices. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter, we will look at the problem of binary classification, which tries to answer a yes/no When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                learn. The model_selection module was added in 0.18, and if you                use an earlier version of scikit-learn, you will need to adjust the imports from this module. A First Application: Classifying Iris Species. In this section, we will go through a simple machine learning application and create our first model. In the process, we'll introduce some core concepts and terms. We'll also go through some examples of how to use the model to identify iris species. The goal is to build a machine learning model that can learn from the measurements of irises whose species is known. The desired output for a single data point (an iris) is the species of this flower. In this problem, we want to predict one of several options (the species of iris). This is an example of a classification problem. Every iris in the dataset                belongs to one of three classes, so this problem is a three-class classification problem and is a supervised learning problem. The possi‐phthalble outputs (different species of Iris) are called classes. For more information on the project, visit the project's website or go to: http://iris.org/iris. We will use several datasets to illustrate the different algorithms. Some of the datasets will be small and synthetic (meaning made-up), designed to highlight particular aspects of the algorithms. The data we will use for this example is the Iris dataset, a classical dataset in machine learning.understanding the models will give you a better feeling for the different waysmachine learning algorithms can work. This chapter can also be used as a referenceguide, and you can come back to it when you are unsure about the workings of any of thegorithms. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available in paperback and Kindle versions for $39.99. The forge dataset consists of 26 data points, with 2 features. As is always the case in scatter plots, each data point isrepresented as one dot. The wave dataset has a single input feature and a continuous target variable (or response) that we want to model. To illustrate regression algorithms, we will use the synthetic wave dataset. For more information on the forge dataset, visit the forge website. For a more detailed look at the forge data set, see the forge site. For the rest of the article, please visit the Forge website. The plot created here (Figure 2-3) shows the single feature on the X-axis and the regression target (the output) on the y-axis. We started this chapter with a discussion of model complexity, then discussed gener‐ overseeing, or learning a model that is able to perform well on new, previously unseen data. In the next section, we will look at the impact of machine learning on the economy. We will end the chapter with an overview of the latest developments in machine learning. We hope to see you in the next chapter. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. This chapter contains a lot of information about the algorithms. Some knowledge of the models described here is important for successfully applying machine learning in practice. For small datasets, good as a baseline, easy to explain. In general, multiclass classifica­tion results are harder to understand than binary classification results. For example, what does being 85% accurate mean on this dataset? Here is a quick summary of when to use each model:. Nearest neighbors are defined as points belonging to class A, 10% belonging. to class B, and 5% to class C. For more information on how to use the models in this chapter, visit the Machine Learning Handbook. The Machine learning Handbook is published by Oxford University Press. Common tools are the confusion matrix and the classification report we saw in the previous section. The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. In Figure 5-18, each row corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 4-18. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate and score. For example, we could test classifying images of pedestrians against non-reviewedpedestrians and measure accuracy. The closest metric should be used whenever possible for model evaluation andselection. For more information, visit mglearn.tools. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any number in programming terms (or real number in mathematical terms), the error rate is a good measure of its predictive performance. The number of errors we make does not contain all the information we are interested in, however. An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting a person’s annual income from their education, their age, and where they live. When predicting income, the predicted value is an                amount, and can be any number in a given range. Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields,weather, and number of employees working on the farm. In supervised learning, we want to build a model on the training data and then make accurate predictions on new, unseen data. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French. We say a model is able to generalize from the training set to the test set. If our algorithm predicts $39,999 or $40,001 when it should have predicted $40,.000, we don’t mind that much. We call this generalization ‘overfitting’ or ‘underfitting.’ We call it ‘the ability to make predictions on unseen data’ We want to build a model that is able to generalize as accurately as possible. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                in the test set, this computes its nearest neighbors in the training set. We see that our model is about 86% accurate, meaning the model predicted the class                correctly for 86% of the samples in the test dataset. For two-dimensional datasets, we can also illustrate the prediction for all possible testpoints in the xy-plane. For three-dimensional dataset, we show how the prediction works in the 2D and 3D world. We can also show how our model works in 2D. Supervised Machine Learning Algorithms                 |                 37                . The following code produces the visualizations of the decision boundaries for one, three, and nine neighbors shown in Figure 2-6. We color the plane according to the class that would beassigned to a point in this region. This lets us view the decision boundary, which is the divide between where the algorithm assigns class 0 versus where it assigns class 1. For example, the boundary between class 0 and class 1 could be as small as 0.5 or as large as 1.5. For more details on the algorithm, see the Machine Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                amount, and can be any number in a given range. Another example is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an                arbitrary number. An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between                possible outcomes, then the problem is a regression problem. If the output is not consistent, then it is a classification problem. In supervised learning, we want to build a model on the training data and then be able to make accurate predictions on new, unseen data. If a model is able to generalize from the training set to the test set, we say it can generalize. In this article, we look at the difference between generalization, overfitting, and underfitting in a supervised learning model. We conclude that supervised learning can be used to improve the accuracy of computerized models of human behavior. The next step is to test the model’s ability to make predictions on unseen data that has the same characteris­tics as the trainingSet that we used. Back to the page you came from. There are two major types of supervised machine learning problems. The goal is to predict a class label, which is a choice from a predefined list of possibilities. In Chapter 1 we used the example of classifying irises into one of three possible species. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                Classification and Regression are two different types of machine learning. We want to build a model that is able to generalize as accurately as possible. We hope to show how this can be done in the next few chapters of the book. For more information, visit the book’s website or read the first chapter online. When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. The mean squared error is the sum of the squared differences between the predictions and the true values. When predicting income, the predicted value is an                and how model complexity can be controlled. We will now take a look at the most popular linear models for regression and how they can be used to predict income. Back to Mail Online home. back to the page you came from. The article was originally published in the online version of this article. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number, while the coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. That is to separate them from parameters that are set by the user. The training set and test set performance is shown in the plot below. We added a coordinate cross into the plot to make it easier to understand the line.Looking at w[0] we see that the slope should be around 0.4, which we can confirm visually. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. There are many different linear models for regression. The difference between these models lies in how the model parameters w and b are learned from the training data, and how model complexity can be controlled. We will now take a look at the most popular linear models. The most popular model is ordinary least squares (OLS)",
                    "children": [
                        {
                            "id": "chapter-2-section-1-subsection-1",
                            "title": "Understanding Classification",
                            "content": "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nour first model. In the process, we will introduce some core concepts and terms.\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\niris flowers that she has found. She has collected some measurements associated with\neach iris: the length and width of the petals and the length and width of the sepals, all\nmeasured in centimeters (see Figure 1-2).\nShe also has the measurements of some irises that have been previously identified by\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\nthat these are the only species our hobby botanist will encounter in the wild.\nOur goal is to build a machine learning model that can learn from the measurements\nof these irises whose species is known, so that we can predict the species for a new\niris.\nA First Application: Classifying Iris Species \n| \n13\nFigure 1-2. Parts of the iris flower\nBecause we have measurements for which we know the correct species of iris, this is a\nsupervised learning problem. In this problem, we want to predict one of several\noptions (the species of iris). This is an example of a classification problem. The possi‐\nble outputs (different species of irises) are called classes. Every iris in the dataset\nbelongs to one of three classes, so this problem is a three-class classification problem.\nThe desired output for a single data point (an iris) is the species of this flower. For a\nparticular data point, the species it belongs to is called its label.\nMeet the Data\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\nthe algorithms.\nSome Sample Datasets\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\nwill be small and synthetic (meaning made-up), designed to highlight particular\naspects of the algorithms. Other datasets will be large, real-world examples.\nAn example of a synthetic two-class classification dataset is the forge dataset, which\nhas two features. The following code creates a scatter plot (Figure 2-2) visualizing all\nof the data points in this dataset. The plot has the first feature on the x-axis and the\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\nrepresented as one dot. The color and shape of the dot indicates its class:\nIn[2]:\n# generate dataset\nX, y = mglearn.datasets.make_forge()\n# plot dataset\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))\nOut[2]:\nX.shape: (26, 2)\n30 \n| \nChapter 2: Supervised Learning\nFigure 2-2. Scatter plot of the forge dataset\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\ndataset has a single input feature and a continuous target variable (or response) that\nwe want to model. The plot created here (Figure 2-3) shows the single feature on the\nx-axis and the regression target (the output) on the y-axis:\nIn[3]:\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")\nSupervised Machine Learning Algorithms \n| \n31\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nTo evaluate how well our model generalizes, we can call the score method with the\ntest data together with the test labels:\nIn[16]:\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\nOut[16]:\nTest set accuracy: 0.86\nWe see that our model is about 86% accurate, meaning the model predicted the class\ncorrectly for 86% of the samples in the test dataset.\nAnalyzing KNeighborsClassifier\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\npoints in the xy-plane. We color the plane according to the class that would be\nassigned to a point in this region. This lets us view the decision boundary, which is the\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\nSupervised Machine Learning Algorithms \n| \n37\nThe following code produces the visualizations of the decision boundaries for one,\nthree, and nine neighbors shown in Figure 2-6:\nIn[17]:\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # the fit method returns the object self, so we can instantiate\n    # and fit in one line\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\naxes[0].legend(loc=3)\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\nues of n_neighbors\nAs you can see on the left in the figure, using a single neighbor results in a decision\nboundary that follows the training data closely. Considering more and more neigh‐",
                            "summary": "There are two major types of supervised machine learning problems. Classification and Regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. Regression is the process of learning to accept a label from a list of possible labels, rather than choosing one from a set of choices. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter, we will look at the problem of binary classification, which tries to answer a yes/no When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                learn. The model_selection module was added in 0.18, and if you                use an earlier version of scikit-learn, you will need to adjust the imports from this module. A First Application: Classifying Iris Species. In this section, we will go through a simple machine learning application and create our first model. In the process, we'll introduce some core concepts and terms. We'll also go through some examples of how to use the model to identify iris species. The goal is to build a machine learning model that can learn from the measurements of irises whose species is known. The desired output for a single data point (an iris) is the species of this flower. In this problem, we want to predict one of several options (the species of iris). This is an example of a classification problem. Every iris in the dataset                belongs to one of three classes, so this problem is a three-class classification problem and is a supervised learning problem. The possi‐phthalble outputs (different species of Iris) are called classes. For more information on the project, visit the project's website or go to: http://iris.org/iris. We will use several datasets to illustrate the different algorithms. Some of the datasets will be small and synthetic (meaning made-up), designed to highlight particular aspects of the algorithms. The data we will use for this example is the Iris dataset, a classical dataset in machine learning.understanding the models will give you a better feeling for the different waysmachine learning algorithms can work. This chapter can also be used as a referenceguide, and you can come back to it when you are unsure about the workings of any of thegorithms. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available in paperback and Kindle versions for $39.99. The forge dataset consists of 26 data points, with 2 features. As is always the case in scatter plots, each data point isrepresented as one dot. The wave dataset has a single input feature and a continuous target variable (or response) that we want to model. To illustrate regression algorithms, we will use the synthetic wave dataset. For more information on the forge dataset, visit the forge website. For a more detailed look at the forge data set, see the forge site. For the rest of the article, please visit the Forge website. The plot created here (Figure 2-3) shows the single feature on the X-axis and the regression target (the output) on the y-axis. We started this chapter with a discussion of model complexity, then discussed gener‐ overseeing, or learning a model that is able to perform well on new, previously unseen data. In the next section, we will look at the impact of machine learning on the economy. We will end the chapter with an overview of the latest developments in machine learning. We hope to see you in the next chapter. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. This chapter contains a lot of information about the algorithms. Some knowledge of the models described here is important for successfully applying machine learning in practice. For small datasets, good as a baseline, easy to explain. In general, multiclass classifica­tion results are harder to understand than binary classification results. For example, what does being 85% accurate mean on this dataset? Here is a quick summary of when to use each model:. Nearest neighbors are defined as points belonging to class A, 10% belonging. to class B, and 5% to class C. For more information on how to use the models in this chapter, visit the Machine Learning Handbook. The Machine learning Handbook is published by Oxford University Press. Common tools are the confusion matrix and the classification report we saw in the previous section. The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. In Figure 5-18, each row corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 4-18. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate and score. For example, we could test classifying images of pedestrians against non-reviewedpedestrians and measure accuracy. The closest metric should be used whenever possible for model evaluation andselection. For more information, visit mglearn.tools. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any number in programming terms (or real number in mathematical terms), the error rate is a good measure of its predictive performance. The number of errors we make does not contain all the information we are interested in, however. An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting a person’s annual income from their education, their age, and where they live. When predicting income, the predicted value is an                amount, and can be any number in a given range. Another example of a regression task is predicting the yield of a corn farm given attributes such as previous yields,weather, and number of employees working on the farm. In supervised learning, we want to build a model on the training data and then make accurate predictions on new, unseen data. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French. We say a model is able to generalize from the training set to the test set. If our algorithm predicts $39,999 or $40,001 when it should have predicted $40,.000, we don’t mind that much. We call this generalization ‘overfitting’ or ‘underfitting.’ We call it ‘the ability to make predictions on unseen data’ We want to build a model that is able to generalize as accurately as possible. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                in the test set, this computes its nearest neighbors in the training set. We see that our model is about 86% accurate, meaning the model predicted the class                correctly for 86% of the samples in the test dataset. For two-dimensional datasets, we can also illustrate the prediction for all possible testpoints in the xy-plane. For three-dimensional dataset, we show how the prediction works in the 2D and 3D world. We can also show how our model works in 2D. Supervised Machine Learning Algorithms                 |                 37                . The following code produces the visualizations of the decision boundaries for one, three, and nine neighbors shown in Figure 2-6. We color the plane according to the class that would beassigned to a point in this region. This lets us view the decision boundary, which is the divide between where the algorithm assigns class 0 versus where it assigns class 1. For example, the boundary between class 0 and class 1 could be as small as 0.5 or as large as 1.5. For more details on the algorithm, see the Machine Using a single neighbor results in a decisionboundary that follows the training data closely. Decision boundaries created by the nearest neighbors model for different val­ues of n_neighbors.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-2",
                            "title": "Understanding Regression",
                            "content": "number in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐",
                            "summary": "An easy way to distinguish between classification and regression tasks is to ask whether there is some kind of continuity in the output. If there is continuity between possible outcomes, then the problem is a regression problem. Think about predicting                annual income. The predicted value is anamount, and can be any number in a given range. Another example of a regressiontask is predicting the yield of a corn farm given attributes such as previous yields, weather, and number of employees working on the farm. The yield again can be an                arbitrary number. It can be a real number in programming terms or a programming number in mathematical terms. In supervised learning, we want to build a model on the training data and then make accurate predictions on new, unseen data. A website is in one language, or it is in another. There is no continuity between languages, and there is no language that is between English and French. We say a model is able to generalize from the training set to the test set. If our algorithm predicts $39,999 or $40,001 when it should have predicted $40,.000, we don’t mind that much. We call this generalization ‘overfitting’ or ‘underfitting.’ We call it ‘the ability to make predictions on unseen data’ There are two major types of supervised machine learning problems. The goal is to predict a class label, which is a choice from a predefined list of possibilities. In Chapter 1 we used the example of classifying irises into one of three possible species. In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a                Classification and Regression are two different types of machine learning. We want to build a model that is able to generalize as accurately as possible. We hope to show how this can be done in the next few chapters of the book. For more information, visit the book’s website or read the first chapter online. When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. The mean squared error is the sum of the squared differences between the predictions and the true values. When predicting income, the predicted value is an                and how model complexity can be controlled. We will now take a look at the most popular linear models for regression and how they can be used to predict income. Back to Mail Online home. back to the page you came from. The article was originally published in the online version of this article. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number, while the coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. That is to separate them from parameters that are set by the user. The training set and test set performance is shown in the plot below. We added a coordinate cross into the plot to make it easier to understand the line.Looking at w[0] we see that the slope should be around 0.4, which we can confirm visually. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. There are many different linear models for regression. The difference between these models lies in how the model parameters w and b are learned from the training data, and how model complexity can be controlled. We will now take a look at the most popular linear models. The most popular model is ordinary least squares (OLS)",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-3",
                            "title": "Comparing Approaches",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-2",
                    "title": "Generalization, Overfitting, and Underfitting",
                    "content": "want to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\none in the future.\nUsually we build a model in such a way that it can make accurate predictions on the\ntraining set. If the training and test sets have enough in common, we expect the\nmodel to also be accurate on the test set. However, there are some cases where this\ncan go wrong. For example, if we allow ourselves to build very complex models, we\ncan always be as accurate as we like on the training set.\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\ntist wants to predict whether a customer will buy a boat, given records of previous\nboat buyers and customers who we know are not interested in buying a boat.2 The\ngoal is to send out promotional emails to people who are likely to actually make a\npurchase, but not bother those customers who won’t be interested.\nSuppose we have the customer records shown in Table 2-1.\nTable 2-1. Example data about customers\nAge\nNumber of \ncars owned\nOwns house\nNumber of children\nMarital status\nOwns a dog\nBought a boat\n66\n1\nyes\n2\nwidowed\nno\nyes\n52\n2\nyes\n3\nmarried\nno\nyes\n22\n0\nno\n0\nmarried\nyes\nno\n25\n1\nno\n1\nsingle\nno\nno\n44\n0\nno\n2\ndivorced\nyes\nno\n39\n1\nyes\n2\nmarried\nyes\nno\n26\n1\nno\n2\nsingle\nno\nno\n40\n3\nyes\n1\nmarried\nyes\nno\n53\n2\nyes\n2\ndivorced\nno\nyes\n64\n2\nyes\n3\ndivorced\nno\nno\n58\n2\nyes\n2\nmarried\nyes\nyes\n33\n1\nno\n1\nsingle\nno\nno\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is",
                    "summary": "In the real world, this is actually a tricky problem. We. want to build a model that is able to generalize as accurately as possible. If the training and test sets have enough in common, we expect the model to also be accurate on the test set. However, if we allow ourselves to build very complex models, we can’t always be as accurate as we like on the training set. Let’s take a look at a made-up example to illustrate this point. In the first part of this article, we looked at how to use supervised learning to predict the future. In this section, we look at how we can use this technique to predict future events. The goal is to send out promotional emails to people who are likely to actually make a purchase. The aim is to not bother those customers who won’t be interested in buying a boat. The customer records shown in Table 2-1 are used to make the decision. The data is used to predict whether a customer will buy a boat, given Data scientist: “If the customer is older than 45, and has less than 3 children or is not married or divorced, then they want to buy a boat.” When asked how well this rule of his does, the data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is.   The rule of thumb is that if you’re over 45 and have 3 children and not married, then you should buy a yacht. The rule is based on the age of the customer and the number of children you have. If you are under 45, then the rule is that you shouldn’t have any children or be married.",
                    "children": [
                        {
                            "id": "chapter-2-section-2-subsection-1",
                            "title": "Concept of Generalization",
                            "content": "want to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\none in the future.\nUsually we build a model in such a way that it can make accurate predictions on the\ntraining set. If the training and test sets have enough in common, we expect the\nmodel to also be accurate on the test set. However, there are some cases where this\ncan go wrong. For example, if we allow ourselves to build very complex models, we\ncan always be as accurate as we like on the training set.\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\ntist wants to predict whether a customer will buy a boat, given records of previous\nboat buyers and customers who we know are not interested in buying a boat.2 The\ngoal is to send out promotional emails to people who are likely to actually make a\npurchase, but not bother those customers who won’t be interested.\nSuppose we have the customer records shown in Table 2-1.\nTable 2-1. Example data about customers\nAge\nNumber of \ncars owned\nOwns house\nNumber of children\nMarital status\nOwns a dog\nBought a boat\n66\n1\nyes\n2\nwidowed\nno\nyes\n52\n2\nyes\n3\nmarried\nno\nyes\n22\n0\nno\n0\nmarried\nyes\nno\n25\n1\nno\n1\nsingle\nno\nno\n44\n0\nno\n2\ndivorced\nyes\nno\n39\n1\nyes\n2\nmarried\nyes\nno\n26\n1\nno\n2\nsingle\nno\nno\n40\n3\nyes\n1\nmarried\nyes\nno\n53\n2\nyes\n2\ndivorced\nno\nyes\n64\n2\nyes\n3\ndivorced\nno\nno\n58\n2\nyes\n2\nmarried\nyes\nyes\n33\n1\nno\n1\nsingle\nno\nno\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is",
                            "summary": "In the real world, this is actually a tricky problem. We. want to build a model that is able to generalize as accurately as possible. If the training and test sets have enough in common, we expect the model to also be accurate on the test set. However, if we allow ourselves to build very complex models, we can’t always be as accurate as we like on the training set. Let’s take a look at a made-up example to illustrate this point. In the first part of this article, we looked at how to use supervised learning to predict the future. In this section, we look at how we can use this technique to predict future events. The goal is to send out promotional emails to people who are likely to actually make a purchase. The aim is to not bother those customers who won’t be interested in buying a boat. The customer records shown in Table 2-1 are used to make the decision. The data is used to predict whether a customer will buy a boat, given Data scientist: “If the customer is older than 45, and has less than 3 children or is not married or divorced, then they want to buy a boat.” When asked how well this rule of his does, the data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is.   The rule of thumb is that if you’re over 45 and have 3 children and not married, then you should buy a yacht. The rule is based on the age of the customer and the number of children you have. If you are under 45, then the rule is that you shouldn’t have any children or be married.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-2",
                            "title": "Overfitting Problems",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-3",
                            "title": "Underfitting Problems",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-4",
                            "title": "Finding the Balance",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-3",
                    "title": "Relation of Model Complexity to Dataset Size",
                    "content": "data, where all the features have similar meanings. For data that has very different\nkinds of features, tree-based models might work better. Tuning neural network\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\nface of possible ways to adjust neural network models and how to train them.\nEstimating complexity in neural networks.    The most important parameters are the num‐\nber of layers and the number of hidden units per layer. You should start with one or\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\nlayer is often similar to the number of input features, but rarely higher than in the low\nto mid-thousands.\nA helpful measure when thinking about the model complexity of a neural network is\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\n100 = 10,000 weights between the input and the first hidden layer. There are also\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\nhidden layers of size 100.\nA common way to adjust parameters in a neural network is to first create a network\nthat is large enough to overfit, making sure that the task can actually be learned by\nthe network. Then, once you know the training data can be learned, either shrink the scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very",
                    "summary": "The number of nodes per hidden layer is often similar to the number of input features. For data that has very different kinds of features, tree-based models might work better. You should start with one or two hidden layers, and possibly expand from there. A helpful measure when thinking about the model complexity of a neural network is the amount of weights or coefficients that are learned. If you have 100 features and 100 hidden units, then there are 100 * 100 = 10,000 weights between the input and the first hidden layer. The number of hidden units per layer is rarely higher than in the low to mid-thousands. The most important parameters are the num‐consumingber of layers and the number-of-hidden-units per A common way to adjust parameters in a neural network is to first create a network that is large enough to overfit, making sure that the task can actually be learned by the network. If instead you use one layer with 1,000 hidden units, you are learning 100,000 weights from the input to the hidden layer. If you add a second hidden layer you add 1,100,000weights, for a whopping total of 1,101,000—50 times larger than the model with two hidden layers of size 100. The results are shown in the video below, which is part of a series of videos on how to use neural networks in the open-source software Caffe2. Unsupervised learning algorithms don’t make use of the supervised information, making them unsupervised. Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. A common practice is to adjust the features so that the data representation is more suitable for these algorithms. The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of                the numbers are larger than x. Then, once you know the training data can be learned, either shrink the scaling are often used in tandem with supervised learning algorithms, or use a different method to learn the data in the first place. Four plots show four different ways to transform the data that yield more standard ranges. The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler works similarly to the Standard scaler in that it ensures statistical properties that guarantee that each feature is on the same scale. The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two feature levels of 10 and 15. The second plot shows a dataset with a feature level of 1 and a feature range of 9 and 10. The lower quartile is the number x such that one-fourth of the numbers are smaller than x, and the upper quartiles are the numbers that are The RobustScaler uses the median and quartiles,1 instead of the mean and variance.",
                    "children": [
                        {
                            "id": "chapter-2-section-3-subsection-1",
                            "title": "Dataset Size Impact",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-3-subsection-2",
                            "title": "Model Complexity Considerations",
                            "content": "data, where all the features have similar meanings. For data that has very different\nkinds of features, tree-based models might work better. Tuning neural network\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\nface of possible ways to adjust neural network models and how to train them.\nEstimating complexity in neural networks.    The most important parameters are the num‐\nber of layers and the number of hidden units per layer. You should start with one or\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\nlayer is often similar to the number of input features, but rarely higher than in the low\nto mid-thousands.\nA helpful measure when thinking about the model complexity of a neural network is\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\n100 = 10,000 weights between the input and the first hidden layer. There are also\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\nhidden layers of size 100.\nA common way to adjust parameters in a neural network is to first create a network\nthat is large enough to overfit, making sure that the task can actually be learned by\nthe network. Then, once you know the training data can be learned, either shrink the",
                            "summary": "The number of nodes per hidden layer is often similar to the number of input features. For data that has very different kinds of features, tree-based models might work better. You should start with one or two hidden layers, and possibly expand from there. A helpful measure when thinking about the model complexity of a neural network is the amount of weights or coefficients that are learned. If you have 100 features and 100 hidden units, then there are 100 * 100 = 10,000 weights between the input and the first hidden layer. The number of hidden units per layer is rarely higher than in the low to mid-thousands. The most important parameters are the num‐consumingber of layers and the number-of-hidden-units per A common way to adjust parameters in a neural network is to first create a network that is large enough to overfit, making sure that the task can actually be learned by the network. Then, once you know the training data can be learned, either shrink the.   or shrink the model. If instead you use one layer with 1,000 hidden units, you are learning 100,000 weights from the input to the hidden layer. If you add a second hidden layer you add 1,.000,000weights, for a whopping total of 1,101,000—50 times larger than the model with two hidden layers of size 100. For more information on how to use neural networks, go to neuralnetwork.org.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-3-subsection-3",
                            "title": "Scaling Relationships",
                            "content": "scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very",
                            "summary": "Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms. This is a simple per-feature rescaling and shift of theData. The following code shows a simple example of such a rescaling. The code is called plot_scaling() and the data is plotted on a plot. The data is then plotted on the plot and the scaling is applied to the plot. For example, the plot looks like this: plot.scaling(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler uses the median and quartiles,1 instead of the mean and variance. This makes the Robust scaler ignore data points that are very.    The following four plots show four different ways to transform the data that yield more standard ranges. Different ways to rescale and preprocess a dataset are shown in Figure 3-1, 3-2, and 3-3. The plots are based on a synthetic two-class classification dataset with two feature levels of 10 and 15. The first feature (the x-axis value) is between around 1 and 9 and the second",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-4",
                    "title": "Supervised Machine Learning Algorithms",
                    "content": "though. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\nprises, most user-facing services employ A/B testing, a form of blind user study. In\nFrom Prototype to Production \n| \n359\nA/B testing, without their knowledge a selected portion of users will be provided with\na website or service using algorithm A, while the rest of the users will be provided\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\nand a selection between the two approaches will be made according to these metrics.\nUsing A/B testing enables us to evaluate the algorithms “in the wild,” which might\nhelp us to discover unexpected consequences when users are interacting with our\nmodel. Often A is a new model, while B is the established system. There are more\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\nrithms for Website Optimization by John Myles White (O’Reilly). \nBuilding Your Own Estimator\nThis book has covered a variety of tools and algorithms implemented in scikit-\nlearn that can be used on a wide range of tasks. However, often there will be some\nparticular processing you need to do for your data that is not implemented in\nscikit-learn. It may be enough to just preprocess your data before passing it to your\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\nand you want to apply a grid search or cross-validation, things become trickier.\nIn Chapter 6 we discussed the importance of putting all data-dependent processing",
                    "summary": "Most user-facing services employ A/B testing, a form of blind user study. Without their knowledge a selected portion of users will be provided with a website or service using algorithm A, while the rest of the users are provided with one with algorithm B. For both groups, relevant success metrics will be recorded for a set period of time. The next step is usually online testing or live testing, where the consequences of employing the algorithm in the overall system are evaluated. In the next section, we look at some of the best practices used in this type of testing. Using A/B testing enables us to evaluate the algorithms “in the wild,” which mighthelp us to discover unexpected consequences when users are interacting with our model. Often A is a new model, while B is the established system. Then, the metrics of algorithm A and algorithm B will be compared, and a selection between the two approaches will be made according to these metrics. A great introduction to this subject can be found in the book Bandit Algo‐rithms for Website Optimization by John Myles White (O’Reilly).  Creating Your Own Estimator is a guide to building your own estimator in scikit-learn. The book has covered a variety of tools and algorithms implemented In Chapter 6 we discussed the importance of putting all data-dependent processing. However, if your preprocessing is data dependent and you want to apply",
                    "children": [
                        {
                            "id": "chapter-2-section-4-subsection-1",
                            "title": "Overview of Algorithms",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-2",
                            "title": "Algorithm Selection",
                            "content": "though. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\nprises, most user-facing services employ A/B testing, a form of blind user study. In\nFrom Prototype to Production \n| \n359\nA/B testing, without their knowledge a selected portion of users will be provided with\na website or service using algorithm A, while the rest of the users will be provided\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\nand a selection between the two approaches will be made according to these metrics.\nUsing A/B testing enables us to evaluate the algorithms “in the wild,” which might\nhelp us to discover unexpected consequences when users are interacting with our\nmodel. Often A is a new model, while B is the established system. There are more\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\nrithms for Website Optimization by John Myles White (O’Reilly). \nBuilding Your Own Estimator\nThis book has covered a variety of tools and algorithms implemented in scikit-\nlearn that can be used on a wide range of tasks. However, often there will be some\nparticular processing you need to do for your data that is not implemented in\nscikit-learn. It may be enough to just preprocess your data before passing it to your\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\nand you want to apply a grid search or cross-validation, things become trickier.\nIn Chapter 6 we discussed the importance of putting all data-dependent processing",
                            "summary": "Most user-facing services employ A/B testing, a form of blind user study. Without their knowledge a selected portion of users will be provided with a website or service using algorithm A, while the rest of the users are provided with one with algorithm B. For both groups, relevant success metrics will be recorded for a set period of time. The next step is usually online testing or live testing, where the consequences of employing the algorithm in the overall system are evaluated. In the next section, we look at some of the best practices used in this type of testing. Using A/B testing enables us to evaluate the algorithms “in the wild,” which mighthelp us to discover unexpected consequences when users are interacting with our model. Often A is a new model, while B is the established system. Then, the metrics of algorithm A and algorithm B will be compared, and a selection between the two approaches will be made according to these metrics. A great introduction to this subject can be found in the book Bandit Algo‐rithms for Website Optimization by John Myles White (O’Reilly).  Creating Your Own Estimator is a guide to building your own estimator in scikit-learn. The book has covered a variety of tools and algorithms implemented In Chapter 6 we discussed the importance of putting all data-dependent processing. However, if your preprocessing is data dependent and you want to apply",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-3",
                            "title": "Performance Comparison",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-5",
                    "title": "Some Sample Datasets",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-2-section-5-subsection-1",
                            "title": "Dataset Examples",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-5-subsection-2",
                            "title": "Dataset Characteristics",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-5-subsection-3",
                            "title": "Data Preparation",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-6",
                    "title": "k-Nearest Neighbors",
                    "content": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0] the training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nTo build the model on the training set, we call the fit method of the knn object,\nwhich takes as arguments the NumPy array X_train containing the training data and\nthe NumPy array y_train of the corresponding training labels:\nIn[26]:\nknn.fit(X_train, y_train)\nOut[26]:\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')\nThe fit method returns the knn object itself (and modifies it in place), so we get a\nstring representation of our classifier. The representation shows us which parameters\nwere used in creating the model. Nearly all of them are the default values, but you can\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\nscikit-learn have many parameters, but the majority of them are either speed opti‐\nmizations or for very special use cases. You don’t have to worry about the other\nparameters shown in this representation. Printing a scikit-learn model can yield\nvery long strings, but don’t be intimidated by these. We will cover all the important\nparameters in Chapter 2. In the remainder of this book, we will not show the output\nof fit because it doesn’t contain any new information.\nA First Application: Classifying Iris Species \n| \n21\nMaking Predictions\nWe can now make predictions using this model on new data for which we might not\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\nfeatures (4):",
                    "summary": "Using only a single neighbor, each point in the training set has an obvious influence on the predictions. This leads to a very unsteady prediction. Considering more neighbors leads to smoother predictions, but these do not fit the training data as well. In practice, using a small number of neighbors like three or five often works well, but you shouldcertainly adjust this parameter. The KNeighbors classifier has a number of strengths and weaknesses. For more information on how to use the classifier, see the KNeighbor's guide to machine learning and the KNeighbors tutorial on the KNet website. For a more detailed look at the Kneighbors K-NN is a good baseline method to try before considering more advanced techniques. When using the k-NN algorithm, it’s important to preprocess your data. This approach often does not perform well on datasets with many features. It does particularly badly with datasets where most featuresare 0 most of the time (so-called sparse datasets). So, while the nearest k-neighbors algorithm is easy to understand, it is not often used to train machine learning algorithms. For more information on how to train a machine learning algorithm, see Machine Learning Algorithms: How Do You Train A Machine learning Algorithm? and How Can I train a Machine Learning algorithm? See www.machine learning.org for details. The k-NN algorithm is arguably the simplest machine learning algorithm. To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset. The prediction is then simply the known output for this training point. Figure 2-4.illustrates this for the case of classification on the forge dataset:In[10]: purposefullymglearn.plots.plot_knn_classification(n_neighbors=1)Figure 2-3. The k-Nearest Neighbors classification algorithm for forge is shown in the figure below. In its simplest version, the k-Neighbors algorithm only considers exactly one nearest neigh‐ Instead of considering only the closest neighbor, we can also consider an arbitrarynumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithmcomes from. For each test point, we count how many neighbors belong to 0 and 1. We then assign the class that is more frequent: in other words, the majority class among the k.-nearest neighbours. When considering more than one neighbors, we use voting to assign a label to each point. The prediction of the one-neaver-neighbor algo is shown by the color of the cross in the image. The model was trained on the forge dataset, which has more than 100,000 data points. The method can be applied to datasets with any number of classes. We then assign the class that is more frequent: in other words, the majority class among the k-nearest neighbors. The Prediction is shown as the color of the cross. You can see that the prediction for the new data point at the top left is not the same as the prediction when we used only one neighbor. This illustration is for a binary classification problem, but the method can also be used for a number of different types of data. For example, we could use a dataset with a class 0 and how many neighbors belong to class 1 The k-nearest neighbors algorithm can be used to predict the most common class. We split our data into a training and a test set so we can evaluate generalization performance. For more classes, we count how many neighbors belong to each class and again predict most common. The algorithm is called KNeighborsClassifier by scikit-reprehensivelearn. It can be downloaded from the GitHub repository. The code for the algorithm is available on GitHub. It is available in the following versions: 1.0.1, 2.0, and 3.1. For KNeighborsClassifier this means storing the dataset, so we can compute neighbors during prediction. To make predictions on the test data, we call the predict method. For each data point in the test set, this computes its nearest neighbors in the training set and finds the most common class among these. It will also hold the information that the algorithm has extracted from the training data, as well as the algorithm to make predictions. To build the model on the training set, we call the fit method of the knn object. Most models inscikit-learn have many parameters, but the majority of them are either speed opti‐mizations or for very special use cases. In the case of KNeighborsClassifier, it will just store the training data. The representation shows us which parameters were used in creating the model. Nearly all are the default values, but you can also find n_neighbors=1, which is the parameter that we passed. The fit method returns theknn object itself (and modifies it in place), so we get a string representation of our classifier. We can now make predictions using this model on new data for which we might not know the correct labels. We will cover all the importantparameters in Chapter 2. printing a scikit-learn model can yield very long strings, but don’t be intimidated by these. In the remainder of this book, we will not show the output of fit because it doesn't contain any new information. You don't have to worry about the otherparameters shown in this representation. You can print a model from the source code and use it to make your own predictions. For example, we could predict the species of iris based on the length, width, and petal length. We can do this by multiplying the number of",
                    "children": [
                        {
                            "id": "chapter-2-section-6-subsection-1",
                            "title": "KNN Algorithm",
                            "content": "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]",
                            "summary": "Using only a single neighbor, each point in the training set has an obvious influence on the predictions. This leads to a very unsteady prediction. Considering more neighbors leads to smoother predictions, but these do not fit the training data as well. In practice, using a small number of neighbors like three or five often works well, but you shouldcertainly adjust this parameter. The KNeighbors classifier has a number of strengths and weaknesses. For more information on how to use the classifier, see the KNeighbor's guide to machine learning and the KNeighbors tutorial on the KNet website. For a more detailed look at the Kneighbors K-NN is a good baseline method to try before considering more advanced techniques. When using the k-NN algorithm, it’s important to preprocess your data. This approach often does not perform well on datasets with many features. It does particularly badly with datasets where most featuresare 0 most of the time (so-called sparse datasets). So, while the nearest k-neighbors algorithm is easy to understand, it is not often used to train machine learning algorithms. For more information on how to train a machine learning algorithm, see Machine Learning Algorithms: How Do You Train A Machine learning Algorithm? and How Can I train a Machine Learning algorithm? See www.machine learning.org for details. The k-NN algorithm is arguably the simplest machine learning algorithm. To make a prediction for a new data point, the algorithm finds the closest data points in the training dataset. The prediction is then simply the known output for this training point. Figure 2-4.illustrates this for the case of classification on the forge dataset:In[10]: purposefullymglearn.plots.plot_knn_classification(n_neighbors=1)Figure 2-3. The k-Nearest Neighbors classification algorithm for forge is shown in the figure below. In its simplest version, the k-Neighbors algorithm only considers exactly one nearest neigh‐ Instead of considering only the closest neighbor, we can also consider an arbitrarynumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithmcomes from. For each test point, we count how many neighbors belong to 0 and 1. We then assign the class that is more frequent: in other words, the majority class among the k.-nearest neighbours. When considering more than one neighbors, we use voting to assign a label to each point. The prediction of the one-neaver-neighbor algo is shown by the color of the cross in the image. The model was trained on the forge dataset, which has more than 100,000 data points. The method can be applied to datasets with any number of classes. We then assign the class that is more frequent: in other words, the majority class among the k-nearest neighbors. The Prediction is shown as the color of the cross. You can see that the prediction for the new data point at the top left is not the same as the prediction when we used only one neighbor. This illustration is for a binary classification problem, but the method can also be used for a number of different types of data. For example, we could use a dataset with a class 0 and how many neighbors belong to class 1 The k-nearest neighbors algorithm can be used to predict the most common class. We split our data into a training and a test set so we can evaluate generalization performance. For more classes, we count how many neighbors belong to each class and again predict most common. The algorithm is called KNeighborsClassifier by scikit-reprehensivelearn. It can be downloaded from the GitHub repository. The code for the algorithm is available on GitHub. It is available in the following versions: 1.0.1, 2.0, and 3.1. For KNeighborsClassifier this means storing the dataset, so we can compute neighbors during prediction. To make predictions on the test data, we call the predict method. For each data point, this computes its nearest neighbors in the training set and finds the most common class among these.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-6-subsection-2",
                            "title": "KNN Parameters",
                            "content": "the training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nTo build the model on the training set, we call the fit method of the knn object,\nwhich takes as arguments the NumPy array X_train containing the training data and\nthe NumPy array y_train of the corresponding training labels:\nIn[26]:\nknn.fit(X_train, y_train)\nOut[26]:\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')\nThe fit method returns the knn object itself (and modifies it in place), so we get a\nstring representation of our classifier. The representation shows us which parameters\nwere used in creating the model. Nearly all of them are the default values, but you can\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\nscikit-learn have many parameters, but the majority of them are either speed opti‐\nmizations or for very special use cases. You don’t have to worry about the other\nparameters shown in this representation. Printing a scikit-learn model can yield\nvery long strings, but don’t be intimidated by these. We will cover all the important\nparameters in Chapter 2. In the remainder of this book, we will not show the output\nof fit because it doesn’t contain any new information.\nA First Application: Classifying Iris Species \n| \n21\nMaking Predictions\nWe can now make predictions using this model on new data for which we might not\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\nfeatures (4):",
                            "summary": "KNeighborsClassifier stores the training data, as well as the algorithm to make predictions on new data points. The fit method returns the knn object itself (and modifies it in place), so we get a string representation of our classifier. The representation shows us which parameters were used in creating the model. Nearly all of them are the default values, but you can also find n_neighbors=1, which is the parameter that we passed.    The model is called KNeighbors Classifier and it is trained on a set of data points called X_train and y_train. The training data is stored in the NumPy array X_ train and the training labels are stored Most models inscikit-learn have many parameters, but the majority of them are either speed opti‐mizations or for very special use cases. Printing a scikit-learn model can yield very long strings, but don’t be intimidated by these. We will cover all the importantparameters in Chapter 2. We can now make predictions using this model on new data for which we might not already know the correct labels. For example, imagine we found an iris in the wild with a sepal length of 1.5 cm, a sePal width of 2.9 cm, and a petal length of 0.2 cm. What species of iris would this be? We can put this data",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-6-subsection-3",
                            "title": "KNN Applications",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-6-subsection-4",
                            "title": "Implementation Details",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-7",
                    "title": "Linear Models",
                    "content": "and how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐ and >50k. It would also be possible to predict the exact income, and make this a\nregression task. However, that would be much more difficult, and the 50K division is\ninteresting to understand on its own.\nIn this dataset, age and hours-per-week are continuous features, which we know\nhow to treat. The workclass, education, sex, and occupation features are categori‐\ncal, however. All of them come from a fixed list of possible values, as opposed to a\nrange, and denote a qualitative property, as opposed to a quantity.\n212 \n| \nChapter 4: Representing Data and Engineering Features\nAs a starting point, let’s say we want to learn a logistic regression classifier on this\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\nthe following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\nway when applying logistic regression. The next section will explain how we can\novercome this problem.\nOne-Hot-Encoding (Dummy Variables)\nBy far the most common way to represent categorical variables is using the one-hot-\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\ndummy variables is to replace a categorical variable with one or more new features\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\nlinear binary classification (and for all other models in scikit-learn), and we can\nrepresent any number of categories by introducing one new feature per category, as\ndescribed here.\nLet’s say for the workclass feature we have possible values of \"Government\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\nrated\". To encode these four possible values, we create four new features, called \"Gov\ndifferent values of C\nSupervised Machine Learning Algorithms \n| \n61\nIf we desire a more interpretable model, using L1 regularization might help, as it lim‐\nits the model to using only a few features. Here is the coefficient plot and classifica‐\ntion accuracies for L1 regularization (Figure 2-18):\nIn[46]:\nfor C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_train, y_train)))\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_test, y_test)))\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)\nOut[46]:\nTraining accuracy of l1 logreg with C=0.001: 0.91\nTest accuracy of l1 logreg with C=0.001: 0.92\nTraining accuracy of l1 logreg with C=1.000: 0.96\nTest accuracy of l1 logreg with C=1.000: 0.96\nTraining accuracy of l1 logreg with C=100.000: 0.99\nTest accuracy of l1 logreg with C=100.000: 0.98\nAs you can see, there are many parallels between linear models for binary classifica‐\ntion and linear models for regression. As in regression, the main difference between\nthe models is the penalty parameter, which influences the regularization and\nwhether the model will use all available features or select only a subset.\n62 \n| \nChapter 2: Supervised Learning\nFigure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\n94 \n| \nChapter 2: Supervised Learning\nFigure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\nfeature derived from feature1\nIn the new representation of the data, it is now indeed possible to separate the two\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\nting a linear model to the augmented data (see Figure 2-39):\nIn[79]:\nlinear_svm_3d = LinearSVC().fit(X_new, y)\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n# show linear decision boundary\nfigure = plt.figure()\nax = Axes3D(figure, elev=-152, azim=-26)\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\nXX, YY = np.meshgrid(xx, yy)\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature0 ** 2\")\nSupervised Machine Learning Algorithms \n| \n95\nFigure 2-39. Decision boundary found by a linear SVM on the expanded three-\ndimensional dataset\nAs a function of the original features, the linear SVM model is not actually linear any‐\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\n(Figure 2-40):\nIn[80]:\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models",
                    "summary": "Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. Linear regression finds the parameters w and b that mini‐mize the mean squared error between predictions and the true regression targets. We will now take a look at the most popular linear models for regression and how model complexity can be controlled. Back to Mail Online home. back to the page you came from. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number. Coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. The ElasticNet class combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for L2 regularization. The prediction is made using the following formula: w[0] * x[0) + w[1]* x[1) + ...  ...         The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. For linear models for classification, the decision boundary is a linear function of the input. There are many algorithms for learning linear models. We will see examples of that in this section. For example, a (binary) lin‐ipientear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐phthalplane. The output, ŷ, is a Linear Regression Algorithm (LRA), which is similar to a linear regression algorithm but uses a different type of coefficients and a different cutoff point. Different algorithms choose different ways to measure what “fitting the training set                well’ means. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce. These algorithms all differ in the following two ways: The way in which they measure how well a particular combination of coefficients and intercept fits the training data. If and what kind of regularization they use to ensure that the model fits the data. We show the predictions of a linear model on the wave dataset in Figure 2-11. We added a coordinate cross into the plot to make it easier to understand the line. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. The difference between these models lies in how the model parameters w and b are learned from the training data. We will now take a look at the most popular linear models for regression. The most popular model is ordinary least squares (OLS), which is the simplest and most classic lin‐ and >50k. It would also be possible to predict the exact income, and make this a Regression task. However, that would be much more difficult, and the 50K division isinteresting to understand on its own. It is also possible to treat categori­cal features such as workclass, The most common way to represent categorical variables is using the one-hot-encoding or one-out-of-N encoding, also known as dummy variables. All of them come from a fixed list of possible values, and denote a qualitative property, as opposed to a quantity. The next section will explain how we can overcome the problem of how to represent data in this way. We will also look at how to use features to help us understand data and engineering features in a more general way. Back to the page you came from. Click here to read the next part of the series, \"How to Use Engineering Features in a Logistic Regression Classifier\", and to see the rest of the book. The idea behind dummy variables is to replace a categorical variable with one or more new features. The values 0 and 1 make sense in the formula for linear binary classification. We canrepresent any number of categories by introducing one new feature per category. L1 regularization might help, as it lim‐ishlyits the model to using only a few features. If we desire a more interpretable model, using L1Regularization might also help. We have four new features, called \"Gov.,\" \"Employee\", \"Private Employee\", \"Self Employed\", and \"Incorporated\". To encode these four possible values, we create four new L1 regularization is an example of a linear regression model. There are many parallels between linear models for binary classifica‐phthaltion and linear model for regression. Here is the coefficient plot and classific a‐reprehensive accuracy for L1 regularisation (Figure 2-18):. The coefficient plot shows the accuracy of the L1 logreg with C=0.001, C=1.000, and C=100.000. The test accuracy of l1 log Reg with C is 0.98. The regression accuracy of C is 1.0, which is the same as the test accuracy for C. Many linear classification models are for binary classification only, and don’t extendaturally to the multiclass case (with the exception of logistic regression). A common the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for L2 regularization. The main difference between the models is the penalty parameter, which influences the regularization and whether the model will use all available features or select only a subset. Let’s look at binary classifi‐urouscation first. First, we’re going to look at classification. The output, ŷ, is a linear function of the features: a line, plane, or hyperplane. If the function is smaller than zero, we predict the class –1; if it is larger thanzero, we prediction the class +1. This prediction rule is common to all linear models for classifica­tion. There are many different ways to find the coefficients (w) and the inter‐centriccept (b) for a classifier that separates two classes using a line or a hyper­plane. For linear models, the decision boundary is a Linear Decision Boundary (LDB), which is the same as the decision threshold for a regression model. The LDB is a type of linear decision model. There are many algorithms for learning linear models. These algorithms all differ in the way they measure how well a particular combination of coefficientsand intercept fits the training data. Different algorithms choose different ways to measure what “fitting the training set                well’ means. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce, as one might want. There are two major types of supervised machine learning problems, called classifica­tion and regression. In classification, the goal is to predict a class label, which is a choice from a predefined list of possibilities. We will see examples of that in this section.  Classification is sometimes separated into binary classification, which is distinguishing between exactly two classes, and multiclass classification. You can think of binary classification as trying to answer a yes/no question. Classifying emails as either spam or not spam is an example of a binary classification problem. We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities. Back to Mail Online home. Back To the page you came from.   Back to the pageyou came from,    The Mail Online page you were coming from, back to the pages you were Coming from,        Â  Â Â. The iris example, on the other hand, is an example of a multiclass classification prob‐lem. Another example is predicting what language a website is in from the text on the website. Which of the two classes is called positive is often asubjective matter, and specific to the domain. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms). Predicting aperson’s annual income from their education, When predicting income, the predicted value is an estimate of income. All three binary classifiers clas‐ encompass points there as “rest.” Which class would a point there be assigned to? The answer is the one with the highest value for the classification formula. The class of the closest line is the most likely to be the class of a point in the middle of the plot. For example, if points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as ‘rest’ Therefore, any point in this area will be classified as class 0 The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to. The new representation of the data. is now indeed possible to separate the two. classes using a linear model, a plane in three dimensions.    It is now. possible to split the two Classes using a. linear model.  The. new representation is shown in Figure 2-38. It is also possible to. separate the. twoclasses using a Linear model. We can fit a linear model to the augmented data (see Figure 2-39) We can confirm this by fit‐                ting a LinearSVC model to augmented data. A linear model is fitted to the data using the following algorithm. We can see the results of this fit in Figure 2 2-39 by using the Axes3D model. The model is then fitted to a set of augmented data to test the model's predictions. The results are shown in Figure 2 2.95 in the next section of this article. The linear SVM model is not actually linear any more. It is not a line, but more of an ellipse, as you can see from the plot created here. We started this chapter with a discussion of model complexity, then discussed gener‐ grotesquealization, or learning a model that is able to perform well on new, previously unseen data. The next section will focus on the decision boundary found by a linear S VM on the expanded three-dimensional dataset. The final section will look at the predictions made by the model on the new data. It will conclude with a look at how the model can be used to predict future events. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. This chapter contains a lot of information about the algorithms. Some knowledge of the models described here is important for successfully applying machine learning in practice. blindly applying an algorithm to a dataset without understanding the assumptions the model makes and the meanings of the parameter settings will rarely lead to an accurate model. For small datasets, good as a baseline, easy to explain. For a specific situa­tion, know which models to use and when to use them. For large datasets, know how to use different models.",
                    "children": [
                        {
                            "id": "chapter-2-section-7-subsection-1",
                            "title": "Linear Regression",
                            "content": "and how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐",
                            "summary": "Linear regression, or ordinary least squares (OLS), is the simplest and most classic method for regression. Linear regression finds the parameters w and b that mini‐mize the mean squared error between predictions and the true regression targets. We will now take a look at the most popular linear models for regression and how model complexity can be controlled. Back to Mail Online home. back to the page you came from. The “slope” parameters (w), also called weights or coefficients, are stored in the coef_ attribute. The offset or intercept (b) is storage in the intercept_ attribute: lr.coef_: [ 0.394] b.intercept_: -0.031804343026759746. Linear regression has no parameters, which is a benefit, but it also has no way to control model complexity. Scikit-learn always stores anything that is derived from the training data in attributes that end with a trailing underscore. The code that produces the model you can see in The intercept_ attribute is always a single float number. Coef_ is a NumPy array with one entry per input feature. As we only have a single input in the wave dataset, lr.coef_ only has a single entry. The ElasticNet class combines the penalties of Lasso and Ridge. In practice, this combination works best, though at the price of having two parameters to adjust: one for the L1 regularization, and one for L2 regularization. The prediction is made using the following formula: w[0] * x[0) + w[1]* x[1) + ...  ...         The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. For linear models for classification, the decision boundary is a linear function of the input. There are many algorithms for learning linear models. We will see examples of that in this section. For example, a (binary) lin‐ipientear classifier is a classifier that separates two classes using a line, a plane, or a hyper‐phthalplane. The output, ŷ, is a Linear Regression Algorithm (LRA), which is similar to a linear regression algorithm but uses a different type of coefficients and a different cutoff point. Different algorithms choose different ways to measure what “fitting the training set                well’ means. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce. These algorithms all differ in the following two ways: The way in which they measure how well a particular combination of coefficients and intercept fits the training data. If and what kind of regularization they use to ensure that the model fits the data. We show the predictions of a linear model on the wave dataset in Figure 2-11. We added a coordinate cross into the plot to make it easier to understand the line. Linear models for regression can be characterized as regression models for which theprediction is a line for a single feature. The intercept is where the prediction line should cross the y-axis. In Figure 2-10, using a straight line to make predictions seems very restrictive. It is a strong (and somewhat unrealistic) assumption that our target y is a linear line. For datasets with many features, linear models can be very pow‐                erful. For more information, see the Supervised Learning section of this book. The next two chapters in the book will focus on machine learning and supervised learning. The final chapter will be about machine learning in the form of a supervised learning framework. The code for this section is available on the If you have more features than training data points, any target y can be perfectly modeled (on the training set) as a linear function. There are many different linear models for regression. The difference between these models lies in how the model parameters w and b are learned from the training data, and how model complexity can be controlled. We will now take a look at the most popular linear models. The most popular model is ordinary least squares (OLS)",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-7-subsection-2",
                            "title": "Logistic Regression",
                            "content": "and >50k. It would also be possible to predict the exact income, and make this a\nregression task. However, that would be much more difficult, and the 50K division is\ninteresting to understand on its own.\nIn this dataset, age and hours-per-week are continuous features, which we know\nhow to treat. The workclass, education, sex, and occupation features are categori‐\ncal, however. All of them come from a fixed list of possible values, as opposed to a\nrange, and denote a qualitative property, as opposed to a quantity.\n212 \n| \nChapter 4: Representing Data and Engineering Features\nAs a starting point, let’s say we want to learn a logistic regression classifier on this\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\nthe following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\nway when applying logistic regression. The next section will explain how we can\novercome this problem.\nOne-Hot-Encoding (Dummy Variables)\nBy far the most common way to represent categorical variables is using the one-hot-\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\ndummy variables is to replace a categorical variable with one or more new features\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\nlinear binary classification (and for all other models in scikit-learn), and we can\nrepresent any number of categories by introducing one new feature per category, as\ndescribed here.\nLet’s say for the workclass feature we have possible values of \"Government\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\nrated\". To encode these four possible values, we create four new features, called \"Gov\ndifferent values of C\nSupervised Machine Learning Algorithms \n| \n61\nIf we desire a more interpretable model, using L1 regularization might help, as it lim‐\nits the model to using only a few features. Here is the coefficient plot and classifica‐\ntion accuracies for L1 regularization (Figure 2-18):\nIn[46]:\nfor C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_train, y_train)))\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_test, y_test)))\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)\nOut[46]:\nTraining accuracy of l1 logreg with C=0.001: 0.91\nTest accuracy of l1 logreg with C=0.001: 0.92\nTraining accuracy of l1 logreg with C=1.000: 0.96\nTest accuracy of l1 logreg with C=1.000: 0.96\nTraining accuracy of l1 logreg with C=100.000: 0.99\nTest accuracy of l1 logreg with C=100.000: 0.98\nAs you can see, there are many parallels between linear models for binary classifica‐\ntion and linear models for regression. As in regression, the main difference between\nthe models is the penalty parameter, which influences the regularization and\nwhether the model will use all available features or select only a subset.\n62 \n| \nChapter 2: Supervised Learning\nFigure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common",
                            "summary": "In this dataset, age and hours-per-week are continuous features, which we know how to treat. The workclass, education, sex, and occupation features are categori‐cal, however. All of them come from a fixed list of possible values, as opposed to a range. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using the following formula: w[0] * x [0] + w[1] / x[1) + ... + w [p] * X[p] + b > 0                . The 50K division is                interesting to understand on its own. It would also be possible to predict the exact income, and make this a                reg The most common way to represent categorical variables is using the one-hot-encoding or one-out-of-N encoding, also known as dummy variables. The idea behind dummy variables is to replace a categorical variable with one or more new features that can have the values 0 and 1. We can represent any number of categories by introducing one new feature per category, as described here. The next section will explain how we can overcome this problem by using the One-Hot-Encoding model in scikit-learn. The formula for binary classification makes sense when x[i] is numbers, but not when x [2] is \"Masters\" or \"Bachelors\" If we desire a more interpretable model, using L1 regularization might help, as it limites the model to using only a few features. To encode these four possible values, we create four new features, called L1 regularization is an example of a linear regression model. There are many parallels between linear models for binary classifica‐phthaltion and linear model for regression. Here is the coefficient plot and classific a‐reprehensive accuracy for L1 regularisation (Figure 2-18):. The coefficient plot shows the accuracy of the L1 logreg with C=0.001, C=1.000, and C=100.000. The test accuracy of l1 log Reg with C is 0.98. The regression accuracy of C is 1.0, which is the same as the test accuracy for C. Many linear classification models are for binary classification only, and don’t extendaturally to the multiclass case. As in regression, the main difference between the models is the penalty parameter. The penalty parameter influences the regularization and whether the model will use all available features or select only a subset. A common penalty parameter is the L1 penalty, which is used in logistic regression.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-7-subsection-3",
                            "title": "Linear Classification",
                            "content": "the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\n94 \n| \nChapter 2: Supervised Learning\nFigure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\nfeature derived from feature1\nIn the new representation of the data, it is now indeed possible to separate the two\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\nting a linear model to the augmented data (see Figure 2-39):\nIn[79]:\nlinear_svm_3d = LinearSVC().fit(X_new, y)\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n# show linear decision boundary\nfigure = plt.figure()\nax = Axes3D(figure, elev=-152, azim=-26)\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\nXX, YY = np.meshgrid(xx, yy)\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature0 ** 2\")\nSupervised Machine Learning Algorithms \n| \n95\nFigure 2-39. Decision boundary found by a linear SVM on the expanded three-\ndimensional dataset\nAs a function of the original features, the linear SVM model is not actually linear any‐\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\n(Figure 2-40):\nIn[80]:\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models",
                            "summary": "Linear models are also extensively used for classification. In this case, a prediction is made using the following formula. The formula looks very similar to the one for linear regression, but instead of just returning the weighted sum of the features, we threshold the predicted value at zero. If the function is smaller than zero, we predict the class –1; if it is larger thanzero, we expect the class +1. This prediction rule is common to all linear models for classifica­tion. The ElasticNet class, which combines the penalties of Lasso and Ridge, is the most popular classifier for machine learning. It is based on a combination of the L1 regularization and the L2 regularization. There are many algorithms for learning linear models. These algorithms all differ in the following two ways: The way in which they measure how well a particular combination of coefficients and intercept fits the training data. If and what kind of regularization they use, we will see examples of that in this section. We will also look at how different algorithms choose different ways to measure what “fitting the training set’s training data’ means. We hope this will help you understand some of the concepts behind linear models and how they can be applied to your own data. Back to the page you came from. Click here to read the next part of the series, “How Do We Use These Algorithms?� The goal is to predict a class label, which is a choice from a predefined list of possibilities. For technical mathematical reasons, it is not possible to adjust w and b to minimize the number of misclassifications the algorithms produce. There are two major types of supervised machine learning problems, called classifica‐ tumultuoustion and regression. In Chapter 1 we used the example of classifying irises into one of three possible species. In this chapter we will focus on the problem of binary classification, a special case of distinguishing between exactly two classes. We will also look at multiclass classification, which involves classifying between more than two groups. When looking for spam, “positive” could mean the spam class. The iris example, on the other hand, is an example of a multiclass classification prob. Another example is predicting what language a website is in from the text on the website. The classes here would be a pre-defined list of possible languages. For regression tasks, the goal is to predict a continuous number, or a floating-point number in programming terms (or real number in mathematical terms).  The goal of the regression task is to find a number that can be predicted from a list of words that are in the same language as the words in the text of the text. For example, the answer to the question “Is this email Predicting a person’s annual income from their education, their age, and where they live is an example of a regression task. When predicting income, the predicted value is an                belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Which class would a point there be assigned to? The answerhenyis the one with the highest value for the classification formula: the class of the closest line to the point in the middle of the plot. The result of the classifica­tion confidence formula for classifier 0 is greater than zero, while it is smaller for the other two classes The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to. The new representation of the data. is now indeed possible to separate the two. classes using a linear model, a plane in three dimensions.    It is now. possible to split the two Classes using a. linear model.  The. new representation is shown in Figure 2-38. It is also possible to. separate the. twoclasses using a Linear model. We can fit a linear model to the augmented data (see Figure 2-39) We can confirm this by fit‐                ting a LinearSVC model to augmented data. A linear model is fitted to the data using the following algorithm. We can see the results of this fit in Figure 2 2-39 by using the Axes3D model. The model is then fitted to a set of augmented data to test the model's predictions. The results are shown in Figure 2 2.95 in the next section of this article. The linear SVM model is not actually linear any more. It is not a line, but more of an ellipse, as you can see from the plot created here. We started this chapter with a discussion of model complexity, then discussed gener‐ grotesquealization, or learning a model that is able to perform well on new, previously unseen data. The next section will focus on the decision boundary found by a linear S VM on the expanded three-dimensional dataset. The final section will look at the predictions made by the model on the new data. It will conclude with a look at how the model can be used to predict future events. We discussed a wide array of machine learning models for classification and regression. We saw that for many of the algorithms, setting the rightparameters is important for good performance. We also discussed how to control model complexity for each of them. Some of the models are sensitive to how the input data is represented, and in particular to how features are scaled. We concluded by discussing some of the advantages and disadvantages of each of these models and how to use them for your own data collection and analysis. Back to the page you came from. This chapter contains a lot of information about the algorithms. Some knowledge of the models described here is important for successfully applying machine learning in practice. blindly applying an algorithm to a dataset without understanding the assumptions the model makes and the meanings of the parameter settings will rarely lead to an accurate model. For small datasets, good as a baseline, easy to explain. For a specific situa­tion, know which models to use and when to use them. For large datasets, know how to use different models.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-7-subsection-4",
                            "title": "Model Interpretation",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-8",
                    "title": "Naive Bayes Classifiers",
                    "content": "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents). iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).",
                    "summary": "The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. Naive Bayes are great baseline models and are often used on very large datasets, where training even a linear model might take too long. The decision trees are widely used models for classification and regression tasks. For more information, see the Decision Trees section of this article. These questions are similar to the questions you might ask in a game of 20 Questions. Your goal is to get to the right answer by asking as few if/elsequestions as possible. You might start off by asking whether the animal has feathers, aquestion that narrows down your possible animals to just two. If the answer is “yes,” you can ask another question that could help you distinguish between hawks andpenguins. For example, you could ask whether theanimal can fly. A series of questions can be expressed as a decision tree, as shown in Figure 2-22. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics from each feature. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers like LinearSVC and LogisticRegression. For more information, see the Supervised Learning section of this article. The full article is available on the MIT Press website, and can be downloaded for free in the U.S. and Europe. There are three kinds of naive Bayes classifiers implemented in scikit-68. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data. MultinomialNBassumes count data (that is, that each feature represents an integer count of some‐                �thing, like how often a word appears in a sentence) The classifier counts how often every feature of each class is not zero. There are two classes, 1.0 and 1.1, which are mostly used in text data classification. For example, in the example above, we have four data points, with four binary features each. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times, the second feature iszero one time and non zero one time, and so on. Counting the nonzero entries per class in essence looks like this: count (sum) entries of 1 per feature. The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. For class 1, the counts are calculated for the data points in the second class. These same counts are then calculated for class 2. The same thing happens for class 3 and 4. The results are shown in the figure below. MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The naive Bayes models share many of the strengths and weaknesses of the linear models. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of nonzero features (i.e., large documents). iants of naiveBayes are widely used for sparse count data such as text. The naive models are based on the Bayes Bayes algorithm, which was developed in the 1960s by Richard Stallman and others. Decision trees are widely used models for classification and regression tasks. They learn a hierarchy of if/else questions, leading to a decision. Naive Bayes models are great baseline models andare often used on very large datasets. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. For more information on Decision Trees, visit the Decision Trees website. The decision trees website can be found at: http://www.decision-trees.org/. For more on the NaiveBayes model, visit: http:/www.naive-bayes.org. This series of questions can be expressed as a decision tree, as shown in Figure 2-22. In this illustration, each node in the tree either represents a question or a terminal training. For example, you could ask whether the animal can fly. If the animaldoesn’t have feathers, your possible animal choices are dolphins and bears, and you need to ask a question to distinguish between these two animals. If you want to know if the animal has fins, you can ask whether it has fins or not. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data and MultinomialNB assumes count data. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers likeLogisticRegression and LinearSVC. There are three kinds of naive Baye classifiers implemented in scikit-68. The BernoulliNB classifier counts how often every feature of each class is not zero. There are two classes, 0.0 and 1.0. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times. These same counts are then calculated for the data points in the second and third class. The classifier then counts how many times each feature is nonzero one time, and so on. This is most easily understood with an example of the classifier in action. For example, in the example above we have four data points The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. Counting the nonzero entries per class in essence looks like this: For each label in np.unique(y): # count (sum) entries of 1 per feature                    counts[label] = X[y == label].sum(axis=0) print(\"Feature counts:\\n{}\".format(counts) MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. GaussianNB is mostly used on very high-dimensional data, while the other two naive Bayes are widely used for sparse count data such as text. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of",
                    "children": [
                        {
                            "id": "chapter-2-section-8-subsection-1",
                            "title": "Naive Bayes Concept",
                            "content": "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).",
                            "summary": "The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. Naive Bayes are great baseline models and are often used on very large datasets, where training even a linear model might take too long. The decision trees are widely used models for classification and regression tasks. For more information, see the Decision Trees section of this article. These questions are similar to the questions you might ask in a game of 20 Questions. Your goal is to get to the right answer by asking as few if/elsequestions as possible. You might start off by asking whether the animal has feathers, aquestion that narrows down your possible animals to just two. If the answer is “yes,” you can ask another question that could help you distinguish between hawks andpenguins. For example, you could ask whether theanimal can fly. A series of questions can be expressed as a decision tree, as shown in Figure 2-22. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics from each feature. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers like LinearSVC and LogisticRegression. For more information, see the Supervised Learning section of this article. The full article is available on the MIT Press website, and can be downloaded for free in the U.S. and Europe. There are three kinds of naive Bayes classifiers implemented in scikit-68. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data. MultinomialNBassumes count data (that is, that each feature represents an integer count of some‐                �thing, like how often a word appears in a sentence) The classifier counts how often every feature of each class is not zero. There are two classes, 1.0 and 1.1, which are mostly used in text data classification. For example, in the example above, we have four data points, with four binary features each. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times, the second feature iszero one time and non zero one time, and so on. Counting the nonzero entries per class in essence looks like this: count (sum) entries of 1 per feature. The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. For class 1, the counts are calculated for the data points in the second class. These same counts are then calculated for class 2. The same thing happens for class 3 and 4. The results are shown in the figure below. MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. GaussianNB is mostly used on very high-dimensional data, while the other two naive Bayes are widely used for sparse count data such as text. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-8-subsection-2",
                            "title": "Types of Naive Bayes",
                            "content": "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\n# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).",
                            "summary": "The naive Bayes models share many of the strengths and weaknesses of the linear models. They are very fast to train and to predict, and the training procedure is easy to understand. The models work very well with high-dimensional sparse data and are relatively robust to the parameters. Naive Bayes are great baseline models and are often used on very large datasets, where training even a linear model might take too long. The decision trees are widely used models for classification and regression tasks. For more information, see the Decision Trees section of this article. These questions are similar to the questions you might ask in a game of 20 Questions. Your goal is to get to the right answer by asking as few if/elsequestions as possible. You might start off by asking whether the animal has feathers, aquestion that narrows down your possible animals to just two. If the answer is “yes,” you can ask another question that could help you distinguish between hawks andpenguins. For example, you could ask whether theanimal can fly. A series of questions can be expressed as a decision tree, as shown in Figure 2-22. The reason that naive Bayes models are so efficient is that they learn parameters by looking at each feature individually and collecting simple per-class statistics from each feature. The price paid for this efficiency is that naiveBayes models often provide generalization performance that is slightly worse than that of linear classifiers like LinearSVC and LogisticRegression. For more information, see the Supervised Learning section of this article. The full article is available on the MIT Press website, and can be downloaded for free in the U.S. and Europe. There are three kinds of naive Bayes classifiers implemented in scikit-68. GaussianNB can be applied to continuous data, while BernoulliNB assumes binary data. MultinomialNBassumes count data (that is, that each feature represents an integer count of some‐                �thing, like how often a word appears in a sentence) The classifier counts how often every feature of each class is not zero. There are two classes, 1.0 and 1.1, which are mostly used in text data classification. For example, in the example above, we have four data points, with four binary features each. For class 0 (the first and third data points), the first feature is zero two times and nonzero zero times, the second feature iszero one time and non zero one time, and so on. Counting the nonzero entries per class in essence looks like this: count (sum) entries of 1 per feature. The other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐ferent in what kinds of statistics they compute. For class 1, the counts are calculated for the data points in the second class. These same counts are then calculated for class 2. The same thing happens for class 3 and 4. The results are shown in the figure below. MultinomialNB takes into account the average value of each feature for each class, while GaussianNB stores the standard deviation of each class. The way alpha works is that the algorithm adds to the data alpha many virtual data points that have positive values for all the features. This leads to a prediction formula that is of the same form as in the lin‐                ear models (see “Linear models for classification” on page 56). Unfortunately, coef_ for the naive Bayes models has a somewhat different meaning than in the linear mod‐                els, in that coef is not the same as w.                Supervised Machine Learning Algorithms ipient| 69 grotesquely. The algorithm’s performance is relatively robust to the setting of alpha, meaning that setting alpha is not critical for good performance. A large alpha means more smoothing, resulting in lesscomplex models. GaussianNB is mostly used on very high-dimensional data, while the other two naive Bayes are widely used for sparse count data such as text. MultinomialNBusually performs better than BinaryNB, particularly on datasets with a relatively largenumber of",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-8-subsection-3",
                            "title": "Applications",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-8-subsection-4",
                            "title": "Implementation",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-9",
                    "title": "Decision Trees",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-2-section-9-subsection-1",
                            "title": "Tree Structure",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-9-subsection-2",
                            "title": "Split Criteria",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-9-subsection-3",
                            "title": "Tree Parameters",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-9-subsection-4",
                            "title": "Pruning Techniques",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-10",
                    "title": "Ensembles of Decision Trees",
                    "content": "preprocessing, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\nkinds of, 133\nparameter selection with, 306\npipelines and, 317\npurpose of, 132\nscaling training and test data, 136\nprincipal component analysis (PCA)\ndrawbacks of, 146\nexample of, 140\nfeature extraction with, 147\nunsupervised nature of, 145\nvisualizations with, 142\nwhitening option, 150\nprobabilistic modeling, 363\n372 \n| \nIndex\nprobabilistic programming, 363\nproblem solving\nbuilding your own estimators, 360\nbusiness metrics and, 358\ninitial approach to, 357\nresources, 361-366\nsimple vs. complicated cases, 358\nsteps of, 358\ntesting your system, 359\ntool choice, 359\nproduction systems\ntesting, 359\ntool choice, 359\npruning for decision trees, 74\npseudorandom number generators, 18\npure leafs, 73\nPyMC language, 364\nPython\nbenefits of, 5\nprepackaged distributions, 6\nPython 2 vs. Python 3, 12\nPython(x,y), 6\nstatsmodel package, 362\nR\nR language, 362\nradial basis function (RBF) kernel, 97\nrandom forests\nanalyzing, 85\nbuilding, 84\ndata representation and, 220-224\nvs. decision trees, 83\nvs. gradient boosted regression trees, 88\nparameters, 88\npredictions with, 84\nrandomization in, 83\nstrengths and weaknesses, 87\nrandom_state parameter, 18\nranking, 363\nreal numbers, 26\nrecall, 282\nreceiver operating characteristics (ROC)\ncurves, 292-296\nrecommender systems, 363\nrectified linear unit (relu), 106\nrectifying nonlinearity, 106\nrecurrent neural networks (RNNs), 356\nrecursive feature elimination (RFE), 240\nregression\nf_regression, 236, 310\nLinearRegression, 47-56, 81, 247\nregression problems\nBoston Housing dataset, 34\nvs. classification problems, 26\nevaluation metrics and scoring, 299\nexamples of, 26\ngoals for, 26\nk-nearest neighbors, 40\nLasso, 53\nlinear models, 45\nridge regression, 49\nwave dataset illustration, 31\nregularization\nL1 regularization, 53\nL2 regularization, 49, 60\nrescaling\nexample of, 132-140\nkernel SVMs, 102\nresources, ix\nridge regression, 49\nrobustness-based clustering, 194\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\nrithm we described for the decision tree is slightly modified. Instead of looking for\nthe best test for each node, in each node the algorithm randomly selects a subset of\nthe features, and it looks for the best possible test involving one of these features. The\nnumber of features that are selected is controlled by the max_features parameter.\nThis selection of a subset of features is repeated separately in each node, so that each\nnode in a tree can make a decision using a different subset of the features.\nThe bootstrap sampling leads to each decision tree in the random forest being built\non a slightly different dataset. Because of the selection of features in each node, each\nsplit in each tree operates on a different subset of features. Together, these two mech‐\nanisms ensure that all the trees in the random forest are different.\nA critical parameter in this process is max_features. If we set max_features to n_fea\ntures, that means that each split can look at all features in the dataset, and no ran‐\ndomness will be injected in the feature selection (the randomness due to the\nbootstrapping remains, though). If we set max_features to 1, that means that the\nsplits have no choice at all on which feature to test, and can only search over different\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\ntures means that the trees in the random forest will be quite similar, and they will be\nable to fit the data easily, using the most distinctive features. A low max_features\nmeans that the trees in the random forest will be quite different, and that each tree\nmight need to be very deep in order to fit the data well.\nTo make a prediction using the random forest, the algorithm first makes a prediction\nfor every tree in the forest. For regression, we can average these results to get our final\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\ndata. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other ing completely ignored some of the features.\nAs both gradient boosting and random forests perform well on similar kinds of data,\na common approach is to first try random forests, which work quite robustly. If ran‐\ndom forests work well but prediction time is at a premium, or it is important to\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\ning to gradient boosting often helps.\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\nlooking into the xgboost package and its Python interface, which at the time of writ‐\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\ngradient boosting on many datasets.\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\nmost powerful and widely used models for supervised learning. Their main drawback\nis that they require careful tuning of the parameters and may take a long time to\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\nand on a mixture of binary and continuous features. As with other tree-based models,\nit also often does not work well on high-dimensional sparse data.\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\nmators, and the learning_rate, which controls the degree to which each tree is\nallowed to correct the mistakes of the previous trees. These two parameters are highly\nSupervised Machine Learning Algorithms \n| \n91\ninterconnected, as a lower learning_rate means that more trees are needed to build\na model of similar complexity. In contrast to random forests, where a higher n_esti\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\n# all will be split in a consistent manner\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n# build the gradient boosting model\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train_named)\nUncertainty Estimates from Classifiers \n| \n119\nThe Decision Function\nIn the binary classification case, the return value of decision_function is of shape\n(n_samples,), and it returns one floating-point number for each sample:\nIn[106]:\nprint(\"X_test.shape: {}\".format(X_test.shape))\nprint(\"Decision function shape: {}\".format(\n    gbrt.decision_function(X_test).shape))\nOut[106]:\nX_test.shape: (25, 2)\nDecision function shape: (25,)\nThis value encodes how strongly the model believes a data point to belong to the\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\ntive class, and negative values indicate a preference for the “negative” (other) class:\nIn[107]:\n# show the first few entries of decision_function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))\nOut[107]:\nDecision function:\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\nWe can recover the prediction by looking only at the sign of the decision function:\nIn[108]:\nprint(\"Thresholded decision function:\\n{}\".format(\n    gbrt.decision_function(X_test) > 0))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[108]:\nThresholded decision function:\n[ True False False False  True  True False  True  True  True False  True\n  True False  True False False False  True  True  True  True  True False\n  False]\nPredictions:\n['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\nFor binary classification, the “negative” class is always the first entry of the classes_\nattribute, and the “positive” class is the second entry of classes_. So if you want to\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\nboosted models, often not deeper than five splits.\nKernelized Support Vector Machines\nThe next type of supervised model we will discuss is kernelized support vector\nmachines. We explored the use of linear support vector machines for classification in\n“Linear models for classification” on page 56. Kernelized support vector machines\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\nport vector machines for classification and regression, we will restrict ourselves to the\nclassification case, as implemented in SVC. Similar concepts apply to support vector\nregression, as implemented in SVR.\nThe math behind kernelized support vector machines is a bit involved, and is beyond\nthe scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, and\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\nsense of the idea behind the method.\nLinear models and nonlinear features\nAs you saw in Figure 2-15, linear models can be quite limiting in low-dimensional\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\nmodel more flexible is by adding more features—for example, by adding interactions\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\npredictions on part of the data, and so more and more trees are added to iteratively\nimprove performance.\nGradient boosted trees are frequently the winning entries in machine learning com‐\npetitions, and are widely used in industry. They are generally a bit more sensitive to\nparameter settings than random forests, but can provide better accuracy if the param‐\neters are set correctly.\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\ntant parameter of gradient boosting is the learning_rate, which controls how\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\nrate means each tree can make stronger corrections, allowing for more complex mod‐\nels. Adding more trees to the ensemble, which can be accomplished by increasing\nn_estimators, also increases the model complexity, as the model has more chances\nto correct mistakes on the training set.\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\nIn[72]:\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[72]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.958\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\nlower the learning rate:\nSupervised Machine Learning Algorithms \n| \n89\nIn[73]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train))) data. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other",
                    "summary": "Python 2 vs. Python 2, 132-140 data transformation application, 134-140 supervised learning, 138-140 unsupervised learning, 133parameter selection with, 306-366 decision trees, 18 pure leafs, 73 PyMC language, 364 python distributions, 6 prepackaged distributions, and 6 pre-packaged python distributions. The Python programming language is based on the programming language Python 2.2, which was released in June 2011. It is available in Python 3, Python 4, Python 5, Python 6, and Python 3.0. It uses the Python 2 programming language and the Python Python 3, 12, 12-12-12, 6-6-6, 362R, 24-24-24, 240-240, 310-310, 47-56, 81-81, 247-247. Python 3 has 362 R, 24 R, 240 R, 236 R, 256 R, 310 R, 81 R, 85 R, 84 R, 88 R, 87 R, 96 R, 98 R, 97 R, 83 R, 86 R would never have become a core contributor to scikit-learn or learned to under‐stand this package as well as I do now. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning and gave me ideas for a textbook. My thanks also go out to all the other contribsiveutors who donate their time to improve and maintain this package. I would like to thank all of the contributors for their help with this article. I hope to see you in the next issue of the textbook. I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. The algorithm randomly selects a subset of features, and it looks for the best possible test involving one of these features. The decision tree is built based on this newly created dataset. And finally, thanks                to DTS, for your everlasting and endless support. Back to the page you came from.   Back to The page you were from.     The page that you were coming from was from  DTS.   Back to  the page that was from DTS. The bootstrap sampling leads to each decision tree in the random forest being built on a slightly different dataset. The number of features that are selected is controlled by the max_features parameter. This selection of a subset of features is repeated separately in each node, so that eachnode in a tree can make a decision using a different subset of the features. Together, these two mech‐phthalanisms ensure that all the trees in therandom forest are different. The result is that each split in each tree operates on a different set of features. If we set max_ features to n_fea                tures, that means that every split can look at all features in the dataset. To make a prediction using the random forest, the algorithm first makes a predictionfor every tree in the forest. If we set max_features to 1, that means that the algorithm has no choice at all on which feature to test, and can only search over differentresholds for the feature that was selected randomly. A high max_fea                tures means the trees in the random Forest will be quite similar, and they will be                able to fit the data easily, using the most distinctive features. A lowmax_features                means that the trees will be very We can average these results to get our final regression results. For regression, we can average 47, 224-232 supportive vector machines (SVMs), 56 linkage arrays, 185 live testing, 359 log functions. For machine learning, we average 32machine learning algorithms. For data representation, we use 211-250 examples of, 1, 13-23, and 5-13 supervised and unsupervised learning. We also use 163 mathematical functions for feature transforma‐tions. We use 314-321 pipelines and 314-315syntax for, 313-321manifold learning algorithms, and ix-366 for Scikit-learn. We can also use the Python programming language to test our results. Andreas C. Müller & Sarah Guido.Introduction to                  Machine Learning with Python. A GUIDE FOR DATA SCIENTISTS. A guide to Bayes classifiers and other Bayes-based data analysis tools. A toolkit for Bayes Bayes analysis with Python. An overview of the main features of BayesBayes. An introduction to the Python programming language and a guide to the programming tools used to use it. An explanation of some of the concepts used in the Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido. A Guide for Data Scientists. O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. Online editions are also available for most titles (http://safaribooksonline.com). The book is published in the United States of America and can be purchased for educational or business use. The O’Reilly logo is a registered trademark of O'Reilly Media, Inc. See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details. For more information, contact our corporate/institutional sales department: 800-998- Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information andinstructions contained in this work are accurate, they disclaim all responsibility for errors or omissions. The author is not responsible for damages resulting from the We hope we have convinced you of the usefulness of machine learning in a wide range of applications. Use of the information and instructions contained in this work is at your own peril. Keep digging into the data, and don’t lose sight of the larger picture. We hope that the information we introduce will be helpful for scientists and researchers, as well as data scien­tists working on commercial applications. For more information on machine learning, see the Machine Learning for Data: A/B Testing and Data Analysis section of this article. For information on how to use machine learning for data analysis, visit the Machine learning for Data analysis section of the same article, or contact us at:  http://www.scientist We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. If you are interested in the mathematics of machine learning, we recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and JeromeFriedman. We will also not describe how to write machinelearning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries. The book is available for free at the authors’ website. Machine learning is being used as a commodity tool in research and commercial applications. There are few resources out there that fully cover all the important aspects of implementing machine learning. We hope this book will help people who want to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its                applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes the tools and techniques we will use to implement machine learning in the real world. Chapter 4 explains how to use machine learning to analyze data. Ensembles of Decision Trees are methods that combine multiple machine learning models to create more powerful models. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. The main downside of decision trees is that even with the use of pre-pruning, they. tend to overfit and provide poor generalization performance. In most applications, the ensemble methods we discuss next are usually used in place of a sin‐giangle decision tree A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. The idea behind random forests is that each tree might do a relativelygood job of predicting, but will likely overfit on part of the data. If we build many decision trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. The results of this approach can be seen in the following image. The image is of a random forest with a gradient boos‐phthalted decision tree. The gradient boo­phthalted tree is a tree with a slightly different shape than the rest of the tree. To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other trees. This reduction in overfitting, while retaining the predictive power of the trees, can be shown using rigorous mathematics. For example, we can use gradient boosting and random forests, which perform well on similar kinds of data. Gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time totrain. If you want to apply gradient boosting to a large-scale problem, it might be worth looking into the xgboost package and its Python interface. The algorithm works well without scaling and on a mixture of binary and continuous features. It is faster (and sometimes easier to tune) than the scikit-learn implementation of gradient boosting on many datasets. For more information on gradient boosting, see gradientboost.org and gradientboosting.org. For a more detailed description of the algorithm, see the gradientboost The main parameters of gradient boosted tree models are the number of trees, n_estimators, and the learning_rate, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees. These two parameters are highly                Supervised Machine Learning Algorithms. As with other tree-based models, it also often does not work well on high-dimensional sparse data. In contrast to random forests, where a higher n_mostimators value is always better, increasing n_stimators in The decision_function is of shape of n_samples, and it returns one floating-point number for each sample. This value encodes how strongly the model believes a data point to belong to the “positive” class, in this case class 1. A common practice is to fitimizen_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. The decision function is called gbrt.decision_function(X_test).shape. It returns a shape of 25, which is the shape of a For binary classification, the “negative” class is always the first entry of the classes_attribute. The “positive’ class is the second entry of classes. Positive values indicate a preference for the posi‐ grotesquetive class, and negative values indicate the ‘negative’ (other) class. We can recover the prediction by looking only at the sign of the decision function. For example, we can see the first few entries of decision_function in the following example. The prediction is based on the thresholded decision function, which has a value of 4.2. We see the prediction in the next example, with the threshold level at 3.6. An important parameter is max_depth (or alternatively max_leaf_nodes), to reduce the complexity of each tree. Usually max_ depth is set very low for gradientboosted models, often not deeper than five splits. A common practice is to fit n_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. We explored the use of linear support vector machines for classification in “Linear models for classification” on page 56 of the book. The next type of supervised model we will discuss is kernelized support vectormachines. We will discuss kernelized Support Vector Machines in the next section. The math behind kernelized support vector machines is a bit involved, and is beyond the scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, andFriedman’s The Elements of Statistical Learning. However, we will try to give you some sense of the idea behind the method. We will restrict ourselves to the classification case, as implemented in SVC. Similar concepts apply to support vectorregression, as implement in SVR. We hope this will help you understand some of the One way to make a linear model more flexible is by adding more features. More and more trees are added to iterativelyimprove performance. Gradient boosted trees are frequently the winning entries in machine learning competitions. They are generally a bit more sensitive toparameter settings than random forests. But they can provide better accuracy if the param‐ worrisomeeters are set correctly. The learning_rate controls how strongly each tree tries to correct the mistakes of the previous trees. The number of trees in the ensemble also controls how quickly each tree corrects the previous tree’s mistakes. It is possible to add interactions between features and polynomials of the input features to improve the performance of the model. Using GradientBoostingClassifier on the Breast Cancerdataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used. A higher learning rate means each tree can make stronger corrections. Adding more trees to the ensemble also increases the model complexity, as the model has more chances to correct mistakes on the training set. The training set accuracy is 100%, we are likely to be overfitting.   The test set is 0.958, and the test set's accuracy is 1,000, so we don't need to overfit. The main downside of decision trees is that even with the use of pre-pruning, they they tend to overfit and provide poor generalization performance. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. In particular, decision trees work well when you have features that are on completely different scales, or a mix of binary and con‐tinuous features. To reduce overfit‐                ting, we could either apply stronger pre- Ensembles are methods that combine multiple machine learning models to create more powerful models. There are two ensemble models that have proven to                be effective on a wide range of datasets for classification and regression. Both of these models use decision trees as their building blocks. The idea behind random forests is that each tree might do a relatively                good job of predicting, but will likely overfit on part of the data. The ensemble methods we discuss next are usually used in place of a sin‐                gle decision tree in most applications. They are called random forests and gradient boos‐                ted decision trees. The methods are described in the next section of this article. The full article can be found at: http://www.rese To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other. If we build many trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. This reduction in overfitting,",
                    "children": [
                        {
                            "id": "chapter-2-section-10-subsection-1",
                            "title": "Random Forests",
                            "content": "preprocessing, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\nkinds of, 133\nparameter selection with, 306\npipelines and, 317\npurpose of, 132\nscaling training and test data, 136\nprincipal component analysis (PCA)\ndrawbacks of, 146\nexample of, 140\nfeature extraction with, 147\nunsupervised nature of, 145\nvisualizations with, 142\nwhitening option, 150\nprobabilistic modeling, 363\n372 \n| \nIndex\nprobabilistic programming, 363\nproblem solving\nbuilding your own estimators, 360\nbusiness metrics and, 358\ninitial approach to, 357\nresources, 361-366\nsimple vs. complicated cases, 358\nsteps of, 358\ntesting your system, 359\ntool choice, 359\nproduction systems\ntesting, 359\ntool choice, 359\npruning for decision trees, 74\npseudorandom number generators, 18\npure leafs, 73\nPyMC language, 364\nPython\nbenefits of, 5\nprepackaged distributions, 6\nPython 2 vs. Python 3, 12\nPython(x,y), 6\nstatsmodel package, 362\nR\nR language, 362\nradial basis function (RBF) kernel, 97\nrandom forests\nanalyzing, 85\nbuilding, 84\ndata representation and, 220-224\nvs. decision trees, 83\nvs. gradient boosted regression trees, 88\nparameters, 88\npredictions with, 84\nrandomization in, 83\nstrengths and weaknesses, 87\nrandom_state parameter, 18\nranking, 363\nreal numbers, 26\nrecall, 282\nreceiver operating characteristics (ROC)\ncurves, 292-296\nrecommender systems, 363\nrectified linear unit (relu), 106\nrectifying nonlinearity, 106\nrecurrent neural networks (RNNs), 356\nrecursive feature elimination (RFE), 240\nregression\nf_regression, 236, 310\nLinearRegression, 47-56, 81, 247\nregression problems\nBoston Housing dataset, 34\nvs. classification problems, 26\nevaluation metrics and scoring, 299\nexamples of, 26\ngoals for, 26\nk-nearest neighbors, 40\nLasso, 53\nlinear models, 45\nridge regression, 49\nwave dataset illustration, 31\nregularization\nL1 regularization, 53\nL2 regularization, 49, 60\nrescaling\nexample of, 132-140\nkernel SVMs, 102\nresources, ix\nridge regression, 49\nrobustness-based clustering, 194\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\nrithm we described for the decision tree is slightly modified. Instead of looking for\nthe best test for each node, in each node the algorithm randomly selects a subset of\nthe features, and it looks for the best possible test involving one of these features. The\nnumber of features that are selected is controlled by the max_features parameter.\nThis selection of a subset of features is repeated separately in each node, so that each\nnode in a tree can make a decision using a different subset of the features.\nThe bootstrap sampling leads to each decision tree in the random forest being built\non a slightly different dataset. Because of the selection of features in each node, each\nsplit in each tree operates on a different subset of features. Together, these two mech‐\nanisms ensure that all the trees in the random forest are different.\nA critical parameter in this process is max_features. If we set max_features to n_fea\ntures, that means that each split can look at all features in the dataset, and no ran‐\ndomness will be injected in the feature selection (the randomness due to the\nbootstrapping remains, though). If we set max_features to 1, that means that the\nsplits have no choice at all on which feature to test, and can only search over different\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\ntures means that the trees in the random forest will be quite similar, and they will be\nable to fit the data easily, using the most distinctive features. A low max_features\nmeans that the trees in the random forest will be quite different, and that each tree\nmight need to be very deep in order to fit the data well.\nTo make a prediction using the random forest, the algorithm first makes a prediction\nfor every tree in the forest. For regression, we can average these results to get our final\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers\nAndreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\ndata. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other",
                            "summary": "Python 2 vs. Python 2, 132-140 data transformation application, 134-140 supervised learning, 138-140 unsupervised learning, 133parameter selection with, 306-366 decision trees, 18 pure leafs, 73 PyMC language, 364 python distributions, 6 prepackaged distributions, and 6 pre-packaged python distributions. The Python programming language is based on the programming language Python 2.2, which was released in June 2011. It is available in Python 3, Python 4, Python 5, Python 6, and Python 3.0. It uses the Python 2 programming language and the Python Python 3, 12, 12-12-12, 6-6-6, 362R, 24-24-24, 240-240, 310-310, 47-56, 81-81, 247-247. Python 3 has 362 R, 24 R, 240 R, 236 R, 256 R, 310 R, 81 R, 85 R, 84 R, 88 R, 87 R, 96 R, 98 R, 97 R, 83 R, 86 R would never have become a core contributor to scikit-learn or learned to under‐stand this package as well as I do now. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning and gave me ideas for a textbook. My thanks also go out to all the other contribsiveutors who donate their time to improve and maintain this package. I would like to thank all of the contributors for their help with this article. I hope to see you in the next issue of the textbook. I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. The algorithm randomly selects a subset of features, and it looks for the best possible test involving one of these features. The decision tree is built based on this newly created dataset. And finally, thanks                to DTS, for your everlasting and endless support. Back to the page you came from.   Back to The page you were from.     The page that you were coming from was from  DTS.   Back to  the page that was from DTS. The bootstrap sampling leads to each decision tree in the random forest being built on a slightly different dataset. The number of features that are selected is controlled by the max_features parameter. This selection of a subset of features is repeated separately in each node, so that eachnode in a tree can make a decision using a different subset of the features. Together, these two mech‐phthalanisms ensure that all the trees in therandom forest are different. The result is that each split in each tree operates on a different set of features. If we set max_ features to n_fea                tures, that means that every split can look at all features in the dataset. To make a prediction using the random forest, the algorithm first makes a predictionfor every tree in the forest. If we set max_features to 1, that means that the algorithm has no choice at all on which feature to test, and can only search over differentresholds for the feature that was selected randomly. A high max_fea                tures means the trees in the random Forest will be quite similar, and they will be                able to fit the data easily, using the most distinctive features. A lowmax_features                means that the trees will be very We can average these results to get our final regression results. For regression, we can average 47, 224-232 supportive vector machines (SVMs), 56 linkage arrays, 185 live testing, 359 log functions. For machine learning, we average 32machine learning algorithms. For data representation, we use 211-250 examples of, 1, 13-23, and 5-13 supervised and unsupervised learning. We also use 163 mathematical functions for feature transforma‐tions. We use 314-321 pipelines and 314-315syntax for, 313-321manifold learning algorithms, and ix-366 for Scikit-learn. We can also use the Python programming language to test our results. Andreas C. Müller & Sarah Guido.Introduction to                  Machine Learning with Python. A GUIDE FOR DATA SCIENTISTS. A guide to Bayes classifiers and other Bayes-based data analysis tools. A toolkit for Bayes Bayes analysis with Python. An overview of the main features of BayesBayes. An introduction to the Python programming language and a guide to the programming tools used to use it. An explanation of some of the concepts used in the Introduction to Machine Learning with Python by Andreas C. Müller and Sarah Guido. A Guide for Data Scientists. O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472. Online editions are also available for most titles (http://safaribooksonline.com). The book is published in the United States of America and can be purchased for educational or business use. The O’Reilly logo is a registered trademark of O'Reilly Media, Inc. See http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details. For more information, contact our corporate/institutional sales department: 800-998- Python, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc. While the publisher and the authors have used good faith efforts to ensure that the information andinstructions contained in this work are accurate, they disclaim all responsibility for errors or omissions. The author is not responsible for damages resulting from the We hope we have convinced you of the usefulness of machine learning in a wide range of applications. Use of the information and instructions contained in this work is at your own peril. Keep digging into the data, and don’t lose sight of the larger picture. We hope that the information we introduce will be helpful for scientists and researchers, as well as data scien­tists working on commercial applications. For more information on machine learning, see the Machine Learning for Data: A/B Testing and Data Analysis section of this article. For information on how to use machine learning for data analysis, visit the Machine learning for Data analysis section of the same article, or contact us at:  http://www.scientist We made a conscious effort not to focus too much on the math, but rather on thepractical aspects of using machine learning algorithms. You will get the most out of the book if you are somewhat familiar with Python and the NumPy and matplotlib libraries. If you are interested in the mathematics of machine learning, we recommend the book The Elements of Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and JeromeFriedman. We will also not describe how to write machinelearning algorithms from scratch, and will instead focus on how to use the large array of models already implemented in scikit-learn and other libraries. The book is available for free at the authors’ website. Machine learning is being used as a commodity tool in research and commercial applications. There are few resources out there that fully cover all the important aspects of implementing machine learning. We hope this book will help people who want to apply machine learning without reading up on years’ worth of calculus, linear algebra, and probability theory. The book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning and its                applications. Chapter 2 describes the setup we will be using throughout the book. Chapter 3 describes the tools and techniques we will use to implement machine learning in the real world. Chapter 4 explains how to use machine learning to analyze data. Ensembles of Decision Trees are methods that combine multiple machine learning models to create more powerful models. As each feature is processed separately, and the possible splits of the data don’tdepend on scaling, no preprocessing like normalization or standardization of features is needed for decision tree algorithms. The main downside of decision trees is that even with the use of pre-pruning, they. tend to overfit and provide poor generalization performance. In most applications, the ensemble methods we discuss next are usually used in place of a sin‐giangle decision tree A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. The idea behind random forests is that each tree might do a relativelygood job of predicting, but will likely overfit on part of the data. If we build many decision trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. The results of this approach can be seen in the following image. The image is of a random forest with a gradient boos‐phthalted decision tree. The gradient boo­phthalted tree is a tree with a slightly different shape than the rest of the tree. To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other. This reduction in overfitting, while retaining the predictive power",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-10-subsection-2",
                            "title": "Gradient Boosting",
                            "content": "ing completely ignored some of the features.\nAs both gradient boosting and random forests perform well on similar kinds of data,\na common approach is to first try random forests, which work quite robustly. If ran‐\ndom forests work well but prediction time is at a premium, or it is important to\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\ning to gradient boosting often helps.\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\nlooking into the xgboost package and its Python interface, which at the time of writ‐\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\ngradient boosting on many datasets.\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\nmost powerful and widely used models for supervised learning. Their main drawback\nis that they require careful tuning of the parameters and may take a long time to\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\nand on a mixture of binary and continuous features. As with other tree-based models,\nit also often does not work well on high-dimensional sparse data.\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\nmators, and the learning_rate, which controls the degree to which each tree is\nallowed to correct the mistakes of the previous trees. These two parameters are highly\nSupervised Machine Learning Algorithms \n| \n91\ninterconnected, as a lower learning_rate means that more trees are needed to build\na model of similar complexity. In contrast to random forests, where a higher n_esti\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\n# all will be split in a consistent manner\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n# build the gradient boosting model\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train_named)\nUncertainty Estimates from Classifiers \n| \n119\nThe Decision Function\nIn the binary classification case, the return value of decision_function is of shape\n(n_samples,), and it returns one floating-point number for each sample:\nIn[106]:\nprint(\"X_test.shape: {}\".format(X_test.shape))\nprint(\"Decision function shape: {}\".format(\n    gbrt.decision_function(X_test).shape))\nOut[106]:\nX_test.shape: (25, 2)\nDecision function shape: (25,)\nThis value encodes how strongly the model believes a data point to belong to the\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\ntive class, and negative values indicate a preference for the “negative” (other) class:\nIn[107]:\n# show the first few entries of decision_function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))\nOut[107]:\nDecision function:\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\nWe can recover the prediction by looking only at the sign of the decision function:\nIn[108]:\nprint(\"Thresholded decision function:\\n{}\".format(\n    gbrt.decision_function(X_test) > 0))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[108]:\nThresholded decision function:\n[ True False False False  True  True False  True  True  True False  True\n  True False  True False False False  True  True  True  True  True False\n  False]\nPredictions:\n['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\nFor binary classification, the “negative” class is always the first entry of the classes_\nattribute, and the “positive” class is the second entry of classes_. So if you want to\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\nboosted models, often not deeper than five splits.\nKernelized Support Vector Machines\nThe next type of supervised model we will discuss is kernelized support vector\nmachines. We explored the use of linear support vector machines for classification in\n“Linear models for classification” on page 56. Kernelized support vector machines\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\nport vector machines for classification and regression, we will restrict ourselves to the\nclassification case, as implemented in SVC. Similar concepts apply to support vector\nregression, as implemented in SVR.\nThe math behind kernelized support vector machines is a bit involved, and is beyond\nthe scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, and\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\nsense of the idea behind the method.\nLinear models and nonlinear features\nAs you saw in Figure 2-15, linear models can be quite limiting in low-dimensional\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\nmodel more flexible is by adding more features—for example, by adding interactions\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\npredictions on part of the data, and so more and more trees are added to iteratively\nimprove performance.\nGradient boosted trees are frequently the winning entries in machine learning com‐\npetitions, and are widely used in industry. They are generally a bit more sensitive to\nparameter settings than random forests, but can provide better accuracy if the param‐\neters are set correctly.\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\ntant parameter of gradient boosting is the learning_rate, which controls how\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\nrate means each tree can make stronger corrections, allowing for more complex mod‐\nels. Adding more trees to the ensemble, which can be accomplished by increasing\nn_estimators, also increases the model complexity, as the model has more chances\nto correct mistakes on the training set.\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\nIn[72]:\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[72]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.958\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\nlower the learning rate:\nSupervised Machine Learning Algorithms \n| \n89\nIn[73]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))",
                            "summary": " gradient boosting and random forests perform well on similar kinds of data. Gradient boosted decision trees are among the most powerful and widely used models for supervised learning. Their main drawback is that they require careful tuning of the parameters and may take a long time to train. If you want to apply gradient boosting to a large-scale problem, it might be worth looking into the xgboost package and its Python interface, which is faster (and sometimes easier to tune) than the scikit-learn implementation of gradient boosting on many datasets. If prediction time is at a premium, or it is important tosqueeze out the last percentage of accuracy from the machine learning model, moving to gradient boosting often helps. The algorithm works well without scaling and on a mixture of binary and continuous features. As with other tree-based models, the algorithm often does not work well on high-dimensional sparse data. The main parameters of gradient boosted tree models are the number of trees, n_esti                mators, and the learning_rate, which controls the degree to which each tree is allowed to correct the mistakes of the previous trees. These two parameters are highly                Supervised Machine Learning Algorithms  interconnected, as a lowerlearning_rate means that more trees are needed to build a model of similar complexity. The decision_function is of shape of n_samples, and it returns one floating-point number for each sample. This value encodes how strongly the model believes a data point to belong to the “positive” class, in this case class 1. A common practice is to fitimizen_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. The decision function is called gbrt.decision_function(X_test).shape. It returns a shape of 25, which is the shape of a For binary classification, the “negative” class is always the first entry of the classes_attribute. The “positive’ class is the second entry of classes. Positive values indicate a preference for the posi‐ grotesquetive class, and negative values indicate the ‘negative’ (other) class. We can recover the prediction by looking only at the sign of the decision function. For example, we can see the first few entries of decision_function in the following example. The prediction is based on the thresholded decision function, which has a value of 4.2. We see the prediction in the next example, with the threshold level at 3.6. An important parameter is max_depth (or alternatively max_leaf_nodes), to reduce the complexity of each tree. Usually max_ depth is set very low for gradientboosted models, often not deeper than five splits. A common practice is to fit n_estimators depending on the time and memory budget, and then search over dif‐ferent learning_rates. We explored the use of linear support vector machines for classification in “Linear models for classification” on page 56 of the book. The next type of supervised model we will discuss is kernelized support vectormachines. We will discuss kernelized Support Vector Machines in the next section. The math behind kernelized support vector machines is a bit involved, and is beyond the scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, andFriedman’s The Elements of Statistical Learning. However, we will try to give you some sense of the idea behind the method. We will restrict ourselves to the classification case, as implemented in SVC. Similar concepts apply to support vectorregression, as implement in SVR. We hope this will help you understand some of the One way to make a linear model more flexible is by adding more features. More and more trees are added to iterativelyimprove performance. Gradient boosted trees are frequently the winning entries in machine learning competitions. They are generally a bit more sensitive toparameter settings than random forests. But they can provide better accuracy if the param‐ worrisomeeters are set correctly. The learning_rate controls how strongly each tree tries to correct the mistakes of the previous trees. The number of trees in the ensemble also controls how quickly each tree corrects the previous tree’s mistakes. It is possible to add interactions between features and polynomials of the input features to improve the performance of the model. Using GradientBoostingClassifier on the Breast Cancerdataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used. A higher learning rate means each tree can make stronger corrections. Adding more trees to the ensemble also increases the model complexity, as the model has more chances to correct mistakes on the training set. The training set accuracy is 100%, we are likely to be overfitting.   The test set is 0.958, and the test set's accuracy is 1,000, so we don't need to overfit. We could either apply stronger pre-pruning by limiting the maximum depth or lower the learning rate. To reduce overfit, we could use the GradientBoostingClassifier.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-10-subsection-3",
                            "title": "Ensemble Methods",
                            "content": "data. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other",
                            "summary": "Ensembles of Decision Trees are methods that combine multiple machine learning models to create more powerful models. Decision trees work well when you have features that are on completely different scales, or a mix of binary and con‐tinuous features. The main downside of decision trees is that even with the use of pre-pruning, they. tend to overfit and provide poor generalization performance. In most applications, the ensemble methods we discuss next are usually used in place of a sin‐ensiblygle decision tree. We will discuss how to use these ensemble methods in the next section A main drawback of decision trees is that they tend to overfit the training data. Random forests are one way to address this problem. The idea behind random forests is that each tree might do a relativelygood job of predicting, but will likely overfit on part of the data. If we build many decision trees, all of which work well and overfit in different ways, we can reduce the amount of overfitting by averaging their results. The results of this approach can be seen in the following image. The image is of a random forest with a gradient boos‐phthalted decision tree. The gradient boo­phthalted tree is a tree with a slightly different shape than the rest of the tree. To implement this strategy, we need to build many decision trees. Each tree should do an acceptable job of predicting the target, and should also be different from the other. This reduction in overfitting, while retaining the predictive power",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-10-subsection-4",
                            "title": "Performance Optimization",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-11",
                    "title": "Kernelized Support Vector Machines",
                    "content": "dimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nX, y = mglearn.tools.make_handcrafted_dataset()\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n# plot support vectors\nsv = svm.support_vectors_\n# class labels of support vectors are given by the sign of the dual coefficients\nsv_labels = svm.dual_coef_.ravel() > 0\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n98 \n| \nChapter 2: Supervised Learning\nFigure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\nwhich we will now discuss in detail.\nTuning SVM parameters\nThe gamma parameter is the one shown in the formula given in the previous section,\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\nmeans for points to be close together. The C parameter is a regularization parameter,\nsimilar to that used in the linear models. It limits the importance of each point (or\nmore precisely, their dual_coef_).\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\nIn[82]:\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\nfor ax, C in zip(axes, [-1, 0, 3]):\n    for a, gamma in zip(ax, range(-1, 2)):\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n                  ncol=4, loc=(.9, 1.2))\nSupervised Machine Learning Algorithms \n| \n99\nFigure 2-42. Decision boundaries and support vectors for different settings of the param‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\nthe most commonly used ones.\nGrid Search\nNow that we know how to evaluate how well a model generalizes, we can take the\nnext step and improve the model’s generalization performance by tuning its parame‐\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\nin Chapters 2 and 3, and it is important to understand what the parameters mean\nbefore trying to adjust them. Finding the values of the important parameters of a\nmodel (the ones that provide the best generalization performance) is a tricky task, but\nnecessary for almost all models and datasets. Because it is such a common task, there\nare standard methods in scikit-learn to help you with it. The most commonly used\nmethod is grid search, which basically means trying all possible combinations of the\nparameters of interest.\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\nsame for gamma. Because we have six different settings for C and gamma that we want to\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\n260 \n| \nChapter 5: Model Evaluation and Improvement\nC = 0.001\nC = 0.01\n… C = 10\ngamma=0.001\nSVC(C=0.001, gamma=0.001)\nSVC(C=0.01, gamma=0.001)\n… SVC(C=10, gamma=0.001)\ngamma=0.01\nSVC(C=0.001, gamma=0.01)\nSVC(C=0.01, gamma=0.01)\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n# naive grid search implementation\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {}   size of test set: {}\".format(\n      X_train.shape[0], X_test.shape[0]))\nbest_score = 0\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters, train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_test, y_test)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_parameters))\nOut[18]:\nSize of training set: 112   size of test set: 38\nBest score: 0.97\nBest parameters: {'C': 100, 'gamma': 0.001}\nThe Danger of Overfitting the Parameters and the Validation Set\nGiven this result, we might be tempted to report that we found a model that performs\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\njust wrong), for the following reason: we tried many different parameters and\nGrid Search \n| \n261\nselected the one with best accuracy on the test set, but this accuracy won’t necessarily\ncarry over to new data. Because we used the test data to adjust the parameters, we can\nno longer use it to assess how good the model is. This is the same reason we needed\nto split the data into training and test sets in the first place; we need an independent\n2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\nSupervised Machine Learning Algorithms \n| \n97\n11 This follows from the Taylor expansion of the exponential map.\nit considers all possible polynomials of all degrees, but the importance of the features\ndecreases for higher degrees.11\nIn practice, the mathematical details behind the kernel SVM are not that important,\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\nquite easily—we’ll do so in the next section.\nUnderstanding SVMs\nDuring training, the SVM learns how important each of the training data points is to\nrepresent the decision boundary between the two classes. Typically only a subset of\nthe training points matter for defining the decision boundary: the ones that lie on the\nborder between the classes. These are called support vectors and give the support vec‐\ntor machine its name.\nTo make a prediction for a new point, the distance to each of the support vectors is\nmeasured. A classification decision is made based on the distances to the support vec‐\ntor, and the importance of the support vectors that was learned during training\n(stored in the dual_coef_ attribute of SVC).\nThe distance between data points is measured by the Gaussian kernel:\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\nis a parameter that controls the width of the Gaussian kernel.\nFigure 2-41 shows the result of training a support vector machine on a two-\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nrange_on_training = (X_train - min_on_training).max(axis=0)\n# subtract the min, and divide by range\n# afterward, min=0 and max=1 for each feature\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\nprint(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))\n102 \n| \nChapter 2: Supervised Learning\nOut[85]:\nMinimum for each feature\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\nMaximum for each feature\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\nIn[86]:\n# use THE SAME transformation on the test set,\n# using min and range of the training set (see Chapter 3 for details)\nX_test_scaled = (X_test - min_on_training) / range_on_training\nIn[87]:\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[87]:\nAccuracy on training set: 0.948\nAccuracy on test set: 0.951\nScaling the data made a huge difference! Now we are actually in an underfitting\nregime, where training and test set performance are quite similar but less close to\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\nplex model. For example:\nIn[88]:\nsvc = SVC(C=1000)\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[88]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\naccuracy.\nSupervised Machine Learning Algorithms \n| \n103\nStrengths, weaknesses, and parameters\nsvm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_valid, y_valid)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n262 \n| \nChapter 5: Model Evaluation and Improvement\n# rebuild a model on the combined training and validation set,\n# and evaluate it on the test set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\ntest_score = svm.score(X_test, y_test)\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\nprint(\"Best parameters: \", best_parameters)\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\nOut[20]:\nSize of training set: 84   size of validation set: 28   size of test set: 38\nBest score on validation set: 0.96\nBest parameters:  {'C': 10, 'gamma': 0.001}\nTest set score with best parameters: 0.92\nThe best score on the validation set is 96%: slightly lower than before, probably\nbecause we used less data to train the model (X_train is smaller now because we split\nour dataset twice). However, the score on the test set—the score that actually tells us\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\ndata 92% correctly, not 97% correctly as we thought before!\nThe distinction between the training set, validation set, and test set is fundamentally\nimportant to applying machine learning methods in practice. Any choices made\nbased on the test set accuracy “leak” information from the test set into the model.\nTherefore, it is important to keep a separate test set, which is only used for the final\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐",
                    "summary": "The decision boundary is shown in black, and the sup‐reprehensiveport vectors are larger points with the wide outline. The SVM yields a very smooth and nonlinear (not a straight line) bound. The decision boundary and support vectors found by an SVM with RBF kernel are shown in Figure 2-41. The following code creates a plot by training an S VM on the forge dataset. The code is based on the sklearn.tools.make_handcrafted_dataset() and the forge. dimensional two-class dataset. In the next section, we will look at how this code can be used to train an S The gamma parameter controls the width of the Gaussian kernel. It determines the scale of what it means for points to be close together. The C parameter is a regularization parameter, similar to that used in the linear models. It limits the importance of each point (or more precisely, their dual_coef_).Let’s have a look at what happens when we vary these parameters (Figure 2-42): Figure 2- 42. Supervised Machine Learning Algorithms                 |                 99                99        ‘SVM’ is the name of the algorithm we are using to train the SVM. The SVM is a type of supervised machine learning. SVM will learn an SVM on the scaled training data. Decision boundaries and support vectors for different settings of the param will be used. We will use GridSearchCV to find better parameters for SVC using GridSearch CV, as dis‐cussed in Chapter 5. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, The standard KFold and StratifiedKFold are by far the most commonly used. For each split in the cross-validation, some part of the original training set will be declared the training part. The test part is used to measure what new data will look like to a model trained on the training set. We used the information from the entire training set to find the right scaling of the data. However, we already used the infor‐ipientmation contained in the test part. of the split, when scaling theData. Remember thatthe test part in each split is part ofthe training set, and weused the information. from the whole training set for this example. Finding the values of the important parameters of a model is a tricky task. Because it is such a common task, there are standard methods in scikit-learn to help you with it. The most commonly used method is grid search, which basically means trying all possible combinations of the parameters of interest. It is important to understand what the parameters mean before trying to adjust them. For example, the kernel bandwidth, gamma, and the regularization parameter, C, are important. The parameters are used in the SVC class of the scik it-learn algorithm. The kernel bandwidth and gamma are important for generalization. The regularizationparameters are also important for the generalization of the model. \"We have 36 combinations of parameters in total. Because we have six different settings for C and gamma We can implement a simple grid search just as for loops over the two parameters, evaluating a classifier for each combination. Looking at all possible combina­tions creates a table (or grid) of parameter settings for the SVM. We might be tempted to report that we found a model that performs.with 97% accuracy on our dataset. The Danger of Overfitting the Parameter Set and the Validation Set. The Model Evaluation and Improvement section of the book includes the model evaluation and improvement section. The book also includes the training and evaluation section, the test set section, and the validation set section. For more information on the book, visit: http://www.sklearn.com/svm/book/SVC We tried many different parameters and selected the one with best accuracy on the test set. This accuracy won’t necessarilycarry over to new data. Because we used the test data to adjust the parameters, we can't use it to assess how good the model is. This is the same reason we needed to split the data into training and test sets in the first place. We need an independent independent feature space. The radial basis function (RBF) kernel, also known as theGaussian kernel, is a bit harder to explain, as it corresponds to an infinite-dimensional featureSpace. During training, the SVM learns how important each of the training data points is to represent the decision boundary between the two classes. One way to explain the Gaussian kernel is that that it considers all possible polynomials of all degrees, but the importance of the features progressivelydecreases for higher degrees. In practice, the mathematical details behind the kernel SVM are not that important, though, and how an SVM with an RBF kernel makes a decision can be summarizedquite easily—we’ll do so in the next section. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. Figure 2-41 shows the result of training a support vector machine on a two-dimensional two-class dataset. The decision boundary is shown in black, and the sup‐port vectors are larger points with the wide outline. A classification decision is made based on the distances to the support vectors. The importance of the support vector that was learned during training is stored in the dual_coef_ attribute of SVC. The distance between data points is measured by the Gaussian kernel:                krbf(x1, x2) = exp (°x1 The main downside of the use of cross-validation is the time it takes to train all these models. To evaluate the accuracy of the SVM using a particular setting of C and gamma, we need to train 36 * 5 = 180 models. The following code creates a plot by training an SVM on the forge dataset. For each parameter setting (only a subset is shown), five accuracy values are compu‐phthalted, one for each split in the cross- validation. Then the mean validation accuracy iscomputed for eachparameter setting. The visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding code. For more information, visit sklearn.com. The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a The GridSearchCV class can be used to search over parameters. To use the class, you first need to specify theparameters you want to search. GridSearch CV will then per‐range_on_training = (X_train - min_on-training).max(axis=0) For each feature, min=0 and max=1 for each feature. For more information on how to use this class, visit the Grid searchCV website. The class can also be used as part of the Supervised Learning toolkit, which is available on the MIT Open Learning Project website and on the Google Play Store. The training and test set performance are quite similar but less close to 100% accuracy. Now we are actually in an underfitting regime. From here, we can try increasing either C or gamma to fit a more com‐ hypertrophicplex model.  1.] Use THE SAME transformation on the test set, and using min and range of the training set (see Chapter 3 for details) 2.] X_test_scaled = (X_test - min_on_training) / range_ on_training. 3 Supervised Machine Learning Algorithms                   103                Strengths, weaknesses, and parameters                svm = SVC(gamma=gamma, C=C) svm.fit(X_train, y_train) best_parameters = {'C': C, 'gamma': gamma} best_score = score.best_score (best_ score, C, gamma) best parameters = 'C': 10, 'Gamma': 0.001' best score on validation set: 0.96Best parameters: 'C' (C, gamma), 'Test set' (test_score), 'Best parameters' (worst_score, test_score) The distinction between the training set, validation set, and test set is fundamentallyimportant to applying machine learning methods in practice. It is good practice to do all exploratory analysis and model selection using the combination of a training and a validation set and reserve the test set for a final evaluation. Strictly speaking, evaluat‐.  “leak’ information from the testSet into the model’s model” is not a good idea, but it can be used in certain situations to test the accuracy of a machine learning algorithm. The test set score is 92%, not 97%, so we can only claim to classify new data",
                    "children": [
                        {
                            "id": "chapter-2-section-11-subsection-1",
                            "title": "Kernel Concept",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-11-subsection-2",
                            "title": "SVM Parameters",
                            "content": "dimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nX, y = mglearn.tools.make_handcrafted_dataset()\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n# plot support vectors\nsv = svm.support_vectors_\n# class labels of support vectors are given by the sign of the dual coefficients\nsv_labels = svm.dual_coef_.ravel() > 0\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n98 \n| \nChapter 2: Supervised Learning\nFigure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\nwhich we will now discuss in detail.\nTuning SVM parameters\nThe gamma parameter is the one shown in the formula given in the previous section,\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\nmeans for points to be close together. The C parameter is a regularization parameter,\nsimilar to that used in the linear models. It limits the importance of each point (or\nmore precisely, their dual_coef_).\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\nIn[82]:\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\nfor ax, C in zip(axes, [-1, 0, 3]):\n    for a, gamma in zip(ax, range(-1, 2)):\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n                  ncol=4, loc=(.9, 1.2))\nSupervised Machine Learning Algorithms \n| \n99\nFigure 2-42. Decision boundaries and support vectors for different settings of the param‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\nthe most commonly used ones.\nGrid Search\nNow that we know how to evaluate how well a model generalizes, we can take the\nnext step and improve the model’s generalization performance by tuning its parame‐\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\nin Chapters 2 and 3, and it is important to understand what the parameters mean\nbefore trying to adjust them. Finding the values of the important parameters of a\nmodel (the ones that provide the best generalization performance) is a tricky task, but\nnecessary for almost all models and datasets. Because it is such a common task, there\nare standard methods in scikit-learn to help you with it. The most commonly used\nmethod is grid search, which basically means trying all possible combinations of the\nparameters of interest.\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\nsame for gamma. Because we have six different settings for C and gamma that we want to\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\n260 \n| \nChapter 5: Model Evaluation and Improvement\nC = 0.001\nC = 0.01\n… C = 10\ngamma=0.001\nSVC(C=0.001, gamma=0.001)\nSVC(C=0.01, gamma=0.001)\n… SVC(C=10, gamma=0.001)\ngamma=0.01\nSVC(C=0.001, gamma=0.01)\nSVC(C=0.01, gamma=0.01)\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n# naive grid search implementation\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {}   size of test set: {}\".format(\n      X_train.shape[0], X_test.shape[0]))\nbest_score = 0\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters, train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_test, y_test)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_parameters))\nOut[18]:\nSize of training set: 112   size of test set: 38\nBest score: 0.97\nBest parameters: {'C': 100, 'gamma': 0.001}\nThe Danger of Overfitting the Parameters and the Validation Set\nGiven this result, we might be tempted to report that we found a model that performs\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\njust wrong), for the following reason: we tried many different parameters and\nGrid Search \n| \n261\nselected the one with best accuracy on the test set, but this accuracy won’t necessarily\ncarry over to new data. Because we used the test data to adjust the parameters, we can\nno longer use it to assess how good the model is. This is the same reason we needed\nto split the data into training and test sets in the first place; we need an independent\n2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\nSupervised Machine Learning Algorithms \n| \n97\n11 This follows from the Taylor expansion of the exponential map.\nit considers all possible polynomials of all degrees, but the importance of the features\ndecreases for higher degrees.11\nIn practice, the mathematical details behind the kernel SVM are not that important,\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\nquite easily—we’ll do so in the next section.\nUnderstanding SVMs\nDuring training, the SVM learns how important each of the training data points is to\nrepresent the decision boundary between the two classes. Typically only a subset of\nthe training points matter for defining the decision boundary: the ones that lie on the\nborder between the classes. These are called support vectors and give the support vec‐\ntor machine its name.\nTo make a prediction for a new point, the distance to each of the support vectors is\nmeasured. A classification decision is made based on the distances to the support vec‐\ntor, and the importance of the support vectors that was learned during training\n(stored in the dual_coef_ attribute of SVC).\nThe distance between data points is measured by the Gaussian kernel:\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\nis a parameter that controls the width of the Gaussian kernel.\nFigure 2-41 shows the result of training a support vector machine on a two-\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nrange_on_training = (X_train - min_on_training).max(axis=0)\n# subtract the min, and divide by range\n# afterward, min=0 and max=1 for each feature\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\nprint(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))\n102 \n| \nChapter 2: Supervised Learning\nOut[85]:\nMinimum for each feature\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\nMaximum for each feature\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\nIn[86]:\n# use THE SAME transformation on the test set,\n# using min and range of the training set (see Chapter 3 for details)\nX_test_scaled = (X_test - min_on_training) / range_on_training\nIn[87]:\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[87]:\nAccuracy on training set: 0.948\nAccuracy on test set: 0.951\nScaling the data made a huge difference! Now we are actually in an underfitting\nregime, where training and test set performance are quite similar but less close to\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\nplex model. For example:\nIn[88]:\nsvc = SVC(C=1000)\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[88]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\naccuracy.\nSupervised Machine Learning Algorithms \n| \n103\nStrengths, weaknesses, and parameters\nsvm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_valid, y_valid)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n262 \n| \nChapter 5: Model Evaluation and Improvement\n# rebuild a model on the combined training and validation set,\n# and evaluate it on the test set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\ntest_score = svm.score(X_test, y_test)\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\nprint(\"Best parameters: \", best_parameters)\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\nOut[20]:\nSize of training set: 84   size of validation set: 28   size of test set: 38\nBest score on validation set: 0.96\nBest parameters:  {'C': 10, 'gamma': 0.001}\nTest set score with best parameters: 0.92\nThe best score on the validation set is 96%: slightly lower than before, probably\nbecause we used less data to train the model (X_train is smaller now because we split\nour dataset twice). However, the score on the test set—the score that actually tells us\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\ndata 92% correctly, not 97% correctly as we thought before!\nThe distinction between the training set, validation set, and test set is fundamentally\nimportant to applying machine learning methods in practice. Any choices made\nbased on the test set accuracy “leak” information from the test set into the model.\nTherefore, it is important to keep a separate test set, which is only used for the final\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐",
                            "summary": "The decision boundary is shown in black, and the sup‐reprehensiveport vectors are larger points with the wide outline. The SVM yields a very smooth and nonlinear (not a straight line) bound. The decision boundary and support vectors found by an SVM with RBF kernel are shown in Figure 2-41. The following code creates a plot by training an S VM on the forge dataset. The code is based on the sklearn.tools.make_handcrafted_dataset() and the forge. dimensional two-class dataset. In the next section, we will look at how this code can be used to train an S The gamma parameter controls the width of the Gaussian kernel. It determines the scale of what it means for points to be close together. The C parameter is a regularization parameter, similar to that used in the linear models. It limits the importance of each point (or more precisely, their dual_coef_).Let’s have a look at what happens when we vary these parameters (Figure 2-42): Figure 2- 42. Supervised Machine Learning Algorithms                 |                 99                99        ‘SVM’ is the name of the algorithm we are using to train the SVM. The SVM is a type of supervised machine learning. SVM will learn an SVM on the scaled training data. Decision boundaries and support vectors for different settings of the param will be used. We will use GridSearchCV to find better parameters for SVC using GridSearch CV, as dis‐cussed in Chapter 5. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, The standard KFold and StratifiedKFold are by far the most commonly used. For each split in the cross-validation, some part of the original training set will be declared the training part. The test part is used to measure what new data will look like to a model trained on the training set. We used the information from the entire training set to find the right scaling of the data. However, we already used the infor‐ipientmation contained in the test part. of the split, when scaling theData. Remember thatthe test part in each split is part ofthe training set, and weused the information. from the whole training set for this example. Finding the values of the important parameters of a model is a tricky task. Because it is such a common task, there are standard methods in scikit-learn to help you with it. The most commonly used method is grid search, which basically means trying all possible combinations of the parameters of interest. It is important to understand what the parameters mean before trying to adjust them. For example, the kernel bandwidth, gamma, and the regularization parameter, C, are important. The parameters are used in the SVC class of the scik it-learn algorithm. The kernel bandwidth and gamma are important for generalization. The regularizationparameters are also important for the generalization of the model. \"We have 36 combinations of parameters in total. Because we have six different settings for C and gamma We can implement a simple grid search just as for loops over the two parameters, evaluating a classifier for each combination. Looking at all possible combina­tions creates a table (or grid) of parameter settings for the SVM. We might be tempted to report that we found a model that performs.with 97% accuracy on our dataset. The Danger of Overfitting the Parameter Set and the Validation Set. The Model Evaluation and Improvement section of the book includes the model evaluation and improvement section. The book also includes the training and evaluation section, the test set section, and the validation set section. For more information on the book, visit: http://www.sklearn.com/svm/book/SVC We tried many different parameters and selected the one with best accuracy on the test set. This accuracy won’t necessarilycarry over to new data. Because we used the test data to adjust the parameters, we can't use it to assess how good the model is. This is the same reason we needed to split the data into training and test sets in the first place. We need an independent independent feature space. The radial basis function (RBF) kernel, also known as theGaussian kernel, is a bit harder to explain, as it corresponds to an infinite-dimensional featureSpace. During training, the SVM learns how important each of the training data points is to represent the decision boundary between the two classes. One way to explain the Gaussian kernel is that that it considers all possible polynomials of all degrees, but the importance of the features progressivelydecreases for higher degrees. In practice, the mathematical details behind the kernel SVM are not that important, though, and how an SVM with an RBF kernel makes a decision can be summarizedquite easily—we’ll do so in the next section. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. Figure 2-41 shows the result of training a support vector machine on a two-dimensional two-class dataset. The decision boundary is shown in black, and the sup‐port vectors are larger points with the wide outline. A classification decision is made based on the distances to the support vectors. The importance of the support vector that was learned during training is stored in the dual_coef_ attribute of SVC. The distance between data points is measured by the Gaussian kernel:                krbf(x1, x2) = exp (°x1 The main downside of the use of cross-validation is the time it takes to train all these models. To evaluate the accuracy of the SVM using a particular setting of C and gamma, we need to train 36 * 5 = 180 models. The following code creates a plot by training an SVM on the forge dataset. For each parameter setting (only a subset is shown), five accuracy values are compu‐phthalted, one for each split in the cross- validation. Then the mean validation accuracy iscomputed for eachparameter setting. The visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding code. For more information, visit sklearn.com. The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a The GridSearchCV class can be used to search over parameters. To use the class, you first need to specify theparameters you want to search. GridSearch CV will then per‐range_on_training = (X_train - min_on-training).max(axis=0) For each feature, min=0 and max=1 for each feature. For more information on how to use this class, visit the Grid searchCV website. The class can also be used as part of the Supervised Learning toolkit, which is available on the MIT Open Learning Project website and on the Google Play Store. The training and test set performance are quite similar but less close to 100% accuracy. Now we are actually in an underfitting regime. From here, we can try increasing either C or gamma to fit a more com‐ hypertrophicplex model.  1.] Use THE SAME transformation on the test set, and using min and range of the training set (see Chapter 3 for details) 2.] X_test_scaled = (X_test - min_on_training) / range_ on_training. 3 Supervised Machine Learning Algorithms                   103                Strengths, weaknesses, and parameters                svm = SVC(gamma=gamma, C=C) svm.fit(X_train, y_train) best_parameters = {'C': C, 'gamma': gamma} best_score = score.best_score (best_ score, C, gamma) best parameters = 'C': 10, 'Gamma': 0.001' best score on validation set: 0.96Best parameters: 'C' (C, gamma), 'Test set' (test_score), 'Best parameters' (worst_score, test_score) The distinction between the training set, validation set, and test set is fundamentallyimportant to applying machine learning methods in practice. It is good practice to do all exploratory analysis and model selection using the combination of a training and a validation set and reserve the test set for a final evaluation. Strictly speaking, evaluat‐.  “leak’ information from the testSet into the model’s model” is not a good idea, but it can be used in certain situations to test the accuracy of a machine learning algorithm. The test set score is 92%, not 97%, so we can only claim to classify new data",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-11-subsection-3",
                            "title": "Kernel Selection",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-11-subsection-4",
                            "title": "Implementation Details",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-12",
                    "title": "Neural Networks (Deep Learning)",
                    "content": "sian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nplt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\n106 \n| \nChapter 2: Supervised Learning\nFigure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\ntion function\nFor the small neural network pictured in Figure 2-45, the full formula for computing\nŷ in the case of regression would be (when using a tanh nonlinearity):\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\nHere, w are the weights between the input x and the hidden layer h, and v are the\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\ncomputations. An important parameter that needs to be set by the user is the number\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\nsets and as big as 10,000 for very complex data. It is also possible to add additional\nhidden layers, as shown in Figure 2-47:\nSupervised Machine Learning Algorithms \n| \n107\nIn[92]:\nmglearn.plots.plot_two_hidden_layer_graph()\nFigure 2-47. A multilayer perceptron with two hidden layers\nHaving large neural networks made up of many of these layers of computation is\nwhat inspired the term “deep learning.”\nTuning neural networks\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\ntwo_moons dataset we used earlier in this chapter. The results are shown in\nFigure 2-48:\nIn[93]:\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)",
                    "summary": "The family of algorithms known as neural networks has recently seen a revival under the name “deep learning’ Gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Good settings for the twoparameters are usually strongly correlated, and C and gamma should be adjusted together. Here, we will only discuss some relatively simple methods, namelymultilayer perceptrons for classification and regression, that can serve as a starting point for more involved deep learning methods. sian kernel. Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neuralnetworks. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.Remember that the prediction by a linear regressor is given as: ŷ is a weighted sum of the input features x[0] to x[p], weighted by the learned coefficients w[0) to w[p]. We could visualize this graphically as shown in Figure 2-46. Visualization of logistic regression, where input features and predictions are shown as nodes, and the coefficients are connections between the nodes. In an MLP this process of computing weighted sums is repeated multiple times, first by computing hidden units that represent an intermediate processing step, which are then used to compute the final sum of all the input features. The result is a weighted sum of the inputs and the hidden units. For the small neural network pictured in Figure 2-45, the full formula for computing                ŷ in the case of regression would be (when using a tanh nonlinearity):. The weights v and w are learned from data, x are the input features, ŷ is the computed output, and h are intermediatecomputations. An important parameter that needs to be set by the user is the number of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐sets and as big as 10,000 for very complex data. The hyperbolic tangent activation function and the rectified linear activa‐paralleledtion function are used in the network. Having large neural networks made up of many of these layers of computation is what inspired the term “deep learning.” The MLP classifier can be used to tune neural networks. The results are shown in Figure 2-48 and 2-49. We have also shown how to add additional hidden layers to a neural network. We will end the chapter with a look at the results of the MLP on the two-moons dataset we used earlier in this chapter.",
                    "children": [
                        {
                            "id": "chapter-2-section-12-subsection-1",
                            "title": "Neural Network Basics",
                            "content": "sian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nplt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\n106 \n| \nChapter 2: Supervised Learning\nFigure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\ntion function\nFor the small neural network pictured in Figure 2-45, the full formula for computing\nŷ in the case of regression would be (when using a tanh nonlinearity):\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\nHere, w are the weights between the input x and the hidden layer h, and v are the\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\ncomputations. An important parameter that needs to be set by the user is the number\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\nsets and as big as 10,000 for very complex data. It is also possible to add additional\nhidden layers, as shown in Figure 2-47:\nSupervised Machine Learning Algorithms \n| \n107\nIn[92]:\nmglearn.plots.plot_two_hidden_layer_graph()\nFigure 2-47. A multilayer perceptron with two hidden layers\nHaving large neural networks made up of many of these layers of computation is\nwhat inspired the term “deep learning.”\nTuning neural networks\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\ntwo_moons dataset we used earlier in this chapter. The results are shown in\nFigure 2-48:\nIn[93]:\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)",
                            "summary": "The family of algorithms known as neural networks has recently seen a revival under the name “deep learning’ Gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Good settings for the twoparameters are usually strongly correlated, and C and gamma should be adjusted together. Here, we will only discuss some relatively simple methods, namelymultilayer perceptrons for classification and regression, that can serve as a starting point for more involved deep learning methods. sian kernel. Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neuralnetworks. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.Remember that the prediction by a linear regressor is given as: ŷ is a weighted sum of the input features x[0] to x[p], weighted by the learned coefficients w[0) to w[p]. We could visualize this graphically as shown in Figure 2-46. Visualization of logistic regression, where input features and predictions are shown as nodes, and the coefficients are connections between the nodes. In an MLP this process of computing weighted sums is repeated multiple times, first by computing hidden units that represent an intermediate processing step, which are then used to compute the final sum of all the input features. The result is a weighted sum of the inputs and the hidden units. For the small neural network pictured in Figure 2-45, the full formula for computing                ŷ in the case of regression would be (when using a tanh nonlinearity):. The weights v and w are learned from data, x are the input features, ŷ is the computed output, and h are intermediatecomputations. An important parameter that needs to be set by the user is the number of nodes in the hidden layer. This can be as small as 10 for very small or simple data‐sets and as big as 10,000 for very complex data. The hyperbolic tangent activation function and the rectified linear activa‐paralleledtion function are used in the network. Having large neural networks made up of many of these layers of computation is what inspired the term “deep learning.” The MLP classifier can be used to tune neural networks. The results are shown in Figure 2-48 and 2-49. We have also shown how to add additional hidden layers to a neural network. We will end the chapter with a look at the results of the MLP on the two-moons dataset we used earlier in this chapter.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-12-subsection-2",
                            "title": "Architectures",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-12-subsection-3",
                            "title": "Training Process",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-12-subsection-4",
                            "title": "Optimization Techniques",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-13",
                    "title": "Uncertainty Estimates from Classifiers",
                    "content": "markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\nUncertainty Estimates from Classifiers \n| \n123\nFigure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\ning model shown in Figure 2-55\nThe boundaries in this plot are much more well-defined, and the small areas of\nuncertainty are clearly visible.\nThe scikit-learn website has a great comparison of many models and what their\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\nage you to go though the example there.\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\ncourtesy http://scikit-learn.org)\nUncertainty in Multiclass Classification\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\ndecision_function and predict_proba methods also work in the multiclass setting.\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\n124 \n| \nChapter 2: Supervised Learning\nIn[115]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\ngbrt.fit(X_train, y_train)\nIn[116]:\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n# plot the first few entries of the decision function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\n13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\never, if that happens, the prediction is made at random.\nOut[113]:\nPredicted probabilities:\n[[ 0.016  0.984]\n [ 0.843  0.157]\n [ 0.981  0.019]\n [ 0.974  0.026]\n [ 0.014  0.986]\n [ 0.025  0.975]]\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\nbe above 50% certainty. That class is the one that is predicted.13\nYou can see in the previous output that the classifier is relatively certain for most\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\nthe model and the parameters. A model that is more overfitted tends to make more\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\nally has more uncertainty in its predictions. A model is called calibrated if the\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\ndiction made with 70% certainty would be correct 70% of the time.\nIn the following example (Figure 2-56) we again show the decision boundary on the\ndataset, next to the class probabilities for the class 1:\nIn[114]:\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nmglearn.tools.plot_2d_separator(\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common\ntechnique to extend a binary classification algorithm to a multiclass classification\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\nlearned for each class that tries to separate that class from all of the other classes,\nresulting in as many binary models as there are classes. To make a prediction, all\nbinary classifiers are run on a test point. The classifier that has the highest score on its\nsingle class “wins,” and this class label is returned as the prediction.\nSupervised Machine Learning Algorithms \n| \n63\nHaving one binary classifier per class results in having one vector of coefficients (w)\nand one intercept (b) for each class. The class for which the result of the classification\nconfidence formula given here is highest is the assigned class label:\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\nand the same method of making a prediction is applied.\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\nuse a two-dimensional dataset, where each class is given by data sampled from a\nGaussian distribution (see Figure 2-19):\nIn[47]:\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\nFigure 2-19. Two-dimensional toy dataset containing three classes\n64 \n| \nChapter 2: Supervised Learning\nNow, we train a LinearSVC classifier on the dataset:\nIn[48]:\nlinear_svm = LinearSVC().fit(X, y)\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\nn_classes) and each column provides a “certainty score” for each class, where a large\nscore means that a class is more likely and a small score means the class is less likely.\nYou can recover the predictions from these scores by finding the maximum entry for\neach data point:\nIn[117]:\nprint(\"Argmax of decision function:\\n{}\".format(\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[117]:\nArgmax of decision function:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\nthe probabilities for the possible classes for each data point sum to 1:\nUncertainty Estimates from Classifiers \n| \n125\nIn[118]:\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n# show that sums across rows are one\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\nOut[118]:\nPredicted probabilities:\n[[ 0.107  0.784  0.109]\n [ 0.789  0.106  0.105]\n [ 0.102  0.108  0.789]\n [ 0.107  0.784  0.109]\n [ 0.108  0.663  0.228]\n [ 0.789  0.106  0.105]]\nSums: [ 1.  1.  1.  1.  1.  1.]\nWe can again recover the predictions by computing the argmax of predict_proba:\nIn[119]:\nprint(\"Argmax of predicted probabilities:\\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[119]:\nArgmax of predicted probabilities:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to",
                    "summary": "The boundaries in this plot are much more well-defined, and the small areas ofuncertainty are clearly visible. The scikit-learn website has a great comparison of many models and what their uncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encourage you to go through the example there. We also show how the uncertainty estimates in binary classification can be compared to those in multiclass classification. We conclude by showing how we can use this information to improve the accuracy of our training data. We hope you’ll find this information useful. The decision_function and predict_proba methods also work in the multiclass setting. Let’s apply them on the Iris dataset, which is a three-class classification dataset. Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. The decision function has the shape (n_samples, n_probability) of a circle. We can plot the first few entries of the decision function to get the shape of the function. We’ll also use these methods to predict the outcome of a test. Predicted probabilities: 0.016  0.984, 0.843  0,157. The classifier is relatively certain for most points. How well the uncertainty actually reflects uncertainty in the data depends on the model and the parameters. A model that is more overfitted tends to make morecertain predictions, even if they might be wrong. The model with less complexity has more uncertainty in its predictions. The prediction is made at random if the probabilities for the two classes sum to 1, and exactly one of the classes will be predicted with above 50% certainty. A model is called calibrated if thereported uncertainty actually matches how correct it is. In a calibrated model, a pre‐paralleleddiction made with 70% certainty would be correct 70% of the time. In the following example we again show the decision boundary on the dataset. Next to the class probabilities for the class 1, we show the training and test points. We then plot the training points next to the test points in Figure 2-56. The training points are shown in the figure's right-hand corner. The test points are seen in the left-hand side of the figure. Binary classification is arguably the most common and conceptually simple applica‐tion of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. We will also look at the impact of choosing one model over another and how to use these metrics in the real world. We hope this article has helped you better understand machine learning and its potential applications to your business. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available on Amazon Kindle and the Mac. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. What does being 85% accurate mean on this dataset? In general, multiclass classifica­tion results are harder to understand than binary classification results. Common tools are the confusion matrix and the classification report we saw in the binary case in the previous section. For any point in the dataset, 10% of The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. Each row of the confusion matrix corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 5-18. A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs-rest approach. For the binary case, each row corresponds to the true label and the column to the predictedlabel. For multiclass case, the rows and columns are the same, but the labels are different in the multiclass class. For more information, see the Multiclass Classification Al Binary classifiers are run on a test point. The classifier that has the highest score “wins” The result of the classifier is then used to predict the outcome of the test. For example, the result of a test for a class is the result for the class for which the class is named. For a class that is unknown, the class will be called “unknown” by the binary classifier. For more information on the “one-to-one” method, see the ‘How to do it’ guide. The mathematics behind multiclass logistic regression differ somewhat from the one-vs.-rest approach, but they also result in one coefficient vector and intercept per class. The same method of making a prediction is applied. We use a two-dimensional dataset, where each class is given by data sampled from aGaussian distribution (see Figure 2-19). The results are published in the open-source version of the book, “Machine Learning for Dummies”. Two-dimensional toy dataset containing three classes. Each column provides a “certainty score” for each class. A large score means that a class is more likely and a small score means the class is less likely. The output of predict_proba has the same shape, (n_samples, n_classes) as the decision_function has the shape (n-1, n-2,n-3) We train a LinearSVC classifier on the dataset. We use the following code to train the classifier. The results are shown in the next section of the paper. The code is available on GitHub. It can be downloaded from the GitHub site here. The probabilities for the possible classes for each data point sum to 1 are shown in the following table. The probabilities for each class are given by the first few entries of predict_proba. For example, the probabilities are 0.107, 0.784 and 0.105 for the first and second classes respectively. Predictions: points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Any point in this area will be classified as class 0 by the final classifier. The class with the highest confidence formula is the class of the closest line. All three binary classifiers clas‐phthalsify points in the middle of the plot as ‘rest’ The classifier 0 is greater than zero, while it is smaller than zero for the other two classes. The result of the classifica­tion confidence formula for classifiers 0 and 1 is ‘0’. The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values foralpha or small values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to.",
                    "children": [
                        {
                            "id": "chapter-2-section-13-subsection-1",
                            "title": "The Decision Function",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-13-subsection-2",
                            "title": "Predicting Probabilities",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-13-subsection-3",
                            "title": "Uncertainty in Multiclass Classification",
                            "content": "markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\nUncertainty Estimates from Classifiers \n| \n123\nFigure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\ning model shown in Figure 2-55\nThe boundaries in this plot are much more well-defined, and the small areas of\nuncertainty are clearly visible.\nThe scikit-learn website has a great comparison of many models and what their\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\nage you to go though the example there.\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\ncourtesy http://scikit-learn.org)\nUncertainty in Multiclass Classification\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\ndecision_function and predict_proba methods also work in the multiclass setting.\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\n124 \n| \nChapter 2: Supervised Learning\nIn[115]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\ngbrt.fit(X_train, y_train)\nIn[116]:\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n# plot the first few entries of the decision function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\n13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\never, if that happens, the prediction is made at random.\nOut[113]:\nPredicted probabilities:\n[[ 0.016  0.984]\n [ 0.843  0.157]\n [ 0.981  0.019]\n [ 0.974  0.026]\n [ 0.014  0.986]\n [ 0.025  0.975]]\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\nbe above 50% certainty. That class is the one that is predicted.13\nYou can see in the previous output that the classifier is relatively certain for most\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\nthe model and the parameters. A model that is more overfitted tends to make more\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\nally has more uncertainty in its predictions. A model is called calibrated if the\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\ndiction made with 70% certainty would be correct 70% of the time.\nIn the following example (Figure 2-56) we again show the decision boundary on the\ndataset, next to the class probabilities for the class 1:\nIn[114]:\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nmglearn.tools.plot_2d_separator(\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common\ntechnique to extend a binary classification algorithm to a multiclass classification\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\nlearned for each class that tries to separate that class from all of the other classes,\nresulting in as many binary models as there are classes. To make a prediction, all\nbinary classifiers are run on a test point. The classifier that has the highest score on its\nsingle class “wins,” and this class label is returned as the prediction.\nSupervised Machine Learning Algorithms \n| \n63\nHaving one binary classifier per class results in having one vector of coefficients (w)\nand one intercept (b) for each class. The class for which the result of the classification\nconfidence formula given here is highest is the assigned class label:\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\nand the same method of making a prediction is applied.\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\nuse a two-dimensional dataset, where each class is given by data sampled from a\nGaussian distribution (see Figure 2-19):\nIn[47]:\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\nFigure 2-19. Two-dimensional toy dataset containing three classes\n64 \n| \nChapter 2: Supervised Learning\nNow, we train a LinearSVC classifier on the dataset:\nIn[48]:\nlinear_svm = LinearSVC().fit(X, y)\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\nn_classes) and each column provides a “certainty score” for each class, where a large\nscore means that a class is more likely and a small score means the class is less likely.\nYou can recover the predictions from these scores by finding the maximum entry for\neach data point:\nIn[117]:\nprint(\"Argmax of decision function:\\n{}\".format(\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[117]:\nArgmax of decision function:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\nthe probabilities for the possible classes for each data point sum to 1:\nUncertainty Estimates from Classifiers \n| \n125\nIn[118]:\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n# show that sums across rows are one\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\nOut[118]:\nPredicted probabilities:\n[[ 0.107  0.784  0.109]\n [ 0.789  0.106  0.105]\n [ 0.102  0.108  0.789]\n [ 0.107  0.784  0.109]\n [ 0.108  0.663  0.228]\n [ 0.789  0.106  0.105]]\nSums: [ 1.  1.  1.  1.  1.  1.]\nWe can again recover the predictions by computing the argmax of predict_proba:\nIn[119]:\nprint(\"Argmax of predicted probabilities:\\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[119]:\nArgmax of predicted probabilities:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to",
                            "summary": "The boundaries in this plot are much more well-defined, and the small areas ofuncertainty are clearly visible. The scikit-learn website has a great comparison of many models and what their uncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encourage you to go through the example there. We also show how the uncertainty estimates in binary classification can be compared to those in multiclass classification. We conclude by showing how we can use this information to improve the accuracy of our training data. We hope you’ll find this information useful. The decision_function and predict_proba methods also work in the multiclass setting. Let’s apply them on the Iris dataset, which is a three-class classification dataset. Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. The decision function has the shape (n_samples, n_probability) of a circle. We can plot the first few entries of the decision function to get the shape of the function. We’ll also use these methods to predict the outcome of a test. Predicted probabilities: 0.016  0.984, 0.843  0,157. The classifier is relatively certain for most points. How well the uncertainty actually reflects uncertainty in the data depends on the model and the parameters. A model that is more overfitted tends to make morecertain predictions, even if they might be wrong. The model with less complexity has more uncertainty in its predictions. The prediction is made at random if the probabilities for the two classes sum to 1, and exactly one of the classes will be predicted with above 50% certainty. A model is called calibrated if thereported uncertainty actually matches how correct it is. In a calibrated model, a pre‐paralleleddiction made with 70% certainty would be correct 70% of the time. In the following example we again show the decision boundary on the dataset. Next to the class probabilities for the class 1, we show the training and test points. We then plot the training points next to the test points in Figure 2-56. The training points are shown in the figure's right-hand corner. The test points are seen in the left-hand side of the figure. Binary classification is arguably the most common and conceptually simple applica‐tion of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. We will also look at the impact of choosing one model over another and how to use these metrics in the real world. We hope this article has helped you better understand machine learning and its potential applications to your business. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available on Amazon Kindle and the Mac. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. What does being 85% accurate mean on this dataset? In general, multiclass classifica­tion results are harder to understand than binary classification results. Common tools are the confusion matrix and the classification report we saw in the binary case in the previous section. For any point in the dataset, 10% of The model has an accuracy of 95.3%, which already tells us that we are doing pretty well. The confusion matrix provides us with some more detail. Let’s apply these two detailed evaluationmethods on the task of classifying the 10 different handwritten digits in the digits.dataset. The accuracy of the model is 0.953%, which is a pretty good result for a training set of 10 handwritten digits. We’re going to use this data to train a new training set. The training set will be called digits.data, and the confusion matrix will be known as digits.confusion. matrix. We will use the training set to train the new set of digits. Each row of the confusion matrix corresponds to a true label, and each column to a predicted label. You can find a visually more appealing plot in Figure 5-18. A common technique to extend a binary classification algorithm to a multiclass classification algorithm is the one-vs-rest approach. For the binary case, each row corresponds to the true label and the column to the predictedlabel. For multiclass case, the rows and columns are the same, but the labels are different in the multiclass class. For more information, see the Multiclass Classification Al Binary classifiers are run on a test point. The classifier that has the highest score “wins” The result of the classifier is then used to predict the outcome of the test. For example, the result of a test for a class is the result for the class for which the class is named. For a class that is unknown, the class will be called “unknown” by the binary classifier. For more information on the “one-to-one” method, see the ‘How to do it’ guide. The mathematics behind multiclass logistic regression differ somewhat from the one-vs.-rest approach, but they also result in one coefficient vector and intercept per class. The same method of making a prediction is applied. We use a two-dimensional dataset, where each class is given by data sampled from aGaussian distribution (see Figure 2-19). The results are published in the open-source version of the book, “Machine Learning for Dummies”. Two-dimensional toy dataset containing three classes. Each column provides a “certainty score” for each class. A large score means that a class is more likely and a small score means the class is less likely. The output of predict_proba has the same shape, (n_samples, n_classes) as the decision_function has the shape (n-1, n-2,n-3) We train a LinearSVC classifier on the dataset. We use the following code to train the classifier. The results are shown in the next section of the paper. The code is available on GitHub. It can be downloaded from the GitHub site here. The probabilities for the possible classes for each data point sum to 1 are shown in the following table. The probabilities for each class are given by the first few entries of predict_proba. For example, the probabilities are 0.107, 0.784 and 0.105 for the first and second classes respectively. Predictions: points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as “rest.” Any point in this area will be classified as class 0 by the final classifier. The class with the highest confidence formula is the class of the closest line. All three binary classifiers clas‐phthalsify points in the middle of the plot as ‘rest’ The classifier 0 is greater than zero, while it is smaller than zero for the other two classes. The result of the classifica­tion confidence formula for classifiers 0 and 1 is ‘0’. The main parameter of linear models is the regularization parameter, called alpha in the regression models and C in the linearSVC and LogisticRegression models. The main goal of supervised learning is to learn decision boundaries. Figure 2-21 shows the predictions for all regions of the 2Dspace. The decision boundaries are derived from the three one-vs-rest classifiers. For more information on supervised learning, see the Supervised Learning Handbook. The book is published by Oxford University Press and is available in hard copy for $66 (US) and $99 (UK) Large values foralpha or small values for C mean simple models. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-13-subsection-4",
                            "title": "Confidence Metrics",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-14",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-2-section-14-subsection-1",
                            "title": "Chapter Summary",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-14-subsection-2",
                            "title": "Key Learning Points",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-14-subsection-3",
                            "title": "Future Directions",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-3",
            "title": "3. Unsupervised Learning and Preprocessing",
            "content": "procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps. Dimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and information on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any separately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\ncalled transform on the training and test sets. You can see that the dataset in the sec‐\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\nthe features are between 0 and 1. You can also see that the minimum and maximum\nfeature values for the test data (the triangles) are not 0 and 1.\nThe third panel shows what would happen if we scaled the training set and test set\nseparately. In this case, the minimum and maximum feature values for both the train‐\ning and the test set are 0 and 1. But now the dataset looks different. The test points\nmoved incongruously to the training set, as they were scaled differently. We changed\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\ndo.\nAs another way to think about this, imagine your test set is a single point. There is no\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\nments of the MinMaxScaler. But the size of your test set should not change your\nprocessing.\nShortcuts and Efficient Alternatives\nOften, you want to fit a model on some dataset, and then transform it. This is a very\ncommon task, which can often be computed more efficiently than by simply calling\nfit and then transform. For this use case, all models that have a transform method\nalso have a fit_transform method. Here is an example using StandardScaler:\nIn[9]:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calling fit and transform in sequence (using method chaining)\nX_scaled = scaler.fit(X).transform(X)\n# same result, but more efficient computation\nX_scaled_d = scaler.fit_transform(X)\n# split it into training and test sets\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n# plot the training and test sets\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\n136 \n| \nChapter 3: Unsupervised Learning and Preprocessing\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"Original Data\")\n# scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# visualize the properly scaled data\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[1].set_title(\"Scaled Data\")\n# rescale the test set separately\n# so test set min is 0 and test set max is 1\n# DO NOT DO THIS! For illustration purposes only.\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n# visualize wrongly scaled data\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"training set\", s=60)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\naxes[2].set_title(\"Improperly Scaled Data\")\nfor ax in axes:\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\nseparately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nseparate training and test sets to evaluate the supervised model we will build after the\npreprocessing):\nIn[3]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nOut[3]:\n(426, 30)\n(143, 30)\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\nments. We split the dataset into 426 samples for the training set and 143 samples for\nthe test set.\nAs with the supervised models we built earlier, we first import the class that imple‐\nments the preprocessing, and then instantiate it:\nIn[4]:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n134 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nWe then fit the scaler using the fit method, applied to the training data. For the Min\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\nused:\nIn[5]:\nscaler.fit(X_train)\nOut[5]:\nMinMaxScaler(copy=True, feature_range=(0, 1))\nTo apply the transformation that we just learned—that is, to actually scale the training\ndata—we use the transform method of the scaler. The transform method is used in\nscikit-learn whenever a model returns a new representation of the data:\nIn[6]:\n# transform data\nX_train_scaled = scaler.transform(X_train)\n# print dataset properties before and after scaling\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\nprint(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\nprint(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\nprint(\"per-feature minimum after scaling:\\n {}\".format( You should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nhappen when building processing chains without the pipeline class (like forgetting\nto apply all transformers on the test set, or not applying them in the right order).\nChoosing the right combination of feature extraction, preprocessing, and models is\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\nlines, this “trying out” of many different processing steps is quite simple. When\n320 \n| \nChapter 6: Algorithm Chains and Pipelines\nexperimenting, be careful not to overcomplicate your processes, and make sure to\nevaluate whether every component you are including in your model is necessary.\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\nrithms provided by scikit-learn. You now possess all the required skills and know\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\nwe will dive in more detail into one particular type of data that is commonly seen in\npractice, and that requires some special expertise to handle correctly: text data.\nSummary and Outlook \n| \n321\nCHAPTER 7\nWorking with Text Data\nIn Chapter 4, we talked about two kinds of features that can represent properties of\nthe data: continuous features that describe a quantity, and categorical features that are\nitems from a fixed list. There is a third kind of feature that can be found in many\napplications, which is text. For example, if we want to classify an email message as\neither a legitimate email or spam, the content of the email will certainly contain\nimportant information for this classification task. Or maybe we want to learn about\nthe opinion of a politician on the topic of immigration. Here, that individual’s\nspeeches or tweets might provide useful information. In customer service, we often\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\nand content of a message to automatically determine the customer’s intent, which Principal Component Analysis (PCA)\nPrincipal component analysis is a method that rotates the dataset in a way such that\nthe rotated features are statistically uncorrelated. This rotation is often followed by\nselecting only a subset of the new features, according to how important they are for\nexplaining the data. The following example (Figure 3-3) illustrates the effect of PCA\non a synthetic two-dimensional dataset:\nIn[13]:\nmglearn.plots.plot_pca_illustration()\nThe first plot (top left) shows the original data points, colored to distinguish among\nthem. The algorithm proceeds by first finding the direction of maximum variance,\nlabeled “Component 1.” This is the direction (or vector) in the data that contains most\nof the information, or in other words, the direction along which the features are most\ncorrelated with each other. Then, the algorithm finds the direction that contains the\nmost information while being orthogonal (at a right angle) to the first direction. In\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\nhead and the tail are; we could have drawn the first component from the center up to\n140 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nthe top left instead of down to the bottom right. The directions found using this pro‐\ncess are called principal components, as they are the main directions of variance in the\ndata. In general, there are as many principal components as original features.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\ncancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nYou can see that in the first component, all features have the same sign (it’s negative,\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\nThat means that there is a general correlation between all features. As one measure‐\nment is high, the others are likely to be high as well. The second component has\nmixed signs, and both of the components involve all of the 30 features. This mixing of\nall features is what makes explaining the axes in Figure 3-6 so tricky.\nEigenfaces for feature extraction\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\nbehind feature extraction is that it is possible to find a representation of your data\nthat is better suited to analysis than the raw representation you were given. A great\nexample of an application where feature extraction is helpful is with images. Images\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\nObjects in images are usually made up of thousands of pixels, and only together are\nthey meaningful.\nWe will give a very simple application of feature extraction on images using PCA, by\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\ncontains face images of celebrities downloaded from the Internet, and it includes\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\nscale versions of these images, and scale them down for faster processing. You can see\nsome of the images in Figure 3-7:\nIn[21]:\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\ncombinations of the original features. However, these combinations are usually very\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\ncomponents_ attribute of the PCA object during fitting:\nIn[18]:\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\nOut[18]:\nPCA component shape: (2, 30)\nEach row in components_ corresponds to one principal component, and they are sor‐\nted by their importance (the first principal component comes first, etc.). The columns\ncorrespond to the original features attribute of the PCA in this example, “mean\nradius,” “mean texture,” and so on. Let’s have a look at the content of components_:\nIn[19]:\nprint(\"PCA components:\\n{}\".format(pca.components_))\nOut[19]:\nPCA components:\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\nWe can also visualize the coefficients using a heat map (Figure 3-6), which might be\neasier to understand:\nIn[20]:\nplt.matshow(pca.components_, cmap='viridis')\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),\n           cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nIn[28]:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\nOut[28]:\nTest set accuracy: 0.36\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\nintuition that the principal components might provide a better representation of the\ndata.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n151\nFor image data, we can also easily visualize the principal components that are found.\nRemember that components correspond to directions in the input space. The input\nspace here is 50×37-pixel grayscale images, so directions within this space are also\n50×37-pixel grayscale images.\nLet’s look at the first couple of principal components (Figure 3-9):\nIn[29]:\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\nOut[29]:\npca.components_.shape: (100, 5655)\nIn[30]:\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape),\n              cmap='viridis')\n    ax.set_title(\"{}. component\".format((i + 1)))\nWhile we certainly cannot understand all aspects of these components, we can guess\nwhich aspects of the face images some of the components are capturing. The first\ncomponent seems to mostly encode the contrast between the face and the back‐\nground, the second component encodes differences in lighting between the right and\nthe left half of the face, and so on. While this representation is slightly more semantic\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\n# keep the first two principal components of the data\npca = PCA(n_components=2)\n# fit PCA model to breast cancer data\npca.fit(X_scaled)\n# transform data onto the first two principal components\nX_pca = pca.transform(X_scaled)\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\n144 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nOut[16]:\nOriginal shape: (569, 30)\nReduced shape: (569, 2)\nWe can now plot the first two principal components (Figure 3-5):\nIn[17]:\n# plot first vs. second principal component, colored by class\nplt.figure(figsize=(8, 8))\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\nplt.legend(cancer.target_names, loc=\"best\")\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\nprincipal components\nIt is important to note that PCA is an unsupervised method, and does not use any class\ninformation when finding the rotation. It simply looks at the correlations in the data.\nFor the scatter plot shown here, we plotted the first principal component against the\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n145\nsecond principal component, and then used the class information to color the points.\nYou can see that the two classes separate quite well in this two-dimensional space.\nThis leads us to believe that even a linear classifier (that would learn a line in this\nspace) could do a reasonably good job at distinguishing the two classes. We can also\nsee that the malignant (red) points are more spread out than the benign (blue) points\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\nwhat a human would perceive first. When asking people to rate similarity of faces,\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\nkeep in mind that algorithms often interpret data (particularly visual data, such as\nimages, which humans are very familiar with) quite differently from how a human\nwould.\n152 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-9. Component vectors of the first 15 principal components of the faces dataset\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\nmation as rotating the data and then dropping the components with low variance.\nAnother useful interpretation is to try to find some numbers (the new feature values\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\nprincipal components (see Figure 3-10).\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\ncomponents\nHere, x0, x1, and so on are the coefficients of the principal components for this data\npoint; in other words, they are the representation of the image in the rotated space.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n153\nAnother way we can try to understand what a PCA model is doing is by looking at\nthe reconstructions of the original data using only some components. In Figure 3-3,\nafter dropping the second component and arriving at the third panel, we undid the\nrotation and added the mean back to obtain new points in the original space with the\nsecond component removed, as shown in the last panel. We can do a similar transfor‐ plt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\ndata.\nApplying NMF to synthetic data\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\nto be able to operate on the data. This means where the data lies relative to the origin\n(0, 0) actually matters for NMF. Therefore, you can think of the non-negative compo‐\nnents that are extracted as directions from (0, 0) toward the data.\nThe following example (Figure 3-13) shows the results of NMF on the two-\ndimensional toy data:\nIn[34]:\nmglearn.plots.plot_nmf_illustration()\n156 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-13. Components found by non-negative matrix factorization with two compo‐\nnents (left) and one component (right)\nFor NMF with two components, as shown on the left, it is clear that all points in the\ndata can be written as a positive combination of the two components. If there are\nenough components to perfectly reconstruct the data (as many components as there\nare features), the algorithm will choose directions that point toward the extremes of\nthe data.\nIf we only use a single component, NMF creates a component that points toward the\nmean, as pointing there best explains the data. You can see that in contrast with PCA,\nreducing the number of components not only removes some directions, but creates\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\nponents, where all the data can be explained perfectly, the randomness has little effect\n(though it might change the order or scale of the components). In more complex sit‐\nuations, there might be more drastic changes.\nApplying NMF to face images\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\nmain parameter of NMF is how many components we want to extract. Usually this is\nlower than the number of input features (otherwise, the data could be explained by\nmaking each pixel a separate component).\nFirst, let’s inspect how the number of components impacts how well the data can be\nreconstructed using NMF (Figure 3-14):\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n157\nIn[35]:\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\nFigure 3-14. Reconstructing three face images using increasing numbers of components\nfound by NMF\nThe quality of the back-transformed data is similar to when using PCA, but slightly\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\nfor finding interesting patterns within the data.\nAs a first look into the data, let’s try extracting only a few components (say, 15).\nFigure 3-15 shows the result:\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape))\n    ax.set_title(\"{}. component\".format(i))\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\nnents\nThese components are all positive, and so resemble prototypes of faces much more so\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\nthat component 3 shows a face rotated somewhat to the right, while component 7\nshows a face somewhat rotated to the left. Let’s look at the images for which these\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n159\nIn[37]:\ncompn = 3\n# sort by 3rd component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\ncompn = 7\n# sort by 7th component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\nFigure 3-16. Faces that have a large coefficient for component 3\n160 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-17. Faces that have a large coefficient for component 7 feature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\nimage for each class:\nIn[43]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\n                         subplot_kw={'xticks':(), 'yticks': ()})\nfor ax, img in zip(axes.ravel(), digits.images):\n    ax.imshow(img)\n164 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-20. Example images from the digits dataset\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\nprincipal components, and color each dot by its class (see Figure 3-21):\nIn[44]:\n# build a PCA model\npca = PCA(n_components=2)\npca.fit(digits.data)\n# transform the digits data onto the first two principal components\ndigits_pca = pca.transform(digits.data)\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\nThe Decision Function                                                                                              120\nPredicting Probabilities                                                                                             122\nUncertainty in Multiclass Classification                                                                 124\nSummary and Outlook                                                                                                 127\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\nTypes of Unsupervised Learning                                                                                 131\nChallenges in Unsupervised Learning                                                                        132\nPreprocessing and Scaling                                                                                            132\nDifferent Kinds of Preprocessing                                                                             133\nApplying Data Transformations                                                                               134\nScaling Training and Test Data the Same Way                                                       136\nThe Effect of Preprocessing on Supervised Learning                                           138\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\nPrincipal Component Analysis (PCA)                                                                    140\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\ncolor = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nThe digits zero, six, and four are relatively well separated using the first two principal\ncomponents, though they still overlap. Most of the other digits overlap significantly.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n165\nFigure 3-21. Scatter plot of the digits dataset using the first two principal components\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\nsupport transforming new data, the TSNE class has no transform method. Instead, we\ncan call the fit_transform method, which will build the model and immediately\nreturn the transformed data (see Figure 3-22):\nIn[45]:\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\n# use fit_transform instead of fit, as TSNE has no transform method\ndigits_tsne = tsne.fit_transform(digits.data)\n166 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[46]:\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"t-SNE feature 0\")\nplt.xlabel(\"t-SNE feature 1\")\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236 In[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nindicate cluster membership. We specified that we are looking for three clusters, so\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\nassigned to the cluster center it is closest to (see “Assign Points (1)”). Next, the cluster\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\n(1)”). Then the process is repeated two more times. After the third iteration, the\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\nGiven new data points, k-means will assign each to the closest cluster center. The next\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\nin Figure 3-23:\nIn[48]:\nmglearn.plots.plot_kmeans_boundaries()\nClustering \n| \n169\n3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\nvalue.\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\nclass, and set the number of clusters we are looking for.3 Then we call the fit method\nwith the data:\nIn[49]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n# generate synthetic two-dimensional data\nX, y = make_blobs(random_state=1)\n# build the clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\nfind these labels in the kmeans.labels_ attribute:\n170 \n|\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n(that is, we have 10 new features), with all features being 0, apart from the one that\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\near model, which would not have been possible using the original two features. It is\nalso possible to get an even more expressive representation of the data by using the\ndistances to each of the cluster centers as features. This can be accomplished using\nthe transform method of kmeans:\nIn[60]:\ndistance_features = kmeans.transform(X)\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\nprint(\"Distance features:\\n{}\".format(distance_features))\nOut[60]:\nDistance feature shape: (200, 10)\nDistance features:\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\n ...,\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\nk-means is a very popular algorithm for clustering, not only because it is relatively\neasy to understand and implement, but also because it runs relatively quickly. k-\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\nOne of the drawbacks of k-means is that it relies on a random initialization, which\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\nlearn runs the algorithm 10 times with 10 different random initializations, and\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nlier. You might remember that PCA tries to find directions of maximum variance in\nthe data, while NMF tries to find additive components, which often correspond to\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\nresent each data point using a cluster center. You can think of that as each point being\nrepresented using only a single component, which is given by the cluster center. This\nview of k-means as a decomposition method, where each point is represented using a\nsingle component, is called vector quantization.\n176 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nLet’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\nnents extracted (Figure 3-30), as well as reconstructions of faces from the test set\nusing 100 components (Figure 3-31). For k-means, the reconstruction is the closest\ncluster center found on the training set:\nIn[57]:\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\nnmf = NMF(n_components=100, random_state=0)\nnmf.fit(X_train)\npca = PCA(n_components=100, random_state=0)\npca.fit(X_train)\nkmeans = KMeans(n_clusters=100, random_state=0)\nkmeans.fit(X_train)\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\nIn[58]:\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"Extracted Components\")\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐ returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nAgglomerative clustering produces what is known as a hierarchical clustering. The\nclustering proceeds iteratively, and every point makes a journey from being a single\npoint cluster to belonging to some final cluster. Each intermediate step provides a\nclustering of the data (with a different number of clusters). It is sometimes helpful to\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\nhow each cluster breaks up into smaller clusters:\nIn[63]:\nmglearn.plots.plot_agglomerative()\n184 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\ntive clustering, with numbered data points (cf. Figure 3-36)\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\nrelies on the two-dimensional nature of the data and therefore cannot be used on\ndatasets that have more than two features. There is, however, another tool to visualize\nhierarchical clustering, called a dendrogram, that can handle multidimensional\ndatasets.\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\nrithms. SciPy provides a function that takes a data array X and computes a linkage\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\nIn[64]:\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n5 We could also use the labels_ attribute, as we did for k-means.\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\nclosest are merged. In the first four steps, two single-point clusters are picked and\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\ning. As we specified that we are looking for three clusters, the algorithm then stops.\nLet’s have a look at how agglomerative clustering performs on the simple three-\ncluster data we used here. Because of the way the algorithm works, agglomerative\nclustering cannot make predictions for new data points. Therefore, Agglomerative\nClustering has no predict method. To build the model and get the cluster member‐\nships on the training set, use the fit_predict method instead.5 The result is shown\nin Figure 3-34:\nIn[62]:\nfrom sklearn.cluster import AgglomerativeClustering\nX, y = make_blobs(random_state=1)\nagg = AgglomerativeClustering(n_clusters=3)\nassignment = agg.fit_predict(X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nClustering \n| \n183\nFigure 3-34. Cluster assignment using agglomerative clustering with three clusters\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that regions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\nIf there are at least min_samples many data points within a distance of eps to a given\ndata point, that data point is classified as a core sample. Core samples that are closer\nto each other than the distance eps are put into the same cluster by DBSCAN.\nThe algorithm works by picking an arbitrary point to start with. It then finds all\npoints with distance eps or less from that point. If there are less than min_samples\npoints within distance eps of the starting point, this point is labeled as noise, meaning\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\nThen, all neighbors (within eps) of the point are visited. If they have not been\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\ngrows until there are no more core samples within distance eps of the cluster. Then\nanother point that hasn’t yet been visited is picked, and the same procedure is\nrepeated.\nClustering \n| \n187\nIn the end, there are three kinds of points: core points, points that are within distance\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\nis run on a particular dataset multiple times, the clustering of the core points is always\nthe same, and the same points will always be labeled as noise. However, a boundary\npoint might be neighbor to core samples of more than one cluster. Therefore, the\ncluster membership of boundary points depends on the order in which points are vis‐\nited. Usually there are only few boundary points, and this slight dependence on the\norder of points is not important. returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# accuracy is zero, as none of the labels are the same\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# adjusted rand score is 1, as the clustering is exactly the same\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\nOut[69]:\nAccuracy: 0.00\nARI: 1.00\nEvaluating clustering without ground truth\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\nthere is a big problem with using measures like ARI. When applying clustering algo‐\nrithms, there is usually no ground truth to which to compare the results. If we knew\nthe right clustering of the data, we could use this information to build a supervised\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\nin developing algorithms, not in assessing success in an application.\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\nhouette coefficient. However, these often don’t work well in practice. The silhouette\nscore computes the compactness of a cluster, where higher is better, with a perfect\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\nshapes.\nHere is an example comparing the outcome of k-means, agglomerative clustering,\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\nIn[70]:\nfrom sklearn.metrics.cluster import silhouette_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nClustering \n| \n193\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n    cmap=mglearn.cm3, s=60)\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nnot implemented in scikit-learn at the time of writing.\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\nknow if there is any semantic meaning in the clustering, or whether the clustering\n194 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nreflects an aspect of the data that we are interested in. Let’s go back to the example of\nface images. We hope to find groups of similar faces—say, men and women, or old\npeople and young people, or people with beards and without. Let’s say we cluster the\ndata into two clusters, and all algorithms agree about which points should be clus‐\ntered together. We still don’t know if the clusters that are found correspond in any\nway to the concepts we are interested in. It could be that they found side views versus\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\ntures taken with iPhones versus pictures taken with Android phones. The only way to\nknow whether the clustering corresponds to anything we are interested in is to ana‐\nlyze the clusters manually.\nComparing algorithms on the faces dataset\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\nWe will use the eigenface representation of the data, as produced by\nPCA(whiten=True), with 100 components:\nIn[71]:\n# extract eigenfaces from lfw data and transform data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100, whiten=True, random_state=0)\npca.fit_transform(X_people)\nX_pca = pca.transform(X_people)\nWe saw earlier that this is a more semantic representation of the face images than the\nraw pixels. It will also make computation faster. A good exercise would be for you to\nrun the following experiments on the original data, without PCA, and see if you find\nsimilar clusters.\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\njust discussed: procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.",
            "summary": "We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k-Means allows for a char‐rouacterization of the clusters using the cluster means. DBS CAN allows for the detection of ‘noise points’ that are not assigned any cluster. agglomersative clustering allows you to specify the number of desired clusters, whileDBSCAN lets you define proximity using the eps parameter, which indirectly influences cluster size. all three methods can be used on large, real-world datasets, and allow for clustering into many clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data. dimensionality reduction is a common application for unsuper supervised transformations. Another application is finding the parts or compo­nents that “make up” the data, for example, topic extraction on collections of text documents. We will also look at how machine learning can be used to extract knowledge from this data. We hope this chapter will help you understand some of the ideas behind machine learning and its use in the real world. Back to the page you came from. Clustering algorithms partition data into distinct groups of similar items. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. The site doesn’t know how many different people appear in your photo collection. A sensibleapproach would be to extract all the faces and divide them into groups of faces that look similar.  The site might want to group together pictures that show the same person. However, the site doesn't know which pictures show whom, and it can't tell how many people are in Using the cluster labels to index another array. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms. The challenges in applying clustering algorithms is that it is very hard to assess how well an algorithm worked, and to compare outcomes between different algo‐rithms. There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. In Figure 3-39, we show what it looks like when we randomly assign points to two clusters. We also show how the data looks when we rescale the data to zero mean and unit variance. In the plot, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. The plot also shows how the points look when the points are randomly assigned to each cluster. The results are shown in the interactive version of this article. The full version is available at the bottom of the page. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐                rithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are. The longest branches are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the two remaining clusters into a single cluster again bridges a relatively large distance. DBSCAN (which stands for “density-based spatial clustering of applications with noise’) works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori, it can cap‐                ture clusters of complex shapes, and it can identify points that aren’t part of any cluster. But the same is not true for the next algorithm we will look at, which is called the ‘core point’ algorithm The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. The results are quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. PCA is often a good first approach for transforming your data so that you might be able to visualize it using a scatter plot. A particularly useful one is the t-SNE algorithm. It allows for much more complex map‐insuredpings, and often provide better visualizations. It is not to be confused with the much larger MNIST dataset. It can be used to create more complex maps, such as those of the Labeled Faces in the Wild dataset. For more information, see the Colorskit_learn user guide on independent component analysis (ICA), factor analysis (FA), and sparse coding (d The idea behind t-SNE is to find a two-dimensional representation of the data that preserves the distances between points as best as possible. It tries to preserve the information indicating which points are neighbors to each other. Manifold learning can be useful for explora‐tory data analysis, but is rarely used if the final goal is supervised learning. Some of these algorithms don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. The idea is to make points that are close in the original feature space closer, and those that are far apart in theoriginal feature space farther apart. We can use PCA for dimensionality reduction by retaining only some of the principalcomponents. This reduces the data from a two-dimensional dataset to a one- dimensional dataset. We might keep only the first principal component, as shown in the third panel in Figure 3-3. The correlation matrix of the data in this representation is zero except for the diagonal. For more information on PCA, visit PCA.org or go to the PCA website at PCA’s official website at: http://www.pca.org/pca-online/features/features-extraction-and-manifold-learning. One of the most common applications of PCA is visualizing high-dimensional data‐sets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more than two features. This transformation is sometimes used to remove noise effects from the data or to visualize what part of the information is retained using the principal components. We can undo the rotation and add the mean back to the data. This will result in the data shown in the last panel in Figure 3-3. These points are in the original fea‐ purposefullyture space, but we kept only the information contained in the first principal compo­nent. The data can be downloaded from the PCA website here. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data. Unsupervised transformations of a dataset are algorithms that create a new representa­tions of theData which might be easier for humans or other machine learning algo‐rithms to understand. A common application of un supervised transformations is dimensionality reduction. This takes a high-dimensional representation of the Data, consisting of many features, and finds a new way to represent this data that summarizes the essential characteristics with fewer features. An example of this is topic extraction on collections oftext documents. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. Clustering algorithms, on the other hand, partition data into distinct groups of similar items. For example, a social media site might want to group together pictures that show the same person. However, the site doesn’t know which pictures show whom, and it doesn't know how many different people appear in your collection. For visualization purposes, a common application for dimensionality reduction is reduction to two dimensions for visualization purposes. When we use only the first two principal components the whole data is just a big blob, with no separation of classes visible. This is not very surprising, given that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐                tures very rough characteristics of the faces. A sensibleapproach would be to extract all the faces and divide them into groups of faces that                look similar. Hopefully, these correspond to the same person, and the images can be                grouped together for you. For more information on unsupervised learning, see the Wikipedia article on Unsupervised Learning and the Wikipedia page on Manifold Learning and Manipulation. NMF can be used to decompose data into a non-negative weighted sum. It works similarly to PCA and can also be used fordimensionality reduction. NMF can identify the original components that make up the combined data. It is particularlyhelpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. The method can only be applied to data where each feature is non- negative, as the sum of non- Negative components cannotbecome negative. It can be also used to reduce the number of data points in a data set to a single point. For more information, visit the NMF website.  NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret can‐cellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and information on this topic. In this chapter we discussed cross-validation, grid search, and evaluation metrics, the cornerstones of evaluating and improving machine learning algorithms. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. Evaluation for regression can be done in similar detail as we did for classification. In most applications, using the default R2 used in the scoremethod of all regressors is enough. Sometimes business decisions are made on thebasis of mean squared error or mean absolute error, which might give incentive to tweak models using these metrics. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring or scoring metric. For example, Micro average f1 score: 0.953. Macro average f 1 score:0.954. Using Evaluation Metrics in Model Selection using GridSearchCV or cross_val_score. In general, though, we have found R2 to be a more                intuitive metric to evaluate regression models. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. The scoring argument that can be used in both GridSearch CV and cross_Val_score is called scikit-learn. The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐consuming \"roc_auc\" as the scoring parameter. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal. This closest metric should be used whenever possible for model evaluation andselection. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make is not a good measure of predictive performance, as it does not contain all the information we are interested in. For more information, see the Model Evaluation and Improvement section of the book. The training set and test set were scaled using the MinMaxScaler. The test data was moved incongruously to the training set. For any separately (right) the training and test data were scaled separately. The training set is shown as circles and the test set as triangles. The minimum and maximum feature values for both the train and test sets are 0 and 1. For the test data (the triangles) the minimum andmaximum feature values are not 0 or 1, but are between 1 and 0. The result is that the test and training data are scaled separately, and the training data is scaled differently, as they were scaled differently. The results are shown in the second and third panels. There is no way to scale a single point correctly. But the size of your test set should not change your processing. All models that have a transform method also have a fit_transform method. The MinMaxScaler can be used to fit a model on some dataset, and then transform it. This is a very common task, which can often be computed more efficiently than by simply calling fit and thentransform. For more information, visit the MinMax Scaler website or go to: www.minmaxscaler.org/minmax-scaler. Using StandardScaler, we can scale data using method chaining. We can plot the training and test sets using plt.subplots. Here is an example of a training set with the test set set to 1. For illustration purposes only, we use MinMaxScaler for the training set. The training set min is 0 and test set max is 1. The test set min and max are 0 and 1, respectively, for the test and training sets, respectively. For more information, visit sklearn.preprocessing.com/learn/supervised-learning-and-preprocessing/unsupervised learning and preprocessing/Unsupervised Learning and Preprocessing. Preprocessing and Scaling are often used in tandem with supervised learning algorithms. Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms.Often, this is a simple per-feature rescaling and shift of theData. Effect of scaling training and test data shown on the left together (center) and separately (right) The data in the first panel is an unscaled two-dimensional dataset, with the training set shown as triangles. The second panel is the same data, but the scale is The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two features. The first feature (the x-axis value) is between 10 and 15. The second feature(the y-axisvalue) is around 1 to 9. The following four plots show four different ways to transform the data that yield more standard ranges. The code for the plots can be found at the bottom of the page. For more information on how to use the plots, see the code at the end of this article. For further information, please visit the GitHub repository at: http://www.jupiter.com/jupiter-research/preprocessing-and-unsupervised-learning.html#preprocessing. The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler uses the median and quartiles,1 instead of mean and variance. This scaling does not ensure any particular minimum and maximumvalues for the features. It ensures statistical properties that guarantee that they are on the same scale. The dataset contains 569 data points, each represented by 30 measure­ments. This makes the RobustScaler ignore data points that are very very small. We use separate training and test sets to evaluate the supervised model we will build after the preprocessing. The supervised model is based on the load_breast_cancer We split the dataset into 426 samples for the training set and 143 for the test set. We then fit the scaler using the fit method, applied to the training data. In contrast to the classifiers and regressors of Chapter 2, the MinMaxScaler is only provided with the data (X_train) when fit is called, and y_train is not used. To apply the transformation that we just learned—that is, to actually scale the trainingData—we use the transform method of the Scaler. The MinMax Scaler is then used to scale the test data to a more realistic level. The training data is then scaled to a higher level using the scale method. The transform method is used when a model returns a new representation of the data. You should now be in a position where you have some idea of how to apply, tune, andanalyze the models we discussed here. In this chapter, we focused on the binary clas‐sification case, as this is usually easiest to understand. Most of the algorithms presen‐phthalted have classification and regression variants, however, and all of the classificationalgorithms support both binary and multiclass classification.    Playing around with the algorithms on different datasets will give you a better feel for how long they need to train, how easy it is to analyze the models, and how sensitive they are to the representation of the data. Try applying any of these algorithms to the built-in datasets in scikit-learn, like the boston_housing or diabetes datasets for regression, or the digits dataset for multiclass classification. Building a model that actually generalizes well to new data is a bit trickier than that. The second family of machine learning algorithms that we will discuss is unsupervised learning algorithms. Unsupervisedlearning subsumes all kinds of machinelearning where there is no known output, no teacher to instruct the learning algo. We will see how to properly adjust parameters and how to find good parameters automatically in Chapter 6. In the next chapter, we will dive in more detail into unsuper supervised learning and pre Unsupervised learning is a form of machine learning. The goal is to learn how to use data to make better decisions. This book is a collection of articles on the subject of unsupervised learning. It is intended to be a starting point for a discussion of how to apply this type of learning to other areas of the world. The book is written in the form of an open-source open source software called Caffeine for Machine Learning (C4ML) It is written to help people understand how C4ML works and how it can be applied to a variety of situations. It also provides a way to learn I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. Choosing the right combination of feature extraction, preprocessing, and models issomewhat of an art, and often requires some trial and error. This “trying out” of many different processing steps is quite simple using pipe‐like lines. And finally, thanks to DTS, for your everlasting and endless support. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. The second part will be published on December 6, 2013, and is available for download on the DTS website. Click here to read the second part of the article. The third and final installment will be released on In Chapter 6: Algorithm Chains and Pipelines, be careful not to overcomplicate your processes. In the next chapter, we will dive in more detail into one particular type of data that is commonly seen in practice and that requires some special expertise to handle correctly: text data. We have completed our survey of general-purpose tools and algo-rithms provided by scikit-learn. You now possess all the required skills and know the necessary mechanisms to apply machine learning in practice. The next chapter will focus on working with text data, which is a third kind of feature that can be found in many applications. We hope this chapter has given you a better understanding of how machine learning works. Principal component analysis is a method that rotates the dataset in a way such that the rotated features are statistically uncorrelated. This rotation is often followed byselecting only a subset of the new features, according to how important they are for explaining the data. Principal Component Analysis (PCA) can be used to classify email messages into legitimate or spam. It can also be used in customer service to find out if a message is a complaint or an inquiry. The subject line and content of a message can be analyzed to determine the customer’s intent, which PCA can then use to classify the message into a legitimate or a spam category. The algorithm proceeds by first finding the direction of maximum variance. This is the direction along which the features are most correlated with each other. Then, the algorithm finds the direction that contains the most information while being orthogonal (at a right angle) to the first direction. The following example (Figure 3-3) illustrates the effect of PCA on a synthetic two-dimensional dataset. The first plot (top left) shows the original data points, colored to distinguish among them In two dimensions, there is only one possible orientation that is at a right angle. In higher-dimensional spaces, there would be (infinitely) many orthogonal directions. The directions are called principal components, as they are the main directions of variance in the data. In general, there are as many principal components as original features. The second plot (top right) shows the same data, but now rotated so that the firstprincipal component aligns with the x-axis and the second principal component aligned with the y-axis. The first component is drawn from the center up to the top left instead of down to the bottom right of the plot. Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset. All features have the same sign (it’s negative, but it doesn’t matter which direction the arrow points in) The second component has mixed signs, and both of the components involve all of the 30 features. This mixing of features is what makes explaining the axes in Figure3-6 so tricky. Another application of PCA that we mentioned earlier is feature extraction. For feature extraction, use the feature extraction tool on the right side of the page. For more information on feature extraction and how to use it in your own data, see the ‘Feature Extraction’ section. Feature extraction is the process of finding a representation of your data that is better suited to analysis than the raw representation you were given. Imagesare made up of pixels, usually stored as red, green, and blue (RGB) intensities. We will give a very simple application of feature extraction on images using PCA, by working with face images from the Labeled Faces in the Wild dataset. This datasetcontains face images of celebrities downloaded from the Internet, and it includes faces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐scale versions of these images, and scale them down for faster processing.   We can use PCA for dimensionality reduction by retaining only some of the principal components. In the rotated representation found by PCA, the two axes are uncorrelated, meaning that the correlation matrix of the data in this representation is zero except for the diagonal diagonal. You can see                some of the images in Figure 3-7: worrisomeIn[21]: worrisomeIn the second plot, the data is rotated so that the first principal component aligns with the x-axis and the second principal component aligned with the y-axis. Before the rotation, the mean was subtracted from the data, so                that the transformed data In this example, we might keep only the first principal component, as shown in the third panel in Figure 3-3 (bottom left) This reduces the data from a two-dimensional to a one-dimensional dataset. Note, however, that instead of keeping only one of the original features, we found the most interesting direction and kept this direction. We can undo the rotation and add the mean back to the data. These points are in the original fea‐ipientture space, but we kept only the information contained in the first Principal Compo‐nent. This will resultin the data shown inthe last Applying PCA to the cancer dataset for visualization. One of the most common applications of PCA is visualizing high-dimensional data. The principal components correspond to directions in the original data, so they are combinations of the original features. However, these combinations are usually verycomplex, as we’ll see shortly. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data by showing us all the possible configurations. The two axes in the plot are often not very easy to interpret, but we can still get a good idea of what the data is about by looking at the histograms in Figure 3-4. It is possible to use PCA for data that has more The principal components themselves are stored in the                components_ attribute of the PCA object during fitting. Each row in components_ corresponds to one principal component, and they are sor‐                ted by their importance. The columns “meanradius,” ‘mean texture,’ and so on correspond to the original features attribute of We can also visualize the coefficients using a heat map (Figure 3-6), which might beeasier to understand. Let’s have a look at the content of components_: grotesqueIn[19]: grotesqueprint(\"PCA components:\\n{}\".format(pca.components_) grotesqueOut[19): grotesqueprint (\"PCA. components: 0.219  0.104 0.228, 0.221  0,143, 0,239, 0,.261, 0.,138,064) grotesque Out[19] : grotesque Our accuracy improved quite significantly, from 26.6% to 35.7%. The principal components might provide a better representation of the data. For image data, we can also easily visualize the principal components that are found.Remember that components correspond to directions in the input space. The heat map of the first two principal components on the Breast Cancer dataset can be seen here. Let’s look at the first couple of principal components (Figure 3-9) The input space here is 50×37-pixel grayscale images, so directions within this space are also 50x37- pixel images. The first component seems to mostly encode the contrast between the face and the back ground. The second component encodes differences in lighting between the right and the left half of the face, and so on. We certainly cannot understand all aspects of these components, but we can guess which aspects of face images some of the components are capturing. The PCA model is based on pixels, the alignment of the face (the position of eyes,chin, and nose) and the lighting both have a strong influence on how similar two images are in their pixel representation. While this representation is slightly more semantic than the raw pixel values, it is still quite far from how a human might perceive a face. We can now plot the first two principal components (Figure 3-5):. plot first vs. first. plot second vs. second. plot third vs. third. plot fourth vs. fourth. plot fifth vs. fifth. plot sixth vs. sixth. plot seventh vs. seventh. plot eighth vs. ninth. plot tenth vs. tenth Figure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first and second principal components. PCA is an unsupervised method, and does not use any class information when finding the rotation. It is important to note that PCA does not used any class The two classes separate quite well in this two-dimensional space. This leads us to believe that even a linear classifier could do a reasonably good job at distinguishing the two classes. We can also see that the malignant (red) points are more spread out than the benign (blue) points. The principal components correspond to directions in the original data, so they areabeled in Figure 3-12. A downside of PCA is that the two axes in the plot are often not very easy to interpret. It simply looks at the correlations in the data. It is possible to use PCA to learn a line in this space by looking at the correlation between two points in the same space. Unsupervised learning algorithm PCA can be used to extract useful features. It works similarly to PCA and can also be used for dimensionality reduction. We are trying to write each data point as aweighted sum of some components, as illustrated in Figure 3-10. Non-Negative Matrix Factorization (NMF) is another unsupervisedlearning algorithm that can be also used for feature extraction. The results of NMF can be seen in the next section of this article, which includes the results of Manifold Learning for the cancer dataset. For more information on NMF, see http://www.manifoldlearning.org/. NMF decomposes data into a non-negative weighted sum. It is particularly helpful for data that is created as the addition (or overlay) of several independent sources. In these situations, NMF can identify the original components that make up the combined data. NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret can‐ autoimmunecellation effects. The method can only be applied to data where each feature is non- negative, as a non-'negative sum' cannotbecome negative. The process of decomposing data is particularlyhelpful fordata that is added (or overlaid) of multiple independent sources, such as an audio track of multiple people speaking, As the PCA model is based on pixels, the alignment of the face (the position of eyes, and the lighting both have a strong influence on how similar two pixel representation are in their pixel representation) But alignment and lighting are probably not what a human would perceive first. When asking people to rate similarity of faces, they are more likely to use attributes like age, gender, facial expression, and hair style, which are hard to infer from the pixel intensities. It’s important tokeep in mind that algorithms often interpret data (particularly visual data, such as images, which humans are very familiar with) quite differently from how a humanwould. The eigenfaces in Figure 3-9, for example, contain both The PCA transfor‐                mation as rotating the data and then dropping the components with low variance. Another useful interpretation is to try to find some numbers (the new feature values) so that we can express the test points as a weighted sum of the principal components. Another way we can try to understand what a PCA model is doing is by looking at the reconstructions of the original data using only some components. The coefficients of these principal components are the representation of the image in the rotated space and are the key to understanding PCA. The PCA algorithm is based on the principle of Dimensionality Reduction, Feature Extraction, and Manifold Learning In Figure 3-3, we undid therotation and added the mean back to obtain new points in the original space with the second component removed. We can do a similar transfor‐ plt. plot of the faces dataset using the first two principal components. The whole data is just a big blob, with no separation of classes visible. This is not very surprising, given that even with 10 components, PCA only cap‐tures very rough characteristics of the Faces dataset. The results are consistent with unsupervised learning algorithm thataims to extract useful features. For the cancer dataset, we can use Non-Negative Matrix Factorization (NMF) and Manifold Learning. NMF can be used to decompose data into a non-negative weighted sum. It works similarly to PCA and can also be used fordimensionality reduction. NMF can identify the original components that make up the combined data. It is particularlyhelpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. The method can only be applied to data where each feature is non- negative, as the sum of non- Negative components cannotbecome negative. It can be also used to reduce the number of data points in a data set to a single point. For more information, visit the NMF website.  NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret effects. The eigenfaces in Figure 3-9, for example, contain both positive and negative parts. This means where the data lies relative to the origin(0, 0) actually matters for NMF. To apply NMF to synthetic data, we need to ensure that our data is positive for the NMFto be able to operate on the data. For example, the sign is actually ‘0’ rather than ‘1’, and this is because the data is ‘synthetic’ not ‘real’.  NMF creates a component that points toward the extremes of the data, as pointing there best explains the data. If there are enough components to perfectly reconstruct the data (as many components as there are features), the algorithm will choose directions that point toward the extreme. The following example shows the results of NMF on the two-dimensional toy data. The results are shown in Figure 3-13, which is an example of a non-negative matrix factorization with two compo'reporters and one component. For NMF with two components, as shown on the left, it is clear that all points in the data can be written as a positive combination of the two components. In contrast with PCA, reducing the number of components not only removes some directions, but creates an entirely different set of components. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component”: all components play an equal part. NMF uses a random initialization, which might lead to different results depending on the type of component you are trying to reduce. In the next section, we'll look at how to CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Click here for NMF uses a random initialization, which might lead to different results depending on the random seed. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component” NMF uses an entirely different set of components! All components play an equal part. NMF is based on the Haskell programming language. The main parameter of NMF is how many components we want to extract. Usually this islower than the number of input features. In more complex situations, there might be more drastic changes. The number of components impacts how well the data can be reconstructed using NMF (Figure 3-14):Dimensionality Reduction, Feature Extraction, and Manifold Learning. In[35]: atrophymglearn.plots.plot_nmf_faces(X_train, X_test, image_shape) Figure 3- 14. NMF reconstruction of the Labeled Faces in the Wild dataset. The data was taken from a set of images called ‘ The quality of the back-transformed data is similar to when using PCA, but slightly worse. This is expected, as PCA finds the optimum directions in terms of reconstruc  NMF is usually not used for its ability to reconstruct or encode data, but rather for finding interesting patterns within the data. As a first look into the data, let’s try extracting only a few components (say, 15). Figure 3-15 shows the result: grotesquely158, apologetically158, and properly158. Figure 3: Unsupervised Learning and Preprocessing is available at: http://www.sklearn.com/learn/features/unsupervised-learning-and-preprocessing.  NMF found the components found by NMF on the faces dataset when using 15 compo‐nents. These components are all positive, and so resemble prototypes of faces much more sothan the components shown for PCA in Figure 3-9. For example, one can clearly see that component 3 shows a face rotated somewhat Dimensionality Reduction, Feature Extraction, and Manifold Learning | 159In[37]: grotesquecompn = 3# sort by 3rd component, plot first 10 images. Let’s look at the images for which these four components are particularly strong. Figures 3-16 and 3-17: Dimensionsality Reduction and Feature Extracted. Figures 2-3: Manifolds, and Figures 4-5: Faces that have a large coefficient for component 3. Figures 6-7: Faces with large coefficients for component 4. T-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Faces that have a large coefficient for component 7 feature space farther apart are more likely to be faces that are neighbors to each other. We will use PCA to visualize the data reduced to two dimensions. Figure 3-20 shows an example image of a handwritten digit between 0 and 1 for each class. We use the t-Sne manifold learning algorithm on a dataset of handwritten dig‐centricits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐scale image of an 8-digit digit. In Figure 3-21, we build a PCA model and plot the first two principal components. We color each dot by its class and plot it as text instead of using a scatter plot. We actually used the true digit classes as glyphs, to show which class is where. The PCA method is often a good first approach for transforming your data. But the nature of the method (applying arotation and then dropping directions) limits its usefulness, as we saw with the scatter plot of the Labeled Faces in the Wild dataset. For more information on PCA, visit PCA.org. For other data decomposition methods, visit the PCA site or the Data Decomposition Wiki. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. Some of them, including t-SNE, com‐                pute a new representation of the training data, but don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. Manifoldlearning can be useful for explora‐                tory data analysis, but is rarely used if the final goal is supervised learning. A particularly useful one is the t- SNE algorithm, which allows for much more complex map-like visualizations of training data.  t-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The results are quite remarkable, and the results can be seen in the video below. The video shows the results of three different methods: Feature Extraction, Dimensionality Reduction, and Manifold Learning. The result is quite remarkable. The videos can be The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. 3. Unsupervised Learning and Preprocessing. 3.3. The Decision Function. 4. The Multiclass Classification. 5. The Classification of Classifiers. 6. The Classification of Classifications. 7. The classification of Classifieds. 8. The classification of classifiers. 9. The The Effect of Preprocessing on Supervised Learning and Scaling Training and Test Data the Same Way. Manifold Learning with t-SNE and Manifolds. The Effect of Data Transformations on Manifolding Learning. The Effects of Data Transformation on Super supervised Learning and the Effect ofData Transformations On Supervised learning. The effect of data transformations on the effect of training and test data the same way. The effects of data transformation on the impact of testing and training data. The impact of data transformations on training and testing data Let’s apply t-SNE to the same dataset, and compare the results. Most of the other digits overlap significantly. The TSNE class has no transform method. As t- SNE does not support transforming new data, we use the TSNE The fit_transform method will build the model and immediately return the transformed data. Instead of fit, we can call the fit_ transform method, which will call the model. We plot the digits as text instead of using scatter using plt. Figure 3-22 shows the plot of the digits with different colors and fontdicts. We also plot the numbers as text using the text-based text-blend tool, which is also used in the TensorFlow tool. The result of t-SNE is quite remarkable. Scatter plot of the digits dataset using two components found by t- The ones and nines are somewhat split up, but most of the classes form a single dense dense. All the classes are quite clearly separated. The students are divided into three groups: the ones, the nines, the ones and the ones plus the ones. The ones CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Click here for Categorical Variables     211     Categorically categorical variables can be written as one-hot-Encoding (Dummy Variables)   212 The k-means algorithm assigns data points to the closest cluster center. The process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops. We specified that we are looking for three clusters, sothe algorithm wasInitialized by declaring three data points randomly as cluster cen‐ters (see “Initialization”). Then the iterative algorithm starts, assigning each data point to the cluster center it is closest to. The cluster centers are updated to be the mean of the assigned points ( see “Recompute Centers(1)”) Applying k-means with scikit-learn is quite straightforward. Here, we apply it to the synthetic data that we used for the preceding plots. We instantiate the KMeans class and set the number of clusters we are looking for. The next example shows the boundaries of the cluster centers that were learned in Figure 3-23. If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this method, but it could be useful in certain situations. We call the fit method with the data: kmeans.fit(X) (Figure 3-24) K-means, DBSCAN, and agglomerative cluster‐fledgeding. All three methods can be used on large, real-world datasets. They are rel‐ purposefully easy to understand, and allow for clustering into many clusters. You can find these labels in the kmeans.labels_ attribute:                170  procedure, and often most helpful in the exploratory phase of data analysis. The algorithms have somewhat different strengths, but all have a way of controlling the granularity of clustering. The most common way to use them is to use the cluster means method, where each data point is represented by its cluster center. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible configurations of the data. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster, and it can help automatically determine the number of clusters. The algorithm can be applied for exploratory data analysis and preprocessing. It can also be used to create complex cluster shapes, as we saw in the two Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. These tools can be the only ways to make sense of data in the absence of supervision information. Having the right representa­gtion of the data is often crucial for supervised or unsupervised learning K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐forming on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms is one of the challenges in applying clustering algorithms. It is very hard to assess how well an algorithm worked, and to compare outcomes between There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. We create a random cluster assignment for reference. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows the results of the cluster clustering and random assignment experiments. We have 10 new features, with all features being 0, apart from the one thatrepresents the cluster center the point is assigned to. The results are plotted on a plot of axes, with the axes on the left and the right side of the plot. The plot shows how the cluster cluster looks when the points are randomly assigned to two different clusters. The cluster center is the center of the point that was randomly assigned. Using this 10-dimensional repre‐sentation, it would now be possible to separate the two half-moon shapes using a linear model, which would not have been possible using the original two features. It isalso possible to get an even more expressive representation of the k-means is a very popular algorithm for clustering. It is relatively easy to understand and implement. It runs relatively quickly. It can be used to create a simple clustering algorithm. It uses the transform method of kmeans. This can be accomplished using the following code:distance_features.transform(X) distance_features = distance.features.shape.print(\"Distance features:\\n{}\".format(distance_ features.shape)Distance features: 0.922 K-means is a clustering algorithm that can be used to solve problems. It has some downsides, such as relying on a random seed to start the algorithm. It can also be used for decomposition methods like PCA and NMF. The MiniBatchKMeans class can handle very large datasets, and scikit-learn even includes a more scalable Variant of k-Means for large datasets. The algorithm can be seen as a form of vector quantization, or seeing k- means as decomposition of a data set. It is not possible to use k-meants to solve complex problems in a real-world application, but it can be useful for training models. PCA and NMF try to find additive components, which often correspond to “extremes” or “parts” of the data. k-means, on the other hand, tries to represent each data point using a cluster center. You can think of that as each point being represented using only a single component, which is given by the cluster. This view of k-Means as a decomposition method, where each point is represented using asingle component, is called vector quantization. The results are shown in Figure 3-30, as well as reconstructions of faces from the test set using 100 components (Figure 3-31). The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. For k-means, the reconstruction is the closest cluster center found on the training set. The reconstruction is also the closest clustering center on the learning set for k-Means. The results are shown in the figure below, which includes a plot of the feature extractions. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up the data so that points within a single cluster are very similar and points in different clusters are different. The algorithm is one of the simplest and most commonly used clustering algo­rithms. The k-means algorithm tries to find cluster centers that are representative of certain regions of the data. It alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of all data points that are assigned to it. The algorithm is finished when the assignment of instances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]: purposefullymglearn.plots.plot_kmeans_al In[55]: # generate some random cluster data and transform the data to be stretched. # plot the cluster assignments and cluster centers. # Plot the data and cluster assignments into features and centers. Figure 3-28. Colors are shown as black, white, red, blue, and green. For more information, see the Wikipedia article on Supervised Learning and Preprocessing. K-means fails to identify nonspherical clusters. It also performs poorly if the clusters have more complex shapes. Here, we would hope that the clustering algorithm can discover the two half-moonshapes. The algorithm was used to generate synthetic two_moons data (with less noise this time) The results are shown in Figure 3-29 of Chapter 2 of the book, \"K-Means and the Two-Moons Algorithm,\" which is available on Amazon.com for $99.99. For confidential support, call the Samaritans on 08 K-means is a clustering algorithm that is similar to decomposition methods like PCA and NMF. The downsides of k-Means are the relatively restrictiveassumptions made on the shape of clusters, and the requirement to specify the num‐                ber of clusters you are looking for. Next, we will look at two more clustering algorithms that improve upon these proper‐                ties in some ways.Agglomerative Clustering refers to a collection of clustering. algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satis‐                �fied. There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. Agglomerative clustering produces what is known as a hierarchical clustering. Every point makes a journey from being a single point to belonging to some final cluster. Each intermediate step provides a different number of clusters for the final clustering of the data. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example), average or complete might work better. We will discuss next how to choose the right number for the algorithm to find clusters for a given data set. For more information, see Chapter 3: Unsupervised Learning and Preprocessing and Chapter 4: The Clustering Algorithm. Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐centricive clustering, with numbered data points (cf. Figure 3-36) The visualization provides a very detailed view of the hierarchical clustering. It is sometimes helpful to look at all possible clusterings jointly. The next example provides some insight into how each cluster breaks up into smaller clusters. The visualization cannot be used on datasets that have more than two features. There is another tool to visualize hierarchical clusters, called a dendrogram, that can handle multidimensionaldatasets. You can generate them easily using SciPy. SciPy provides a function that takes a data array X and computes a linkagearray, which encodes hierarchical cluster similarities. We can then feed this linkagearray into the scipy dendrogram function to plot the d endrogram. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐forming. All three methods can be used on large, real-world datasets, are rel‐phthalatively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths, but all have a way of controlling the granularity of clustering. The methods are often most helpful in the exploratory phase of data analysis. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster. Agglomerative clustering can provide a whole hierarchy of possible hierarchy of data. k-means allows for a char‐                acterization of the clusters using the cluster means. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. This chapter introduced a range of unsupervised learning algorithms that can be applied for exploratory data analysis and preprocessing. The next chapter will look at how these algorithms can be used in a variety of data analysis scenarios. The final chapter will focus on the use of these algorithms in Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. Agglomerative clustering iteratively joins the two closest clusters on a two-dimensional dataset. Preprocessing and decomposition methods play an important part in data prepa‐ curtailration. Even in a supervised setting, exploratory learning can be useful. The next section of the book will focus on unsupervised learning. The book is available in paperback and e-book versions. For more information, visit the book’s website or go to: http://www.k-means.com/book/unsupervised-learning.html. Let’s have a look at how agglomerative clustering performs on the simple three-cluster data we used here. In the first four steps, two single-point clusters are picked and these are joined into two-point clustering. In step 5, one of the two- point clusters is extended to a third point, and so on. As we specified that we are looking for three clusters, the algorithm then stops. Because of the way the algorithm In Figure 3-34, we use the fit_predict method to build the model and get the cluster member‐ships on the training set. The result is shown in Figure 3.5. We will discuss hierarchical clustering and dendrograms in the next section. We also discuss how to use the cluster labels to index another array. We conclude the article with an overview of the latest developments in supervised learning and preprocessing. We hope you will find this information helpful. Back to the page you came from.. The next section will focus on unsupervised Learning and Preprocessing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that there is no notion of cluster center in agglomerative clustering. We simply show the first couple of points in each cluster. Colors show a particular number of clusters that is a good fit. In[82] we show the number of points in each cluster to the left of the first image. Some of the clusters seem to have a semantic theme, many of them are too large to be actually homogeneous. We show the results of unsupervised learning and preprocessing in Figure 3-46. In[82) we show random images from the clusters generated by In. Each row corre‐porre‐sponds to one cluster; the number to the right lists number of images in that cluster. We also show the result of Unsupervised Learning and Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. To get more homogeneous clusters, we can run the algorithm again, this time with 40 clusters, and pick out some of the clusters that areparticularly interesting (Figure 3-47): In[86]: # extract clusters with ward agglomeratives clustering. We can compute the ARI to measure whether the two partitions of the data are similar. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have                little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to have little in common for k We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines DBSCAN tries to cluster all data points together. The algorithm works by picking an arbitrary point to start with. It then finds all points with distance eps or less from that point. If there are less than min_samples many data points within a distance of eps of the starting point, this point is labeled as noise, meaning that it doesn’t belong to any cluster. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that regions of data, separated by regions that are relatively empty. This is not surprising, given the results of DBSCAN, which tried to cluster the data. The DBSCAN clustering algorithm is used to identify core points and noise points. Core points are points that are within distance eps of core points (called boundary points) The same points will always be labeled as noise. However, a boundary point might be neighbor to core samples of more than one cluster. The clustering of the core points is always the same, and the same points are always labeled as Noise. The algorithm can be used to find core points in a large data set, and to identify noise points in large data sets. It can also be used for finding core samples in a larger data set. It is currently being tested on a large dataset with more than 100,000 points. K-means is a clustering algorithm based on the idea that each point is its own cluster. The algorithm merges the two most similar clusters until some stopping criterion is met. The downsides are the relatively restrictiveassumptions made on the shape of clusters, and the requirement to specify the num‐                ber of clusters you are looking for. Next, we will look at two more clustering algorithms that improve upon these proper‐                ties in some ways. The algorithms are called Agglomerative Clustering and Grouping Clustered Algorithm (ACCLA) and Grouped Clusters (GCHA) respectively. For more information on these algorithms, see www.ac There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐centricing on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. When applying clustering algo‐rithms, there is usually no ground truth to which to compare the results. If you want to evaluate clustering algorithms without ground truth, you can use the clustering algorithm in Chapter 3: Unsupervised Learning and Preprocessing. For more information on the algorithm, please visit: http://www.npr.org/ Using metrics like ARI and NMI usually only helps in developing algorithms, not in assessing success in an application. The silhouettescore computes the compactness of a cluster, where higher is better, with a perfect score of 1. If we knew the right clustering of the data, we could use this information to build a supervised model like a classifier.  compact clusters are good, compactness doesn’t allow for complexshapes. Here is an example comparing the outcome of k-means, agglomerative clustering, and DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40): In[70]: At the bottom of the page, please share your data analysis results with us. We would like to hear from you. If you have a data analysis problem, please send it to: jennifer.smith@mailonline.co.uk. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k- means allows for a char‐                acterization of the clusters using the cluster means. DBS CAN lets you define proximity using the eps parameter, which indirectly influences cluster size. The three methods can be used on large, real-world datasets, are rel‐                atively easy to understand, and allow for clustering into many clusters. The algorithms have somewhat different strengths, but all have similar strengths. The results of the study are published in the open- DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. There are metrics that can be used to assess the outcome of a clustering algorithmrelative to a ground truth clustering. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐ hypertative measure between 0 and 1. We will now compare them on some real-world datasets to see how well they work. The results will be presented in the next section of the book, ‘Unsupervised Learning and Preprocessing’. The book is published by Oxford University Press, London, UK, priced £16.99. The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to havelittle in common for k-Means. We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that scikit-learn was not implemented at the time of writing. This is not surprising, given the results of DBSCAN, which tried to cluster all points together. We still don’t know if the clusters that are found correspond in any way to the concepts we are interested in. We hope to find groups of similar faces—say, men and women, or old people and young people, or people with beards and without. The clustering does not reveal any semantic meaning, or whether the points should be clus‐tered together. Let’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to theLabeled Faces in the Wild dataset. The only way to know whether the clustering corresponds to anything we are interested in is to ana‐ishlyze the clusters manually. We will use the eigenface representation of the data, as produced by the PCA algorithm. This is a more semantic representation of face images than theraw pixels. It will also make computation faster. The results will be published in the next version of this blog post, which will be updated with the latest data from sklearn.com and the latest version of the pca algorithm, as well as the latest pca data. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k- means allows for a char‐                acterization of the clusters using the cluster means. DBS CAN lets you define proximity using the eps parameter, which indirectly influences cluster size. All 3 methods can be used on large, real-world datasets, are rel‐                ceratively easy to understand, and allow for clustering into many clusters. A good exercise would be for you to run the following experiments on the original data, without PCA, and see if you find                similar clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible hierarchies of data. DBSCANallows for the detection of “noise points” that are not assigned any cluster, and it canhelp automatically determine the number of clusters. The algorithm can be applied for exploratory data analysis and preprocessing. It can also be viewed as a decom‐otypeposition method, where each data point is represented by its cluster center. This chapter introduced a range of unsupervised learning algorithms that can Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. These tools can be the only ways to make sense of data in the absence of supervision information. Having the right representa­gtion of the data is often crucial for supervised or unsupervised learning K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the Carbuncleclosest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]:                mglearn.plots.plot_kmeans_algorithm() grotesque168, grotesque190, grotesque168. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 We will look into two kinds of unsupervised learning in this chapter: transformationsof the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data which might be easier for humans or other machine learning algo‐rithms to understand compared to the original representation. We will also look at what it looks like when we randomly assign points to two clusters for comparison (see Figure 3-39): grotesqueClustering, grotesqueTransformation. And finally, we will look at how we use clustering to extract knowledge from this data. We'll look at the results of our clustering experiment in the next chapter. The next chapter will be published in the spring of A common application of unsupervised transformations is dimensionality reduction. Another application is finding the parts of the data that “make up” the data. An example of this is topic extraction on collections of documents. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media sites. For visualization purposes, unsuper supervised transformations can be used to reduce data to two dimensions for visualization purposes. For more information, visit the Open Data Project.    The OpenData Project is a project of the University of California, San Diego, and the California Institute of Technology.  For more details, go to the OpenDataproject.org. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐rithm two clusters get merged. The length of each branch also shows how far apart                the merged clusters are. There are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐                ters in the lefthand side of the plot. The site doesn't know which pictures show whom, and it                doesn't know how many different people appear in your photo collection. A sensibleapproach would be to extract all The longest branches in this dendrogram are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the tworemaining clusters into a single cluster again bridges a relatively large distance.Unfortunately, agglomerative clustering still fails at separating complex shapes like the two_moons dataset. But the same is not true for the next algorithm we will look at, DBSCAN. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori. DBSCAN works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The idea behind DBS CAN is that clusters form dense regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points)",
            "children": [
                {
                    "id": "chapter-3-section-1",
                    "title": "Types of Unsupervised Learning",
                    "content": "procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps. Dimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and",
                    "summary": "We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k-Means allows for a char‐rouacterization of the clusters using the cluster means. DBS CAN allows for the detection of ‘noise points’ that are not assigned any cluster. agglomersative clustering allows you to specify the number of desired clusters, whileDBSCAN lets you define proximity using the eps parameter, which indirectly influences cluster size. all three methods can be used on large, real-world datasets, and allow for clustering into many clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data. dimensionality reduction is a common application for unsuper supervised transformations. Another application is finding the parts or compo­nents that “make up” the data, for example, topic extraction on collections of text documents. We will also look at how machine learning can be used to extract knowledge from this data. We hope this chapter will help you understand some of the ideas behind machine learning and its use in the real world. Back to the page you came from. Clustering algorithms partition data into distinct groups of similar items. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. The site doesn’t know how many different people appear in your photo collection. A sensibleapproach would be to extract all the faces and divide them into groups of faces that look similar.  The site might want to group together pictures that show the same person. However, the site doesn't know which pictures show whom, and it can't tell how many people are in Using the cluster labels to index another array. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms. The challenges in applying clustering algorithms is that it is very hard to assess how well an algorithm worked, and to compare outcomes between different algo‐rithms. There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. In Figure 3-39, we show what it looks like when we randomly assign points to two clusters. We also show how the data looks when we rescale the data to zero mean and unit variance. In the plot, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. The plot also shows how the points look when the points are randomly assigned to each cluster. The results are shown in the interactive version of this article. The full version is available at the bottom of the page. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐                rithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are. The longest branches are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the two remaining clusters into a single cluster again bridges a relatively large distance. DBSCAN (which stands for “density-based spatial clustering of applications with noise’) works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori, it can cap‐                ture clusters of complex shapes, and it can identify points that aren’t part of any cluster. But the same is not true for the next algorithm we will look at, which is called the ‘core point’ algorithm The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. The results are quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. PCA is often a good first approach for transforming your data so that you might be able to visualize it using a scatter plot. A particularly useful one is the t-SNE algorithm. It allows for much more complex map‐insuredpings, and often provide better visualizations. It is not to be confused with the much larger MNIST dataset. It can be used to create more complex maps, such as those of the Labeled Faces in the Wild dataset. For more information, see the Colorskit_learn user guide on independent component analysis (ICA), factor analysis (FA), and sparse coding (d The idea behind t-SNE is to find a two-dimensional representation of the data that preserves the distances between points as best as possible. It tries to preserve the information indicating which points are neighbors to each other. Manifold learning can be useful for explora‐tory data analysis, but is rarely used if the final goal is supervised learning. Some of these algorithms don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. The idea is to make points that are close in the original feature space closer, and those that are far apart in theoriginal feature space farther apart. We can use PCA for dimensionality reduction by retaining only some of the principalcomponents. This reduces the data from a two-dimensional dataset to a one- dimensional dataset. We might keep only the first principal component, as shown in the third panel in Figure 3-3. The correlation matrix of the data in this representation is zero except for the diagonal. For more information on PCA, visit PCA.org or go to the PCA website at PCA’s official website at: http://www.pca.org/pca-online/features/features-extraction-and-manifold-learning. One of the most common applications of PCA is visualizing high-dimensional data‐sets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more than two features. This transformation is sometimes used to remove noise effects from the data or to visualize what part of the information is retained using the principal components. We can undo the rotation and add the mean back to the data. This will result in the data shown in the last panel in Figure 3-3. These points are in the original fea‐ purposefullyture space, but we kept only the information contained in the first principal compo­nent. The data can be downloaded from the PCA website here. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data. Unsupervised transformations of a dataset are algorithms that create a new representa­tions of theData which might be easier for humans or other machine learning algo‐rithms to understand. A common application of un supervised transformations is dimensionality reduction. This takes a high-dimensional representation of the Data, consisting of many features, and finds a new way to represent this data that summarizes the essential characteristics with fewer features. An example of this is topic extraction on collections oftext documents. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. Clustering algorithms, on the other hand, partition data into distinct groups of similar items. For example, a social media site might want to group together pictures that show the same person. However, the site doesn’t know which pictures show whom, and it doesn't know how many different people appear in your collection. For visualization purposes, a common application for dimensionality reduction is reduction to two dimensions for visualization purposes. When we use only the first two principal components the whole data is just a big blob, with no separation of classes visible. This is not very surprising, given that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐                tures very rough characteristics of the faces. A sensibleapproach would be to extract all the faces and divide them into groups of faces that                look similar. Hopefully, these correspond to the same person, and the images can be                grouped together for you. For more information on unsupervised learning, see the Wikipedia article on Unsupervised Learning and the Wikipedia page on Manifold Learning and Manipulation. NMF can be used to decompose data into a non-negative weighted sum. It works similarly to PCA and can also be used fordimensionality reduction. NMF can identify the original components that make up the combined data. It is particularlyhelpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. The method can only be applied to data where each feature is non- negative, as the sum of non- Negative components cannotbecome negative. It can be also used to reduce the number of data points in a data set to a single point. For more information, visit the NMF website.  NMF leads to more interpretable components than PCA. Negative components and coefficients can lead to hard-to-interpret can‐cellation effects",
                    "children": [
                        {
                            "id": "chapter-3-section-1-subsection-1",
                            "title": "Clustering Methods",
                            "content": "procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.",
                            "summary": "We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k-Means allows for a char‐rouacterization of the clusters using the cluster means. DBS CAN allows for the detection of ‘noise points’ that are not assigned any cluster. agglomersative clustering allows you to specify the number of desired clusters, whileDBSCAN lets you define proximity using the eps parameter, which indirectly influences cluster size. all three methods can be used on large, real-world datasets, and allow for clustering into many clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data. dimensionality reduction is a common application for unsuper supervised transformations. Another application is finding the parts or compo­nents that “make up” the data, for example, topic extraction on collections of text documents. We will also look at how machine learning can be used to extract knowledge from this data. We hope this chapter will help you understand some of the ideas behind machine learning and its use in the real world. Back to the page you came from. Clustering algorithms partition data into distinct groups of similar items. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. The site doesn’t know how many different people appear in your photo collection. A sensibleapproach would be to extract all the faces and divide them into groups of faces that look similar.  The site might want to group together pictures that show the same person. However, the site doesn't know which pictures show whom, and it can't tell how many people are in Using the cluster labels to index another array. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms. The challenges in applying clustering algorithms is that it is very hard to assess how well an algorithm worked, and to compare outcomes between different algo‐rithms. There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. In Figure 3-39, we show what it looks like when we randomly assign points to two clusters. We also show how the data looks when we rescale the data to zero mean and unit variance. In the plot, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. The plot also shows how the points look when the points are randomly assigned to each cluster. The results are shown in the interactive version of this article. The full version is available at the bottom of the page. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐                rithm two clusters get merged. The length of each branch also shows how far apart the merged clusters are. The longest branches are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the two remaining clusters into a single cluster again bridges a relatively large distance. DBSCAN (which stands for “density-based spatial clustering of applications with noise’) works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori, it can cap‐                ture clusters of complex shapes, and it can identify points that aren’t part of any cluster. But the same is not true for the next algorithm we will look at, DBSCann.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-1-subsection-2",
                            "title": "Dimensionality Reduction",
                            "content": "Dimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and",
                            "summary": "The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. The results are quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. PCA is often a good first approach for transforming your data so that you might be able to visualize it using a scatter plot. A particularly useful one is the t-SNE algorithm. It allows for much more complex map‐insuredpings, and often provide better visualizations. It is not to be confused with the much larger MNIST dataset. It can be used to create more complex maps, such as those of the Labeled Faces in the Wild dataset. For more information, see the Colorskit_learn user guide on independent component analysis (ICA), factor analysis (FA), and sparse coding (d The idea behind t-SNE is to find a two-dimensional representation of the data that preserves the distances between points as best as possible. It tries to preserve the information indicating which points are neighbors to each other. Manifold learning can be useful for explora‐tory data analysis, but is rarely used if the final goal is supervised learning. Some of these algorithms don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. The idea is to make points that are close in the original feature space closer, and those that are far apart in theoriginal feature space farther apart. We can use PCA for dimensionality reduction by retaining only some of the principalcomponents. This reduces the data from a two-dimensional dataset to a one- dimensional dataset. We might keep only the first principal component, as shown in the third panel in Figure 3-3. The correlation matrix of the data in this representation is zero except for the diagonal. For more information on PCA, visit PCA.org or go to the PCA website at PCA’s official website at: http://www.pca.org/pca-online/features/features-extraction-and-manifold-learning. One of the most common applications of PCA is visualizing high-dimensional data‐sets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more than two features. This transformation is sometimes used to remove noise effects from the data or to visualize what part of the information is retained using the principal components. We can undo the rotation and add the mean back to the data. This will result in the data shown in the last panel in Figure 3-3. These points are in the original fea‐ purposefullyture space, but we kept only the information contained in the first principal compo­nent. The data can be downloaded from the PCA website here. We will look into two kinds of unsupervised learning in this chapter: transformations of the dataset and clustering. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data. Unsupervised transformations of a dataset are algorithms that create a new representa­tions of theData which might be easier for humans or other machine learning algo‐rithms to understand. A common application of un supervised transformations is dimensionality reduction. This takes a high-dimensional representation of the Data, consisting of many features, and finds a new way to represent this data that summarizes the essential characteristics with fewer features. An example of this is topic extraction on collections oftext documents. Here, the task is to find the unknown topics that are talked about in each document. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media. Clustering algorithms, on the other hand, partition data into distinct groups of similar items. For example, a social media site might want to group together pictures that show the same person. However, the site doesn’t know which pictures show whom, and it doesn't know how many different people appear in your collection. For visualization purposes, a common application for dimensionality reduction is reduction to two dimensions for visualization purposes. When we use only the first two principal components the whole data is just a big blob, with no separation of classes visible. This is not very surprising, given that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐                tures very rough characteristics of the faces. A sensibleapproach would be to extract all the faces and divide them into groups of faces that                look similar. Hopefully, these correspond to the same person, and the images can be                grouped together for you. For more information on unsupervised learning, see the Wikipedia article on Unsupervised Learning and the Wikipedia page on Manifold Learning and Manipulation. NMF can be used to decompose data into a non-negative weighted sum. It works similarly to PCA and can also be used fordimensionality reduction. NMF can identify the original components that make up the combined data. It is particularlyhelpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. The method can only be applied to data where each feature is non- negative, as the sum of non- Negative components cannotbecome negative. It can be also used to reduce the number of data points in a data set to a single point. For more information, visit the NMF website.  NMF leads to more interpretable components than PCA. Negative components and coefficients can lead to hard-to-interpret can‐cellation effects",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-1-subsection-3",
                            "title": "Density Estimation",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-1-subsection-4",
                            "title": "Pattern Discovery",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-2",
                    "title": "Challenges in Unsupervised Learning",
                    "content": "information on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
                    "summary": "In this chapter we discussed cross-validation, grid search, and evaluation metrics, the cornerstonestones of evaluating and improving machine learning algorithms. information on this topic. In the next chapter, we will look at how we can use these metrics to improve machine learning. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. Evaluation for regression can be done in similar detail as we did for classification. In most applications, using the default R2 used in the scoremethod of all regressors is enough. Sometimes business decisions are made on thebasis of mean squared error or mean absolute error, which might give incentive to tweak models using these metrics. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring or scoring metric. For example, Micro average f1 score: 0.953. Macro average f 1 score:0.954. Using Evaluation Metrics in Model Selection using GridSearchCV or cross_val_score. In general, though, we have found R2 to be a more                intuitive metric to evaluate regression models. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. The scoring argument that can be used in both GridSearch CV and cross_Val_score is called scikit-learn. The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐consuming \"roc_auc\" as the scoring parameter. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal. This closest metric should be used whenever possible for model evaluation andselection. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any. application, we would call a positive test (an indi‐cation of cancer) the positiveclass, and anegative test the negative class. If you have any questions about our model, please email us at jennifer.",
                    "children": [
                        {
                            "id": "chapter-3-section-2-subsection-1",
                            "title": "Data Quality Issues",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-2",
                            "title": "Parameter Selection",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-3",
                            "title": "Evaluation Metrics",
                            "content": "information on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
                            "summary": "In this chapter we discussed cross-validation, grid search, and evaluation metrics, the cornerstonestones of evaluating and improving machine learning algorithms. information on this topic. In the next chapter, we will look at how we can use these metrics to improve machine learning. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. Evaluation for regression can be done in similar detail as we did for classification. In most applications, using the default R2 used in the scoremethod of all regressors is enough. Sometimes business decisions are made on thebasis of mean squared error or mean absolute error, which might give incentive to tweak models using these metrics. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring or scoring metric. For example, Micro average f1 score: 0.953. Macro average f 1 score:0.954. Using Evaluation Metrics in Model Selection using GridSearchCV or cross_val_score. In general, though, we have found R2 to be a more                intuitive metric to evaluate regression models. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. The scoring argument that can be used in both GridSearch CV and cross_Val_score is called scikit-learn. The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐consuming \"roc_auc\" as the scoring parameter. For example, we could test classifying images of pedestrians against non-pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal. This closest metric should be used whenever possible for model evaluation andselection. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any. application, we would call a positive test (an indi‐cation of cancer) the positiveclass, and anegative test the negative class. If you have any questions about our model, please email us at jennifer.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-4",
                            "title": "Validation Approaches",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-3",
                    "title": "Preprocessing and Scaling",
                    "content": "separately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\ncalled transform on the training and test sets. You can see that the dataset in the sec‐\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\nthe features are between 0 and 1. You can also see that the minimum and maximum\nfeature values for the test data (the triangles) are not 0 and 1.\nThe third panel shows what would happen if we scaled the training set and test set\nseparately. In this case, the minimum and maximum feature values for both the train‐\ning and the test set are 0 and 1. But now the dataset looks different. The test points\nmoved incongruously to the training set, as they were scaled differently. We changed\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\ndo.\nAs another way to think about this, imagine your test set is a single point. There is no\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\nments of the MinMaxScaler. But the size of your test set should not change your\nprocessing.\nShortcuts and Efficient Alternatives\nOften, you want to fit a model on some dataset, and then transform it. This is a very\ncommon task, which can often be computed more efficiently than by simply calling\nfit and then transform. For this use case, all models that have a transform method\nalso have a fit_transform method. Here is an example using StandardScaler:\nIn[9]:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calling fit and transform in sequence (using method chaining)\nX_scaled = scaler.fit(X).transform(X)\n# same result, but more efficient computation\nX_scaled_d = scaler.fit_transform(X)\n# split it into training and test sets\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n# plot the training and test sets\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\n136 \n| \nChapter 3: Unsupervised Learning and Preprocessing\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"Original Data\")\n# scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# visualize the properly scaled data\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[1].set_title(\"Scaled Data\")\n# rescale the test set separately\n# so test set min is 0 and test set max is 1\n# DO NOT DO THIS! For illustration purposes only.\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n# visualize wrongly scaled data\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"training set\", s=60)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\naxes[2].set_title(\"Improperly Scaled Data\")\nfor ax in axes:\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\nseparately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nseparate training and test sets to evaluate the supervised model we will build after the\npreprocessing):\nIn[3]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nOut[3]:\n(426, 30)\n(143, 30)\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\nments. We split the dataset into 426 samples for the training set and 143 samples for\nthe test set.\nAs with the supervised models we built earlier, we first import the class that imple‐\nments the preprocessing, and then instantiate it:\nIn[4]:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n134 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nWe then fit the scaler using the fit method, applied to the training data. For the Min\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\nused:\nIn[5]:\nscaler.fit(X_train)\nOut[5]:\nMinMaxScaler(copy=True, feature_range=(0, 1))\nTo apply the transformation that we just learned—that is, to actually scale the training\ndata—we use the transform method of the scaler. The transform method is used in\nscikit-learn whenever a model returns a new representation of the data:\nIn[6]:\n# transform data\nX_train_scaled = scaler.transform(X_train)\n# print dataset properties before and after scaling\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\nprint(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\nprint(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\nprint(\"per-feature minimum after scaling:\\n {}\".format( You should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nhappen when building processing chains without the pipeline class (like forgetting\nto apply all transformers on the test set, or not applying them in the right order).\nChoosing the right combination of feature extraction, preprocessing, and models is\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\nlines, this “trying out” of many different processing steps is quite simple. When\n320 \n| \nChapter 6: Algorithm Chains and Pipelines\nexperimenting, be careful not to overcomplicate your processes, and make sure to\nevaluate whether every component you are including in your model is necessary.\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\nrithms provided by scikit-learn. You now possess all the required skills and know\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\nwe will dive in more detail into one particular type of data that is commonly seen in\npractice, and that requires some special expertise to handle correctly: text data.\nSummary and Outlook \n| \n321\nCHAPTER 7\nWorking with Text Data\nIn Chapter 4, we talked about two kinds of features that can represent properties of\nthe data: continuous features that describe a quantity, and categorical features that are\nitems from a fixed list. There is a third kind of feature that can be found in many\napplications, which is text. For example, if we want to classify an email message as\neither a legitimate email or spam, the content of the email will certainly contain\nimportant information for this classification task. Or maybe we want to learn about\nthe opinion of a politician on the topic of immigration. Here, that individual’s\nspeeches or tweets might provide useful information. In customer service, we often\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\nand content of a message to automatically determine the customer’s intent, which",
                    "summary": "The training set is shown as circles and the test set shown as triangles. The test points are moved incongruously to the training set, as they were scaled differently. We changed the arrangement of the data in an arbitrary way. We called fit on the training and test sets. separately (right)Preprocessing and Scaling |                          137   ‘Scaling’ and ‘Fitting’ the Training Set and Test Set, and then ‘Transforming’ The Training Set was scaled using the MinMaxScaler, and the Test Set with the Min MaxScaler. The training set and test set were scaled separately (left) and “Transformed” the Training The size of your test set should not change your processing. All models that have a transform method also have a fit_transform method. The MinMaxScaler can be used to fit a model on some dataset, and then transform it. This is a very common task, which can often be computed more efficiently than by simply calling fit and transform. For more information, visit the MinMax Scaler website or the Min Max Scaler GitHub page, or see the official site for more information on how to use it. Using StandardScaler, we can scale data using method chaining. We can plot the training and test sets using plt.subplots. Here is an example of a training set with the test set set to 1. For illustration purposes only, we use MinMaxScaler for the training set. The training set min is 0 and test set max is 1. The test set min and max are 0 and 1, respectively, for the test and training sets, respectively. For more information, visit sklearn.preprocessing.com/learn/supervised-learning-and-preprocessing/unsupervised learning and preprocessing/Unsupervised Learning and Preprocessing. Preprocessing and Scaling are often used in tandem with supervised learning algorithms. Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms.Often, this is a simple per-feature rescaling and shift of theData. Effect of scaling training and test data shown on the left together (center) and separately (right) The data in the first panel is an unscaled two-dimensional dataset, with the training set shown as triangles. The second panel is the same data, but the scale is The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two features. The first feature (the x-axis value) is between 10 and 15. The second feature(the y-axisvalue) is around 1 to 9. The following four plots show four different ways to transform the data that yield more standard ranges. The code for the plots can be found at the bottom of the page. For more information on how to use the plots, see the code at the end of this article. For further information, please visit the GitHub repository at: http://www.jupiter.com/jupiter-research/preprocessing-and-unsupervised-learning.html#preprocessing. The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler uses the median and quartiles,1 instead of mean and variance. This scaling does not ensure any particular minimum and maximumvalues for the features. It ensures statistical properties that guarantee that they are on the same scale. The dataset contains 569 data points, each represented by 30 measure­ments. This makes the RobustScaler ignore data points that are very very small. We use separate training and test sets to evaluate the supervised model we will build after the preprocessing. The supervised model is based on the load_breast_cancer We split the dataset into 426 samples for the training set and 143 for the test set. We then fit the scaler using the fit method, applied to the training data. In contrast to the classifiers and regressors of Chapter 2, the MinMaxScaler is only provided with the data (X_train) when fit is called, and y_train is not used. To apply the transformation that we just learned—that is, to actually scale the trainingData—we use the transform method of the Scaler. The MinMax Scaler is then used to scale the test data to a more realistic level. The training data is then scaled to a higher level using the scale method. The transform method is used when a model returns a new representation of the data. You should now be in a position where you have some idea of how to apply, tune, andanalyze the models we discussed here. In this chapter, we focused on the binary clas‐sification case, as this is usually easiest to understand. Most of the algorithms presen‐phthalted have classification and regression variants, however, and all of the classificationalgorithms support both binary and multiclass classification.    Playing around with the algorithms on different datasets will give you a better feel for how long they need to train, how easy it is to analyze the models, and how sensitive they are to the representation of the data. Try applying any of these algorithms to the built-in datasets in scikit-learn, like the boston_housing or diabetes datasets for regression, or the digits dataset for multiclass classification. Building a model that actually generalizes well to new data is a bit trickier than that. The second family of machine learning algorithms that we will discuss is unsupervised learning algorithms. Unsupervisedlearning subsumes all kinds of machinelearning where there is no known output, no teacher to instruct the learning algo. We will see how to properly adjust parameters and how to find good parameters automatically in Chapter 6. In the next chapter, we will dive in more detail into unsuper supervised learning and pre Unsupervised learning is a form of machine learning. The goal is to learn how to use data to make better decisions. This book is a collection of articles on the subject of unsupervised learning. It is intended to be a starting point for a discussion of how to apply this type of learning to other areas of the world. The book is written in the form of an open-source open source software called Caffeine for Machine Learning (C4ML) It is written to help people understand how C4ML works and how it can be applied to a variety of situations. It also provides a way to learn I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. Choosing the right combination of feature extraction, preprocessing, and models issomewhat of an art, and often requires some trial and error. This “trying out” of many different processing steps is quite simple using pipe‐like lines. And finally, thanks to DTS, for your everlasting and endless support. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. The second part will be published on December 6, 2013, and is available for download on the DTS website. Click here to read the second part of the article. The third and final installment will be released on In Chapter 6: Algorithm Chains and Pipelines, be careful not to overcomplicate your processes. In the next chapter, we will dive in more detail into one particular type of data that is commonly seen in practice and that requires some special expertise to handle correctly: text data. We have completed our survey of general-purpose tools and algo-rithms provided by scikit-learn. You now possess all the required skills and know the necessary mechanisms to apply machine learning in practice. The next chapter will focus on working with text data, which is a third kind of feature that can be found in many applications. We hope this chapter has given you a better understanding of how machine learning works. In customer service, we often want to find out if a message is a complaint or an inquiry. We can use the subject lineand content of a message to automatically determine the customer’s intent, which. can be used to classify messages. For example, if we want to classify an email message aseither a legitimate email or spam, the content of the email will certainly containimportant information. Or maybe we want the opinion of a politician on the",
                    "children": [
                        {
                            "id": "chapter-3-section-3-subsection-1",
                            "title": "Different Kinds of Preprocessing",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-2",
                            "title": "Applying Data Transformations",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-3",
                            "title": "Scaling Training and Test Data the Same Way",
                            "content": "separately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\ncalled transform on the training and test sets. You can see that the dataset in the sec‐\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\nthe features are between 0 and 1. You can also see that the minimum and maximum\nfeature values for the test data (the triangles) are not 0 and 1.\nThe third panel shows what would happen if we scaled the training set and test set\nseparately. In this case, the minimum and maximum feature values for both the train‐\ning and the test set are 0 and 1. But now the dataset looks different. The test points\nmoved incongruously to the training set, as they were scaled differently. We changed\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\ndo.\nAs another way to think about this, imagine your test set is a single point. There is no\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\nments of the MinMaxScaler. But the size of your test set should not change your\nprocessing.\nShortcuts and Efficient Alternatives\nOften, you want to fit a model on some dataset, and then transform it. This is a very\ncommon task, which can often be computed more efficiently than by simply calling\nfit and then transform. For this use case, all models that have a transform method\nalso have a fit_transform method. Here is an example using StandardScaler:\nIn[9]:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calling fit and transform in sequence (using method chaining)\nX_scaled = scaler.fit(X).transform(X)\n# same result, but more efficient computation\nX_scaled_d = scaler.fit_transform(X)\n# split it into training and test sets\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n# plot the training and test sets\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\n136 \n| \nChapter 3: Unsupervised Learning and Preprocessing\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"Original Data\")\n# scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# visualize the properly scaled data\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[1].set_title(\"Scaled Data\")\n# rescale the test set separately\n# so test set min is 0 and test set max is 1\n# DO NOT DO THIS! For illustration purposes only.\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n# visualize wrongly scaled data\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"training set\", s=60)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\naxes[2].set_title(\"Improperly Scaled Data\")\nfor ax in axes:\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\nseparately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nseparate training and test sets to evaluate the supervised model we will build after the\npreprocessing):\nIn[3]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nOut[3]:\n(426, 30)\n(143, 30)\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\nments. We split the dataset into 426 samples for the training set and 143 samples for\nthe test set.\nAs with the supervised models we built earlier, we first import the class that imple‐\nments the preprocessing, and then instantiate it:\nIn[4]:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n134 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nWe then fit the scaler using the fit method, applied to the training data. For the Min\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\nused:\nIn[5]:\nscaler.fit(X_train)\nOut[5]:\nMinMaxScaler(copy=True, feature_range=(0, 1))\nTo apply the transformation that we just learned—that is, to actually scale the training\ndata—we use the transform method of the scaler. The transform method is used in\nscikit-learn whenever a model returns a new representation of the data:\nIn[6]:\n# transform data\nX_train_scaled = scaler.transform(X_train)\n# print dataset properties before and after scaling\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\nprint(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\nprint(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\nprint(\"per-feature minimum after scaling:\\n {}\".format(",
                            "summary": "The training set is shown as circles and the test set shown as triangles. The test points are moved incongruously to the training set, as they were scaled differently. We changed the arrangement of the data in an arbitrary way. We called fit on the training and test sets. separately (right)Preprocessing and Scaling |                          137   ‘Scaling’ and ‘Fitting’ the Training Set and Test Set, and then ‘Transforming’ The Training Set was scaled using the MinMaxScaler, and the Test Set with the Min MaxScaler. The training set and test set were scaled separately (left) and “Transformed” the Training The size of your test set should not change your processing. All models that have a transform method also have a fit_transform method. The MinMaxScaler can be used to fit a model on some dataset, and then transform it. This is a very common task, which can often be computed more efficiently than by simply calling fit and transform. For more information, visit the MinMax Scaler website or the Min Max Scaler GitHub page, or see the official site for more information on how to use it. Using StandardScaler, we can scale data using method chaining. We can plot the training and test sets using plt.subplots. Here is an example of a training set with the test set set to 1. For illustration purposes only, we use MinMaxScaler for the training set. The training set min is 0 and test set max is 1. The test set min and max are 0 and 1, respectively, for the test and training sets, respectively. For more information, visit sklearn.preprocessing.com/learn/supervised-learning-and-preprocessing/unsupervised learning and preprocessing/Unsupervised Learning and Preprocessing. Preprocessing and Scaling are often used in tandem with supervised learning algorithms. Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms.Often, this is a simple per-feature rescaling and shift of theData. Effect of scaling training and test data shown on the left together (center) and separately (right) The data in the first panel is an unscaled two-dimensional dataset, with the training set shown as triangles. The second panel is the same data, but the scale is The first plot in Figure 3-1 shows a synthetic two-class classification dataset with two features. The first feature (the x-axis value) is between 10 and 15. The second feature(the y-axisvalue) is around 1 to 9. The following four plots show four different ways to transform the data that yield more standard ranges. The code for the plots can be found at the bottom of the page. For more information on how to use the plots, see the code at the end of this article. For further information, please visit the GitHub repository at: http://www.jupiter.com/jupiter-research/preprocessing-and-unsupervised-learning.html#preprocessing. The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler uses the median and quartiles,1 instead of mean and variance. This scaling does not ensure any particular minimum and maximumvalues for the features. It ensures statistical properties that guarantee that they are on the same scale. The dataset contains 569 data points, each represented by 30 measure­ments. This makes the RobustScaler ignore data points that are very very small. We use separate training and test sets to evaluate the supervised model we will build after the preprocessing. The supervised model is based on the load_breast_cancer We split the dataset into 426 samples for the training set and 143 for the test set. We then fit the scaler using the fit method, applied to the training data. In contrast to the classifiers and regressors of Chapter 2, the MinMaxScaler is only provided with the data (X_train) when fit is called, and y_train is not used. To apply the transformation that we just learned—that is, to actually scale the trainingData—we use the transform method of the Scaler. The MinMax Scaler is then used to scale the test data to a more realistic level. The training data is then scaled to a higher level using the scale method. The transform method is used inscikit-learn whenever a model returns a new representation of the data. It is used to print dataset properties before and after scaling. For example, it can print the per-feature minimum and maximum before",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-4",
                            "title": "The Effect of Preprocessing on Supervised Learning",
                            "content": "You should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nhappen when building processing chains without the pipeline class (like forgetting\nto apply all transformers on the test set, or not applying them in the right order).\nChoosing the right combination of feature extraction, preprocessing, and models is\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\nlines, this “trying out” of many different processing steps is quite simple. When\n320 \n| \nChapter 6: Algorithm Chains and Pipelines\nexperimenting, be careful not to overcomplicate your processes, and make sure to\nevaluate whether every component you are including in your model is necessary.\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\nrithms provided by scikit-learn. You now possess all the required skills and know\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\nwe will dive in more detail into one particular type of data that is commonly seen in\npractice, and that requires some special expertise to handle correctly: text data.\nSummary and Outlook \n| \n321\nCHAPTER 7\nWorking with Text Data\nIn Chapter 4, we talked about two kinds of features that can represent properties of\nthe data: continuous features that describe a quantity, and categorical features that are\nitems from a fixed list. There is a third kind of feature that can be found in many\napplications, which is text. For example, if we want to classify an email message as\neither a legitimate email or spam, the content of the email will certainly contain\nimportant information for this classification task. Or maybe we want to learn about\nthe opinion of a politician on the topic of immigration. Here, that individual’s\nspeeches or tweets might provide useful information. In customer service, we often\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\nand content of a message to automatically determine the customer’s intent, which",
                            "summary": "You should now be in a position where you have some idea of how to apply, tune, andanalyze the models we discussed here. In this chapter, we focused on the binary clas‐                sification case, as this is usually easiest to understand. Most of the algorithms presen‐                ted have Playing around with the algorithms on different datasets will give you a better feel for how long they need to train, how easy it is to analyze the models, and how sensitive they are to the representation of the data. Try applying any of these algorithms to the built-in datasets in scikit-learn, like the boston_housing or diabetes datasets for regression, or the digits dataset for multiclass classification. Building a model that actually generalizes well to new data is a bit trickier than that. The second family of machine learning algorithms that we will discuss is unsupervised learning algorithms. Unsupervisedlearning subsumes all kinds of machinelearning where there is no known output, no teacher to instruct the learning algo. We will see how to properly adjust parameters and how to find good parameters automatically in Chapter 6. In the next chapter, we will dive in more detail into unsuper supervised learning and pre Unsupervised learning is a form of machine learning. The goal is to learn how to use data to make better decisions. This book is a collection of articles on the subject of unsupervised learning. It is intended to be a starting point for a discussion of how to apply this type of learning to other areas of the world. The book is written in the form of an open-source open source software called Caffeine for Machine Learning (C4ML) It is written to help people understand how C4ML works and how it can be applied to a variety of situations. It also provides a way to learn I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. Choosing the right combination of feature extraction, preprocessing, and models issomewhat of an art, and often requires some trial and error. This “trying out” of many different processing steps is quite simple using pipe‐like lines. And finally, thanks to DTS, for your everlasting and endless support. Back to Mail Online home. Back To the page you came from. The first part of this article was published on November 14, 2013. The second part will be published on December 6, 2013, and is available for download on the DTS website. Click here to read the second part of the article. The third and final installment will be released on In Chapter 6: Algorithm Chains and Pipelines, be careful not to overcomplicate your processes. In the next chapter, we will dive in more detail into one particular type of data that is commonly seen in practice and that requires some special expertise to handle correctly: text data. We have completed our survey of general-purpose tools and algo-rithms provided by scikit-learn. You now possess all the required skills and know the necessary mechanisms to apply machine learning in practice. The next chapter will focus on working with text data, which is a third kind of feature that can be found in many applications. We hope this chapter has given you a better understanding of how machine learning works. In customer service, we often want to find out if a message is a complaint or an inquiry. We can use the subject lineand content of a message to automatically determine the customer’s intent, which. can be used to classify messages. For example, if we want to classify an email message aseither a legitimate email or spam, the content of the email will certainly containimportant information. Or maybe we want the opinion of a politician on the",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-4",
                    "title": "Dimensionality Reduction, Feature Extraction, and Manifold Learning",
                    "content": "Principal Component Analysis (PCA)\nPrincipal component analysis is a method that rotates the dataset in a way such that\nthe rotated features are statistically uncorrelated. This rotation is often followed by\nselecting only a subset of the new features, according to how important they are for\nexplaining the data. The following example (Figure 3-3) illustrates the effect of PCA\non a synthetic two-dimensional dataset:\nIn[13]:\nmglearn.plots.plot_pca_illustration()\nThe first plot (top left) shows the original data points, colored to distinguish among\nthem. The algorithm proceeds by first finding the direction of maximum variance,\nlabeled “Component 1.” This is the direction (or vector) in the data that contains most\nof the information, or in other words, the direction along which the features are most\ncorrelated with each other. Then, the algorithm finds the direction that contains the\nmost information while being orthogonal (at a right angle) to the first direction. In\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\nhead and the tail are; we could have drawn the first component from the center up to\n140 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nthe top left instead of down to the bottom right. The directions found using this pro‐\ncess are called principal components, as they are the main directions of variance in the\ndata. In general, there are as many principal components as original features.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\ncancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nYou can see that in the first component, all features have the same sign (it’s negative,\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\nThat means that there is a general correlation between all features. As one measure‐\nment is high, the others are likely to be high as well. The second component has\nmixed signs, and both of the components involve all of the 30 features. This mixing of\nall features is what makes explaining the axes in Figure 3-6 so tricky.\nEigenfaces for feature extraction\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\nbehind feature extraction is that it is possible to find a representation of your data\nthat is better suited to analysis than the raw representation you were given. A great\nexample of an application where feature extraction is helpful is with images. Images\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\nObjects in images are usually made up of thousands of pixels, and only together are\nthey meaningful.\nWe will give a very simple application of feature extraction on images using PCA, by\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\ncontains face images of celebrities downloaded from the Internet, and it includes\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\nscale versions of these images, and scale them down for faster processing. You can see\nsome of the images in Figure 3-7:\nIn[21]:\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\ncombinations of the original features. However, these combinations are usually very\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\ncomponents_ attribute of the PCA object during fitting:\nIn[18]:\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\nOut[18]:\nPCA component shape: (2, 30)\nEach row in components_ corresponds to one principal component, and they are sor‐\nted by their importance (the first principal component comes first, etc.). The columns\ncorrespond to the original features attribute of the PCA in this example, “mean\nradius,” “mean texture,” and so on. Let’s have a look at the content of components_:\nIn[19]:\nprint(\"PCA components:\\n{}\".format(pca.components_))\nOut[19]:\nPCA components:\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\nWe can also visualize the coefficients using a heat map (Figure 3-6), which might be\neasier to understand:\nIn[20]:\nplt.matshow(pca.components_, cmap='viridis')\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),\n           cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nIn[28]:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\nOut[28]:\nTest set accuracy: 0.36\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\nintuition that the principal components might provide a better representation of the\ndata.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n151\nFor image data, we can also easily visualize the principal components that are found.\nRemember that components correspond to directions in the input space. The input\nspace here is 50×37-pixel grayscale images, so directions within this space are also\n50×37-pixel grayscale images.\nLet’s look at the first couple of principal components (Figure 3-9):\nIn[29]:\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\nOut[29]:\npca.components_.shape: (100, 5655)\nIn[30]:\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape),\n              cmap='viridis')\n    ax.set_title(\"{}. component\".format((i + 1)))\nWhile we certainly cannot understand all aspects of these components, we can guess\nwhich aspects of the face images some of the components are capturing. The first\ncomponent seems to mostly encode the contrast between the face and the back‐\nground, the second component encodes differences in lighting between the right and\nthe left half of the face, and so on. While this representation is slightly more semantic\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\n# keep the first two principal components of the data\npca = PCA(n_components=2)\n# fit PCA model to breast cancer data\npca.fit(X_scaled)\n# transform data onto the first two principal components\nX_pca = pca.transform(X_scaled)\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\n144 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nOut[16]:\nOriginal shape: (569, 30)\nReduced shape: (569, 2)\nWe can now plot the first two principal components (Figure 3-5):\nIn[17]:\n# plot first vs. second principal component, colored by class\nplt.figure(figsize=(8, 8))\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\nplt.legend(cancer.target_names, loc=\"best\")\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\nprincipal components\nIt is important to note that PCA is an unsupervised method, and does not use any class\ninformation when finding the rotation. It simply looks at the correlations in the data.\nFor the scatter plot shown here, we plotted the first principal component against the\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n145\nsecond principal component, and then used the class information to color the points.\nYou can see that the two classes separate quite well in this two-dimensional space.\nThis leads us to believe that even a linear classifier (that would learn a line in this\nspace) could do a reasonably good job at distinguishing the two classes. We can also\nsee that the malignant (red) points are more spread out than the benign (blue) points\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\nwhat a human would perceive first. When asking people to rate similarity of faces,\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\nkeep in mind that algorithms often interpret data (particularly visual data, such as\nimages, which humans are very familiar with) quite differently from how a human\nwould.\n152 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-9. Component vectors of the first 15 principal components of the faces dataset\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\nmation as rotating the data and then dropping the components with low variance.\nAnother useful interpretation is to try to find some numbers (the new feature values\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\nprincipal components (see Figure 3-10).\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\ncomponents\nHere, x0, x1, and so on are the coefficients of the principal components for this data\npoint; in other words, they are the representation of the image in the rotated space.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n153\nAnother way we can try to understand what a PCA model is doing is by looking at\nthe reconstructions of the original data using only some components. In Figure 3-3,\nafter dropping the second component and arriving at the third panel, we undid the\nrotation and added the mean back to obtain new points in the original space with the\nsecond component removed, as shown in the last panel. We can do a similar transfor‐ plt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\ndata.\nApplying NMF to synthetic data\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\nto be able to operate on the data. This means where the data lies relative to the origin\n(0, 0) actually matters for NMF. Therefore, you can think of the non-negative compo‐\nnents that are extracted as directions from (0, 0) toward the data.\nThe following example (Figure 3-13) shows the results of NMF on the two-\ndimensional toy data:\nIn[34]:\nmglearn.plots.plot_nmf_illustration()\n156 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-13. Components found by non-negative matrix factorization with two compo‐\nnents (left) and one component (right)\nFor NMF with two components, as shown on the left, it is clear that all points in the\ndata can be written as a positive combination of the two components. If there are\nenough components to perfectly reconstruct the data (as many components as there\nare features), the algorithm will choose directions that point toward the extremes of\nthe data.\nIf we only use a single component, NMF creates a component that points toward the\nmean, as pointing there best explains the data. You can see that in contrast with PCA,\nreducing the number of components not only removes some directions, but creates\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\nponents, where all the data can be explained perfectly, the randomness has little effect\n(though it might change the order or scale of the components). In more complex sit‐\nuations, there might be more drastic changes.\nApplying NMF to face images\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\nmain parameter of NMF is how many components we want to extract. Usually this is\nlower than the number of input features (otherwise, the data could be explained by\nmaking each pixel a separate component).\nFirst, let’s inspect how the number of components impacts how well the data can be\nreconstructed using NMF (Figure 3-14):\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n157\nIn[35]:\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\nFigure 3-14. Reconstructing three face images using increasing numbers of components\nfound by NMF\nThe quality of the back-transformed data is similar to when using PCA, but slightly\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\nfor finding interesting patterns within the data.\nAs a first look into the data, let’s try extracting only a few components (say, 15).\nFigure 3-15 shows the result:\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape))\n    ax.set_title(\"{}. component\".format(i))\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\nnents\nThese components are all positive, and so resemble prototypes of faces much more so\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\nthat component 3 shows a face rotated somewhat to the right, while component 7\nshows a face somewhat rotated to the left. Let’s look at the images for which these\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n159\nIn[37]:\ncompn = 3\n# sort by 3rd component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\ncompn = 7\n# sort by 7th component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\nFigure 3-16. Faces that have a large coefficient for component 3\n160 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-17. Faces that have a large coefficient for component 7 feature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\nimage for each class:\nIn[43]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\n                         subplot_kw={'xticks':(), 'yticks': ()})\nfor ax, img in zip(axes.ravel(), digits.images):\n    ax.imshow(img)\n164 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-20. Example images from the digits dataset\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\nprincipal components, and color each dot by its class (see Figure 3-21):\nIn[44]:\n# build a PCA model\npca = PCA(n_components=2)\npca.fit(digits.data)\n# transform the digits data onto the first two principal components\ndigits_pca = pca.transform(digits.data)\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\nThe Decision Function                                                                                              120\nPredicting Probabilities                                                                                             122\nUncertainty in Multiclass Classification                                                                 124\nSummary and Outlook                                                                                                 127\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\nTypes of Unsupervised Learning                                                                                 131\nChallenges in Unsupervised Learning                                                                        132\nPreprocessing and Scaling                                                                                            132\nDifferent Kinds of Preprocessing                                                                             133\nApplying Data Transformations                                                                               134\nScaling Training and Test Data the Same Way                                                       136\nThe Effect of Preprocessing on Supervised Learning                                           138\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\nPrincipal Component Analysis (PCA)                                                                    140\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\ncolor = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nThe digits zero, six, and four are relatively well separated using the first two principal\ncomponents, though they still overlap. Most of the other digits overlap significantly.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n165\nFigure 3-21. Scatter plot of the digits dataset using the first two principal components\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\nsupport transforming new data, the TSNE class has no transform method. Instead, we\ncan call the fit_transform method, which will build the model and immediately\nreturn the transformed data (see Figure 3-22):\nIn[45]:\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\n# use fit_transform instead of fit, as TSNE has no transform method\ndigits_tsne = tsne.fit_transform(digits.data)\n166 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[46]:\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"t-SNE feature 0\")\nplt.xlabel(\"t-SNE feature 1\")\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236",
                    "summary": "Principal component analysis is a method that rotates the dataset in a way such that rotated features are statistically uncorrelated. This rotation is often followed byselecting only a subset of the new features, according to how important they are for explaining the data. The following example illustrates the effect of PCA on a synthetic two-dimensional dataset. The algorithm proceeds by first finding the direction (or vector) in the data that contains most of the information. Then, the algorithm finds the direction that contains the most information while being orthogonal (at a right angle) to the first direction. The first plot (top left) shows the original data points, colored to distinguish among them. In two dimensions, there is only one possible orientation that is at a right angle. In higher-dimensional spaces, there would be (infinitely) many orthogonal directions. The directions are called principal components, as they are the main directions of variance in the data. In general, there are as many principal components as original features. The second plot (top right) shows the same data, but now rotated so that the firstprincipal component aligns with the x-axis and the second principal component aligned with the y-axis. The first component is drawn from the center up to the top left instead of down to the bottom right of the plot. Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset. All features have the same sign (it’s negative, but it doesn’t matter which direction the arrow points in) The second component has mixed signs, and both of the components involve all of the 30 features. This mixing of features is what makes explaining the axes in Figure3-6 so tricky. Another application of PCA that we mentioned earlier is feature extraction. For feature extraction, use the feature extraction tool on the right side of the page. For more information on feature extraction and how to use it in your own data, see the ‘Feature Extraction’ section. Feature extraction is the process of finding a representation of your data that is better suited to analysis than the raw representation you were given. Imagesare made up of pixels, usually stored as red, green, and blue (RGB) intensities. We will give a very simple application of feature extraction on images using PCA, by working with face images from the Labeled Faces in the Wild dataset. This datasetcontains face images of celebrities downloaded from the Internet, and it includes faces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐scale versions of these images, and scale them down for faster processing.   We can use PCA for dimensionality reduction by retaining only some of the principal components. In the rotated representation found by PCA, the two axes are uncorrelated, meaning that the correlation matrix of the data in this representation is zero except for the diagonal diagonal. You can see                some of the images in Figure 3-7: worrisomeIn[21]: worrisomeIn the second plot, the data is rotated so that the first principal component aligns with the x-axis and the second principal component aligned with the y-axis. Before the rotation, the mean was subtracted from the data, so                that the transformed data In this example, we might keep only the first principal component, as shown in the third panel in Figure 3-3 (bottom left) This reduces the data from a two-dimensional to a one-dimensional dataset. Note, however, that instead of keeping only one of the original features, we found the most interesting direction and kept this direction. We can undo the rotation and add the mean back to the data. These points are in the original fea‐ipientture space, but we kept only the information contained in the first Principal Compo‐nent. This will resultin the data shown inthe last Applying PCA to the cancer dataset for visualization. One of the most common applications of PCA is visualizing high-dimensional data. The principal components correspond to directions in the original data, so they are combinations of the original features. However, these combinations are usually verycomplex, as we’ll see shortly. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data by showing us all the possible configurations. The two axes in the plot are often not very easy to interpret, but we can still get a good idea of what the data is about by looking at the histograms in Figure 3-4. It is possible to use PCA for data that has more The principal components themselves are stored in the                components_ attribute of the PCA object during fitting. Each row in components_ corresponds to one principal component, and they are sor‐                ted by their importance. The columns “meanradius,” ‘mean texture,’ and so on correspond to the original features attribute of We can also visualize the coefficients using a heat map (Figure 3-6), which might beeasier to understand. Let’s have a look at the content of components_: grotesqueIn[19]: grotesqueprint(\"PCA components:\\n{}\".format(pca.components_) grotesqueOut[19): grotesqueprint (\"PCA. components: 0.219  0.104 0.228, 0.221  0,143, 0,239, 0,.261, 0.,138,064) grotesque Out[19] : grotesque Our accuracy improved quite significantly, from 26.6% to 35.7%. The principal components might provide a better representation of the data. For image data, we can also easily visualize the principal components that are found.Remember that components correspond to directions in the input space. The heat map of the first two principal components on the Breast Cancer dataset can be seen here. Let’s look at the first couple of principal components (Figure 3-9) The input space here is 50×37-pixel grayscale images, so directions within this space are also 50x37- pixel images. The first component seems to mostly encode the contrast between the face and the back ground. The second component encodes differences in lighting between the right and the left half of the face, and so on. We certainly cannot understand all aspects of these components, but we can guess which aspects of face images some of the components are capturing. The PCA model is based on pixels, the alignment of the face (the position of eyes,chin, and nose) and the lighting both have a strong influence on how similar two images are in their pixel representation. While this representation is slightly more semantic than the raw pixel values, it is still quite far from how a human might perceive a face. We can now plot the first two principal components (Figure 3-5):. plot first vs. first. plot second vs. second. plot third vs. third. plot fourth vs. fourth. plot fifth vs. fifth. plot sixth vs. sixth. plot seventh vs. seventh. plot eighth vs. ninth. plot tenth vs. tenth Figure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first and second principal components. PCA is an unsupervised method, and does not use any class information when finding the rotation. It is important to note that PCA does not used any class The two classes separate quite well in this two-dimensional space. This leads us to believe that even a linear classifier could do a reasonably good job at distinguishing the two classes. We can also see that the malignant (red) points are more spread out than the benign (blue) points. The principal components correspond to directions in the original data, so they areabeled in Figure 3-12. A downside of PCA is that the two axes in the plot are often not very easy to interpret. It simply looks at the correlations in the data. It is possible to use PCA to learn a line in this space by looking at the correlation between two points in the same space. Unsupervised learning algorithm PCA can be used to extract useful features. It works similarly to PCA and can also be used for dimensionality reduction. We are trying to write each data point as aweighted sum of some components, as illustrated in Figure 3-10. Non-Negative Matrix Factorization (NMF) is another unsupervisedlearning algorithm that can be also used for feature extraction. The results of NMF can be seen in the next section of this article, which includes the results of Manifold Learning for the cancer dataset. For more information on NMF, see http://www.manifoldlearning.org/. NMF decomposes data into a non-negative weighted sum. It is particularly helpful for data that is created as the addition (or overlay) of several independent sources. In these situations, NMF can identify the original components that make up the combined data. NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret can‐ autoimmunecellation effects. The method can only be applied to data where each feature is non- negative, as a non-'negative sum' cannotbecome negative. The process of decomposing data is particularlyhelpful fordata that is added (or overlaid) of multiple independent sources, such as an audio track of multiple people speaking, As the PCA model is based on pixels, the alignment of the face (the position of eyes, and the lighting both have a strong influence on how similar two pixel representation are in their pixel representation) But alignment and lighting are probably not what a human would perceive first. When asking people to rate similarity of faces, they are more likely to use attributes like age, gender, facial expression, and hair style, which are hard to infer from the pixel intensities. It’s important tokeep in mind that algorithms often interpret data (particularly visual data, such as images, which humans are very familiar with) quite differently from how a humanwould. The eigenfaces in Figure 3-9, for example, contain both The PCA transfor‐                mation as rotating the data and then dropping the components with low variance. Another useful interpretation is to try to find some numbers (the new feature values) so that we can express the test points as a weighted sum of the principal components. Another way we can try to understand what a PCA model is doing is by looking at the reconstructions of the original data using only some components. The coefficients of these principal components are the representation of the image in the rotated space and are the key to understanding PCA. The PCA algorithm is based on the principle of Dimensionality Reduction, Feature Extraction, and Manifold Learning In Figure 3-3, we undid therotation and added the mean back to obtain new points in the original space with the second component removed. We can do a similar transfor‐ plt. plot of the faces dataset using the first two principal components. The whole data is just a big blob, with no separation of classes visible. This is not very surprising, given that even with 10 components, PCA only cap‐tures very rough characteristics of the Faces dataset. The results are consistent with unsupervised learning algorithm thataims to extract useful features. For the cancer dataset, we can use Non-Negative Matrix Factorization (NMF) and Manifold Learning. NMF can be used to decompose data into a non-negative weighted sum. It works similarly to PCA and can also be used fordimensionality reduction. NMF can identify the original components that make up the combined data. It is particularlyhelpful for data that is created as the addition (or overlay) of several independent sources, such as an audio track of multiple people speaking, or music with many instruments. The method can only be applied to data where each feature is non- negative, as the sum of non- Negative components cannotbecome negative. It can be also used to reduce the number of data points in a data set to a single point. For more information, visit the NMF website.  NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret effects. The eigenfaces in Figure 3-9, for example, contain both positive and negative parts. This means where the data lies relative to the origin(0, 0) actually matters for NMF. To apply NMF to synthetic data, we need to ensure that our data is positive for the NMFto be able to operate on the data. For example, the sign is actually ‘0’ rather than ‘1’, and this is because the data is ‘synthetic’ not ‘real’.  NMF creates a component that points toward the extremes of the data, as pointing there best explains the data. If there are enough components to perfectly reconstruct the data (as many components as there are features), the algorithm will choose directions that point toward the extreme. The following example shows the results of NMF on the two-dimensional toy data. The results are shown in Figure 3-13, which is an example of a non-negative matrix factorization with two compo'reporters and one component. For NMF with two components, as shown on the left, it is clear that all points in the data can be written as a positive combination of the two components. In contrast with PCA, reducing the number of components not only removes some directions, but creates an entirely different set of components. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component”: all components play an equal part. NMF uses a random initialization, which might lead to different results depending on the type of component you are trying to reduce. In the next section, we'll look at how to CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Click here for NMF uses a random initialization, which might lead to different results depending on the random seed. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component” NMF uses an entirely different set of components! All components play an equal part. NMF is based on the Haskell programming language. The main parameter of NMF is how many components we want to extract. Usually this islower than the number of input features. In more complex situations, there might be more drastic changes. The number of components impacts how well the data can be reconstructed using NMF (Figure 3-14):Dimensionality Reduction, Feature Extraction, and Manifold Learning. In[35]: atrophymglearn.plots.plot_nmf_faces(X_train, X_test, image_shape) Figure 3- 14. NMF reconstruction of the Labeled Faces in the Wild dataset. The data was taken from a set of images called ‘ The quality of the back-transformed data is similar to when using PCA, but slightly worse. This is expected, as PCA finds the optimum directions in terms of reconstruc  NMF is usually not used for its ability to reconstruct or encode data, but rather for finding interesting patterns within the data. As a first look into the data, let’s try extracting only a few components (say, 15). Figure 3-15 shows the result: grotesquely158, apologetically158, and properly158. Figure 3: Unsupervised Learning and Preprocessing is available at: http://www.sklearn.com/learn/features/unsupervised-learning-and-preprocessing.  NMF found the components found by NMF on the faces dataset when using 15 compo‐nents. These components are all positive, and so resemble prototypes of faces much more sothan the components shown for PCA in Figure 3-9. For example, one can clearly see that component 3 shows a face rotated somewhat Dimensionality Reduction, Feature Extraction, and Manifold Learning | 159In[37]: grotesquecompn = 3# sort by 3rd component, plot first 10 images. Let’s look at the images for which these four components are particularly strong. Figures 3-16 and 3-17: Dimensionsality Reduction and Feature Extracted. Figures 2-3: Manifolds, and Figures 4-5: Faces that have a large coefficient for component 3. Figures 6-7: Faces with large coefficients for component 4. T-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Faces that have a large coefficient for component 7 feature space farther apart are more likely to be faces that are neighbors to each other. We will use PCA to visualize the data reduced to two dimensions. Figure 3-20 shows an example image of a handwritten digit between 0 and 1 for each class. We use the t-Sne manifold learning algorithm on a dataset of handwritten dig‐centricits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐scale image of an 8-digit digit. In Figure 3-21, we build a PCA model and plot the first two principal components. We color each dot by its class and plot it as text instead of using a scatter plot. We actually used the true digit classes as glyphs, to show which class is where. The PCA method is often a good first approach for transforming your data. But the nature of the method (applying arotation and then dropping directions) limits its usefulness, as we saw with the scatter plot of the Labeled Faces in the Wild dataset. For more information on PCA, visit PCA.org. For other data decomposition methods, visit the PCA site or the Data Decomposition Wiki. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. Some of them, including t-SNE, com‐                pute a new representation of the training data, but don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. Manifoldlearning can be useful for explora‐                tory data analysis, but is rarely used if the final goal is supervised learning. A particularly useful one is the t- SNE algorithm, which allows for much more complex map-like visualizations of training data.  t-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The results are quite remarkable, and the results can be seen in the video below. The video shows the results of three different methods: Feature Extraction, Dimensionality Reduction, and Manifold Learning. The result is quite remarkable. The videos can be The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. 3. Unsupervised Learning and Preprocessing. 3.3. The Decision Function. 4. The Multiclass Classification. 5. The Classification of Classifiers. 6. The Classification of Classifications. 7. The classification of Classifieds. 8. The classification of classifiers. 9. The The Effect of Preprocessing on Supervised Learning and Scaling Training and Test Data the Same Way. Manifold Learning with t-SNE and Manifolds. The Effect of Data Transformations on Manifolding Learning. The Effects of Data Transformation on Super supervised Learning and the Effect ofData Transformations On Supervised learning. The effect of data transformations on the effect of training and test data the same way. The effects of data transformation on the impact of testing and training data. The impact of data transformations on training and testing data Let’s apply t-SNE to the same dataset, and compare the results. Most of the other digits overlap significantly. The TSNE class has no transform method. As t- SNE does not support transforming new data, we use the TSNE The fit_transform method will build the model and immediately return the transformed data. Instead of fit, we can call the fit_ transform method, which will call the model. We plot the digits as text instead of using scatter using plt. Figure 3-22 shows the plot of the digits with different colors and fontdicts. We also plot the numbers as text using the text-based text-blend tool, which is also used in the TensorFlow tool. The result of t-SNE is quite remarkable. Scatter plot of the digits dataset using two components found by t- The ones and nines are somewhat split up, but most of the classes form a single dense dense. All the classes are quite clearly separated. The students are divided into three groups: the ones, the nines, the ones and the ones plus the ones. The ones Categorical Variables. Representing Data and Engineering Features. One-Hot-Encoding (Dummy Variables) One- hot-encoding is a form of binary code. Categoricals can be written as numbers. The table of Contents includes the table of numbers and a list of the features that can be used to code them. The",
                    "children": [
                        {
                            "id": "chapter-3-section-4-subsection-1",
                            "title": "Principal Component Analysis (PCA)",
                            "content": "Principal Component Analysis (PCA)\nPrincipal component analysis is a method that rotates the dataset in a way such that\nthe rotated features are statistically uncorrelated. This rotation is often followed by\nselecting only a subset of the new features, according to how important they are for\nexplaining the data. The following example (Figure 3-3) illustrates the effect of PCA\non a synthetic two-dimensional dataset:\nIn[13]:\nmglearn.plots.plot_pca_illustration()\nThe first plot (top left) shows the original data points, colored to distinguish among\nthem. The algorithm proceeds by first finding the direction of maximum variance,\nlabeled “Component 1.” This is the direction (or vector) in the data that contains most\nof the information, or in other words, the direction along which the features are most\ncorrelated with each other. Then, the algorithm finds the direction that contains the\nmost information while being orthogonal (at a right angle) to the first direction. In\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\nhead and the tail are; we could have drawn the first component from the center up to\n140 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nthe top left instead of down to the bottom right. The directions found using this pro‐\ncess are called principal components, as they are the main directions of variance in the\ndata. In general, there are as many principal components as original features.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\ncancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nYou can see that in the first component, all features have the same sign (it’s negative,\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\nThat means that there is a general correlation between all features. As one measure‐\nment is high, the others are likely to be high as well. The second component has\nmixed signs, and both of the components involve all of the 30 features. This mixing of\nall features is what makes explaining the axes in Figure 3-6 so tricky.\nEigenfaces for feature extraction\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\nbehind feature extraction is that it is possible to find a representation of your data\nthat is better suited to analysis than the raw representation you were given. A great\nexample of an application where feature extraction is helpful is with images. Images\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\nObjects in images are usually made up of thousands of pixels, and only together are\nthey meaningful.\nWe will give a very simple application of feature extraction on images using PCA, by\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\ncontains face images of celebrities downloaded from the Internet, and it includes\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\nscale versions of these images, and scale them down for faster processing. You can see\nsome of the images in Figure 3-7:\nIn[21]:\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\ncombinations of the original features. However, these combinations are usually very\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\ncomponents_ attribute of the PCA object during fitting:\nIn[18]:\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\nOut[18]:\nPCA component shape: (2, 30)\nEach row in components_ corresponds to one principal component, and they are sor‐\nted by their importance (the first principal component comes first, etc.). The columns\ncorrespond to the original features attribute of the PCA in this example, “mean\nradius,” “mean texture,” and so on. Let’s have a look at the content of components_:\nIn[19]:\nprint(\"PCA components:\\n{}\".format(pca.components_))\nOut[19]:\nPCA components:\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\nWe can also visualize the coefficients using a heat map (Figure 3-6), which might be\neasier to understand:\nIn[20]:\nplt.matshow(pca.components_, cmap='viridis')\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),\n           cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nIn[28]:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\nOut[28]:\nTest set accuracy: 0.36\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\nintuition that the principal components might provide a better representation of the\ndata.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n151\nFor image data, we can also easily visualize the principal components that are found.\nRemember that components correspond to directions in the input space. The input\nspace here is 50×37-pixel grayscale images, so directions within this space are also\n50×37-pixel grayscale images.\nLet’s look at the first couple of principal components (Figure 3-9):\nIn[29]:\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\nOut[29]:\npca.components_.shape: (100, 5655)\nIn[30]:\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape),\n              cmap='viridis')\n    ax.set_title(\"{}. component\".format((i + 1)))\nWhile we certainly cannot understand all aspects of these components, we can guess\nwhich aspects of the face images some of the components are capturing. The first\ncomponent seems to mostly encode the contrast between the face and the back‐\nground, the second component encodes differences in lighting between the right and\nthe left half of the face, and so on. While this representation is slightly more semantic\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\n# keep the first two principal components of the data\npca = PCA(n_components=2)\n# fit PCA model to breast cancer data\npca.fit(X_scaled)\n# transform data onto the first two principal components\nX_pca = pca.transform(X_scaled)\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\n144 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nOut[16]:\nOriginal shape: (569, 30)\nReduced shape: (569, 2)\nWe can now plot the first two principal components (Figure 3-5):\nIn[17]:\n# plot first vs. second principal component, colored by class\nplt.figure(figsize=(8, 8))\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\nplt.legend(cancer.target_names, loc=\"best\")\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\nprincipal components\nIt is important to note that PCA is an unsupervised method, and does not use any class\ninformation when finding the rotation. It simply looks at the correlations in the data.\nFor the scatter plot shown here, we plotted the first principal component against the\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n145\nsecond principal component, and then used the class information to color the points.\nYou can see that the two classes separate quite well in this two-dimensional space.\nThis leads us to believe that even a linear classifier (that would learn a line in this\nspace) could do a reasonably good job at distinguishing the two classes. We can also\nsee that the malignant (red) points are more spread out than the benign (blue) points\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\nwhat a human would perceive first. When asking people to rate similarity of faces,\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\nkeep in mind that algorithms often interpret data (particularly visual data, such as\nimages, which humans are very familiar with) quite differently from how a human\nwould.\n152 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-9. Component vectors of the first 15 principal components of the faces dataset\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\nmation as rotating the data and then dropping the components with low variance.\nAnother useful interpretation is to try to find some numbers (the new feature values\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\nprincipal components (see Figure 3-10).\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\ncomponents\nHere, x0, x1, and so on are the coefficients of the principal components for this data\npoint; in other words, they are the representation of the image in the rotated space.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n153\nAnother way we can try to understand what a PCA model is doing is by looking at\nthe reconstructions of the original data using only some components. In Figure 3-3,\nafter dropping the second component and arriving at the third panel, we undid the\nrotation and added the mean back to obtain new points in the original space with the\nsecond component removed, as shown in the last panel. We can do a similar transfor‐",
                            "summary": "Principal component analysis is a method that rotates the dataset in a way such that rotated features are statistically uncorrelated. This rotation is often followed byselecting only a subset of the new features, according to how important they are for explaining the data. The following example illustrates the effect of PCA on a synthetic two-dimensional dataset. The algorithm proceeds by first finding the direction (or vector) in the data that contains most of the information. Then, the algorithm finds the direction that contains the most information while being orthogonal (at a right angle) to the first direction. The first plot (top left) shows the original data points, colored to distinguish among them. In two dimensions, there is only one possible orientation that is at a right angle. In higher-dimensional spaces, there would be (infinitely) many orthogonal directions. The directions are called principal components, as they are the main directions of variance in the data. In general, there are as many principal components as original features. The second plot (top right) shows the same data, but now rotated so that the firstprincipal component aligns with the x-axis and the second principal component aligned with the y-axis. The first component is drawn from the center up to the top left instead of down to the bottom right of the plot. Figure 3-6. Heat map of the first two principal components on the Breast Cancer dataset. All features have the same sign (it’s negative, but it doesn’t matter which direction the arrow points in) The second component has mixed signs, and both of the components involve all of the 30 features. This mixing of features is what makes explaining the axes in Figure3-6 so tricky. Another application of PCA that we mentioned earlier is feature extraction. For feature extraction, use the feature extraction tool on the right side of the page. For more information on feature extraction and how to use it in your own data, see the ‘Feature Extraction’ section. Feature extraction is the process of finding a representation of your data that is better suited to analysis than the raw representation you were given. Imagesare made up of pixels, usually stored as red, green, and blue (RGB) intensities. We will give a very simple application of feature extraction on images using PCA, by working with face images from the Labeled Faces in the Wild dataset. This datasetcontains face images of celebrities downloaded from the Internet, and it includes faces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐scale versions of these images, and scale them down for faster processing.   We can use PCA for dimensionality reduction by retaining only some of the principal components. In the rotated representation found by PCA, the two axes are uncorrelated, meaning that the correlation matrix of the data in this representation is zero except for the diagonal diagonal. You can see                some of the images in Figure 3-7: worrisomeIn[21]: worrisomeIn the second plot, the data is rotated so that the first principal component aligns with the x-axis and the second principal component aligned with the y-axis. Before the rotation, the mean was subtracted from the data, so                that the transformed data In this example, we might keep only the first principal component, as shown in the third panel in Figure 3-3 (bottom left) This reduces the data from a two-dimensional to a one-dimensional dataset. Note, however, that instead of keeping only one of the original features, we found the most interesting direction and kept this direction. We can undo the rotation and add the mean back to the data. These points are in the original fea‐ipientture space, but we kept only the information contained in the first Principal Compo‐nent. This will resultin the data shown inthe last Applying PCA to the cancer dataset for visualization. One of the most common applications of PCA is visualizing high-dimensional data. The principal components correspond to directions in the original data, so they are combinations of the original features. However, these combinations are usually verycomplex, as we’ll see shortly. For the Iris dataset, we were able to create a pair plot that gave us a partial picture of the data by showing us all the possible configurations. The two axes in the plot are often not very easy to interpret, but we can still get a good idea of what the data is about by looking at the histograms in Figure 3-4. It is possible to use PCA for data that has more The principal components themselves are stored in the                components_ attribute of the PCA object during fitting. Each row in components_ corresponds to one principal component, and they are sor‐                ted by their importance. The columns “meanradius,” ‘mean texture,’ and so on correspond to the original features attribute of We can also visualize the coefficients using a heat map (Figure 3-6), which might beeasier to understand. Let’s have a look at the content of components_: grotesqueIn[19]: grotesqueprint(\"PCA components:\\n{}\".format(pca.components_) grotesqueOut[19): grotesqueprint (\"PCA. components: 0.219  0.104 0.228, 0.221  0,143, 0,239, 0,.261, 0.,138,064) grotesque Out[19] : grotesque Our accuracy improved quite significantly, from 26.6% to 35.7%. The principal components might provide a better representation of the data. For image data, we can also easily visualize the principal components that are found.Remember that components correspond to directions in the input space. The heat map of the first two principal components on the Breast Cancer dataset can be seen here. Let’s look at the first couple of principal components (Figure 3-9) The input space here is 50×37-pixel grayscale images, so directions within this space are also 50x37- pixel images. The first component seems to mostly encode the contrast between the face and the back ground. The second component encodes differences in lighting between the right and the left half of the face, and so on. We certainly cannot understand all aspects of these components, but we can guess which aspects of face images some of the components are capturing. The PCA model is based on pixels, the alignment of the face (the position of eyes,chin, and nose) and the lighting both have a strong influence on how similar two images are in their pixel representation. While this representation is slightly more semantic than the raw pixel values, it is still quite far from how a human might perceive a face. We can now plot the first two principal components (Figure 3-5):. plot first vs. first. plot second vs. second. plot third vs. third. plot fourth vs. fourth. plot fifth vs. fifth. plot sixth vs. sixth. plot seventh vs. seventh. plot eighth vs. ninth. plot tenth vs. tenth Figure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first and second principal components. PCA is an unsupervised method, and does not use any class information when finding the rotation. It is important to note that PCA does not used any class The two classes separate quite well in this two-dimensional space. This leads us to believe that even a linear classifier could do a reasonably good job at distinguishing the two classes. We can also see that the malignant (red) points are more spread out than the benign (blue) points. The principal components correspond to directions in the original data, so they areabeled in Figure 3-12. A downside of PCA is that the two axes in the plot are often not very easy to interpret. It simply looks at the correlations in the data. It is possible to use PCA to learn a line in this space by looking at the correlation between two points in the same space. Unsupervised learning algorithm PCA can be used to extract useful features. It works similarly to PCA and can also be used for dimensionality reduction. We are trying to write each data point as aweighted sum of some components, as illustrated in Figure 3-10. Non-Negative Matrix Factorization (NMF) is another unsupervisedlearning algorithm that can be also used for feature extraction. The results of NMF can be seen in the next section of this article, which includes the results of Manifold Learning for the cancer dataset. For more information on NMF, see http://www.manifoldlearning.org/. NMF decomposes data into a non-negative weighted sum. It is particularly helpful for data that is created as the addition (or overlay) of several independent sources. In these situations, NMF can identify the original components that make up the combined data. NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret can‐ autoimmunecellation effects. The method can only be applied to data where each feature is non- negative, as a non-'negative sum' cannotbecome negative. The process of decomposing data is particularlyhelpful fordata that is added (or overlaid) of multiple independent sources, such as an audio track of multiple people speaking, As the PCA model is based on pixels, the alignment of the face (the position of eyes, and the lighting both have a strong influence on how similar two pixel representation are in their pixel representation) But alignment and lighting are probably not what a human would perceive first. When asking people to rate similarity of faces, they are more likely to use attributes like age, gender, facial expression, and hair style, which are hard to infer from the pixel intensities. It’s important tokeep in mind that algorithms often interpret data (particularly visual data, such as images, which humans are very familiar with) quite differently from how a humanwould. The eigenfaces in Figure 3-9, for example, contain both The PCA transfor‐                mation as rotating the data and then dropping the components with low variance. Another useful interpretation is to try to find some numbers (the new feature values) so that we can express the test points as a weighted sum of the principal components. Another way we can try to understand what a PCA model is doing is by looking at the reconstructions of the original data using only some components. The coefficients of these principal components are the representation of the image in the rotated space and are the key to understanding PCA. The PCA algorithm is based on the principle of Dimensionality Reduction, Feature Extraction, and Manifold Learning In Figure 3-3, we undid therotation and added the mean back to obtain new points in the original space with the second component removed. We can do a similar transfor‐.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-4-subsection-2",
                            "title": "Non-Negative Matrix Factorization (NMF)",
                            "content": "plt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\ndata.\nApplying NMF to synthetic data\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\nto be able to operate on the data. This means where the data lies relative to the origin\n(0, 0) actually matters for NMF. Therefore, you can think of the non-negative compo‐\nnents that are extracted as directions from (0, 0) toward the data.\nThe following example (Figure 3-13) shows the results of NMF on the two-\ndimensional toy data:\nIn[34]:\nmglearn.plots.plot_nmf_illustration()\n156 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-13. Components found by non-negative matrix factorization with two compo‐\nnents (left) and one component (right)\nFor NMF with two components, as shown on the left, it is clear that all points in the\ndata can be written as a positive combination of the two components. If there are\nenough components to perfectly reconstruct the data (as many components as there\nare features), the algorithm will choose directions that point toward the extremes of\nthe data.\nIf we only use a single component, NMF creates a component that points toward the\nmean, as pointing there best explains the data. You can see that in contrast with PCA,\nreducing the number of components not only removes some directions, but creates\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\nponents, where all the data can be explained perfectly, the randomness has little effect\n(though it might change the order or scale of the components). In more complex sit‐\nuations, there might be more drastic changes.\nApplying NMF to face images\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\nmain parameter of NMF is how many components we want to extract. Usually this is\nlower than the number of input features (otherwise, the data could be explained by\nmaking each pixel a separate component).\nFirst, let’s inspect how the number of components impacts how well the data can be\nreconstructed using NMF (Figure 3-14):\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n157\nIn[35]:\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\nFigure 3-14. Reconstructing three face images using increasing numbers of components\nfound by NMF\nThe quality of the back-transformed data is similar to when using PCA, but slightly\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\nfor finding interesting patterns within the data.\nAs a first look into the data, let’s try extracting only a few components (say, 15).\nFigure 3-15 shows the result:\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape))\n    ax.set_title(\"{}. component\".format(i))\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\nnents\nThese components are all positive, and so resemble prototypes of faces much more so\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\nthat component 3 shows a face rotated somewhat to the right, while component 7\nshows a face somewhat rotated to the left. Let’s look at the images for which these\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n159\nIn[37]:\ncompn = 3\n# sort by 3rd component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\ncompn = 7\n# sort by 7th component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\nFigure 3-16. Faces that have a large coefficient for component 3\n160 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-17. Faces that have a large coefficient for component 7",
                            "summary": "Unsupervised learning algorithm PCA can be used to extract useful features. Non-negative matrix factorization (NMF) can also be used for dimensionality reduction. Manifold Learning is another unsupervised algorithm that can extract features from a large set of data points. We are trying to write each data point as a weighted sum of some components, as illustrated in Figure 3-10. The results are shown in Figures 3-5 and 3-12. For the cancer dataset, we use only the first two principal components of the faces dataset. The whole data is just a big blob, with no separation of classes visible. We can see that even with 10 components, PCA only cap NMF decomposes data into a non-negative weighted sum. It is particularly helpful for data that is created as the addition (or overlay) of several independent sources. In these situations, NMF can identify the original components that make up the combined data. NMF leads to more interpretable components than PCA, as negative components and coefficients can lead to hard-to-interpret can‐ autoimmunecellation effects. The method can only be applied to data where each feature is non- negative, as a non-'negative sum' cannotbecome negative. The process of decomposing data is particularlyhelpful fordata that is added (or overlaid) of multiple independent sources, such as an audio track of multiple people speaking,  NMF leads to more interpretable components than PCA. Negative components and coefficients can lead to hard-to-interpret effects. We need to ensure that our data is positive for NMF to be able to operate on the data. The eigenfaces in Figure 3-9, for example, contain both positive and negative parts. This means where the data lies relative to the origin of the data actually matters for the NMF method. The sign is actually more important than the sign of the face in this case. The result is that the data can be interpreted as either positive or negative.  NMF creates a component that points toward the extremes of the data, as pointing there best explains the data. If there are enough components to perfectly reconstruct the data (as many components as there are features), the algorithm will choose directions that point toward the extreme. The following example shows the results of NMF on the two-dimensional toy data. The results are shown in Figure 3-13, which is an example of a non-negative matrix factorization with two compo'reporters and one component. For NMF with two components, as shown on the left, it is clear that all points in the data can be written as a positive combination of the two components. In contrast with PCA, reducing the number of components not only removes some directions, but creates an entirely different set of components. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component”: all components play an equal part. NMF uses a random initialization, which might lead to different results depending on the type of component you are trying to reduce. In the next section, we'll look at how to CNN.com will feature iReporter photos in a weekly Travel Snapshots gallery. Visit CNN.com/Travel each week for a new gallery of snapshots. Click here for NMF uses a random initialization, which might lead to different results depending on the random seed. Components in NMF are also not ordered in a specific way, so there is no “first non-negative component” NMF uses an entirely different set of components! All components play an equal part. NMF is based on the Haskell programming language. The main parameter of NMF is how many components we want to extract. Usually this islower than the number of input features. In more complex situations, there might be more drastic changes. The number of components impacts how well the data can be reconstructed using NMF (Figure 3-14):Dimensionality Reduction, Feature Extraction, and Manifold Learning. In[35]: atrophymglearn.plots.plot_nmf_faces(X_train, X_test, image_shape) Figure 3- 14. NMF reconstruction of the Labeled Faces in the Wild dataset. The data was taken from a set of images called ‘ The quality of the back-transformed data is similar to when using PCA, but slightly worse. This is expected, as PCA finds the optimum directions in terms of reconstruc  NMF is usually not used for its ability to reconstruct or encode data, but rather for finding interesting patterns within the data. As a first look into the data, let’s try extracting only a few components (say, 15). Figure 3-15 shows the result: grotesquely158, apologetically158, and properly158. Figure 3: Unsupervised Learning and Preprocessing is available at: http://www.sklearn.com/learn/features/unsupervised-learning-and-preprocessing.  NMF found the components found by NMF on the faces dataset when using 15 compo‐nents. These components are all positive, and so resemble prototypes of faces much more sothan the components shown for PCA in Figure 3-9. For example, one can clearly see that component 3 shows a face rotated somewhat Dimensionality Reduction, Feature Extraction, and Manifold Learning | 159In[37]: grotesquecompn = 3# sort by 3rd component, plot first 10 images. Let’s look at the images for which these four components are particularly strong. Figures 3-16 and 3-17: Dimensionsality Reduction and Feature Extracted. Figures 2-3: Manifolds, and Figures 4-5: Faces that have a large coefficient for component 3. Figures 6-7: Faces with large coefficients for component 4. Faces that have a large coefficient for component 7. ",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-4-subsection-3",
                            "title": "Manifold Learning with t-SNE",
                            "content": "feature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\nimage for each class:\nIn[43]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\n                         subplot_kw={'xticks':(), 'yticks': ()})\nfor ax, img in zip(axes.ravel(), digits.images):\n    ax.imshow(img)\n164 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-20. Example images from the digits dataset\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\nprincipal components, and color each dot by its class (see Figure 3-21):\nIn[44]:\n# build a PCA model\npca = PCA(n_components=2)\npca.fit(digits.data)\n# transform the digits data onto the first two principal components\ndigits_pca = pca.transform(digits.data)\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\nThe Decision Function                                                                                              120\nPredicting Probabilities                                                                                             122\nUncertainty in Multiclass Classification                                                                 124\nSummary and Outlook                                                                                                 127\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\nTypes of Unsupervised Learning                                                                                 131\nChallenges in Unsupervised Learning                                                                        132\nPreprocessing and Scaling                                                                                            132\nDifferent Kinds of Preprocessing                                                                             133\nApplying Data Transformations                                                                               134\nScaling Training and Test Data the Same Way                                                       136\nThe Effect of Preprocessing on Supervised Learning                                           138\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\nPrincipal Component Analysis (PCA)                                                                    140\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\ncolor = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nThe digits zero, six, and four are relatively well separated using the first two principal\ncomponents, though they still overlap. Most of the other digits overlap significantly.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n165\nFigure 3-21. Scatter plot of the digits dataset using the first two principal components\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\nsupport transforming new data, the TSNE class has no transform method. Instead, we\ncan call the fit_transform method, which will build the model and immediately\nreturn the transformed data (see Figure 3-22):\nIn[45]:\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\n# use fit_transform instead of fit, as TSNE has no transform method\ndigits_tsne = tsne.fit_transform(digits.data)\n166 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[46]:\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"t-SNE feature 0\")\nplt.xlabel(\"t-SNE feature 1\")\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236",
                            "summary": " t-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Figure 3-20 shows an example image of a handwritten digit between 0 and 1 for each class. Let’s use PCA to visualize the data reduced to two dimensions. We will use the PCA algorithm on a dataset of handwrittenDigits that is included in scikit-learn.2 Each data point in this dataset is an 8-by-8 gray‐scale image of an 8×8 handwritten digit. We’ll apply the t- SNE manifold learning algorithm on the digits dataset. In Figure 3-21, we build a PCA model and plot the first two principal components. We color each dot by its class and plot it as text instead of using a scatter plot. We actually used the true digit classes as glyphs, to show which class is where. The PCA method is often a good first approach for transforming your data. But the nature of the method (applying arotation and then dropping directions) limits its usefulness, as we saw with the scatter plot of the Labeled Faces in the Wild dataset. For more information on PCA, visit PCA.org. For other data decomposition methods, visit the PCA site or the Data Decomposition Wiki. Manifold learning algorithms are mainly aimed at visualization, and so are rarely used to generate more than two new features. Some of them, including t-SNE, com‐                pute a new representation of the training data, but don’t allow transformations of new data. This means these algorithms cannot be applied to a test set: rather, they can onlytransform the data they were trained for. Manifoldlearning can be useful for explora‐                tory data analysis, but is rarely used if the final goal is supervised learning. A particularly useful one is the t- SNE algorithm, which allows for much more complex map-like visualizations of training data.  t-SNE puts more emphasis on points that are close by, rather than preserving distances between far-apart points. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The results are quite remarkable, and the results can be seen in the video below. The video shows the results of three different methods: Feature Extraction, Dimensionality Reduction, and Manifold Learning. The result is quite remarkable. The videos can be The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are shown in triangles. 3. Unsupervised Learning and Preprocessing. 3.3. The Decision Function. 4. The Multiclass Classification. 5. The Classification of Classifiers. 6. The Classification of Classifications. 7. The classification of Classifieds. 8. The classification of classifiers. 9. The The Effect of Preprocessing on Supervised Learning and Scaling Training and Test Data the Same Way. Manifold Learning with t-SNE and Manifolds. The Effect of Data Transformations on Manifolding Learning. The Effects of Data Transformation on Super supervised Learning and the Effect ofData Transformations On Supervised learning. The effect of data transformations on the effect of training and test data the same way. The effects of data transformation on the impact of testing and training data. The impact of data transformations on training and testing data Let’s apply t-SNE to the same dataset, and compare the results. Most of the other digits overlap significantly. The TSNE class has no transform method. As t- SNE does not support transforming new data, we use the TSNE The fit_transform method will build the model and immediately return the transformed data. Instead of fit, we can call the fit_ transform method, which will call the model. We plot the digits as text instead of using scatter using plt. Figure 3-22 shows the plot of the digits with different colors and fontdicts. We also plot the numbers as text using the text-based text-blend tool, which is also used in the TensorFlow tool. The result of t-SNE is quite remarkable. Scatter plot of the digits dataset using two components found by t- The ones and nines are somewhat split up, but most of the classes form a single dense dense. All the classes are quite clearly separated. The students are divided into three groups: the ones, the nines, the ones and the ones plus the ones. The ones Categorical Variables. Representing Data and Engineering Features. One-Hot-Encoding (Dummy Variables) One- hot-encoding is a form of binary code. Categoricals can be written as numbers. The table of Contents includes the table of numbers and a list of the features that can be used to code them. The",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-4-subsection-4",
                            "title": "Comparison of Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-5",
                    "title": "Clustering",
                    "content": "In[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nindicate cluster membership. We specified that we are looking for three clusters, so\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\nassigned to the cluster center it is closest to (see “Assign Points (1)”). Next, the cluster\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\n(1)”). Then the process is repeated two more times. After the third iteration, the\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\nGiven new data points, k-means will assign each to the closest cluster center. The next\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\nin Figure 3-23:\nIn[48]:\nmglearn.plots.plot_kmeans_boundaries()\nClustering \n| \n169\n3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\nvalue.\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\nclass, and set the number of clusters we are looking for.3 Then we call the fit method\nwith the data:\nIn[49]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n# generate synthetic two-dimensional data\nX, y = make_blobs(random_state=1)\n# build the clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\nfind these labels in the kmeans.labels_ attribute:\n170 \n|\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n(that is, we have 10 new features), with all features being 0, apart from the one that\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\near model, which would not have been possible using the original two features. It is\nalso possible to get an even more expressive representation of the data by using the\ndistances to each of the cluster centers as features. This can be accomplished using\nthe transform method of kmeans:\nIn[60]:\ndistance_features = kmeans.transform(X)\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\nprint(\"Distance features:\\n{}\".format(distance_features))\nOut[60]:\nDistance feature shape: (200, 10)\nDistance features:\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\n ...,\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\nk-means is a very popular algorithm for clustering, not only because it is relatively\neasy to understand and implement, but also because it runs relatively quickly. k-\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\nOne of the drawbacks of k-means is that it relies on a random initialization, which\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\nlearn runs the algorithm 10 times with 10 different random initializations, and\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nlier. You might remember that PCA tries to find directions of maximum variance in\nthe data, while NMF tries to find additive components, which often correspond to\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\nresent each data point using a cluster center. You can think of that as each point being\nrepresented using only a single component, which is given by the cluster center. This\nview of k-means as a decomposition method, where each point is represented using a\nsingle component, is called vector quantization.\n176 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nLet’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\nnents extracted (Figure 3-30), as well as reconstructions of faces from the test set\nusing 100 components (Figure 3-31). For k-means, the reconstruction is the closest\ncluster center found on the training set:\nIn[57]:\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\nnmf = NMF(n_components=100, random_state=0)\nnmf.fit(X_train)\npca = PCA(n_components=100, random_state=0)\npca.fit(X_train)\nkmeans = KMeans(n_clusters=100, random_state=0)\nkmeans.fit(X_train)\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\nIn[58]:\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"Extracted Components\")\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐ returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nAgglomerative clustering produces what is known as a hierarchical clustering. The\nclustering proceeds iteratively, and every point makes a journey from being a single\npoint cluster to belonging to some final cluster. Each intermediate step provides a\nclustering of the data (with a different number of clusters). It is sometimes helpful to\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\nhow each cluster breaks up into smaller clusters:\nIn[63]:\nmglearn.plots.plot_agglomerative()\n184 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\ntive clustering, with numbered data points (cf. Figure 3-36)\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\nrelies on the two-dimensional nature of the data and therefore cannot be used on\ndatasets that have more than two features. There is, however, another tool to visualize\nhierarchical clustering, called a dendrogram, that can handle multidimensional\ndatasets.\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\nrithms. SciPy provides a function that takes a data array X and computes a linkage\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\nIn[64]:\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n5 We could also use the labels_ attribute, as we did for k-means.\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\nclosest are merged. In the first four steps, two single-point clusters are picked and\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\ning. As we specified that we are looking for three clusters, the algorithm then stops.\nLet’s have a look at how agglomerative clustering performs on the simple three-\ncluster data we used here. Because of the way the algorithm works, agglomerative\nclustering cannot make predictions for new data points. Therefore, Agglomerative\nClustering has no predict method. To build the model and get the cluster member‐\nships on the training set, use the fit_predict method instead.5 The result is shown\nin Figure 3-34:\nIn[62]:\nfrom sklearn.cluster import AgglomerativeClustering\nX, y = make_blobs(random_state=1)\nagg = AgglomerativeClustering(n_clusters=3)\nassignment = agg.fit_predict(X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nClustering \n| \n183\nFigure 3-34. Cluster assignment using agglomerative clustering with three clusters\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that regions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\nIf there are at least min_samples many data points within a distance of eps to a given\ndata point, that data point is classified as a core sample. Core samples that are closer\nto each other than the distance eps are put into the same cluster by DBSCAN.\nThe algorithm works by picking an arbitrary point to start with. It then finds all\npoints with distance eps or less from that point. If there are less than min_samples\npoints within distance eps of the starting point, this point is labeled as noise, meaning\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\nThen, all neighbors (within eps) of the point are visited. If they have not been\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\ngrows until there are no more core samples within distance eps of the cluster. Then\nanother point that hasn’t yet been visited is picked, and the same procedure is\nrepeated.\nClustering \n| \n187\nIn the end, there are three kinds of points: core points, points that are within distance\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\nis run on a particular dataset multiple times, the clustering of the core points is always\nthe same, and the same points will always be labeled as noise. However, a boundary\npoint might be neighbor to core samples of more than one cluster. Therefore, the\ncluster membership of boundary points depends on the order in which points are vis‐\nited. Usually there are only few boundary points, and this slight dependence on the\norder of points is not important. returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# accuracy is zero, as none of the labels are the same\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# adjusted rand score is 1, as the clustering is exactly the same\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\nOut[69]:\nAccuracy: 0.00\nARI: 1.00\nEvaluating clustering without ground truth\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\nthere is a big problem with using measures like ARI. When applying clustering algo‐\nrithms, there is usually no ground truth to which to compare the results. If we knew\nthe right clustering of the data, we could use this information to build a supervised\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\nin developing algorithms, not in assessing success in an application.\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\nhouette coefficient. However, these often don’t work well in practice. The silhouette\nscore computes the compactness of a cluster, where higher is better, with a perfect\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\nshapes.\nHere is an example comparing the outcome of k-means, agglomerative clustering,\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\nIn[70]:\nfrom sklearn.metrics.cluster import silhouette_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nClustering \n| \n193\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n    cmap=mglearn.cm3, s=60)\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nnot implemented in scikit-learn at the time of writing.\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\nknow if there is any semantic meaning in the clustering, or whether the clustering\n194 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nreflects an aspect of the data that we are interested in. Let’s go back to the example of\nface images. We hope to find groups of similar faces—say, men and women, or old\npeople and young people, or people with beards and without. Let’s say we cluster the\ndata into two clusters, and all algorithms agree about which points should be clus‐\ntered together. We still don’t know if the clusters that are found correspond in any\nway to the concepts we are interested in. It could be that they found side views versus\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\ntures taken with iPhones versus pictures taken with Android phones. The only way to\nknow whether the clustering corresponds to anything we are interested in is to ana‐\nlyze the clusters manually.\nComparing algorithms on the faces dataset\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\nWe will use the eigenface representation of the data, as produced by\nPCA(whiten=True), with 100 components:\nIn[71]:\n# extract eigenfaces from lfw data and transform data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100, whiten=True, random_state=0)\npca.fit_transform(X_people)\nX_pca = pca.transform(X_people)\nWe saw earlier that this is a more semantic representation of the face images than the\nraw pixels. It will also make computation faster. A good exercise would be for you to\nrun the following experiments on the original data, without PCA, and see if you find\nsimilar clusters.\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\njust discussed: procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.",
                    "summary": "The k-means algorithm assigns data points to the closest cluster center. The process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops. We specified that we are looking for three clusters. The algorithm was initialized by declaring three data points randomly as cluster cen‐paralleledters. The next three steps of the algorithm are shown in Figure 3-23. For more information, visit the K-Means website or see the paper ‘Unsupervised Learning and Preprocessing’ (http://www.kmeans.org/blog/2013/01/23/unsupervised-learning-and-pre Applying k-means with scikit-learn is quite straightforward. Here, we apply it to the synthetic data that we used for the preceding plots. We instantiate the KMeans class and set the number of clusters we are looking for. The next example shows the boundaries of the cluster centers that were learned in Figure 3-23. If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this method, but it could be useful in certain situations. We call the fit method with the data: kmeans.fit(X) (Figure 3-24) K-means, DBSCAN, and agglomerative cluster‐fledgeding. All three methods can be used on large, real-world datasets. They are rel‐ purposefully easy to understand, and allow for clustering into many clusters. You can find these labels in the kmeans.labels_ attribute:                170  procedure, and often most helpful in the exploratory phase of data analysis. The algorithms have somewhat different strengths, but all have a way of controlling the granularity of clustering. The most common way to use them is to use the cluster means method, where each data point is represented by its cluster center. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible configurations of the data. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster, and it can help automatically determine the number of clusters. The algorithm can be applied for exploratory data analysis and preprocessing. It can also be used to create complex cluster shapes, as we saw in the two Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. These tools can be the only ways to make sense of data in the absence of supervision information. Having the right representa­gtion of the data is often crucial for supervised or unsupervised learning K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐forming on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms is one of the challenges in applying clustering algorithms. It is very hard to assess how well an algorithm worked, and to compare outcomes between There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. We create a random cluster assignment for reference. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows the results of the cluster clustering and random assignment experiments. We have 10 new features, with all features being 0, apart from the one thatrepresents the cluster center the point is assigned to. The results are plotted on a plot of axes, with the axes on the left and the right side of the plot. The plot shows how the cluster cluster looks when the points are randomly assigned to two different clusters. The cluster center is the center of the point that was randomly assigned. Using this 10-dimensional repre‐sentation, it would now be possible to separate the two half-moon shapes using a linear model, which would not have been possible using the original two features. It isalso possible to get an even more expressive representation of the k-means is a very popular algorithm for clustering. It is relatively easy to understand and implement. It runs relatively quickly. It can be used to create a simple clustering algorithm. It uses the transform method of kmeans. This can be accomplished using the following code:distance_features.transform(X) distance_features = distance.features.shape.print(\"Distance features:\\n{}\".format(distance_ features.shape)Distance features: 0.922 K-means is a clustering algorithm that can be used to solve problems. It has some downsides, such as relying on a random seed to start the algorithm. It can also be used for decomposition methods like PCA and NMF. The MiniBatchKMeans class can handle very large datasets, and scikit-learn even includes a more scalable Variant of k-Means for large datasets. The algorithm can be seen as a form of vector quantization, or seeing k- means as decomposition of a data set. It is not possible to use k-meants to solve complex problems in a real-world application, but it can be useful for training models. PCA and NMF try to find additive components, which often correspond to “extremes” or “parts” of the data. k-means, on the other hand, tries to represent each data point using a cluster center. You can think of that as each point being represented using only a single component, which is given by the cluster. This view of k-Means as a decomposition method, where each point is represented using asingle component, is called vector quantization. The results are shown in Figure 3-30, as well as reconstructions of faces from the test set using 100 components (Figure 3-31). The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. For k-means, the reconstruction is the closest cluster center found on the training set. The reconstruction is also the closest clustering center on the learning set for k-Means. The results are shown in the figure below, which includes a plot of the feature extractions. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up the data so that points within a single cluster are very similar and points in different clusters are different. The algorithm is one of the simplest and most commonly used clustering algo­rithms. The k-means algorithm tries to find cluster centers that are representative of certain regions of the data. It alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of all data points that are assigned to it. The algorithm is finished when the assignment of instances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]: purposefullymglearn.plots.plot_kmeans_al In[55]: # generate some random cluster data and transform the data to be stretched. # plot the cluster assignments and cluster centers. # Plot the data and cluster assignments into features and centers. Figure 3-28. Colors are shown as black, white, red, blue, and green. For more information, see the Wikipedia article on Supervised Learning and Preprocessing. K-means fails to identify nonspherical clusters. It also performs poorly if the clusters have more complex shapes. Here, we would hope that the clustering algorithm can discover the two half-moonshapes. The algorithm was used to generate synthetic two_moons data (with less noise this time) The results are shown in Figure 3-29 of Chapter 2 of the book, \"K-Means and the Two-Moons Algorithm,\" which is available on Amazon.com for $99.99. For confidential support, call the Samaritans on 08 K-means is a clustering algorithm that is similar to decomposition methods like PCA and NMF. The downsides of k-Means are the relatively restrictiveassumptions made on the shape of clusters, and the requirement to specify the num‐                ber of clusters you are looking for. Next, we will look at two more clustering algorithms that improve upon these proper‐                ties in some ways.Agglomerative Clustering refers to a collection of clustering. algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until some stopping criterion is satis‐                �fied. There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. Agglomerative clustering produces what is known as a hierarchical clustering. Every point makes a journey from being a single point to belonging to some final cluster. Each intermediate step provides a different number of clusters for the final clustering of the data. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example), average or complete might work better. We will discuss next how to choose the right number for the algorithm to find clusters for a given data set. For more information, see Chapter 3: Unsupervised Learning and Preprocessing and Chapter 4: The Clustering Algorithm. Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐centricive clustering, with numbered data points (cf. Figure 3-36) The visualization provides a very detailed view of the hierarchical clustering. It is sometimes helpful to look at all possible clusterings jointly. The next example provides some insight into how each cluster breaks up into smaller clusters. The visualization cannot be used on datasets that have more than two features. There is another tool to visualize hierarchical clusters, called a dendrogram, that can handle multidimensionaldatasets. You can generate them easily using SciPy. SciPy provides a function that takes a data array X and computes a linkagearray, which encodes hierarchical cluster similarities. We can then feed this linkagearray into the scipy dendrogram function to plot the d endrogram. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐forming. All three methods can be used on large, real-world datasets, are rel‐phthalatively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths, but all have a way of controlling the granularity of clustering. The methods are often most helpful in the exploratory phase of data analysis. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster. Agglomerative clustering can provide a whole hierarchy of possible hierarchy of data. k-means allows for a char‐                acterization of the clusters using the cluster means. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. This chapter introduced a range of unsupervised learning algorithms that can be applied for exploratory data analysis and preprocessing. The next chapter will look at how these algorithms can be used in a variety of data analysis scenarios. The final chapter will focus on the use of these algorithms in Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. Agglomerative clustering iteratively joins the two closest clusters on a two-dimensional dataset. Preprocessing and decomposition methods play an important part in data prepa‐ curtailration. Even in a supervised setting, exploratory learning can be useful. The next section of the book will focus on unsupervised learning. The book is available in paperback and e-book versions. For more information, visit the book’s website or go to: http://www.k-means.com/book/unsupervised-learning.html. Let’s have a look at how agglomerative clustering performs on the simple three-cluster data we used here. In the first four steps, two single-point clusters are picked and these are joined into two-point clustering. In step 5, one of the two- point clusters is extended to a third point, and so on. As we specified that we are looking for three clusters, the algorithm then stops. Because of the way the algorithm In Figure 3-34, we use the fit_predict method to build the model and get the cluster member‐ships on the training set. The result is shown in Figure 3.5. We will discuss hierarchical clustering and dendrograms in the next section. We also discuss how to use the cluster labels to index another array. We conclude the article with an overview of the latest developments in supervised learning and preprocessing. We hope you will find this information helpful. Back to the page you came from.. The next section will focus on unsupervised Learning and Preprocessing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that there is no notion of cluster center in agglomerative clustering. We simply show the first couple of points in each cluster. Colors show a particular number of clusters that is a good fit. In[82] we show the number of points in each cluster to the left of the first image. Some of the clusters seem to have a semantic theme, many of them are too large to be actually homogeneous. We show the results of unsupervised learning and preprocessing in Figure 3-46. In[82) we show random images from the clusters generated by In. Each row corre‐porre‐sponds to one cluster; the number to the right lists number of images in that cluster. We also show the result of Unsupervised Learning and Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. To get more homogeneous clusters, we can run the algorithm again, this time with 40 clusters, and pick out some of the clusters that areparticularly interesting (Figure 3-47): In[86]: # extract clusters with ward agglomeratives clustering. We can compute the ARI to measure whether the two partitions of the data are similar. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have                little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to have little in common for k We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines DBSCAN tries to cluster all data points together. The algorithm works by picking an arbitrary point to start with. It then finds all points with distance eps or less from that point. If there are less than min_samples many data points within a distance of eps of the starting point, this point is labeled as noise, meaning that it doesn’t belong to any cluster. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that regions of data, separated by regions that are relatively empty. This is not surprising, given the results of DBSCAN, which tried to cluster the data. The DBSCAN clustering algorithm is used to identify core points and noise points. Core points are points that are within distance eps of core points (called boundary points) The same points will always be labeled as noise. However, a boundary point might be neighbor to core samples of more than one cluster. The clustering of the core points is always the same, and the same points are always labeled as Noise. The algorithm can be used to find core points in a large data set, and to identify noise points in large data sets. It can also be used for finding core samples in a larger data set. It is currently being tested on a large dataset with more than 100,000 points. K-means is a clustering algorithm based on the idea that each point is its own cluster. The algorithm merges the two most similar clusters until some stopping criterion is met. The downsides are the relatively restrictiveassumptions made on the shape of clusters, and the requirement to specify the num‐                ber of clusters you are looking for. Next, we will look at two more clustering algorithms that improve upon these proper‐                ties in some ways. The algorithms are called Agglomerative Clustering and Grouping Clustered Algorithm (ACCLA) and Grouped Clusters (GCHA) respectively. For more information on these algorithms, see www.ac There are several linkage criteria that specify how exactly the “most similar cluster” is measured. The stopping criterion implemented in scikit-learn is the number of clusters. This measure is always defined between two existing clusters. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela­tively equally sized. We will use ward in our examples to show how this works on most datasets, and we will use it in our example dataset for this article. We hope that this will help you with your own data analysis in the future. The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐centricing on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. When applying clustering algo‐rithms, there is usually no ground truth to which to compare the results. If you want to evaluate clustering algorithms without ground truth, you can use the clustering algorithm in Chapter 3: Unsupervised Learning and Preprocessing. For more information on the algorithm, please visit: http://www.npr.org/ Using metrics like ARI and NMI usually only helps in developing algorithms, not in assessing success in an application. The silhouettescore computes the compactness of a cluster, where higher is better, with a perfect score of 1. If we knew the right clustering of the data, we could use this information to build a supervised model like a classifier.  compact clusters are good, compactness doesn’t allow for complexshapes. Here is an example comparing the outcome of k-means, agglomerative clustering, and DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40): In[70]: At the bottom of the page, please share your data analysis results with us. We would like to hear from you. If you have a data analysis problem, please send it to: jennifer.smith@mailonline.co.uk. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k- means allows for a char‐                acterization of the clusters using the cluster means. DBS CAN lets you define proximity using the eps parameter, which indirectly influences cluster size. The three methods can be used on large, real-world datasets, are rel‐                atively easy to understand, and allow for clustering into many clusters. The algorithms have somewhat different strengths, but all have similar strengths. The results of the study are published in the open- DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. There are metrics that can be used to assess the outcome of a clustering algorithmrelative to a ground truth clustering. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐ hypertative measure between 0 and 1. We will now compare them on some real-world datasets to see how well they work. The results will be presented in the next section of the book, ‘Unsupervised Learning and Preprocessing’. The book is published by Oxford University Press, London, UK, priced £16.99. The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to havelittle in common for k-Means. We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that scikit-learn was not implemented at the time of writing. This is not surprising, given the results of DBSCAN, which tried to cluster all points together. We still don’t know if the clusters that are found correspond in any way to the concepts we are interested in. We hope to find groups of similar faces—say, men and women, or old people and young people, or people with beards and without. The clustering does not reveal any semantic meaning, or whether the points should be clus‐tered together. Let’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to theLabeled Faces in the Wild dataset. The only way to know whether the clustering corresponds to anything we are interested in is to ana‐ishlyze the clusters manually. We will use the eigenface representation of the data, as produced by the PCA algorithm. This is a more semantic representation of face images than theraw pixels. It will also make computation faster. The results will be published in the next version of this blog post, which will be updated with the latest data from sklearn.com and the latest version of the pca algorithm, as well as the latest pca data. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k- means allows for a char‐                acterization of the clusters using the cluster means. DBS CAN lets you define proximity using the eps parameter, which indirectly influences cluster size. All 3 methods can be used on large, real-world datasets, are rel‐                ceratively easy to understand, and allow for clustering into many clusters. A good exercise would be for you to run the following experiments on the original data, without PCA, and see if you find                similar clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible hierarchies of data. DBSCANallows for the detection of “noise points” that are not assigned any cluster, and it canhelp automatically determine the number of clusters. The algorithm can be applied for exploratory data analysis and preprocessing. It can also be viewed as a decom‐otypeposition method, where each data point is represented by its cluster center. This chapter introduced a range of unsupervised learning algorithms that can Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. These tools can be the only ways to make sense of data in the absence of supervision information. Having the right representa­gtion of the data is often crucial for supervised or unsupervised learning K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the Carbuncleclosest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]:                mglearn.plots.plot_kmeans_algorithm() grotesque168, grotesque190, grotesque168. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 We will look into two kinds of unsupervised learning in this chapter: transformationsof the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data which might be easier for humans or other machine learning algo‐rithms to understand compared to the original representation. We will also look at what it looks like when we randomly assign points to two clusters for comparison (see Figure 3-39): grotesqueClustering, grotesqueTransformation. And finally, we will look at how we use clustering to extract knowledge from this data. We'll look at the results of our clustering experiment in the next chapter. The next chapter will be published in the spring of A common application of unsupervised transformations is dimensionality reduction. Another application is finding the parts of the data that “make up” the data. An example of this is topic extraction on collections of documents. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media sites. For visualization purposes, unsuper supervised transformations can be used to reduce data to two dimensions for visualization purposes. For more information, visit the Open Data Project.    The OpenData Project is a project of the University of California, San Diego, and the California Institute of Technology.  For more details, go to the OpenDataproject.org. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐rithm two clusters get merged. The length of each branch also shows how far apart                the merged clusters are. There are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐                ters in the lefthand side of the plot. The site doesn't know which pictures show whom, and it                doesn't know how many different people appear in your photo collection. A sensibleapproach would be to extract all The longest branches in this dendrogram are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the tworemaining clusters into a single cluster again bridges a relatively large distance.Unfortunately, agglomerative clustering still fails at separating complex shapes like the two_moons dataset. But the same is not true for the next algorithm we will look at, DBSCAN. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori. DBSCAN works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The idea behind DBS CAN is that clusters form dense regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points)",
                    "children": [
                        {
                            "id": "chapter-3-section-5-subsection-1",
                            "title": "k-Means Clustering",
                            "content": "In[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nindicate cluster membership. We specified that we are looking for three clusters, so\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\nassigned to the cluster center it is closest to (see “Assign Points (1)”). Next, the cluster\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\n(1)”). Then the process is repeated two more times. After the third iteration, the\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\nGiven new data points, k-means will assign each to the closest cluster center. The next\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\nin Figure 3-23:\nIn[48]:\nmglearn.plots.plot_kmeans_boundaries()\nClustering \n| \n169\n3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\nvalue.\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\nclass, and set the number of clusters we are looking for.3 Then we call the fit method\nwith the data:\nIn[49]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n# generate synthetic two-dimensional data\nX, y = make_blobs(random_state=1)\n# build the clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\nfind these labels in the kmeans.labels_ attribute:\n170 \n|\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n(that is, we have 10 new features), with all features being 0, apart from the one that\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\near model, which would not have been possible using the original two features. It is\nalso possible to get an even more expressive representation of the data by using the\ndistances to each of the cluster centers as features. This can be accomplished using\nthe transform method of kmeans:\nIn[60]:\ndistance_features = kmeans.transform(X)\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\nprint(\"Distance features:\\n{}\".format(distance_features))\nOut[60]:\nDistance feature shape: (200, 10)\nDistance features:\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\n ...,\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\nk-means is a very popular algorithm for clustering, not only because it is relatively\neasy to understand and implement, but also because it runs relatively quickly. k-\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\nOne of the drawbacks of k-means is that it relies on a random initialization, which\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\nlearn runs the algorithm 10 times with 10 different random initializations, and\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nlier. You might remember that PCA tries to find directions of maximum variance in\nthe data, while NMF tries to find additive components, which often correspond to\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\nresent each data point using a cluster center. You can think of that as each point being\nrepresented using only a single component, which is given by the cluster center. This\nview of k-means as a decomposition method, where each point is represented using a\nsingle component, is called vector quantization.\n176 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nLet’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\nnents extracted (Figure 3-30), as well as reconstructions of faces from the test set\nusing 100 components (Figure 3-31). For k-means, the reconstruction is the closest\ncluster center found on the training set:\nIn[57]:\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\nnmf = NMF(n_components=100, random_state=0)\nnmf.fit(X_train)\npca = PCA(n_components=100, random_state=0)\npca.fit(X_train)\nkmeans = KMeans(n_clusters=100, random_state=0)\nkmeans.fit(X_train)\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\nIn[58]:\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"Extracted Components\")\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐",
                            "summary": "The k-means algorithm assigns data points to the closest cluster center. The process is repeated two more times. After the third iteration, the assignment of points to cluster centers remained unchanged, so the algorithm stops. We specified that we are looking for three clusters. The algorithm was initialized by declaring three data points randomly as cluster cen‐paralleledters. The next three steps of the algorithm are shown in Figure 3-23. For more information, visit the K-Means website or see the paper ‘Unsupervised Learning and Preprocessing’ (http://www.kmeans.org/blog/2013/01/23/unsupervised-learning-and-pre Applying k-means with scikit-learn is quite straightforward. Here, we apply it to the synthetic data that we used for the preceding plots. We instantiate the KMeans class and set the number of clusters we are looking for. The next example shows the boundaries of the cluster centers that were learned in Figure 3-23. If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this method, but it could be useful in certain situations. We call the fit method with the data: kmeans.fit(X) (Figure 3-24) K-means, DBSCAN, and agglomerative cluster‐fledgeding. All three methods can be used on large, real-world datasets. They are rel‐ purposefully easy to understand, and allow for clustering into many clusters. You can find these labels in the kmeans.labels_ attribute:                170  procedure, and often most helpful in the exploratory phase of data analysis. The algorithms have somewhat different strengths, but all have a way of controlling the granularity of clustering. The most common way to use them is to use the cluster means method, where each data point is represented by its cluster center. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possible configurations of the data. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster, and it can help automatically determine the number of clusters. The algorithm can be applied for exploratory data analysis and preprocessing. It can also be used to create complex cluster shapes, as we saw in the two Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. These tools can be the only ways to make sense of data in the absence of supervision information. Having the right representa­gtion of the data is often crucial for supervised or unsupervised learning K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐forming on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. Cluster assignment found by DBSCAN using the default value of eps=0.5. Comparing and Evaluating Clustering Algorithms is one of the challenges in applying clustering algorithms. It is very hard to assess how well an algorithm worked, and to compare outcomes between There are metrics that can be used to assess the outcome of a clustering algorithm. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI) Here, we compare the k-means, agglomerative clustering, and DBSCAN algorithms using ARI and NMI. We will then compare them on some real-world datasets. We create a random cluster assignment for reference. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows the results of the cluster clustering and random assignment experiments. We have 10 new features, with all features being 0, apart from the one thatrepresents the cluster center the point is assigned to. The results are plotted on a plot of axes, with the axes on the left and the right side of the plot. The plot shows how the cluster cluster looks when the points are randomly assigned to two different clusters. The cluster center is the center of the point that was randomly assigned. Using this 10-dimensional repre‐sentation, it would now be possible to separate the two half-moon shapes using a linear model, which would not have been possible using the original two features. It isalso possible to get an even more expressive representation of the k-means is a very popular algorithm for clustering. It is relatively easy to understand and implement. It runs relatively quickly. It can be used to create a simple clustering algorithm. It uses the transform method of kmeans. This can be accomplished using the following code:distance_features.transform(X) distance_features = distance.features.shape.print(\"Distance features:\\n{}\".format(distance_ features.shape)Distance features: 0.922 K-means is a clustering algorithm that can be used to solve problems. It has some downsides, such as relying on a random seed to start the algorithm. It can also be used for decomposition methods like PCA and NMF. The MiniBatchKMeans class can handle very large datasets, and scikit-learn even includes a more scalable Variant of k-Means for large datasets. The algorithm can be seen as a form of vector quantization, or seeing k- means as decomposition of a data set. It is not possible to use k-meants to solve complex problems in a real-world application, but it can be useful for training models. PCA and NMF try to find additive components, which often correspond to “extremes” or “parts” of the data. k-means, on the other hand, tries to represent each data point using a cluster center. You can think of that as each point being represented using only a single component, which is given by the cluster. This view of k-Means as a decomposition method, where each point is represented using asingle component, is called vector quantization. The results are shown in Figure 3-30, as well as reconstructions of faces from the test set using 100 components (Figure 3-31). The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. For k-means, the reconstruction is the closest cluster center found on the training set. The reconstruction is also the closest clustering center on the learning set for k-Means. The results are shown in the figure below, which includes a plot of the feature extractions. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. The goal is to split up the data so that points within a single cluster are very similar and points in different clusters are different. The algorithm is one of the simplest and most commonly used clustering algo­rithms. The k-means algorithm tries to find cluster centers that are representative of certain regions of the data. It alternates between two steps: assigning each data point to the closest cluster center, and then setting each cluster center as the mean of all data points that are assigned to it. The algorithm is finished when the assignment of instances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]: purposefullymglearn.plots.plot_kmeans_al In[55]: # generate some random cluster data and transform the data to be stretched. # plot the cluster assignments and cluster centers. # Plot the data and cluster assignments into features and centers. Figure 3-28. Colors are shown as black, white, red, blue, and green. For more information, see the Wikipedia article on Supervised Learning and Preprocessing. K-means fails to identify nonspherical clusters. It also performs poorly if the clusters have more complex shapes. Here, we would hope that the clustering algorithm can discover the two half-moonshapes. The algorithm was used to generate synthetic two_moons data (with less noise this time) The results are shown in Figure 3-29 of Chapter 2 of the book, \"K-Means and the Two-Moons Algorithm,\" which is available on Amazon.com for $99.99. For confidential support, call the Samaritans on 08  vector quantization, or seeing k-means as decomposition. However, this is not possible using the k- means algorithm. There are interesting parallels between k-Means and",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-5-subsection-2",
                            "title": "Agglomerative Clustering",
                            "content": "returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nAgglomerative clustering produces what is known as a hierarchical clustering. The\nclustering proceeds iteratively, and every point makes a journey from being a single\npoint cluster to belonging to some final cluster. Each intermediate step provides a\nclustering of the data (with a different number of clusters). It is sometimes helpful to\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\nhow each cluster breaks up into smaller clusters:\nIn[63]:\nmglearn.plots.plot_agglomerative()\n184 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\ntive clustering, with numbered data points (cf. Figure 3-36)\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\nrelies on the two-dimensional nature of the data and therefore cannot be used on\ndatasets that have more than two features. There is, however, another tool to visualize\nhierarchical clustering, called a dendrogram, that can handle multidimensional\ndatasets.\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\nrithms. SciPy provides a function that takes a data array X and computes a linkage\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\nIn[64]:\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n5 We could also use the labels_ attribute, as we did for k-means.\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\nclosest are merged. In the first four steps, two single-point clusters are picked and\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\ning. As we specified that we are looking for three clusters, the algorithm then stops.\nLet’s have a look at how agglomerative clustering performs on the simple three-\ncluster data we used here. Because of the way the algorithm works, agglomerative\nclustering cannot make predictions for new data points. Therefore, Agglomerative\nClustering has no predict method. To build the model and get the cluster member‐\nships on the training set, use the fit_predict method instead.5 The result is shown\nin Figure 3-34:\nIn[62]:\nfrom sklearn.cluster import AgglomerativeClustering\nX, y = make_blobs(random_state=1)\nagg = AgglomerativeClustering(n_clusters=3)\nassignment = agg.fit_predict(X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nClustering \n| \n183\nFigure 3-34. Cluster assignment using agglomerative clustering with three clusters\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that",
                            "summary": "K-means has some downsides, such as the requirement to specify the number of clusters you are looking for. Agglomerative clustering uses the same principles, but merges the two most similar clusters until some stopping criterion is met. The stopping criterion implemented in scikit-learn is the number. of clusters, so similar clusters are merged until only the specified number of. clusters are left. The algorithm starts by declaring each point its own cluster,and then merges it with the next most similar point to get the best result. The goal is to merge the most similar points into a single cluster that is the most “like” to the previous one. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our Agglomerative clustering produces what is known as a hierarchical clustering. Every point makes a journey from being a single point to belonging to some final cluster. Each intermediate step provides a different number of clusters for the final clustering of the data. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example), average or complete might work better. We will discuss next how to choose the right number for the algorithm to find clusters for a given data set. For more information, see Chapter 3: Unsupervised Learning and Preprocessing and Chapter 4: The Clustering Algorithm. Figure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐centricive clustering, with numbered data points (cf. Figure 3-36) The visualization provides a very detailed view of the hierarchical clustering. It is sometimes helpful to look at all possible clusterings jointly. The next example provides some insight into how each cluster breaks up into smaller clusters. The visualization cannot be used on datasets that have more than two features. There is another tool to visualize hierarchical clusters, called a dendrogram, that can handle multidimensionaldatasets. You can generate them easily using SciPy. SciPy provides a function that takes a data array X and computes a linkagearray, which encodes hierarchical cluster similarities. We can then feed this linkagearray into the scipy dendrogram function to plot the d endrogram. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐forming. All three methods can be used on large, real-world datasets, are rel‐phthalatively easy to understand, and allow for clustering into many clusters. Each of the algorithms has somewhat different strengths, but all have a way of controlling the granularity of clustering. The methods are often most helpful in the exploratory phase of data analysis. DBSCAN allows for the detection of ‘noise points’ that are not assigned any cluster. Agglomerative clustering can provide a whole hierarchy of possible hierarchy of data. k-means allows for a char‐                acterization of the clusters using the cluster means. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. This chapter introduced a range of unsupervised learning algorithms that can be applied for exploratory data analysis and preprocessing. The next chapter will look at how these algorithms can be used in a variety of data analysis scenarios. The final chapter will focus on the use of these algorithms in Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. Agglomerative clustering iteratively joins the two closest clusters on a two-dimensional dataset. Preprocessing and decomposition methods play an important part in data prepa‐ curtailration. Even in a supervised setting, exploratory learning can be useful. The next section of the book will focus on unsupervised learning. The book is available in paperback and e-book versions. For more information, visit the book’s website or go to: http://www.k-means.com/book/unsupervised-learning.html. Let’s have a look at how agglomerative clustering performs on the simple three-cluster data we used here. In the first four steps, two single-point clusters are picked and these are joined into two-point clustering. In step 5, one of the two- point clusters is extended to a third point, and so on. As we specified that we are looking for three clusters, the algorithm then stops. Because of the way the algorithm In Figure 3-34, we use the fit_predict method to build the model and get the cluster member‐ships on the training set. The result is shown in Figure 3.5. We will discuss hierarchical clustering and dendrograms in the next section. We also discuss how to use the cluster labels to index another array. We conclude the article with an overview of the latest developments in supervised learning and preprocessing. We hope you will find this information helpful. Back to the page you came from.. The next section will focus on unsupervised Learning and Preprocessing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that there is no notion of cluster center in agglomerative clustering. We simply show the first couple of points in each cluster. Colors show a particular number of clusters that is a good fit. In[82] we show the number of points in each cluster to the left of the first image. Some of the clusters seem to have a semantic theme, many of them are too large to be actually homogeneous. We show the results of unsupervised learning and preprocessing in Figure 3-46. In[82) we show random images from the clusters generated by In. Each row corre‐porre‐sponds to one cluster; the number to the right lists number of images in that cluster. We also show the result of Unsupervised Learning and Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. To get more homogeneous clusters, we can run the algorithm again, this time with 40 clusters, and pick out some of the clusters that areparticularly interesting (Figure 3-47): In[86]: # extract clusters with ward agglomeratives clustering. We can compute the ARI to measure whether the two partitions of the data are similar. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have                little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to have little in common for k We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that. ",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-5-subsection-3",
                            "title": "DBSCAN",
                            "content": "regions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\nIf there are at least min_samples many data points within a distance of eps to a given\ndata point, that data point is classified as a core sample. Core samples that are closer\nto each other than the distance eps are put into the same cluster by DBSCAN.\nThe algorithm works by picking an arbitrary point to start with. It then finds all\npoints with distance eps or less from that point. If there are less than min_samples\npoints within distance eps of the starting point, this point is labeled as noise, meaning\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\nThen, all neighbors (within eps) of the point are visited. If they have not been\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\ngrows until there are no more core samples within distance eps of the cluster. Then\nanother point that hasn’t yet been visited is picked, and the same procedure is\nrepeated.\nClustering \n| \n187\nIn the end, there are three kinds of points: core points, points that are within distance\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\nis run on a particular dataset multiple times, the clustering of the core points is always\nthe same, and the same points will always be labeled as noise. However, a boundary\npoint might be neighbor to core samples of more than one cluster. Therefore, the\ncluster membership of boundary points depends on the order in which points are vis‐\nited. Usually there are only few boundary points, and this slight dependence on the\norder of points is not important.",
                            "summary": "The algorithm works by picking an arbitrary point to start with. It then finds all nearby data points with distance eps or less from that point. If there are more than min_samples points within a distance of eps, the point is labeled a core sample and assigned a new cluster label. The algorithm then visits all neighbors (within eps) of the point to find out if they are in the same cluster. If they are not, they are labeled as noise, meaning that they don’t belong to any cluster. A cluster is a collection of regions of data, separated by regions that are relatively empty. Core samples that are closer to each other than thedistance eps are put into the same Cluster by D Clustering involves visiting core samples of a cluster. If they are core samples, their neighbors are visited in turn, and so on. The cluster grows until there are no more core samples within distance eps of the cluster. Then another point that hasn’t yet been visited is picked, and the same procedure isrepeated. When the DBSCAN algorithm is run on a particular dataset multiple times, the clustering of the core points is always the same. The same points will always be labeled as noise. However, a boundarypoint might be neighbor tocore samples of more than one cluster. Usually there are only few boundary points, and this slight dependence on the order of points is not important.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-5-subsection-4",
                            "title": "Comparing and Evaluating Clustering Algorithms",
                            "content": "returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# accuracy is zero, as none of the labels are the same\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# adjusted rand score is 1, as the clustering is exactly the same\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\nOut[69]:\nAccuracy: 0.00\nARI: 1.00\nEvaluating clustering without ground truth\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\nthere is a big problem with using measures like ARI. When applying clustering algo‐\nrithms, there is usually no ground truth to which to compare the results. If we knew\nthe right clustering of the data, we could use this information to build a supervised\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\nin developing algorithms, not in assessing success in an application.\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\nhouette coefficient. However, these often don’t work well in practice. The silhouette\nscore computes the compactness of a cluster, where higher is better, with a perfect\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\nshapes.\nHere is an example comparing the outcome of k-means, agglomerative clustering,\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\nIn[70]:\nfrom sklearn.metrics.cluster import silhouette_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nClustering \n| \n193\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n    cmap=mglearn.cm3, s=60)\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nnot implemented in scikit-learn at the time of writing.\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\nknow if there is any semantic meaning in the clustering, or whether the clustering\n194 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nreflects an aspect of the data that we are interested in. Let’s go back to the example of\nface images. We hope to find groups of similar faces—say, men and women, or old\npeople and young people, or people with beards and without. Let’s say we cluster the\ndata into two clusters, and all algorithms agree about which points should be clus‐\ntered together. We still don’t know if the clusters that are found correspond in any\nway to the concepts we are interested in. It could be that they found side views versus\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\ntures taken with iPhones versus pictures taken with Android phones. The only way to\nknow whether the clustering corresponds to anything we are interested in is to ana‐\nlyze the clusters manually.\nComparing algorithms on the faces dataset\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\nWe will use the eigenface representation of the data, as produced by\nPCA(whiten=True), with 100 components:\nIn[71]:\n# extract eigenfaces from lfw data and transform data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100, whiten=True, random_state=0)\npca.fit_transform(X_people)\nX_pca = pca.transform(X_people)\nWe saw earlier that this is a more semantic representation of the face images than the\nraw pixels. It will also make computation faster. A good exercise would be for you to\nrun the following experiments on the original data, without PCA, and see if you find\nsimilar clusters.\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\njust discussed:",
                            "summary": "K-means has some downsides, such as the requirement to specify the number of clusters you are looking for. Agglomerative clustering uses the same principles, but merges the two most similar clusters until some stopping criterion is met. The stopping criterion implemented in scikit-learn is the number. of clusters, so similar clusters are merged until only the specified number of. clusters are left. The algorithm starts by declaring each point its own cluster,and then merges it with the next most similar point to get the best result. The goal is to merge the most similar points into a single cluster that is the most “like” to the previous one. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐centricing on a two-dimensional dataset. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for example), average or complete might work better. When applying clustering algo‐rithms, there is usually no ground truth to which to compare the results. If you want to evaluate clustering algorithms without ground truth, you can use the clustering algorithm in Chapter 3: Unsupervised Learning and Preprocessing. For more information on the algorithm, please visit: http://www.npr.org/ Using metrics like ARI and NMI usually only helps in developing algorithms, not in assessing success in an application. The silhouettescore computes the compactness of a cluster, where higher is better, with a perfect score of 1. If we knew the right clustering of the data, we could use this information to build a supervised model like a classifier.  compact clusters are good, compactness doesn’t allow for complexshapes. Here is an example comparing the outcome of k-means, agglomerative clustering, and DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40): In[70]: At the bottom of the page, please share your data analysis results with us. We would like to hear from you. If you have a data analysis problem, please send it to: jennifer.smith@mailonline.co.uk. We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k- means allows for a char‐                acterization of the clusters using the cluster means. DBS CAN lets you define proximity using the eps parameter, which indirectly influences cluster size. The three methods can be used on large, real-world datasets, are rel‐                atively easy to understand, and allow for clustering into many clusters. The algorithms have somewhat different strengths, but all have similar strengths. The results of the study are published in the open- DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. There are metrics that can be used to assess the outcome of a clustering algorithmrelative to a ground truth clustering. The most important ones are the adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐ hypertative measure between 0 and 1. We will now compare them on some real-world datasets to see how well they work. The results will be presented in the next section of the book, ‘Unsupervised Learning and Preprocessing’. The book is published by Oxford University Press, London, UK, priced £16.99. The result of t-SNE is quite remarkable. We also include what it looks like when we randomly assign points to two clusters for comparison. Figure 3-39 shows how the data looks when we plot random cluster assignments. Figure 4-5 shows the results when we use different clustering algorithms. Figure 5-6 shows what the results look like when two clusters are randomly assigned to each other. Figure 7-8 shows the result when the data is plotted against a random cluster assignment. Figure 9-10 shows the difference between two clusters with and without the same cluster. The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. The goal is to split up the data in such a way that points within a single cluster are very similar and points in different clusters are different. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. It can find a representation of theData in two dimensions that clearly separates the classes. K-Means clustering is one of the simplest and most commonly used clustering algorithms. It tries to find cluster centers that are representative of certain regions of the data. The algorithm alternates between two steps: assigning each data point to the cluster center, and then setting each cluster center as the mean of the points that are assigned to it. The following example illus‐trates the algorithm on a synthetic dataset. The data points are shown as circles, while the cluster centers are seen as triangles. The final step of the algorithm is to set each cluster to its own cluster center. This step is called the ‘closest cluster’ step and is the last step in the clustering algorithm. Agglomerative clustering also produces relatively equally sized clusters, with cluster sizes between 26 and 623. These are more uneven than those produced by k-means, but much more even than the ones produced by DBSCAN. An ARI of only 0.13 means that the two clusterings labels_agg and labels_km have little in common. This is not very surprising, given the fact that points further away from the cluster centers seem to havelittle in common for k-Means. We plot the dendrogram of agglomerative clustering on the faces dataset. We’ll limit the depth of the tree to 2,063 data points, as branching down to the individual data points would result in an unreadably dense plot. For the faces data, there doesn’t seem to be a very natural cutoff point. There are some branches that represent more distinct groups, but there is no good fit for a particular number of clusters that is a good fit. In the toy data shown in Figure 3-36, you could see by thelength of the branches that two or three clusters might capture the data appropriately. The tree at the very top, where there are 10 vertical lines Let’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that scikit-learn was not implemented at the time of writing. This is not surprising, given the results of DBSCAN, which tried to cluster all points together. We still don’t know if the clusters that are found correspond in any way to the concepts we are interested in. We hope to find groups of similar faces—say, men and women, or old people and young people, or people with beards and without. The clustering does not reveal any semantic meaning, or whether the points should be clus‐tered together. Let’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to theLabeled Faces in the Wild dataset. The only way to know whether the clustering corresponds to anything we are interested in is to ana‐ishlyze the clusters manually. We will use the eigenface representation of the data, as produced by the PCA algorithm. This is a more semantic representation of face images than theraw pixels. It will also make computation faster. The results will be published in the next version of this blog post, which will be updated with the latest data from sklearn.com and the latest version of the pca algorithm, as well as the latest pca data. Analyzing the faces dataset with DBSCAN. A good exercise would be for you to run the following experiments on the original data, without PCA, and see if you find similar clusters",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-5-subsection-5",
                            "title": "Summary of Clustering Methods",
                            "content": "procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.",
                            "summary": "We looked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐                ing. All three have a way of controlling the granularity of clustering. k-Means allows for a char‐rouacterization of the clusters using the cluster means. DBS CAN allows for the detection of ‘noise points’ that are not assigned any cluster. agglomersative clustering allows you to specify the number of desired clusters, whileDBSCAN lets you define proximity using the eps parameter, which indirectly influences cluster size. all three methods can be used on large, real-world datasets, and allow for clustering into many clusters. DBSCAN sometimes produces clusters of very differing size, which can be a strength or a weakness. Agglomerative clustering can provide a whole hierarchy of possiblepartitions of the data. Decomposition, manifold learning, and clustering are essential tools to further your understanding of your data. They can be the only ways to make sense of yourData in the absence of supervision information. Having the right representa‐protocol of theData is often crucial for supervised or unsupervised learning to succeed. Preprocessing and decomposition methods play an important part in data prepa‐Prototype and preprocessing. Data preprocessing can be used for exploratory data analysis and pre processing. K-means has some downsides, including the requirement to specify the number of clusters you are looking for. Agglomerative clustering builds upon the same principles, but merges the two most similar clusters until some stopping criterion is met. Even in a supervised setting, exploratory clustering returns the best result. We will look at two more clustering algorithms that improve upon these proper‐fledgedties in some ways. We hope this article will shed some light on the differences between these two algorithms and scikit-learn. We are happy to provide any further information on these algorithms that may be of interest to users of this article. We would like to hear from you about your experiences with these algorithms. The default choice, ward picks the two clusters to merge such that the variancewithin all clusters increases the least. This often leads to clusters that are rela‐tively equally sized. The two clusters that have the smallest average distance between all their points are merged. This measure is always defined between two existing clusters. The three choices are implemented in scikit-learn:ward, average and complete. The default choice ward works on most datasets, and we will use it in our The result of t-SNE is quite remarkable. All the classes are quite clearly separated. The ones and nines are somewhat split up, but most of the classes form a single dense group. Keep in mind that this method has no knowledge of the class labels: it is com‐pletely unsupervised. If the clusters have very dissimilar numbers of members (if one is much bigger than all the others, for. example, average or complete might work better), average may be better. The method is called agglomerative cluster‐forming The t-SNE algorithm has some tuning parameters, though it often works well with the default settings. You can try playing with perplexity and early_exaggeration, but the effects are usually minor. Still, it can find a representation of the data in two dimensions that clearly separates the classes, based solely on how close points are in the original space. The goal is to split up theData in such a way that points within a single cluster are very similar and points in different clusters are different. Similarly to clas‐ worrisomesification algorithms, clustering algorithms assign (or predict) a number to each data point, indicating which cluster a particular point belongs to. It tries to find cluster centers that are representative of certain Algorithm alternates between two steps: assigning each data point to the Carbuncleclosest cluster center, and then setting each cluster center as the mean of the datapoints that are assigned to it. The algorithm is finished when the assignment ofinstances to clusters no longer changes. The following example illus‐trates the algorithm on a synthetic dataset:. In[47]:                mglearn.plots.plot_kmeans_algorithm() grotesque168, grotesque190, grotesque168. We compare the k-means, agglomerative clustering, and DBSCAN algorithms. The metrics that can be used to assess the outcome of a clustering algorithm are adjusted randindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐tative measure between 0 and 1. We compare the clustering algorithms using ARI and NMI on some real-world datasets. The results are published in the open-source version of this article, which can be downloaded for free from the Apache Software Foundation website. For confidential support, call the Samaritans on 08457 909090 We will look into two kinds of unsupervised learning in this chapter: transformationsof the dataset and clustering. Unsupervised transformations of a dataset are algorithms that create a new representa­tion of the data which might be easier for humans or other machine learning algo‐rithms to understand compared to the original representation. We will also look at what it looks like when we randomly assign points to two clusters for comparison (see Figure 3-39): grotesqueClustering, grotesqueTransformation. And finally, we will look at how we use clustering to extract knowledge from this data. We'll look at the results of our clustering experiment in the next chapter. The next chapter will be published in the spring of A common application of unsupervised transformations is dimensionality reduction. Another application is finding the parts of the data that “make up” the data. An example of this is topic extraction on collections of documents. This can be useful for tracking the discussion of themes like elections, gun control, or pop stars on social media sites. For visualization purposes, unsuper supervised transformations can be used to reduce data to two dimensions for visualization purposes. For more information, visit the Open Data Project.    The OpenData Project is a project of the University of California, San Diego, and the California Institute of Technology.  For more details, go to the OpenDataproject.org. The y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐rithm two clusters get merged. The length of each branch also shows how far apart                the merged clusters are. There are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the other consisting of Points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐                ters in the lefthand side of the plot. The site doesn't know which pictures show whom, and it                doesn't know how many different people appear in your photo collection. A sensibleapproach would be to extract all The longest branches in this dendrogram are the three lines that are marked by the dashed line labeled “three clusters.” That these are the longestbranches indicates that going from three to two clusters meant merging some very far-apart points. We see this again at the top of the chart, where merging the tworemaining clusters into a single cluster again bridges a relatively large distance.Unfortunately, agglomerative clustering still fails at separating complex shapes like the two_moons dataset. But the same is not true for the next algorithm we will look at, DBSCAN. The main benefits of DBS CAN are that it does not require the user to set the number of clusters a priori. DBSCAN works by identifying points that are in “crowded” regions of the feature space. DBSCAN is somewhat slower than agglomerative clustering and k-means, but still scales to relatively large datasets. The idea behind DBS CAN is that clusters form dense regions of data, separated by regions that are relatively empty. Points that are within a dense region are called core samples (or core points)",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-6",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-3-section-6-subsection-1",
                            "title": "Chapter Review",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-6-subsection-2",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-6-subsection-3",
                            "title": "Advanced Topics",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-4",
            "title": "4. Representing Data and Engineering Features",
            "content": "these are usually not numeric. The distinction between categorical features and con‐\ntinuous features is analogous to the distinction between classification and regression,\nonly on the input side rather than the output side. Examples of continuous features\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\nExamples of categorical features are the brand of a product, the color of a product, or\nthe department (books, clothing, hardware) it is sold in. These are all properties that\ncan describe a product, but they don’t vary in a continuous way. A product belongs\neither in the clothing department or in the books department. There is no middle\nground between books and clothing, and no natural order for the different categories\n(books is not greater or less than clothing, hardware is not between books and cloth‐\ning, etc.).\nRegardless of the types of features your data consists of, how you represent them can\nhave an enormous effect on the performance of machine learning models. We saw in\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\nhelpful to augment your data with additional features, like adding interactions (prod‐\nucts) of features or more general polynomials.\nThe question of how to represent your data best for a particular application is known\nas feature engineering, and it is one of the main tasks of data scientists and machine\n211\nlearning practitioners trying to solve real-world problems. Representing your data in\nthe right way can have a bigger influence on the performance of a supervised model\nthan the exact parameters you choose.\nIn this chapter, we will first go over the important and very common case of categori‐\ncal features, and then give some examples of helpful transformations for specific 10.125     14.094     19.618     27.307]\n [     0.592      0.350      0.207      0.123      0.073      0.043\n       0.025      0.015      0.009      0.005]\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n    -159.516    329.222   -679.478   1402.367]]\nYou can obtain the semantics of the features by calling the get_feature_names\nmethod, which provides the exponent for each feature:\nInteractions and Polynomials \n| \n227\nIn[24]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[24]:\nPolynomial feature names:\n['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']\nYou can see that the first column of X_poly corresponds exactly to X, while the other\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\nferent from the rest.\nUsing polynomial features together with a linear regression model yields the classical\nmodel of polynomial regression (see Figure 4-5):\nIn[26]:\nreg = LinearRegression().fit(X_poly, y)\nline_poly = poly.transform(line)\nplt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-5. Linear regression with tenth-degree polynomial features\n228 \n| \nChapter 4: Representing Data and Engineering Features\nAs you can see, polynomial features yield a very smooth fit on this one-dimensional\ndata. However, polynomials of high degree tend to behave in extreme ways on the\nboundaries or in regions with little data.\nAs a comparison, here is a kernel SVM model learned on the original data, without\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different\noriginal features, as well as the square of each original feature. degree=2 here means\nthat we look at all features that are the product of up to two original features. The\nexact correspondence between input and output features can be found using the\nget_feature_names method:\nIn[29]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[29]:\nPolynomial feature names:\n['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6',\n'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2',\n230 \n| \nChapter 4: Representing Data and Engineering Features\n'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10',\n'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7',\n'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5',\n'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2',\n'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12',\n'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12',\n'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2',\n'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10',\n'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11',\n'x10 x12', 'x11^2', 'x11 x12', 'x12^2']\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n(\"x0^2\") and combinations of the first and the other features.\nLet’s compare the performance using Ridge on the data with and without interac‐\ntions:\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n92 \n| \nChapter 2: Supervised Learning\n10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\nA linear model for classification can only separate points using a line, and will not be\nable to do a very good job on this dataset (see Figure 2-37):\nIn[77]:\nfrom sklearn.svm import LinearSVC\nlinear_svm = LinearSVC().fit(X, y)\nmglearn.plots.plot_2d_separator(linear_svm, X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nNow let’s expand the set of input features, say by also adding feature1 ** 2, the\nsquare of the second feature, as a new feature. Instead of representing each data point\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\ndimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\nSupervised Machine Learning Algorithms \n| \n93\nFigure 2-37. Decision boundary found by a linear SVM\nIn[78]:\n# add the squared first feature\nX_new = np.hstack([X, X[:, 1:] ** 2])\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\nfigure = plt.figure()\n# visualize in 3D\nax = Axes3D(figure, elev=-152, azim=-26)\n# plot first all the points with y == 0, then all with y == 1\nmask = y == 0\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\nprediction to the polynomial regression without an explicit transformation of the\nfeatures.\nInteractions and Polynomials \n| \n229\nAs a more realistic application of interactions and polynomials, let’s look again at the\nBoston Housing dataset. We already used polynomial features on this dataset in\nChapter 2. Now let’s have a look at how these features were constructed, and at how\nmuch the polynomial features help. First we load the data, and rescale it to be\nbetween 0 and 1 using MinMaxScaler:\nIn[27]:\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split\n    (boston.data, boston.target, random_state=0)\n# rescale data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nNow, we extract polynomial features and interactions up to a degree of 2:\nIn[28]:\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\nX_train_poly = poly.transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nwhile the columns—the properties that describe these entities—are called features.\nLater in this book we will go into more detail on the topic of building a good repre‐\nsentation of your data, which is called feature extraction or feature engineering. You\nshould keep in mind, however, that no machine learning algorithm will be able to\nmake a prediction on data for which it has no information. For example, if the only\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\ndict their gender. This information is simply not contained in your data. If you add\nanother feature that contains the patient’s first name, you will have much better luck,\nas it is often possible to tell the gender by a person’s first name.\nKnowing Your Task and Knowing Your Data\nQuite possibly the most important part in the machine learning process is under‐\nstanding the data you are working with and how it relates to the task you want to\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\nit. It is necessary to understand what is going on in your dataset before you begin\nbuilding a model. Each algorithm is different in terms of what kind of data and what\nproblem setting it works best for. While you are building a machine learning solution,\nyou should answer, or at least keep in mind, the following questions:\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131 helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and gle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nhelpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237 the application or domain should be discarded. Often, domain experts can help in\nidentifying useful features that are much more informative than the initial represen‐\ntation of the data. Imagine you work for a travel agency and want to predict flight\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\ntions, and destinations. A machine learning model might be able to build a decent\nmodel from that. Some important factors in flight prices, however, cannot be learned.\nFor example, flights are usually more expensive during peak vacation months and\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\ntheir effect can therefore be learned from the date, others might depend on the phases\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\ndays). These events cannot be learned from the data if each flight is only recorded\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\nflight was on, preceding, or following a public or school holiday. In this way, prior\nknowledge about the nature of the task can be encoded in the features to aid a\nmachine learning algorithm. Adding a feature does not force a machine learning\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\ntive for flight prices, augmenting the data with this information doesn’t hurt.\nWe’ll now look at one particular case of using expert knowledge—though in this case\nit might be more rightfully called “common sense.” The task is predicting bicycle rent‐\nals in front of Andreas’s house.\nIn New York, Citi Bike operates a network of bicycle rental stations with a subscrip‐\ntion system. The stations are all over the city and provide a convenient way to get\naround. Bike rental data is made public in an anonymized form and has been ana‐\nlyzed in various ways. The task we want to solve is to predict for a given time and day",
            "summary": "Categorical features are properties that describe a product, but don't vary in a continuous way. Examples of continuous features are pixel brightnesses and size measurements of plant flowers. The distinction between categorical features and con­tinuous features is analogous to the distinction between classification and regression, only on the input side rather than the output side.Regardless of the types of features your data consists of, how you represent them can have an enormous effect on the performance of machine learning models. For more information on how to use machine learning in your data, see the Machine Learning for Data (MCLD) project and the MCLD training set (MLCD 2.0). For more data on machine learning for data, visit the The question of how to represent your data best for a particular application is known as feature engineering. It is one of the main tasks of data scientists and machine learning practitioners trying to solve real-world problems. We saw in                Chapters 2 and 3 that scaling of the data is important. We also saw in Chapter 2 that it can be                helpful to augment your data with additional features, like adding interactions (prod‐ucts) of features or more general polynomials. In this chapter, we will first go over the important and very common case of categori‐cal features, and then give some examples of helpful transformations. Representing your data in the right way can have a bigger influence on the performance of a supervised model than the exact parameters you choose. We will also look at how to get the semantics of the features by calling the get_feature_namesmethod, which provides the exponent for each feature. In the next section, we'll look at some of the most common and helpful transformations for specific categori­cal features. We'll end the chapter with a look at the next chapter, which will be about how to transform categori‑cal features for specific data. Using polynomial features together with a linear regression model yields the classicalmodel of polynometric regression. It’s interesting to see how large some of the values can get. The second column has entries above 20,000, orders of magnitude dif‐ferent from the rest. The result is a very smooth fit on this one-dimensional data set. For more information, visit the Data and Engineering section of the book. The next chapter is about how to use data and The data originally had 13 features, which were expanded into 105 interaction fea‐                tures. These new features represent all possible interactions between two different original features. The polynomials of high degree tend to behave in extreme ways on the                boundaries or in regions with little data. Here is a kernel SVM model learned on the original data, without any transformation (see Figure 4-6): grotesquely, the model is more accurate than the original model, which had no transformation. The model is also more robust than the model with a transformation, which has a much higher degree. The first new feature is a constant feature, called \"1\" here. The exact correspondence between input and output features can be found using the polynomialget_feature_names method. For example, the first input feature is \"x1\", followed by \"x2\", and then \"x3\", with \"x4\" and \"x11\" after \"x12\" The first output feature is called \"x7\", and the second is \"X8\", with the third being \"x9\" and the fourth being \"X12\" For more information on the Polynomial feature names method, seepolynomialfeaturenames.org. For more details on thePolynomial Feature Names method, visit polynom The synthetic dataset we used in “Feature importance in trees’ on page 77 is shown in Figure 2-29. The next 13 features are the original features (called \"x0\" to \"x12\"). Then follows the first feature squared(\"x0^2\") and combinations of the first and the other features. We picked this particular feature to add for illustration purposes. The choice is not particularly important. The performance using Ridge on the data with and without interac‐                tions:                or polynomials of the input A linear model for classification can only separate points using a line, and will not be able to do a very good job on this dataset. Let’s expand the set of input features, say by also adding feature1 ** 2, the square of the second feature, as a new feature. Instead of representing each data point as a two-dimensional point, (feature0, feature1), we now represent it as a three-dimensionalpoint, ( feature0,feature1, feature 1 ** 2).10 This new representa‐                tion is illustrated in Figure 2-38 in a 3-dimensional scatter plot. X_new = np.hstack([X, X[:, 1:] ** 2], c='b', c='r', marker='^', cmap=mglearn.cm2, s=60) X_new.scatter(X_ new[mask, 0], X.new[ mask, 1], X_ new [mask, 2), c=b, c=r) X. scatter(x_new[~ mask, 0), X. new[~mask, 1, X. mask, 2] X. Using a more complex model, a kernel SVM, we are able to learn a similarly complex prediction to the polynomial regression without an explicit transformation of the features. We already used polynomials on the Boston Housing dataset. Now let’s have a look at how these features were constructed, and at how much the poylnomials help. We are using a more realistic application of interactions and poyleomials The data originally had 13 features, which were expanded into 105 interaction fea­tures. First we load the data, and rescale it to be between 0 and 1 using MinMaxScaler. Then, we extract polynomial features and interactions up to a degree of 2. The results are shown in the figure below. The full code can be downloaded from the sklearn.com website. The code is available for download from the GitHub repository. Model-based feature selection uses a supervised machine learning model to judge the importance of each feature. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which directly encodes the importance. These new features represent all possible interactions between two different features. They can be helpful if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. It doesn’t need to be the same model that is used for the final Model-based selection considers all features at once, and so can capture interactions (if the model can capture them) This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. In contrast to univariate selection, model-basedselection considers allfeatures at once. It can also capture feature importances by considering the                absolute values. The SelectFromModel class selects all features that have an importance measure of greater than the provided thresh‐old. To use model-based feature selection, we need to use the transformer: grotesqueSelectFromModel. We used univariate feature selection to get a comparable result to what we got with automatic feature selection. We then compared the results to the results we got from the two different models. We also compared the two models to see if the results were comparable. To see the full report, go to: http://www.cnn.com/2013/01/24/science/features/features-and Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. In iterative feature selection, a series of models are built, with varying numbers of features. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. One particular method is recursive feature elimination (RFE), which starts with all features and builds a model, and discards the least important feature. These methods are much more computationally expensive than the methods we discussed previously, and need to be split into two sets of data to work out the best fit for each set of data. For example, we can use the data from the U.S. Bureau of The feature selection got better compared to the univariate and model-based selec­tion, but one feature was still missed. We use the same random forest model that we                used earlier, and get the results shown in Figure 4-11. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. For this to work, the model needs to provide some way to determine feature importance, as was the case with the model- based selection. For example, we can visualize the selected features by using a mask to show the shape of the feature. A random forest model is trained 40 times, once for each feature that is dropped. To make the task a bit harder, we’ll add some noninformative noise to the data. Let’s test the accuracy of the logisticregression model when using RFE for feature selection. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. Feature selection is based on a threshold of k features. We expect the feature selection to be able to identify the features that are noninformative and remove them. To make the task a bit harder, we’ll add some non informative noise to the data. The feature selection for classification on the                236                 |                  cancer dataset will be based on the threshold of k features. The threshold will be determined by a number of factors, such as Each entity or row here is known as a sample (or data point) in machine learning. The properties that describe these entities are called features. You might describe an image of a tumor by the grayscale values of each pixel. No machine learning algorithm will be able to make a prediction on data for which it has no information. For example, if the only feature that you have for a patient is their last name, no algorithm can pre‐dict their gender. This information is simply not contained in your data. Later in this book we will go into more detail on the topic of building a good repre‐sentation of your data, which is called feature extraction or feature engineering. Knowing Your Task and Knowing Your Data is the most important part of machine learning. It will not be effective to randomly choose an algorithm and throw your data at it. It is necessary to understand what is going on in your dataset before you begin building a model. Each algorithm is different in terms of what kind of data and what problem setting it works best for. If you add another feature that contains the patient’s first name, you will have much better luck. Often it is possible to tell the gender by a person’S first name. Machine learning is the process of building a machine learning solution to a problem. When building a solution, you should answer the following questions: What question(s) am I trying to answer? Do I think the data collected can answer that question? Have I collected enough data to represent the problem I want to solve? Are there enough data for the problem to be solved? Do you have any questions? If so, please share them with us in the comments below. We would like to hear from you about your machine learning solutions. Please send us your ideas in the form of a comment or email at jennifer.smith@mailonline.co.uk. Back to the page you came from. Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature, and keeps only the most important ones. This can be helpful if there are such a large number of features that building a model on them is infeasible. It can also be helpful, if you suspect that many features are completely uninformative. For more information, visit the MIT OpenAI website.  The OpenAI Project is open-source and free to use.  It is available in the U.S., Canada, Australia, New Zealand, England, France, Germany, Italy, Spain, Sweden, and the Netherlands. The feature selection model needs to provide some measure of importance for each feature. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which can be used to capture the importance of features. Linear models have a feature importances value that can be considered by considering the feature absolute values. This is used for sparse models, which only use a small subset of features, such as the L1 penalty learnable model in Chapter 2. The final supervised model doesn’t need to be the The SelectFromModel class selects all features that have an importance measure of greater than the provided thresh‐old. To get a comparable result to what we got with univariate feature selection, we need to use a model to select features. In contrast to univariate selection, model-based feature selection considers all features at once, and so can capture interactions (if the model can capture them). To use model- based feature selection we use the transformer:In[42]: grotesquelySelectFromModel. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached. Because a series of models are built, these methods are much morecomputationally expensive than the methods we discussed previously. For this to work, the model used for selection needs to provide some way to determine feature importance, as was the case for the model-based selection. One particularmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐phthaltures, builds a model, and discards the least important feature. Then a new model is built using all but the discarded feature, and so on until only a prespecified The feature selection got better compared to the univariate and model-based selec­tion, but one feature was still missed. Here, we use the same random forest model that we used earlier, and get the results shown in Figure 4-11. Running this code also takes significantly longer than that for the model- based selection, because a random Forest model is trained 40 times, once for each feature that is dropped. The results are shown in the figure below, and the code can be downloaded from the GitHub repository. Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. To make the task a bit harder, we’ll add some noninformative noisefeatures to the The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. The meth‐ocreods differ in how they compute this threshold, with the simplest ones being SelectKBest and SelectPercentile. To make the task a bit harder, we’ll add some noninformative noisefeatures to the data. Let’s apply the feature selection for classification on the                236  We expect the feature selection to be able to identify the features that are noninformative and remove them. We expect the features to be identified and removed by using the following criteria. The criteria include:Grid Search with Cross-Validation, Binary Classification, Multiclass Classification Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. Building Pipelines    305                Parameter Selection with Preprocessing.  306                Using Pipelines in Grid Searches.  309                The General Pipeline Interface.  314                Accessing Attributes in a Grid-Searched Pipeline.  315                Grid-Searching Preprocessing Steps and Model parameters.  317                Model Evaluation and Improvement.   317                                                                                                                                             “Pipelines’ In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached, or starting with all features and removing them. One particularmethod of this kind is recursive feature elimination (RFE), which builds a model, and discards the least important feature. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. These methods are much morecomputationally expensive than the methods we discussed previously. We will first intro‐idated cross-validation, a more robust way to assess generalization performance. The feature selection got better compared to the univariate and model-based selec‐tion, but one feature was still missed. Features selected by recursive feature elimination with the random forest classifier model got better. The results are shown in Figure 4-11 in the next section of the book. The book is called Representing Data and Engineering Features and is published by Piatkus & Co. in the U.S. and Europe. For more information on the book, visit: www.piatkus.com. A random forest model is trained 40 times, once for each feature that is dropped. Running this code also takes significantly longerthan that Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature and keeps only the most important ones. If you suspect that many features are completely uninformative, or if there are a large number of features that building a model on is infeasible, you may not want to build a model. The most important features to consider are: Grid Search, the most popular search engine, and the most common search terms. The feature selection model needs to provide some measure of importance for each feature. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which can be used to capture the importance of features. Linear models have a feature importances value that can be considered by considering the feature absolute values. This is used for sparse models, which only use a small subset of features, such as the L1 penalty learnable model in Chapter 2. The final supervised model doesn’t need to be the The SelectFromModel class selects all features that have an importance measure of the feature greater than the provided thresh‐old. To get a comparable result to what we got with univariate feature selection, we add some noninformative noise to the data. To make the task a bit harder, we’ll add someNonInformative Noise (NIS) to the dataset. To use model-based feature selection we need to use theSelectFromModel transformer:In[42]: grotesquely, we use the transformer to select all features. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. Often, domain experts can help in identifying useful features that are much more informative than the initial represen­en­tation of the data. For example, imagine you work for a travel agency and want to predict flight prices. Let’s say you have a record of prices together with dates, airlines, start loca­tions, and destinations.    The application or domain should be discarded. For more information, or to talk to a domain expert, visit: http://www.dns.org/dns/features/feature-selection. Some important factors in flight prices, however, cannot be learned. For example, flights are usually more expensive during peak vacation months andaround holidays. A machine learning model might be able to build a decent decentmodel from that. However, it is easy to add a feature that encodes whether a flight was on, preceding, or following a public or school holiday. In this way, prior knowledge about the nature of the task can be encoded in the features to aid amachine learning algorithm. The results of the study were published in the open-source journal, The Open Knowledge Project (http://www.openknowledgeproject.org/product/the-open-knowledge-project/2015/01/ In New York, Citi Bike operates a network of bicycle rental stations. The stations are all over the city and provide a convenient way to get around. Bike rental data is made public in an anonymized form. The task we want to solve is to predict for a given time and day. We’ll now look at one particular case of using expert knowledge—though in this case, it might be more rightfully called “common sense.” The task is predicting bicycle rent‐algorithm in front of Andreas’s house. It predicts that he will rent a bike at a certain time of the day.",
            "children": [
                {
                    "id": "chapter-4-section-1",
                    "title": "Categorical Variables",
                    "content": "these are usually not numeric. The distinction between categorical features and con‐\ntinuous features is analogous to the distinction between classification and regression,\nonly on the input side rather than the output side. Examples of continuous features\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\nExamples of categorical features are the brand of a product, the color of a product, or\nthe department (books, clothing, hardware) it is sold in. These are all properties that\ncan describe a product, but they don’t vary in a continuous way. A product belongs\neither in the clothing department or in the books department. There is no middle\nground between books and clothing, and no natural order for the different categories\n(books is not greater or less than clothing, hardware is not between books and cloth‐\ning, etc.).\nRegardless of the types of features your data consists of, how you represent them can\nhave an enormous effect on the performance of machine learning models. We saw in\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\nhelpful to augment your data with additional features, like adding interactions (prod‐\nucts) of features or more general polynomials.\nThe question of how to represent your data best for a particular application is known\nas feature engineering, and it is one of the main tasks of data scientists and machine\n211\nlearning practitioners trying to solve real-world problems. Representing your data in\nthe right way can have a bigger influence on the performance of a supervised model\nthan the exact parameters you choose.\nIn this chapter, we will first go over the important and very common case of categori‐\ncal features, and then give some examples of helpful transformations for specific",
                    "summary": "Categorical features are properties that describe a product, but don't vary in a continuous way. Examples of continuous features are pixel brightnesses and size measurements of plant flowers. The distinction between categorical features and con­tinuous features is analogous to the distinction between classification and regression, only on the input side rather than the output side.Regardless of the types of features your data consists of, how you represent them can have an enormous effect on the performance of machine learning models. For more information on how to use machine learning in your data, see the Machine Learning for Data (MCLD) project and the MCLD training set (MLCD 2.0). For more data on machine learning for data, visit the The question of how to represent your data best for a particular application is known as feature engineering. Representing your data in the right way can have a bigger influence on the performance of a supervised model than the exact parameters you choose. In this chapter, we will first go over the important and very common case of categori‐                cal features, and then give some examples of helpful transformations for specific. applications. We will end with an overview of some of the most common features that can be added to your data to make it more useful for machine learning practitioners. We hope that this chapter will help you understand how machine learning can be used to help you solve real-world problems. Back to the page you came from.",
                    "children": [
                        {
                            "id": "chapter-4-section-1-subsection-1",
                            "title": "Types of Categories",
                            "content": "these are usually not numeric. The distinction between categorical features and con‐\ntinuous features is analogous to the distinction between classification and regression,\nonly on the input side rather than the output side. Examples of continuous features\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\nExamples of categorical features are the brand of a product, the color of a product, or\nthe department (books, clothing, hardware) it is sold in. These are all properties that\ncan describe a product, but they don’t vary in a continuous way. A product belongs\neither in the clothing department or in the books department. There is no middle\nground between books and clothing, and no natural order for the different categories\n(books is not greater or less than clothing, hardware is not between books and cloth‐\ning, etc.).\nRegardless of the types of features your data consists of, how you represent them can\nhave an enormous effect on the performance of machine learning models. We saw in\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\nhelpful to augment your data with additional features, like adding interactions (prod‐\nucts) of features or more general polynomials.\nThe question of how to represent your data best for a particular application is known\nas feature engineering, and it is one of the main tasks of data scientists and machine\n211\nlearning practitioners trying to solve real-world problems. Representing your data in\nthe right way can have a bigger influence on the performance of a supervised model\nthan the exact parameters you choose.\nIn this chapter, we will first go over the important and very common case of categori‐\ncal features, and then give some examples of helpful transformations for specific",
                            "summary": "Categorical features are properties that describe a product, but don't vary in a continuous way. Examples of continuous features are pixel brightnesses and size measurements of plant flowers. The distinction between categorical features and con­tinuous features is analogous to the distinction between classification and regression, only on the input side rather than the output side.Regardless of the types of features your data consists of, how you represent them can have an enormous effect on the performance of machine learning models. For more information on how to use machine learning in your data, see the Machine Learning for Data (MCLD) project and the MCLD training set (MLCD 2.0). For more data on machine learning for data, visit the The question of how to represent your data best for a particular application is known as feature engineering. Representing your data in the right way can have a bigger influence on the performance of a supervised model than the exact parameters you choose. In this chapter, we will first go over the important and very common case of categori‐                cal features, and then give some examples of helpful transformations for specific. applications. We will end with an overview of some of the most common features that can be added to your data to make it more useful for machine learning practitioners. We hope that this chapter will help you understand how machine learning can be used to help you solve real-world problems. Back to the page you came from.",
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-1-subsection-2",
                            "title": "Encoding Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-1-subsection-3",
                            "title": "Handling Missing Values",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-2",
                    "title": "One-Hot-Encoding (Dummy Variables)",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-4-section-2-subsection-1",
                            "title": "Encoding Process",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-2-subsection-2",
                            "title": "Handling Missing Values",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-2-subsection-3",
                            "title": "Implementation Details",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-3",
                    "title": "Numbers Can Encode Categoricals",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-4-section-3-subsection-1",
                            "title": "Ordinal Encoding",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-3-subsection-2",
                            "title": "Label Encoding",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-3-subsection-3",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-4",
                    "title": "Binning, Discretization, Linear Models, and Trees",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-4-section-4-subsection-1",
                            "title": "Binning Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-4-subsection-2",
                            "title": "Discretization Techniques",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-4-subsection-3",
                            "title": "Impact on Models",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-5",
                    "title": "Interactions and Polynomials",
                    "content": "10.125     14.094     19.618     27.307]\n [     0.592      0.350      0.207      0.123      0.073      0.043\n       0.025      0.015      0.009      0.005]\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n    -159.516    329.222   -679.478   1402.367]]\nYou can obtain the semantics of the features by calling the get_feature_names\nmethod, which provides the exponent for each feature:\nInteractions and Polynomials \n| \n227\nIn[24]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[24]:\nPolynomial feature names:\n['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']\nYou can see that the first column of X_poly corresponds exactly to X, while the other\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\nferent from the rest.\nUsing polynomial features together with a linear regression model yields the classical\nmodel of polynomial regression (see Figure 4-5):\nIn[26]:\nreg = LinearRegression().fit(X_poly, y)\nline_poly = poly.transform(line)\nplt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-5. Linear regression with tenth-degree polynomial features\n228 \n| \nChapter 4: Representing Data and Engineering Features\nAs you can see, polynomial features yield a very smooth fit on this one-dimensional\ndata. However, polynomials of high degree tend to behave in extreme ways on the\nboundaries or in regions with little data.\nAs a comparison, here is a kernel SVM model learned on the original data, without\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different\noriginal features, as well as the square of each original feature. degree=2 here means\nthat we look at all features that are the product of up to two original features. The\nexact correspondence between input and output features can be found using the\nget_feature_names method:\nIn[29]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[29]:\nPolynomial feature names:\n['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6',\n'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2',\n230 \n| \nChapter 4: Representing Data and Engineering Features\n'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10',\n'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7',\n'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5',\n'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2',\n'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12',\n'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12',\n'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2',\n'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10',\n'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11',\n'x10 x12', 'x11^2', 'x11 x12', 'x12^2']\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n(\"x0^2\") and combinations of the first and the other features.\nLet’s compare the performance using Ridge on the data with and without interac‐\ntions:\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n92 \n| \nChapter 2: Supervised Learning\n10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\nA linear model for classification can only separate points using a line, and will not be\nable to do a very good job on this dataset (see Figure 2-37):\nIn[77]:\nfrom sklearn.svm import LinearSVC\nlinear_svm = LinearSVC().fit(X, y)\nmglearn.plots.plot_2d_separator(linear_svm, X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nNow let’s expand the set of input features, say by also adding feature1 ** 2, the\nsquare of the second feature, as a new feature. Instead of representing each data point\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\ndimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\nSupervised Machine Learning Algorithms \n| \n93\nFigure 2-37. Decision boundary found by a linear SVM\nIn[78]:\n# add the squared first feature\nX_new = np.hstack([X, X[:, 1:] ** 2])\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\nfigure = plt.figure()\n# visualize in 3D\nax = Axes3D(figure, elev=-152, azim=-26)\n# plot first all the points with y == 0, then all with y == 1\nmask = y == 0\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\nprediction to the polynomial regression without an explicit transformation of the\nfeatures.\nInteractions and Polynomials \n| \n229\nAs a more realistic application of interactions and polynomials, let’s look again at the\nBoston Housing dataset. We already used polynomial features on this dataset in\nChapter 2. Now let’s have a look at how these features were constructed, and at how\nmuch the polynomial features help. First we load the data, and rescale it to be\nbetween 0 and 1 using MinMaxScaler:\nIn[27]:\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split\n    (boston.data, boston.target, random_state=0)\n# rescale data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nNow, we extract polynomial features and interactions up to a degree of 2:\nIn[28]:\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\nX_train_poly = poly.transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nwhile the columns—the properties that describe these entities—are called features.\nLater in this book we will go into more detail on the topic of building a good repre‐\nsentation of your data, which is called feature extraction or feature engineering. You\nshould keep in mind, however, that no machine learning algorithm will be able to\nmake a prediction on data for which it has no information. For example, if the only\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\ndict their gender. This information is simply not contained in your data. If you add\nanother feature that contains the patient’s first name, you will have much better luck,\nas it is often possible to tell the gender by a person’s first name.\nKnowing Your Task and Knowing Your Data\nQuite possibly the most important part in the machine learning process is under‐\nstanding the data you are working with and how it relates to the task you want to\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\nit. It is necessary to understand what is going on in your dataset before you begin\nbuilding a model. Each algorithm is different in terms of what kind of data and what\nproblem setting it works best for. While you are building a machine learning solution,\nyou should answer, or at least keep in mind, the following questions:\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131",
                    "summary": "You can see that the first column of X_poly corresponds exactly to X, while the otherColumns are the powers of the first entry. It’s interesting to see how large some of the values can get. You can obtain the semantics of the features by calling the get_feature_namesmethod, which provides the exponent for each feature:Interactions and Polynomials  ARTICLE: Polynomial feature names. READ: How do you use polynomial features? Using polynomial features together with a linear regression model yields the classical model. The second column has entries above 20,000, orders of magnitude dif‐ferent from the rest.Figure 4-5. Linear regression with tenth-degree polynomial features with a one-dimensional data set. The classical model of polynnomial regression is shown in Figure 4-6. The data originally had 13 features, which were expanded into 105 interaction fea‐                tures. These new features represent all possible interactions between two different original features. The polynomials of high degree tend to behave in extreme ways on the                boundaries or in regions with little data. Here is a kernel SVM model learned on the original data, without any transformation (see Figure 4-6): grotesquely, the model is more accurate than the original model, which had no transformation. The model is also more robust than the model with a transformation, which has a much higher degree. The first new feature is a constant feature, called \"1\" here. The exact correspondence between input and output features can be found using the polynomialget_feature_names method. For example, the first input feature is \"x1\", followed by \"x2\", and then \"x3\", with \"x4\" and \"x11\" after \"x12\" The first output feature is called \"x7\", and the second is \"X8\", with the third being \"x9\" and the fourth being \"X12\" For more information on the Polynomial feature names method, seepolynomialfeaturenames.org. For more details on thePolynomial Feature Names method, visit polynom The synthetic dataset we used in “Feature importance in trees’ on page 77 is shown in Figure 2-29. The next 13 features are the original features (called \"x0\" to \"x12\"). Then follows the first feature squared(\"x0^2\") and combinations of the first and the other features. We picked this particular feature to add for illustration purposes. The choice is not particularly important. The performance using Ridge on the data with and without interac‐                tions:                or polynomials of the input A linear model for classification can only separate points using a line, and will not be able to do a very good job on this dataset. Let’s expand the set of input features, say by also adding feature1 ** 2, the square of the second feature, as a new feature. Instead of representing each data point as a two-dimensional point, (feature0, feature1), we now represent it as a three-dimensionalpoint, ( feature0,feature1, feature 1 ** 2).10 This new representa‐                tion is illustrated in Figure 2-38 in a 3-dimensional scatter plot. X_new = np.hstack([X, X[:, 1:] ** 2], c='b', c='r', marker='^', cmap=mglearn.cm2, s=60) X_new.scatter(X_ new[mask, 0], X.new[ mask, 1], X_ new [mask, 2), c=b, c=r) X. scatter(x_new[~ mask, 0), X. new[~mask, 1, X. mask, 2] X. Using a more complex model, a kernel SVM, we are able to learn a similarly complex prediction to the polynomial regression without an explicit transformation of the features. We already used polynomials on the Boston Housing dataset. Now let’s have a look at how these features were constructed, and at how much the poylnomials help. We are using a more realistic application of interactions and poyleomials The data originally had 13 features, which were expanded into 105 interaction fea­tures. First we load the data, and rescale it to be between 0 and 1 using MinMaxScaler. Then, we extract polynomial features and interactions up to a degree of 2. The results are shown in the figure below. The full code can be downloaded from the sklearn.com website. The code is available for download from the GitHub repository. Model-based feature selection uses a supervised machine learning model to judge the importance of each feature. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which directly encodes the importance. These new features represent all possible interactions between two different features. They can be helpful if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. It doesn’t need to be the same model that is used for the final Model-based selection considers all features at once, and so can capture interactions (if the model can capture them) This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. In contrast to univariate selection, model-basedselection considers allfeatures at once. It can also capture feature importances by considering the                absolute values. The SelectFromModel class selects all features that have an importance measure of greater than the provided thresh‐old. To use model-based feature selection, we need to use the transformer: grotesqueSelectFromModel. We used univariate feature selection to get a comparable result to what we got with automatic feature selection. We then compared the results to the results we got from the two different models. We also compared the two models to see if the results were comparable. To see the full report, go to: http://www.cnn.com/2013/01/24/science/features/features-and Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. In iterative feature selection, a series of models are built, with varying numbers of features. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. One particular method is recursive feature elimination (RFE), which starts with all features and builds a model, and discards the least important feature. These methods are much more computationally expensive than the methods we discussed previously, and need to be split into two sets of data to work out the best fit for each set of data. For example, we can use the data from the U.S. Bureau of The feature selection got better compared to the univariate and model-based selec­tion, but one feature was still missed. We use the same random forest model that we                used earlier, and get the results shown in Figure 4-11. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. For this to work, the model needs to provide some way to determine feature importance, as was the case with the model- based selection. For example, we can visualize the selected features by using a mask to show the shape of the feature. A random forest model is trained 40 times, once for each feature that is dropped. To make the task a bit harder, we’ll add some noninformative noise to the data. Let’s test the accuracy of the logisticregression model when using RFE for feature selection. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. Feature selection is based on a threshold of k features. We expect the feature selection to be able to identify the features that are noninformative and remove them. To make the task a bit harder, we’ll add some non informative noise to the data. The feature selection for classification on the                236                 |                  cancer dataset will be based on the threshold of k features. The threshold will be determined by a number of factors, such as Each entity or row here is known as a sample (or data point) in machine learning. The properties that describe these entities are called features. You might describe an image of a tumor by the grayscale values of each pixel. No machine learning algorithm will be able to make a prediction on data for which it has no information. For example, if the only feature that you have for a patient is their last name, no algorithm can pre‐dict their gender. This information is simply not contained in your data. Later in this book we will go into more detail on the topic of building a good repre‐sentation of your data, which is called feature extraction or feature engineering. Knowing Your Task and Knowing Your Data is the most important part of machine learning. It will not be effective to randomly choose an algorithm and throw your data at it. It is necessary to understand what is going on in your dataset before you begin building a model. Each algorithm is different in terms of what kind of data and what problem setting it works best for. If you add another feature that contains the patient’s first name, you will have much better luck. Often it is possible to tell the gender by a person’S first name. Machine learning is the process of building a machine learning solution to a problem. When building a solution, you should answer the following questions: What question(s) am I trying to answer? Do I think the data collected can answer that question? Have I collected enough data to represent the problem I want to solve? Are there enough data for the problem to be solved? Do you have any questions? If so, please share them with us in the comments below. We would like to hear from you about your machine learning solutions. Please send us your ideas in the form of a comment or email at jennifer.smith@mailonline.co.uk. Back to the page you came from. There are 131. applications for, 131. multiclass, 25. examples of, 26. goals for, 25                iris classification example, 14.k-nearest neighbors, 35.linear models, 56naive Bayes classifiers, 68.naive regression problems, 26classifiers, and 26 goals for. 26 examples of multiclass.",
                    "children": [
                        {
                            "id": "chapter-4-section-5-subsection-1",
                            "title": "Feature Interactions",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-5-subsection-2",
                            "title": "Polynomial Features",
                            "content": "10.125     14.094     19.618     27.307]\n [     0.592      0.350      0.207      0.123      0.073      0.043\n       0.025      0.015      0.009      0.005]\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n    -159.516    329.222   -679.478   1402.367]]\nYou can obtain the semantics of the features by calling the get_feature_names\nmethod, which provides the exponent for each feature:\nInteractions and Polynomials \n| \n227\nIn[24]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[24]:\nPolynomial feature names:\n['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']\nYou can see that the first column of X_poly corresponds exactly to X, while the other\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\nferent from the rest.\nUsing polynomial features together with a linear regression model yields the classical\nmodel of polynomial regression (see Figure 4-5):\nIn[26]:\nreg = LinearRegression().fit(X_poly, y)\nline_poly = poly.transform(line)\nplt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-5. Linear regression with tenth-degree polynomial features\n228 \n| \nChapter 4: Representing Data and Engineering Features\nAs you can see, polynomial features yield a very smooth fit on this one-dimensional\ndata. However, polynomials of high degree tend to behave in extreme ways on the\nboundaries or in regions with little data.\nAs a comparison, here is a kernel SVM model learned on the original data, without\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different\noriginal features, as well as the square of each original feature. degree=2 here means\nthat we look at all features that are the product of up to two original features. The\nexact correspondence between input and output features can be found using the\nget_feature_names method:\nIn[29]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[29]:\nPolynomial feature names:\n['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6',\n'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2',\n230 \n| \nChapter 4: Representing Data and Engineering Features\n'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10',\n'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7',\n'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5',\n'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2',\n'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12',\n'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12',\n'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2',\n'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10',\n'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11',\n'x10 x12', 'x11^2', 'x11 x12', 'x12^2']\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n(\"x0^2\") and combinations of the first and the other features.\nLet’s compare the performance using Ridge on the data with and without interac‐\ntions:\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n92 \n| \nChapter 2: Supervised Learning\n10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\nA linear model for classification can only separate points using a line, and will not be\nable to do a very good job on this dataset (see Figure 2-37):\nIn[77]:\nfrom sklearn.svm import LinearSVC\nlinear_svm = LinearSVC().fit(X, y)\nmglearn.plots.plot_2d_separator(linear_svm, X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nNow let’s expand the set of input features, say by also adding feature1 ** 2, the\nsquare of the second feature, as a new feature. Instead of representing each data point\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\ndimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\nSupervised Machine Learning Algorithms \n| \n93\nFigure 2-37. Decision boundary found by a linear SVM\nIn[78]:\n# add the squared first feature\nX_new = np.hstack([X, X[:, 1:] ** 2])\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\nfigure = plt.figure()\n# visualize in 3D\nax = Axes3D(figure, elev=-152, azim=-26)\n# plot first all the points with y == 0, then all with y == 1\nmask = y == 0\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\nprediction to the polynomial regression without an explicit transformation of the\nfeatures.\nInteractions and Polynomials \n| \n229\nAs a more realistic application of interactions and polynomials, let’s look again at the\nBoston Housing dataset. We already used polynomial features on this dataset in\nChapter 2. Now let’s have a look at how these features were constructed, and at how\nmuch the polynomial features help. First we load the data, and rescale it to be\nbetween 0 and 1 using MinMaxScaler:\nIn[27]:\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split\n    (boston.data, boston.target, random_state=0)\n# rescale data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nNow, we extract polynomial features and interactions up to a degree of 2:\nIn[28]:\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\nX_train_poly = poly.transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different",
                            "summary": "You can see that the first column of X_poly corresponds exactly to X, while the otherColumns are the powers of the first entry. It’s interesting to see how large some of the values can get. You can obtain the semantics of the features by calling the get_feature_namesmethod, which provides the exponent for each feature:Interactions and Polynomials  ARTICLE: Polynomial feature names. READ: How do you use polynomial features? Using polynomial features together with a linear regression model yields the classical model. The second column has entries above 20,000, orders of magnitude dif‐ferent from the rest.Figure 4-5. Linear regression with tenth-degree polynomial features with a one-dimensional data set. The classical model of polynnomial regression is shown in Figure 4-6. The data originally had 13 features, which were expanded into 105 interaction fea‐                tures. These new features represent all possible interactions between two different original features. The polynomials of high degree tend to behave in extreme ways on the                boundaries or in regions with little data. Here is a kernel SVM model learned on the original data, without any transformation (see Figure 4-6): grotesquely, the model is more accurate than the original model, which had no transformation. The model is also more robust than the model with a transformation, which has a much higher degree. The first new feature is a constant feature, called \"1\" here. The exact correspondence between input and output features can be found using the polynomialget_feature_names method. For example, the first input feature is \"x1\", followed by \"x2\", and then \"x3\", with \"x4\" and \"x11\" after \"x12\" The first output feature is called \"x7\", and the second is \"X8\", with the third being \"x9\" and the fourth being \"X12\" For more information on the Polynomial feature names method, seepolynomialfeaturenames.org. For more details on thePolynomial Feature Names method, visit polynom The synthetic dataset we used in “Feature importance in trees’ on page 77 is shown in Figure 2-29. The next 13 features are the original features (called \"x0\" to \"x12\"). Then follows the first feature squared(\"x0^2\") and combinations of the first and the other features. We picked this particular feature to add for illustration purposes. The choice is not particularly important. The performance using Ridge on the data with and without interac‐                tions:                or polynomials of the input A linear model for classification can only separate points using a line, and will not be able to do a very good job on this dataset. Let’s expand the set of input features, say by also adding feature1 ** 2, the square of the second feature, as a new feature. Instead of representing each data point as a two-dimensional point, (feature0, feature1), we now represent it as a three-dimensionalpoint, ( feature0,feature1, feature 1 ** 2).10 This new representa‐                tion is illustrated in Figure 2-38 in a 3-dimensional scatter plot. X_new = np.hstack([X, X[:, 1:] ** 2], c='b', c='r', marker='^', cmap=mglearn.cm2, s=60) X_new.scatter(X_ new[mask, 0], X.new[ mask, 1], X_ new [mask, 2), c=b, c=r) X. scatter(x_new[~ mask, 0), X. new[~mask, 1, X. mask, 2] X. Using a more complex model, a kernel SVM, we are able to learn a similarly complex prediction to the polynomial regression without an explicit transformation of the features. We already used polynomials on the Boston Housing dataset. Now let’s have a look at how these features were constructed, and at how much the poylnomials help. We are using a more realistic application of interactions and poyleomials The data originally had 13 features, which were expanded into 105 interaction fea­tures. First we load the data, and rescale it to be between 0 and 1 using MinMaxScaler. Then, we extract polynomial features and interactions up to a degree of 2. The results are shown in the figure below. The full code can be downloaded from the sklearn.com website. The code is available for download from the GitHub repository. These new features represent all possible interactions between two different. ",
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-5-subsection-3",
                            "title": "Feature Selection",
                            "content": "helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nwhile the columns—the properties that describe these entities—are called features.\nLater in this book we will go into more detail on the topic of building a good repre‐\nsentation of your data, which is called feature extraction or feature engineering. You\nshould keep in mind, however, that no machine learning algorithm will be able to\nmake a prediction on data for which it has no information. For example, if the only\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\ndict their gender. This information is simply not contained in your data. If you add\nanother feature that contains the patient’s first name, you will have much better luck,\nas it is often possible to tell the gender by a person’s first name.\nKnowing Your Task and Knowing Your Data\nQuite possibly the most important part in the machine learning process is under‐\nstanding the data you are working with and how it relates to the task you want to\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\nit. It is necessary to understand what is going on in your dataset before you begin\nbuilding a model. Each algorithm is different in terms of what kind of data and what\nproblem setting it works best for. While you are building a machine learning solution,\nyou should answer, or at least keep in mind, the following questions:\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131",
                            "summary": "Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which directly encodes the importance of eachFeature. The supervised model that is used for feature selection doesn’t need to be the same model used for the final supervised modeling. It can be helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative. The SelectFromModel class selects all features that have an importance measure of                the feature (as provided by the supervised model) greater than the provided thresh‐old. This can be viewed as a form of Feature Selection for the model itself, but can also be used as a preprocessing step to selecting features for another model. In contrast to univariate selection, model-based Feature Selection considers all features at once, and so can capture interactions (if the model can capture them). To use model- based Feature Selection, we need to use the SelectFrom model transformer. We used univariate feature selection to get a comparable result to what we got with automatic feature selection. We then compared the results to the results we got from the two different models. We also compared the two models to see if the results were comparable. To see the full report, go to: http://www.cnn.com/2013/01/24/science/features/features-and Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. In iterative feature selection, a series of models are built, with varying numbers of features. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. One particular method is recursive feature elimination (RFE), which starts with all features and builds a model, and discards the least important feature. These methods are much more computationally expensive than the methods we discussed previously, and need to be split into two sets of data to work out the best fit for each set of data. For example, we can use the data from the U.S. Bureau of The feature selection got better compared to the univariate and model-based selec­tion, but one feature was still missed. We use the same random forest model that we                used earlier, and get the results shown in Figure 4-11. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. For this to work, the model needs to provide some way to determine feature importance, as was the case with the model- based selection. For example, we can visualize the selected features by using a mask to show the shape of the feature. A random forest model is trained 40 times, once for each feature that is dropped. To make the task a bit harder, we’ll add some noninformative noise to the data. Let’s test the accuracy of the logisticregression model when using RFE for feature selection. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. Feature selection is based on a threshold of k features. We expect the feature selection to be able to identify the features that are noninformative and remove them. To make the task a bit harder, we’ll add some non informative noise to the data. The feature selection for classification on the                236                 |                  cancer dataset will be based on the threshold of k features. The threshold will be determined by a number of factors, such as Each entity or row here is known as a sample (or data point) in machine learning. The properties that describe these entities are called features. You might describe an image of a tumor by the grayscale values of each pixel. No machine learning algorithm will be able to make a prediction on data for which it has no information. For example, if the only feature that you have for a patient is their last name, no algorithm can pre‐dict their gender. This information is simply not contained in your data. Later in this book we will go into more detail on the topic of building a good repre‐sentation of your data, which is called feature extraction or feature engineering. Knowing Your Task and Knowing Your Data is the most important part of machine learning. It will not be effective to randomly choose an algorithm and throw your data at it. It is necessary to understand what is going on in your dataset before you begin building a model. Each algorithm is different in terms of what kind of data and what problem setting it works best for. If you add another feature that contains the patient’s first name, you will have much better luck. Often it is possible to tell the gender by a person’S first name. Machine learning is the process of building a machine learning solution to a problem. When building a solution, you should answer the following questions: What question(s) am I trying to answer? Do I think the data collected can answer that question? Have I collected enough data to represent the problem I want to solve? Are there enough data for the problem to be solved? Do you have any questions? If so, please share them with us in the comments below. We would like to hear from you about your machine learning solutions. Please send us your ideas in the form of a comment or email at jennifer.smith@mailonline.co.uk. Back to the page you came from. There are 131. applications for, 131. multiclass, 25. examples of, 26. goals for, 25                iris classification example, 14.k-nearest neighbors, 35.linear models, 56naive Bayes classifiers, 68.naive regression problems, 26classifiers, and 26 goals for. 26 examples of multiclass.",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-6",
                    "title": "Univariate Nonlinear Transformations",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-4-section-6-subsection-1",
                            "title": "Log Transform",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-6-subsection-2",
                            "title": "Power Transform",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-6-subsection-3",
                            "title": "Other Transformations",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-7",
                    "title": "Automatic Feature Selection",
                    "content": "helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and gle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nhelpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237",
                    "summary": "Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which directly encodes the importance of eachFeature. The supervised model that is used for feature selection doesn’t need to be the same model used for the final supervised modeling. It can be helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative. The SelectFromModel class selects all features that have an importance measure of                the feature (as provided by the supervised model) greater than the provided thresh‐old. This can be viewed as a form of Feature Selection for the model itself, but can also be used as a preprocessing step to selecting features for another model. In contrast to univariate selection, model-based Feature Selection considers all features at once, and so can capture interactions (if the model can capture them). To use model- based Feature Selection, we need to use the SelectFrom model transformer. In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached. One particular method of this kind is recursive feature elimination (RFE), which starts with all features, builds a model, and discards the least important feature. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. These methods are much morecomputationally expensive than the methods we discussed previously.  The results of these methods are comparable to what we got with univariate feature selection. The feature selection got better compared to the univariate and model-based selec‐tion, but one feature was still missed. Features selected by recursive feature elimination with the random forest classifier model got better. The results are shown in Figure 4-11 in the next section of the book. The book is called Representing Data and Engineering Features and is published by Piatkus & Co. in the U.S. and Europe. For more information on the book, visit: www.piatkus.com. A random forest model is trained 40 times, once for each feature that is dropped. Running this code also takes significantly longerthan that Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. To make the task a bit harder, we’ll add some noninformative noisefeatures to the The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. The meth‐ocreods differ in how they compute this threshold, with the simplest ones being SelectKBest and SelectPercentile. To make the task a bit harder, we’ll add some noninformative noisefeatures to the data. Let’s apply the feature selection for classification on the                236  We expect the feature selection to be able to identify the features that are noninformative and remove them. We expect the features to be identified and removed by using the following criteria. The criteria include:Grid Search with Cross-Validation, Binary Classification, Multiclass Classification Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. Building Pipelines    305                Parameter Selection with Preprocessing.  306                Using Pipelines in Grid Searches.  309                The General Pipeline Interface.  314                Accessing Attributes in a Grid-Searched Pipeline.  315                Grid-Searching Preprocessing Steps and Model parameters.  317                Model Evaluation and Improvement.   317                                                                                                                                             “Pipelines’ In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached, or starting with all features and removing them. One particularmethod of this kind is recursive feature elimination (RFE), which builds a model, and discards the least important feature. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. These methods are much morecomputationally expensive than the methods we discussed previously. We will first intro‐idated cross-validation, a more robust way to assess generalization performance. The feature selection got better compared to the univariate and model-based selec‐tion, but one feature was still missed. Features selected by recursive feature elimination with the random forest classifier model got better. The results are shown in Figure 4-11 in the next section of the book. The book is called Representing Data and Engineering Features and is published by Piatkus & Co. in the U.S. and Europe. For more information on the book, visit: www.piatkus.com. A random forest model is trained 40 times, once for each feature that is dropped. Running this code also takes significantly longerthan that Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature and keeps only the most important ones. If you suspect that many features are completely uninformative, or if there are a large number of features that building a model on is infeasible, you may not want to build a model. The most important features to consider are: Grid Search, the most popular search engine, and the most common search terms. The feature selection model needs to provide some measure of importance for each feature. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which can be used to capture the importance of features. Linear models have a feature importances value that can be considered by considering the feature absolute values. This is used for sparse models, which only use a small subset of features, such as the L1 penalty learnable model in Chapter 2. The final supervised model doesn’t need to be the The SelectFromModel class selects all features that have an importance measure of the feature greater than the provided thresh‐old. To get a comparable result to what we got with univariate feature selection, we add some noninformative noise to the data. To make the task a bit harder, we’ll add someNonInformative Noise (NIS) to the dataset. To use model-based feature selection we need to use theSelectFromModel transformer:In[42]: grotesquely, we use the transformer to select all features. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can find out which features have been selected using the get_ support method. Figure 4-9 shows how to visualize the mask -- black is True, white is False. For example, the black mask looks like the color gray. The white mask is the color of the sample index. For more information, see \"Automatic Feature Selection\" and \"Aut",
                    "children": [
                        {
                            "id": "chapter-4-section-7-subsection-1",
                            "title": "Univariate Statistics",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-7-subsection-2",
                            "title": "Model-Based Feature Selection",
                            "content": "helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and",
                            "summary": "Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature. The feature selection model needs to provide some measure of importance for each feature, so that they can be ranked by this measure. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which directly encodes the importance of eachFeature. The supervised model that is used for feature selection doesn’t need to be the same model used for the final supervised modeling. It can be helpful, though, if there is such a large number of features that building a model on them is infeasible, or if you suspect that many features are completely uninformative. The SelectFromModel class selects all features that have an importance measure of                the feature (as provided by the supervised model) greater than the provided thresh‐old. This can be viewed as a form of Feature Selection for the model itself, but can also be used as a preprocessing step to selecting features for another model. In contrast to univariate selection, model-based Feature Selection considers all features at once, and so can capture interactions (if the model can capture them). To use model- based Feature Selection, we need to use the SelectFrom model transformer. In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached. One particular method of this kind is recursive feature elimination (RFE), which starts with all features, builds a model, and discards the least important feature. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. These methods are much morecomputationally expensive than the methods we discussed previously.  The results of these methods are comparable to what we got with univariate feature selection. The feature selection got better compared to the univariate and model-based selec‐tion, but one feature was still missed. Features selected by recursive feature elimination with the random forest classifier model got better. The results are shown in Figure 4-11 in the next section of the book. The book is called Representing Data and Engineering Features and is published by Piatkus & Co. in the U.S. and Europe. For more information on the book, visit: www.piatkus.com. A random forest model is trained 40 times, once for each feature that is dropped. Running this code also takes significantly longerthan that Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Cross-Validation in scikit-learn is a key part of the learning process. It is essential for linear models, but might not be essential for all models. The benefits of cross-validation can be found in the benefits of Sometimes it is also a good idea to transform the target variable y in regression. This is particularly true for less complex models like linear models and naive Bayes models. Tree-based models, on the other hand, are often able to discover important interactions them‐selves, and don’t require transforming the data explicitly most of the time.2.2 Univariate Nonlinear Transformations                 |                 235                � ‘Univariate Non linear Transformations’ How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. To make the task a bit harder, we’ll add some noninformative noisefeatures to the The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can visualize the mask -- black is True, white is False. All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. In univariate statistics, we compute whether there is a statistically significant relation between each feature and the target. We'll look at these methods in detail later in this article, but for now, let's look at the automatic feature selection method in Figure 4-9. It's a lot of work, but it's worth it. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. To use univariate feature selection in scikit-learn, you need to choose a test, either f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The p- values used in the tests are determined by the type of test being used. The meth‐ocreods differ in how they compute this threshold, with the simplest ones being SelectKBest and SelectPercentile. To make the task a bit harder, we’ll add some noninformative noisefeatures to the data. Let’s apply the feature selection for classification on the                236  We expect the feature selection to be able to identify the features that are noninformative and remove them. We expect the features to be identified and removed by using the following criteria. The criteria include:Grid Search with Cross-Validation, Binary Classification, Multiclass Classification Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. Building Pipelines    305                Parameter Selection with Preprocessing.  306                Using Pipelines in Grid Searches.  309                The General Pipeline Interface.  314                Accessing Attributes in a Grid-Searched Pipeline.  315                Grid-Searching Preprocessing Steps and Model parameters.  317                Model Evaluation and Improvement.   317                                                                                                                                             “Pipelines’ In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. Cross-validation is a more robust way to assess general",
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-7-subsection-3",
                            "title": "Iterative Feature Selection",
                            "content": "gle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nhelpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237",
                            "summary": "In iterative feature selection, a series of models are built, with varying numbers of features. There are two basic methods: starting with no features and adding features one by one until some stopping criterion is reached. For this to work, the model used for selecting features needs to provide some way to determine feature importance. One particular method of this kind is recursive feature elimination (RFE), which starts with all features, builds a model, and discards the least important feature according to the model. Then a new model is built using all but the discarded feature, and so on until only a prespecified number of features are left. These methods are much more computationally expensive than the methods we discussed previously. The feature selection got better compared to the univariate and model-based selec­tion, but one feature was still missed. Here, we use the same random forest model that we used earlier, and get the results shown in Figure 4-11. Running this code also takes significantly longer than that for the model- based selection, because a random Forest model is trained 40 times, once for each feature that is dropped. The results are shown in the figure below, and the code can be downloaded from the GitHub repository. Let’s test the accuracy of the logisticRegression model when using RFE for feature selection. We will use the RFE model to test three different types of feature selection: automatic, interactive and model-based. We’ll start with automatic feature selection and then move on to model-Based Feature Selection. We'll then look at the model- . . . .. .. Model-based feature selection uses a supervised machine learning model to judge theimportance of each feature and keeps only the most important ones. If you suspect that many features are completely uninformative, or if there are a large number of features that building a model on is infeasible, you may not want to build a model. The most important features to consider are: Grid Search, the most popular search engine, and the most common search terms. The feature selection model needs to provide some measure of importance for each feature. This can be viewed as a form of feature selection for the model itself, but can also be used as a preprocessing step to select features for another model. Decision trees and decision tree–based models provide a feature_importances_238 attribute, which can be used to capture the importance of features. Linear models have a feature importances value that can be considered by considering the feature absolute values. This is used for sparse models, which only use a small subset of features, such as the L1 penalty learnable model in Chapter 2. The final supervised model doesn’t need to be the The SelectFromModel class selects all features that have an importance measure of the feature greater than the provided thresh‐old. To get a comparable result to what we got with univariate feature selection, we add some noninformative noise to the data. To make the task a bit harder, we’ll add someNonInformative Noise (NIS) to the dataset. To use model-based feature selection we need to use theSelectFromModel transformer:In[42]: grotesquely, we use the transformer to select all features. The first 30 features are from the dataset, the next 50 are noise. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. The number of features was reduced from 80 to 40 (50 percent of the original number offeatures) The training set was split into two: X_train and Y_train. The training data was fed into the training set using the load_breast_cancer and train_test_split functions. The data was then fed into a training set called X_w_noise. The train set was then transformed into X_trained and X_test. The get_support method returns a Boolean mask of the selected features. We can find out which features have been selected using the get_ support method. Figure 4-9 shows how to visualize the mask -- black is True, white is False. For example, the black mask looks like the color gray. The white mask is the color of the sample index. For more information, see \"Automatic Feature Selection\" and \"Aut",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-8",
                    "title": "Utilizing Expert Knowledge",
                    "content": "the application or domain should be discarded. Often, domain experts can help in\nidentifying useful features that are much more informative than the initial represen‐\ntation of the data. Imagine you work for a travel agency and want to predict flight\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\ntions, and destinations. A machine learning model might be able to build a decent\nmodel from that. Some important factors in flight prices, however, cannot be learned.\nFor example, flights are usually more expensive during peak vacation months and\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\ntheir effect can therefore be learned from the date, others might depend on the phases\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\ndays). These events cannot be learned from the data if each flight is only recorded\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\nflight was on, preceding, or following a public or school holiday. In this way, prior\nknowledge about the nature of the task can be encoded in the features to aid a\nmachine learning algorithm. Adding a feature does not force a machine learning\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\ntive for flight prices, augmenting the data with this information doesn’t hurt.\nWe’ll now look at one particular case of using expert knowledge—though in this case\nit might be more rightfully called “common sense.” The task is predicting bicycle rent‐\nals in front of Andreas’s house.\nIn New York, Citi Bike operates a network of bicycle rental stations with a subscrip‐\ntion system. The stations are all over the city and provide a convenient way to get\naround. Bike rental data is made public in an anonymized form and has been ana‐\nlyzed in various ways. The task we want to solve is to predict for a given time and day",
                    "summary": "Some important factors in flight prices cannot be learned from the data. For example, flights are usually more expensive during peak vacation months and around holidays. The application or domain should be discarded. Often, domain experts can help in identifying useful features that are much more informative than the initial represenen‐tation of the data, the authors say. The authors conclude that machine learning can be a useful tool for predicting flight prices, but it should not be used to predict the future of the economy or the economy in general. The researchers conclude that the use of machine learning to predict flight prices is a waste of time and money. The results of the study were published in the open-source book “Flight Pricing: A Machine Learning Machine learning can be used to predict the timing and location of events. In this case, the task is predicting bicycle rent in front of Andreas’s house. Citi Bike operates a network of bicycle rental stations in New York. The stations are all over the city and provide a convenient way to get around. Bike rental data is made public in an anonymized form and has been ana‐ purposefullylyzed in various ways. The task we want to solve is to predict for a given time and day, and it is easy to add a feature that encodes whether a flight was on, preceding, or following a public or school holiday. It is also easy to use prior knowledge about the task to aid a machine learning algorithm.",
                    "children": [
                        {
                            "id": "chapter-4-section-8-subsection-1",
                            "title": "Domain Expertise",
                            "content": "the application or domain should be discarded. Often, domain experts can help in\nidentifying useful features that are much more informative than the initial represen‐\ntation of the data. Imagine you work for a travel agency and want to predict flight\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\ntions, and destinations. A machine learning model might be able to build a decent\nmodel from that. Some important factors in flight prices, however, cannot be learned.\nFor example, flights are usually more expensive during peak vacation months and\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\ntheir effect can therefore be learned from the date, others might depend on the phases\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\ndays). These events cannot be learned from the data if each flight is only recorded\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\nflight was on, preceding, or following a public or school holiday. In this way, prior\nknowledge about the nature of the task can be encoded in the features to aid a\nmachine learning algorithm. Adding a feature does not force a machine learning\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\ntive for flight prices, augmenting the data with this information doesn’t hurt.\nWe’ll now look at one particular case of using expert knowledge—though in this case\nit might be more rightfully called “common sense.” The task is predicting bicycle rent‐\nals in front of Andreas’s house.\nIn New York, Citi Bike operates a network of bicycle rental stations with a subscrip‐\ntion system. The stations are all over the city and provide a convenient way to get\naround. Bike rental data is made public in an anonymized form and has been ana‐\nlyzed in various ways. The task we want to solve is to predict for a given time and day",
                            "summary": "Some important factors in flight prices cannot be learned from the data. For example, flights are usually more expensive during peak vacation months and around holidays. The application or domain should be discarded. Often, domain experts can help in identifying useful features that are much more informative than the initial represenen‐tation of the data, the authors say. The authors conclude that machine learning can be a useful tool for predicting flight prices, but it should not be used to predict the future of the economy or the economy in general. The researchers conclude that the use of machine learning to predict flight prices is a waste of time and money. The results of the study were published in the open-source book “Flight Pricing: A Machine Learning Machine learning can be used to predict the timing and location of events. In this case, the task is predicting bicycle rent in front of Andreas’s house. Citi Bike operates a network of bicycle rental stations in New York. The stations are all over the city and provide a convenient way to get around. Bike rental data is made public in an anonymized form and has been ana‐ purposefullylyzed in various ways. The task we want to solve is to predict for a given time and day, and it is easy to add a feature that encodes whether a flight was on, preceding, or following a public or school holiday. It is also easy to use prior knowledge about the task to aid a machine learning algorithm.",
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-8-subsection-2",
                            "title": "Feature Engineering",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-8-subsection-3",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-4-section-9",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-4-section-9-subsection-1",
                            "title": "Key Concepts",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-9-subsection-2",
                            "title": "Advanced Topics",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-4-section-9-subsection-3",
                            "title": "Future Directions",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-5",
            "title": "5. Model Evaluation and Improvement",
            "content": "using standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ndescribed earlier, we still have a single split of the data into training and test sets,\nwhich might make our results unstable and make us depend too much on this single\nsplit of the data. We can go a step further, and instead of splitting the original data\ninto training and test sets once, use multiple splits of cross-validation. This will result\nin what is called nested cross-validation. In nested cross-validation, there is an outer\nloop over splits of the data into training and test sets. For each of them, a grid search\nis run (which might result in different best parameters for each split in the outer\nloop). Then, for each outer split, the test set score using the best settings is reported.\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\nting. The scores tell us how well a model generalizes, given the best parameters found\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\nvalidation is rarely used when looking for a predictive model to apply to future data.\nHowever, it can be useful for evaluating how well a given model works on a particular\ndataset.\nImplementing nested cross-validation in scikit-learn is straightforward. We call\ncross_val_score with an instance of GridSearchCV as the model:\nIn[34]:\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\nOut[34]:\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\nMean cross-validation score:  0.98\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\nless.\nHere, we used stratified five-fold cross-validation in both the inner and the outer\nloop. As our param_grid contains 36 combinations of parameters, this results in a\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement\nCross-Validation in scikit-learn\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\ntion from the model_selection module. The parameters of the cross_val_score\nfunction are the model we want to evaluate, the training data, and the ground-truth\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\nIn[4]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\niris = load_iris()\nlogreg = LogisticRegression()\nscores = cross_val_score(logreg, iris.data, iris.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[4]:\nCross-validation scores: [ 0.961  0.922  0.958]\nBy default, cross_val_score performs three-fold cross-validation, returning three\naccuracy values. We can change the number of folds used by changing the cv parame‐\nter:\nIn[5]:\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[5]:\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\nA common way to summarize the cross-validation accuracy is to compute the mean:\nIn[6]:\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nOut[6]:\nAverage cross-validation score: 0.96\nUsing the mean cross-validation we can conclude that we expect the model to be\naround 96% accurate on average. Looking at all five scores produced by the five-fold\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n| \nChapter 6: Algorithm Chains and Pipelines\nThis is fundamentally different from how new data looks to the model. If we observe\nnew data (say, in form of our test set), this data will not have been used to scale the\ntraining data, and it might have a different minimum and maximum than the train‐\ning data. The following example (Figure 6-1) shows how the data processing during\ncross-validation and the final evaluation differ:\nIn[4]:\nmglearn.plots.plot_improper_processing()\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\nSo, the splits in the cross-validation no longer correctly mirror how new data will\nlook to the modeling process. We already leaked information from these parts of the\ndata into our modeling process. This will lead to overly optimistic results during\ncross-validation, and possibly the selection of suboptimal parameters.\nTo get around this problem, the splitting of the dataset during cross-validation should\nbe done before doing any preprocessing. Any process that extracts knowledge from the\ndataset should only ever be applied to the training portion of the dataset, so any\ncross-validation should be the “outermost loop” in your processing.\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\nParameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\nshuffle-split cross-validation, each split samples train_size many points for the\ntraining set and test_size many (disjoint) point for the test set. This splitting is\nrepeated n_iter times. Figure 5-3 illustrates running four iterations of splitting a\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\neach (you can use integers for train_size and test_size to use absolute sizes for\nthese sets, or floating-point numbers to use fractions of the whole dataset):\nIn[14]:\nmglearn.plots.plot_shuffle_split()\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\nThe following code splits the dataset into 50% training set and 50% test set for 10\niterations:\nIn[15]:\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\nOut[15]:\nCross-validation scores:\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\nShuffle-split cross-validation allows for control over the number of iterations inde‐\npendently of the training and test sizes, which can sometimes be helpful. It also allows\nfor using only part of the data in each iteration, by providing train_size and\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest), when applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\ndiscuss methods to evaluate classification and regression performance that go beyond\nthe default measures of accuracy and R2 provided by the score method.\nWe will also discuss grid search, an effective method for adjusting the parameters in\nsupervised models for the best generalization performance.\nCross-Validation\nCross-validation is a statistical method of evaluating generalization performance that\nis more stable and thorough than using a split into a training and a test set. In cross-\nvalidation, the data is instead split repeatedly and multiple models are trained. The\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\nNext, a sequence of models is trained. The first model is trained using the first fold as\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\nFor each of these five splits of the data into training and test sets, we compute the\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement fied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\nwould be only class 0, and the training set would be only classes 1 and 2. As the\nclasses in training and test sets would be different for all three splits, the three-fold\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\nwe can do much better than 0% accuracy on iris.\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\ntion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\nsplit the data such that the proportions between classes are the same in each fold as\nthey are in the whole dataset, as illustrated in Figure 5-2:\nIn[8]:\nmglearn.plots.plot_stratified_cross_validation()\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\nwhen the data is ordered by class label\nCross-Validation \n| \n255\nFor example, if 90% of your samples belong to class A and 10% of your samples\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\nsamples belong to class A and 10% of samples belong to class B.\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\ngeneralization performance. In the case of only 10% of samples belonging to class B,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\n258 \n| \nChapter 5: Model Evaluation and Improvement\nCross-validation with groups\nAnother very common setting for cross-validation is when there are groups in the\ndata that are highly related. Say you want to build a system to recognize emotions\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\nperson is captured multiple times, showing various emotions. The goal is to build a\nclassifier that can correctly identify emotions of people not in the dataset. You could\nuse the default stratified cross-validation to measure the performance of a classifier\nhere. However, it is likely that pictures of the same person will be in both the training\nand the test set. It will be much easier for a classifier to detect emotions in a face that\nis part of the training set, compared to a completely new face. To accurately evaluate\nthe generalization to new faces, we must therefore ensure that the training and test\nsets contain images of different people.\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\nthat we can use to indicate which person is in the image. The groups array here indi‐\ncates groups in the data that should not be split when creating the training and test\nsets, and should not be confused with the class label.\nThis example of groups in the data is common in medical applications, where you\nmight have multiple samples from the same patient, but are interested in generalizing\nto new patients. Similarly, in speech recognition, you might have multiple recordings\nof the same speaker in your dataset, but are interested in recognizing speech of new\nspeakers. Grid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\ning more than one model on the test set and choosing the better of the two will result\nin an overly optimistic estimate of how accurate the model is.\nGrid Search with Cross-Validation\nWhile the method of splitting the data into a training, a validation, and a test set that\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\nexactly the data is split. From the output of the previous code snippet we can see that\nGridSearchCV selects 'C': 10, 'gamma': 0.001 as the best parameters, while the\noutput of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\nbest parameters. For a better estimate of the generalization performance, instead of\nusing a single split into a training and a validation set, we can use cross-validation to\nevaluate the performance of each parameter combination. This method can be coded\nup as follows:\nGrid Search \n| \n263\nIn[21]:\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters,\n        # train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        # perform cross-validation\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),\neter is searching over interesting values but the C parameter is not—or it could mean\nthe C parameter is not important.\nThe third panel shows changes in both C and gamma. However, we can see that in the\nentire bottom left of the plot, nothing interesting is happening. We can probably\nexclude the very small values from future grid searches. The optimum parameter set‐\nting is at the top right. As the optimum is in the border of the plot, we can expect that\nthere might be even better values beyond this border, and we might want to change\nour search range to include more parameters in this region.\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\ngood way to explore the importance of different parameters. However, you should\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\n270 \n| \nChapter 5: Model Evaluation and Improvement\nuation of the test set should happen only once we know exactly what model we want\nto use.\nSearch over spaces that are not grids\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\ndepending on which kernel is chosen, other parameters will be relevant. If ker\nnel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\nboth the C and gamma parameters are used (but not other parameters like degree). In\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\nmake sense: if kernel='linear', gamma is not used, and trying different values for\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\niris.data, iris.target, random_state=0)\nThe grid_search object that we created behaves just like a classifier; we can call the\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\nrun cross-validation for each combination of parameters we specified in param_grid:\nIn[27]:\ngrid_search.fit(X_train, y_train)\nFitting the GridSearchCV object not only searches for the best parameters, but also\nautomatically fits a new model on the whole training dataset with the parameters that\nyielded the best cross-validation performance. What happens in fit is therefore\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\nGridSearchCV class provides a very convenient interface to access the retrained\nmodel using the predict and score methods. To evaluate how well the best found\nparameters generalize, we can call score on the test set:\nIn[28]:\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\nOut[28]:\nTest set score: 0.97\nChoosing the parameters using cross-validation, we actually found a model that ach‐\nieves 97% accuracy on the test set. The important thing here is that we did not use the\ntest set to choose the parameters. The parameters that were found are scored in the\n266 \n| \nChapter 5: Model Evaluation and Improvement\nbest_params_ attribute, and the best cross-validation accuracy (the mean accuracy\nover the different splits for this parameter setting) is stored in best_score_:\nIn[29]:\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[29]:\nBest parameters: {'C': 100, 'gamma': 0.01}\nBest cross-validation score: 0.97\nAgain, be careful not to confuse best_score_ with the generaliza‐\ntion performance of the model as computed by the score method\non the test set. Using the score method (or evaluating the output of\nthe predict method) employs a model trained on the whole train‐\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nGrid Search \n| \n267\ncontains a lot of details, as you can see in the following output, and is best looked at\nafter converting it to a pandas DataFrame:\nIn[31]:\nimport pandas as pd\n# convert to DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n# show the first 5 rows\ndisplay(results.head())\nOut[31]:\n    param_C   param_gamma   params                        mean_test_score\n0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\n1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\n2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\n3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\n4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\n0               22              0.375           0.347           0.363\n1               22              0.375           0.347           0.363\n2               22              0.375           0.347           0.363\n3               22              0.375           0.347           0.363\n4               22              0.375           0.347           0.363\n       split3_test_score  split4_test_score  std_test_score\n0           0.363              0.380           0.011\n1           0.363              0.380           0.011\n2           0.363              0.380           0.011\n3           0.363              0.380           0.011\n4           0.363              0.380           0.011\nEach row in results corresponds to one particular parameter setting. For each set‐\nting, the results of all cross-validation splits are recorded, as well as the mean and\nstandard deviation over all splits. As we were searching a two-dimensional grid of\nparameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\nspond to C and gamma:\nIn[32]:\nscores = np.array(results.mean_test_score).reshape(6, 6) compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317 compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\n8       0.93      0.90      0.91        48\n          9       0.96      0.94      0.95        47\navg / total       0.95      0.95      0.95       450\n298 \n| \nChapter 5: Model Evaluation and Improvement\nUnsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\nsions with this class. For class 7, on the other hand, precision is 1 because no other\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\nand 3.\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\npute one binary f-score per class, with that class being the positive class and the other\nclasses making up the negative classes. Then, these per-class f-scores are averaged\nusing one of the following strategies:\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\nweight to all classes, no matter what their size is.\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\ntheir support. This is what is reported in the classification report.\n• \"micro\" averaging computes the total number of false positives, false negatives,\nand true positives over all classes, and then computes precision, recall, and f-\nscore using these counts.\nIf you care about each sample equally much, it is recommended to use the \"micro\"\naverage f1-score; if you care about each class equally much, it is recommended to use\nthe \"macro\" average f1-score:\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317 In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\") In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\ninformation on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\naccurate predictions, but in using these predictions as part of a larger decision-\nmaking process. Before picking a machine learning metric, you should think about\nthe high-level goal of the application, often called the business metric. The conse‐\nquences of choosing a particular algorithm for a machine learning application are\nEvaluation Metrics and Scoring \n| \n275\n2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\nimpact” being used in that realm.\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\ndecreasing the number of hospital admissions. It could also be getting more users for\nyour website, or having users spend more money in your shop. When choosing a\nmodel or adjusting parameters, you should pick the model or parameter values that\nhave the most positive influence on the business metric. Often this is hard, as assess‐\ning the business impact of a particular model might require putting it in production\nin a real-life system.\nIn the early stages of development, and for adjusting parameters, it is often infeasible\nto put models into production just for testing purposes, because of the high business\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\ncapabilities of a self-driving car by just letting it drive around, without verifying it\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nfunction used for model selection and model evaluation. The theory of how to make\nbusiness decisions from the predictions of a machine learning model is somewhat\nbeyond the scope of this book.7 However, it is rarely the case that the end goal of a\nmachine learning task is building a model with a high accuracy. Make sure that the\nmetric you choose to evaluate and select a model for is a good stand-in for what the\nmodel will actually be used for. In reality, classification problems rarely have balanced\nclasses, and often false positives and false negatives have very different consequences.\n302 \n| \nChapter 5: Model Evaluation and Improvement\nMake sure you understand what these consequences are, and pick an evaluation met‐\nric accordingly.\nThe model evaluation and selection techniques we have described so far are the most\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\ndescribed them in this chapter can only be applied to a single supervised model. We\nhave seen before, however, that many models require preprocessing, and that in some\napplications, like the face recognition example in Chapter 3, extracting a different\nrepresentation of the data can be useful. In the next chapter, we will introduce the\nPipeline class, which allows us to use grid search and cross-validation on these com‐\nplex chains of algorithms.\nSummary and Outlook \n| \n303\nCHAPTER 6\nAlgorithm Chains and Pipelines\nFor many machine learning algorithms, the particular representation of the data that\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\nthe data and combining features by hand and goes all the way to learning features\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
            "summary": "For regression, scikit-learn uses the standard k-fold cross-validation by default. We can adjust the number of folds that are used in the classifier using the cv parameter. For most use cases, the defaults work well, but there are some cases where you might want to use a different strategy. For example, using one fold as a test set would not be very informative about the overall performance of the classifiers. It would be possible to also try to make each fold representative of the different values                the regression target has, but this is not a commonly used strategy and would be sur‐                prising to most users. For more information, visit the project's website. The k-fold cross-validation is used to repro‐duce someone else’s results. To do this, we first have to import the KFold splitter class from the model_selection module and instantiate it with the number of folds we want to use. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score We can verify that it is indeed a really bad idea to use three-fold (nonstrati‐fied) cross-validation on the iris dataset. We still have a single split of the data into training and test sets. This might make our results unstable and make us depend too much on this single split. We can go a step further, and instead of splitting the original data                into training andtest sets once, use multiple splits of cross- validation. This will resultin what is called nested cross- validateation. In nested cross validation, there is an outerloop over splits of theData. For each of them, a grid search is run (which might result in different best parameters for each Scores tell us how well a model generalizes, given the best parameters found by the grid. As it doesn’t provide a model that can be used on new data, nested cross-validation is rarely used when looking for a predictive model to apply to future data. However, it can be useful for evaluating whether a given model works on a particular data set.Implementing nested cross_val_score in scikit-learn is straightforward. We call it cross_Val_score with an instance of GridSearchCV as the model. Then, for each outer split, the test set score using the best settings is reported. We used stratified five-fold cross-validation in both the inner and the outerloop. As our param_grid contains 36 combinations of parameters, this results in a score of 0.98. The result can be summarized as “SVC can achieve 98%mean cross- validation accuracy on the iris dataset’. The parameters of the cross_val_score func are the model we want to evaluate, the training data, and the ground-truth labeling. The model evaluation and Improvement section of this article focuses on the scikit-learn version of cross-validation. The full article can be found at: http://www. Let’s evaluate LogisticRegression on the iris dataset. By default, cross_val_score performs three-fold cross-validation. We can change the number of folds used by changing the cv parame‐ter. The results of the analysis are shown in the figure below. The full code can be downloaded from the GitHub repository. A common way to summarize the cross-validation accuracy is to compute the mean. We can conclude that we expect the model to be around 96% accurate on average. There is a relatively high variance in the Accuracy between folds, ranging from 100% accuracy to 90% accuracy. This couldimply that the model is very dependent on the particular folds used for training, but it could also be due to the fact that each fold corresponds to one of the classes in the iris dataset, and so nothing can be learned from it. The model is now ready to be used in the next phase of the project. The next step is to evaluate the model and improve it. Shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. If we shuffle the data, we need to fix the random_state to get a reproducible shuffling. Shuffling the data before splitting it yields a much better result. You can think of leave-one-out cross-validation as k-fold cross- validation where each fold is a singlesample. Another frequently used cross- validate method is leave- one-out. It is a simple way to test whether a given data set is valid for a given test. It can be used to test the validity of a given set of data. For each split, you pick a single data point to be the test set. This is fundamentally different from how new data looks to the model. Remember that the test part in each split in the cross-validation is part of the training set. We then use the information from the entire training set to find the right scaling of the data. This can be very time consuming, particularly for large datasets, but sometimes provides better esti‐mates on small datasets. For more information, see sklearn.model_selection and sklearnlearn.com/learn/learn-model-selection. The final version of this article includes an improved version of the code used in the Figure 6-1. Data usage when preprocessing outside the cross-validation loop. This will lead to overly optimistic results. Figure 6-2. Data use when pre processing outside theCross-Validation Loop. This could lead to suboptimal results. Figures 6-3. Data used when pre-processing outside of the Cross-Valuation Loop. These data are used to help with the analysis of the data. Figures 7-8. The data used for this article was taken from the following website: http://www.npr.org.uk/2013/01/29/14/383838/data-use-in The Pipeline class is a class that allows “gluing” together multiple processing steps into a single scikit-learn estima‐Parameter Selection with Preprocessing. The Pipeline class has fit, predict, and score methods and behaves just like any other model in scik it-learn. With one exception: the name can’t contain a double underscore, so the Pipeline class’s name should not be “Pipeline”. The most common use case of the Pipeline is cross-validation. Figure 5-3 illustrates running four iterations of splitting a dataset of 10 points. Each split samples train_size many points for the training set. This splitting isrepeated n_iter times. Another, very flexible strategy for cross- validateation is shuffle-split cross- validation. This strategy uses fractions of the whole dataset instead of absolute sizes. The Pipeline is available on GitHub for $1.99 per pipeline. For more information, visit the Pipeline website. The most popular use cases of the pipeline are cross- Validation and shuffle-Split Cross-Validation Shuffle-split cross-validation allows for control over the number of iterations inde‐pendently of the training and test sizes. It also allows for using only part of the data in each iteration, by providing train_size and test_size settings that don’t add up to one. The code splits the dataset into 50% training set and 50% test set for 10iterations. ShuffleSplit with 10 points,Train_size=5, test_ size=2, and The main downside of the use of cross-validation is the time it takes to train all these models. Subsampling the data in this way can beuseful for experimenting with large datasets. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit, which can provide more reliable results for classification tasks. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross- validation, we need to train 36 * 5 = 180 models. The following visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. For each parameter setting (only a subset is shown), five accuracy values are compu‐                ted, one for each split in the cross-validation. Then the mean validation accuracy iscomputed for each parameters setting. The parameters with the highest mean valida‐                tion accuracy are chosen, marked by the circle. For this reason,many people use the term cross- validateation colloquially to refer to grid search with cross- validation. The results of this study are published in the open-source version of the book  grid search with cross-validation is such a commonly used method to adjustparameters. scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. To use the Grid search CV class, you first need to specify theparameters GridSearchCV will then per‐score = clf.score(X[inner_test], y[ inner_test]) cv_scores.append(score) # compute mean score over inner folds over the iris dataset. mean_score = np.mean(cv_score) if mean_ score > best_score: # if better than so far, remember parameters. best_ score = mean Grid search and cross-validation are ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. This means that building a. model using a particular parameter setting on a particular cross.validation split can be done completely independently from the other. parameter settings and models.     0.967  0. 967   1.9 Scikit-learn does not allow nesting of parallel operations. You can set n_jobs=-1 to use all available cores. When using train_test_split, we usually use 75% of the data for training and 25% for evaluation. More data will usually result in more accurate models. The main disadvantage of cross-validation is increased computational cost. If you are using the n_ jobs option on your model (for example, a random forest), when applied to new data, use the train_ test_split option instead of the train-test-split option. It will save you time and reduce the amount of data you have to work with It is important to keep in mind that cross-validation is not a way tobuild a model that can be applied to new data. When calling cross_val_score, multiple models are built internally. The purpose of cross- validation is only to evaluate how well a given algorithm will generalize when trained on a specific dataset.Splitting the dataset into k folds by starting with the first one-k-th part of the data might not always be a good idea. It is possible to train k models instead of a single model by doing a single split. The iris dataset is a set of labels. The labels are sorted by the class 0. The class 1 labels are the labels that are most likely to be used. The iris data is stored in a file called iris.datasets. We can use the iris file to test our models. We will show the results of our tests in the next section of this article. We hope that this will help you with your own models. The next part of the article will show how we can improve our models by adding more features. We are now ready for the next phase of the project. We’ll show you how we improve our model by adding new features and adding more labels. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐ tumultuous our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using five-fold split, it can make predictions for data that was not observed during training. Another benefit of cross-validation as compared to using a single split of theData is that we use our data more effectively when using train_test_split, we usually use 75% of the dataset for training and 25% for evaluation. Cross-validation is a statistical method of evaluating generalization performance. It is more stable and thorough than using a split into a training and a test set. We will also discuss grid search, an effective method for adjusting the parameters insupervised models for the best generalized performance. We'll also discuss methods to evaluate classification and regression performance that go beyond the default measures of accuracy and R2 provided by the score method. The most commonly used version of cross-validated is k-fold cross- validation, where k is a user-specified number, usually 5 or 10. The data is first partitioned into five parts of (approximately) equal size, called folds. The process is illustrated in Figure 5-1. The first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The process is repeated using folds 3, 4, and 5 as test sets. In the end, we have collected five accuracy values. For each of these five splits of the data into training and test sets, we compute theiablyaccuracy. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Data splitting in five-fold cross-validation can be difficult. Each fold corresponds to one of the classes in the iris dataset. Another way to resolve this problem is to shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. We can do that by setting the shuffle parameter of KFold to True. If we shuffle theData, we need to fix the random_state to get a reproducible shuffling.    0.0.0 is the number of folds that the data can be split into. It is the same number as the score for the first five folds of the data splitting. Another frequently used cross-validation method is leave-one-out. For each split, you pick a single data point to be the test set. Each run of cross_val_score would yield a different result, as each time a different split would be used. Shuffling the data beforesplitting it yields a much better result. You can think of leave-One-out cross- validateation as k-fold cross- validation. Cross-validation can be very time consuming, particularly for large datasets. When using a single split of the data, we usually use 75% of our data for training and 25% of it for evaluation. With five-foldCross-Validation, in each iteration we can use four-fifths of theData (80%) to fit the model. With 10-fold cross-validated, we can using nine-tenths of the Data (90%) tofit the model, and an accuracy of 0.95% for each iteration. We can use our data more effectively by splitting it into smaller pieces. Cross-validation is not a way to build a model that can be applied to new data. More data will usually result in more accurate models. When calling cross_val_score, multiple models are built internally, but the purpose is only to evaluate how well a given algorithm will generalize on a specific dataset. Splitting the dataset into k folds by starting with the first one-k-th part of the data, might not always be a good idea. The main disadvantage of cross-validated data is increased computational cost, as we are training k models instead of a single model. It is roughly ktimes slower than doing a single split of theData. For regression, scikit-learn uses the standard k-fold cross-validation by default. For example, let’s have a look at the iris dataset: 254. The first third of the data is the class 0, the second third is theclass 1. Using this fold as a test set would not be very informative about the overall performance of the classifier. For more information on how to use sklearn, please visit: http://www.sklearn.org/en/sklearn-demo/learn-model-evaluation-and-improvement.html. For the full version of this article, please go to: http: www.klearn.com/demo Scikit-learn allows for more control over what happens during the splitting of the data. For most use cases, the defaults of k-fold cross-validation for regression and stratified k- fold for classification work well. But there are some cases where you might want to use a different strategy. For example, if you want to repro-duce someone else’s results, you might need to change the way the data is broken up. For more information, visit scikitlearn.org or follow us on Twitter @scikitllearn and @researchers_reparations. Back to the page you came from. We can test that cross-validation on the iris dataset is bad. To do this, we first have to import the KFold splitter class from the model_selection module. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score. We can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 90% accuracy. And we can verify that it is indeed a really bad idea to use three-fold (nonstrati­fied) cross- validation on theiris dataset. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐tive our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using train_test_split, we usually use 75% of theData for training and 25% ofThe data for evaluation. The model needs to generalize well to all of the samples in the dataset for all of its cross-validation scores to be The first third of the data is the class 0, the second third is theclass 1, and the last third is class 2. Imagine doing three-fold cross-validation on this dataset. The first fold would be only class 0. As the classes in training and test sets would be different for all three splits, the three- fold cross- validation accuracy would be zero. That is not very helpful, as we can do much better than 0% accuracy on iris. The simple k-fold strategy fails here, so scikit-learn does not use it for classifica‐phthaltion, but rather uses stratified k- Fold Cross-Validation (KFC) In stratified cross-validation, we split the data such that the proportions between classes are the same in each fold as they are in the whole dataset. For example, if 90% of samples belong to class A and 10% of your samples                belong to class B, then stratifiedCross-Validation ensures that in each Fold 90% are of class A. It is usually a good idea to use stratified k-fold cross- validation to evaluate a classifier, because it results in more reliable estimates of generalization performance. In the case of only 10 per cent of samples belonging toclass B, it might easily happen that one fold only con‐ purposefullytains samples ofclass A. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit. Subsampling the data in this way can beuseful for experimenting with large datasets. The default stratified cross-validation can be used to measure the performance of a classifier. The classifier can also be used as a test set to see how well it understands a given set of data. For more information on how to use the classifier in your project, go to: http://www.crankstudio.com/classifiers/classifier.html#classifier-performance-evaluation-and-improvement. For information about how to test your classifier, visit:  It is likely that pictures of the same person will be in both the training and test set. To accurately evaluate the generalization to new faces, we must ensure that the training set contains images of different people. To achieve this, we can use GroupKFold, which takes an array of groups as argument. The groups array here indi‐phthalcates groups in the data that should not be split when creating the training. and testsets, and shouldn't be confused with the class label. It is common in medical applications, where you might have multiple samples from the same patient, but are interested in generalizing to new patients.    The group array can be used to indicate which person is in the image. In speech recognition, you might have multiple recordingsof the same speaker in your dataset, but are interested in recognizing 6. Algorithm Chains and Pipelines. Grid Search with Cross-Validation. Using Evaluation Metrics in Model Selection. Keeping the End Goal in Mind. Scoring for Binary Classification and Multiclass Classification. Using Metrics for Multimodal Classification and Multi-classification. Using the Metrics and Scoring tool for Binary and Multipliericlass Classes. Building Pipelines       The General Pipeline Interface is a set of steps and parameters that are used to select a pipeline for construction. The Pipeline is a series of steps, steps, and parameters used to build a pipeline. The method of splitting the data into a training, a validation, and a test set is workable, and relatively commonly used. It is quite sensitive to how exactly the data is split. Strictly speaking, evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is. It's good practice to do all exploratory analysis and model selection using the combination of a training and a validation set. The test set should be reserved for a final evaluation, even for exploratory visualization. The output of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the best parameters. Cross-validation can be used to evaluate the performance of each parameter combination. Instead of a single split into a training and a validation set, we The method can be coded up as follows:. Search                  |    263                 In[21]:. For gamma in [0.001, 0.1, 1, 10, 100]:. Test the accuracy of the SVM using a particular setting of C and gamma using. score = clf.score(X[inner_test], y[inner-test]):. If score > best_score:. best_ score = score;. If mean_score is higher than best-score, it means the model is more accurate. For more information, see the sklearn.model_selection page. The code for the search function can be found here:. Grid search and cross-validation are ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. This means that building a. model using a particular parameter setting on a particular cross.validation split can be done completely independently from the other. parameter settings and models.     0.967  0. 967   1.9 The third panel shows changes in both C and gamma. You can set n_jobs=-1 to use all available cores. You should be aware that scikit-learn does not allow nesting of parallel operations. If you are using the n_ jobs option on your model (for example, a random forest), you might be searching over interesting values but the C parameter is not. This could mean that the C parameters are not important. The optimum parameter set‐ting is at the top right. As the optimum is in the border of the plot, we can expect that there might be even better values beyond this border, and we might want to changeour search range to include more parameters in this region. In some cases, trying all possible combinations of all parameters is not a good idea. For example, SVC has a kernel parameter, and depending on which kernel is chosen, other parameters will be relevant. To deal with these kinds of “conditional” parameters, GridSearchCV allows the param_grid to be a list of dictionaries. However, you should not test different parameter ranges on the final test set—as we discussed earlier, eval‐270270 is not the same as eval-270. For more information on how to use GridSearch CV, visit the Grid searchCV website. The latest version of this article can be downloaded for free from the Google Play store. The grid_search object behaves just like a classifier. We can call the methods fit, predict, and score on it. When we call fit, it will run cross-validation for each combination of parameters we specified in param_grid. The GridSearchCV class provides a very convenient interface to access the retrained model using the predict and score methods. It also searches for the best parameters, but alsoautomatically fits a new model on the whole training dataset with the parameters that yield the best cross- validation performance. It is therefore roughly equivalent to the result of the In[21] code we saw at the beginning of this section. It can be used to test the accuracy of the model. Choosing the parameters using cross-validation, we actually found a model that ach‐ encompasses 97% accuracy on the test set. The important thing here is that we did not use the Test set to choose the parameters. To evaluate how well the best foundparameters generalize, we can call The parameters that were found are scored in the parameters attribute. The best cross-validation accuracy (the mean accuracy) is stored in best_score_:                . The parameters are stored in the best_params_ attribute, and the best score is stored as best_ score_:                            Best parameters: {'C': 100, 'gamma': 0.01, Using the score method (or evaluating the output of the predict method) employs a model trained on the whole train. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine, the main downside of the use of cross- validation is the time it takes to train all these models. The following visualization (Figure 5-6) illustrates how the best parameter setting isselected in the preceding code. For each parameter setting (only a subset is shown), five accuracy values are compu‐ purposefullyted, one for each split in the cross- validate. Then the mean validation accuracy iscomputed for The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a GridSearchCV can be used to find better parameters for SVC. To use the GridSearchCV class, you first need to specify theparameters you want to search over using a dictionary. GridSearch CV will then learn an SVM on the scaled training data and score the test data. For example, the test score is 0.95. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, For each split in the cross-validation process, some part of the original training set will be declared the training part. The test part is used to measure what new data willlook like to a model trained on the trainingpart. However, we already used the infor‐mation contained in the test part when scaling the data The data is best looked at after converting it to a pandas DataFrame. Remember that the test part in each split in the cross-validation is part of the training set. We used the information from the entire training set to find the right scaling of the data. The output of the search function is shown in the following output. Each row in particular corresponds to one particular setting in the data set. For example, the first 5 rows show the first five rows of the first set of results in the search. For the second and third sets of results, each row corresponds to a particular row in the second set. The third row shows the first row of the second group of results. For each set, the results of all cross-validation splits are recorded. This is best visualized as a heat map (Figure 5-8) First we extract the mean validation scores, then we reshape the scores so that the axes corre‐                spond to C and gamma. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate. It pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation andselection. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make is not a good measure of predictive performance, as it does not contain all the information we are interested in. For more information, see the Model Evaluation and Improvement section of the book. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier, which makes random predictions. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. This makes it very                hard to judge which of these results is actually helpful. For any concerns about the accuracy of the classifier, please contact us at [email protected] or @sklearn. Accu‐ Disorders is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐ Disorders that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. We will use confusion matrices to represent the result of evaluating binary clas‐ Disorders in the next section of the book. The predictions of LogisticRegresuo’s We already stored the predictions on the test set in pred_logreg: evaluations and scoring. We import confusion_matrix from sklearn.metrics. We use clustering and clustering clustering models. We also use a bag-of-words representation for movie reviews and a business metric for SVC. We show how to use these models in a number of different ways. We will also show how we can use them in other ways, such as for cancer diagnoses. We hope this will be a useful tool to help people understand how to make better use of machine learning in the real world. We'll also discuss how to apply these techniques to other areas of computer science. There are 25 examples of, 26goals for, 25                iris classification example, 14                k 6. regression problems, 26. classifiers, 75. decision trees, 75, 80. regression trees and 75. regression regression regressors. 7. clustering algorithms, 168. algorithms for clustering, 182-187. 8. validation methods for, 131. grid search with Cross-Validation. 9. validation techniques for,131. validation of the clustering algorithm. 10. validation procedures for, 130. validation for the grid search algorithm. Pipelines. Algorithm Chains and Pipelines. The General Pipeline Interface. Using Pipelines in Grid Searches. Preprocessing Steps and Model parameters. Pipeline Creation with make_pipeline. Using Pipeline Attributes in a Grid-Searched Pipeline. Using pipeline attributes in a grid-searched pipeline. Using the Pipeline Interface to create a Pipeline. Use the Pipeline interface to make a Pipeline creation. Use pipeline attributes to create Pipelines Binary classification is arguably the most common and conceptually simple applica‐tion of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. We will also look at the impact of choosing one model over another and how to use these metrics in the real world. We hope this article has helped you better understand machine learning and its potential applications to your business. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available on Amazon Kindle and the Mac. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. Unsurprisingly, precision and recall are a perfect 1 for class 0. For class 7, on the other hand, precision is 1 because no otherclass was mistakenly classified as 7. We can also see that the model has particular difficulties with classes 8 and 3. The most commonly used metric for imbalanced datasets is the multiclass version of the f-score. The idea behind this metric is to com‐                pute one binary f- Score per class, with that class being the positive class and the other Classes making up the negative classes. Then, these per-class f-scores are averaged using one of the following strategies: \"macro\" averaging computes the unweighted per- class f- scores. \"Weighted\" averaging computes the mean of the per-class f-scores. This gives equalweight to \"Micro\" averaging computes the total number of false positives, false negatives, and true positives over all classes. \"Macro\" average f1-score computes precision, recall, and f-score using these counts. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. The default classifier makes random predictions but produces classes with the same proportions as in the training set. It produces very good results, with a score of 0.98. Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Evaluation Metrics and Scoring are used to test the predictions on the test set in pred_logreg. The predictions are based on the tests set in the pred_ logreg. We already have the predictions for the test in place. We can use these predictions to help students understand how to use the Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. Pipelines can be used to select parameters for a model or to build a pipeline. In most applications we’ve seen, using the default R2 used in the score grotesquemethod of all regressors is enough. We’ll use the R2 as the default method in the rest of the article. We hope this article has helped you with your understanding of how to use pipelines in your project. We are happy to help you with any questions you may have about the project. Please send us your questions in the comments below or email them to jennifer R2 is a moreintuitive metric to evaluate regression models. Scoring can be used in both GridSearchCV and cross_val_score. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUCscore. The scoring argument is a very simple way to achieve this, via the scoring argument that can be found in GridSearch CV or cross_Val_ score. It can also be used to select a model using the gridSearchCV or cross-Val_score argument. For more information on scikit-learn, visit The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐consuming \"roc_auc\" as the scoring parameter. In[66]: grotesqueprint(\"Micro average f1 score: {:.3f}\".format) In[67]: grotesque print(\"Macro average f2 score: 0.953\") In[68]: grotesque Print(f1_score(y_test, pred, average=\"micro\"), average=\"macro\"), false (false, false, false), true (false), false) R2 is a moreintuitive metric to evaluate regression models. Scoring can be used in both GridSearchCV and cross_val_score. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUCscore. The scoring argument is a very simple way to achieve this, via the scoring argument that can be found in GridSearch CV or cross_Val_ score. It can also be used to select a model using the gridSearchCV or cross-Val_score argument. For more information on scikit-learn, visit The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐                ing \"roc_auc\" as the scoring parameter. We will focus on the supervised methods, regression and classification, as evaluating models in unsupervised learning is often a very qualitative process. We have split our dataset into a training set and a test set using the train_test_split function. We built a model on the training set by calling the fit method, and evaluated it on the test set by the score method, which for classification computes the fraction of correctly classified samples. We are now ready to move on to the next chapter. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. We will first intro‐                duce cross-validation, a more robust way to assess generalization performance. We will then look at how we can use the metrics to assess performance in a variety of different ways. We'll also look at the use of Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. In this chapter we discussed cross-validation, grid search, evaluation metrics, and improving machine learning algorithms. We also discussed building Pipelines and the General Pipeline Interface. In the next chapter, we will look at the use of pipelines in the context of machine learning. We will also look at how machine learning can be used to improve the performance of a machine learning algorithm. The next chapter will be published on November 14, 2014. We hope to see you in the next few chapters. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot(according to accuracy), while Logisticregression produces very good results. Even the random classifier yields over 80% accuracy. The most commonly used form (as described ear­lier) is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring. For example, in the example above, the score is 0.98 Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the end goal is equally important in science, though the authors are not aware of a similar phrase to “business impact” being used in that realm. The conse‐quences of choosing a particular algorithm for a machinelearning application are discussed in the section “Machine Learning Algorithms”. The article was originally published in the online magazine “The Next Web’s” January 2013 issue, which also featured the “Artificial Intelligence and Machine Learning When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production. It could also be getting more users for your website, or having users spend more money in your shop. It is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidancecapabilities of a self-driving car by just letting it drive around, without verifying it first. If your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using The theory of how to make business decisions from the predictions of a machine learning model is somewhat beyond the scope of this book. Make sure that the metrics you choose to evaluate and select a model for is a good stand-in for what the model will actually be used for. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. The model evaluation and selection techniques we have described so far are the mostimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’vedescribed them in this chapter can only be applied to a single supervised model. Many models require preprocessing, and in some cases extracting a differentrepresentation of the data can be useful. In reality, classification problems rarely have balanced classes, and often false positives and false negatives have very different consequences. Make sure you understand what these consequences are, and pick an evaluation met‐                 The particular representation of the data that you provide is very important. This starts with scaling the data and combining features by hand and goes all the way to learning features for compute. In the next chapter, we will introduce the horriblyPipeline class, which allows us to use grid search and cross-validation on these com‐plex chains of algorithms. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any. application, we would call a positive test (an indi‐cation of cancer) the positiveclass, and anegative test the negative class. If you have any questions about our model, please email us at jennifer.",
            "children": [
                {
                    "id": "chapter-5-section-1",
                    "title": "Cross-Validation",
                    "content": "using standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ndescribed earlier, we still have a single split of the data into training and test sets,\nwhich might make our results unstable and make us depend too much on this single\nsplit of the data. We can go a step further, and instead of splitting the original data\ninto training and test sets once, use multiple splits of cross-validation. This will result\nin what is called nested cross-validation. In nested cross-validation, there is an outer\nloop over splits of the data into training and test sets. For each of them, a grid search\nis run (which might result in different best parameters for each split in the outer\nloop). Then, for each outer split, the test set score using the best settings is reported.\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\nting. The scores tell us how well a model generalizes, given the best parameters found\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\nvalidation is rarely used when looking for a predictive model to apply to future data.\nHowever, it can be useful for evaluating how well a given model works on a particular\ndataset.\nImplementing nested cross-validation in scikit-learn is straightforward. We call\ncross_val_score with an instance of GridSearchCV as the model:\nIn[34]:\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\nOut[34]:\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\nMean cross-validation score:  0.98\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\nless.\nHere, we used stratified five-fold cross-validation in both the inner and the outer\nloop. As our param_grid contains 36 combinations of parameters, this results in a\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement\nCross-Validation in scikit-learn\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\ntion from the model_selection module. The parameters of the cross_val_score\nfunction are the model we want to evaluate, the training data, and the ground-truth\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\nIn[4]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\niris = load_iris()\nlogreg = LogisticRegression()\nscores = cross_val_score(logreg, iris.data, iris.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[4]:\nCross-validation scores: [ 0.961  0.922  0.958]\nBy default, cross_val_score performs three-fold cross-validation, returning three\naccuracy values. We can change the number of folds used by changing the cv parame‐\nter:\nIn[5]:\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[5]:\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\nA common way to summarize the cross-validation accuracy is to compute the mean:\nIn[6]:\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nOut[6]:\nAverage cross-validation score: 0.96\nUsing the mean cross-validation we can conclude that we expect the model to be\naround 96% accurate on average. Looking at all five scores produced by the five-fold\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n| \nChapter 6: Algorithm Chains and Pipelines\nThis is fundamentally different from how new data looks to the model. If we observe\nnew data (say, in form of our test set), this data will not have been used to scale the\ntraining data, and it might have a different minimum and maximum than the train‐\ning data. The following example (Figure 6-1) shows how the data processing during\ncross-validation and the final evaluation differ:\nIn[4]:\nmglearn.plots.plot_improper_processing()\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\nSo, the splits in the cross-validation no longer correctly mirror how new data will\nlook to the modeling process. We already leaked information from these parts of the\ndata into our modeling process. This will lead to overly optimistic results during\ncross-validation, and possibly the selection of suboptimal parameters.\nTo get around this problem, the splitting of the dataset during cross-validation should\nbe done before doing any preprocessing. Any process that extracts knowledge from the\ndataset should only ever be applied to the training portion of the dataset, so any\ncross-validation should be the “outermost loop” in your processing.\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\nParameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\nshuffle-split cross-validation, each split samples train_size many points for the\ntraining set and test_size many (disjoint) point for the test set. This splitting is\nrepeated n_iter times. Figure 5-3 illustrates running four iterations of splitting a\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\neach (you can use integers for train_size and test_size to use absolute sizes for\nthese sets, or floating-point numbers to use fractions of the whole dataset):\nIn[14]:\nmglearn.plots.plot_shuffle_split()\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\nThe following code splits the dataset into 50% training set and 50% test set for 10\niterations:\nIn[15]:\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\nOut[15]:\nCross-validation scores:\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\nShuffle-split cross-validation allows for control over the number of iterations inde‐\npendently of the training and test sizes, which can sometimes be helpful. It also allows\nfor using only part of the data in each iteration, by providing train_size and\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest), when applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\ndiscuss methods to evaluate classification and regression performance that go beyond\nthe default measures of accuracy and R2 provided by the score method.\nWe will also discuss grid search, an effective method for adjusting the parameters in\nsupervised models for the best generalization performance.\nCross-Validation\nCross-validation is a statistical method of evaluating generalization performance that\nis more stable and thorough than using a split into a training and a test set. In cross-\nvalidation, the data is instead split repeatedly and multiple models are trained. The\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\nNext, a sequence of models is trained. The first model is trained using the first fold as\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\nFor each of these five splits of the data into training and test sets, we compute the\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement fied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\nwould be only class 0, and the training set would be only classes 1 and 2. As the\nclasses in training and test sets would be different for all three splits, the three-fold\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\nwe can do much better than 0% accuracy on iris.\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\ntion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\nsplit the data such that the proportions between classes are the same in each fold as\nthey are in the whole dataset, as illustrated in Figure 5-2:\nIn[8]:\nmglearn.plots.plot_stratified_cross_validation()\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\nwhen the data is ordered by class label\nCross-Validation \n| \n255\nFor example, if 90% of your samples belong to class A and 10% of your samples\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\nsamples belong to class A and 10% of samples belong to class B.\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\ngeneralization performance. In the case of only 10% of samples belonging to class B,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\n258 \n| \nChapter 5: Model Evaluation and Improvement\nCross-validation with groups\nAnother very common setting for cross-validation is when there are groups in the\ndata that are highly related. Say you want to build a system to recognize emotions\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\nperson is captured multiple times, showing various emotions. The goal is to build a\nclassifier that can correctly identify emotions of people not in the dataset. You could\nuse the default stratified cross-validation to measure the performance of a classifier\nhere. However, it is likely that pictures of the same person will be in both the training\nand the test set. It will be much easier for a classifier to detect emotions in a face that\nis part of the training set, compared to a completely new face. To accurately evaluate\nthe generalization to new faces, we must therefore ensure that the training and test\nsets contain images of different people.\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\nthat we can use to indicate which person is in the image. The groups array here indi‐\ncates groups in the data that should not be split when creating the training and test\nsets, and should not be confused with the class label.\nThis example of groups in the data is common in medical applications, where you\nmight have multiple samples from the same patient, but are interested in generalizing\nto new patients. Similarly, in speech recognition, you might have multiple recordings\nof the same speaker in your dataset, but are interested in recognizing speech of new\nspeakers.",
                    "summary": "For regression, scikit-learn uses the standard k-fold cross-validation by default. We can adjust the number of folds that are used in the classifier using the cv parameter. For most use cases, the defaults work well, but there are some cases where you might want to use a different strategy. For example, using one fold as a test set would not be very informative about the overall performance of the classifiers. It would be possible to also try to make each fold representative of the different values                the regression target has, but this is not a commonly used strategy and would be sur‐                prising to most users. For more information, visit the project's website. The k-fold cross-validation is used to repro‐duce someone else’s results. To do this, we first have to import the KFold splitter class from the model_selection module and instantiate it with the number of folds we want to use. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score We can verify that it is indeed a really bad idea to use three-fold (nonstrati‐fied) cross-validation on the iris dataset. We still have a single split of the data into training and test sets. This might make our results unstable and make us depend too much on this single split. We can go a step further, and instead of splitting the original data                into training andtest sets once, use multiple splits of cross- validation. This will resultin what is called nested cross- validateation. In nested cross validation, there is an outerloop over splits of theData. For each of them, a grid search is run (which might result in different best parameters for each Scores tell us how well a model generalizes, given the best parameters found by the grid. As it doesn’t provide a model that can be used on new data, nested cross-validation is rarely used when looking for a predictive model to apply to future data. However, it can be useful for evaluating whether a given model works on a particular data set.Implementing nested cross_val_score in scikit-learn is straightforward. We call it cross_Val_score with an instance of GridSearchCV as the model. Then, for each outer split, the test set score using the best settings is reported. We used stratified five-fold cross-validation in both the inner and the outerloop. As our param_grid contains 36 combinations of parameters, this results in a score of 0.98. The result can be summarized as “SVC can achieve 98%mean cross- validation accuracy on the iris dataset’. The parameters of the cross_val_score func are the model we want to evaluate, the training data, and the ground-truth labeling. The model evaluation and Improvement section of this article focuses on the scikit-learn version of cross-validation. The full article can be found at: http://www. Let’s evaluate LogisticRegression on the iris dataset. By default, cross_val_score performs three-fold cross-validation. We can change the number of folds used by changing the cv parame‐ter. The results of the analysis are shown in the figure below. The full code can be downloaded from the GitHub repository. A common way to summarize the cross-validation accuracy is to compute the mean. We can conclude that we expect the model to be around 96% accurate on average. There is a relatively high variance in the Accuracy between folds, ranging from 100% accuracy to 90% accuracy. This couldimply that the model is very dependent on the particular folds used for training, but it could also be due to the fact that each fold corresponds to one of the classes in the iris dataset, and so nothing can be learned from it. The model is now ready to be used in the next phase of the project. The next step is to evaluate the model and improve it. Shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. If we shuffle the data, we need to fix the random_state to get a reproducible shuffling. Shuffling the data before splitting it yields a much better result. You can think of leave-one-out cross-validation as k-fold cross- validation where each fold is a singlesample. Another frequently used cross- validate method is leave- one-out. It is a simple way to test whether a given data set is valid for a given test. It can be used to test the validity of a given set of data. For each split, you pick a single data point to be the test set. This is fundamentally different from how new data looks to the model. Remember that the test part in each split in the cross-validation is part of the training set. We then use the information from the entire training set to find the right scaling of the data. This can be very time consuming, particularly for large datasets, but sometimes provides better esti‐mates on small datasets. For more information, see sklearn.model_selection and sklearnlearn.com/learn/learn-model-selection. The final version of this article includes an improved version of the code used in the Figure 6-1. Data usage when preprocessing outside the cross-validation loop. This will lead to overly optimistic results. Figure 6-2. Data use when pre processing outside theCross-Validation Loop. This could lead to suboptimal results. Figures 6-3. Data used when pre-processing outside of the Cross-Valuation Loop. These data are used to help with the analysis of the data. Figures 7-8. The data used for this article was taken from the following website: http://www.npr.org.uk/2013/01/29/14/383838/data-use-in The Pipeline class is a class that allows “gluing” together multiple processing steps into a single scikit-learn estima‐Parameter Selection with Preprocessing. The Pipeline class has fit, predict, and score methods and behaves just like any other model in scik it-learn. With one exception: the name can’t contain a double underscore, so the Pipeline class’s name should not be “Pipeline”. The most common use case of the Pipeline is cross-validation. Figure 5-3 illustrates running four iterations of splitting a dataset of 10 points. Each split samples train_size many points for the training set. This splitting isrepeated n_iter times. Another, very flexible strategy for cross- validateation is shuffle-split cross- validation. This strategy uses fractions of the whole dataset instead of absolute sizes. The Pipeline is available on GitHub for $1.99 per pipeline. For more information, visit the Pipeline website. The most popular use cases of the pipeline are cross- Validation and shuffle-Split Cross-Validation Shuffle-split cross-validation allows for control over the number of iterations inde‐pendently of the training and test sizes. It also allows for using only part of the data in each iteration, by providing train_size and test_size settings that don’t add up to one. The code splits the dataset into 50% training set and 50% test set for 10iterations. ShuffleSplit with 10 points,Train_size=5, test_ size=2, and The main downside of the use of cross-validation is the time it takes to train all these models. Subsampling the data in this way can beuseful for experimenting with large datasets. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit, which can provide more reliable results for classification tasks. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross- validation, we need to train 36 * 5 = 180 models. The following visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. For each parameter setting (only a subset is shown), five accuracy values are compu‐                ted, one for each split in the cross-validation. Then the mean validation accuracy iscomputed for each parameters setting. The parameters with the highest mean valida‐                tion accuracy are chosen, marked by the circle. For this reason,many people use the term cross- validateation colloquially to refer to grid search with cross- validation. The results of this study are published in the open-source version of the book  grid search with cross-validation is such a commonly used method to adjustparameters. scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. To use the Grid search CV class, you first need to specify theparameters GridSearchCV will then per‐score = clf.score(X[inner_test], y[ inner_test]) cv_scores.append(score) # compute mean score over inner folds over the iris dataset. mean_score = np.mean(cv_score) if mean_ score > best_score: # if better than so far, remember parameters. best_ score = mean Grid search and cross-validation are ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. This means that building a. model using a particular parameter setting on a particular cross.validation split can be done completely independently from the other. parameter settings and models.     0.967  0. 967   1.9 Scikit-learn does not allow nesting of parallel operations. You can set n_jobs=-1 to use all available cores. When using train_test_split, we usually use 75% of the data for training and 25% for evaluation. More data will usually result in more accurate models. The main disadvantage of cross-validation is increased computational cost. If you are using the n_ jobs option on your model (for example, a random forest), when applied to new data, use the train_ test_split option instead of the train-test-split option. It will save you time and reduce the amount of data you have to work with It is important to keep in mind that cross-validation is not a way tobuild a model that can be applied to new data. When calling cross_val_score, multiple models are built internally. The purpose of cross- validation is only to evaluate how well a given algorithm will generalize when trained on a specific dataset.Splitting the dataset into k folds by starting with the first one-k-th part of the data might not always be a good idea. It is possible to train k models instead of a single model by doing a single split. The iris dataset is a set of labels. The labels are sorted by the class 0. The class 1 labels are the labels that are most likely to be used. The iris data is stored in a file called iris.datasets. We can use the iris file to test our models. We will show the results of our tests in the next section of this article. We hope that this will help you with your own models. The next part of the article will show how we can improve our models by adding more features. We are now ready for the next phase of the project. We’ll show you how we improve our model by adding new features and adding more labels. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐ tumultuous our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using five-fold split, it can make predictions for data that was not observed during training. Another benefit of cross-validation as compared to using a single split of theData is that we use our data more effectively when using train_test_split, we usually use 75% of the dataset for training and 25% for evaluation. Cross-validation is a statistical method of evaluating generalization performance. It is more stable and thorough than using a split into a training and a test set. We will also discuss grid search, an effective method for adjusting the parameters insupervised models for the best generalized performance. We'll also discuss methods to evaluate classification and regression performance that go beyond the default measures of accuracy and R2 provided by the score method. The most commonly used version of cross-validated is k-fold cross- validation, where k is a user-specified number, usually 5 or 10. The data is first partitioned into five parts of (approximately) equal size, called folds. The process is illustrated in Figure 5-1. The first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The process is repeated using folds 3, 4, and 5 as test sets. In the end, we have collected five accuracy values. For each of these five splits of the data into training and test sets, we compute theiablyaccuracy. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Data splitting in five-fold cross-validation can be difficult. Each fold corresponds to one of the classes in the iris dataset. Another way to resolve this problem is to shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. We can do that by setting the shuffle parameter of KFold to True. If we shuffle theData, we need to fix the random_state to get a reproducible shuffling.    0.0.0 is the number of folds that the data can be split into. It is the same number as the score for the first five folds of the data splitting. Another frequently used cross-validation method is leave-one-out. For each split, you pick a single data point to be the test set. Each run of cross_val_score would yield a different result, as each time a different split would be used. Shuffling the data beforesplitting it yields a much better result. You can think of leave-One-out cross- validateation as k-fold cross- validation. Cross-validation can be very time consuming, particularly for large datasets. When using a single split of the data, we usually use 75% of our data for training and 25% of it for evaluation. With five-foldCross-Validation, in each iteration we can use four-fifths of theData (80%) to fit the model. With 10-fold cross-validated, we can using nine-tenths of the Data (90%) tofit the model, and an accuracy of 0.95% for each iteration. We can use our data more effectively by splitting it into smaller pieces. Cross-validation is not a way to build a model that can be applied to new data. More data will usually result in more accurate models. When calling cross_val_score, multiple models are built internally, but the purpose is only to evaluate how well a given algorithm will generalize on a specific dataset. Splitting the dataset into k folds by starting with the first one-k-th part of the data, might not always be a good idea. The main disadvantage of cross-validated data is increased computational cost, as we are training k models instead of a single model. It is roughly ktimes slower than doing a single split of theData. For regression, scikit-learn uses the standard k-fold cross-validation by default. For example, let’s have a look at the iris dataset: 254. The first third of the data is the class 0, the second third is theclass 1. Using this fold as a test set would not be very informative about the overall performance of the classifier. For more information on how to use sklearn, please visit: http://www.sklearn.org/en/sklearn-demo/learn-model-evaluation-and-improvement.html. For the full version of this article, please go to: http: www.klearn.com/demo Scikit-learn allows for more control over what happens during the splitting of the data. For most use cases, the defaults of k-fold cross-validation for regression and stratified k- fold for classification work well. But there are some cases where you might want to use a different strategy. For example, if you want to repro-duce someone else’s results, you might need to change the way the data is broken up. For more information, visit scikitlearn.org or follow us on Twitter @scikitllearn and @researchers_reparations. Back to the page you came from. We can test that cross-validation on the iris dataset is bad. To do this, we first have to import the KFold splitter class from the model_selection module. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score. We can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 90% accuracy. And we can verify that it is indeed a really bad idea to use three-fold (nonstrati­fied) cross- validation on theiris dataset. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐tive our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using train_test_split, we usually use 75% of theData for training and 25% ofThe data for evaluation. The model needs to generalize well to all of the samples in the dataset for all of its cross-validation scores to be The first third of the data is the class 0, the second third is theclass 1, and the last third is class 2. Imagine doing three-fold cross-validation on this dataset. The first fold would be only class 0. As the classes in training and test sets would be different for all three splits, the three- fold cross- validation accuracy would be zero. That is not very helpful, as we can do much better than 0% accuracy on iris. The simple k-fold strategy fails here, so scikit-learn does not use it for classifica‐phthaltion, but rather uses stratified k- Fold Cross-Validation (KFC) In stratified cross-validation, we split the data such that the proportions between classes are the same in each fold as they are in the whole dataset. For example, if 90% of samples belong to class A and 10% of your samples                belong to class B, then stratifiedCross-Validation ensures that in each Fold 90% are of class A. It is usually a good idea to use stratified k-fold cross- validation to evaluate a classifier, because it results in more reliable estimates of generalization performance. In the case of only 10 per cent of samples belonging toclass B, it might easily happen that one fold only con‐ purposefullytains samples ofclass A. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit. Subsampling the data in this way can beuseful for experimenting with large datasets. The default stratified cross-validation can be used to measure the performance of a classifier. The classifier can also be used as a test set to see how well it understands a given set of data. For more information on how to use the classifier in your project, go to: http://www.crankstudio.com/classifiers/classifier.html#classifier-performance-evaluation-and-improvement. For information about how to test your classifier, visit:  It is likely that pictures of the same person will be in both the training and test set. To accurately evaluate the generalization to new faces, we must ensure that the training set contains images of different people. To achieve this, we can use GroupKFold, which takes an array of groups as argument. The groups array here indi‐phthalcates groups in the data that should not be split when creating the training. and testsets, and shouldn't be confused with the class label. It is common in medical applications, where you might have multiple samples from the same patient, but are interested in generalizing to new patients.    The group array can be used to indicate which person is in the image. In speech recognition, you might have multiple recordingsof the same speaker in your dataset, but are interested in recognizing",
                    "children": [
                        {
                            "id": "chapter-5-section-1-subsection-1",
                            "title": "Cross-Validation in scikit-learn",
                            "content": "using standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ndescribed earlier, we still have a single split of the data into training and test sets,\nwhich might make our results unstable and make us depend too much on this single\nsplit of the data. We can go a step further, and instead of splitting the original data\ninto training and test sets once, use multiple splits of cross-validation. This will result\nin what is called nested cross-validation. In nested cross-validation, there is an outer\nloop over splits of the data into training and test sets. For each of them, a grid search\nis run (which might result in different best parameters for each split in the outer\nloop). Then, for each outer split, the test set score using the best settings is reported.\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\nting. The scores tell us how well a model generalizes, given the best parameters found\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\nvalidation is rarely used when looking for a predictive model to apply to future data.\nHowever, it can be useful for evaluating how well a given model works on a particular\ndataset.\nImplementing nested cross-validation in scikit-learn is straightforward. We call\ncross_val_score with an instance of GridSearchCV as the model:\nIn[34]:\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\nOut[34]:\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\nMean cross-validation score:  0.98\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\nless.\nHere, we used stratified five-fold cross-validation in both the inner and the outer\nloop. As our param_grid contains 36 combinations of parameters, this results in a\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement\nCross-Validation in scikit-learn\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\ntion from the model_selection module. The parameters of the cross_val_score\nfunction are the model we want to evaluate, the training data, and the ground-truth\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\nIn[4]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\niris = load_iris()\nlogreg = LogisticRegression()\nscores = cross_val_score(logreg, iris.data, iris.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[4]:\nCross-validation scores: [ 0.961  0.922  0.958]\nBy default, cross_val_score performs three-fold cross-validation, returning three\naccuracy values. We can change the number of folds used by changing the cv parame‐\nter:\nIn[5]:\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[5]:\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\nA common way to summarize the cross-validation accuracy is to compute the mean:\nIn[6]:\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nOut[6]:\nAverage cross-validation score: 0.96\nUsing the mean cross-validation we can conclude that we expect the model to be\naround 96% accurate on average. Looking at all five scores produced by the five-fold\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n| \nChapter 6: Algorithm Chains and Pipelines\nThis is fundamentally different from how new data looks to the model. If we observe\nnew data (say, in form of our test set), this data will not have been used to scale the\ntraining data, and it might have a different minimum and maximum than the train‐\ning data. The following example (Figure 6-1) shows how the data processing during\ncross-validation and the final evaluation differ:\nIn[4]:\nmglearn.plots.plot_improper_processing()\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\nSo, the splits in the cross-validation no longer correctly mirror how new data will\nlook to the modeling process. We already leaked information from these parts of the\ndata into our modeling process. This will lead to overly optimistic results during\ncross-validation, and possibly the selection of suboptimal parameters.\nTo get around this problem, the splitting of the dataset during cross-validation should\nbe done before doing any preprocessing. Any process that extracts knowledge from the\ndataset should only ever be applied to the training portion of the dataset, so any\ncross-validation should be the “outermost loop” in your processing.\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\nParameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\nshuffle-split cross-validation, each split samples train_size many points for the\ntraining set and test_size many (disjoint) point for the test set. This splitting is\nrepeated n_iter times. Figure 5-3 illustrates running four iterations of splitting a\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\neach (you can use integers for train_size and test_size to use absolute sizes for\nthese sets, or floating-point numbers to use fractions of the whole dataset):\nIn[14]:\nmglearn.plots.plot_shuffle_split()\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\nThe following code splits the dataset into 50% training set and 50% test set for 10\niterations:\nIn[15]:\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\nOut[15]:\nCross-validation scores:\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\nShuffle-split cross-validation allows for control over the number of iterations inde‐\npendently of the training and test sizes, which can sometimes be helpful. It also allows\nfor using only part of the data in each iteration, by providing train_size and\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),",
                            "summary": "For regression, scikit-learn uses the standard k-fold cross-validation by default. We can adjust the number of folds that are used in the classifier using the cv parameter. For most use cases, the defaults work well, but there are some cases where you might want to use a different strategy. For example, using one fold as a test set would not be very informative about the overall performance of the classifiers. It would be possible to also try to make each fold representative of the different values                the regression target has, but this is not a commonly used strategy and would be sur‐                prising to most users. For more information, visit the project's website. The k-fold cross-validation is used to repro‐duce someone else’s results. To do this, we first have to import the KFold splitter class from the model_selection module and instantiate it with the number of folds we want to use. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score We can verify that it is indeed a really bad idea to use three-fold (nonstrati‐fied) cross-validation on the iris dataset. We still have a single split of the data into training and test sets. This might make our results unstable and make us depend too much on this single split. We can go a step further, and instead of splitting the original data                into training andtest sets once, use multiple splits of cross- validation. This will resultin what is called nested cross- validateation. In nested cross validation, there is an outerloop over splits of theData. For each of them, a grid search is run (which might result in different best parameters for each Scores tell us how well a model generalizes, given the best parameters found by the grid. As it doesn’t provide a model that can be used on new data, nested cross-validation is rarely used when looking for a predictive model to apply to future data. However, it can be useful for evaluating whether a given model works on a particular data set.Implementing nested cross_val_score in scikit-learn is straightforward. We call it cross_Val_score with an instance of GridSearchCV as the model. Then, for each outer split, the test set score using the best settings is reported. We used stratified five-fold cross-validation in both the inner and the outerloop. As our param_grid contains 36 combinations of parameters, this results in a score of 0.98. The result can be summarized as “SVC can achieve 98%mean cross- validation accuracy on the iris dataset’. The parameters of the cross_val_score func are the model we want to evaluate, the training data, and the ground-truth labeling. The model evaluation and Improvement section of this article focuses on the scikit-learn version of cross-validation. The full article can be found at: http://www. Let’s evaluate LogisticRegression on the iris dataset. By default, cross_val_score performs three-fold cross-validation. We can change the number of folds used by changing the cv parame‐ter. The results of the analysis are shown in the figure below. The full code can be downloaded from the GitHub repository. A common way to summarize the cross-validation accuracy is to compute the mean. We can conclude that we expect the model to be around 96% accurate on average. There is a relatively high variance in the Accuracy between folds, ranging from 100% accuracy to 90% accuracy. This couldimply that the model is very dependent on the particular folds used for training, but it could also be due to the fact that each fold corresponds to one of the classes in the iris dataset, and so nothing can be learned from it. The model is now ready to be used in the next phase of the project. The next step is to evaluate the model and improve it. Shuffle the data instead of stratifying the folds, to remove the ordering of the samples by label. If we shuffle the data, we need to fix the random_state to get a reproducible shuffling. Shuffling the data before splitting it yields a much better result. You can think of leave-one-out cross-validation as k-fold cross- validation where each fold is a singlesample. Another frequently used cross- validate method is leave- one-out. It is a simple way to test whether a given data set is valid for a given test. It can be used to test the validity of a given set of data. For each split, you pick a single data point to be the test set. This is fundamentally different from how new data looks to the model. Remember that the test part in each split in the cross-validation is part of the training set. We then use the information from the entire training set to find the right scaling of the data. This can be very time consuming, particularly for large datasets, but sometimes provides better esti‐mates on small datasets. For more information, see sklearn.model_selection and sklearnlearn.com/learn/learn-model-selection. The final version of this article includes an improved version of the code used in the Figure 6-1. Data usage when preprocessing outside the cross-validation loop. This will lead to overly optimistic results. Figure 6-2. Data use when pre processing outside theCross-Validation Loop. This could lead to suboptimal results. Figures 6-3. Data used when pre-processing outside of the Cross-Valuation Loop. These data are used to help with the analysis of the data. Figures 7-8. The data used for this article was taken from the following website: http://www.npr.org.uk/2013/01/29/14/383838/data-use-in The Pipeline class is a class that allows “gluing” together multiple processing steps into a single scikit-learn estima‐Parameter Selection with Preprocessing. The Pipeline class has fit, predict, and score methods and behaves just like any other model in scik it-learn. With one exception: the name can’t contain a double underscore, so the Pipeline class’s name should not be “Pipeline”. The most common use case of the Pipeline is cross-validation. Figure 5-3 illustrates running four iterations of splitting a dataset of 10 points. Each split samples train_size many points for the training set. This splitting isrepeated n_iter times. Another, very flexible strategy for cross- validateation is shuffle-split cross- validation. This strategy uses fractions of the whole dataset instead of absolute sizes. The Pipeline is available on GitHub for $1.99 per pipeline. For more information, visit the Pipeline website. The most popular use cases of the pipeline are cross- Validation and shuffle-Split Cross-Validation Shuffle-split cross-validation allows for control over the number of iterations inde‐pendently of the training and test sizes. It also allows for using only part of the data in each iteration, by providing train_size and test_size settings that don’t add up to one. The code splits the dataset into 50% training set and 50% test set for 10iterations. ShuffleSplit with 10 points,Train_size=5, test_ size=2, and The main downside of the use of cross-validation is the time it takes to train all these models. Subsampling the data in this way can beuseful for experimenting with large datasets. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit, which can provide more reliable results for classification tasks. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross- validation, we need to train 36 * 5 = 180 models. The following visualization (Figure 5-6) illustrates how the best parameter setting is selected in the preceding The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. For each parameter setting (only a subset is shown), five accuracy values are compu‐                ted, one for each split in the cross-validation. Then the mean validation accuracy iscomputed for each parameters setting. The parameters with the highest mean valida‐                tion accuracy are chosen, marked by the circle. For this reason,many people use the term cross- validateation colloquially to refer to grid search with cross- validation. The results of this study are published in the open-source version of the book  grid search with cross-validation is such a commonly used method to adjustparameters. scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. To use the Grid search CV class, you first need to specify theparameters GridSearchCV will then per‐score = clf.score(X[inner_test], y[ inner_test]) cv_scores.append(score) # compute mean score over inner folds over the iris dataset. mean_score = np.mean(cv_score) if mean_ score > best_score: # if better than so far, remember parameters. best_ score = mean Running a grid search over many parameters and on large datasets can be com‐putationally challenging, but it is also embarrassingly parallel. This makes grid search and cross-validation ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. You should be aware that scikit-learn does not allow nesting of parallel operations. If you are using the n-jobs option on your model (for example, a random forest),.    0.967  1.068   2.038   3.0",
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-1-subsection-2",
                            "title": "Benefits of Cross-Validation",
                            "content": "when applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\ndiscuss methods to evaluate classification and regression performance that go beyond\nthe default measures of accuracy and R2 provided by the score method.\nWe will also discuss grid search, an effective method for adjusting the parameters in\nsupervised models for the best generalization performance.\nCross-Validation\nCross-validation is a statistical method of evaluating generalization performance that\nis more stable and thorough than using a split into a training and a test set. In cross-\nvalidation, the data is instead split repeatedly and multiple models are trained. The\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\nNext, a sequence of models is trained. The first model is trained using the first fold as\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\nFor each of these five splits of the data into training and test sets, we compute the\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement",
                            "summary": "The main disadvantage of cross-validation is increased computational cost. More data will usually result in more accurate models. It is important to keep in mind that cross- validation is not a way to build a model that can be applied to new data. When using train_test_split, we usually use 75% of the data for training and 25% for evaluation. In each iteration we can use four-fifths of theData (80%) to fit themodel. When we are training k models instead of a single model, cross- validateation will be roughly k times slower than doing a single split of the Data. It does not return a model. when applied tonew data. Stratified k-Fold Cross-Validation and Other Strategies. Splitting the dataset into k folds by starting with the first one-k-th part of the data might not always be a good idea. When calling cross_val_score, multiple models are built internally. The iris dataset is a set of labels. The labels are sorted by the class 0. The class 1 labels are the labels that are most likely to be used. The iris data is stored in a file called iris.datasets. We can use the iris file to test our models. We will show the results of our tests in the next section of this article. We hope that this will help you with your own models. The next part of the article will show how we can improve our models by adding more features. We are now ready for the next phase of the project. We’ll show you how we improve our model by adding new features and adding more labels. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐ tumultuous our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using five-fold split, it can make predictions for data that was not observed during training. Another benefit of cross-validation as compared to using a single split of theData is that we use our data more effectively when using train_test_split, we usually use 75% of the dataset for training and 25% for evaluation. Cross-validation is a statistical method of evaluating generalization performance. It is more stable and thorough than using a split into a training and a test set. We will also discuss grid search, an effective method for adjusting the parameters insupervised models for the best generalized performance. We'll also discuss methods to evaluate classification and regression performance that go beyond the default measures of accuracy and R2 provided by the score method. The most commonly used version of cross-validated is k-fold cross- validation, where k is a user-specified number, usually 5 or 10. The data is first partitioned into five parts of (approximately) equal size, called folds. The process is illustrated in Figure 5-1. The first model is trained using the first fold as the test set, and the remaining folds (2–5) are used as the training set. The model is built using the data in folds 2–5, and then the accuracy is evaluated on fold 1. In the end, we have collected five accuracy values. The process is repeated using folds 3, 4, and 5 as test sets. For each of these five splits of the data into training and test sets, we compute theiablyaccuracy. In[3] we show how the data is split in five-fold cross-validation. We also show how we test the accuracy of our models.",
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-1-subsection-3",
                            "title": "Stratified k-Fold Cross-Validation and Other Strategies",
                            "content": "fied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\n1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\nwould be only class 0, and the training set would be only classes 1 and 2. As the\nclasses in training and test sets would be different for all three splits, the three-fold\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\nwe can do much better than 0% accuracy on iris.\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\ntion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\nsplit the data such that the proportions between classes are the same in each fold as\nthey are in the whole dataset, as illustrated in Figure 5-2:\nIn[8]:\nmglearn.plots.plot_stratified_cross_validation()\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\nwhen the data is ordered by class label\nCross-Validation \n| \n255\nFor example, if 90% of your samples belong to class A and 10% of your samples\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\nsamples belong to class A and 10% of samples belong to class B.\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\ngeneralization performance. In the case of only 10% of samples belonging to class B,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\n258 \n| \nChapter 5: Model Evaluation and Improvement\nCross-validation with groups\nAnother very common setting for cross-validation is when there are groups in the\ndata that are highly related. Say you want to build a system to recognize emotions\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\nperson is captured multiple times, showing various emotions. The goal is to build a\nclassifier that can correctly identify emotions of people not in the dataset. You could\nuse the default stratified cross-validation to measure the performance of a classifier\nhere. However, it is likely that pictures of the same person will be in both the training\nand the test set. It will be much easier for a classifier to detect emotions in a face that\nis part of the training set, compared to a completely new face. To accurately evaluate\nthe generalization to new faces, we must therefore ensure that the training and test\nsets contain images of different people.\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\nthat we can use to indicate which person is in the image. The groups array here indi‐\ncates groups in the data that should not be split when creating the training and test\nsets, and should not be confused with the class label.\nThis example of groups in the data is common in medical applications, where you\nmight have multiple samples from the same patient, but are interested in generalizing\nto new patients. Similarly, in speech recognition, you might have multiple recordings\nof the same speaker in your dataset, but are interested in recognizing speech of new\nspeakers.",
                            "summary": "KFold is used for cross-validation on the iris dataset. It can't be used to stratify the data, so we need to fix the random_state. We can do that by setting the shuffle parameter of KFold to True. It's possible to get a reproducible shuffling of the data by changing the random state. This can be done using the following code:kfold = KF Fold(n_splits=3) cross_val_score(logreg, iris.data, iri.target, cv=kfold) kfold.fied (kfold. fied Another frequently used cross-validation method is leave-one-out. Shuffling the data before splitting it yields a much better result. For each split, you pick a single data point to be the test set. You can think of this as k-fold cross- validation where each fold is a single sample. Cross-validation can be very time consuming, particularly for large datasets. When using a single split of the data, we usually use 75% of our data for training and 25% of it for evaluation. With five-foldCross-Validation, in each iteration we can use four-fifths of theData (80%) to fit the model. With 10-fold cross-validated, we can using nine-tenths of the Data (90%) tofit the model, and an accuracy of 0.95% for each iteration. We can use our data more effectively by splitting it into smaller pieces. Cross-validation is not a way to build a model that can be applied to new data. More data will usually result in more accurate models. When calling cross_val_score, multiple models are built internally, but the purpose is only to evaluate how well a given algorithm will generalize on a specific dataset. Splitting the dataset into k folds by starting with the first one-k-th part of the data, might not always be a good idea. The main disadvantage of cross-validated data is increased computational cost, as we are training k models instead of a single model. It is roughly ktimes slower than doing a single split of theData. For regression, scikit-learn uses the standard k-fold cross-validation by default. For example, let’s have a look at the iris dataset: 254. The first third of the data is the class 0, the second third is theclass 1. Using this fold as a test set would not be very informative about the overall performance of the classifier. For more information on how to use sklearn, please visit: http://www.sklearn.org/en/sklearn-demo/learn-model-evaluation-and-improvement.html. For the full version of this article, please go to: http: www.klearn.com/demo Scikit-learn allows for more control over what happens during the splitting of the data. For most use cases, the defaults of k-fold cross-validation for regression and stratified k- fold for classification work well. But there are some cases where you might want to use a different strategy. For example, if you want to repro-duce someone else’s results, you might need to change the way the data is broken up. For more information, visit scikitlearn.org or follow us on Twitter @scikitllearn and @researchers_reparations. Back to the page you came from. We can test that cross-validation on the iris dataset is bad. To do this, we first have to import the KFold splitter class from the model_selection module. Then, we can pass the kfold splitter object as the cv parameter to cross_val_score. We can also conclude that there is a relatively high variance in the accuracy between folds, ranging from 100% accuracy to 90% accuracy. And we can verify that it is indeed a really bad idea to use three-fold (nonstrati­fied) cross- validation on theiris dataset. There are several benefits to using cross-validation instead of a single split into a training and a test set. Imagine that we are “lucky’ when randomly splitting the data, and all examples that are hard to classify end up in the training set. In that case, the test set will only contain “easy” examples, and our test set accuracy will be unrealistically high. However, when usingCross-Validation, each example will be in theTraining set exactly once. Each example is in one of the folds, and each fold is the testSet once. The model is very dependent on the particular folds used for training, but it could also just be a consequence of the small size of the Having multiple splits of the data also provides some information about how sensi‐tive our model is to the selection of the training dataset. For the iris dataset, we sawaccuracies between 90% and 100%. This is quite a range, and it provides us with an idea about how the model might perform in the worst case and best case scenarios when applied to new data. When using train_test_split, we usually use 75% of theData for training and 25% ofThe data for evaluation. The model needs to generalize well to all of the samples in the dataset for all of its cross-validation scores to be The first third of the data is the class 0, the second third is theclass 1, and the last third is class 2. Imagine doing three-fold cross-validation on this dataset. The first fold would be only class 0. As the classes in training and test sets would be different for all three splits, the three- fold cross- validation accuracy would be zero. That is not very helpful, as we can do much better than 0% accuracy on iris. The simple k-fold strategy fails here, so scikit-learn does not use it for classifica‐phthaltion, but rather uses stratified k- Fold Cross-Validation (KFC) In stratified cross-validation, we split the data such that the proportions between classes are the same in each fold as they are in the whole dataset. For example, if 90% of samples belong to class A and 10% of your samples                belong to class B, then stratifiedCross-Validation ensures that in each Fold 90% are of class A. It is usually a good idea to use stratified k-fold cross- validation to evaluate a classifier, because it results in more reliable estimates of generalization performance. In the case of only 10 per cent of samples belonging toclass B, it might easily happen that one fold only con‐ purposefullytains samples ofclass A. There is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS                plit. Subsampling the data in this way can beuseful for experimenting with large datasets. The default stratified cross-validation can be used to measure the performance of a classifier. The classifier can also be used as a test set to see how well it understands a given set of data. For more information on how to use the classifier in your project, go to: http://www.crankstudio.com/classifiers/classifier.html#classifier-performance-evaluation-and-improvement. For information about how to test your classifier, visit:  It is likely that pictures of the same person will be in both the training and test set. To accurately evaluate the generalization to new faces, we must ensure that the training set contains images of different people. To achieve this, we can use GroupKFold, which takes an array of groups as argument. The groups array here indi‐phthalcates groups in the data that should not be split when creating the training. and testsets, and shouldn't be confused with the class label. It is common in medical applications, where you might have multiple samples from the same patient, but are interested in generalizing to new patients.    The group array can be used to indicate which person is in the image. In speech recognition, you might have multiple recordingsof the same speaker in your dataset, but are interested in recognizing",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-5-section-2",
                    "title": "Grid Search",
                    "content": "Grid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\ning more than one model on the test set and choosing the better of the two will result\nin an overly optimistic estimate of how accurate the model is.\nGrid Search with Cross-Validation\nWhile the method of splitting the data into a training, a validation, and a test set that\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\nexactly the data is split. From the output of the previous code snippet we can see that\nGridSearchCV selects 'C': 10, 'gamma': 0.001 as the best parameters, while the\noutput of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\nbest parameters. For a better estimate of the generalization performance, instead of\nusing a single split into a training and a validation set, we can use cross-validation to\nevaluate the performance of each parameter combination. This method can be coded\nup as follows:\nGrid Search \n| \n263\nIn[21]:\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters,\n        # train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        # perform cross-validation\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),\neter is searching over interesting values but the C parameter is not—or it could mean\nthe C parameter is not important.\nThe third panel shows changes in both C and gamma. However, we can see that in the\nentire bottom left of the plot, nothing interesting is happening. We can probably\nexclude the very small values from future grid searches. The optimum parameter set‐\nting is at the top right. As the optimum is in the border of the plot, we can expect that\nthere might be even better values beyond this border, and we might want to change\nour search range to include more parameters in this region.\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\ngood way to explore the importance of different parameters. However, you should\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\n270 \n| \nChapter 5: Model Evaluation and Improvement\nuation of the test set should happen only once we know exactly what model we want\nto use.\nSearch over spaces that are not grids\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\ndepending on which kernel is chosen, other parameters will be relevant. If ker\nnel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\nboth the C and gamma parameters are used (but not other parameters like degree). In\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\nmake sense: if kernel='linear', gamma is not used, and trying different values for\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\niris.data, iris.target, random_state=0)\nThe grid_search object that we created behaves just like a classifier; we can call the\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\nrun cross-validation for each combination of parameters we specified in param_grid:\nIn[27]:\ngrid_search.fit(X_train, y_train)\nFitting the GridSearchCV object not only searches for the best parameters, but also\nautomatically fits a new model on the whole training dataset with the parameters that\nyielded the best cross-validation performance. What happens in fit is therefore\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\nGridSearchCV class provides a very convenient interface to access the retrained\nmodel using the predict and score methods. To evaluate how well the best found\nparameters generalize, we can call score on the test set:\nIn[28]:\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\nOut[28]:\nTest set score: 0.97\nChoosing the parameters using cross-validation, we actually found a model that ach‐\nieves 97% accuracy on the test set. The important thing here is that we did not use the\ntest set to choose the parameters. The parameters that were found are scored in the\n266 \n| \nChapter 5: Model Evaluation and Improvement\nbest_params_ attribute, and the best cross-validation accuracy (the mean accuracy\nover the different splits for this parameter setting) is stored in best_score_:\nIn[29]:\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[29]:\nBest parameters: {'C': 100, 'gamma': 0.01}\nBest cross-validation score: 0.97\nAgain, be careful not to confuse best_score_ with the generaliza‐\ntion performance of the model as computed by the score method\non the test set. Using the score method (or evaluating the output of\nthe predict method) employs a model trained on the whole train‐\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nGrid Search \n| \n267\ncontains a lot of details, as you can see in the following output, and is best looked at\nafter converting it to a pandas DataFrame:\nIn[31]:\nimport pandas as pd\n# convert to DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n# show the first 5 rows\ndisplay(results.head())\nOut[31]:\n    param_C   param_gamma   params                        mean_test_score\n0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\n1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\n2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\n3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\n4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\n0               22              0.375           0.347           0.363\n1               22              0.375           0.347           0.363\n2               22              0.375           0.347           0.363\n3               22              0.375           0.347           0.363\n4               22              0.375           0.347           0.363\n       split3_test_score  split4_test_score  std_test_score\n0           0.363              0.380           0.011\n1           0.363              0.380           0.011\n2           0.363              0.380           0.011\n3           0.363              0.380           0.011\n4           0.363              0.380           0.011\nEach row in results corresponds to one particular parameter setting. For each set‐\nting, the results of all cross-validation splits are recorded, as well as the mean and\nstandard deviation over all splits. As we were searching a two-dimensional grid of\nparameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\nspond to C and gamma:\nIn[32]:\nscores = np.array(results.mean_test_score).reshape(6, 6)",
                    "summary": "6. Algorithm Chains and Pipelines. Grid Search with Cross-Validation. Using Evaluation Metrics in Model Selection. Keeping the End Goal in Mind. Scoring for Binary Classification and Multiclass Classification. Using Metrics for Multimodal Classification and Multi-classification. Using the Metrics and Scoring tool for Binary and Multipliericlass Classes. Building Pipelines       The General Pipeline Interface is a set of steps and parameters that are used to select a pipeline for construction. The Pipeline is a series of steps, steps, and parameters used to build a pipeline. The method of splitting the data into a training, a validation, and a test set is workable, and relatively commonly used. It is quite sensitive to how exactly the data is split. Strictly speaking, evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is. It's good practice to do all exploratory analysis and model selection using the combination of a training and a validation set. The test set should be reserved for a final evaluation, even for exploratory visualization. The output of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the best parameters. Cross-validation can be used to evaluate the performance of each parameter combination. Instead of a single split into a training and a validation set, we The method can be coded up as follows:. Search                  |    263                 In[21]:. For gamma in [0.001, 0.1, 1, 10, 100]:. Test the accuracy of the SVM using a particular setting of C and gamma using. score = clf.score(X[inner_test], y[inner-test]):. If score > best_score:. best_ score = score;. If mean_score is higher than best-score, it means the model is more accurate. For more information, see the sklearn.model_selection page. The code for the search function can be found here:. Grid search and cross-validation are ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. This means that building a. model using a particular parameter setting on a particular cross.validation split can be done completely independently from the other. parameter settings and models.     0.967  0. 967   1.9 The third panel shows changes in both C and gamma. You can set n_jobs=-1 to use all available cores. You should be aware that scikit-learn does not allow nesting of parallel operations. If you are using the n_ jobs option on your model (for example, a random forest), you might be searching over interesting values but the C parameter is not. This could mean that the C parameters are not important. The optimum parameter set‐ting is at the top right. As the optimum is in the border of the plot, we can expect that there might be even better values beyond this border, and we might want to changeour search range to include more parameters in this region. In some cases, trying all possible combinations of all parameters is not a good idea. For example, SVC has a kernel parameter, and depending on which kernel is chosen, other parameters will be relevant. To deal with these kinds of “conditional” parameters, GridSearchCV allows the param_grid to be a list of dictionaries. However, you should not test different parameter ranges on the final test set—as we discussed earlier, eval‐270270 is not the same as eval-270. For more information on how to use GridSearch CV, visit the Grid searchCV website. The latest version of this article can be downloaded for free from the Google Play store. The grid_search object behaves just like a classifier. We can call the methods fit, predict, and score on it. When we call fit, it will run cross-validation for each combination of parameters we specified in param_grid. The GridSearchCV class provides a very convenient interface to access the retrained model using the predict and score methods. It also searches for the best parameters, but alsoautomatically fits a new model on the whole training dataset with the parameters that yield the best cross- validation performance. It is therefore roughly equivalent to the result of the In[21] code we saw at the beginning of this section. It can be used to test the accuracy of the model. Choosing the parameters using cross-validation, we actually found a model that ach‐ encompasses 97% accuracy on the test set. The important thing here is that we did not use the Test set to choose the parameters. To evaluate how well the best foundparameters generalize, we can call The parameters that were found are scored in the parameters attribute. The best cross-validation accuracy (the mean accuracy) is stored in best_score_:                . The parameters are stored in the best_params_ attribute, and the best score is stored as best_ score_:                            Best parameters: {'C': 100, 'gamma': 0.01, Using the score method (or evaluating the output of the predict method) employs a model trained on the whole train. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine, the main downside of the use of cross- validation is the time it takes to train all these models. The following visualization (Figure 5-6) illustrates how the best parameter setting isselected in the preceding code. For each parameter setting (only a subset is shown), five accuracy values are compu‐ purposefullyted, one for each split in the cross- validate. Then the mean validation accuracy iscomputed for The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a GridSearchCV can be used to find better parameters for SVC. To use the GridSearchCV class, you first need to specify theparameters you want to search over using a dictionary. GridSearch CV will then learn an SVM on the scaled training data and score the test data. For example, the test score is 0.95. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, For each split in the cross-validation process, some part of the original training set will be declared the training part. The test part is used to measure what new data willlook like to a model trained on the trainingpart. However, we already used the infor‐mation contained in the test part when scaling the data The data is best looked at after converting it to a pandas DataFrame. Remember that the test part in each split in the cross-validation is part of the training set. We used the information from the entire training set to find the right scaling of the data. The output of the search function is shown in the following output. Each row in particular corresponds to one particular setting in the data set. For example, the first 5 rows show the first five rows of the first set of results in the search. For the second and third sets of results, each row corresponds to a particular row in the second set. The third row shows the first row of the second group of results. For each set, the results of all cross-validation splits are recorded, as well as the mean and standard deviation over all splits. As we were searching a two-dimensional grid ofparameters (C and gamma), this is best visualized as a heat map. First we extract the mean validation scores, then we reshape the scores",
                    "children": [
                        {
                            "id": "chapter-5-section-2-subsection-1",
                            "title": "Simple Grid Search",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-2-subsection-2",
                            "title": "The Danger of Overfitting the Parameters and the Validation Set",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-2-subsection-3",
                            "title": "Grid Search with Cross-Validation",
                            "content": "Grid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\ning more than one model on the test set and choosing the better of the two will result\nin an overly optimistic estimate of how accurate the model is.\nGrid Search with Cross-Validation\nWhile the method of splitting the data into a training, a validation, and a test set that\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\nexactly the data is split. From the output of the previous code snippet we can see that\nGridSearchCV selects 'C': 10, 'gamma': 0.001 as the best parameters, while the\noutput of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\nbest parameters. For a better estimate of the generalization performance, instead of\nusing a single split into a training and a validation set, we can use cross-validation to\nevaluate the performance of each parameter combination. This method can be coded\nup as follows:\nGrid Search \n| \n263\nIn[21]:\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters,\n        # train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        # perform cross-validation\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nscore = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),\neter is searching over interesting values but the C parameter is not—or it could mean\nthe C parameter is not important.\nThe third panel shows changes in both C and gamma. However, we can see that in the\nentire bottom left of the plot, nothing interesting is happening. We can probably\nexclude the very small values from future grid searches. The optimum parameter set‐\nting is at the top right. As the optimum is in the border of the plot, we can expect that\nthere might be even better values beyond this border, and we might want to change\nour search range to include more parameters in this region.\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\ngood way to explore the importance of different parameters. However, you should\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\n270 \n| \nChapter 5: Model Evaluation and Improvement\nuation of the test set should happen only once we know exactly what model we want\nto use.\nSearch over spaces that are not grids\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\ndepending on which kernel is chosen, other parameters will be relevant. If ker\nnel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\nboth the C and gamma parameters are used (but not other parameters like degree). In\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\nmake sense: if kernel='linear', gamma is not used, and trying different values for\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\niris.data, iris.target, random_state=0)\nThe grid_search object that we created behaves just like a classifier; we can call the\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\nrun cross-validation for each combination of parameters we specified in param_grid:\nIn[27]:\ngrid_search.fit(X_train, y_train)\nFitting the GridSearchCV object not only searches for the best parameters, but also\nautomatically fits a new model on the whole training dataset with the parameters that\nyielded the best cross-validation performance. What happens in fit is therefore\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\nGridSearchCV class provides a very convenient interface to access the retrained\nmodel using the predict and score methods. To evaluate how well the best found\nparameters generalize, we can call score on the test set:\nIn[28]:\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\nOut[28]:\nTest set score: 0.97\nChoosing the parameters using cross-validation, we actually found a model that ach‐\nieves 97% accuracy on the test set. The important thing here is that we did not use the\ntest set to choose the parameters. The parameters that were found are scored in the\n266 \n| \nChapter 5: Model Evaluation and Improvement\nbest_params_ attribute, and the best cross-validation accuracy (the mean accuracy\nover the different splits for this parameter setting) is stored in best_score_:\nIn[29]:\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[29]:\nBest parameters: {'C': 100, 'gamma': 0.01}\nBest cross-validation score: 0.97\nAgain, be careful not to confuse best_score_ with the generaliza‐\ntion performance of the model as computed by the score method\non the test set. Using the score method (or evaluating the output of\nthe predict method) employs a model trained on the whole train‐\nbest_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|\nGrid Search \n| \n267\ncontains a lot of details, as you can see in the following output, and is best looked at\nafter converting it to a pandas DataFrame:\nIn[31]:\nimport pandas as pd\n# convert to DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n# show the first 5 rows\ndisplay(results.head())\nOut[31]:\n    param_C   param_gamma   params                        mean_test_score\n0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\n1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\n2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\n3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\n4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\n0               22              0.375           0.347           0.363\n1               22              0.375           0.347           0.363\n2               22              0.375           0.347           0.363\n3               22              0.375           0.347           0.363\n4               22              0.375           0.347           0.363\n       split3_test_score  split4_test_score  std_test_score\n0           0.363              0.380           0.011\n1           0.363              0.380           0.011\n2           0.363              0.380           0.011\n3           0.363              0.380           0.011\n4           0.363              0.380           0.011\nEach row in results corresponds to one particular parameter setting. For each set‐\nting, the results of all cross-validation splits are recorded, as well as the mean and\nstandard deviation over all splits. As we were searching a two-dimensional grid of\nparameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\nspond to C and gamma:\nIn[32]:\nscores = np.array(results.mean_test_score).reshape(6, 6)",
                            "summary": "6. Algorithm Chains and Pipelines. Grid Search with Cross-Validation. Using Evaluation Metrics in Model Selection. Keeping the End Goal in Mind. Scoring for Binary Classification and Multiclass Classification. Using Metrics for Multimodal Classification and Multi-classification. Using the Metrics and Scoring tool for Binary and Multipliericlass Classes. Building Pipelines       The General Pipeline Interface is a set of steps and parameters that are used to select a pipeline for construction. The Pipeline is a series of steps, steps, and parameters used to build a pipeline. The method of splitting the data into a training, a validation, and a test set is workable, and relatively commonly used. It is quite sensitive to how exactly the data is split. Strictly speaking, evaluating more than one model on the test set and choosing the better of the two will result in an overly optimistic estimate of how accurate the model is. It's good practice to do all exploratory analysis and model selection using the combination of a training and a validation set. The test set should be reserved for a final evaluation, even for exploratory visualization. The output of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the best parameters. Cross-validation can be used to evaluate the performance of each parameter combination. Instead of a single split into a training and a validation set, we The method can be coded up as follows:. Search                  |    263                 In[21]:. For gamma in [0.001, 0.1, 1, 10, 100]:. Test the accuracy of the SVM using a particular setting of C and gamma using. score = clf.score(X[inner_test], y[inner-test]):. If score > best_score:. best_ score = score;. If mean_score is higher than best-score, it means the model is more accurate. For more information, see the sklearn.model_selection page. The code for the search function can be found here:. Grid search and cross-validation are ideal candidates for parallelization over multiple CPU cores or over a cluster. You can make use of multiple cores in GridSearchCV and cross_val_score by setting the n_jobs parameter to the number of CPU cores you want to use. This means that building a. model using a particular parameter setting on a particular cross.validation split can be done completely independently from the other. parameter settings and models.     0.967  0. 967   1.9 The third panel shows changes in both C and gamma. You can set n_jobs=-1 to use all available cores. You should be aware that scikit-learn does not allow nesting of parallel operations. If you are using the n_ jobs option on your model (for example, a random forest), you might be searching over interesting values but the C parameter is not. This could mean that the C parameters are not important. The optimum parameter set‐ting is at the top right. As the optimum is in the border of the plot, we can expect that there might be even better values beyond this border, and we might want to changeour search range to include more parameters in this region. In some cases, trying all possible combinations of all parameters is not a good idea. For example, SVC has a kernel parameter, and depending on which kernel is chosen, other parameters will be relevant. To deal with these kinds of “conditional” parameters, GridSearchCV allows the param_grid to be a list of dictionaries. However, you should not test different parameter ranges on the final test set—as we discussed earlier, eval‐270270 is not the same as eval-270. For more information on how to use GridSearch CV, visit the Grid searchCV website. The latest version of this article can be downloaded for free from the Google Play store. The grid_search object behaves just like a classifier. We can call the methods fit, predict, and score on it. When we call fit, it will run cross-validation for each combination of parameters we specified in param_grid. The GridSearchCV class provides a very convenient interface to access the retrained model using the predict and score methods. It also searches for the best parameters, but alsoautomatically fits a new model on the whole training dataset with the parameters that yield the best cross- validation performance. It is therefore roughly equivalent to the result of the In[21] code we saw at the beginning of this section. It can be used to test the accuracy of the model. Choosing the parameters using cross-validation, we actually found a model that ach‐ encompasses 97% accuracy on the test set. The important thing here is that we did not use the Test set to choose the parameters. To evaluate how well the best foundparameters generalize, we can call The parameters that were found are scored in the parameters attribute. The best cross-validation accuracy (the mean accuracy) is stored in best_score_:                . The parameters are stored in the best_params_ attribute, and the best score is stored as best_ score_:                            Best parameters: {'C': 100, 'gamma': 0.01, Using the score method (or evaluating the output of the predict method) employs a model trained on the whole train. To evaluate the accuracy of the SVM using a particular setting of C and gamma using five-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine, the main downside of the use of cross- validation is the time it takes to train all these models. The following visualization (Figure 5-6) illustrates how the best parameter setting isselected in the preceding code. For each parameter setting (only a subset is shown), five accuracy values are compu‐ purposefullyted, one for each split in the cross- validate. Then the mean validation accuracy iscomputed for The overall process of splitting the data, running the grid search, and evaluating thefinal parameters is illustrated in Figure 5-7. Grid search with cross-validation is such a commonly used method to adjustparameters, scikit-learn provides the GridSearchCV class, which implements it in the form of an estimator. The parameters with the highest mean valida­tion accuracy are chosen, marked by the circle. For more information, visit the ScikitLearn website. The GridSearch CV class is available for free on the Google Play store and the GitHub repository. For confidential support, call the Samaritans on 08457 90 90 90, visit a GridSearchCV can be used to find better parameters for SVC. To use the GridSearchCV class, you first need to specify theparameters you want to search over using a dictionary. GridSearch CV will then learn an SVM on the scaled training data and score the test data. For example, the test score is 0.95. The grid search is based on scaled data. We used all the data in the training set to find out how to train it. We then use the scaled training data to run our grid search using cross-validation. How should we go about doing this? A naive approach might look like this:From sklearn.model_selection import GridSearchCV # for illustration purposes only, don't use this code!param_grid = {'C': [0.001, 0.1, 1, 10, 100], 'gamma': [ 0.001,. 0.01, 1,. 10,. 100] } grid.fit(X_train_scaled, For each split in the cross-validation process, some part of the original training set will be declared the training part. The test part is used to measure what new data willlook like to a model trained on the trainingpart. However, we already used the infor‐mation contained in the test part when scaling the data The data is best looked at after converting it to a pandas DataFrame. Remember that the test part in each split in the cross-validation is part of the training set. We used the information from the entire training set to find the right scaling of the data. The output of the search function is shown in the following output. Each row in particular corresponds to one particular setting in the data set. For example, the first 5 rows show the first five rows of the first set of results in the search. For the second and third sets of results, each row corresponds to a particular row in the second set. The third row shows the first row of the second group of results. For each set, the results of all cross-validation splits are recorded, as well as the mean and standard deviation over all splits. As we were searching a two-dimensional grid ofparameters (C and gamma), this is best visualized as a heat map. First we extract the mean validation scores, then we reshape the scores",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-5-section-3",
                    "title": "Evaluation Metrics and Scoring",
                    "content": "compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317 compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\n8       0.93      0.90      0.91        48\n          9       0.96      0.94      0.95        47\navg / total       0.95      0.95      0.95       450\n298 \n| \nChapter 5: Model Evaluation and Improvement\nUnsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\nsions with this class. For class 7, on the other hand, precision is 1 because no other\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\nand 3.\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\npute one binary f-score per class, with that class being the positive class and the other\nclasses making up the negative classes. Then, these per-class f-scores are averaged\nusing one of the following strategies:\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\nweight to all classes, no matter what their size is.\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\ntheir support. This is what is reported in the classification report.\n• \"micro\" averaging computes the total number of false positives, false negatives,\nand true positives over all classes, and then computes precision, recall, and f-\nscore using these counts.\nIf you care about each sample equally much, it is recommended to use the \"micro\"\naverage f1-score; if you care about each class equally much, it is recommended to use\nthe \"macro\" average f1-score:\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317 In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\") In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\ninformation on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\naccurate predictions, but in using these predictions as part of a larger decision-\nmaking process. Before picking a machine learning metric, you should think about\nthe high-level goal of the application, often called the business metric. The conse‐\nquences of choosing a particular algorithm for a machine learning application are\nEvaluation Metrics and Scoring \n| \n275\n2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\nimpact” being used in that realm.\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\ndecreasing the number of hospital admissions. It could also be getting more users for\nyour website, or having users spend more money in your shop. When choosing a\nmodel or adjusting parameters, you should pick the model or parameter values that\nhave the most positive influence on the business metric. Often this is hard, as assess‐\ning the business impact of a particular model might require putting it in production\nin a real-life system.\nIn the early stages of development, and for adjusting parameters, it is often infeasible\nto put models into production just for testing purposes, because of the high business\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\ncapabilities of a self-driving car by just letting it drive around, without verifying it\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nfunction used for model selection and model evaluation. The theory of how to make\nbusiness decisions from the predictions of a machine learning model is somewhat\nbeyond the scope of this book.7 However, it is rarely the case that the end goal of a\nmachine learning task is building a model with a high accuracy. Make sure that the\nmetric you choose to evaluate and select a model for is a good stand-in for what the\nmodel will actually be used for. In reality, classification problems rarely have balanced\nclasses, and often false positives and false negatives have very different consequences.\n302 \n| \nChapter 5: Model Evaluation and Improvement\nMake sure you understand what these consequences are, and pick an evaluation met‐\nric accordingly.\nThe model evaluation and selection techniques we have described so far are the most\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\ndescribed them in this chapter can only be applied to a single supervised model. We\nhave seen before, however, that many models require preprocessing, and that in some\napplications, like the face recognition example in Chapter 3, extracting a different\nrepresentation of the data can be useful. In the next chapter, we will introduce the\nPipeline class, which allows us to use grid search and cross-validation on these com‐\nplex chains of algorithms.\nSummary and Outlook \n| \n303\nCHAPTER 6\nAlgorithm Chains and Pipelines\nFor many machine learning algorithms, the particular representation of the data that\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\nthe data and combining features by hand and goes all the way to learning features\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
                    "summary": "In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation andselection. It could be that you have 10% more customers, but each cus‐                tomer will spend 15% less For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier, which makes random predictions. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. This makes it very                hard to judge which of these results is actually helpful. For any concerns about the accuracy of the classifier, please contact us at [email protected] or @sklearn. Accu‐ Disorders is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐ Disorders that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. We will use confusion matrices to represent the result of evaluating binary clas‐ Disorders in the next section of the book. The predictions of LogisticRegresuo’s We already stored the predictions on the test set in pred_logreg: evaluations and scoring. We import confusion_matrix from sklearn.metrics. We use clustering and clustering clustering models. We also use a bag-of-words representation for movie reviews and a business metric for SVC. We show how to use these models in a number of different ways. We will also show how we can use them in other ways, such as for cancer diagnoses. We hope this will be a useful tool to help people understand how to make better use of machine learning in the real world. We'll also discuss how to apply these techniques to other areas of computer science. There are 25 examples of, 26goals for, 25                iris classification example, 14                k 6. regression problems, 26. classifiers, 75. decision trees, 75, 80. regression trees and 75. regression regression regressors. 7. clustering algorithms, 168. algorithms for clustering, 182-187. 8. validation methods for, 131. grid search with Cross-Validation. 9. validation techniques for,131. validation of the clustering algorithm. 10. validation procedures for, 130. validation for the grid search algorithm. Pipelines. Algorithm Chains and Pipelines. The General Pipeline Interface. Using Pipelines in Grid Searches. Preprocessing Steps and Model parameters. Pipeline Creation with make_pipeline. Using Pipeline Attributes in a Grid-Searched Pipeline. Using pipeline attributes in a grid-searched pipeline. Using the Pipeline Interface to create a Pipeline. Use the Pipeline interface to make a Pipeline creation. Use pipeline attributes to create Pipelines Binary classification is arguably the most common and conceptually simple applica‐tion of machine learning in practice. However, there are still a number of caveats in evaluating even this simple task. In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. We will also look at the impact of choosing one model over another and how to use these metrics in the real world. We hope this article has helped you better understand machine learning and its potential applications to your business. For more information on machine learning, see the Machine Learning Handbook. The book is published by Oxford University Press and is available on Amazon Kindle and the Mac. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. Unsurprisingly, precision and recall are a perfect 1 for class 0. For class 7, on the other hand, precision is 1 because no otherclass was mistakenly classified as 7. We can also see that the model has particular difficulties with classes 8 and 3. The most commonly used metric for imbalanced datasets is the multiclass version of the f-score. The idea behind this metric is to com‐                pute one binary f- Score per class, with that class being the positive class and the other Classes making up the negative classes. Then, these per-class f-scores are averaged using one of the following strategies: \"macro\" averaging computes the unweighted per- class f- scores. \"Weighted\" averaging computes the mean of the per-class f-scores. This gives equalweight to \"Micro\" averaging computes the total number of false positives, false negatives, and true positives over all classes. \"Macro\" average f1-score computes precision, recall, and f-score using these counts. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. The default classifier makes random predictions but produces classes with the same proportions as in the training set. It produces very good results, with a score of 0.98. Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Evaluation Metrics and Scoring are used to test the predictions on the test set in pred_logreg. The predictions are based on the tests set in the pred_ logreg. We already have the predictions for the test in place. We can use these predictions to help students understand how to use the Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. Pipelines can be used to select parameters for a model or to build a pipeline. In most applications we’ve seen, using the default R2 used in the score grotesquemethod of all regressors is enough. We’ll use the R2 as the default method in the rest of the article. We hope this article has helped you with your understanding of how to use pipelines in your project. We are happy to help you with any questions you may have about the project. Please send us your questions in the comments below or email them to jennifer R2 is a moreintuitive metric to evaluate regression models. Scoring can be used in both GridSearchCV and cross_val_score. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUCscore. The scoring argument is a very simple way to achieve this, via the scoring argument that can be found in GridSearch CV or cross_Val_ score. It can also be used to select a model using the gridSearchCV or cross-Val_score argument. For more information on scikit-learn, visit The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐consuming \"roc_auc\" as the scoring parameter. In[66]: grotesqueprint(\"Micro average f1 score: {:.3f}\".format) In[67]: grotesque print(\"Macro average f2 score: 0.953\") In[68]: grotesque Print(f1_score(y_test, pred, average=\"micro\"), average=\"macro\"), false (false, false, false), true (false), false) R2 is a moreintuitive metric to evaluate regression models. Scoring can be used in both GridSearchCV and cross_val_score. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUCscore. The scoring argument is a very simple way to achieve this, via the scoring argument that can be found in GridSearch CV or cross_Val_ score. It can also be used to select a model using the gridSearchCV or cross-Val_score argument. For more information on scikit-learn, visit The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐                ing \"roc_auc\" as the scoring parameter. We will focus on the supervised methods, regression and classification, as evaluating models in unsupervised learning is often a very qualitative process. We have split our dataset into a training set and a test set using the train_test_split function. We built a model on the training set by calling the fit method, and evaluated it on the test set by the score method, which for classification computes the fraction of correctly classified samples. We are now ready to move on to the next chapter. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. We will first intro‐                duce cross-validation, a more robust way to assess generalization performance. We will then look at how we can use the metrics to assess performance in a variety of different ways. We'll also look at the use of Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. In this chapter we discussed cross-validation, grid search, evaluation metrics, and improving machine learning algorithms. We also discussed building Pipelines and the General Pipeline Interface. In the next chapter, we will look at the use of pipelines in the context of machine learning. We will also look at how machine learning can be used to improve the performance of a machine learning algorithm. The next chapter will be published on November 14, 2014. We hope to see you in the next few chapters. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot(according to accuracy), while Logisticregression produces very good results. Even the random classifier yields over 80% accuracy. The most commonly used form (as described ear­lier) is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring. For example, in the example above, the score is 0.98 Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the end goal is equally important in science, though the authors are not aware of a similar phrase to “business impact” being used in that realm. The conse‐quences of choosing a particular algorithm for a machinelearning application are discussed in the section “Machine Learning Algorithms”. The article was originally published in the online magazine “The Next Web’s” January 2013 issue, which also featured the “Artificial Intelligence and Machine Learning When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production. It could also be getting more users for your website, or having users spend more money in your shop. It is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidancecapabilities of a self-driving car by just letting it drive around, without verifying it first. If your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using The theory of how to make business decisions from the predictions of a machine learning model is somewhat beyond the scope of this book. Make sure that the metrics you choose to evaluate and select a model for is a good stand-in for what the model will actually be used for. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. The model evaluation and selection techniques we have described so far are the mostimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’vedescribed them in this chapter can only be applied to a single supervised model. Many models require preprocessing, and in some cases extracting a differentrepresentation of the data can be useful. In reality, classification problems rarely have balanced classes, and often false positives and false negatives have very different consequences. Make sure you understand what these consequences are, and pick an evaluation met‐                 The particular representation of the data that you provide is very important. This starts with scaling the data and combining features by hand and goes all the way to learning features for compute. In the next chapter, we will introduce the horriblyPipeline class, which allows us to use grid search and cross-validation on these com‐plex chains of algorithms. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any. application, we would call a positive test (an indi‐cation of cancer) the positiveclass, and anegative test the negative class. If you have any questions about our model, please email us at jennifer.",
                    "children": [
                        {
                            "id": "chapter-5-section-3-subsection-1",
                            "title": "Keep the End Goal in Mind",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-3-subsection-2",
                            "title": "Metrics for Binary Classification",
                            "content": "compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317",
                            "summary": "In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation andselection. It could be that you have 10% more customers, but each cus‐                tomer will spend 15% less For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier, which makes random predictions. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. This makes it very                hard to judge which of these results is actually helpful. For any concerns about the accuracy of the classifier, please contact us at [email protected] or @sklearn. Accu‐ Disorders is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐ Disorders that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. We will use confusion matrices to represent the result of evaluating binary clas‐ Disorders in the next section of the book. The predictions of LogisticRegresuo’s We already stored the predictions on the test set in pred_logreg: evaluations and scoring. We import confusion_matrix from sklearn.metrics. We use clustering and clustering clustering models. We also use a bag-of-words representation for movie reviews and a business metric for SVC. We show how to use these models in a number of different ways. We will also show how we can use them in other ways, such as for cancer diagnoses. We hope this will be a useful tool to help people understand how to make better use of machine learning in the real world. We'll also discuss how to apply these techniques to other areas of computer science. There are 25 examples of, 26goals for, 25                iris classification example, 14                k 6. regression problems, 26. classifiers, 75. decision trees, 75, 80. regression trees and 75. regression regression regressors. 7. clustering algorithms, 168. algorithms for clustering, 182-187. 8. validation methods for, 131. grid search with Cross-Validation. 9. validation techniques for,131. validation of the clustering algorithm. 10. validation procedures for, 130. validation for the grid search algorithm. Pipelines. Algorithm Chains and Pipelines. The General Pipeline Interface. Using Pipelines in Grid Searches. Preprocessing Steps and Model parameters. Pipeline Creation with make_pipeline. Using Pipeline Attributes in a Grid-Searched Pipeline. Pipeline Construction with Make_Pipeline and other Pipeline Creation tools. Pipeline Building with Pipelines and Other Pipeline Construction tools. Pipeline Building with Other Pipeline Building tools. Building Pipelines",
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-3-subsection-3",
                            "title": "Metrics for Multiclass Classification",
                            "content": "compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\n8       0.93      0.90      0.91        48\n          9       0.96      0.94      0.95        47\navg / total       0.95      0.95      0.95       450\n298 \n| \nChapter 5: Model Evaluation and Improvement\nUnsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\nsions with this class. For class 7, on the other hand, precision is 1 because no other\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\nand 3.\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\npute one binary f-score per class, with that class being the positive class and the other\nclasses making up the negative classes. Then, these per-class f-scores are averaged\nusing one of the following strategies:\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\nweight to all classes, no matter what their size is.\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\ntheir support. This is what is reported in the classification report.\n• \"micro\" averaging computes the total number of false positives, false negatives,\nand true positives over all classes, and then computes precision, recall, and f-\nscore using these counts.\nIf you care about each sample equally much, it is recommended to use the \"micro\"\naverage f1-score; if you care about each class equally much, it is recommended to use\nthe \"macro\" average f1-score:\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317",
                            "summary": "In this section, we will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. This closest metric should be used whenever possible for model evaluation andselection. It could be that you have 10% more customers, but each cus‐                tomer will spend 15% less For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, thepatient will undergo additional screening. Here, we would call a positive test (an indi‐cation of cancer) thepositive class, and anegative test the negative class. We can’t assume that our model will always work perfectly, and it will make mistakes. The number of mistakes we make does not contain all the information we are interested in. Unsurprisingly, precision and recall are a perfect 1 for class 0. For class 7, on the other hand, precision is 1 because no otherclass was mistakenly classified as 7. We can also see that the model has particular difficulties with classes 8 and 3. The most commonly used metric for imbalanced datasets is the multiclass version of the f-score. The idea behind this metric is to com‐                pute one binary f- Score per class, with that class being the positive class and the other Classes making up the negative classes. Then, these per-class f-scores are averaged using one of the following strategies: \"macro\" averaging computes the unweighted per- class f- scores. \"Weighted\" averaging computes the mean of the per-class f-scores. This gives equalweight to \"Micro\" averaging computes the total number of false positives, false negatives, and true positives over all classes. \"Macro\" average f1-score computes precision, recall, and f-score using these counts. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot according to accuracy. However, even the random classifier yields over 80% accuracy. The default classifier makes random predictions but produces classes with the same proportions as in the training set. It produces very good results, with a score of 0.98. Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Evaluation Metrics and Scoring are used to test the predictions on the test set in pred_logreg. The predictions are based on the tests set in the pred_ logreg. We already have the predictions for the test in place. We can use these predictions to help students understand how to use the Pipelines. Algorithm Chains and Pipelines. The General Pipeline Interface. Using Pipelines in Grid Searches. Preprocessing Steps and Model parameters. Pipeline Creation with make_pipeline. Using Pipeline Attributes in a Grid-Searched Pipeline. Pipeline Construction with Make_Pipeline and other Pipeline Creation tools. Pipeline Building with Pipelines and Other Pipeline Construction tools. Pipeline Building with Other Pipeline Building tools. Building Pipelines",
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-3-subsection-4",
                            "title": "Regression Metrics",
                            "content": "In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")",
                            "summary": "Evaluation for regression can be done in similar detail as we did for classification. The default R2 used in the scoremethod of all regressors is enough. Sometimes business decisions are made on the basis of mean squared error or mean absolute error, which might give incentive to tweak models using these metrics. For example, by analyzing overpredicting the target versus underp predicting the target, we can see if a model is being over- or under-predicted. Using Evaluation Metrics in Model Selection using GridSearchCV or cross_val_score. In general, though, we have found R2 to be a more                intuitive metric to evaluate regression models. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. The scoring argument that can be used in both GridSearch CV and cross_Val_score is called scikit-learn. The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid­ing \"roc_auc\" as the scoring parameter.",
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-3-subsection-5",
                            "title": "Using Evaluation Metrics in Model Selection",
                            "content": "In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\ninformation on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\naccurate predictions, but in using these predictions as part of a larger decision-\nmaking process. Before picking a machine learning metric, you should think about\nthe high-level goal of the application, often called the business metric. The conse‐\nquences of choosing a particular algorithm for a machine learning application are\nEvaluation Metrics and Scoring \n| \n275\n2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\nimpact” being used in that realm.\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\ndecreasing the number of hospital admissions. It could also be getting more users for\nyour website, or having users spend more money in your shop. When choosing a\nmodel or adjusting parameters, you should pick the model or parameter values that\nhave the most positive influence on the business metric. Often this is hard, as assess‐\ning the business impact of a particular model might require putting it in production\nin a real-life system.\nIn the early stages of development, and for adjusting parameters, it is often infeasible\nto put models into production just for testing purposes, because of the high business\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\ncapabilities of a self-driving car by just letting it drive around, without verifying it\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nfunction used for model selection and model evaluation. The theory of how to make\nbusiness decisions from the predictions of a machine learning model is somewhat\nbeyond the scope of this book.7 However, it is rarely the case that the end goal of a\nmachine learning task is building a model with a high accuracy. Make sure that the\nmetric you choose to evaluate and select a model for is a good stand-in for what the\nmodel will actually be used for. In reality, classification problems rarely have balanced\nclasses, and often false positives and false negatives have very different consequences.\n302 \n| \nChapter 5: Model Evaluation and Improvement\nMake sure you understand what these consequences are, and pick an evaluation met‐\nric accordingly.\nThe model evaluation and selection techniques we have described so far are the most\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\ndescribed them in this chapter can only be applied to a single supervised model. We\nhave seen before, however, that many models require preprocessing, and that in some\napplications, like the face recognition example in Chapter 3, extracting a different\nrepresentation of the data can be useful. In the next chapter, we will introduce the\nPipeline class, which allows us to use grid search and cross-validation on these com‐\nplex chains of algorithms.\nSummary and Outlook \n| \n303\nCHAPTER 6\nAlgorithm Chains and Pipelines\nFor many machine learning algorithms, the particular representation of the data that\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\nthe data and combining features by hand and goes all the way to learning features\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
                            "summary": "Evaluation for regression can be done in similar detail as we did for classification. The default R2 used in the scoremethod of all regressors is enough. Sometimes business decisions are made on the basis of mean squared error or mean absolute error, which might give incentive to tweak models using these metrics. For example, by analyzing overpredicting the target versus underp predicting the target, we can see if a model is being over- or under-predicted. Using Evaluation Metrics in Model Selection using GridSearchCV or cross_val_score. In general, though, we have found R2 to be a more                intuitive metric to evaluate regression models. You can simply provide a stringdescribing the evaluation metric you want to use. Say, for example, we want to evaluate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC score. The scoring argument that can be used in both GridSearch CV and cross_Val_score is called scikit-learn. The default scoring for classification is accuracy. Changing the score from the default (accuracy) to AUC can be done by provid‐                ing \"roc_auc\" as the scoring parameter. We will focus on the supervised methods, regression and classification, as evaluating models in unsupervised learning is often a very qualitative process. We have split our dataset into a training set and a test set using the train_test_split function. We built a model on the training set by calling the fit method, and evaluated it on the test set by the score method, which for classification computes the fraction of correctly classified samples. We are now ready to move on to the next chapter. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. We will first intro‐                duce cross-validation, a more robust way to assess generalization performance. We will then look at how we can use the metrics to assess performance in a variety of different ways. We'll also look at the use of Algorithm Chains and Pipelines is a series of articles on algorithm chains and pipelines. This article is the first of a two-part series. Read the second part of this article to learn more about algorithm chains. In this chapter we discussed cross-validation, grid search, evaluation metrics, and improving machine learning algorithms. We also discussed building Pipelines and the General Pipeline Interface. In the next chapter, we will look at the use of pipelines in the context of machine learning. We will also look at how machine learning can be used to improve the performance of a machine learning algorithm. The next chapter will be published on November 14, 2014. We hope to see you in the next few chapters. Cross-validation or the use of a test set allow us to evaluate a machine learning model as it will perform in the future. If we use the test set to select a model or select model parameters, we “use up” the test data. We therefore need to resort to a split into training data for model building and validation data formodel and parameter selection. Instead of a simple split, we can replace each of these splits with cross-validated data. The tools described in this chapter, together with the algorithms described in Chapters 2 and 3, are the bread and butter of every machine learning practitioner. For more information on how to use these tools, visit the Machine Learning Handbook. For comparison purposes, let’s evaluate two more classifiers, LogisticRegression and the default DummyClassifier. The dummy classifier that produces random output is clearly the worst of the lot(according to accuracy), while Logisticregression produces very good results. Even the random classifier yields over 80% accuracy. The most commonly used form (as described ear­lier) is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. The second point has to do with the importance of the evaluation metric or scoring. For example, in the example above, the score is 0.98 Accu‐paralleledracy is an inadequate measure for quantifying predictive performance. For the rest of this chapter, we will explore alternative metrics that provide better guidance in selecting models. In particular, we would like to have met‐paralleledrics that tell us how much better a model is than making “most frequent” predictions or random predictions. If we use a metric to assess our models, it should definitely be able to weed out these nonsense predictions. For example, we could use the confusion_matrix function to inspect the predictions of LogisticRegres                sion from the previous section using the confusion.Matrix function.    Before picking a machine learning metric, you should think about the high-level goal of the application, often called the business metric. We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the end goal is equally important in science, though the authors are not aware of a similar phrase to “business impact” being used in that realm. The conse‐quences of choosing a particular algorithm for a machinelearning application are discussed in the section “Machine Learning Algorithms”. The article was originally published in the online magazine “The Next Web’s” January 2013 issue, which also featured the “Artificial Intelligence and Machine Learning When choosing a model or adjusting parameters, you should pick the model or parameter values that have the most positive influence on the business metric. Often this is hard, as assessing the business impact of a particular model might require putting it in production. It could also be getting more users for your website, or having users spend more money in your shop. It is often infeasible to put models into production just for testing purposes, because of the high business or personal risks that can be involved. Imagine evaluating the pedestrian avoidancecapabilities of a self-driving car by just letting it drive around, without verifying it first. If your model is bad, pedestrians will be in trouble! Therefore we often need to find some surrogate evaluation procedure, using The theory of how to make business decisions from the predictions of a machine learning model is somewhat beyond the scope of this book. Make sure that the metrics you choose to evaluate and select a model for is a good stand-in for what the model will actually be used for. The most commonly used form is a training/test split for evaluation, and using cross-validation on the training set for model and parameter selection. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. The model evaluation and selection techniques we have described so far are the mostimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’vedescribed them in this chapter can only be applied to a single supervised model. Many models require preprocessing, and in some cases extracting a differentrepresentation of the data can be useful. In reality, classification problems rarely have balanced classes, and often false positives and false negatives have very different consequences. Make sure you understand what these consequences are, and pick an evaluation met‐                 The particular representation of the data that you provide is very important. This starts with scaling the data and combining features by hand and goes all the way to learning features for compute. In the next chapter, we will introduce the horriblyPipeline class, which allows us to use grid search and cross-validation on these com‐plex chains of algorithms. For example, we could test classifying images of pedestrians against non-                pedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it pays off to find the closest metric to the original business goal that is feasible to evaluate. Binary classification is arguably the most common and conceptually simple applica­tion of machine learning in practice. However, there are still a number of caveats inevaluating even this simple task. Before we dive into alternative metrics, let’s have a look at the ways in which measuring accuracy might be misleading. We will first discuss metrics for the important special case of binary classification, then turn to multiclass classification and finally regression. The end result of this evaluation might not be a single number, but it should capture the expected business impact of choosing one model over another. For binary classification, we often speak of a positive class and a negative class, with the understanding that the positive class is the one we are looking for. Imagine an application to screen for the early detection of cancer using an automated test. If the test is negative, the patient will be assumed healthy, while if it is positive, they will undergo additional screening. We can’tassume that our model will always work perfectly, and it will make mistakes. For any. application, we would call a positive test (an indi‐cation of cancer) the positiveclass, and anegative test the negative class. If you have any questions about our model, please email us at jennifer.",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-5-section-4",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-5-section-4-subsection-1",
                            "title": "Chapter Summary",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-4-subsection-2",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-5-section-4-subsection-3",
                            "title": "Next Steps",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-6",
            "title": "6. Algorithm Chains and Pipelines",
            "content": "them in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]: GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"List of grids:\\n{}\".format(param_grid))\nOut[34]:\nList of grids:\n[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nIn the first grid, the kernel parameter is always set to 'rbf' (not that the entry for\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\nlet’s apply this more complex parameter search:\nIn[35]:\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[35]:\nBest parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 0.01}\nBest cross-validation score: 0.97\nGrid Search \n| \n271\nLet’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\nvaried:\nIn[36]:\nresults = pd.DataFrame(grid_search.cv_results_)\n# we display the transposed table so that it better fits on the page:\ndisplay(results.T)\nOut[36]:\n0\n1\n2\n3\n… 38\n39\n40\n41\nparam_C\n0.001\n0.001\n0.001\n0.001\n… 0.1\n1\n10\n100\nparam_gamma\n0.001\n0.01\n0.1\n1\n… NaN\nNaN\nNaN\nNaN\nparam_kernel\nrbf\nrbf\nrbf\nrbf\n… linear\nlinear\nlinear\nlinear\nparams\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.001}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.01}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.1}\n{C: 0.001,\nkernel: rbf,\ngamma: 1}\n… {C: 0.1,\nkernel:\nlinear}\n{C: 1,\nkernel:\nlinear}\n{C: 10,\nkernel:\nlinear} Parameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nclass is in chaining preprocessing steps (like scaling of the data) together with a\nsupervised model like a classifier.\nBuilding Pipelines\nLet’s look at how we can use the Pipeline class to express the workflow for training\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\ncontaining a name (any string of your choosing1) and an instance of an estimator:\nIn[5]:\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\nany other scikit-learn estimator:\nIn[6]:\npipe.fit(X_train, y_train)\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\nthe test data, we simply call pipe.score:\nIn[7]:\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\nOut[7]:\nTest score: 0.95\nCalling the score method on the pipeline first transforms the test data using the\nscaler, and then calls the score method on the SVM using the scaled test data. As you\ncan see, the result is identical to the one we got from the code at the beginning of the\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\ncode needed for our “preprocessing + classification” process. The main benefit of\nusing the pipeline, however, is that we can now use this single estimator in\ncross_val_score or GridSearchCV.\n308 \n|\nIn[15]:\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # iterate over all but the final step\n        # fit and transform the data\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # fit the last step\n    self.steps[-1][1].fit(X_transformed, y)\n    return self\nWhen predicting using Pipeline, we similarly transform the data using all but the\nlast step, and then call predict on the last step:\nIn[16]:\ndef predict(self, X):\n    X_transformed = X\n    for step in self.steps[:-1]:\n        # iterate over all but the final step\n        # transform the data\n        X_transformed = step[1].transform(X_transformed)\n    # fit the last step\n    return self.steps[-1][1].predict(X_transformed)\n312 \n| \nChapter 6: Algorithm Chains and Pipelines\nThe process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a\nclassifier (called Classifier).\nFigure 6-3. Overview of the pipeline training and prediction process\nThe pipeline is actually even more general than this. There is no requirement for the\nlast step in a pipeline to have a predict function, and we could create a pipeline just\ncontaining, for example, a scaler and PCA. Then, because the last step (PCA) has a\ntransform method, we could call transform on the pipeline to get the output of\nPCA.transform applied to the data that was processed by the previous step. The last\nstep of a pipeline is only required to have a fit method.\nConvenient Pipeline Creation with make_pipeline\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\nand we often don’t need user-specified names for each step. There is a convenience\nfunction, make_pipeline, that will create a pipeline for us and automatically name\neach step based on its class. The syntax for make_pipeline is as follows:\nIn[17]:\nfrom sklearn.pipeline import make_pipeline\n# standard syntax\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# abbreviated syntax alpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\ntures are actually important, you should use L1. Otherwise, you should default to L2.\nL1 can also be useful if interpretability of the model is important. As L1 will use only\na few features, it is easier to explain which features are important to the model, and\nwhat the effects of these features are.\nLinear models are very fast to train, and also fast to predict. They scale to very large\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\nsands or millions of samples, you might want to investigate using the solver='sag'\noption in LogisticRegression and Ridge, which can be faster than the default on\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\nclass, which implement even more scalable versions of the linear models described\nhere.\nAnother strength of linear models is that they make it relatively easy to understand\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\nare. This is particularly true if your dataset has highly correlated features; in these\ncases, the coefficients might be hard to interpret.\nSupervised Machine Learning Algorithms \n| \n67\nLinear models often perform well when the number of features is large compared to\nthe number of samples. They are also often used on very large datasets, simply\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\nother models might yield better generalization performance. We will look at some\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics",
            "summary": "All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. A key property of these tests is that they are univari­ate, meaning that they only consider each feature individually. In the case of classification, this is also known as ANOVA (analyses of variance) and we'll look at. them in detail later in this article. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The simplest ones are SelectKBoutineest, which selects a fixed number k of features, and SelectPercentile, whichselects a fixed percentage of features. The default is f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. In[39]: GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the list is expanded into an independent grid. Let’s apply the feature selection for classification on the cancer dataset. The feature selection should be able to identify the features that are noninformative and remove them. In the first grid, the kernel parameter is always set to 'rbf' (not that the entry for                kernel is a list of length one) In the second grid, both the C and gamma parameters are varied. A possible grid search could look like this:. print(\"List of grids:\\n{}\".format(param_grid),. list('grid', {' kernel': ['linear'], 'C': [0.001, 0.1, 1, 10, 100]],. list ('grid', 'grid'): {'kernel': [' linear' Let’s look at the cv_results_ again. Now try a more complex parameter search. The result is the same, but the parameters are much more complex. Pipeline class has fit, predict, and score methods and behaves just like any other model in scikit-learn. With one exception: the name can’t contain a double underscore, __                tor. Parameter Selection with Preprocessing  ‘Pipelines’ is a class with the same name as the ‘pipeline’ class. The Pipeline class has the name ‘PIPELINE’ and the class is called ‘The Pipeline’. It has the following parameters: ‘Linear’, ‘Parallel,’ ‘Continuous’,. ‘Reverse,  , � Pipeline is a class that lets you chain preprocessing steps together with a supervised model like a classifier. Let's look at how we can use the Pipeline class to express the workflow for training an SVM after scaling the data with MinMaxScaler. First, we create two steps: the first, called \"scaler\", is an instance of MinMax Scaler, and the second, called ‘svm’ is an instances of SVC. Then, we build a pipeline object by providing it with a list of steps. Each step is a tuplecontaining a name (any string of The pipeline first transforms the test data using the scaler, and then calls the score method on the SVM using the scaled test data. The result is identical to the one we got from the code at the beginning of the chapter, when doing the transformations by hand. Using the pipeline, we can reduce the code needed for our “preprocessing + classification” process. The pipeline can be used to fit any other scikit-learn estimator, like the one in this article. The code for the pipeline is available on GitHub, and you can download it from the project page here. The main benefit of using the pipeline is that we can now use this single estimator in cross_val_score or GridSearchCV. The process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a classifier (called Classifier) For predicting using Pipeline, we similarly transform the data using all but the last step, and then call predict on the final step. The pipeline can also be used to transform data using a single transformer, such as T1 or T2. The algorithm chains and Pipelines section of this article is divided into two parts: Algorithm Chains and Algorithm Pipelines, and algorithm Pipelines. The pipeline is actually even more general than this. There is no requirement for the last step in a pipeline to have a predict function. We could create a pipeline just containing a scaler and PCA. The last step (PCA) has a transform method, so we could call transform on the pipeline to get the output of PCA applied to the data that was processed by the previous step. We often don’t need user-specified names for each step. The make_pipeline function creates a pipeline for us and automatically name each step based on its class. It can also be used to create pipelines for other types of data, such as text and images. Linear models are very fast to train, and also fast to predict. They scale to very largedatasets and work well with sparse data. tuning these parameters is quite important. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to use L1 regularization or L2 regularization. L1 can also be useful if interpretability of the model is important. As L1 will use only a few features, it is easier to explain which features are important to the model, and what the effects of these features are. The syntax for make_pipeline is as follows:In[17] # abbreviated syntax alpha or small values for Linear models often perform well when the number of features is large. If your data consists of hundreds of thou‐ Disorders or millions of samples, you might want to investigate using the solver='sag' option in LogisticRegression and Ridge. Other options are the SGDClassifier class and the SGDRegressorclass, which implement even more scalable versions of the linear models described here. Another strength of linear models is that they make it relatively easy to understand how a prediction is made. Unfortunately, it is often not entirely clear why coefficients are the way they are. This is particularly true if your dataset has highly correlated features. In these cases, the coefficients might be hard to interpret. other models might yield better generalization performance. They are also often used on very large datasets, simplybecause it’s not We will look at some of the different types of features that can be selected. We will also look at how these features can be combined to create a more holistic view of the world. We are going to look at a number of different ways to select features. We'll start with a look at the \"Automatic Feature Selection\" option. We will focus on the supervised methods, regression and classification, as well as evaluating and selecting models in supervised learning. We will split our dataset into supervised models and unsupervised models, so that we can evaluate supervised models. We have built a training set and a test set using a very qualitative process (as we saw in Chapter 3). We have also built a model on the train set using the test set, and evaluated it on the training set using classification for computes. We are now going to dive more deeply into evaluating models and selecting parameters. We hope this article will help you understand how to use machine learning to improve your knowledge of the world around you. Back to the page you came from. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. Cross-validation is a more robust way to assess generalization performance, and might be essential for linear models. Sometimes it is also a good idea to transform the target variable y in regression. Tree-based models, on the other hand, are often able to discover important interactions and don’t require transforming the data explicitly most of the time. This is particularlytrue for less complex models like linear models and naive Bayes models. We will first intro the univariate nonlinear transformations, then look at the tree-based nonlinear Transformations (U-transform) section. How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training",
            "children": [
                {
                    "id": "chapter-6-section-1",
                    "title": "Parameter Selection with Preprocessing",
                    "content": "them in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:",
                    "summary": "All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. A key property of these tests is that they are univari­ate, meaning that they only consider each feature individually. In the case of classification, this is also known as ANOVA (analyses of variance) and we'll look at. them in detail later in this article. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The simplest ones are SelectKBoutineest, which selects a fixed number k of features, and SelectPercentile, whichselects a fixed percentage of features. The default is f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. In[39]:. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. Let’s apply the featureselection for classification on the cancer dataset. In[39:. We’ll add",
                    "children": [
                        {
                            "id": "chapter-6-section-1-subsection-1",
                            "title": "Parameter Types",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-1-subsection-2",
                            "title": "Selection Methods",
                            "content": "them in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:",
                            "summary": "All of these methods are supervised methods, meaning they need the target for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training part of the data. A key property of these tests is that they are univari­ate, meaning that they only consider each feature individually. In the case of classification, this is also known as ANOVA (analyses of variance) and we'll look at. them in detail later in this article. Univariate tests are often very fast to compute, and don’t require building a model. They are completely independent of the model that you might want to apply after the feature selection. All methods for discarding parameters use a threshold to discard all features with too high a p-value (which means they are unlikely to be related to the target) The simplest ones are SelectKBoutineest, which selects a fixed number k of features, and SelectPercentile, whichselects a fixed percentage of features. The default is f_classif (the default) for classification or f_regression for regression, and a method to discard features based on the p-values determined in the test. In[39]:. We expect the feature selection to be able to identify the featuresthat are noninformative and remove them. Let’s apply the featureselection for classification on the cancer dataset. In[39:. We’ll add",
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-1-subsection-3",
                            "title": "Optimization Strategies",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-2",
                    "title": "Building Pipelines",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-2-subsection-1",
                            "title": "Pipeline Components",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-2-subsection-2",
                            "title": "Pipeline Construction",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-2-subsection-3",
                            "title": "Error Handling",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-3",
                    "title": "Using Pipelines in Grid Searches",
                    "content": "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"List of grids:\\n{}\".format(param_grid))\nOut[34]:\nList of grids:\n[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nIn the first grid, the kernel parameter is always set to 'rbf' (not that the entry for\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\nlet’s apply this more complex parameter search:\nIn[35]:\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[35]:\nBest parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 0.01}\nBest cross-validation score: 0.97\nGrid Search \n| \n271\nLet’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\nvaried:\nIn[36]:\nresults = pd.DataFrame(grid_search.cv_results_)\n# we display the transposed table so that it better fits on the page:\ndisplay(results.T)\nOut[36]:\n0\n1\n2\n3\n… 38\n39\n40\n41\nparam_C\n0.001\n0.001\n0.001\n0.001\n… 0.1\n1\n10\n100\nparam_gamma\n0.001\n0.01\n0.1\n1\n… NaN\nNaN\nNaN\nNaN\nparam_kernel\nrbf\nrbf\nrbf\nrbf\n… linear\nlinear\nlinear\nlinear\nparams\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.001}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.01}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.1}\n{C: 0.001,\nkernel: rbf,\ngamma: 1}\n… {C: 0.1,\nkernel:\nlinear}\n{C: 1,\nkernel:\nlinear}\n{C: 10,\nkernel:\nlinear}",
                    "summary": "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the list is expanded into an independent grid. A possible grid search involving kernel and parameters could look like this:. The kernel parameter is always set to 'rbf' and both the C and gamma parameters are varied. In the second grid, the kernel is set to linear, and only C is varied.    The grid search results can be found at: http://www.gridSearchCV.com/grid-search/ grid-search-cv.html#grid- search-results-can-be-found-at: http: //www. GridSearch CV. Let’s look at the cv_results_ again. Now try a more complex parameter search. The result is the same, but the parameters are much more complex. If kernel is 'linear', then only C is 'varied' We display the transposed table so that it better fits on the page. As expected, if kernel is ‘linear’, then only ‘varied’ is C. If kernel is not ‘ linear,’ then C is the same as the ‘normal’ version of the kernel. For example, kernel could be called ‘R’ and C could be ‘C’.",
                    "children": [
                        {
                            "id": "chapter-6-section-3-subsection-1",
                            "title": "Search Strategy",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-3-subsection-2",
                            "title": "Parameter Grid",
                            "content": "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"List of grids:\\n{}\".format(param_grid))\nOut[34]:\nList of grids:\n[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nIn the first grid, the kernel parameter is always set to 'rbf' (not that the entry for\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\nlet’s apply this more complex parameter search:\nIn[35]:\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[35]:\nBest parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 0.01}\nBest cross-validation score: 0.97\nGrid Search \n| \n271\nLet’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\nvaried:\nIn[36]:\nresults = pd.DataFrame(grid_search.cv_results_)\n# we display the transposed table so that it better fits on the page:\ndisplay(results.T)\nOut[36]:\n0\n1\n2\n3\n… 38\n39\n40\n41\nparam_C\n0.001\n0.001\n0.001\n0.001\n… 0.1\n1\n10\n100\nparam_gamma\n0.001\n0.01\n0.1\n1\n… NaN\nNaN\nNaN\nNaN\nparam_kernel\nrbf\nrbf\nrbf\nrbf\n… linear\nlinear\nlinear\nlinear\nparams\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.001}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.01}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.1}\n{C: 0.001,\nkernel: rbf,\ngamma: 1}\n… {C: 0.1,\nkernel:\nlinear}\n{C: 1,\nkernel:\nlinear}\n{C: 10,\nkernel:\nlinear}",
                            "summary": "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the list is expanded into an independent grid. A possible grid search involving kernel and parameters could look like this:. The kernel parameter is always set to 'rbf' and both the C and gamma parameters are varied. In the second grid, the kernel is set to linear, and only C is varied.    The grid search results can be found at: http://www.gridSearchCV.com/grid-search/ grid-search-cv.html#grid- search-results-can-be-found-at: http: //www. GridSearch CV. Let’s look at the cv_results_ again. Now try a more complex parameter search. The result is the same, but the parameters are much more complex. If kernel is 'linear', then only C is 'varied' We display the transposed table so that it better fits on the page. As expected, if kernel is ‘linear’, then only ‘varied’ is C. If kernel is not ‘ linear,’ then C is the same as the ‘normal’ version of the kernel. For example, kernel could be called ‘R’ and C could be ‘C’.",
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-3-subsection-3",
                            "title": "Optimization Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-4",
                    "title": "The General Pipeline Interface",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-4-subsection-1",
                            "title": "Interface Components",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-4-subsection-2",
                            "title": "Usage Patterns",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-4-subsection-3",
                            "title": "Best Practices",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-5",
                    "title": "Convenient Pipeline Creation with make_pipeline",
                    "content": "Parameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nclass is in chaining preprocessing steps (like scaling of the data) together with a\nsupervised model like a classifier.\nBuilding Pipelines\nLet’s look at how we can use the Pipeline class to express the workflow for training\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\ncontaining a name (any string of your choosing1) and an instance of an estimator:\nIn[5]:\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\nany other scikit-learn estimator:\nIn[6]:\npipe.fit(X_train, y_train)\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\nthe test data, we simply call pipe.score:\nIn[7]:\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\nOut[7]:\nTest score: 0.95\nCalling the score method on the pipeline first transforms the test data using the\nscaler, and then calls the score method on the SVM using the scaled test data. As you\ncan see, the result is identical to the one we got from the code at the beginning of the\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\ncode needed for our “preprocessing + classification” process. The main benefit of\nusing the pipeline, however, is that we can now use this single estimator in\ncross_val_score or GridSearchCV.\n308 \n|\nIn[15]:\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # iterate over all but the final step\n        # fit and transform the data\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # fit the last step\n    self.steps[-1][1].fit(X_transformed, y)\n    return self\nWhen predicting using Pipeline, we similarly transform the data using all but the\nlast step, and then call predict on the last step:\nIn[16]:\ndef predict(self, X):\n    X_transformed = X\n    for step in self.steps[:-1]:\n        # iterate over all but the final step\n        # transform the data\n        X_transformed = step[1].transform(X_transformed)\n    # fit the last step\n    return self.steps[-1][1].predict(X_transformed)\n312 \n| \nChapter 6: Algorithm Chains and Pipelines\nThe process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a\nclassifier (called Classifier).\nFigure 6-3. Overview of the pipeline training and prediction process\nThe pipeline is actually even more general than this. There is no requirement for the\nlast step in a pipeline to have a predict function, and we could create a pipeline just\ncontaining, for example, a scaler and PCA. Then, because the last step (PCA) has a\ntransform method, we could call transform on the pipeline to get the output of\nPCA.transform applied to the data that was processed by the previous step. The last\nstep of a pipeline is only required to have a fit method.\nConvenient Pipeline Creation with make_pipeline\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\nand we often don’t need user-specified names for each step. There is a convenience\nfunction, make_pipeline, that will create a pipeline for us and automatically name\neach step based on its class. The syntax for make_pipeline is as follows:\nIn[17]:\nfrom sklearn.pipeline import make_pipeline\n# standard syntax\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# abbreviated syntax",
                    "summary": "The Pipeline class has fit, predict, and score methods and behaves justlike any other model in scikit-learn. The most common use case is in chaining preprocessing steps (like scaling of the data) together with a supervised model like a classifier. Let’s look at how we can use the Pipeline class to express the workflow for training                an SVM after scaling the data with MinMaxScaler (for now without the grid search) First, we build a pipeline object by providing it with a list of steps. Each step is a tuplecontaining a name (any string of your choosing1) and an instance of an estimator:pipe = Pipeline([(\"scaler\", MinMax scaler The pipeline first transforms the test data using the scaler, and then calls the score method on the SVM using the scaled test data. The result is identical to the one we got from the code at the beginning of the chapter, when doing the transformations by hand. Using the pipeline, we can reduce the code needed for our “preprocessing + classification” process. The pipeline can be used to fit any other scikit-learn estimator, like the one in this article. The code for the pipeline is available on GitHub, and you can download it from the project page here. The main benefit of using the pipeline is that we can now use this single estimator in cross_val_score or GridSearchCV. The process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a classifier (called Classifier) For predicting using Pipeline, we similarly transform the data using all but the last step, and then call predict on the final step. The pipeline can also be used to transform data using a single transformer, such as T1 or T2. The algorithm chains and Pipelines section of this article is divided into two parts: Algorithm Chains and Algorithm Pipelines, and algorithm Pipelines. The pipeline is actually even more general than this. There is no requirement for the last step in a pipeline to have a predict function. We could create a pipeline just containing a scaler and PCA. The last step (PCA) has a transform method, so we could call transform on the pipeline to get the output of PCA applied to the data that was processed by the previous step. We often don’t need user-specified names for each step. The make_pipeline function creates a pipeline for us and automatically name each step based on its class. It can also be used to create pipelines for other types of data, such as text and images. The syntax for make_pipeline is as follows: In[17]:from sklearn.p",
                    "children": [
                        {
                            "id": "chapter-6-section-5-subsection-1",
                            "title": "Creation Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-5-subsection-2",
                            "title": "Pipeline Options",
                            "content": "Parameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nclass is in chaining preprocessing steps (like scaling of the data) together with a\nsupervised model like a classifier.\nBuilding Pipelines\nLet’s look at how we can use the Pipeline class to express the workflow for training\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\ncontaining a name (any string of your choosing1) and an instance of an estimator:\nIn[5]:\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\nany other scikit-learn estimator:\nIn[6]:\npipe.fit(X_train, y_train)\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\nthe test data, we simply call pipe.score:\nIn[7]:\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\nOut[7]:\nTest score: 0.95\nCalling the score method on the pipeline first transforms the test data using the\nscaler, and then calls the score method on the SVM using the scaled test data. As you\ncan see, the result is identical to the one we got from the code at the beginning of the\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\ncode needed for our “preprocessing + classification” process. The main benefit of\nusing the pipeline, however, is that we can now use this single estimator in\ncross_val_score or GridSearchCV.\n308 \n|\nIn[15]:\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # iterate over all but the final step\n        # fit and transform the data\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # fit the last step\n    self.steps[-1][1].fit(X_transformed, y)\n    return self\nWhen predicting using Pipeline, we similarly transform the data using all but the\nlast step, and then call predict on the last step:\nIn[16]:\ndef predict(self, X):\n    X_transformed = X\n    for step in self.steps[:-1]:\n        # iterate over all but the final step\n        # transform the data\n        X_transformed = step[1].transform(X_transformed)\n    # fit the last step\n    return self.steps[-1][1].predict(X_transformed)\n312 \n| \nChapter 6: Algorithm Chains and Pipelines\nThe process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a\nclassifier (called Classifier).\nFigure 6-3. Overview of the pipeline training and prediction process\nThe pipeline is actually even more general than this. There is no requirement for the\nlast step in a pipeline to have a predict function, and we could create a pipeline just\ncontaining, for example, a scaler and PCA. Then, because the last step (PCA) has a\ntransform method, we could call transform on the pipeline to get the output of\nPCA.transform applied to the data that was processed by the previous step. The last\nstep of a pipeline is only required to have a fit method.\nConvenient Pipeline Creation with make_pipeline\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\nand we often don’t need user-specified names for each step. There is a convenience\nfunction, make_pipeline, that will create a pipeline for us and automatically name\neach step based on its class. The syntax for make_pipeline is as follows:\nIn[17]:\nfrom sklearn.pipeline import make_pipeline\n# standard syntax\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# abbreviated syntax",
                            "summary": "The Pipeline class has fit, predict, and score methods and behaves justlike any other model in scikit-learn. The most common use case is in chaining preprocessing steps (like scaling of the data) together with a supervised model like a classifier. Let’s look at how we can use the Pipeline class to express the workflow for training                an SVM after scaling the data with MinMaxScaler (for now without the grid search) First, we build a pipeline object by providing it with a list of steps. Each step is a tuplecontaining a name (any string of your choosing1) and an instance of an estimator:pipe = Pipeline([(\"scaler\", MinMax scaler The pipeline first transforms the test data using the scaler, and then calls the score method on the SVM using the scaled test data. The result is identical to the one we got from the code at the beginning of the chapter, when doing the transformations by hand. Using the pipeline, we can reduce the code needed for our “preprocessing + classification” process. The pipeline can be used to fit any other scikit-learn estimator, like the one in this article. The code for the pipeline is available on GitHub, and you can download it from the project page here. The main benefit of using the pipeline is that we can now use this single estimator in cross_val_score or GridSearchCV. The process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a classifier (called Classifier) For predicting using Pipeline, we similarly transform the data using all but the last step, and then call predict on the final step. The pipeline can also be used to transform data using a single transformer, such as T1 or T2. The algorithm chains and Pipelines section of this article is divided into two parts: Algorithm Chains and Algorithm Pipelines, and algorithm Pipelines. The pipeline is actually even more general than this. There is no requirement for the last step in a pipeline to have a predict function. We could create a pipeline just containing a scaler and PCA. The last step (PCA) has a transform method, so we could call transform on the pipeline to get the output of PCA applied to the data that was processed by the previous step. We often don’t need user-specified names for each step. The make_pipeline function creates a pipeline for us and automatically name each step based on its class. It can also be used to create pipelines for other types of data, such as text and images. The syntax for make_pipeline is as follows: In[17]:from sklearn.p",
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-5-subsection-3",
                            "title": "Common Patterns",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-6",
                    "title": "Accessing Step Attributes",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-6-subsection-1",
                            "title": "Attribute Types",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-6-subsection-2",
                            "title": "Access Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-6-subsection-3",
                            "title": "Debugging Tips",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-7",
                    "title": "Accessing Attributes in a Grid-Searched Pipeline",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-7-subsection-1",
                            "title": "Search Results",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-7-subsection-2",
                            "title": "Attribute Access",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-7-subsection-3",
                            "title": "Result Analysis",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-8",
                    "title": "Grid-Searching Preprocessing Steps and Model Parameters",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-8-subsection-1",
                            "title": "Search Space",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-8-subsection-2",
                            "title": "Optimization",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-8-subsection-3",
                            "title": "Performance Tuning",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-9",
                    "title": "Grid-Searching Which Model To Use",
                    "content": "alpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\ntures are actually important, you should use L1. Otherwise, you should default to L2.\nL1 can also be useful if interpretability of the model is important. As L1 will use only\na few features, it is easier to explain which features are important to the model, and\nwhat the effects of these features are.\nLinear models are very fast to train, and also fast to predict. They scale to very large\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\nsands or millions of samples, you might want to investigate using the solver='sag'\noption in LogisticRegression and Ridge, which can be faster than the default on\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\nclass, which implement even more scalable versions of the linear models described\nhere.\nAnother strength of linear models is that they make it relatively easy to understand\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\nare. This is particularly true if your dataset has highly correlated features; in these\ncases, the coefficients might be hard to interpret.\nSupervised Machine Learning Algorithms \n| \n67\nLinear models often perform well when the number of features is large compared to\nthe number of samples. They are also often used on very large datasets, simply\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\nother models might yield better generalization performance. We will look at some\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics",
                    "summary": "Linear models are very fast to train, and also fast to predict. They scale to very largedatasets and work well with sparse data. tuning these parameters is quite important. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to use L1 regularization or L2 regularization. L1 can also be useful if interpretability of the model is important. As L1 will use only a few features, it is easier to explain which features are important to the model, and what the effects of these features are. The solver='sag'option in LogisticRegression and Ridge can be faster than the default on large datasets. Linear models often perform well when the number of features is large. They are also often used on very large datasets, simply because it’s not feasible to train other models. In lower-dimensional spaces, other models might yield better generalization performance. Other options are the SGDClassifier class and the SGDRegressor class, which implement even more scalable versions of the linear models described here. For more information, visit the Machine Learning Algorithms blog and the Google Machine Learning Blog. For a more detailed look at how these algorithms work, see the Machine learning Algorithm blog and Google Machine learning Blog. for more information. We will look at some of the different types of features that can be selected. We will also look at how these features can be combined to create a more holistic view of the world. We are going to look at a number of different ways to select features. We'll start with a look at the \"Automatic Feature Selection\" option. We will focus on the supervised methods, regression and classification, as well as evaluating and selecting models in supervised learning. We will split our dataset into supervised models and unsupervised models, so that we can evaluate supervised models. We have built a training set and a test set using a very qualitative process (as we saw in Chapter 3). We have also built a model on the train set using the test set, and evaluated it on the training set using classification for computes. We are now going to dive more deeply into evaluating models and selecting parameters. We hope this article will help you understand how to use machine learning to improve your knowledge of the world around you. Back to the page you came from. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. Cross-validation is a more robust way to assess generalization performance, and might be essential for linear models. Sometimes it is also a good idea to transform the target variable y in regression. Tree-based models, on the other hand, are often able to discover important interactions and don’t require transforming the data explicitly most of the time. This is particularlytrue for less complex models like linear models and naive Bayes models. We will first intro the univariate nonlinear transformations, then look at the tree-based nonlinear Transformations (U-transform) section. How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training",
                    "children": [
                        {
                            "id": "chapter-6-section-9-subsection-1",
                            "title": "Model Selection",
                            "content": "alpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\ntures are actually important, you should use L1. Otherwise, you should default to L2.\nL1 can also be useful if interpretability of the model is important. As L1 will use only\na few features, it is easier to explain which features are important to the model, and\nwhat the effects of these features are.\nLinear models are very fast to train, and also fast to predict. They scale to very large\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\nsands or millions of samples, you might want to investigate using the solver='sag'\noption in LogisticRegression and Ridge, which can be faster than the default on\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\nclass, which implement even more scalable versions of the linear models described\nhere.\nAnother strength of linear models is that they make it relatively easy to understand\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\nare. This is particularly true if your dataset has highly correlated features; in these\ncases, the coefficients might be hard to interpret.\nSupervised Machine Learning Algorithms \n| \n67\nLinear models often perform well when the number of features is large compared to\nthe number of samples. They are also often used on very large datasets, simply\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\nother models might yield better generalization performance. We will look at some\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics",
                            "summary": "Linear models are very fast to train, and also fast to predict. They scale to very largedatasets and work well with sparse data. tuning these parameters is quite important. Usually C and alpha are searched for on a logarithmic scale. The other decision you have to make is whether you want to use L1 regularization or L2 regularization. L1 can also be useful if interpretability of the model is important. As L1 will use only a few features, it is easier to explain which features are important to the model, and what the effects of these features are. The solver='sag'option in LogisticRegression and Ridge can be faster than the default on large datasets. Linear models often perform well when the number of features is large. They are also often used on very large datasets, simply because it’s not feasible to train other models. In lower-dimensional spaces, other models might yield better generalization performance. Other options are the SGDClassifier class and the SGDRegressor class, which implement even more scalable versions of the linear models described here. For more information, visit the Machine Learning Algorithms blog and the Google Machine Learning Blog. For a more detailed look at how these algorithms work, see the Machine learning Algorithm blog and Google Machine learning Blog. for more information. We will look at some of the different types of features that can be selected. We will also look at how these features can be combined to create a more holistic view of the world. We are going to look at a number of different ways to select features. We'll start with a look at the \"Automatic Feature Selection\" option. We will focus on the supervised methods, regression and classification, as well as evaluating and selecting models in supervised learning. We will split our dataset into supervised models and unsupervised models, so that we can evaluate supervised models. We have built a training set and a test set using a very qualitative process (as we saw in Chapter 3). We have also built a model on the train set using the test set, and evaluated it on the training set using classification for computes. We are now going to dive more deeply into evaluating models and selecting parameters. We hope this article will help you understand how to use machine learning to improve your knowledge of the world around you. Back to the page you came from. In this chapter, we will expand on two aspects of this evaluation. We are not interested in how well our model fit the training set, but rather in how horribly it can make predictions for data that was not observed during training. The reason we split our data into training and test sets is that we are inter‐protested in measuring how well we model generalizes to new, previously unseen data. The next two chapters will focus on how we can use this data to make predictions about the future. The final chapter will look at how we could use the data to predict the future in the form of a prediction model. The end of the chapter will be a discussion of how to use the model to predict future data. Cross-validation is a more robust way to assess generalization performance, and might be essential for linear models. Sometimes it is also a good idea to transform the target variable y in regression. Tree-based models, on the other hand, are often able to discover important interactions and don’t require transforming the data explicitly most of the time. This is particularlytrue for less complex models like linear models and naive Bayes models. We will first intro the univariate nonlinear transformations, then look at the tree-based nonlinear Transformations (U-transform) section. How can you know how good each feature is? There are three basic strategies: univariatestatistics, model-based selection, and iterative selection. We will discuss all three of them in detail. The goal of this article is to show how these strategies can be applied to high-dimensional datasets. The aim is to help you understand how to use these strategies to improve your data analysis. We hope that this will help you make better decisions about how to analyze your data. Back to the page you came from. The post was originally published on November 14, 2013. Back into the page. Back To the page that was originally posted. The original article was published on December 7, 2013, at 10:30am ET. All of these methods are supervised methods, meaning they need thetarget for fitting the model. This means we need to split the data into training and testsets, and fit the feature selection only on the training",
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-9-subsection-2",
                            "title": "Comparison Metrics",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-9-subsection-3",
                            "title": "Decision Criteria",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-6-section-10",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-6-section-10-subsection-1",
                            "title": "Key Learnings",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-10-subsection-2",
                            "title": "Advanced Topics",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-6-section-10-subsection-3",
                            "title": "Future Developments",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-7",
            "title": "7. Working with Text Data",
            "content": "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\ngories that will capture responses in a way that makes sense for your application. You\nmight then have some categories for standard colors, maybe a category “multicol‐\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\ntion where you can influence data collection, we highly recommend avoiding man‐\nually entered values for concepts that are better captured using categorical variables.\nOften, manually entered values do not correspond to fixed categories, but still have\nsome underlying structure, like addresses, names of places or people, dates, telephone\n324 \n| \nChapter 7: Working with Text Data\n1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\nthemselves.\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\ntheir treatment is highly dependent on context and domain. A systematic treatment\nof these cases is beyond the scope of this book.\nThe final category of string data is freeform text data that consists of phrases or sen‐\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nGrid-Searching Which Model To Use                                                                        319\nSummary and Outlook                                                                                                 320\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\nTypes of Data Represented as Strings                                                                         323\nTable of Contents \n| \nv\nExample Application: Sentiment Analysis of Movie Reviews                                 325\nRepresenting Text Data as a Bag of Words                                                                 327\nApplying Bag-of-Words to a Toy Dataset                                                               329\nBag-of-Words for Movie Reviews                                                                            330\nStopwords                                                                                                                       334\nRescaling the Data with tf–idf                                                                                      336\nInvestigating Model Coefficients                                                                                 338\nBag-of-Words with More Than One Word (n-Grams)                                            339\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called detail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\nBag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only \"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features: are too frequent to be informative. There are two main approaches: using a language-\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\nlearn has a built-in list of English stopwords in the feature_extraction.text\nmodule:\n334 \n| \nChapter 7: Working with Text Data\nIn[20]:\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\nOut[20]:\nNumber of stop words: 318\nEvery 10th stopword:\n['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\nClearly, removing the stopwords in the list can only decrease the number of features\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\nmance. Let’s give it a try:\nIn[21]:\n# Specifying stop_words=\"english\" uses the built-in list.\n# We could also augment it and pass our own.\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with stop words:\\n{}\".format(repr(X_train)))\nOut[21]:\nX_train with stop words:\n<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n    with 2149958 stored elements in Compressed Sparse Row format>\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\nIn[22]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[22]:\nBest cross-validation score: 0.88\nThe grid search performance decreased slightly using the stopwords—not enough to which takes in the text data and does both the bag-of-words feature extraction and\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\nwhich you can read about on Wikipedia. The tf–idf score for word w in document d\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\nby:7\ntfidf w, d = tf log\nN + 1\nNw + 1 + 1\nwhere N is the number of documents in the training set, Nw is the number of docu‐\nments in the training set that the word w appears in, and tf (the term frequency) is the\nnumber of times that the word w appears in the query document d (the document\nyou want to transform or encode). Both classes also apply L2 normalization after\ncomputing the tf–idf representation; in other words, they rescale the representation\nof each document to have Euclidean norm 1. Rescaling in this way means that the\nlength of a document (the number of words) does not change the vectorized repre‐\nsentation.\nBecause tf–idf actually makes use of the statistical properties of the training data, we\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\nare valid. This leads to the following code:\n336 \n| \nChapter 7: Working with Text Data\nIn[23]:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[23]:\nBest cross-validation score: 0.89\nAs you can see, there is some improvement when using tf–idf instead of just word\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only Bag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\nming, but it usually produces better results than stemming when used for normaliz‐\ning tokens for machine learning.\nWhile scikit-learn implements neither form of normalization, CountVectorizer\nallows specifying your own tokenizer to convert each document into a list of tokens\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\ncallable that will take a string and produce a list of lemmas:\nIn[38]:\n# Technicality: we want to use the regexp-based tokenizer\n# that is used by CountVectorizer and only use the lemmatization\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n# with the regexp-based tokenization.\nimport re\n# regexp used in CountVectorizer\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n# load spacy language model and save old tokenizer\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n# replace the tokenizer with the preceding regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n    regexp.findall(string))\n# create a custom tokenizer using the spacy document processing pipeline\n# (now using our own tokenizer)\ndef custom_tokenizer(document):\n    doc_spacy = en_nlp(document, entity=False, parse=False)\n    return [token.lemma_ for token in doc_spacy]\n# define a count vectorizer with the custom tokenizer\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\nLet’s transform the data and inspect the vocabulary size:\nIn[39]:\n# transform text_train using CountVectorizer with lemmatization\nX_train_lemma = lemma_vect.fit_transform(text_train)\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n# standard CountVectorizer for reference\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n# load spacy's English-language models\nen_nlp = spacy.load('en')\n# instantiate nltk's Porter stemmer\nstemmer = nltk.stem.PorterStemmer()\n# define function to compare lemmatization in spacy with stemming in nltk\ndef compare_normalization(doc):\n    # tokenize document in spacy\n    doc_spacy = en_nlp(doc)\n    # print lemmas found by spacy\n    print(\"Lemmatization:\")\n    print([token.lemma_ for token in doc_spacy])\n    # print tokens found by Porter stemmer\n    print(\"Stemming:\")\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\nshow some of the differences:\nIn[37]:\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\n                       \"I'm scared of meeting the clients tomorrow.\")\nOut[37]:\nLemmatization:\n['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming:\n['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\ntogether frequently. LDA also requires that each document can be understood as a\n“mixture” of a subset of the topics. It is important to understand that for the machine\nlearning model a “topic” might not be what we would normally call a topic in every‐\nday speech, but that it resembles more the components extracted by PCA or NMF\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\ning. Even if there is a semantic meaning for an LDA “topic”, it might not be some‐\nthing we’d usually call a topic. Going back to the example of news articles, we might\nhave a collection of articles about sports, politics, and finance, written by two specific\nauthors. In a politics article, we might expect to see words like “governor,” “vote,”\n“party,” etc., while in a sports article we might expect words like “team,” “score,” and\n“season.” Words in each of these groups will likely appear together, while it’s less likely\nthat, for example, “team” and “governor” will appear together. However, these are not\nthe only groups of words we might expect to appear together. The two reporters\nmight prefer different phrases or different choices of words. Maybe one of them likes\nto use the word “demarcate” and one likes the word “polarize.” Other “topics” would\nthen be “words often used by reporter A” and “words often used by reporter B,”\nthough these are not topics in the usual sense of the word.\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\nunsupervised text document models, it is often good to remove very common words,\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nax[col].set_xlim(0, 2000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()\nTopic Modeling and Document Clustering \n| \n353\nFigure 7-6. Topic weights learned by LDA\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\nlaudatory words.\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\nas most reviews are made up of some movie-specific comments and some comments\nthat justify or emphasize the rating.\nTopic models like LDA are interesting methods to understand large text corpora in\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\nrandomized, though, and changing the random_state parameter can lead to quite\n354 \n| \nChapter 7: Working with Text Data\ndifferent outcomes. While identifying topics can be helpful, any conclusions you\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\nommend verifying your intuition by looking at the documents in a specific topic. The\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\npact representation for supervised learning. This is particularly helpful when few\ntraining examples are available.\nSummary and Outlook\nIn this chapter we talked about the basics of processing text, also known as natural\nlanguage processing (NLP), with an example application classifying movie reviews.\nThe tools discussed here should serve as a great starting point when trying to process\ntext data. In particular for text classification tasks such as spam and fraud detection\nor sentiment analysis, bag-of-words representations provide a simple and powerful\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndirector      children      director      plays         horror\nminutes       10            played        jack          young\npretty        kid           performance   joe           goes\ndoesn         old           mr            performances  around\nJudging from the important words, topic 1 seems to be about historical and war mov‐\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\nseems to capture some very common words, while topic 6 appears to be about child‐\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\neach of the topics needs to be very broad, so that they can together cover all the dif‐\nferent kinds of reviews in our dataset.\nNext, we will learn another model, this time with 100 topics. Using more topics\nmakes the analysis much harder, but makes it more likely that topics can specialize to\ninteresting subsets of the data:\nIn[46]:\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n                                   max_iter=25, random_state=0)\ndocument_topics100 = lda100.fit_transform(X)\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\ning and representative topics:\n350 \n| \nChapter 7: Working with Text Data\nIn[47]:\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vect.get_feature_names())\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\nOut[48]:\ntopic 7       topic 16      topic 24      topic 25      topic 28\n--------      --------      --------      --------      --------\nOut[43]:\n(10, 10000)\nTo understand better what the different topics mean, we will look at the most impor‐\ntant words for each of the topics. The print_topics function provides a nice format‐\nting for these features:\nIn[44]:\n# For each topic (a row in the components_), sort the features (ascending)\n# Invert rows with [:, ::-1] to make sorting descending\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n# Get the feature names from the vectorizer\nfeature_names = np.array(vect.get_feature_names())\nIn[45]:\n# Print out the 10 topics:\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\nTopic Modeling and Document Clustering \n| \n349\nOut[45]:\ntopic 0       topic 1       topic 2       topic 3       topic 4\n--------      --------      --------      --------      --------\nbetween       war           funny         show          didn\nyoung         world         worst         series        saw\nfamily        us            comedy        episode       am\nreal          our           thing         tv            thought\nperformance   american      guy           episodes      years\nbeautiful     documentary   re            shows         book\nwork          history       stupid        season        watched\neach          new           actually      new           now\nboth          own           nothing       television    dvd\ndirector      point         want          years         got\ntopic 5       topic 6       topic 7       topic 8       topic 9\n--------      --------      --------      --------      --------\nhorror        kids          cast          performance   house\naction        action        role          role          woman\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndolph         10            laughs        flesh         felt\ncareer        give          fun           minutes       part\nsabrina       want          re            body          going\nrole          nothing       funniest      living        seemed\ntemple        terrible      laughing      eating        bit\nphantom       crap          joke          flick         found\njudy          must          few           budget        though\nmelissa       reviews       moments       head          nothing\nzorro         imdb          guy           gory          lot\ngets          director      unfunny       evil          saw\nbarbra        thing         times         shot          long\ncast          believe       laughed       low           interesting\nshort         am            comedies      fulci         few\nserial        actually      isn           re            half\nThe topics we extracted this time seem to be more specific, though many are hard to\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\nreviews of comedies. If we want to make further inferences using the topics that were\ndiscovered, we should confirm the intuition we gained from looking at the highest-\nranking words for each topic by looking at the documents that are assigned to these\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\nreviews are assigned to this topic:\nIn[49]:\n# sort by weight of \"music\" topic 45\nmusic = np.argsort(document_topics100[:, 45])[::-1]\n# print the five documents where the topic is most important\nfor i in music[:10]:\n    # pshow first two sentences\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\nOut[49]:\nb'I love this movie and never get tired of watching. The music in it is great.\\n'\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n  band from the 70's decide to give it another try.\\n\" Topic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors",
            "summary": "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500",
            "children": [
                {
                    "id": "chapter-7-section-1",
                    "title": "Types of Data Represented as Strings",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-7-section-1-subsection-1",
                            "title": "Text Types",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-1-subsection-2",
                            "title": "String Representations",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-1-subsection-3",
                            "title": "Processing Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-7-section-2",
                    "title": "Example Application: Sentiment Analysis of Movie Reviews",
                    "content": "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\ngories that will capture responses in a way that makes sense for your application. You\nmight then have some categories for standard colors, maybe a category “multicol‐\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\ntion where you can influence data collection, we highly recommend avoiding man‐\nually entered values for concepts that are better captured using categorical variables.\nOften, manually entered values do not correspond to fixed categories, but still have\nsome underlying structure, like addresses, names of places or people, dates, telephone\n324 \n| \nChapter 7: Working with Text Data\n1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\nthemselves.\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\ntheir treatment is highly dependent on context and domain. A systematic treatment\nof these cases is beyond the scope of this book.\nThe final category of string data is freeform text data that consists of phrases or sen‐\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nGrid-Searching Which Model To Use                                                                        319\nSummary and Outlook                                                                                                 320\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\nTypes of Data Represented as Strings                                                                         323\nTable of Contents \n| \nv\nExample Application: Sentiment Analysis of Movie Reviews                                 325\nRepresenting Text Data as a Bag of Words                                                                 327\nApplying Bag-of-Words to a Toy Dataset                                                               329\nBag-of-Words for Movie Reviews                                                                            330\nStopwords                                                                                                                       334\nRescaling the Data with tf–idf                                                                                      336\nInvestigating Model Coefficients                                                                                 338\nBag-of-Words with More Than One Word (n-Grams)                                            339\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called detail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\nBag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only \"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features: are too frequent to be informative. There are two main approaches: using a language-\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\nlearn has a built-in list of English stopwords in the feature_extraction.text\nmodule:\n334 \n| \nChapter 7: Working with Text Data\nIn[20]:\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\nOut[20]:\nNumber of stop words: 318\nEvery 10th stopword:\n['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\nClearly, removing the stopwords in the list can only decrease the number of features\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\nmance. Let’s give it a try:\nIn[21]:\n# Specifying stop_words=\"english\" uses the built-in list.\n# We could also augment it and pass our own.\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with stop words:\\n{}\".format(repr(X_train)))\nOut[21]:\nX_train with stop words:\n<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n    with 2149958 stored elements in Compressed Sparse Row format>\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\nIn[22]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[22]:\nBest cross-validation score: 0.88\nThe grid search performance decreased slightly using the stopwords—not enough to which takes in the text data and does both the bag-of-words feature extraction and\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\nwhich you can read about on Wikipedia. The tf–idf score for word w in document d\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\nby:7\ntfidf w, d = tf log\nN + 1\nNw + 1 + 1\nwhere N is the number of documents in the training set, Nw is the number of docu‐\nments in the training set that the word w appears in, and tf (the term frequency) is the\nnumber of times that the word w appears in the query document d (the document\nyou want to transform or encode). Both classes also apply L2 normalization after\ncomputing the tf–idf representation; in other words, they rescale the representation\nof each document to have Euclidean norm 1. Rescaling in this way means that the\nlength of a document (the number of words) does not change the vectorized repre‐\nsentation.\nBecause tf–idf actually makes use of the statistical properties of the training data, we\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\nare valid. This leads to the following code:\n336 \n| \nChapter 7: Working with Text Data\nIn[23]:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[23]:\nBest cross-validation score: 0.89\nAs you can see, there is some improvement when using tf–idf instead of just word\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only Bag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\nming, but it usually produces better results than stemming when used for normaliz‐\ning tokens for machine learning.\nWhile scikit-learn implements neither form of normalization, CountVectorizer\nallows specifying your own tokenizer to convert each document into a list of tokens\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\ncallable that will take a string and produce a list of lemmas:\nIn[38]:\n# Technicality: we want to use the regexp-based tokenizer\n# that is used by CountVectorizer and only use the lemmatization\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n# with the regexp-based tokenization.\nimport re\n# regexp used in CountVectorizer\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n# load spacy language model and save old tokenizer\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n# replace the tokenizer with the preceding regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n    regexp.findall(string))\n# create a custom tokenizer using the spacy document processing pipeline\n# (now using our own tokenizer)\ndef custom_tokenizer(document):\n    doc_spacy = en_nlp(document, entity=False, parse=False)\n    return [token.lemma_ for token in doc_spacy]\n# define a count vectorizer with the custom tokenizer\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\nLet’s transform the data and inspect the vocabulary size:\nIn[39]:\n# transform text_train using CountVectorizer with lemmatization\nX_train_lemma = lemma_vect.fit_transform(text_train)\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n# standard CountVectorizer for reference\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n# load spacy's English-language models\nen_nlp = spacy.load('en')\n# instantiate nltk's Porter stemmer\nstemmer = nltk.stem.PorterStemmer()\n# define function to compare lemmatization in spacy with stemming in nltk\ndef compare_normalization(doc):\n    # tokenize document in spacy\n    doc_spacy = en_nlp(doc)\n    # print lemmas found by spacy\n    print(\"Lemmatization:\")\n    print([token.lemma_ for token in doc_spacy])\n    # print tokens found by Porter stemmer\n    print(\"Stemming:\")\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\nshow some of the differences:\nIn[37]:\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\n                       \"I'm scared of meeting the clients tomorrow.\")\nOut[37]:\nLemmatization:\n['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming:\n['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced",
                    "summary": "The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. The text data is not in a format that a machine learning model can handle. We need to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. We discard most of the structure of the input text, like chapters, paragraphs, andsentences, and only count how often each word appears in each text inRepresenting Text Data as a Bag of Words. The result is a simple but effective and commonly used way to represent text in the form of a bag-of-words representation. Computing the bag-of-words representation for a corpus of documents consists of the following three steps. Split each document into the words that appear in it (called tokens), splitting them on whitespace and punctuation. Collect a vocabulary of all words that appears in any of the documents, and number them (say, in alphabetical order). Encoding. For each document, count how often each of the words in the vocabu‐ proprietarylary appear in this document. The output is one vector of word counts for each docu‐ detail later in this chapter.. Figure 7-1 illustrates the process on the string \"This is how you get ants\" using scikit-learn. The bag-of-words representation is implemented in CountVectorizer, which is atransformer. The output is one vector of word counts for each docu‐paralleledment. For each word in the vocabulary, we have a count of how often it appears in each document. For now, let’s look at how we can apply the bag- of-words processing using scikit-learn. The process on the string \"This                is how you get ants\" is shown in Figure 7-1. Note how the order of the words in the original string is completely irrelevant The vocabulary consists of 13 words, from \"be\" to \"wise\" The bag-of-words representation for the training data is a sparse matrix of type '<class 'numpy.int64'>' \"refreshing\" indicates positive movie reviews. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows. We can access the vocabulary_ attribute as well as the bag_of_words representation. The vocabulary size is set to 13, and the bag of words representation is a 2x13 sparse matrix. We call the transform method to create the bag- of-words Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong. The bag-of-words representation is stored in a SciPy sparse matrix that only stores entries that are nonzero. Here is an example on the toy data we used earlier: grotesquelyprint(\"bards_words:\\n{}\".format(bard_words) grotesquely print(\"Vocabulary:\\n {}\". format(cv.get_feature_names())) grotesquelyPrint(\"Bard's words: {}\".format(\"bard\", \"vocabulary\", \"bard\" }); grotesquely Print('Bard', \"v vocabulary\", \"bag_of_ A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary, meaning most entries in the feature array are 0. This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this would result in a MemoryError. Think about how many different words might appear in a movie review compared to all the Words in the English language (which is what the vocabulary models are based on) This is how we try to represent text data as a bag of words. The matrix is of shape 2×13, with one row for each of the two data points and one We can see that the word counts for each word are either 0 or 1. The first string (\"The fool doth think he is wise,\") is repre‐represented as the first row in the sparse matrix. It also contains the second word in the vocabulary, \"but\", zero times. Neither of the twostrings in bards_words contains a word twice. Let’s take a look at how to read these feature vectors using the toarray method. To look at the actual con‐                tent of the sparseMatrix, we can convert it to a “dense” NumPy array. The toarray() method also stores all the 0 entries. The fourth word, \"fool\", the tenth word, 'the', and the thirteenth word 'wise' all appear in both strings. The second word in the vocabulary, 'but', zero times. The third word 'doth', once, and so on. A quick analysis of the data confirms that this is indeed the case. The training and test data from the IMDb reviews are turned into lists of strings (text_train and text_test), which we will now process. The data is stored in Compressed Sparse Row format (CSPR) with 3431196 stored elements in type 'numpy.int64' The training data is turned into a sparse matrix of type The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that the vocabulary contains 74,000 entries. Let’s look at the vocabulary in a bit more detail. The data is stored as a SciPy sparse matrix. The first 20 features are called by the get_feature_name method of the vectorizer. Each entry corresponds to one fea‐repertoire. The vocabulary is stored in a form that makes sense for your strings can take a lot of manual effort and is not easily automated. Youmight then have some categories for standard colors, maybe a category “multicolored” for people that gave answers like “green and red stripes,” and an The content of websites linked to in tweets contains more information than the text of the tweets themselves. The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/. The techniques used in this chapter also apply to other languages that use the Romanalphabet, and partially to languages with word boundary delimiters. If you are in a posi­tion where you can influence data collection, we highly recommend avoiding man­ually entered values for concepts that are better captured using categorical variables. The data is available on http://aas.Stanford.University.edu.    The datasets are available on the Stanford University website. The final category of string data is freeform text data that consists of phrases or sen‐tences. Examples include tweets, chat logs, and hotel reviews. These kinds of strings are often very hard to parse, and their treatment is highly dependent on context and domain. A systematic treatmentof these cases is beyond the scope of this book. Accessing Attributes in a Grid-Searched Pipeline. Grid-Searching Which Model To Use. Preprocessing Steps and Model parameters. Working with Text Data. Using Text Data with a Grid Search. Using Grid Search with a grid search with a text search. Using a grid-search with text data. Using text data with aGrid Search. using text data to search for attributes in a pipeline. Using the Grid Search tool to search a pipeline for attributes. using the Grid search tool to The book is a collection of 50,000 ebooks, works of Shakespeare, Shakespeare, the content of Wikipedia, and content of e-books. The book also includes a selection of the most popular e-book titles of all time, including the works of William Shakespeare. It is available in English, French, Spanish, German, Italian, Spanish and Italian. It was published in the U.S. in 1998 and has been updated In this chapter, we will use a dataset of movie reviews from the IMDb (Internet Movie Database) website. This dataset contains the text of the reviews, together with a label that indi‐phthalcates whether a review is “positive” or “negative.” The IMDb website itself contains reviews from 1 to 10. We will use this dataset as a running example in the next chapter of this book. The next chapter will focus on the analysis of video clips. The final chapter will look at the analysis and analysis of music videos. It will conclude with a discussion of how to use this data to understand music videos in a new way. The dataset is provided as text files in two separate folders. The pos folder contains all the positive reviews, each as a separate text file. The neg folder contains the negative reviews. To simplify the modeling, this annotation is summarized as a two-class classification dataset where reviews with a score of 6 or higher are labeled as                positive, and the rest as negative. We will leave the question of whether this is a goodrepresentation of the data open, and simply use the data as provided by AndrewMaas. The data is provided in two different folders: one for the training data and one forThe training data is in the training folder, and another for the test data in the test folder. The test folder has two sub‐ The bag-of-words representation is implemented in CountVectorizer, which is a transformer. The output is one vector of word counts for each docu‐                 ment. For each word in the vocabulary, we have a count of how often it appears in each document. There is a helper function in scikit-learn to load files stored in such a folder structure, where each sub folder corresponds to a label, called detail later in this chapter. For now, let’s look at how we can apply the bag- of-wordsprocessing using scik it-learn. The result is one feature for each unique word in a whole dataset. For more information, visit the project's website. The vocabulary consists of 13 words, from \"be\" to \"wise\". The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows:.import Count Vectorizer from sklearn.feature_extraction.text import vocabulary_ from sklearning.feature.text. import bag_of_words from Sklearn. FeatureExtraction. text. import text_test from Sklearning. Feature Extraction. Text. Representing Text Data as a Bag of Words is a standard binary classification task. We discard most of the structure of the input text, and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”Computing the bag-of-words representation for a corpus of documents consists of the following three steps:1. Tokenization. 2. Generation of a string representation. 3. The final step is to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. Back to Mail Online home. Back To the page you came from. The bag-of-words process is a way to extract words from documents. The output is one vector of word counts for each doc. Figure 7-1 illustrates the process on the string \"This is how you get ants.\" The output of the process is the word count for each document. The process can be applied to other documents, such as books and movies. For now, let’s look at how we can apply the process to documents using scikit-learn. We will discuss the subtleties involved in step 1 and step 2 in more detail later in this chapter. Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong. The bag-of-words representation is stored in a SciPy sparse matrix that only stores entries that are nonzero. Here is an example on the toy data we used earlier: grotesquelyprint(\"bards_words:\\n{}\".format(bard_words) grotesquely print(\"Vocabulary:\\n {}\". format(cv.get_feature_names())) grotesquelyPrint(\"Bard's words: {}\".format(\"bard\", \"vocabulary\", \"bard\" }); grotesquely Print('Bard', \"v vocabulary\", \"bag_of_ A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary, meaning most entries in the feature array are 0. This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this would result in a MemoryError. Think about how many different words might appear in a movie review compared to all the Words in the English language (which is what the vocabulary models are based on) This is how we try to represent text data as a bag of words. The matrix is of shape 2×13, with one row for each of the two data points and one We can see that the word counts for each word are either 0 or 1. The first string (\"The fool doth think he is wise,\") is repre‐represented as the first row in the sparse matrix. It also contains the second word in the vocabulary, \"but\", zero times. Neither of the twostrings in bards_words contains a word twice. Let’s take a look at how to read these feature vectors using the toarray method. To look at the actual con‐                tent of the sparseMatrix, we can convert it to a “dense” NumPy array. The toarray() method also stores all the 0 entries. The fourth word, \"fool\", the tenth word, 'the', and the thirteenth word 'wise' all appear in both strings. The second word in the vocabulary, 'but', zero times. The third word 'doth', once, and so on. A quick analysis of the data confirms that this is indeed the case. The training and test data from the IMDb reviews are turned into lists of strings (text_train and text_test), which we will now process. The data is stored in Compressed Sparse Row format (CSPR) with 3431196 stored elements in type 'numpy.int64' The training data is turned into a sparse matrix of type The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that The vocabulary can be accessed using the get_feature_name method of the vect. Each entry in the vocabulary corresponds to one fea‐ture. We improved performance by a bit more than a per‐centric by adding bigram and trigram features. Let’s look at the vocabulary in a little more detail. We’ll start with the bag of words with more than one word (n-Grams) and then move on to the trigram and bigram vocabulary. Using bigrams increases performance quite a bit, while adding trigrams only provides a very small benefit in terms of accuracy. We can visualize the cross-validation.accuracy as a function of the ngram_range and C parameter as a heat map, as we did. in Chapter 5 (see Figure 7-3):In[33]:# extract scores from grid_search. # visualize heat map                heatmap = mglearn.tools.heatmap(20, The best model includes unigrams, bigrams, and trigrams. To understand better how the model improved, we can visualize the important coeffi‐342. The word “worth” was not present in the unigram model. \"not worth” is indicative of a negative review, while \"definitely worth\" and \"well worth\" are indicative ofa positive review. This is a good example of how to use the Text Datacient model in a real-world situation. The best model can be found at: http://www.textdatacient.org For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting. This problem can be overcome by representing each word using its word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. It is usually not possible to generalize to all words that have the same word stem, so the stem is used instead to represent words with different verb forms and nouns. For example, the stem of the word \"replaced\" is different to the verb \"to replace\" If a dictionary of known word forms is used, and the role of the word in the sentence is taken into account, the process is referred to as lemmatization and the standardized form ofthe word is known as the lemma. Another interesting case of normalization is spelling correction, which can behelpful in practice but is outside of the scope of this book. For details of the interface, consult the nltk and spacy documentation. For more information on how to work with Text Data, visit the Text Data section of Let’s compare a method for stemming with lemmatization as implemented in the spacy package. Many of the useful bigrams and trigrams consist of common words that would not be informative on their own, such as \"none of the\", \"the only good\", \"on and on\", \"this is one\", \"of the most\", and so on. We’ll visualize only trigrams, to provide further insight into why these features are helpful. We are more interested in the general principles of lemmats than in the specific details of stemming and trigram normalization. We hope this will give us a better understanding of how the heuristics are used. The impact of these features is quite limited compared to the importance of the unigramfeatures. One particular step that is often improved in more sophisticated text-processing applica­tions is the first step in the bag-of-words model: tokenization. Figure 7-5. Visualization of only the important trigram features of the model.Advanced Tokenization, Stemming, and Lemmatization are possible in more advanced text- processing applications. The Tfidf Vectorizer and CountVectorizer are relatively simple, and much more elaborate methods are possible The vocabulary often contains singular and plural versions of                some words. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only \"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best                today’s”. The two strings “it’” and “” have exactly the same representation, even though the mean‐phthalings are inverted. Putting “not” in front of a word is only one example (if Pairs of tokens are known as bigrams, triplets of tokens as trigrams, and sequences of tokens known as n-grams. We can change the range of tokens that are considered as features by changing the ngram_range parameter of Count Vectorizer or Tfidf Vectorizer. The ngram range parameter is a tuple, con‐                Bag-of-Words with More Than One Word (n-Grams)                  Concept of the minimum length and the maximum length of the sequences of token that The default is to create one feature per sequence of tokens that is at least one tokenlong and at most one token long. Here is an example on the toy data we used earlier: \"bards_words\": \"fool doth think he is wise, but the wise man knows himself to be a fool\" \"Vocabulary\": \"vocabulary: works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection of 50,000 ebooks\" In this chapter, we will use a dataset of movie reviews from the IMDb (Internet Movie Database) website. This dataset contains the text of the reviews, together with a label that indi‐phthalcates whether a review is “positive” or “negative.” The IMDb website itself contains reviews from 1 to 10. We will use this dataset as a running example in the next chapter of this book. The next chapter will focus on the analysis of video clips. The final chapter will look at the analysis and analysis of music videos. It will conclude with a discussion of how to use this data to understand music videos in a new way. The dataset is provided as text files in two separate folders. The pos folder contains all the positive reviews, each as a separate text file. The neg folder contains the negative reviews. To simplify the modeling, this annotation is summarized as a two-class classification dataset where reviews with a score of 6 or higher are labeled as                positive, and the rest as negative. We will leave the question of whether this is a goodrepresentation of the data open, and simply use the data as provided by AndrewMaas. The data is provided in two different folders: one for the training data and one forThe training data is in the training folder, and another for the test data in the test folder. The test folder has two sub‐ There is a helper function in scikit-learn to load files stored in a folder structure. Each sub folder corresponds to a label, called feature vectors. The first string (\"The fool doth think he is wise,\") is repre‐centricsented as the first row in the file. It contains the first word in the vocabulary, \"be\", zero times. It also contains the second word, \"but,\" zero times, and so on. Looking at both rows, we can see that the words \"fool\", \"the\", and \"wise\" appear in both strings. The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that the vocabulary contains 74,000 entries. A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself. The data is stored as a SciPy sparse matrix of type 'numpy.int64' with 3431196 stored elements in Compressed Sparse Row format. The training and test data are stored as lists of strings ( There are two main approaches: using a language-specific list of stopwords, or discarding words that appear too frequently. Let’s look at the vocabulary in a bit more detail. First 20 features: are too frequent to be informative. The number of features: 74849. The total number of words in the vocabulary: 2,856. The vocabulary is written as a list of words, with each Scikit-learn has a built-in list of English stopwords in the feature_extraction.text module. The number of stop words in the list can only decrease the length of the list, but it might lead to an improvement in perfor­cemicmance. The list of stopwords is: 318. Every 10th stopword: 'above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough', 'herein', There are now 305 (27,271–26,966) fewer features in the dataset. Most, but not all, of the stopwords appeared. Let’s run the grid search again:GridSearchCV(LogisticRegression() param_grid, cv=5) grid.fit(X_train, y_train) best_score_: 0.88 grid.best_score: 1.0 grid.grid.fit() best-score: 2.2 grid.last_score : 1.5 grid.first_score = 1.1 grid.next_ The tf–idf score for word w in document d is given by the Tfidf transformer and vectorizer classes. Both classes rescale the representation of each document to have Euclidean norm 1. They also apply L2 normalization after computing the TF–idF representation. There are several variants of the TF-idf rescaling scheme, which you can read about on Wikipedia. For example, there is a scheme that rescales the word w into the word frequency of the document it is in. This scheme can be used to transform or encode documents in the same way. The grid search code is based on a pipeline, as described in Chapter 6. Rescaling in this way means that the length of a document (the number of words) does not change the vectorized repre‐ceivesentation. We can also inspect which words tf–idf found most important by inspecting the best cross-validation score. The code for the grid search can be found in Chapter 7: Working with Text Data. For the rest of the article, we'll look at how we can use the pipeline to search for words in a text file using the Tfidf vectorizer. There are particularly interesting features containing the word “worth” that were not present in the unigram model. Many of the useful bigrams and trigrams consist of common words that would not be informative on their own. Keep in mind that the tf–idf scaling is meant to find words that distinguish documents, but it is also a rescaling tool that can be used to find useful words and phrases. Next, we’ll visualize only trigrams, to provide further insight into why these features are helpful, and why they are used in the way they are. The impact of these features is quite limited compared to the importance of the unigramfeatures. One particular step that is often improved in more sophisticated text-processing applica­tions is the first step in the bag-of-words model: tokenization. Figure 7-5. Visualization of only the important trigram features of the model.Advanced Tokenization, Stemming, and Lemmatization are possible in more advanced text- processing applications. The Tfidf Vectorizer and CountVectorizer are relatively simple, and much more elaborate methods are possible The vocabulary often contains singular and plural versions of some words. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only be possible with bigram and trigram features. We improved performance by a bit more than a per‐centric by adding bigrams and trigrams to the model. The results are shown in the figure below. The model is called Bag- of-Words with More Than One Word. Using bigrams increases performance quite a bit, while adding trigrams only provides a very small benefit in terms of accuracy. We can visualize the cross-validation.accuracy as a function of the ngram_range and C parameter as a heat map, as we did. in Chapter 5 (see Figure 7-3):In[33]:# extract scores from grid_search. # visualize heat map                heatmap = mglearn.tools.heatmap(20, There are particularly interesting features containing the word “worth” that were not present in the unigram model. This is a detail later in this chapter. For now, let’s look at how we can apply the bag-of-wordsprocessing using scikit-learn. We use unigrams, bigrams, and trigrams withtf-idf rescaling. We can visualize the important coeffi‐342342                   “Not worth” is indicative of a negative review, while \"definitely worth\" and “well worth’ are indicative of The bag-of-words representation is implemented in CountVectorizer, which is atransformer. The output is one vector of word counts for each docu‐reprehensivement. For each word in the vocabulary, we have a count of how often it appears in each document. That means our numeric representation has one feature for each                unique word in a dataset. Figure 7-1 illustrates the process on the string \"This is how you get ants\" The order of the words in the original string is completely irrelevant The vocabulary consists of 13 words, from \"be\" to \"wise\" The bag-of-words representation for the training data is a sparse matrix of type '<class 'numpy.int64'>' \"refreshing\" indicates positive movie reviews. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows. We can access the vocabulary_ attribute as well as the bag_of_words representation. The vocabulary size is set to 13, and the bag of words representation is a 2x13 sparse matrix. We call the transform method to create the bag- of-words Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong and at most one token long. The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. This is a standard                binary classification task. We will use the toy data we used earlier to test our theory. We are going to use the data from the test data to test the theory. The test data is a set of 25,000 documents called unigrams. We’ll use the unigram data to show that we are working with singletokens, which are also called ‘unig Representing Text Data as a Bag of Words is one of the most simple but effective ways to represent text for machine learning. We discard most of the structure of the input text, and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”Computing the bag-of-words representation for a corpus of documents consists of the following three steps: Tokenization. The first step is to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. The second step is the second step, and the third is the third. The bag-of-words algorithm is used to build a vocabulary of all words that appear in a document. The output is one vector of word counts for each docu‐insuredsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and 'drawers\" The process is similar to the one used to create the string \"This is how you get ants\" in Figure 7-1. We will discuss the subtleties involved in step 1 and step 2 in more detail later in this chapter. For now, let’s look at how we can apply the process to other documents. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting. This problem can be overcome by representing each word using its word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. It is usually not possible to generalize to all words that have the same word stem, so the stem is used instead to represent words with different verb forms and nouns. For example, the stem of the word \"replaced\" is different to the verb \"to replace\" If a dictionary of known word forms is used, and the role of the word in the sentence is taken into account, the process is referred to as lemmatization and the standardized form ofthe word is known as the lemma. Another interesting case of normalization is spelling correction, which can behelpful in practice but is outside of the scope of this book. For details of the interface, consult the nltk and spacy documentation. For more information on how to work with Text Data, visit the Text Data section of To get a better understanding of normalization, let’s compare a method for stemming with lemmatization. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting, and not allow the model to fully exploit the training data. The Porter stemmer is a widely used collection of heuristics. We are more interested in the general This problem can be overcome by representing each word using its word stem, whichinvolves identifying (or conflating) all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually called stemming. If instead a dictionary of known word forms is used, and the role of the word in the sentence is taken into account, the process is referred to as lemmatization and the standardized form ofthe word is known as the lemma. We found the vocabulary includes words like \"replace\", \"replaced\" and \"replaces\", which are different verb forms and a nounrelating to the verb “to replace” Lemmatization and stemming are forms of normalization that try to extract some normal form of a word. Lemmatization is a form of spelling correction. The Porter stemmer is a widely used collection of heuristics. For details of the interface, consult the nltk and spacy documentation. We are more interested in the general principles of lemmatization than in the specific details of each method. We will use the spacy package to work with text data in this book. We hope that this will help you with your own text-heavy work. Back to the page you came from. Click here to read the next section, \"Working with Text Data\" While scikit-learn implements neither form of normalization, CountVectorizerallows specifying your own tokenizer. We can use the lemmatization from spacy to create a callable that will take a string and produce a list of lemmas. We want to use the regexp-based tokenizer# that is used by Count Vectorizer and only use the lmmatized version of spacy. In general, lemm atization is a much more involved process We will compare lemmatization and the Porter stemmer on a sentence designed to show some of the differences. To get a better understanding of normalization, let’s compare a method for stemming—the Porterstemmer, a widely used collection of heuristics. To this end, we replace en_nlp.tokenizer (the spacy tokenizer) with the regexp-based tokenization. We will also use a count vectorizer with the custom tokenizer to transform the data and inspect the vocabulary size. We’ll end with a simple example of how to use the spacy language model in a real-world situation. We'll call the function compare_normalization in spacy with stemming Stemming reduces both occurrences of \"meeting\" to \"meet\" Using lemmatization, the first occurrence of \"Meeting\" is recognized as a noun and left as is. The second occurrence is recognition as a verb and reduced.",
                    "children": [
                        {
                            "id": "chapter-7-section-2-subsection-1",
                            "title": "Representing Text Data as a Bag of Words",
                            "content": "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\ngories that will capture responses in a way that makes sense for your application. You\nmight then have some categories for standard colors, maybe a category “multicol‐\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\ntion where you can influence data collection, we highly recommend avoiding man‐\nually entered values for concepts that are better captured using categorical variables.\nOften, manually entered values do not correspond to fixed categories, but still have\nsome underlying structure, like addresses, names of places or people, dates, telephone\n324 \n| \nChapter 7: Working with Text Data\n1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\nthemselves.\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\ntheir treatment is highly dependent on context and domain. A systematic treatment\nof these cases is beyond the scope of this book.\nThe final category of string data is freeform text data that consists of phrases or sen‐\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nGrid-Searching Which Model To Use                                                                        319\nSummary and Outlook                                                                                                 320\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\nTypes of Data Represented as Strings                                                                         323\nTable of Contents \n| \nv\nExample Application: Sentiment Analysis of Movie Reviews                                 325\nRepresenting Text Data as a Bag of Words                                                                 327\nApplying Bag-of-Words to a Toy Dataset                                                               329\nBag-of-Words for Movie Reviews                                                                            330\nStopwords                                                                                                                       334\nRescaling the Data with tf–idf                                                                                      336\nInvestigating Model Coefficients                                                                                 338\nBag-of-Words with More Than One Word (n-Grams)                                            339\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called",
                            "summary": "The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. The text data is not in a format that a machine learning model can handle. We need to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. We discard most of the structure of the input text, like chapters, paragraphs, andsentences, and only count how often each word appears in each text inRepresenting Text Data as a Bag of Words. The result is a simple but effective and commonly used way to represent text in the form of a bag-of-words representation. Computing the bag-of-words representation for a corpus of documents consists of the following three steps. Split each document into the words that appear in it (called tokens), splitting them on whitespace and punctuation. Collect a vocabulary of all words that appears in any of the documents, and number them (say, in alphabetical order). Encoding. For each document, count how often each of the words in the vocabu‐ proprietarylary appear in this document. The output is one vector of word counts for each docu‐ detail later in this chapter.. Figure 7-1 illustrates the process on the string \"This is how you get ants\" using scikit-learn. The bag-of-words representation is implemented in CountVectorizer, which is atransformer. The output is one vector of word counts for each docu‐paralleledment. For each word in the vocabulary, we have a count of how often it appears in each document. For now, let’s look at how we can apply the bag- of-words processing using scikit-learn. The process on the string \"This                is how you get ants\" is shown in Figure 7-1. Note how the order of the words in the original string is completely irrelevant The vocabulary consists of 13 words, from \"be\" to \"wise\" The bag-of-words representation for the training data is a sparse matrix of type '<class 'numpy.int64'>' \"refreshing\" indicates positive movie reviews. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows. We can access the vocabulary_ attribute as well as the bag_of_words representation. The vocabulary size is set to 13, and the bag of words representation is a 2x13 sparse matrix. We call the transform method to create the bag- of-words Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong. The bag-of-words representation is stored in a SciPy sparse matrix that only stores entries that are nonzero. Here is an example on the toy data we used earlier: grotesquelyprint(\"bards_words:\\n{}\".format(bard_words) grotesquely print(\"Vocabulary:\\n {}\". format(cv.get_feature_names())) grotesquelyPrint(\"Bard's words: {}\".format(\"bard\", \"vocabulary\", \"bard\" }); grotesquely Print('Bard', \"v vocabulary\", \"bag_of_ A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary, meaning most entries in the feature array are 0. This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this would result in a MemoryError. Think about how many different words might appear in a movie review compared to all the Words in the English language (which is what the vocabulary models are based on) This is how we try to represent text data as a bag of words. The matrix is of shape 2×13, with one row for each of the two data points and one We can see that the word counts for each word are either 0 or 1. The first string (\"The fool doth think he is wise,\") is repre‐represented as the first row in the sparse matrix. It also contains the second word in the vocabulary, \"but\", zero times. Neither of the twostrings in bards_words contains a word twice. Let’s take a look at how to read these feature vectors using the toarray method. To look at the actual con‐                tent of the sparseMatrix, we can convert it to a “dense” NumPy array. The toarray() method also stores all the 0 entries. The fourth word, \"fool\", the tenth word, 'the', and the thirteenth word 'wise' all appear in both strings. The second word in the vocabulary, 'but', zero times. The third word 'doth', once, and so on. A quick analysis of the data confirms that this is indeed the case. The training and test data from the IMDb reviews are turned into lists of strings (text_train and text_test), which we will now process. The data is stored in Compressed Sparse Row format (CSPR) with 3431196 stored elements in type 'numpy.int64' The training data is turned into a sparse matrix of type The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that the vocabulary contains 74,000 entries. Let’s look at the vocabulary in a bit more detail. The data is stored as a SciPy sparse matrix. The first 20 features are called by the get_feature_name method of the vectorizer. Each entry corresponds to one fea‐repertoire. The vocabulary is stored in a form that makes sense for your strings can take a lot of manual effort and is not easily automated. Youmight then have some categories for standard colors, maybe a category “multicolored” for people that gave answers like “green and red stripes,” and an The content of websites linked to in tweets contains more information than the text of the tweets themselves. The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/. The techniques used in this chapter also apply to other languages that use the Romanalphabet, and partially to languages with word boundary delimiters. If you are in a posi­tion where you can influence data collection, we highly recommend avoiding man­ually entered values for concepts that are better captured using categorical variables. The data is available on http://aas.Stanford.University.edu.    The datasets are available on the Stanford University website. The final category of string data is freeform text data that consists of phrases or sen‐tences. Examples include tweets, chat logs, and hotel reviews. These kinds of strings are often very hard to parse, and their treatment is highly dependent on context and domain. A systematic treatmentof these cases is beyond the scope of this book. Accessing Attributes in a Grid-Searched Pipeline. Grid-Searching Which Model To Use. Preprocessing Steps and Model parameters. Working with Text Data. Using Text Data with a Grid Search. Using Grid Search with a grid search with a text search. Using a grid-search with text data. Using text data with aGrid Search. using text data to search for attributes in a pipeline. Using the Grid Search tool to search a pipeline for attributes. using the Grid search tool to The book is a collection of 50,000 ebooks, works of Shakespeare, Shakespeare, the content of Wikipedia, and content of e-books. The book also includes a selection of the most popular e-book titles of all time, including the works of William Shakespeare. It is available in English, French, Spanish, German, Italian, Spanish and Italian. It was published in the U.S. in 1998 and has been updated In this chapter, we will use a dataset of movie reviews from the IMDb (Internet Movie Database) website. This dataset contains the text of the reviews, together with a label that indi‐phthalcates whether a review is “positive” or “negative.” The IMDb website itself contains reviews from 1 to 10. We will use this dataset as a running example in the next chapter of this book. The next chapter will focus on the analysis of video clips. The final chapter will look at the analysis and analysis of music videos. It will conclude with a discussion of how to use this data to understand music videos in a new way. The data is provided as text files in two separate folders, one for training data and one for test data. The pos folder contains all the positive reviews, each as a separate text file, and simi‐larly for the neg folder. There is a helper function in scikit-learn to load files stored in such a folder structure, where each sub folder corresponds to a label. To simplify the modeling, this annotation is summarized as a two-class classification dataset where reviews with a score of 6 or higher are labeled as                positive, and the rest as negative. We will leave the question of whether this is a goodrepresentation of the data open, and simply use the data as provided by Andrew                Maas.",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-2",
                            "title": "Applying Bag-of-Words to a Toy Dataset",
                            "content": "detail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\nBag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only",
                            "summary": "The bag-of-words representation is implemented in CountVectorizer, which is a transformer. The output is one vector of word counts for each docu‐paralleledment. For each word in the vocabulary, we have a count of how often it appears in each document. That means our numeric representation has one feature for each                unique word in. the whole dataset. detail later in this chapter. For now, let’s look at how we can apply the bag- of-words.processing using scikit-learn. Figure 7-1 illustrates the process on the string \"This                is how The vocabulary consists of 13 words, from \"be\" to \"wise\". The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows:.import Count Vectorizer from sklearn.feature_extraction.text import vocabulary_ from sklearning.feature.text. import bag_of_words from Sklearn. FeatureExtraction. text. import text_test from Sklearning. Feature Extraction. Text. Representing Text Data as a Bag of Words is a standard binary classification task. We discard most of the structure of the input text, and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”Computing the bag-of-words representation for a corpus of documents consists of the following three steps:1. Tokenization. 2. Generation of a string representation. 3. The final step is to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. Back to Mail Online home. Back To the page you came from. The bag-of-words process is a way to extract words from documents. The output is one vector of word counts for each doc. Figure 7-1 illustrates the process on the string \"This is how you get ants.\" The output of the process is the word count for each document. The process can be applied to other documents, such as books and movies. For now, let’s look at how we can apply the process to documents using scikit-learn. We will discuss the subtleties involved in step 1 and step 2 in more detail later in this chapter. Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong. The bag-of-words representation is stored in a SciPy sparse matrix that only stores entries that are nonzero. Here is an example on the toy data we used earlier: grotesquelyprint(\"bards_words:\\n{}\".format(bard_words) grotesquely print(\"Vocabulary:\\n {}\". format(cv.get_feature_names())) grotesquelyPrint(\"Bard's words: {}\".format(\"bard\", \"vocabulary\", \"bard\" }); grotesquely Print('Bard', \"v vocabulary\", \"bag_of_ A sparse matrix is used as most documents only contain a small subset of the words in the vocabulary, meaning most entries in the feature array are 0. This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this would result in a MemoryError. Think about how many different words might appear in a movie review compared to all the Words in the English language (which is what the vocabulary models are based on) This is how we try to represent text data as a bag of words. The matrix is of shape 2×13, with one row for each of the two data points and one We can see that the word counts for each word are either 0 or 1. The first string (\"The fool doth think he is wise,\") is repre‐represented as the first row in the sparse matrix. It also contains the second word in the vocabulary, \"but\", zero times. Neither of the twostrings in bards_words contains a word twice. Let’s take a look at how to read these feature vectors using the toarray method. To look at the actual con‐                tent of the sparseMatrix, we can convert it to a “dense” NumPy array. The toarray() method also stores all the 0 entries. The fourth word, \"fool\", the tenth word, 'the', and the thirteenth word 'wise' all appear in both strings. The second word in the vocabulary, 'but', zero times. The third word 'doth', once, and so on. A quick analysis of the data confirms that this is indeed the case. The training and test data from the IMDb reviews are turned into lists of strings (text_train and text_test), which we will now process. The data is stored in Compressed Sparse Row format (CSPR) with 3431196 stored elements in type 'numpy.int64' The training data is turned into a sparse matrix of type The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that The vocabulary can be accessed using the get_feature_name method of the vect. Each entry in the vocabulary corresponds to one fea‐ture. We improved performance by a bit more than a per‐centric by adding bigram and trigram features. Let’s look at the vocabulary in a little more detail. We’ll start with the bag of words with more than one word (n-Grams) and then move on to the trigram and bigram vocabulary. Using bigrams increases performance quite a bit, while adding trigrams only provides a very small benefit in terms of accuracy. We can visualize the cross-validation.accuracy as a function of the ngram_range and C parameter as a heat map, as we did. in Chapter 5 (see Figure 7-3):In[33]:# extract scores from grid_search. # visualize heat map                heatmap = mglearn.tools.heatmap(20, The best model includes unigrams, bigrams, and trigrams. To understand better how the model improved, we can visualize the important coeffi‐342. The word “worth” was not present in the unigram model. \"not worth” is indicative of a negative review, while \"definitely worth\" and \"well worth\" are indicative ofa positive review. This is a good example of how to use the Text Datacient model in a real-world situation. The best model can be found at: http://www.textdatacient.org For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting. This problem can be overcome by representing each word using its word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. It is usually not possible to generalize to all words that have the same word stem, so the stem is used instead to represent words with different verb forms and nouns. For example, the stem of the word \"replaced\" is different to the verb \"to replace\" If a dictionary of known word forms is used, and the role of the word in the sentence is taken into account, the process is referred to as lemmatization and the standardized form ofthe word is known as the lemma. Another interesting case of normalization is spelling correction, which can behelpful in practice but is outside of the scope of this book. For details of the interface, consult the nltk and spacy documentation. For more information on how to work with Text Data, visit the Text Data section of Let’s compare a method for stemming with lemmatization as implemented in the spacy package. Many of the useful bigrams and trigrams consist of common words that would not be informative on their own, such as \"none of the\", \"the only good\", \"on and on\", \"this is one\", \"of the most\", and so on. We’ll visualize only trigrams, to provide further insight into why these features are helpful. We are more interested in the general principles of lemmats than in the specific details of stemming and trigram normalization. We hope this will give us a better understanding of how the heuristics are used. The impact of these features is quite limited compared to the importance of the unigramfeatures. One particular step that is often improved in more sophisticated text-processing applica­tions is the first step in the bag-of-words model: tokenization. Figure 7-5. Visualization of only the important trigram features of the model.Advanced Tokenization, Stemming, and Lemmatization are possible in more advanced text- processing applications. The Tfidf Vectorizer and CountVectorizer are relatively simple, and much more elaborate methods are possible We saw earlier that the vocabulary often contains singular and plural versions of                some words. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only. only.",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-3",
                            "title": "Bag-of-Words for Movie Reviews",
                            "content": "\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:",
                            "summary": "Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams, trigrams, and n-grams. \"refreshing\" indicate positive movie reviews. \"bit\", \"job\", and \"today\" might be part of phrases like “good job” or “best                today.” “Not” in front of a word is only one example (if an extreme) of how context matters. “It’s bad, not good at all” has exactly the same representation, even though the mean‐phthalings are We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong and at most one token long. Here is an example on the toy data we used earlier: \"bards_words\": \"fool doth think he is wise, but the wise man knows himself to be a fool\" \"Vocabulary\": \"vocabulary: works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection of 50,000 ebooks\" In this chapter, we will use a dataset of movie reviews from the IMDb (Internet Movie Database) website. This dataset contains the text of the reviews, together with a label that indi‐phthalcates whether a review is “positive” or “negative.” The IMDb website itself contains reviews from 1 to 10. We will use this dataset as a running example in the next chapter of this book. The next chapter will focus on the analysis of video clips. The final chapter will look at the analysis and analysis of music videos. It will conclude with a discussion of how to use this data to understand music videos in a new way. The dataset is provided as text files in two separate folders. The pos folder contains all the positive reviews, each as a separate text file. The neg folder contains the negative reviews. To simplify the modeling, this annotation is summarized as a two-class classification dataset where reviews with a score of 6 or higher are labeled as                positive, and the rest as negative. We will leave the question of whether this is a goodrepresentation of the data open, and simply use the data as provided by AndrewMaas. The data is provided in two different folders: one for the training data and one forThe training data is in the training folder, and another for the test data in the test folder. The test folder has two sub‐ There is a helper function in scikit-learn to load files stored in a folder structure. Each sub folder corresponds to a label, called feature vectors. The first string (\"The fool doth think he is wise,\") is repre‐centricsented as the first row in the file. It contains the first word in the vocabulary, \"be\", zero times. It also contains the second word, \"but,\" zero times, and so on. Looking at both rows, we can see that the words \"fool\", \"the\", and \"wise\" appear in both strings. The shape of X_train, the bag-of-words representation of the training data, is 25,000×74,849, indicating that the vocabulary contains 74,000 entries. A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself. The data is stored as a SciPy sparse matrix of type 'numpy.int64' with 3431196 stored elements in Compressed Sparse Row format. The training and test data are stored as lists of strings ( Another way to access the vocabulary is using the get_feature_name method of the vect. This returns a convenient list where each entry corresponds to one fea‐ture. Let’s look at the vocabulary in a bit more detail.",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-4",
                            "title": "Stopwords",
                            "content": "are too frequent to be informative. There are two main approaches: using a language-\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\nlearn has a built-in list of English stopwords in the feature_extraction.text\nmodule:\n334 \n| \nChapter 7: Working with Text Data\nIn[20]:\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\nOut[20]:\nNumber of stop words: 318\nEvery 10th stopword:\n['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\nClearly, removing the stopwords in the list can only decrease the number of features\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\nmance. Let’s give it a try:\nIn[21]:\n# Specifying stop_words=\"english\" uses the built-in list.\n# We could also augment it and pass our own.\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with stop words:\\n{}\".format(repr(X_train)))\nOut[21]:\nX_train with stop words:\n<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n    with 2149958 stored elements in Compressed Sparse Row format>\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\nIn[22]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[22]:\nBest cross-validation score: 0.88\nThe grid search performance decreased slightly using the stopwords—not enough to",
                            "summary": "Use a language-specific list of stopwords, or discarding words that appear too frequently. are too frequent Scikit-learn has a built-in list of English stopwords in the feature_extraction.text module. The number of stop words in the list can only decrease the length of the list, but it might lead to an improvement in perfor­cemicmance. The list of stopwords is: 318. Every 10th stopword: 'above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough', 'herein', There are now 305 (27,271–26,966) fewer features in the dataset. Most, but not all, of the stopwords appeared. Let’s run the grid search again with stopwords added. The grid search performance decreased slightly using stopwords. But not enough to.    with 2149958 stored elements in Compressed Sparse Row format. The result was a score of 0.88 out of a possible 1,000. The next step is to use the built-in list of",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-5",
                            "title": "Rescaling the Data with tf–idf",
                            "content": "which takes in the text data and does both the bag-of-words feature extraction and\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\nwhich you can read about on Wikipedia. The tf–idf score for word w in document d\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\nby:7\ntfidf w, d = tf log\nN + 1\nNw + 1 + 1\nwhere N is the number of documents in the training set, Nw is the number of docu‐\nments in the training set that the word w appears in, and tf (the term frequency) is the\nnumber of times that the word w appears in the query document d (the document\nyou want to transform or encode). Both classes also apply L2 normalization after\ncomputing the tf–idf representation; in other words, they rescale the representation\nof each document to have Euclidean norm 1. Rescaling in this way means that the\nlength of a document (the number of words) does not change the vectorized repre‐\nsentation.\nBecause tf–idf actually makes use of the statistical properties of the training data, we\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\nare valid. This leads to the following code:\n336 \n| \nChapter 7: Working with Text Data\nIn[23]:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[23]:\nBest cross-validation score: 0.89\nAs you can see, there is some improvement when using tf–idf instead of just word\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only",
                            "summary": "There are several variants of the tf–idf rescaling scheme, which you can read about on Wikipedia. Both classes also apply L2 normalization after computations. They rescale the representation of each document to have Euclidean norm 1. The TF–idF score for word w in document d is implemented in both the TfidfTransformer and Tf idf vectorizer classes. It is given by:7 grotesquetfidf w, d = tf log grotesqueN + 1 grotesqueNw + 1 + 1 cognitively. The term frequency is thenumber of times that the word w appears in the query document d (the document you want to transform or encode) The grid search code is based on a pipeline, as described in Chapter 6. Rescaling in this way means that the length of a document (the number of words) does not change the vectorized repre‐ceivesentation. We can also inspect which words tf–idf found most important by inspecting the best cross-validation score. The code for the grid search can be found in Chapter 7: Working with Text Data. For the rest of the article, we'll look at how we can use the pipeline to search for words in a text file using the Tfidf vectorizer. There are particularly interesting features containing the word “worth” that were not present in the unigram model. Many of the useful bigrams and trigrams consist of common words that would not be informative on their own. Keep in mind that the tf–idf scaling is meant to find words that distinguish documents, but it is also a rescaling tool that can be used to find useful words and phrases. Next, we’ll visualize only trigrams, to provide further insight into why these features are helpful, and why they are used in the way they are. The impact of these features is quite limited compared to the importance of the unigramfeatures. One particular step that is often improved in more sophisticated text-processing applica­tions is the first step in the bag-of-words model: tokenization. Figure 7-5. Visualization of only the important trigram features of the model.Advanced Tokenization, Stemming, and Lemmatization are possible in more advanced text- processing applications. The Tfidf Vectorizer and CountVectorizer are relatively simple, and much more elaborate methods are possible We saw earlier that the vocabulary often contains singular and plural versions of                some words. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will only. only.",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-6",
                            "title": "Investigating Model Coefficients",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-7",
                            "title": "Bag-of-Words with More Than One Word (n-Grams)",
                            "content": "Bag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk",
                            "summary": "As you can see from the results, we improved performance by a bit more than a per-cent by adding bigram and trigram features. We can visualize the cross-validation accuracy as a function of the ngram_range and C parameter as a heat map, as we did in Chapter 5. Figure 7-3 shows the heat map for the Bag-of-Words with More Than One Word (n-Grams) Using bigrams increases performance quite a bit, while adding trigrams only provides a very small benefit in terms of accuracy. To understand better how the model improved, we can visualize the important coeffi‐342                  “features’” in the heat map. The heat map visualization of mean cross-validation accuracy as a function of the parameters ngram_range and C is shown in Figure 7-4. There are particularly interesting features containing the word “worth” that were not present in the unigram model. This is a detail later in this chapter. For now, let’s look at how we can apply the bag-of-words processing using scikit-learn. The output is one vector of word counts for each docu‐phthalment. For each word in the vocabulary, we have a count of how often it appears in                each document. That means our numeric representation has one feature for each                uniqueword in the whole dataset. Figure 7-1 illustrates the process on the string \"This is how you get ants.\" The order of the words in the original string is completely irrelevant to the The bag-of-words representation is implemented in CountVectorizer, which is a transformer. The vocabulary consists of 13 words, from \"be\" to \"wise\" The bag-of-words representation for the training data is a sparse matrix of type '<class 'numpy.int64'>' \"refreshing\" indicates positive movie reviews. Let’s first apply it to a toy dataset, consisting of two samples, to see it working. We import and instantiate the CountVectorizer and fit it to our toy data as follows. We can access the vocabulary_ attribute as well as the bag_of_words representation. The vocabulary size is set to 13, and the bag of words representation is a 2x13 sparse matrix. We call the transform method to create the bag- of-words Bag-of-Words with More Than One Word (n-Grams) is a way of capturing context. The counts of pairs or triplets of tokens that appear next to each other are known as bigrams and trigrams, and generally sequences of tokens are called n-grams. Some words are slightly less clear, like \"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “besttoday.” The two strings “it’s bad, not good at all” and “It’’ have exactly the same representation, even though the mean‐centricings are inverted. We can change the range of tokens that are considered as features by changing the ngram_range parameter of the CountVectorizer or TfidfVectorizer. The ngram range parameter is a tuple, con‐                Bag- The default is to create one feature per sequence of tokens that is at least one tokenlong and at most one token long. The task we want to solve is to assign the label “positive” or “negative” based on the text content of the review. This is a standard                binary classification task. We will use the toy data we used earlier to test our theory. We are going to use the data from the test data to test the theory. The test data is a set of 25,000 documents called unigrams. We’ll use the unigram data to show that we are working with singletokens, which are also called ‘unig Representing Text Data as a Bag of Words is one of the most simple but effective ways to represent text for machine learning. We discard most of the structure of the input text, and only count how often each word appears in each text. Discarding the structure and counting only word occurrences leads to the mental image of representing text as a “bag.”Computing the bag-of-words representation for a corpus of documents consists of the following three steps: Tokenization. The first step is to convert the string representation of the text into a numeric representation that we can apply our machine learning algorithms to. The second step is the second step, and the third is the third. The bag-of-words algorithm is used to build a vocabulary of all words that appear in a document. The output is one vector of word counts for each docu‐insuredsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and 'drawers\" The process is similar to the one used to create the string \"This is how you get ants\" in Figure 7-1. We will discuss the subtleties involved in step 1 and step 2 in more detail later in this chapter. For now, let’s look at how we can apply the process to other documents. For the purposes of a bag-of-words model, the semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting. This problem can be overcome by representing each word using its word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. It is usually not possible to generalize to all words that have the same word stem, so the stem is used instead to represent words with different verb forms and nouns. For example, the stem of the word \"replaced\" is different to the verb \"to replace\" Lemmatization and stemming are forms of normalization that try to extract some normal form of a word. The Porter stemmer is a widely used collection of heuristics. For details of the interface, consult the nltk and spacy documentation. We are more interested in the general principles here. The spacy package can be downloaded from the following URL: http://www.nltk.org/nltK/nlsk.php?language=nlsv&language=Nlsv and the nlsv package is available on the following website:http://nlsj.org/. The package can also be downloaded as a ZIP file from the same URL: http://www",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-2-subsection-8",
                            "title": "Advanced Tokenization, Stemming, and Lemmatization",
                            "content": "some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\nming, but it usually produces better results than stemming when used for normaliz‐\ning tokens for machine learning.\nWhile scikit-learn implements neither form of normalization, CountVectorizer\nallows specifying your own tokenizer to convert each document into a list of tokens\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\ncallable that will take a string and produce a list of lemmas:\nIn[38]:\n# Technicality: we want to use the regexp-based tokenizer\n# that is used by CountVectorizer and only use the lemmatization\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n# with the regexp-based tokenization.\nimport re\n# regexp used in CountVectorizer\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n# load spacy language model and save old tokenizer\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n# replace the tokenizer with the preceding regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n    regexp.findall(string))\n# create a custom tokenizer using the spacy document processing pipeline\n# (now using our own tokenizer)\ndef custom_tokenizer(document):\n    doc_spacy = en_nlp(document, entity=False, parse=False)\n    return [token.lemma_ for token in doc_spacy]\n# define a count vectorizer with the custom tokenizer\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\nLet’s transform the data and inspect the vocabulary size:\nIn[39]:\n# transform text_train using CountVectorizer with lemmatization\nX_train_lemma = lemma_vect.fit_transform(text_train)\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n# standard CountVectorizer for reference\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n# load spacy's English-language models\nen_nlp = spacy.load('en')\n# instantiate nltk's Porter stemmer\nstemmer = nltk.stem.PorterStemmer()\n# define function to compare lemmatization in spacy with stemming in nltk\ndef compare_normalization(doc):\n    # tokenize document in spacy\n    doc_spacy = en_nlp(doc)\n    # print lemmas found by spacy\n    print(\"Lemmatization:\")\n    print([token.lemma_ for token in doc_spacy])\n    # print tokens found by Porter stemmer\n    print(\"Stemming:\")\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\nshow some of the differences:\nIn[37]:\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\n                       \"I'm scared of meeting the clients tomorrow.\")\nOut[37]:\nLemmatization:\n['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming:\n['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced",
                            "summary": "The semantics of \"drawback\" and \"drawbacks\" are so close that distinguishing them will onlyincrease overfitting, and not allow the model to fully exploit the training data. The problem can be overcome by representing each word using its word stem, which involves identifying (or conflating) all the words that have the same word stem. If this is done by using a rule-based heuristic, like dropping common suffixes, it is usually referred to as stemming. We found the vocabulary includes words like \"replace\", \"replaced\", \"replacement\", \" replaces\", and \"replacing\", which are different verb forms and a nounrelating to the verb “to replace” Lemmatization and stemming are forms of normalization that try to extract some normal form of a word. The Porter stemmer is a widely used collection of heuristics. For details of the interface, consult the nltk and spacy documentation. We are more interested in the general guidelines here. For example, let’s compare a method for stemming to lemmatization as implemented in the spacy package:8.8:import spacy, \"meet\", then \"lemma\", then lemma, then \"meeting\" and so on. For more details on how spacy works, see spacy.com/nltk. The book also includes a guide to working with text Using lemmatization, the first occurrence of \"meeting\" is recognized as a                Advanced Tokenization, Stemming, and Lemmatization | 345345noun and left as is. In general, lemm atization is a much more involved process than stem‐ worrisomeming. It usually produces better results than stemming when used for normaliz­ing tokens for machine learning. We can use the le mmatization from spacy to create a callable that will take a string and produce a list of lemmas. We want to use the regexp-based tokenizer# that is used by CountVectorizer and only use the We will compare lemmatization and the Porter stemmer on a sentence designed to show some of the differences. To get a better understanding of normalization, let’s compare a method for stemming—the Porterstemmer, a widely used collection of heuristics. To this end, we replace en_nlp.tokenizer (the spacy tokenizer) with the regexp-based tokenization. We will also use a count vectorizer with the custom tokenizer to transform the data and inspect the vocabulary size. We’ll end with a simple example of how to use the spacy language model in a real-world situation. We'll call the function compare_normalization in spacy with stemming Stemming reduces both occurrences of \"meeting\" to \"meet\" Using lemmatization, the first occurrence of \"Meeting\" is recognized as a noun and left as is. The second occurrence is recognition as a verb and reduced.",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-7-section-3",
                    "title": "Topic Modeling and Document Clustering",
                    "content": "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\ntogether frequently. LDA also requires that each document can be understood as a\n“mixture” of a subset of the topics. It is important to understand that for the machine\nlearning model a “topic” might not be what we would normally call a topic in every‐\nday speech, but that it resembles more the components extracted by PCA or NMF\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\ning. Even if there is a semantic meaning for an LDA “topic”, it might not be some‐\nthing we’d usually call a topic. Going back to the example of news articles, we might\nhave a collection of articles about sports, politics, and finance, written by two specific\nauthors. In a politics article, we might expect to see words like “governor,” “vote,”\n“party,” etc., while in a sports article we might expect words like “team,” “score,” and\n“season.” Words in each of these groups will likely appear together, while it’s less likely\nthat, for example, “team” and “governor” will appear together. However, these are not\nthe only groups of words we might expect to appear together. The two reporters\nmight prefer different phrases or different choices of words. Maybe one of them likes\nto use the word “demarcate” and one likes the word “polarize.” Other “topics” would\nthen be “words often used by reporter A” and “words often used by reporter B,”\nthough these are not topics in the usual sense of the word.\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\nunsupervised text document models, it is often good to remove very common words,\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nax[col].set_xlim(0, 2000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()\nTopic Modeling and Document Clustering \n| \n353\nFigure 7-6. Topic weights learned by LDA\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\nlaudatory words.\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\nas most reviews are made up of some movie-specific comments and some comments\nthat justify or emphasize the rating.\nTopic models like LDA are interesting methods to understand large text corpora in\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\nrandomized, though, and changing the random_state parameter can lead to quite\n354 \n| \nChapter 7: Working with Text Data\ndifferent outcomes. While identifying topics can be helpful, any conclusions you\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\nommend verifying your intuition by looking at the documents in a specific topic. The\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\npact representation for supervised learning. This is particularly helpful when few\ntraining examples are available.\nSummary and Outlook\nIn this chapter we talked about the basics of processing text, also known as natural\nlanguage processing (NLP), with an example application classifying movie reviews.\nThe tools discussed here should serve as a great starting point when trying to process\ntext data. In particular for text classification tasks such as spam and fraud detection\nor sentiment analysis, bag-of-words representations provide a simple and powerful\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndirector      children      director      plays         horror\nminutes       10            played        jack          young\npretty        kid           performance   joe           goes\ndoesn         old           mr            performances  around\nJudging from the important words, topic 1 seems to be about historical and war mov‐\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\nseems to capture some very common words, while topic 6 appears to be about child‐\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\neach of the topics needs to be very broad, so that they can together cover all the dif‐\nferent kinds of reviews in our dataset.\nNext, we will learn another model, this time with 100 topics. Using more topics\nmakes the analysis much harder, but makes it more likely that topics can specialize to\ninteresting subsets of the data:\nIn[46]:\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n                                   max_iter=25, random_state=0)\ndocument_topics100 = lda100.fit_transform(X)\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\ning and representative topics:\n350 \n| \nChapter 7: Working with Text Data\nIn[47]:\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vect.get_feature_names())\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\nOut[48]:\ntopic 7       topic 16      topic 24      topic 25      topic 28\n--------      --------      --------      --------      --------\nOut[43]:\n(10, 10000)\nTo understand better what the different topics mean, we will look at the most impor‐\ntant words for each of the topics. The print_topics function provides a nice format‐\nting for these features:\nIn[44]:\n# For each topic (a row in the components_), sort the features (ascending)\n# Invert rows with [:, ::-1] to make sorting descending\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n# Get the feature names from the vectorizer\nfeature_names = np.array(vect.get_feature_names())\nIn[45]:\n# Print out the 10 topics:\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\nTopic Modeling and Document Clustering \n| \n349\nOut[45]:\ntopic 0       topic 1       topic 2       topic 3       topic 4\n--------      --------      --------      --------      --------\nbetween       war           funny         show          didn\nyoung         world         worst         series        saw\nfamily        us            comedy        episode       am\nreal          our           thing         tv            thought\nperformance   american      guy           episodes      years\nbeautiful     documentary   re            shows         book\nwork          history       stupid        season        watched\neach          new           actually      new           now\nboth          own           nothing       television    dvd\ndirector      point         want          years         got\ntopic 5       topic 6       topic 7       topic 8       topic 9\n--------      --------      --------      --------      --------\nhorror        kids          cast          performance   house\naction        action        role          role          woman\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndolph         10            laughs        flesh         felt\ncareer        give          fun           minutes       part\nsabrina       want          re            body          going\nrole          nothing       funniest      living        seemed\ntemple        terrible      laughing      eating        bit\nphantom       crap          joke          flick         found\njudy          must          few           budget        though\nmelissa       reviews       moments       head          nothing\nzorro         imdb          guy           gory          lot\ngets          director      unfunny       evil          saw\nbarbra        thing         times         shot          long\ncast          believe       laughed       low           interesting\nshort         am            comedies      fulci         few\nserial        actually      isn           re            half\nThe topics we extracted this time seem to be more specific, though many are hard to\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\nreviews of comedies. If we want to make further inferences using the topics that were\ndiscovered, we should confirm the intuition we gained from looking at the highest-\nranking words for each topic by looking at the documents that are assigned to these\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\nreviews are assigned to this topic:\nIn[49]:\n# sort by weight of \"music\" topic 45\nmusic = np.argsort(document_topics100[:, 45])[::-1]\n# print the five documents where the topic is most important\nfor i in music[:10]:\n    # pshow first two sentences\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\nOut[49]:\nb'I love this movie and never get tired of watching. The music in it is great.\\n'\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n  band from the 70's decide to give it another try.\\n\" Topic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors",
                    "summary": "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (o",
                    "children": [
                        {
                            "id": "chapter-7-section-3-subsection-1",
                            "title": "Latent Dirichlet Allocation",
                            "content": "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\ntogether frequently. LDA also requires that each document can be understood as a\n“mixture” of a subset of the topics. It is important to understand that for the machine\nlearning model a “topic” might not be what we would normally call a topic in every‐\nday speech, but that it resembles more the components extracted by PCA or NMF\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\ning. Even if there is a semantic meaning for an LDA “topic”, it might not be some‐\nthing we’d usually call a topic. Going back to the example of news articles, we might\nhave a collection of articles about sports, politics, and finance, written by two specific\nauthors. In a politics article, we might expect to see words like “governor,” “vote,”\n“party,” etc., while in a sports article we might expect words like “team,” “score,” and\n“season.” Words in each of these groups will likely appear together, while it’s less likely\nthat, for example, “team” and “governor” will appear together. However, these are not\nthe only groups of words we might expect to appear together. The two reporters\nmight prefer different phrases or different choices of words. Maybe one of them likes\nto use the word “demarcate” and one likes the word “polarize.” Other “topics” would\nthen be “words often used by reporter A” and “words often used by reporter B,”\nthough these are not topics in the usual sense of the word.\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\nunsupervised text document models, it is often good to remove very common words,\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nax[col].set_xlim(0, 2000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()\nTopic Modeling and Document Clustering \n| \n353\nFigure 7-6. Topic weights learned by LDA\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\nlaudatory words.\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\nas most reviews are made up of some movie-specific comments and some comments\nthat justify or emphasize the rating.\nTopic models like LDA are interesting methods to understand large text corpora in\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\nrandomized, though, and changing the random_state parameter can lead to quite\n354 \n| \nChapter 7: Working with Text Data\ndifferent outcomes. While identifying topics can be helpful, any conclusions you\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\nommend verifying your intuition by looking at the documents in a specific topic. The\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\npact representation for supervised learning. This is particularly helpful when few\ntraining examples are available.\nSummary and Outlook\nIn this chapter we talked about the basics of processing text, also known as natural\nlanguage processing (NLP), with an example application classifying movie reviews.\nThe tools discussed here should serve as a great starting point when trying to process\ntext data. In particular for text classification tasks such as spam and fraud detection\nor sentiment analysis, bag-of-words representations provide a simple and powerful\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndirector      children      director      plays         horror\nminutes       10            played        jack          young\npretty        kid           performance   joe           goes\ndoesn         old           mr            performances  around\nJudging from the important words, topic 1 seems to be about historical and war mov‐\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\nseems to capture some very common words, while topic 6 appears to be about child‐\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\neach of the topics needs to be very broad, so that they can together cover all the dif‐\nferent kinds of reviews in our dataset.\nNext, we will learn another model, this time with 100 topics. Using more topics\nmakes the analysis much harder, but makes it more likely that topics can specialize to\ninteresting subsets of the data:\nIn[46]:\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n                                   max_iter=25, random_state=0)\ndocument_topics100 = lda100.fit_transform(X)\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\ning and representative topics:\n350 \n| \nChapter 7: Working with Text Data\nIn[47]:\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vect.get_feature_names())\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\nOut[48]:\ntopic 7       topic 16      topic 24      topic 25      topic 28\n--------      --------      --------      --------      --------\nOut[43]:\n(10, 10000)\nTo understand better what the different topics mean, we will look at the most impor‐\ntant words for each of the topics. The print_topics function provides a nice format‐\nting for these features:\nIn[44]:\n# For each topic (a row in the components_), sort the features (ascending)\n# Invert rows with [:, ::-1] to make sorting descending\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n# Get the feature names from the vectorizer\nfeature_names = np.array(vect.get_feature_names())\nIn[45]:\n# Print out the 10 topics:\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\nTopic Modeling and Document Clustering \n| \n349\nOut[45]:\ntopic 0       topic 1       topic 2       topic 3       topic 4\n--------      --------      --------      --------      --------\nbetween       war           funny         show          didn\nyoung         world         worst         series        saw\nfamily        us            comedy        episode       am\nreal          our           thing         tv            thought\nperformance   american      guy           episodes      years\nbeautiful     documentary   re            shows         book\nwork          history       stupid        season        watched\neach          new           actually      new           now\nboth          own           nothing       television    dvd\ndirector      point         want          years         got\ntopic 5       topic 6       topic 7       topic 8       topic 9\n--------      --------      --------      --------      --------\nhorror        kids          cast          performance   house\naction        action        role          role          woman\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndolph         10            laughs        flesh         felt\ncareer        give          fun           minutes       part\nsabrina       want          re            body          going\nrole          nothing       funniest      living        seemed\ntemple        terrible      laughing      eating        bit\nphantom       crap          joke          flick         found\njudy          must          few           budget        though\nmelissa       reviews       moments       head          nothing\nzorro         imdb          guy           gory          lot\ngets          director      unfunny       evil          saw\nbarbra        thing         times         shot          long\ncast          believe       laughed       low           interesting\nshort         am            comedies      fulci         few\nserial        actually      isn           re            half\nThe topics we extracted this time seem to be more specific, though many are hard to\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\nreviews of comedies. If we want to make further inferences using the topics that were\ndiscovered, we should confirm the intuition we gained from looking at the highest-\nranking words for each topic by looking at the documents that are assigned to these\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\nreviews are assigned to this topic:\nIn[49]:\n# sort by weight of \"music\" topic 45\nmusic = np.argsort(document_topics100[:, 45])[::-1]\n# print the five documents where the topic is most important\nfor i in music[:10]:\n    # pshow first two sentences\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\nOut[49]:\nb'I love this movie and never get tired of watching. The music in it is great.\\n'\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n  band from the 70's decide to give it another try.\\n\"",
                            "summary": "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (o",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-3-subsection-2",
                            "title": "Document Clustering",
                            "content": "Topic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors",
                            "summary": "Topic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                  ",
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-3-subsection-3",
                            "title": "Evaluation Methods",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-7-section-4",
                    "title": "Summary and Outlook",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-7-section-4-subsection-1",
                            "title": "Chapter Review",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-4-subsection-2",
                            "title": "Advanced Concepts",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-7-section-4-subsection-3",
                            "title": "Future Directions",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-8",
            "title": "8. Wrapping Up",
            "content": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\nA Probabilistic Perspective by Kevin Murphy (MIT Press), a comprehensive (read:\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\nOther Machine Learning Frameworks and Packages\nWhile scikit-learn is our favorite package for machine learning1 and Python is our\nfavorite language for machine learning, there are many other options out there.\nDepending on your needs, Python and scikit-learn might not be the best fit for\nyour particular situation. Often using Python is great for trying out and evaluating\nmodels, but larger web services and applications are more commonly written in Java\nor C++, and integrating into these systems might be necessary for your model to be\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\nmore interested in statistical modeling and inference than prediction. In this case,\nyou should consider the statsmodel package for Python, which implements several\nlinear models with a more statistically minded interface. If you are not married to\nPython, you might also consider using R, another lingua franca of data scientists. R is\na language designed specifically for statistical analysis and is famous for its excellent\nvisualization capabilities and the availability of many (often highly specialized) statis‐\ntical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is\nimportant to keep in mind that production systems have different requirements from\none-off analysis scripts. If an algorithm is deployed into a larger system, software\nengineering aspects like reliability, predictability, runtime, and memory requirements\ngain relevance. Simplicity is key in providing machine learning systems that perform\nwell in these areas. Critically inspect each part of your data processing and prediction\npipeline and ask yourself how much complexity each step creates, how robust each\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\neach component warrants the complexity. If you are building involved machine learn‐\ning systems, we highly recommend reading the paper “Machine Learning: The High\nInterest Credit Card of Technical Debt”, published by researchers in Google’s\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\ning machine learning software in production at a large scale. While the issue of tech‐\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\nlearned can help us build better software even for short-lived and smaller systems.\nTesting Production Systems\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\nthat we collected beforehand. This is known as offline evaluation. If your machine\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\nthough. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365 Neural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nsian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nprocessor within a computer). The data is read from a source like the hard disk or the\nnetwork either one sample at a time or in chunks of multiple samples, so that each\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\nof the models in scikit-learn, and you can find details on it in the online user\nguide. Because out-of-core learning requires all of the data to be processed by a single\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\nlearning algorithms can be implemented in this way.\nThe other strategy for scaling is distributing the data over multiple machines in a\ncompute cluster, and letting each computer process part of the data. This can be\nmuch faster for some models, and the size of the data that can be processed is only\nlimited by the size of the cluster. However, such computations often require relatively\ncomplex infrastructure. One of the most popular distributed computing platforms at\nthe moment is the spark platform built on top of Hadoop. spark includes some\nmachine learning functionality within the MLLib package. If your data is already on a\nHadoop filesystem, or you are already using spark to preprocess your data, this might\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\ning and integrating a spark cluster might be too large an effort, however. The vw\npackage mentioned earlier provides some distributed features and might be a better\nsolution in this case.\nHoning Your Skills\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
            "summary": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optim",
            "children": [
                {
                    "id": "chapter-8-section-1",
                    "title": "Approaching a Machine Learning Problem",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-8-section-1-subsection-1",
                            "title": "Problem Definition",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-1-subsection-2",
                            "title": "Solution Strategy",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-1-subsection-3",
                            "title": "Implementation Steps",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-8-section-2",
                    "title": "Humans in the Loop",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-8-section-2-subsection-1",
                            "title": "Human Interaction",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-2-subsection-2",
                            "title": "Feedback Integration",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-2-subsection-3",
                            "title": "System Design",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-8-section-3",
                    "title": "From Prototype to Production",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-8-section-3-subsection-1",
                            "title": "Testing Production Systems",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-3-subsection-2",
                            "title": "Building Your Own Estimator",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-3-subsection-3",
                            "title": "Deployment Strategies",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-8-section-4",
                    "title": "Where to Go from Here",
                    "content": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\nA Probabilistic Perspective by Kevin Murphy (MIT Press), a comprehensive (read:\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\nOther Machine Learning Frameworks and Packages\nWhile scikit-learn is our favorite package for machine learning1 and Python is our\nfavorite language for machine learning, there are many other options out there.\nDepending on your needs, Python and scikit-learn might not be the best fit for\nyour particular situation. Often using Python is great for trying out and evaluating\nmodels, but larger web services and applications are more commonly written in Java\nor C++, and integrating into these systems might be necessary for your model to be\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\nmore interested in statistical modeling and inference than prediction. In this case,\nyou should consider the statsmodel package for Python, which implements several\nlinear models with a more statistically minded interface. If you are not married to\nPython, you might also consider using R, another lingua franca of data scientists. R is\na language designed specifically for statistical analysis and is famous for its excellent\nvisualization capabilities and the availability of many (often highly specialized) statis‐\ntical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is\nimportant to keep in mind that production systems have different requirements from\none-off analysis scripts. If an algorithm is deployed into a larger system, software\nengineering aspects like reliability, predictability, runtime, and memory requirements\ngain relevance. Simplicity is key in providing machine learning systems that perform\nwell in these areas. Critically inspect each part of your data processing and prediction\npipeline and ask yourself how much complexity each step creates, how robust each\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\neach component warrants the complexity. If you are building involved machine learn‐\ning systems, we highly recommend reading the paper “Machine Learning: The High\nInterest Credit Card of Technical Debt”, published by researchers in Google’s\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\ning machine learning software in production at a large scale. While the issue of tech‐\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\nlearned can help us build better software even for short-lived and smaller systems.\nTesting Production Systems\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\nthat we collected beforehand. This is known as offline evaluation. If your machine\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\nthough. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365 Neural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nsian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nprocessor within a computer). The data is read from a source like the hard disk or the\nnetwork either one sample at a time or in chunks of multiple samples, so that each\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\nof the models in scikit-learn, and you can find details on it in the online user\nguide. Because out-of-core learning requires all of the data to be processed by a single\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\nlearning algorithms can be implemented in this way.\nThe other strategy for scaling is distributing the data over multiple machines in a\ncompute cluster, and letting each computer process part of the data. This can be\nmuch faster for some models, and the size of the data that can be processed is only\nlimited by the size of the cluster. However, such computations often require relatively\ncomplex infrastructure. One of the most popular distributed computing platforms at\nthe moment is the spark platform built on top of Hadoop. spark includes some\nmachine learning functionality within the MLLib package. If your data is already on a\nHadoop filesystem, or you are already using spark to preprocess your data, this might\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\ning and integrating a spark cluster might be too large an effort, however. The vw\npackage mentioned earlier provides some distributed features and might be a better\nsolution in this case.\nHoning Your Skills\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
                    "summary": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optim",
                    "children": [
                        {
                            "id": "chapter-8-section-4-subsection-1",
                            "title": "Theory",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-2",
                            "title": "Other Machine Learning Frameworks and Packages",
                            "content": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\nA Probabilistic Perspective by Kevin Murphy (MIT Press), a comprehensive (read:\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\nOther Machine Learning Frameworks and Packages\nWhile scikit-learn is our favorite package for machine learning1 and Python is our\nfavorite language for machine learning, there are many other options out there.\nDepending on your needs, Python and scikit-learn might not be the best fit for\nyour particular situation. Often using Python is great for trying out and evaluating\nmodels, but larger web services and applications are more commonly written in Java\nor C++, and integrating into these systems might be necessary for your model to be\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\nmore interested in statistical modeling and inference than prediction. In this case,\nyou should consider the statsmodel package for Python, which implements several\nlinear models with a more statistically minded interface. If you are not married to\nPython, you might also consider using R, another lingua franca of data scientists. R is\na language designed specifically for statistical analysis and is famous for its excellent\nvisualization capabilities and the availability of many (often highly specialized) statis‐\ntical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is\nimportant to keep in mind that production systems have different requirements from\none-off analysis scripts. If an algorithm is deployed into a larger system, software\nengineering aspects like reliability, predictability, runtime, and memory requirements\ngain relevance. Simplicity is key in providing machine learning systems that perform\nwell in these areas. Critically inspect each part of your data processing and prediction\npipeline and ask yourself how much complexity each step creates, how robust each\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\neach component warrants the complexity. If you are building involved machine learn‐\ning systems, we highly recommend reading the paper “Machine Learning: The High\nInterest Credit Card of Technical Debt”, published by researchers in Google’s\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\ning machine learning software in production at a large scale. While the issue of tech‐\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\nlearned can help us build better software even for short-lived and smaller systems.\nTesting Production Systems\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\nthat we collected beforehand. This is known as offline evaluation. If your machine\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\nthough. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is",
                            "summary": "Vowpal wabbit is a highly optimized machine learning package written in C++ with a command-line interface. For running machine learning algorithms distributed on a cluster, mllib is a Scala library built on top of the spark distributed computing environment. There are many more kinds of machine learning out there, with many important applications. We focused on the most common machine learning tasks: classification and regression in supervised learning, and clustering and signaling decomposition in unsupervised learning. We did not cover two partic‐                ularly important topics that we didn't cover in this book. We hope that this book has helped you understand some of the most important topics in machine learning. Back to the page you came from. You’ve probably already used a ranking system today. You input a search query and obtain a sorted list of answers, ranked by howrelevant they are. The second topic is recom­mender systems, which provide suggestions to users based on their preferences. There is plenty of literature on the topic, and if you want to dive right in you                might be interested in the now classic “Netflix prize challenge’.  The Netflix prize challenge offered a prize of $1 million to the team that could provide the best recommendations. It was won by a team that was able to find the perfect movie based on a large dataset of movie preferences. The winner of the prize was awarded a $2 million prize. While scikit-learn is our favorite package for machine learning, there are many other options out there. Depending on your needs, Python and scik it-learn might not be the best fit for your particular situation. Other Machine Learning Frameworks and Packages include Pattern Recognition and Machine Learning by Christopher Bishop, Machine Learning: A Probabilistic Perspective by Kevin Murphy, and Machine learning: An AlgorithmicPerspective by Stephen Marsland. For more information on machine learning and other topics, visit the Machine Learning website. For information on how to use Python and other machine learning tools, see the Python Using Python is great for trying out and evaluating models, but larger web services and applications are more commonly written in Java or C++. Another reason you might want to look beyond scikit-learn is if you are more interested in statistical modeling and inference than prediction. In this case, consider the statsmodel package for Python. If you are not married toPython, you might also consider using R, another lingua franca of data scientists. Vowpal wabbit is a highly optimized machine learning package written in C++ with a command-line interface. vw is particularly useful for large datasets and using a high-performance language. It is important to keep in mind that production systems have different requirements from one-off analysis scripts. If an algorithm is deployed into a larger system, software engineering aspects like reliability, predictability, runtime, and memory requirements are also relevant. The tool is written in R, a language designed specifically for statistical analysis. R is famous for its excellentvisualization capabilities and the availability of many (often highly specialized) statis­tical modeling packages. It can be easier thanembedding a whole library or programming language and Simplicity is key in providing machine learning systems that perform well. Critically inspect each part of your data processing and prediction pipeline and ask yourself how much complexity each step creates. If you are building involved machine learn‐ing systems, we highly recommend reading the paper “Machine Learning: The High Interest Credit Card of Technical Debt’. The paper highlights the trade-off in creating and maintaining machine learning software in production at a large scale.    The paper is published by researchers in Google’s machine In this book, we covered how to evaluate algorithmic predictions based on a test set. This is known as offline evaluation. If your machinelearning system is user-facing, this is only the first step in evaluating an algorithm. The next step is usually online testing or live testing, where the consequences of employing the algorithm in the overall system are evaluated. The lessons learned in this book can help us build better software even for short-lived and smaller systems. This book is organized roughly as follows: Chapter 1 introduces the fundamental concepts of machine learning. Chapters 2 and 3 describe the actual machine learning algorithms that are most widely used in practice. Chapter 5 covers advanced methods for model evaluation and parameter tuning. Chapter 6 explains the concept of pipelines for chaining models. Chapter 7 shows how to apply the methods described in earlier chapters to text data, and introduces some text-specific processing techniques. Chapter 8 offers a high-level overview, and includes references to more advancedtopics. For more information on how to use this book, please visit the book’s website or contact the author at: http://www.researchers.com/machine-learning-and-search/machine If you need to build a machine learning system ASAP, we suggest starting with Chapter 1 and the opening sections of Chapter 2. Choose the model that best fits your needs and flip back to read the preface for details. Then you can use the techniques in Chapter 5 to evaluate and tune your model. You can interact directly with the code, using a terminal or other tools like the JupyterNotebook, which we’ll look at shortly. While studying this book, definitely refer to the scikit-learn website for more in-depth documentation of the classes and functions, and many examples. There is also a video course created by Andreas Müller, “Advanced Machine Learning with scikitslearn,� The scikit-learn project is constantly being developed and improved, and it has a very active user community. It contains a number of state-of-the-art machine learning algorithms, as well as comprehensive documentation about each algorithm. It is an open source project, meaning that it is free to use and distribute, and anyone can easily obtain the source code to see what is going on behind the scenes. Python is a general-purpose programming language that allows for the creation of complex graphical user interfaces (GUIs) and web services. It also allows for integration into existing systems. The most prominent Python library for machine learning is calledscikit-learn. Scikit-learn works well with a number of other scientific Python tools. It is widely used in industry and academia. The online documentation is very thorough, and this book will provide you with all the prerequisites in machine learning to understand it in detail. For plot­ting and interactive development, you should also install matplotlib, IPython, and the Jupyter Notebook. It depends on two other Python packages, NumPy and SciPy, which we will discuss later in this chapter. We recommend that you also browse the user guide and API documentation In a larger context, the algorithms and methods in machine learning are only one progressivelypart of a greater process to solve a particular problem. It is good to keep the big picture in mind at all times. We recommend using one of the following prepackaged Python distributions, which will provide the necessary packages: Anaconda, Python 2.4, Python 3, and Python 4.1. We also recommend using the popular Python 3.5 and Python 5 distributions. For more information on how to get started with machine learning, see the official machine learning guide for Python 3 and the official Python 5 guide for python.1 and Python 2, as well as the official python 3.2 and python5.1 guides. Python has become the lingua franca for many data science applications. It combines the power of general-purpose programming languages with the ease of use of domain-specific scripting languages like MATLAB or R. Python has libraries for dataloading, visualization, statistics, natural language processing, image processing, and more. It provides data scientists with a large array of general and special-purpose functionality. It is free and open source, and can be downloaded for free from the Google Play store. For more information on how to use Python in your data science project, visit python.org or see the python.com website. To learn more about machine learning in the U.S., visit the Machine Learning Institute in New York City. Using Python is essential for machine learning and data analysis. One of the main advantages of using Python is the abilitesity to interact directly with the code. I’m also thankful for the discussions with many of my colleagues and peers that helped me understand the challenges of machine learning. I would never have become a core contributor to scikit-learn or learned to under‐ encompasses this package as well as I do now. My thanks also go out to all the other contrib­utors who donate their time to improve and maintain this package. It’s also essential for data analysis and machine learning to be I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. Medical applications, for example, might need very high levels of precision that possibly cannot be achieved by a machine learning algorithm alone. And finally, thanks to DTS, for your everlasting and endless support. Back to Mail Online home. Back To the page you came from.    Back to the page where you came From.  The first part of this article was published on November 14, 2013. We are happy to share it with you for free. The tools we’ve discussed in this book are great for many machine learning applications. Python and scikit-learn are also used in production systems in many organizations. However, many companies have complex infrastructure, and it is not always easy to include Python in these systems. That is not necessarily a problem, however, as Python can be used in a variety of ways to make complex decisions more easily. The book concludes with a look at how to use these tools in your own production systems. For more information, visit the book’s website or go to: http://www.pklearn.org/book/machine-learning-and- Languages like Go, Scala, and Java can be used to build robust systems. This can be easier than using a whole library or programming language and converting from and to the different data formats. It can also be easier to use the same language for different types of data. It is possible to use different languages for the same purpose, but it is more difficult to do it in the same way for different data types. For more information on how to use languages like Go and Scala for data analysis, go to: http://www.cnn.com/2013/",
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-3",
                            "title": "Ranking, Recommender Systems, and Other Kinds of Learning",
                            "content": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365",
                            "summary": "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optim",
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-4",
                            "title": "Probabilistic Modeling, Inference, and Probabilistic Programming",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-5",
                            "title": "Neural Networks",
                            "content": "Neural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nsian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your",
                            "summary": "Machine learning is a rapidly evolving area of machine learning. Recent breakthroughs in machine learning and artificial intelligence have all been driven by these advances. While the progress in this field is so fast-paced that any current reference to the state of the art will soon be outdated, the recent book Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press) is a comprehensive introduction into the subject. We always assumed that the data we were working with could be stored in a NumPy array or SciPy sparse matrix in memory (RAM). We are happy to clarify that this is not the case. We are also happy to point out that in the book, the data is stored in an array or sparse Modern servers often have hundreds of gigabytes (GB) of RAM. Not everybody can afford to buy such a                large machine, or even to rent one from a cloud provider. To process large amounts of data on a budget, there are two basic strategies: out-of-core learning and parallelization over a cluster. To learn from data that cannot be stored in mainmemory, but where the learning takes place on a single computer (or even a single kernel), you need to use an out- of-core system. To build a machine learning system that can process terabytes of data, you need a cluster of machines that can parallelize over a large amount of data. To get the most out of your cluster, you Neural Networks (Deep Learning) is a family of algorithms known as neural networks. gamma and C both control the complexity of the model, with large values in either resulting in a more complex model. Good settings for the twoparameters are usually strongly correlated, and C and gamma should be adjusted together. Here, we will only discuss some relatively simple methods, namelymultilayer perceptrons for classification and regression, that can serve as a starting point for more involved deep learning methods. For more information on how to use neural networks in your computer, visit the Multilayer perceptrons (MLPs) are also known as (vanilla) feed-forward neural networks, or sometimes just neuralnetworks. MLPs can be viewed as generalizations of linear models that perform multiple stages of processing to come to a decision.Remember that the prediction by a linear regressor is given as: ŷ is a weighted sum of the input features x[0] to x[p], weighted by the learned coefficients w[0) to w[p]. We could visualize this graphically as shown in visualization of logistic regression, where input features and predictions are shown as nodes, and coefficients are connections between the nodes. Each node on the left represents an input feature, the connecting lines represent the learned coefficients, and the nodes on the right represent the output, which is a weighted sum of the inputs. In an MLP this process of computing weighted sums is repeated multiple times, first by computing hidden units that represent an intermediate processing step, which are then processed again to get the final sum. I’m also thankful for the discussions with many of my colleagues and peers that helpped me understand the challenges of machine learning and gave me ideas for a textbook. I would never have become a core contributor to scikit- I want to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe, Hugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas, and Dan Cervone. My thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader of an early version of this book, and helped me shape it in many ways. Thanks to the O’Reilly folks for their endless patience. Without Meg Blanchette, without whose help and guidance this project would not have even existed. I also want to thanks the many people in my life whose love and friendship gave me the energy and Machine learning is about extracting knowledge from data. It is a research field at the intersection of statistics, artificial intelligence, and computer science. The application of machine learning methods has in recent years become ubiquitous in everyday life. From auto‐matic recommendations of which movies to watch, to what food to order or which products to buy, to personalized online radio and recognizing your friends in your. social network. And finally, thanks to DTS, for your everlasting and",
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-6",
                            "title": "Scaling to Larger Datasets",
                            "content": "scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nprocessor within a computer). The data is read from a source like the hard disk or the\nnetwork either one sample at a time or in chunks of multiple samples, so that each\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\nof the models in scikit-learn, and you can find details on it in the online user\nguide. Because out-of-core learning requires all of the data to be processed by a single\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\nlearning algorithms can be implemented in this way.\nThe other strategy for scaling is distributing the data over multiple machines in a\ncompute cluster, and letting each computer process part of the data. This can be\nmuch faster for some models, and the size of the data that can be processed is only\nlimited by the size of the cluster. However, such computations often require relatively\ncomplex infrastructure. One of the most popular distributed computing platforms at\nthe moment is the spark platform built on top of Hadoop. spark includes some\nmachine learning functionality within the MLLib package. If your data is already on a\nHadoop filesystem, or you are already using spark to preprocess your data, this might\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\ning and integrating a spark cluster might be too large an effort, however. The vw\npackage mentioned earlier provides some distributed features and might be a better\nsolution in this case.\nHoning Your Skills\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
                            "summary": "Some algorithms, like neural networks and SVMs, are very sensitive to the scaling of the data. Therefore, a common practice is to adjust the features so that the data representation is more suitable for these algorithms. This is a simple per-feature rescaling and shift of theData. The following code shows a simple example of such a rescaling. The code is called plot_scaling() and the data is plotted on a plot. The data is then plotted on the plot and the scaling is applied to the plot. For example, the plot looks like this: plot.scaling(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, The StandardScaler in scikit-learn ensures that for each feature the mean is 0 and the variance is 1. The RobustScaler uses the median and quartiles,1 instead of the mean and variance. The following four plots show four different ways to transform the data that yield more standard ranges. Different ways to rescale and preprocess a dataset are shown in Figure 3-1, 3-2, and 3-3. For example, the first plot shows a synthetic two-class classification dataset with two features. The first feature (the x-axis value) is between 10 and 15, and the second feature is between around 1 and 9. Out-of-core learning describes learning from data that cannot be stored in mainmemory. Data is read from a source like the hard disk or the network either one sample at a time or in chunks of multiple samples. This subset of the data is then processed and the model is upda‐phthalted to reflect what was learned from the data. This makes the RobustScaler ignore data points that are very very difficult to process. To process large amounts of data on a budget, there are two basic strategies: out-of core learning and parallelization over a cluster. The Robust Scaler uses both strategies to process data. For more information on how to use these strategies, visit Robust scaler.com. The other strategy for scaling is distributing the data over multiple machines in a compute cluster. This can bemuch faster for some models, and the size of the data that can be processed is only limited by the cluster size. Such computations often require relativelycomplex infrastructure. One of the most popular distributed computing platforms at the moment is the spark platform built on top of Hadoop. spark includes somemachine learning functionality within the MLLib package. If your data is already on a Hadoops filesystem, or you are already using spark to preprocess your data, this might be the easiest option. If you don’t already have such infrastructure in place, establish‐consuming a spark cluster might be too large an effort. Neural Networks is a rapidly evolving area of machine learning. New innovations and new applications are announced on a weekly basis. The vw package mentioned earlier provides some distributed features and might be a better solution in this case. We touched on the subject of neural networks briefly in Chapters 2 and 7, this is a good opportunity to revisit them. We hope you will use this information to help you improve your skills and learn more about machine learning and artificial intelligence. Back to the page you came from. Deep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press) is a comprehensive introduction into the subject. In most applications, the data that is used to build a machine learning system is relatively small, though. Few machine learning datasets consist of hundreds of gigabites of data or more. This makes expanding your RAM or renting a machine from a cloud provider a viable sol‐cularution in many cases. The progress in this field is so fast-paced thatany current reference to the state of the art will soon be outdated, the authors say. The book is published by MIT Press, and can be downloaded for free from the MIT Press website. There are two basic strategies: out-of-core learning and parallelization over a cluster. If you need to work with terabytes of data, however, or you need a budget, there are two more basic strategies. The first strategy involves learning from data that cannot be stored in mainmemory, but where the learning takes",
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-4-subsection-7",
                            "title": "Honing Your Skills",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-8-section-5",
                    "title": "Conclusion",
                    "content": "",
                    "summary": "",
                    "children": [
                        {
                            "id": "chapter-8-section-5-subsection-1",
                            "title": "Final Thoughts",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-5-subsection-2",
                            "title": "Key Takeaways",
                            "content": "",
                            "summary": null,
                            "children": []
                        },
                        {
                            "id": "chapter-8-section-5-subsection-3",
                            "title": "Next Steps",
                            "content": "",
                            "summary": null,
                            "children": []
                        }
                    ]
                }
            ]
        }
    ]
}