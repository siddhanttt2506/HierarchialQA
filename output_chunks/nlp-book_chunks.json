[
    "Natural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy\nUniversity of Hyderabad\nPublished By\nEss Ess Publications\nFor\nSarada Ranganathan Endowment for Library\nScience\nBangalore, INDIA\n2006\ni\nc\n⃝Kavi Narayana Murthy and Sarada Ranganathan\nEndowment for Library Science (2005)\nAll rights reserved. No part of this publication may be\nreproduced, stored in a retrieval system or transmitted, in\nany form or by any means, electronic, mechanical, photo-\ncopying, recording or otherwise without the prior written\npermission of the publisher.\nThis book has been printed from the camera-ready\ncopy prepared by the author using LATEX\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy, University of Hyderabad\nSarada Ranganathan Endowment Lecture, 24(2004)\nFirst Published 2006\nISBN 81-7000-485-3\nPrice: Rs. 850/-\nPublished by\nEss Ess Publications\n4837/24, Ansari Road, Darya Ganj, New Delhi-110 002\nTel: 001-23260807 Fax: 001-23274173\nE-mail: essess@del3.vsnl.net.in url:\nhttp//www.essessreference.com\nFor\nSarada Ranganathan Endowment for Library\nScience\n702, ‘Upstairs’, 42nd Cross, III Block, Rajajinagar,\nBangalore 560 010\nE-mail: srels@vsnl.com Tel: 080-23305109\nPrinted in India at: Printline, New Delhi 111 002\nii\nPreface\nThe contributions of Dr. S R Ranganathan to the ﬁeld of\nlibrary and information sciences is well known. Sarada Ran-\nganathan Endowment for Library Science (SRELS), founded\nby Dr Ranganathan in 1961 has been carrying out com-\nmendable work in promoting library and information sci-\nences. SRELS has been working towards improvement of\nlibrary and information services in India, training personnel\nin library and information sciences and applying research\nresults in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.",
    "results in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.\nSRELS publishes the\nSRELS Journal of Information Management and has also\npublished a large number of books. This book grew out of\nthe Sarada Ranganathan Endowment Lectures I gave in Au-\ngust 2004 on Natural Language Processing and Intelligent\nInformation Retrieval.\nThe aim of my lectures was to outline broadly the current\nstate and future research directions in Information Retrieval\n(IR) and Natural Language Processing (NLP), highlighting\nthe role of NLP in moving towards more Intelligent Informa-\ntion Retrieval. The book has expanded on this theme and\nincluded material on related areas such as Information Ex-\ntraction, Automatic Text Categorization, Automatic Sum-\nmarization and Machine Translation. The chapter on NLP\nincludes relevant topics in linguistics as well as a brief out-\nline of statistical techniques for machine learning. Included\nare sections on Corpora, Dictionaries, Thesauri and Word-\nNets, Morphology, Syntax and Semantics. The orientation\nis towards Indian Languages although much of the mate-\nrial is also relevant for English and other languages of the\nworld. The book reﬂects to a large extent my own experi-\nence in teaching and research in Artiﬁcial Intelligence, NLP\niii\nand Text Processing over the last 15 years.\nLanguage engineering is a highly multi-disciplinary ﬁeld,\nborrowing as it does, from such varied disciplines as Linguis-\ntics, Psychology, Cognitive Science, Artiﬁcial Intelligence,\nComputer Science, Mathematics, Statistics, Physics and En-\ngineering.\nNo individual can claim expertise in so many\nareas. Yet we need to have some understanding and appre-\nciation of the concerns, aims, terminology, deﬁnitions and\nmethodologies of these various disciplines. While good spe-",
    "Computer Science, Mathematics, Statistics, Physics and En-\ngineering.\nNo individual can claim expertise in so many\nareas. Yet we need to have some understanding and appre-\nciation of the concerns, aims, terminology, deﬁnitions and\nmethodologies of these various disciplines. While good spe-\ncialized books exist, introductory books that give a balanced\nview from diﬀerent perspectives are rare. There is an acute\nshortage of trained manpower in this fascinating and impor-\ntant subject. Perhaps one of reasons for this situation is the\nnon availability of suitable text books. It is hoped that this\nbook will be useful for beginners from varied disciplines.\nThis book grew out of a series of lectures and you will\nﬁnd the same informal, almost conversational style of pre-\nsentation.\nThis is not so much of a formal text book or\na research monograph as it is a beginners’ manual.\nNo\nbackground is assumed. Anybody should be able to read\nthe book without much diﬃculty. The aim has been to en-\nsure technical correctness and soundness while still sound-\ning simple and easy. This is especially diﬃcult in a highly\nmulti-disciplinary area, each area having several competing\ntheories and view points. At times simplicity and under-\nstandability by un-initiated beginners coming from diverse\nbackgrounds had to be given priority over being technically\nthe most precise as per particular theories or models of lan-\nguage.\nThe primary goal of the book is not so much to describe\nstate-of-the-art technologies and advanced research results\nas it is to provide a broad background to help prepare stu-\ndents and researchers coming from diﬀerent backgrounds in-\ncluding computer science, library and information science,\nartiﬁcial intelligence and linguistics. The book aims to pro-\niv\nvide a balanced treatment of linguistic, statistical and tech-\nnological material with an Indian language focus. Treatment\nis kept simple and mathematical jugglery is kept to a mini-",
    "artiﬁcial intelligence and linguistics. The book aims to pro-\niv\nvide a balanced treatment of linguistic, statistical and tech-\nnological material with an Indian language focus. Treatment\nis kept simple and mathematical jugglery is kept to a mini-\nmum. No background in any speciﬁc area is assumed. Style\nof writing is not very formal or terse. The focus is on the\nmajor lines of thinking, ideas and themes. Instead of giving\nready made solutions in all cases, the book raises questions\nand issues, to get budding researchers to start independent\nthinking. The bibliography is limited to readily accessible\ntext books - references to diﬃcult-to-get research articles\nand the ever changing web-sites are avoided. Language En-\ngineerings is a very active area of research and development\nand the interested reader will ﬁnd good resources on the\nweb.\nThere is enough material in the book to be used as a\ntext book for a ﬁrst course on the topic within library and\ninformation sciences, linguistics, computer science, artiﬁcial\nintelligence and other related disciplines. Parts of the book\nmay also be found useful for more advanced courses. It is\nhoped that this book will be well received. Comments and\nsuggestions to improve the book are welcome.\nA second aim of the book has been to provide some as-\nsessment of the status of language technologies in India. It\nis important to stop once in a while and take a fresh, unbi-\nased look at what all we have achieved and where we have\nnot been very successful. An analysis of failures and deﬁ-\nciencies and a feeling that we can actually do much more, is\nabsolutely essential for taking up fresh initiatives with new\nlife and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of",
    "life and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of\nthe limited interface linguists and engineers have had in In-\ndia on topics of relevance here. Linguistics itself has changed\nv\nsubstantially over time. There have been competing theories\nand view points. Focus areas, theories or models adapted\nand view points have varied substantially even within the In-\ndian context. Generalizations and extrapolations can thus\nbe dangerous. Comments on the current state of technology\nshould be taken with care as technology keeps changing very\nfast. All views expressed are especially applicable to the In-\ndian scene. If some of the statements appear a bit negative,\nit is because the assessment being made is with respect to\nthe perfect, the ideal, with respect to what we could have\nachieved. It is not that all the good work that has been done\nis not appreciated. It should be taken in the positive sense\nthat we have the capability to do much more. We can be\nworld leaders. It is essential to have this optimism but at\nthe same time we must plan our work based on the knowl-\nedge and understanding of the ground realities. This is the\nrecipe to success. I hope this view will be appreciated and\nwell received.\nKavi Narayana Murthy\ncaitra s’ukla pratipat, paarthiva naama saMvatsara\n(chaandramaana yugaadi - 9 April 2005)\nvi\nFOREWORD\nWhen I agreed to chair on the occasion of Sarada Ran-\nganathan Memorial Lectures 2004 delivered by my former\ncolleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much",
    "colleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much\ntime and care for the newcomers in the ﬁeld, in the midst\nof numerous academic commitments. KNM has had proba-\nbly a thought of doing something like this, useful for both\nlinguistics as well as AI students for a long time, and this\ncomes now as his wish-fulﬁllment. The perspective has, of\ncourse, been made very clear from the author’s end, in case\nwe want to know for sure about his angle of looking into it,\nand that is stated in the sub-title : ‘The Information Access\nApproach’.\nIt is, therefore, not surprising that KNM would start\nfrom the information science knowledge explosion, and sit-\nuate the text in that context. In fact, he begins with QA-\nsystem under which he deals with ELIZA and early NLP\nsystems. This is followed by a lucid introduction to the two\ngreatest utility works being done world-wide on information\nsciences, namely, ‘Information Retrieval’ and ‘Information\nExtraction’ systems. The other advantages of the latest de-\nvelopments in machine understanding of human language\ntexts are creation of automatic summary as well as auto-\ncategorization of text-types and sub-types.\nIt is obvious that many readers of this book would be\ninterested in getting to know more about Machine Trans-\nlation (MT) as well as Machine-Aided Translation (MAT),\nand the author has devoted a whole section on that area. He\nhas identiﬁed the challenges before the machine translation\nenthusiasts as they exist today. An introduction to the ﬁeld\nof speech technology is given after that. With this, it is easy\nvii\nfor KNM to lay the foundations of NLP in the next section.\nHow doing purely esoteric linguistics is diﬀerent from doing\ncomputational linguistics has been brought about by him\nclearly in a section.",
    "of speech technology is given after that. With this, it is easy\nvii\nfor KNM to lay the foundations of NLP in the next section.\nHow doing purely esoteric linguistics is diﬀerent from doing\ncomputational linguistics has been brought about by him\nclearly in a section.\nThose interested in corpora are also\ngoing to beneﬁt from this text book tremendously as there\nis a long section devoted to this area, besides introductory\nremarks in the earlier section. Under 2.4, a lot of loosely\nstrung issues have been put together in a rather well-written\nand lengthy section.\nThe third chapter deals with the latest researches in ‘In-\nformation Retrieval’, a matter so very dear to all librarians\nand information science persons who were present during the\ntalk. For those who are uninitiated, an introduction to the\nbasic model of retrieval has been presented but the advance\nlearners and persons on the job can also beneﬁt from the\ndescription of an advanced IR model, which he calls the ‘In-\ntelligent IR’. There again, KNM brings out the role played\nby linguists and semanticists.\nThe best part of the book is that it has a very good\nreading list for those interested in NLP in its Bibliography\nsection as well as a few very relevant appendices that go very\nwell with the text.\nI sincerely hope that like his lectures were well-appreciated\nby those who were fortunate enough to attend them in Ban-\ngalore, the text as put together here will also be highly ap-\npreciated by the readers of all times to come. On behalf of\nNLP community, let me thank KNM for a laudable eﬀort,\nand he deserves a word of praise also from the library and\ninformation scientists.\nUdaya Narayana Singh\nDirector, Central Institute of Indian Languages, Mysore\nMay 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having",
    "May 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having\ngiven me an opportunity to present my views to such an\naugust audience.\nThe book has expanded from the top-\nics covered in the lectures, based on my own experience in\nteaching and research in this ﬁeld over the last many years.\nI am grateful to SRELS for coming forward to publish this\nbook.\nMy special thanks go to Prof.\nA Neelameghan upon\nwhose invitation and inspiration I undertook the job of writ-\ning this book. Although the idea of writing a book on NLP\nwas there in me for a long time, it is only due to his encour-\nagement that this book has taken shape so soon.\nProf.\nUdaya Narayana Singh, director CIIL, Mysore\nwas kind enough to inaugurate my lectures at SRELS. His\nspeeches are always as inspiring as they are informative. I\nam also grateful to him for writing an excellent foreword to\nthis book.\nA number of my colleagues, friends and students, com-\ning from diverse backgrounds including linguistics, computer\nscience, artiﬁcial intelligence, mathematics and engineering,\nhave taken pains to go through the drafts and have made\nvery valuable comments and suggestions. My sincere thanks\ngo to G Uma Maheshwara Rao, S Rajendran, S Durga Bha-\nvani, L Sobha, S Baskaran, Ramesh Kumar, G Bharadwaja\nKumar and Hla Hla Htay.\nI sincerely acknowledge the AU-KBC Research Centre, a\nsmall but very active privately funded research organization\nin Chennai, for inviting me as a visiting scientist and giving\nme an opportunity to sit and work peacefully. If I could\ncomplete the book in such a short period, it is because of\nthe nice academic environment at the centre and the cooper-\nix\nation and support of Prof. C N Krishnan, the director, and\nall the scientists and researchers at the centre. My special",
    "me an opportunity to sit and work peacefully. If I could\ncomplete the book in such a short period, it is because of\nthe nice academic environment at the centre and the cooper-\nix\nation and support of Prof. C N Krishnan, the director, and\nall the scientists and researchers at the centre. My special\nthanks go to Dr Sobha and Mr. Baskaran for all the lively\ndiscussions we have had on NLP.\nAlthough the idea of writing a book was there in my\nmind for a long time, ﬁnding time was not easy. I am thank-\nful to the authorities of University of Hyderabad for giving\nme leave. Whatever little I have learnt during the last 15\nyears there is due to the close interactions I have had with\nexperts from varied disciplines including linguistics, philos-\nophy, mathematics and statistics. My colleagues at the De-\npartment of Computer and Information Sciences and my\nstudents have always been a constant source of inspiration.\nNot a single day passes without some lively discussion and\ndebate on some research topic or the other in computer sci-\nence or artiﬁcial intelligence.\nThe book is based largely on my notes, slides, scrib-\nblings, papers and reports written over the last many years.\nI have been greatly beneﬁted by all the lectures and sem-\ninars I have attended and all the discussions I have had\nwith many experts from varied ﬁelds. I would have surely\nborrowed many ideas and examples from many sources. It\nwould be practically very diﬃcult to acknowledge each one\nof them separately. There are few sections that have been in-\nﬂuenced substantially from writings of others. The section\nAutomatic Summarization has drawn partly from the pa-\npers and summaries compiled by Inderjeet Mani and Mark\nT Maybury.\nParts of the section on morphology dealing\nwith English morphology have been inﬂuenced by the book\nby Andrian Akmajian, Richard A Demers and Robert M\nHarnish. Acknowledgements and credits have been included\nat appropriate places in the text.",
    "T Maybury.\nParts of the section on morphology dealing\nwith English morphology have been inﬂuenced by the book\nby Andrian Akmajian, Richard A Demers and Robert M\nHarnish. Acknowledgements and credits have been included\nat appropriate places in the text.\nMy wife Nalini and my daughter Arabhi have put up\nwith many hardships and extended their fullest cooperation\nwhile I was away from home writing the book. I do not have\nx\nwords to thank my father Kavi Krishna Murthy who initi-\nated me into Sanskrit and taught me the joy of working with\nlanguages. Nor do I have words to thank my mother who\nhas in a sense taught me everything I know, not by teaching\nor preaching but by making me think.\nKavi Narayana Murthy\nContents\nPreface\nii\nForeword\nvi\n1\nThe Information Age\n1\n1.1\nThe Information Age . . . . . . . . . . . . . .\n1\n1.2\nTechnology for Accessing Info . . . . . . . . .\n3\n1.3\nQuestion Answering Systems\n. . . . . . . . .\n7\n1.3.1\nELIZA - The Rogerian Therapist . . .\n9\n1.3.2\nEarly NLP Systems\n. . . . . . . . . .\n13\n1.3.3\nFoundations of Story Understanding .\n21\n1.3.4\nIn-Depth Understanding . . . . . . . .\n23\n1.3.5\nTuring Test . . . . . . . . . . . . . . .\n27\n1.4\nInformation Retrieval\n. . . . . . . . . . . . .\n28\n1.4.1\nIR Deﬁned\n. . . . . . . . . . . . . . .\n29\n1.4.2\nDocuments as Bags-of-Words . . . . .\n30\n1.4.3\nThe Vector Space Model . . . . . . . .\n30\n1.4.4\nPerformance Evaluation . . . . . . . .\n31\n1.4.5\nMeasuring Relevance . . . . . . . . . .\n32\n1.4.6\nChallenges in Information Retrieval\n.\n33\n1.5\nInformation Extraction . . . . . . . . . . . . .\n35\n1.5.1\nWhat is Information Extraction? . . .\n35\n1.5.2\nInformation Extraction Tasks . . . . .\n36\n1.5.3\nArchitecture of an IE System . . . . .\n38\n1.6\nAutomatic Summarization . . . . . . . . . . .\n39\nxi\nxii\nCONTENTS\n1.6.1\nWhy Summarization?\n. . . . . . . . .\n39\n1.6.2\nApproaches to Automatic Summariza-\ntion\n. . . . . . . . . . . . . . . . . . .\n41\n1.6.3\nSummarization in Relation to Infor-\nmation Extraction . . . . . . . . . . .\n44\n1.6.4",
    "38\n1.6\nAutomatic Summarization . . . . . . . . . . .\n39\nxi\nxii\nCONTENTS\n1.6.1\nWhy Summarization?\n. . . . . . . . .\n39\n1.6.2\nApproaches to Automatic Summariza-\ntion\n. . . . . . . . . . . . . . . . . . .\n41\n1.6.3\nSummarization in Relation to Infor-\nmation Extraction . . . . . . . . . . .\n44\n1.6.4\nSummarization in Relation to Other\nTechnologies\n. . . . . . . . . . . . . .\n45\n1.6.5\nEvaluation of Summarization Systems\n45\n1.6.6\nSummarization in the Context of In-\ndian Tradition\n. . . . . . . . . . . . .\n45\n1.7\nAutomatic Text Categorization . . . . . . . .\n47\n1.7.1\nWhy Text Categorization? . . . . . . .\n47\n1.7.2\nApproaches to Automatic Text Cate-\ngorization . . . . . . . . . . . . . . . .\n47\n1.7.3\nText Representation . . . . . . . . . .\n50\n1.7.4\nFeature Weighting . . . . . . . . . . .\n51\n1.7.5\nText Classiﬁcation and Clustering\n. .\n53\n1.8\nMachine Translation . . . . . . . . . . . . . .\n54\n1.8.1\nMachine Translation is Hard\n. . . . .\n55\n1.8.2\nDeploying Machine Translation . . . .\n60\n1.8.3\nApproaches to Machine Translation\n.\n62\n1.8.4\nChallenges in Machine Translation . .\n64\n1.8.5\nMachine Translation in India . . . . .\n64\n1.9\nSpeech Technologies\n. . . . . . . . . . . . . .\n71\n1.9.1\nAutomatic Speech Recognition . . . .\n72\n1.9.2\nSpeech Synthesis . . . . . . . . . . . .\n74\n1.9.3\nOther Speech Technologies\n. . . . . .\n76\n1.10 Human and Machine Intelligence . . . . . . .\n77\n1.11 Shape of Things to Come\n. . . . . . . . . . .\n81\n2\nFoundations of NLP\n85\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . .\n85\n2.1.1\nLanguage, Communication, Technology 85\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\n. . . . . . . . .\n87\nCONTENTS\nxiii\n2.1.3\nNLP: An AI Perspective . . . . . . . .\n88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3",
    "88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3\nMorphology . . . . . . . . . . . . . . . 120\n2.2.4\nPOS Tagging . . . . . . . . . . . . . . 162\n2.2.5\nSyntax: Grammars and Parsers . . . . 166\n2.2.6\nSemantics . . . . . . . . . . . . . . . . 215\n2.2.7\nPragmatics . . . . . . . . . . . . . . . 232\n2.2.8\nOther Areas of Linguistics . . . . . . . 232\n2.3\nStatistical Approaches . . . . . . . . . . . . . 233\n2.3.1\nCorpora . . . . . . . . . . . . . . . . . 235\n2.3.2\nStatistical Approaches to Language . . 241\n2.3.3\nMachine Learning\n. . . . . . . . . . . 245\n2.4\nIndian Language Technologies . . . . . . . . . 261\n2.4.1\nThe Text Processing Environment: . . 269\n2.4.2\nThe Alphabet . . . . . . . . . . . . . . 271\n2.4.3\nThe Script Grammar . . . . . . . . . . 273\n2.4.4\nFonts, Glyphs and Encoding Standards 277\n2.4.5\nCharacter Encoding Standards . . . . 281\n2.4.6\nRomanization . . . . . . . . . . . . . . 301\n2.4.7\nSpell Checkers\n. . . . . . . . . . . . . 304\n2.4.8\nOptical Character Recognition\n. . . . 311\n2.4.9\nLanguage Identiﬁcation\n. . . . . . . . 316\n2.4.10 Others Technologies for Indian Lan-\nguages . . . . . . . . . . . . . . . . . . 321\n2.4.11 NLP and Sanskrit\n. . . . . . . . . . . 321\n2.4.12 Epilogue . . . . . . . . . . . . . . . . . 324\n2.5\nConclusions . . . . . . . . . . . . . . . . . . . 325\n3\nAdvances in IR\n327\n3.1\nHistory of IR . . . . . . . . . . . . . . . . . . 327\n3.1.1\nFrom The Library to the Internet . . . 329\nxiv\nCONTENTS\n3.2\nBasic IR Models\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3",
    ". . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE",
    "We\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\ntion from anywhere in the world. We are no longer conﬁned\nto one oﬃce or one library. Today we live in the information\nage.\nWe have billions of pages of material on the Internet to-\nday. With the Digital Library initiatives whole libraries are\ngetting converted to electronic form. Trillions of bytes of\nelectronic data are getting generated everyday. But merely\nhaving some data or information somewhere is of no use.\nWhat is really required is an easy way to access relevant,\ntimely, useful, and authentic information in a well presented\nmanner. How do we know which documents are relevant,\nauthentic and dependable?\nHow do we ﬁsh out what we\nare looking for in this vast ocean of web pages? How do we\ncategorize, classify, index and structure these largely unor-\nganized collections of electronic documents on the Internet\nso that they become more easily accessible and hence more\nuseful?\nTechnology enables us to create, store and process large\nvolumes of information at great speed.\nBut the speed at\nwhich we human beings can read and understand documents\nremains the same, irrespective of technological advances.\nWe still take minutes to browse a page, possibly hours to\nread carefully and understand the content and may be days,\nweeks or even months to chew and fully digest the purport of\na serious piece of writing. Thus we are in a situation today\nwhere we have more information than we can handle. Tech-\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-",
    "nologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2",
    "available or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\nTechnology for Accessing Informa-\ntion\nThe processing speed and memory capacity of computers\nhave been increasing at mind boggling rates making it pos-\nsible to electronically process information at extremely high\nspeeds. But computers are dumb - they have no common\nsense, world knowledge or human-like thinking and reason-\n4\nCHAPTER 1. THE INFORMATION AGE\ning abilities. Human beings are far superior to computers\nin reading and understanding texts but the speed at which\nthey can do this is limited. While computer speeds are in-\ncreasing, the speed of human processing always remains the\nsame. The challenge is to make these dumb computing ma-\nchines give human-like performance, only many times faster.\nOur aim here will be to give an idea about the tools and\ntechnologies being developed today to address this grand\nchallenge.\nThere are several ways we can look at this problem of\naccessing relevant information from large collections of doc-\numents:\n• A Question Answering System accepts a ques-\ntion in natural language, attempts to understand the\nuser’s requirements, accesses relevant information and\npresents it to the user in natural language. As you may\nguess, this is not going to be an easy task. Words have\nseveral senses, meaning depends upon context, natu-\nral language sentences have fairly complex structure,\nand you often need to read between lines to under-\nstand exactly what the user is looking for. Answering\nquestions requires a good deal of intelligence.\n• Browsers allow us to interactively navigate through a\nweb of inter-connected documents, physically located\nin diﬀerent computers spread across the globe. Here\nthe role of the computer is limited to locating and",
    "questions requires a good deal of intelligence.\n• Browsers allow us to interactively navigate through a\nweb of inter-connected documents, physically located\nin diﬀerent computers spread across the globe. Here\nthe role of the computer is limited to locating and\ntaking you to the documents you ask for. You start\nby specifying a URL (Uniform Resource Locator) - an\naddress of a website or a speciﬁc document you are\nlooking for. You decide which links to explore further\nand which documents to read or download and save\nfor future use.\n• Search Engines and Information Retrieval (IR)\nSystems accept a query from the user and attempt to\n1.2. TECHNOLOGY FOR ACCESSING INFO\n5\nretrieve those documents in the collection that seem to\nmatch the user’s needs as expressed in his/her query.\nThe role of the machine is limited to drawing the user’s\nattention to documents that are potentially relevant\nto his/her needs. The challenge is to understand the\nuser’s speciﬁc requirements and locate documents that\nare, hopefully, the most relevant. Search engines have\nbecome an integral part of our everyday use of com-\nputers.\n• There are many search engines but no single search\nengine is good enough in all situations. Meta Search\nEngines combine the best of several Search Engines.\nThey provide a common user interface, format queries\nas required for various search engines, ﬁre the search\nengines serially or in parallel either as a foreground\nor as a background process, collect and collate results,\nstore results in a local database for reuse, personalize\nand adapt to individual user’s needs etc. Search en-\ngines are general purpose solutions while meta search\nengines can reside on your computer and they can be\npersonalized to suit your needs. The challenge is to\nbuild user models as well as to decide which search\nengines to ﬁre for what kind of queries. Collating re-\ntrieved results, removing duplicates etc. also require\nsubstantial amounts of intelligence.",
    "personalized to suit your needs. The challenge is to\nbuild user models as well as to decide which search\nengines to ﬁre for what kind of queries. Collating re-\ntrieved results, removing duplicates etc. also require\nsubstantial amounts of intelligence.\n• Expert Systems and Recommender Systems may\nuse expert knowledge to make suitable recommenda-\ntions to users. There are systems, for example, which\nuse a variety of clues (also called heuristics), to sug-\ngest books that particular readers are likely to get in-\nterested in. The challenge is to make good models of\nusers’ needs and preferences and to locate documents\nthat suit these needs, likings and dis-likings.\n• Document Filtering Systems evaluate documents\n6\nCHAPTER 1. THE INFORMATION AGE\naccording to some speciﬁed criteria and decide to ex-\nclude speciﬁc documents accordingly. For example an\nemail ﬁltering system could be designed to ﬁlter out\njunk mails.\nThe challenge is to decide which docu-\nments are relevant and which ones are not.\n• Routing Systems go one step further and automat-\nically route the documents to appropriate agencies for\nfurther action. Here the system must also decide to\nwhich department or individual a document must be\nsent.\nThis requires modelling these individuals and\ndepartments.\n• Information Extraction (IE) Systems attempt to\nanalyze given documents in depth and detail and ex-\ntract structured information. For example, an IE sys-\ntem can process earthquake related documents and au-\ntomatically ﬁll speciﬁed ﬁelds such as date, time, loca-\ntion, magnitude on the Richter scale, extent of dam-\nage to life and property etc. in a database table or an\nXML form. The challenge is to locate speciﬁed ﬁelds\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce",
    "accurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3",
    "Throughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nQuestion Answering Systems\nThe simplest way to ﬁnd out something is to ask. If comput-\ners could understand our questions and give us appropriate\nanswers that would perhaps be the simplest and easiest way\nto access information. After all that is what we do when we\nwant to know something from somebody.\n8\nCHAPTER 1. THE INFORMATION AGE\nOne of the fundamental goals of Natural Language Pro-\ncessing (NLP) is to build computer systems that can un-\nderstand natural language. But no one knows exactly what\nis the meaning of meaning. We do not know how exactly\nmeanings should be represented as a physical symbol sys-\ntem inside a computer. It is not clear exactly what we mean\nby understanding a text. How then do we know if some-\nbody has understood something or not? We cannot dig into\nsomebody’s brain and see the changes before and after a\nsentence has been read and understood. The only way is to\nguage from external behavior. The best bet is to ask ques-\ntions and see how he or she answers. This is what we do in\ntests and examinations. This is what we do in personal in-\nterviews. In fact this is by and large the only general way to\nascertain the level of understanding of somebody on a given\ntopic. Question-answering is a natural way to test under-\nstanding. Naturally, a lot of attention was given to building\nQuestion Answering systems during the early days of NLP.\nQuestion-answering is a very powerful and ﬂexible means\nfor testing the level of understanding. We are free to select\nthe number, nature and order of questions. We may probe\ndeeper by asking follow-up questions. We may give a set of\noptions to chose from. We may ask the candidate to ﬁll gaps,",
    "Question-answering is a very powerful and ﬂexible means\nfor testing the level of understanding. We are free to select\nthe number, nature and order of questions. We may probe\ndeeper by asking follow-up questions. We may give a set of\noptions to chose from. We may ask the candidate to ﬁll gaps,\nmatch alternatives, deﬁne, describe or explain something, we\nmay ask him to give examples and counter-examples. You\nmight have seen a variety of questions in school examination\nquestion papers, entrance tests and personal interviews. We\nmay prompt the candidate with clues and hints. We may\nask diﬃcult questions just to see how he reacts, rather than\nworry about what actually he says. Applied carefully, one\ncan get a fairly clear idea of somebody’s level of understand-\ning through question-answering.\nOne may also ask a candidate to perform some task that\npresumably requires understanding and guage the level of\nunderstanding by his proﬁciency in performing the task. For\n1.3. QUESTION ANSWERING SYSTEMS\n9\nexample, we may ask the candidate to translate the given\ntext into some other language. Such task based evaluations\nare diﬃcult because the overall proﬁciency in performing\nthe task depends not only on proper understanding of the\ngiven text but also on other factors such as proﬁciency in the\ntarget language for translation. Question answering remains\nthe best bet.\nPresumably, one has to understand the meaning of a text\nbefore he or she can answer questions relating to the given\ntext. Naturally there was a great deal of interest in building\nquestion-answering systems in the early days of NLP. Here\nwe sample a few of them just to get a taste of the nature of\nthe problem and the range of ideas that have been tried out.\nWe begin with a very simple yet highly successful system\nthat was however never intended to be an NLP system nor\nwas it designed to answer any questions at all.\nYet this\nsystem has a simple and neat design and knowing this helps",
    "the problem and the range of ideas that have been tried out.\nWe begin with a very simple yet highly successful system\nthat was however never intended to be an NLP system nor\nwas it designed to answer any questions at all.\nYet this\nsystem has a simple and neat design and knowing this helps\nus to understand other NLP systems better.\n1.3.1\nELIZA - The Rogerian Therapist\nThere are many approaches to improving mental health.\nHere we are talking about a particular approach called Roge-\nrian Therapy and a system called ELIZA developed with\nthis approach in mind. Empathy is the foundation of Carl\nRogers’ client-centered therapy (also known as Rogerian ther-\napy). He asserted that empathy alone is healing. A client\ncentered therapist strives to provide an environment of em-\npathy, unconditional positive regard, and acceptance. Ther-\napists are trained to accept the client where they are at the\nmoment. Client-centered therapists consider diagnosis and\ntreatment planning to be much less important than being\nsupportive to the client. Instead they act as an understand-\ning listener, helping the client by providing advice and al-\nternate interpretations to past events only when asked.\nRogerian therapy involves the therapist’s entry into the\n10\nCHAPTER 1. THE INFORMATION AGE\nclient’s unique phenomenological world. In mirroring this\nworld, the therapist does not disagree or point out contra-\ndictions. Neither does he/she attempt to delve into the un-\nconscious. The focus is on immediate conscious experience.\nRogerian therapy is a process of freeing a person and re-\nmoving obstacles so that normal growth and development\ncan proceed and the client can become independent and self-\ndirected. During the course of therapy the client moves from\nrigidly of self-perception to ﬂuidity.\nThe ELIZA system was developed by Joseph Weizen-\nbaum in 1966. ELIZA is a system whose sole purpose is to\nmaintain a conversation with the client. It is not really re-",
    "directed. During the course of therapy the client moves from\nrigidly of self-perception to ﬂuidity.\nThe ELIZA system was developed by Joseph Weizen-\nbaum in 1966. ELIZA is a system whose sole purpose is to\nmaintain a conversation with the client. It is not really re-\nquired to understand what the user says nor is it expected\nto answer his questions. Such a system is very useful for\nrecording and later analyzing the mind of patients with a\nmental problem. When allowed to talk in a free and con-\nducive environment, such patients tend to come close to the\nsource of their problem irrespective of what topic the con-\nversation starts from. Psychiatrists are trained to analyze\nthe conversations and guess the source of the problems and\nhence plan further investigations. Patients are always con-\nscious of what the other person may think of whatever they\nspeak. It can be expected that patients tend to be more\nfree and forthcoming when they are talking to a dumb, face-\nless machine than with doctors or other people. Hence this\nsystem. Here is a transcript of a typical conversation with\nELIZA:\nMen are all alike.\nIN WHAT WAY\nThey’re always bugging us about something\nor the other.\n1.3. QUESTION ANSWERING SYSTEMS\n11\nCAN YOU THINK OF A SPECIFIC EXAMPLE\nWell, my boyfriend made me come here\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I’m depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\nIt’s true. I am unhappy.\nDO YOU THINK COMING HERE WILL HELP YOU\nNOT TO BE UNHAPPY\nI need some help, that much seems certain.\nWHAT WOULD IT MEAN TO YOU IF YOU GOT\nSOME HELP\nPerhaps I could learn to get along with my mother\nTELL ME MORE ABOUT YOUR FAMILY\nFIG 1.1 Sample Transcripts from ELIZA. ELIZA’s\noutput is in all-caps.\nQuite impressive, right? ELIZA was so successful that\neven its developers were said to have started confessing to\nELIZA! There are many implementations that are freely\navailable and the readers may ﬁnd it interesting to play",
    "FIG 1.1 Sample Transcripts from ELIZA. ELIZA’s\noutput is in all-caps.\nQuite impressive, right? ELIZA was so successful that\neven its developers were said to have started confessing to\nELIZA! There are many implementations that are freely\navailable and the readers may ﬁnd it interesting to play\nwith versions of ELIZA. The gnu-emacs editor comes with\na built-in version too. Source codes are also available and\nthose interested in building similar toy NLP systems may\nﬁnd it useful to go through the code.\n12\nCHAPTER 1. THE INFORMATION AGE\nELIZA is, however, an extremely simple system.\nThe\nsole purpose of ELIZA is to maintain a conversation. Since\nELIZA does not have to really understand what the user is\nsaying, it does not have to linguistically analyze the input\nsentences in any great depth or detail. In fact ELIZA does\nnot perform any of the usual NLP tasks such as dictionary\nlook-up, morphological analysis or syntactic analysis.\nIn\nfact a useful by-product of this simplistic approach is that\nELIZA does not complain if you make spelling mistakes or\nyour sentences are grammatically incorrect or incomplete.\nAll that ELIZA does is to look for keywords. ELIZA has\na set of prioritized keywords and for each keyword a set of\ninput-patterns to match the input sentences against and a\nset of output patterns which it uses to generate responses.\nIf many keywords are found, it simply uses the one with\nhighest priority. If no keywords are found, ELIZA has a set\nof default outputs such as “please continue”, “I see”, “tell me\nmore” and so on. To avoid giving a sense of monotony and\nrepetition, it rotates among a set of alternative responses\nfor a given keyword. Thus if you say the same thing again,\nELIZA’s response could be diﬀerent. Many people have re-\nimplemented ELIZA and the more sophisticated versions can\ndetect if you are too curt and reserved or if you try to abuse\nthe system. Some of them try to pull you back into what\nyou were saying earlier to avoid digressing too far.\nThe",
    "ELIZA’s response could be diﬀerent. Many people have re-\nimplemented ELIZA and the more sophisticated versions can\ndetect if you are too curt and reserved or if you try to abuse\nthe system. Some of them try to pull you back into what\nyou were saying earlier to avoid digressing too far.\nThe\ntable below shows a sample of the kind of keyword based\npattern matching system that ELIZA uses. Letters preﬁxed\nwith a question mark are variables that can match parts of\nsentences.\n1.3. QUESTION ANSWERING SYSTEMS\n13\nKey\nPriority\nPattern\nPossible Outputs\nalike\n10\n?X\nIn what way?\nWhat resemblance\ndo you see?\nare\n3\n?X are you ?Y\nWould you prefer it\nif I weren’t ?Y?\n3\n?X are ?Y\nWhat if they were not\n?Y?\nalways\n5\n?X\nCan you think of a\nspeciﬁc example?\nWhen?\nReally, always?\nwhat\n2\n?X\nWhy do you ask?\nDoes that interest you?\nTABLE 1.1 PATTERN MATCHING SYSTEM IN ELIZA\nELIZA’s power is in its simplicity and generality - it can say\nsomething for any input whatever and what it says is usually\nreasonably well connected with the given input. This strength is\nalso its weakness - its shallow, superﬁcial analysis often leads it\nastray. The system is not scalable or expendable to applications\nthat demand deeper and better understanding.\n1.3.2\nEarly NLP Systems\nA large number of NLP systems were developed in the 1960s and\n1970s.\nMany of them were question answering systems.\nThey\nwere all toy systems by today’s standards but it is instructive\nto take a peep at some of them. For the ﬁrst time, researchers\nwere trying to integrate linguistic analysis at various levels ranging\nfrom dictionaries and morphological analysis through rudiments of\nsyntax to discourse analysis and build complete working systems.\nHere we take a quick look at a few of them.\nSTUDENT\nThe STUDENT system was developed by Daniel G Bobrow in\nthe year 1964. This system was designed to solve algebra story\n14\nCHAPTER 1. THE INFORMATION AGE\nproblems from school text books. These problems are expressed in",
    "Here we take a quick look at a few of them.\nSTUDENT\nThe STUDENT system was developed by Daniel G Bobrow in\nthe year 1964. This system was designed to solve algebra story\n14\nCHAPTER 1. THE INFORMATION AGE\nproblems from school text books. These problems are expressed in\nEnglish sentences. STUDENT constructed from the given English\nsentences algebraic equations and solved those equations.\nThe\nfollowing transcripts from the system illustrate the nature of the\ntask:\nQ: The distance from Newyork to Los Angeles is 3000\nmiles. If the average speed of a jet plane is\n600\nmiles per hour,\nfind\nthe time it takes\nto travel\nfrom Newyork to Los Angeles by jet.\nA: THE TIME IT TAKES TO TRAVEL FROM NEWYORK TO\nLOS\nANGELES BY JET IS 5 HOURS\nQ: Bill’s\nfather’s uncle is twice as old as Bill’s\nfather.\n2 years from now Bill’s father\nwill\nbe 3\ntimes\nas old as\nBill. The sum of thier ages is 92.\nFind Bill’s age.\nA: BILL’S AGE IS 8\nFIG 1.2 Sample Transcripts from STUDENT\nThe big thing is not solving these problems - simple solutions\nexist. The diﬃcult part is in understanding the natural language\nsentences. Let us go through one example in some detail to get\nan idea of the nature of problems and solutions.\nQ: If the number of customers Tom gets is twice the square\nof 20 percent of the number of advertisements he runs, and the\nnumber of advertisements he runs is 45, what is the number of\ncustomers Tom gets?\nIn order to simplify further processing, STUDENT breaks\ncomplex sentences into simple sentences called “kernel” sentences.\nThus the given sentence is broken into “The number of customers\nTom gets is twice the square of 20 percent of the number of adver-\ntisements he runs”, “The number of advertisements he runs is 45”\nand “What is the number of customers Tom gets?”. STUDENT\nworks with objects such as numbers, variables and operators. It\n1.3. QUESTION ANSWERING SYSTEMS\n15\nhas to deal with precedence and associativity of operators. Alge-",
    "tisements he runs”, “The number of advertisements he runs is 45”\nand “What is the number of customers Tom gets?”. STUDENT\nworks with objects such as numbers, variables and operators. It\n1.3. QUESTION ANSWERING SYSTEMS\n15\nhas to deal with precedence and associativity of operators. Alge-\nbraic equations can then be formed. For example, the expression\n“twice the square of the number of advertisements he runs” is for-\nmulated as\n(TIMES 2 (EXPT (NUMBER OF ADVERTISEMENTS\nHE/pro RUNS) 2))\nSTUDENT needs to know that “twice” is same as “2 times”\nan “of” is to be interpreted as “times” in some cases but not al-\nways, “is” means “equals” in some cases but not in every case.\nSTUDENT needs to handle anaphoric references.\n“HE” refers\nto “TOM” in the above example. This looks simple but resolv-\ning anaphoric references is not always that easy. A deﬁnite noun\nphrase such as “the line” may refer to one of the previously men-\ntioned lines and STUDENT needs to ﬁgure out which line we are\ntalking about. We often use equivalent expressions and say “twice\nthe sum of the length and width of the rectangle” or just “perime-\nter of the rectangle”. General world knowledge such as “Distance\nequals Speed times Time” is also essential. Thus STUDENT needs\nsome degree of syntactic, semantic and discourse level analysis as\nwell as world knowledge and ability to perform logical deduction.\nSIR\nSIR (Semantic Information Retriever) was developed by Bertram\nRaphael in 1968. SIR was intended to be a prototype “under-\nstanding” machine.\nIt could accept statements in natural lan-\nguage and answer questions based on them.\nSIR used simple\npattern matching techniques. The following transcripts illustrate\nthe system’s capabilities:\n16\nCHAPTER 1. THE INFORMATION AGE\nEvery keypunch operator is a girl.\nI UNDERSTAND\nAny girl is an example of a person.\nI UNDERSTAND\nIs a keypuch operator a person?\nYES\nIs a person a person?\nYES\nIs a person a girl?\nSOMETIMES\nIs a monkey a keypunch operator?\nINSUFFICIENT INFORMATION",
    "16\nCHAPTER 1. THE INFORMATION AGE\nEvery keypunch operator is a girl.\nI UNDERSTAND\nAny girl is an example of a person.\nI UNDERSTAND\nIs a keypuch operator a person?\nYES\nIs a person a person?\nYES\nIs a person a girl?\nSOMETIMES\nIs a monkey a keypunch operator?\nINSUFFICIENT INFORMATION\nFIG 1.3 Sample Transcripts from SIR\nThese transcripts clearly bring out the diﬀerences between hu-\nman beings and machines. We human beings have a great deal of\nworld knowledge and we are capable of very sophisticated com-\nmon sense reasoning. Machines lack both world knowledge and\ncommon sense reasoning. We do not need to be told that a girl is\na person - we know it. We do not need to be given a rule which\nsays an X is a X - we know it. But computers need to be told\neven such “obvious” things. This is why it has not been possible\nto build machines that show human like intelligent behaviour even\ntill date.\nLanguage is not a completely independent and autonomous\nfaculty of the mind - knowledge representation, reasoning, learn-\n1.3. QUESTION ANSWERING SYSTEMS\n17\ning are all inseparably tied up with the language faculty. There\nwas a time when somebody could become a good painter if he had\nsome imagination and skill in handling the brush. To become a\nsuccessful painter today, one must know the physics of light, the\nchemistry of the paint, the biology of the eye, the psychology of\ncolour and many other things. Language must be explored not\nonly from the point of view of linguistics but also from the per-\nspectives of psychology, cognitive science, artiﬁcial intelligence,\nstatistics, mathematics, physics, computer science and engineer-\ning. We have created artiﬁcial boundaries of departments, disci-\nplines and specializations for the sake of convenience but now they\nseem to have become major barriers for scientiﬁc exploration. Sci-\nence today is highly inter-disciplinary. This is perhaps the most\nimportant lesson we have to learn from these early NLP systems.\nPSP2",
    "plines and specializations for the sake of convenience but now they\nseem to have become major barriers for scientiﬁc exploration. Sci-\nence today is highly inter-disciplinary. This is perhaps the most\nimportant lesson we have to learn from these early NLP systems.\nPSP2\nThe PSP2 system was developed by the author in the eighties.\nPSP2 is a simple natural language question answering system in\nthe domain of ﬁnancial investments. It is a keyword based pattern\nmatching system like most other early NLP systems, only the\npattern matching is a bit more sophisticated. We ﬁrst give sample\ntranscripts and then summarize the salient features.\nOnly the\nqueries given by the user and the system’s interpretation are given.\nAfter taking user’s concurrence, the system gives out pre-stored\nresponses. The spelling errors are all intentional.\nUser:\nPSP?\nSystem:\nWHAT IS PSP2 INVESTMENT PLAN ALL ABOUT?\nUser:\nHow muhc should I ivnest?\nSystem:\nWHAT IS THE INITIAL AMOUNT OF INVESTMENT\nIN THE PSP2 PLAN?\nUser:\nIs that the lest?\nSystem:\nWHAT IS THE MINIMUM AMOUNT OF INVESTMENT?\n18\nCHAPTER 1. THE INFORMATION AGE\nUser:\nWhy?\nSystem:\nWHY IS THERE A MINIMUM AMOUNT FOR INVESTMENT\nIN PSP2?\nUser:\nHow many yearrs\nSystem:\nWHAT IS THE PERIOD OF INVESTMENT?\nUser:\nis that the least?\nSystem:\nWHAT IS THE MINIMUM PERIOD OF INVESTMENT?\nUser:\nHow much iterest do I get?\nSystem:\nWHAT IS THE RATE OF INTEREST IN PSP2 PLAN?\nUser:\nCan I take back my money whenver I want?\nSystem:\nIS THERE A FACILITY FOR PREMATURE\nWITHDRAWALS?\nUser:\nWhere should I sign on the check?\nSystem:\nTELL ME MORE ABOUT PSP2 CHEQUES.\nFIG 1.4 Sample Transcripts from PSP2\nPSP2 accepts free format natural language queries and pro-\nduces natural language responses using a bank of pre-stored fre-\nquently asked questions and corresponding answers. These stored\nquestions and answers are provided by human experts and can\ntherefore be taken as accurate and reliable.\nThe only issue is\nhow to understand the users’ queries and map them to the closest",
    "quently asked questions and corresponding answers. These stored\nquestions and answers are provided by human experts and can\ntherefore be taken as accurate and reliable.\nThe only issue is\nhow to understand the users’ queries and map them to the closest\nstored question. If this could be done correctly, the system can\ngenerate appropriate responses.\nPSP2 answers questions posed by potential customers of a\nﬁnance company. It would not be acceptable for PSP2 to give\n1.3. QUESTION ANSWERING SYSTEMS\n19\nincorrect or inappropriate answers. The answers better be cor-\nrect. Since the answers themselves are given by human experts\nand stored in the machine, this translates to the requirement that\nPSP2 must understand the user’s question correctly. In order to\nbe sure that the users’ questions have been properly understood,\nPSP2 comes back and asks the user to conﬁrm its interpretation.\nContrary to initial expectation, this does not cause much of an-\nnoyance because the conﬁrmation questions are reformulated by\nthe system and the reformulations are often technically more pre-\ncise. Thus “How many years?” is reformulated as “What is the\nperiod of investment?”. Users may actually feel glad that their\nquery has been accurately understood by the machine. The sys-\ntem’s interpretations are usually correct but what if they are not?\nPSP2 generates a ranked list of matching queries and it can give\nseveral options in ranked order for the user to simply choose from.\nOne or the other interpretation can be generally expected to be\nalright. Users are required to re-state their queries in diﬀerent\nterms only if none of the possible interpretations match.\nPattern matching has to be more powerful here than the sim-\nplistic methods we saw in the case of ELIZA. There are many\ndiﬀerent ways the same thing can be expressed and PSP2 needs\nto handle unrestricted, free format queries. Order of keywords\nis important because “I am interested in withdrawing so and so",
    "plistic methods we saw in the case of ELIZA. There are many\ndiﬀerent ways the same thing can be expressed and PSP2 needs\nto handle unrestricted, free format queries. Order of keywords\nis important because “I am interested in withdrawing so and so\namount” is not the same as “How much interest do you charge for\nthe amounts withdrawn”. Simply listing keywords such as “inter-\nest”, “withdraw” and “amount” with priorities is not suﬃcient.\nPSP2 uses not just keywords but “patterns” of keywords. Patterns\ncan specify mandatory, desirable and optional parts. Patterns can\nspecify which parts are order-sensitive and which parts can ap-\npear in any order. Some key words may be required to appear\nnext to each other while others may be permitted to appear with\nother words in between. Logical connectives such as AND, OR\nand NOT can be used. Patterns are constructed hierarchically,\nstarting from individual keywords and gradually building more\nand more complex patterns according to speciﬁed rules. There is\nthus a “grammar” of patterns. In fact PSP2 comes with an auxil-\niary program called PATGEN to assist developers in interactively\ndeveloping patterns according to the rules of the system.\nThere can be several patterns for each stored question.\nA\ngiven query may thus match several patterns in several queries.\n20\nCHAPTER 1. THE INFORMATION AGE\nPatterns are ranked according to degree of match.\nIf nothing\nmatches at all PSP2 has a set of default outputs. It may say for\nexample, “I am sorry, I may not be able to help you this time.\nPlease contact so and so”. Also, as in the case of ELIZA there are\nmultiple output formulations and outputs are selected at random,\navoiding repetitions to reduce monotony.\nPSP2 remembers the previous questions and corresponding\nconﬁrmations. While performing pattern matching, patterns from\nprevious questions are also included, appropriately weighted to en-\nsure that patterns in the current query have more weightage than\npatterns in the past queries.",
    "PSP2 remembers the previous questions and corresponding\nconﬁrmations. While performing pattern matching, patterns from\nprevious questions are also included, appropriately weighted to en-\nsure that patterns in the current query have more weightage than\npatterns in the past queries.\nMore recent queries have higher\nweightage compared to queries in the distant past. This enables\nPSP2 to interpret queries in the context of previous conversation.\nThus it can understand just a “why” as “why is the maximum\nperiod of investment only 9 years” or “why is there a limit on\nthe minimum amount I can invest” based on the context of the\nprevious conversation. Thus follow up questions need not be com-\nplete and self-contained in themselves. This ability to interpret\nqueries in context makes interaction with PSP2 a lot more natu-\nral than other key-word based pattern matching systems we have\nseen before.\nPSP2, like other early NLP systems, does not perform very\ndetailed linguistic analysis of the input queries.\nMorphological\nanalysis with dictionary look-up and a limited analysis of local\nstructure is all that is performed. User queries need not be gram-\nmatically correct or complete.\nHandling ungrammatical inputs\ncan be considered to be a virtue of these simple NLP systems,\nnot easy to realize in much more sophisticated systems. However,\nthis limited, shallow linguistic analysis is also a major source of\nweakness and brittleness of such systems. It is easy to fool the\nsystem and the system does grossly mis-interpret user queries now\nand then.\nPSP2 also has a built-in spelling error detection and correction\nsystem.\nNon-keywords of course do not matter but PSP2 can\ntolerate spelling errors even in the keywords. The spelling errors\nin the transcripts above are intended, observe them carefully.\nThe power of NLP systems such as PSP2 comes from their\nsimplicity. This simplicity is also their main weakness. Programs\nsuch as PSP2, STUDENT and SIR are simplistic, superﬁcial, lim-",
    "in the transcripts above are intended, observe them carefully.\nThe power of NLP systems such as PSP2 comes from their\nsimplicity. This simplicity is also their main weakness. Programs\nsuch as PSP2, STUDENT and SIR are simplistic, superﬁcial, lim-\nited and ad-hoc. They cannot be scaled to real life, wide coverage,\n1.3. QUESTION ANSWERING SYSTEMS\n21\nrobust applications nor can they be easily adapted or retrained\nfor diﬀerent application domains. These systems surely generated\nenough initial interest and curiosity at their times. In fact such\ntoy systems are being developed and used even today, albeit in\na more sophisticated form with animated human faces and voice\noutput. The only purpose they serve is to generate some curios-\nity. The big question remains - can machines really understand\nlanguage at all?\n1.3.3\nFoundations of Story Understanding\nDuring the seventies, Roger Schank and his team at Yale Uni-\nversity were focussing on the foundations of story understanding.\nConceptual Dependency (CD) theory, Scripts, Plans and Goals\nwere some of the ideas that came out of this eﬀort.\nConceptual Dependency is a system designed to represent the\nmeanings of sentences by decomposing them into a small number\nof primitive “acts”. In CD theory, sentences with identical mean-\ning should have the same underlying conceptual representation,\nregardless of the diﬀerences in grammatical form or the language\nused. In this sense, CD was a kind of an inter-lingua. A basic\npremise of the Cd theory is that meaning arises from a combi-\nnation of memory search, planning and inference. Only a small\nfraction of meaning is actually conveyed directly by the lexical\nitems which explicitly appear in a given sentence. For example,\nif we read the sentence “John bought a television” we understand\nmany things - that John probably bought the TV in a store, there\nwas another person in the store, John gave him money, that per-\nson gave John the TV, the person who sold the TV no longer",
    "if we read the sentence “John bought a television” we understand\nmany things - that John probably bought the TV in a store, there\nwas another person in the store, John gave him money, that per-\nson gave John the TV, the person who sold the TV no longer\npossesses that TV, John bought the TV in order to watch the\nshows probably for his own enjoyment, John will plug in the TV\nat home, and so on. Although we can never be sure about such\ninferences, understanding texts necessarily involves a large num-\nber of such inferences. We will have to make such assumptions\nand inferences, even if some of them need to be changed later\non in the light of new information coming in. Note that our un-\nderstanding of the sentence is going to be same or similar if the\nsentence were to read “A TV set was sold to John”. CD theory\nproposed that any sentence in any language can be broken down\nand expressed in terms of eleven primitive acts. Each of these acts\n22\nCHAPTER 1. THE INFORMATION AGE\nhad an associated set of cases such as the actor, the recipient and\nthe instrument. These case-frames encapsulated expectations for\nthe conceptualizations being built. Such expectations are useful\nfor word sense disambiguation, pronoun reference etc. too. The\nsentence above could be conceptualized in Cd as\n(ATRANS\nACTOR\nJohn\nOBJECT\nmoney\nFROM\nJohn\nTO\nindividual)\n(ATRANS\nACTOR\nindividual\nOBJECT\nTV\nFROM\nindividual\nTO\nJohn)\nHere ATRANS stands for transfer of possession. The other\nprimitive acts in CD theory were PROPEL (the application of\nphysical force - throwing, hitting, falling, pulling, kicking, etc.),\nPTRANS (transfer of physical location - driving, ﬂying, taking a\nbus, walking, etc.), INGEST (AN organism taken something from\noutside environment and makes it internal - breathing, eating,\nsmoking, etc.), EXPEL (opposite of INGEST - sweating, crying,\ndefecating, spitting, etc.), MTRANS (transfer of mental infor-\nmation from one individual to another - speaking, reading, etc.),",
    "outside environment and makes it internal - breathing, eating,\nsmoking, etc.), EXPEL (opposite of INGEST - sweating, crying,\ndefecating, spitting, etc.), MTRANS (transfer of mental infor-\nmation from one individual to another - speaking, reading, etc.),\nMBUILD (thought processes which create new conceptualizations\nfrom old ones - deciding, concluding, realizing, considering, imag-\nining, etc.), MOVE (movement of a body part of some animate\norganism - waving, dancing, jumping, etc.), GRASP (the act of\nphysically contacting an object, usually by MOVE arm/hand -\ngrabbing, holding, hugging, etc.), SPEAK (vocalization), and AT-\nTEND (the act of directing a sense organ - hearing, etc.).\nCD theory was considered adequate for representing mun-\ndane physical actions. However, most actions are part of larger\nplans in service of higher-level goals, and much of the compre-\nhension process involves recognizing what these plans and goal\nare. These goals were classiﬁed as S-GOALS (satisfaction goals\nfor satisfying recurring bodily needs such as sex, hunger, sleep\nand thirst), D-GOALS (delta goals standing for a desire for a\n1.3. QUESTION ANSWERING SYSTEMS\n23\nchange of state - mental states (D-KNOW), physical proximity (D-\nPROX), or desire to gain control of something (D-CONT) or some-\none (D-SOCCONT) as in kidnapping), E-GOALS (entertainment\ngoals such as E-COMPANY, E-TRAVEL and E-EXERCISE), A-\nGOALS (achievement goals such as A-GOOD-JOB, A-SKILL)\nand P-GOALS (preservation goals such as P-APPEARANCE, P-\nCOMFORT, P-HEALTH, and P-FINANCES.).\nFor each goal, there are a number of plans which may be used\nto achieve it. For example, you wish to get control over some ob-\nject, you may ASK, INFORM-REASON, BARGAIN, THREATEN,\nOVERPOWER or STEAL. Plans have preconditions. You have\nto be in physical proximity to ask or you must a communication\ndevice such as a phone. In order to steal some object, you must\ngo near that object (not the owner!)",
    "ject, you may ASK, INFORM-REASON, BARGAIN, THREATEN,\nOVERPOWER or STEAL. Plans have preconditions. You have\nto be in physical proximity to ask or you must a communication\ndevice such as a phone. In order to steal some object, you must\ngo near that object (not the owner!)\nDetailed planning is however not always required. There are\nstereotypical situations where a lot of details are generally known.\nTaking a train involves going to the station, buying the ticket,\nreaching the platform, waiting for the train, entraining, looking\nfor a seat, detraining at the destination station, exiting the station\netc. Buying groceries, going for a movie, going to a restaurant or a\nmarriage party can all be visualized this way. Scripts capture the\nfull details of such stereotypical scenarios. Nobody writes down\nin minute detail all that happened and if at all one tries, that\nwould be the most boring story. Writers write only the interest-\ning and signiﬁcant aspects and readers ﬁll in the rest by their\ncommonsense knowledge. Scripts can be used to do this ﬁlling in.\nThe Yale school made signiﬁcant contributions along these\nlines and built many systems. But the big questions remained.\nHow do we make computers understand language in a way we\nhumans can do?\n1.3.4\nIn-Depth Understanding\nAll the systems we have seen above are toy systems. They can-\nnot be easily scaled up to real systems.\nNone of them answer\nserious questions about the feasibility of developing real question-\nanswering systems. Can we, even in principle, build systems that\ncan answer questions like human beings can? Can we build sys-\ntems that can read, understand and then answer questions? In\nearly 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-",
    "early 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-\nsible for computers to demonstrate in-depth understanding and\nanswer complex questions in a human-like manner. Let us look at\nsamples of actual transcripts from BORIS:\nRichard hadn’t heard from his college roommate Paul\nfor years.\nRichard\nhad borrowed\nmoney from\nPaul,\nwhich was\nnever paid back,\nbut now he had no idea\nwhere to find his old friend. When a letter finally\narrived from San Francisco,\nRichard was anxious to\nfind out how Paul was.\nQ What happened to Richard at home?\nA Richard got a letter from Paul\nQ Who is Paul?\nA Richard’s friend\nQ Did Richard want to see Paul?\nA Yes. Richard wanted to know how Paul was\nQ Had Paul lent money to Richard?\nA Yes. Paul had lent money to Richard\nQ Why didn’t Richard pay Paul back?\nA Richard did not know where Paul was\nQ How did Richard feel when the letter appeared?\nA Richard felt glad because Paul and he were friends\nFIG 1.5 Sample Transcripts from BORIS\n1.3. QUESTION ANSWERING SYSTEMS\n25\nThis sample transcript vividly brings out the depth of under-\nstanding by BORIS. It is not straight forward to answer questions\nsuch as what happened to somebody somewhere. Many things\nmight have happened, some of them may be explicitly given to\nus in the story while some we can infer from our world knowl-\nedge of what “typically” happens in given kinds of situations.\nBut everything that happened or could have happened would not\nbe interesting. Something unusual, something special, something\nworth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was",
    "worth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was\nRichard’s friend, BORIS would need to know what colleges are\nand what exactly college room-mate means. It is easier to answer\nquestions on what happened to whom, where, when etc. literally\nbut assessing mental states and reactions of people in diﬀerent\nsituations is a diﬀerent matter. That BORIS could understand\nthat Richard wanted to know how Paul was is surely a human-\nlike response. BORIS knows that if X lends money to Y, that is\nthe same as Y borrowing money from X.\nThe point is not whether the answers are correct or not. Of-\nten there are no correct or wrong answers, only more appropriate\nand less appropriate ones.\nIf somebody asks you where is Taj\nMahal what would you say? If you are in Europe you would per-\nhaps say that Taj Mahal is in India but if you are in some part\nof India you would probably say that it is in Agra. If you happen\nto be already in Agra then you are perhaps expected to say how\nexactly to reach Taj Mahal from wherever you are. An precise\ndescription of the location in terms of latitudes and longitudes\nmay be mathematically more accurate but it may not serve the\npurpose. Appropriateness of answers depends upon the situation\nand understanding situational context is as much a part of natural\nlanguage understanding as all the linguistic processing. Speakers\nhave mental models of listeners and vice versa. To answer some-\nbody you ﬁrst need to understand what exactly he or she wants\nto know. Literal interpretation of given texts does not constitute\nin-depth understanding.\nWith many more examples of this kind, Dyer in his thesis tries\nto convince the reader that in-depth understanding and human-\nlike question answering behaviour is very much possible for com-\n26",
    "to know. Literal interpretation of given texts does not constitute\nin-depth understanding.\nWith many more examples of this kind, Dyer in his thesis tries\nto convince the reader that in-depth understanding and human-\nlike question answering behaviour is very much possible for com-\n26\nCHAPTER 1. THE INFORMATION AGE\nputers. Then why is it that we still do not have computers that\nwe can communicate with in natural language? Why are we still\nstuck with complex programming languages with so little expres-\nsive power? Why don’t we still have “intelligent” computers as-\nsisting us in all walks of life?\nIn-depth understanding requires great amounts of world knowl-\nedge. It is all much more to do with commonsense reasoning than\nwith merely linguistic analysis or straight forward mathematical\nlogic.\nDyer’s purpose was to show that given all the required\nknowledge, represented in suitable structures, computers could be\nexpected to give human-like performance in natural language un-\nderstanding. If every piece of knowledge required was available,\ncomputers could perhaps think like us. But this “if” is a very big\n“if”. Dyer could, over a period of several years, carefully hand-\ncode all the required knowledge for demonstrating in-depth under-\nstanding in a limited set of stories. We cannot possibly hand-code\nsuch vast and complex knowledge structures for real applications\nspanning across a wide range of topics and situations. Nor can\nmachines learn such knowledge structures automatically. Com-\nmonsense is what we have learnt automatically by mere contact\nwith the real world. Commonsense is what cannot be explicitly\ntaught or learnt. We still do not know how exactly we acquire\ncommonsense knowledge. We still do not know how to make com-\nputers acquire commonsense knowledge.\nBORIS demonstrated that in-depth understanding was possi-\nble provided all relevant knowledge could be obtained and prop-\nerly represented in the computer. However, the big question that",
    "commonsense knowledge. We still do not know how to make com-\nputers acquire commonsense knowledge.\nBORIS demonstrated that in-depth understanding was possi-\nble provided all relevant knowledge could be obtained and prop-\nerly represented in the computer. However, the big question that\nremains is how to make the computer obtain all relevant knowl-\nedge. Thus fully automated in-depth understanding has remained\na distant dream even today.\n1.3. QUESTION ANSWERING SYSTEMS\n27\n1.3.5\nTuring Test\nINTERROGATOR\nHUMAN BEING\nMACHINE\nSWITCH\nQUESTION\nANSWER\nANSWER\nANSWER\nFIG 1.6 Turing Test\nIn 1950 Alan Turing devised a test for the thinking ability of\nmachines. An interrogator asks questions and gets answers from\neither a human being or a machine, sitting in diﬀerent rooms, and\ncommunicating only through typed messages. If the interrogator\nis unable to tell the man from the machine purely based on the an-\nswers he gets for his questions, we may conclude that the machine\nis at least as intelligent as the human being. The interrogator is\nfree to ask any question and the machine is allowed to do whatever\nit wants to prove its case. For example, given a simple arithmetic\nproblem the machine may take a long time and answer wrongly.\nHere is a hypothetical conversation given by Turing:\nINTERROGATOR: In the first line of your sonnet which\nreads\n‘‘Shall I compare\nthee\nto a summer’s\nday’’,\nwould not ‘‘a spring day’’ do as well or better?\nA: It woudn’t scan.\nINTERROGATOR:\nHow\nabout\n‘‘a winter’s day’’?\nThat\nwould scan alright.\nA:\nYes,\nbut\nnobody\nwants\nto\nbe\ncompared\nto a\nwinter’s day.\nINTERROGATOR:\nWould\nyou say\nMr. Pickwick\nreminded\nyou of Christmas?\n28\nCHAPTER 1. THE INFORMATION AGE\nA: In a way.\nINTERROGATOR: Yet\nChristmas\nis a winter’s\nday, and\nI do not think Mr. Pickwick would mind the comparison.\nA: I don’t think\nyou are serious.\nBy a winter’s day\none\nmeans a\ntypical\nwinter’s\nday,\nrather\nthan a\nspecial one like Christmas.\nFIG 1.7 A Hypothetical Conversation",
    "A: In a way.\nINTERROGATOR: Yet\nChristmas\nis a winter’s\nday, and\nI do not think Mr. Pickwick would mind the comparison.\nA: I don’t think\nyou are serious.\nBy a winter’s day\none\nmeans a\ntypical\nwinter’s\nday,\nrather\nthan a\nspecial one like Christmas.\nFIG 1.7 A Hypothetical Conversation\nIf the “A” in the above conversation were actually a machine\nrather than a human being, perhaps we can accept that the ma-\nchine was intelligent and capable of thinking and understanding.\nOf course there is no machine today that can pass the Turing\nTest and nobody knows if we will ever be able to develop one.\nIt is interesting to note that question answering based on natural\nlanguage understanding and generation has been used as a test\nof human-like intelligence. Indeed language is at the very core of\nhuman intelligence.\nNLP started oﬀwith simple ideas and toy systems. Over the\nyears, researchers have discovered how complex natural languages\nare. Focus has shifted from building toy systems to developing\nlarge scale linguistic data resources, wide coverage grammars and\nparsers etc. Statistical analysis and machine learning techniques\nare being combined with linguistic approaches. Scalability and\nadaptability or trainability have now become very important is-\nsues. Many useful applications have emerged. Yet we are far from\nthe ultimate goals of NLP - human-like understanding, genera-\ntion and language learning by machines. There are no question-\nanswering systems today that we can reliably use for accessing\nwhatever kind of information we may need.\n1.4\nInformation Retrieval\nIn this section we sketch the rudiments of modern IR systems.\nThe ideas we describe here will form the background for more\nadvanced techniques we will be looking at in Chapter Three.\n1.4. INFORMATION RETRIEVAL\n29\n1.4.1\nIR Deﬁned\nInformation Retrieval is a vast ﬁeld concerned with the storage\nand retrieval of documents. Documents may include texts, im-\nages, video, speech, music and web pages in various combinations.",
    "advanced techniques we will be looking at in Chapter Three.\n1.4. INFORMATION RETRIEVAL\n29\n1.4.1\nIR Deﬁned\nInformation Retrieval is a vast ﬁeld concerned with the storage\nand retrieval of documents. Documents may include texts, im-\nages, video, speech, music and web pages in various combinations.\nProcessing and retrieval of images, video, speech, music and rich\nmulti-media documents has become an increasingly active area of\nresearch in recent times. Nonetheless, text remains the most ba-\nsic, ubiquitous and the most widely used medium of representing\nand communicating information. Text documents take much less\nspace to store and less network bandwidth to move across than\npictures. After all, a picture is worth a thousand words! Here we\nwill mainly look at the issues relating to storage and subsequent\nretrieval of text documents in response to user queries.\nOf particular interest will be what is termed ad hoc retrieval.\nHere an unaided user formulates a query and requests for relevant\ndocuments. The system searches the collection and returns a pos-\nsibly ordered set of potentially useful documents. For example,\nyou may go to a search engine, type “computer” and press the\nsearch button. The search engine comes back and says it found\nso many million web pages that seem to be relevant to your query\nand it also displays the links to the top few that it thinks are the\nmost relevant. A good system will be expected to return most of\nthe relevant documents in the collection and few irrelevant docu-\nments.\nA Document is a unit of text that is indexed and retrieved.\nA document may be an article, a research paper, a whole book,\na chapter or section from a book, or even single sentences. In a\ntraditional library setting, a document may be a book while on the\nInternet a document may be just a single web page. A Collection\nis a set of such documents. A Query refers to a short formulation\nof a user request in a suitable format.\nSEARCH ENGINE\nCRAWLER\nINDEX\nDOCUMENT\nREPOSITORY\nQUERY",
    "traditional library setting, a document may be a book while on the\nInternet a document may be just a single web page. A Collection\nis a set of such documents. A Query refers to a short formulation\nof a user request in a suitable format.\nSEARCH ENGINE\nCRAWLER\nINDEX\nDOCUMENT\nREPOSITORY\nQUERY\nRESPONSE\nUSER\nFIG 1.8 Information Retrieval\n30\nCHAPTER 1. THE INFORMATION AGE\n1.4.2\nDocuments as Bags-of-Words\nThe meaning of a text is indicated by the meanings of the words\nused as also by the syntactic, semantic and pragmatic structures\ninto which these words are placed in a coherent framework. Nev-\nertheless, it is very common in IR systems to take the extreme\nview that texts are merely unordered sets of words, without any\nregard to syntactic, semantic or discourse structure. Thus ’I eat,\ntherefore I am’ and ’I am, therefore I eat’ will be treated as equiv-\nalent! This representation is called the bag-of-words representa-\ntion and has been widely used, despite its obvious limitations. In\nthe bag-of-words representation the document texts as well as the\nuser queries are represented as unordered collections of words and\nword-like features such as phrases, together known as terms.\nQueries may be simple or they may be structured query ex-\npressions involving Boolean operators such as AND, OR and NOT.\nYou may ask, for example, for documents containing the terms\n“Data” AND “Mining” but NOT “Coal” OR “Gold”. If exact\nmatch is expected, there are chances of getting too few or too\nmany matching documents, especially when the document collec-\ntions are large and heterogeneous. The queries may be simply too\ngeneral or too very speciﬁc. Recent research has therefore focused\non probabilistic models where documents are ranked according to\ntheir estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.",
    "their estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.\nIn the bag-of-words approach, features corre-\nspond to the terms - each term (word or phrase) is a potential\nfeature.\nFeatures are given numerical values.\nIn the simplest\ncase, the feature values are Boolean - that is, a value is 1 if the\ncorresponding term occurs in the document or the query as the\ncase may be, and 0, otherwise. Alternatively, the numerical value\nof a feature can be simply the number of times it occurs in the\ndocument. Each vector is thus simply a list of numbers.\nA vector of n such features can be geometrically viewed as\na point in n-dimensional space. The geometric spatial proximity\nbetween two vectors is used as a metaphor for the semantic prox-\n1.4. INFORMATION RETRIEVAL\n31\nimity between the corresponding documents and/or query strings.\nThis model is thus conceptually simple and appealing. The most\nrelevant documents are the ones that include the most terms from\na given query and thus spatially closest to the query vector. The\nexample below illustrates these ideas in just two dimensions. If\nthere were three terms, we need to consider vectors in three dimen-\nsional space and, by extension of this idea, if there are n terms, the\nvectors will be in n-dimensional space - diﬃcult to get a mental\npicture but mathematically a simple extension all the same.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 1.9 Vector-Space Model\nThe direction of the vectors is a good indicator of the semantic\ncontent of the corresponding documents, not the length of the\nvectors. Hence it is usual to normalize the lengths of all vectors\nto unity and consider only the directions.\n1.4.4\nPerformance Evaluation\nPerformance of IR systems is measured in terms of Recall and",
    "content of the corresponding documents, not the length of the\nvectors. Hence it is usual to normalize the lengths of all vectors\nto unity and consider only the directions.\n1.4.4\nPerformance Evaluation\nPerformance of IR systems is measured in terms of Recall and\nPrecision. Recall gives the proportion of relevant documents re-\ntrieved. Of course we may get 100% Recall by simply retrieving\nall the documents in a collection! Thus we must also look at the\nproportion of retrieved documents that are relevant. This latter\nquantity is called Precision. There is a clear trade-oﬀbetween pre-\ncision and recall. We can increase precision by being very choosy\nand retrieve only those documents we think are surely relevant.\nRecall comes down. Alternatively, we can try to retrieve as many\nhopefully relevant documents as possible, possibly bringing down\nthe precision. The challenge is to maximize both Precision and\n32\nCHAPTER 1. THE INFORMATION AGE\nRecall so that we get most of the relevant documents and very\nfew irrelevant documents.\n1.4.5\nMeasuring Relevance\nPerformance measures are dependent on the notion of relevance.\nHow do we know a document is relevant or not? A document’s\nrelevance must be measured in relation to a user’s need at a given\npoint of time. There may be several requirements. The docu-\nment’s topic or aboutness must correspond to the user’s needs.\nThe document should serve the intended use or purpose.\nThe\ndocument should be novel in relation to what the user already\nknows. It should be recent. It should be authentic and accurate.\nIt should be at the right level of abstraction and easily under-\nstandable by the user. The meaning of a text is not ﬁxed and\nunchangeable. Meaning depends upon the reader’s interest, back-\nground, purpose, attitudes etc. Relevance is not a simple yes-no\nquestion. We can therefore think of degree of relevance. No sin-\ngle document may be highly relevant but a combination of two or\nmore may be.",
    "unchangeable. Meaning depends upon the reader’s interest, back-\nground, purpose, attitudes etc. Relevance is not a simple yes-no\nquestion. We can therefore think of degree of relevance. No sin-\ngle document may be highly relevant but a combination of two or\nmore may be.\nClearly, such a strong view of relevance is subjective and im-\npracticable for automatic evaluation. Hence in practice a weaker\ndeﬁnition involving only the topical relevance is used as an indica-\ntor of potentially useful documents. Topical relevance is necessary\nbut not suﬃcient. It is easier to deal with and often a major con-\ntributor to total relevance. Topical relevance is more closely tied\nup with the document itself and not so much to idiosyncrasies\nof individual users. The relevance of documents is measured by\nsimply counting the proportion of terms in the query which are\nfound in the documents retrieved. The retrieved documents are\nranked accordingly.\nWhat we have seen in this section is the bare-bones descrip-\ntion of a modern IR system. It is deﬁcient in many ways and a\nlarge number of ideas have been proposed and used to go beyond\nthis primitive design. Chapter Three of the book is devoted to ad-\ndressing these concerns in some detail and Chapter Two provides\nthe required background in Natural Language Processing.\n1.4. INFORMATION RETRIEVAL\n33\n1.4.6\nChallenges in Information Retrieval\nIn his Turing Award lecture, Jim Grey deﬁned the Software Grand\nChallenge as a software that could answer questions as eﬀectively\nas an educated person. Answering questions, or even just retriev-\ning relevant documents from which we can hopefully ﬁnd answers\nto our speciﬁc questions, is not easy. There are three steps in the\nprocess and each one is a challenge - 1) understand exactly what\nthe user wants 2) understand the contents of the documents so\nyou know which document is relevant for what, and 3) develop\nautomatic methods for matching the user requirements with the",
    "process and each one is a challenge - 1) understand exactly what\nthe user wants 2) understand the contents of the documents so\nyou know which document is relevant for what, and 3) develop\nautomatic methods for matching the user requirements with the\ncontents of the documents available in the collection.\nA search for “veda” from Google got about 1,420,000 matches.\nThe top ten included some relevant entries but also the following\ncompletely irrelevant entries: 1) “The Vestibular Disorders Asso-\nciation (VEDA)...”, 2) “Veda Hille’s Home Page...”, 3) “Veda rent\na car Soﬁa Bulgaria...” and 4) “Armour Home Electronics - HiFi,\nhome cinema and whole house custom ...”. A search engine has\nno clue as to whether I am looking for renting a car or I am a doc-\ntor looking for information about some types of diseases or I am\nactually interested in knowing about ancient Indian traditional\nknowledge sources. IR today is more like a blind man groping in\ndarkness rather than like an intelligent being performing a sys-\ntematic informed search or exploration.\nWe just saw that four out of ten of the top hits were irrelevant.\nThat gives us only a 60% performance.\nAn IR system giving\nsixty percent performance is useable and people do use it. The\nparticular search example took only 0.23 seconds and the user\ncan selectively dig deeper and get some useful information in a\nmatter of minutes or hours. Imagine doing the same thing without\nsuch a technology. A machine translation system that gives 90%\naccuracy may be considered hopeless and utterly unuseable but\nan IR system which gives even 40% performance is still useful,\nalthough far from being completely satisfactory.\nIt appears, therefore, that IR requires a great deal of Natural\nLanguage Understanding. However, some researchers in the IR\nﬁeld have traditionally considered it neither absolutely essential\nnor always highly beneﬁcial to carry out in depth linguistic anal-\nysis of documents or user queries. The challenge they have set for",
    "Language Understanding. However, some researchers in the IR\nﬁeld have traditionally considered it neither absolutely essential\nnor always highly beneﬁcial to carry out in depth linguistic anal-\nysis of documents or user queries. The challenge they have set for\nthemselves is to achieve high levels of retrieval performance with-\n34\nCHAPTER 1. THE INFORMATION AGE\nout recourse to Natural Language Processing (NLP) in any great\nmeasure. Empirical studies of the use of advanced NLP techniques\nhave also given mixed results - in some cases there was some im-\nprovement in performance while in other cases either there was no\nsigniﬁcant improvement or there was actually a small reduction in\nperformance. This could be for various reasons including possibly\nthe kind of linguistic analysis that was carried out and the speciﬁc\nIR tasks and evaluation methods employed.\nPart of the reason for diﬀerences of opinion on the role of\nNLP is the criteria for success. What is it that we want in the\nend? Is the performance measured in terms of Precision, Recall\nor whatever the only criterion for success? Or are we looking for\nintelligent IR systems, intelligent IE systems and so on? In the\nlong run, speciﬁc applications such as IR, IE, Categorization and\nSummarization must be viewed in the context of intelligent pro-\ncessing of human languages by machines. If we can move towards\nmachines that are capable of human-like understanding, generat-\ning and learning natural languages, we can not only get better\nIR systems but also many other applications that have not taken\noﬀyet. Automatic Programming has long remained unsuccess-\nful. If one day we could tell computers instead of program them,\nthe whole world of software engineering will change dramatically.\nResearch should not be constrained too much by forces of pro-\nfessionalism. Asking the right questions is more important than\nbeing successful all the times.\nThe bag of words representation is too crude. Documents are",
    "the whole world of software engineering will change dramatically.\nResearch should not be constrained too much by forces of pro-\nfessionalism. Asking the right questions is more important than\nbeing successful all the times.\nThe bag of words representation is too crude. Documents are\nnot unordered collections of words. ’Ram killed Ravan’ is not the\nsame as ’Ravan killed Ram’! Words have several meanings and we\ncannot aﬀord to equate the various usages and senses. Landing\nof an aeroplane, landing in a staircase and agricultural land are\nall very diﬀerent kinds of land. So are banking of a road near a\ncurve, banking on something to achieve a goal, a bank of ﬁlters,\nbank of a river and the ﬁnancial institution where you deposit\nyour money. Medical case, legal case, suit case, jewel case and\njust in case are not all exactly the same case.\nThere is a lot\nthat can and should be done to move towards a more intelligent\nInformation Retrieval system. Natural Language Processing and\nComputational Linguistics have a signiﬁcant role to play in mov-\ning towards intelligent Information Retrieval. These shall be the\nrecurring themes throughout this book.\n1.5. INFORMATION EXTRACTION\n35\n1.5\nInformation Extraction\n1.5.1\nWhat is Information Extraction?\nHandling large volumes of information represented as natural lan-\nguage texts has become essential today in various walks of life.\nUnlike databases which are structured into records, ﬁelds and so\non, natural language texts show very little explicit structure. For\na computer which really cannot read and understand the meaning\nof natural language sentences, texts appear to be simply sequences\nof words. If we could somehow make computers dig into the hid-\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer",
    "den structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a",
    "TABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nnatural language text, and extraction of the relevant arguments of\nthose events and relationships. IE systems produce a structured\nrepresentation such pieces of information extracted from the given\ntexts. These structured databases can in turn be used for anal-\nysis using conventional database query methods and data-mining\ntechniques, for generating natural language summaries, question-\nanswering, intelligent information retrieval, text indexing and so\non.\n1.5.2\nInformation Extraction Tasks\nIn the mid 1980’s a number of research sites in the United States\nwere working on IE from naval messages, in projects sponsored\nby the Defense Advanced Research Projects Agency (DARPA). In\norder to understand and compare the behavior of such systems,\na number of these message understanding (MU) projects decided\nto work on a set of common messages and then convene to see\nhow their systems perform for some new, unseen messages. This\ngathering constituted an ongoing series of extremely productive\nmessage understanding conferences (MUCs), which have served\nas key events in driving the ﬁeld of IE forward. Because of the\ncomplexity in information extraction task, MUCs divided IE into\ndiﬀerent tasks and then evaluated the performance of IE tasks\nseparately. The IE tasks deﬁned in MUC-7 are:\n1.\nNamed Entity Task: The “NE task” involves ﬁnding and\ncategorizing certain classes of proper names that appear in\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.",
    "1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\nCoreference Task: The “Co task” involves ﬁnding and\nlinking together all references of the same object, set or\nactivity. This task is important, because it helps in proper\nacquisition of attributes and relations between the IE task\nentities.\n3.\nTemplate Element Task: The “TE task” builds on NE\nand Co tasks. The goal of this task is to ﬁnd entities and\nidentify certain features of entities.\n4.\nTemplate Relation Task: The Template Relation (TR)\ntask marks relationships between template elements.\n5.\nScenario Template Task:\nScenario templates are the\nprototypical outputs of IE systems. They tie together TE\nentities into event and relation descriptions. This task re-\nquires identifying instances of a task-speciﬁc event, identify-\ning event attributes, and construction of an object-oriented\nstructure recording the entities and the relationships.\nOne of the advantages of this task orientation is that inputs\nand outputs of an information extraction system can be deﬁned\nprecisely, which facilitates the evaluation of diﬀerent systems and\napproaches. Two important metrics for assessing the performance\nof an IE system are Recall and Precision. Recall measures the\namount of relevant information that the NLP system correctly\nextracts from the test collection. Precision refers to the reliability\nof the information so extracted.\nRecall = correct slot fillers in output templates\nslot fillers in answer keys\nPrecision = correct slot fillers in output templates\nslot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE",
    "slot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE\nwhere P is precision, R is recall, and β is a parameter en-\ncoding the relative importance of recall and precision. When we\ngive equal weightage to precision and recall, F-measure will be\n2PR/(P + R).\n1.5.3\nArchitecture of an IE System\nHobbs proposed a generic architecture for an IE system.\nThe\nHobbs system consists of the following ten modules:\n• Text Zoner - turns a text into a set of segments.\n• Preprocessor - turns a text or text segment into a se-\nquence of sentences, each of which is a sequence of lexical\nitems.\n• A Filter - turns a sequence of sentences into a smaller set\nof sentences by ﬁltering out irrelevant ones.\n• A Preparser - takes a sequence of lexical items and tries\nto identify reliably determinable small-scale structures.\n• A Parser - takes a set of lexical items (words and phrases)\nand outputs a set of parse-tree fragments, which may or\nmay not be complete.\n• Fragment Combiner - attempts to combine parse-tree or\nlogical-form fragments into a structure of the same type for\nthe whole sentence.\n• A Semantic Interpreter - generates semantic structures\nor logical forms from parse-tree fragments.\n• A Lexical Disambiguator - reduces the ambiguity of the\npredicates in the logical form fragments.\n• A Coreference Resolver - identiﬁes diﬀerent descriptions\nof the same entity in diﬀerent parts of a text.\n• A Template Generator - ﬁlls the IE templates from the\nsemantic structures.\nThe SIFT system developed by BBN, the LOLITA system de-\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.",
    "veloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can",
    "or integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\nbe performed on summaries of documents rather than the original\ndocuments, hopefully giving faster and better results. Informa-\ntion Extraction can be performed on summaries rather than the\noriginal documents. Text categorization can be done on the sum-\nmaries. Summaries can be used to ﬁlter out unwanted mails and to\n40\nCHAPTER 1. THE INFORMATION AGE\nroute the rest of the mails automatically to relevant departments.\nDocuments retrieved from the web may be summarized and the\nsummaries translated into other languages. Hopefully, translating\nthe summaries would be easier and quality of translations would\nbe better. A doctor may want to see case histories related to a\nparticular case retrieved, summarized, the treatments compared\nand a report generated, all automatically.\nImagine instructing\nyour home computer to watch the TV news while you are away\nand give you a summary of what all is happening when you come\nback home. Imagine instructing your computer to search the web\nfor relevant documents in various languages and produce a sum-\nmary of what the Russians and the Japanese are saying about a\nparticular move by the United States Government. Think of a\nsystem that can read aloud a summary of a given text for a blind\nperson.\nSummarization can improve the performance of other appli-\ncations. Summarization involves size reduction and this in turn\ncan give speed beneﬁts to other applications. Hopefully, the data\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We",
    "size reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We\noften scan news headlines before or instead of reading the full\nnews. Teachers produce outlines of notes and course materials to\nstudents. Minutes of a meeting summarize what all transpired in\nthe meeting. Previews of movies give a quick advance view of the\nshape of things to come in the movie. Synopses of TV serials are\ntelecast to let the users get a feel for the serials. Book reviews\nare summaries as retold by the reviewer. Newspapers and maga-\nzines publish Radio and TV guides for the coming week or month.\nResumes and Obituaries are biographical summaries. Novels are\noften speciﬁcally abridged for, say, children to read. Weather fore-\ncasts and Market report bulletins are summaries too. Sound Bites\nconsolidate on going debate on a particular issue. Chronologies\nand gists of history are a kind of summary.\nSummarization is\nitself not new. Automatic techniques for summarization are.\nAlta-Vista Discovery uses Inxight’s summarizer for ﬁltering\nweb based IR. Orcale’s Context (data mining of text databases),\nMicrosoft Word’s AutoSummarize, British Telecom’s ProSum are\n1.6. AUTOMATIC SUMMARIZATION\n41\nsome of the commercial implementations.\n1.6.2\nApproaches to Automatic Summarization\nSummarization can be viewed as a reductive transformation of\nsource text to summary text through content reduction by selec-\ntion and / or generalization of what is important in the source.\nText Summarization has been deﬁned as the process of distilling\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is",
    "the most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE",
    "given source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\nSummarization involves reduction in size. Reduction is achieved\nby one of three methods:\n1. Selection of salient or non-redundant information\n2. Aggregation of information - from diﬀerent parts of the doc-\numents, from diﬀerent linguistic descriptions etc.\n3. Generalization of speciﬁc, detailed information with more\nabstract information\nThus we may aggregate terms - diﬀerent inﬂected forms may\nbe aggregated into their root forms, spelling variations can be\nnormalized, synonyms and diﬀerent representations of the same\nentity can be collapsed.\nComputing, computation, computers,\ncomputerization are all morphologically related forms of the same\nroot. Defence and Defense are same. A pachyderm is an elephant.\nThe morning star is the same as the evening star. First February\n2005 is the same as 01/02/2005. Ministry of Communications and\nInformation Technology is the same as MCIT. Particular regions\nof the text may be eliminated as redundant. A sequence of state-\nments describing an object or event may be generalized to name\nthe object or event. Thus a whole paragraph may be replaced\nwith “negotiation of terms and conditions”.\nThe process of summarization can be broadly divided into\nthree phases for facilitating our understanding:\n1. Analysis of given text\n2. Transformation into a summary representation\n3. Synthesis of appropriate natural language summary report\nSummarization has been attempted by selection, aggregation\nand generalization at various levels of linguistic description:\n1. Surface Level: Here shallow, superﬁcial analysis leads to se-\nlection of terms and other surface features which are then\ncombined to produce the summaries.\nThere are several\nkinds of surface level features:",
    "and generalization at various levels of linguistic description:\n1. Surface Level: Here shallow, superﬁcial analysis leads to se-\nlection of terms and other surface features which are then\ncombined to produce the summaries.\nThere are several\nkinds of surface level features:\n(a) Features based on frequency of occurrence of terms\n1.6. AUTOMATIC SUMMARIZATION\n43\n(b) Features based on location - position within the doc-\nument, position within a paragraph, depth of nesting\nof sections, speciﬁed sections\n(c) Features based on background information - terms\npresent in the title or headings, terms present in user\nqueries\n(d) Features based on cue words and phrases - “to sum-\nmarize”, “in conclusion”, “in particular”, “more im-\nportantly”\n(e) Features based on domain speciﬁc “bonus” and “stigma”\nterms - “bear” and “bull” are bonus terms in economy\nand business domain\n2. Entity Level: Here entities and relationships are extracted\nand some internal representation of the source text is built\nin terms of these entities and relationships. The internal\nrepresentation may model the essence of the text as patterns\nof connectivity such as by using a graph. Summaries are\nthen produced from these internal representations. Some of\nthe features that can be used are:\n(a) Similarity - vocabulary overlap\n(b) Proximity\n(c) Co-occurrence\n(d) Thesaural Relationships - synonyms/antonyms, hy-\npernyms/hyponyms, part-of\n(e) Co-reference\n(f) Logical relationships - agreement, contradiction, en-\ntailment, consistency\n(g) Syntactic relationships - subject, object, object of pre-\nposition etc.\n(h) Semantic relationships - predicate-argument relations\n3. Discourse Level: Here global and hierarchical structure of\nthe text and its relation to communicative goals are identi-\nﬁed. Such structures include\n(a) Format - Hypertext Markup, Document Outline\n44\nCHAPTER 1. THE INFORMATION AGE\n(b) Discourse Segments and Themes\n(c) Rhetorical Structure - argumentation, proof, narra-\ntive, etc.",
    "the text and its relation to communicative goals are identi-\nﬁed. Such structures include\n(a) Format - Hypertext Markup, Document Outline\n44\nCHAPTER 1. THE INFORMATION AGE\n(b) Discourse Segments and Themes\n(c) Rhetorical Structure - argumentation, proof, narra-\ntive, etc.\nA summary may be an extract or an abstract. In the former\ncase, parts of the original document, say important sentences, are\nselected and presented as a summary. For example, in any coher-\nent writing, it is usually possible to identify one or two sentences\nin each paragraph which contain the essence of the whole para-\ngraph. Replacing the paragraphs with these extracts will result\nin a condensed form of the original text that, hopefully, retains\nthe essential information.\nExtraction is therefore identiﬁcation\nand “lifting” of important parts of the source text. The output\nis therefore linguistically close to the original source and follows\nthe same general order. An abstract, on the other hand, is gener-\nated from salient pieces of information identiﬁed from the original\ntext. Extracts are usually easier to obtain than abstracts, as can\nbe expected. Generation of natural language sentences is itself a\nvery complex task.\n1.6.3\nSummarization in Relation to Information\nExtraction\nText summarization is related to Information Extraction. Text\nsummarization is an open approach to extracting information in\nthat there is no prior assumption or expectation of what kinds\nof information are important or what kinds of things to look for.\nWhat is important for a source text is marked by the text itself\naccording to some general, linguistically-based importance crite-\nria. In contrast, Information Extraction is a closed approach in\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the",
    "that you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its",
    "instead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nimportant content, where content is a matter of both information\nand its expression, and importance is a matter of what is essential\nas well as what is salient.\nAutomatically generated summaries cannot be expected to\nbe as good as human generated summaries. However, automatic\nsummarizers work much faster and thereby enable us to consider\nmore documents in a given amount of time.\n1.6.6\nSummarization in the Context of Indian\nTradition\nIt is interesting to relate current interest in technology for auto-\nmatic summarization with the Indian scene, especially with re-\n46\nCHAPTER 1. THE INFORMATION AGE\ngard to our ancient tradition. Brevity was considered an essential\nquality and every eﬀort was taken to write brieﬂy and precisely.\nMany of the greatest works, which are widely read, discussed and\ndebated for thousands of years now, are very small in size. The\nbhagavadgiita has only 700 verses. baadaraayaNa’s brahmasuu-\ntras, which purports to explain all of the upanishadic thought\nin a coherent way, is just 555 aphorisms. pataMjali’s yoogasuu-\ntras consist of 195 aphorisms. maaMDuukyoopanishat, considered\nto be the essence of all upanishadic thought, is just 12 mantras.\ns’aMkara’s aatma SaTkam is six verses. His saadhana paMcakam\nis just ﬁve verses. People could remember entire texts by heart.\nThere was in fact not much need for writing.\nSummarization\nmakes no sense.\nIn fact the need was the opposite. Some of the works were so\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and",
    "cryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and\ncan often be completely memorized. This book could be written\nin just a few pages. But that would not be very easy for most\nreaders today to read and understand. Understanding short and\ncryptic statements require more eﬀort and seriousness on the part\nof the readers. (In fact this book has actually been written point\nby point and then expanded.) It looks like we are in a situation\ntoday where we tend to write too much and then see the need for\nproducing summaries.\nWe ﬁnd a plethora of ideas on summarization in the mi-\nimaaMsa and other s’aastras. We can learn a lot about proper\nway of organizing and structuring our thoughts and hence the\ndocuments we create. One of the basic requirements for any co-\nhesive piece of writing, however small or big it may be, is that\nit should be possible to express its purport in just one sentence\n(eekavaakyata). If you cannot express the gist in one sentence, ei-\nther you have not understood the writing properly or the writing\nitself is incoherent. Ramayana, which runs into 24,000 s’lokas, has\nbeen summarized into a few pages, into one page, into one small\nparagraph and into a single statement.\n1.7. AUTOMATIC TEXT CATEGORIZATION\n47\n1.7\nAutomatic Text Categorization\n1.7.1\nWhy Text Categorization?\nOver the past decade, there has been an explosion in the avail-\nability of electronic information. As the availability information\nincreases, the inability of people to assimilate and proﬁtably uti-\nlize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.",
    "lize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.\nAutomatic text categorization has many applications. Infor-\nmation Retrieval systems and Search Engines will greatly ben-\neﬁt if the documents in the collection are already categorized.\nIt would then be possible to limit the search to relevant classes\nof documents, thereby enhancing both the speed and the perfor-\nmance.\nText Categorization system can be used for automatic\nﬁltering and routing of documents. For example, junk mails can\nbe detected and removed and mails can be routed to diﬀerent de-\npartments based on their topic. There are tools that collect news\nfrom various newspapers and other sources on the web, perform\nnews aggregation and suitably re-organize the results. Automatic\ntext categorization can be of great value to such systems. Iden-\ntiﬁcation of topic also helps in many other areas such as word\nsense disambiguation by narrowing the space of possibilities and\nbringing things into sharper focus.\n1.7.2\nApproaches to Automatic Text Catego-\nrization\nBefore the 1990s, the predominant approach to text classiﬁcation\nwas the knowledge based approach. Rules and heuristics based on\nexperience were used to place documents in appropriate classes.\nWhen performed on a small scale, as in the case of individual\nlibraries, classiﬁcation experts manually read the title, front mat-\nter, cover page material, table of contents etc. and decided where\nexactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines",
    "exactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines\nand heuristics can be codiﬁed into computer programs and cate-\ngorization performed automatically. Hence the name knowledge\nbased approach.\nWith the increasing availability of large scale data in electronic\nform, advances in machine learning and statistical inference, there\nhas been a clear shift over the last decade or so towards automatic\nlearning from large scale data. In the Machine Learning approach,\na general inductive process (also called the learner) automatically\nbuilds a classiﬁer for a given category by observing the charac-\nteristics of a set of training documents already classiﬁed under a\nspeciﬁed set of classes. The inductive process gleans from these\nlabeled training data, the characteristics that a new unseen docu-\nment should have in order to be classiﬁed under a given category.\nThe classiﬁcation problem is thus an activity of supervised learn-\ning.\nA Machine Learning program automatically learns to distin-\nguish between diﬀerent classes or categories based on examples.\nLearning here is basically generalizing from examples. The ma-\nchine must ﬁgure out which features are more discriminative and\nwhich ones are not. Accordingly the features are weighted. All\nuseful features are considered but the more discriminative one will\ncarry higher weightage. Under suitable assumptions, it is possible\nto prove that what the machine does is about the best possible,\ngiven the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-",
    "given the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-\numents, typically based on the subject matter or topic, without\nany manual eﬀort. A text categorization program can automati-\ncally categorize thousands of documents in a few minutes. There\nis no way manual classiﬁcation can match that speed. In terms of\naccuracy of classiﬁcation, automatic systems can achieve accura-\ncies of 95% or even higher, depending upon the speciﬁcities of the\ntask. Manual classiﬁcation can in principle be fully correct but in\npractice one must make allowance for some errors.\nAutomated text categorization can be deﬁned as assigning\npre-deﬁned category labels to new documents based on the likeli-\nhood suggested by a training set of labeled documents. Given a\n1.7. AUTOMATIC TEXT CATEGORIZATION\n49\nset of documents with the associated category labels, the system\nlooks for discriminating features that help to set the various cate-\ngories apart. This process is called training. The system learns, so\nto say, how exactly documents within a class are similar and doc-\numents in diﬀerent classes are diﬀerent from each another. Once\nthe system has been trained, it can look at new documents unseen\nbefore and classify them into one of the set categories.\nThe similarities and diﬀerences between documents of various\nclasses are expressed in terms of features. In the simplest case,\nfeatures are the words in these documents. The assumption is that\nthere are words that occur frequently in certain classes but not\nin the others. Words like election, mandate, constituency, party,\nlegislature, parliament are more likely to occur in the political\narena than in sports. Of course there can be politics in sports,\nthere can be elections and parties in sports domain too. That is",
    "in the others. Words like election, mandate, constituency, party,\nlegislature, parliament are more likely to occur in the political\narena than in sports. Of course there can be politics in sports,\nthere can be elections and parties in sports domain too. That is\nwhy is not a good idea to try and ﬁx such terms manually. Given a\nset of representative training documents, the system automatically\nlearns those features that have suﬃcient discriminative power and\nother features that occur more or less equally in all categories are\nignored.\nThe big advantage is that the machine learning system can\neasily adapt to any new kind of classiﬁcation problem whereas\nmanual methods will require starting all over again if the nature\nof the classiﬁcation task changes. For example, if the set of cate-\ngories is re-deﬁned or changed signiﬁcantly, all it takes for an auto-\nmated system is a few minutes of re-training. In fact, once such a\nlearning system has been developed, it can easily be adapted and\ncustomized for a wide range of tasks. It would be possible, for\nexample, to take a system trained for classifying news articles in\nTelugu language and apply it to classify computer programs based\non the programming language used. Almost all the human eﬀort\nthat would have gone into creating a completely manual classiﬁ-\ncation system for Telugu news articles would become practically\nuseless if asked to classify computer programs.\nWe have given a broad deﬁnition of Text Categorization above.\nThere are several variations to the basic theme:\n• In Document Pivoted Categorization a given document is to\nbe assigned category label(s) whereas in a Category Pivoted\nCategorization, all documents that belong to a given cate-\n50\nCHAPTER 1. THE INFORMATION AGE\ngory must be identiﬁed. This distinction is more pragmatic\nthan conceptual. Thus if all the documents are not avail-\nable to start with, document pivoted categorization may\nbe more appropriate while category pivoted categorization",
    "50\nCHAPTER 1. THE INFORMATION AGE\ngory must be identiﬁed. This distinction is more pragmatic\nthan conceptual. Thus if all the documents are not avail-\nable to start with, document pivoted categorization may\nbe more appropriate while category pivoted categorization\nmay be the preferred choice if new categories get added and\nalready classiﬁed documents need to be re-classiﬁed.\n• In Hard categorization, the classiﬁer is required to ﬁrmly\nassign categories to documents (or the other way around)\nwhereas in Ranking Categorization, the system ranks the\nvarious possible assignments and the ﬁnal decision about\nclass assignments is left to the user. This leads us to the\npossibility of semi-automatic or interactive classiﬁers where\nhuman users take the ﬁnal decisions to ensure highest levels\nof accuracy.\n• Constraints may be imposed on the number of categories\nthat may be assigned to each document - exactly k, at least\nk, at most k, and so on. In the single label case, k = 1 and\na single category is to be assigned to each document. If k\nis more than 1, we have the multi-label categorization.\n• The text categorization problem can be reduced to a set of\nbinary classiﬁcation problems one for each category - where\neach document is categorized as either belonging to a given\ncategory or not.\n• If only unlabeled training data is available we may have to\nuse unsupervised learning techniques to perform Text Clus-\ntering instead of classiﬁcation into known classes. Here the\naim is to determine the similarities and diﬀerences among\nthe various documents and ﬁnd out a natural way of group-\ning them so that similar documents are grouped together.\n1.7.3\nText Representation\nClassiﬁcation systems represent documents in terms of sets of fea-\ntures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.",
    "tures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.\nEach word in a text is a potential feature. In the domain of\ntext categorization, words and word-like features (such as phrases)\n1.7. AUTOMATIC TEXT CATEGORIZATION\n51\nare called terms. Documents are treated as bags of terms. Fea-\nture dimensions are thus often very large, running into tens of\nthousands.\nThe curse of dimensionality is that the number of\ntraining data samples required grows exponentially with the num-\nber of features. Choice of the right subset of potential features\nis a major concern. A variety of dimensionality reduction tech-\nniques are used in pattern recognition. The discriminating power\nof features is evaluated and the least discriminating features can\nbe discarded without much loss. Concepts such as Mutual Infor-\nmation and Information Gain have been applied to evaluate the\ndiscriminating power of features. Principal Component Analysis\nis a standard technique for identifying the feature dimensions with\nmaximal variance. Similarly, Singular Value Decomposition is a\ntechnique that rotates the feature space so as to align the most\ndiscriminating features along the axes of the rotated feature space.\nInterested readers are directed to books on Pattern Classiﬁcation\nor Machine Learning for more details.\nCommonly used pre-processing steps include\n• Stop word removal - eliminating function words and other\nvery frequently occurring, less discriminative terms\n• Morphology or Stemming - replacing fully inﬂected words\nwith their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting",
    "with their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting\nNumerical weights need to be computed for the index terms before\nmachine learning techniques can be applied. Here are some of the\nbasic ideas for term weighting:\n• Term Attributes:\nAttributes of the terms such as their\nsyntactic categories can be used to weight the terms.\n• Text attributes:\nThe number of terms in a text, the length\nof the text etc. can be used.\n52\nCHAPTER 1. THE INFORMATION AGE\n• Relation between the term and the text:\nRelative frequency\nof the term in the text, location of the term in the text,\nrelationship with other terms in the text etc.\n• Relation to corpus:\nRelation between the term and the\ndocument corpus or some other reference corpus can also\nbe used.\n• Expert Knowledge:\nExpert knowledge is a potential source\nbut is rarely used.\nThe most common approach is to consider the frequency of\noccurrence of terms in a given document in relation to their fre-\nquencies of occurrence in other documents in the collection. This\nscheme is known as the tf-idf scheme. Here is how tf-idf weights\ncan be computed for given terms:\n• Term Frequency:\nWords that occur more frequently in a\ngiven category are likely to be more signiﬁcant to the spec-\niﬁed category and are thus given higher weightage. Since\nthe occurrence of a rare term in a short text is more sig-\nniﬁcant than its occurrence in a long text, log of the term\nfrequency is used to reduce the importance of raw term\nfrequencies in those collections that have a wide range of\ntext lengths.\nAnaphoric references and synonyms reduce\nthe true term frequency. In morphologically rich languages,\npoor morphological analysis or stemming also adds to this\neﬀect.\n• Inverse Document Frequency: Terms that occur in almost",
    "frequencies in those collections that have a wide range of\ntext lengths.\nAnaphoric references and synonyms reduce\nthe true term frequency. In morphologically rich languages,\npoor morphological analysis or stemming also adds to this\neﬀect.\n• Inverse Document Frequency: Terms that occur in almost\nall documents are useless for classiﬁcation. Therefore, terms\nthat occur in smaller number of documents are given higher\nweightage.\n• Inverse Category Frequency: Inverse Category Frequency\ncould be more appropriate than inverse document frequency\nsince the distribution of documents into categories may be\nskewed. A log can again be taken to weigh this factor down\nso that it does not become over-dominating.\n• Product of tf and idf:\nTerm frequency and Inverse Doc-\nument Frequency are inter-related. Terms that occur fre-\nquently in a particular class but not very frequently in other\n1.7. AUTOMATIC TEXT CATEGORIZATION\n53\nclasses are the most signiﬁcant. Hence a product of tf and\nidf is often used.\n• Length Normalization:\nLong and verbose texts usually use\nthe same terms repeatedly. As a result, the term frequency\nfactors are large for long texts and small for short ones, ob-\nscuring the real term importance. Term frequencies can be\nnormalized for length of texts by dividing them by the total\nword count in the document, or better still, by the frequency\nof the most frequently occurring term in the document.\n• Cosine Normalization The directions of the feature vectors\nrather than their lengths are considered to be better in-\ndicators of the various classes.\nIn cosine normalization,\neach term weight is divided by a factor representing the Eu-\nclidean vector length. Thus all vectors become unity length\nvectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives",
    "vectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives\na brief description of some of these techniques.\nIn particular,\nthere we will show how the Bayesian Learning approach can be\napplied to the task of automatic text categorization. Models are\nbuilt from labelled training data and then applied to classify new\nunseen documents. When there is no labeled training data set\navailable, it is also possible to automatically cluster or group to-\ngether similar documents. A suitable measure of similarity must\nbe deﬁned. Often the term distance is used as a measure of dis-\nsimilarity. Clustering techniques work by attempting to reduce\nthe intra-cluster distances while maximizing the inter-cluster dis-\ntances. See section 2.3.3 for more on this.\nA number of automatic text categorization systems have been\ndeveloped and put to use for English and other major languages of\nthe world. Work on Indian languages has started only recently. A\ncategorization system developed recently by the author could clas-\nsify News Articles in Telugu into broad categories such as Sports,\nPolitics, Economics and Business and Cinema with nearly 95% ac-\ncuracy. The system was trained on a preclassiﬁed set of about 600\n54\nCHAPTER 1. THE INFORMATION AGE\ndocuments and tested on about 200 previously unseen documents.\nDeveloping automatic text categorization systems requires a\nlarge amounts of pre-classiﬁed training data.\nAlso, the perfor-\nmance of the system may deteriorate if the classes considered are\nﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much",
    "ﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nlarger collection of training documents. One may have to resort\nto multi-level or hierarchical classiﬁcation.\nWe have seen that practical text categorization systems treat\ndocuments as unordered collection of words, use these words as\nfeatures and count frequency of occurrence to weigh these features.\nLinguists will be shocked to know that today’s text categorization\nsystems treat “India beat Australia” and “Australia beat India”\nas identical - both have the same three words. There is hardly\nany linguistic analysis of the texts concerned. Speed is not the\nonly criteria in all situations. More intelligent text classiﬁcation\nsystems will surely require a deeper linguistic analysis of the doc-\nument texts. Also, the feature dimensions are extremely large and\ndimensionality reduction is an important issue.\nIt is interesting to note that NLP started with big goals like\nnatural language understanding and generation but only toy sys-\ntems could be built then.\nNowadays much larger applications\nthat give reasonably good performance in real life situations are\nbeing built but with only the most superﬁcial and rudimentary\nlinguistic analysis of the language. Future may perhaps lie in an\nintelligent combination of deep linguistic analysis and statistical\nmethods based on large scale training data.\n1.8\nMachine Translation\nMachine Translation (MT), also known as automatic translation\nthrows open great opportunities. While human costs are going\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only",
    "up, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use",
    "Let us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\nour common sense, world knowledge and context to disam-\nbiguate between diﬀerent usages as in ‘I like mangoes’, ‘I\nlike citrus fruits like oranges and sweet lemons’ and ‘unlike\npoles attract each other and like poles repel one another’.\nNote that a dictionary simply lists all possible grammatical\ncategories - it does not tell us in any great detail, how to\nﬁgure out the actual grammatical category when the word\nis used in a particular context. A large number of words\nin English are both nouns and verbs. Identiﬁcation of the\ncorrect grammatical category is essential to ﬁgure out the\ncorrect meaning. POS tagging systems are far from perfect.\n56\nCHAPTER 1. THE INFORMATION AGE\n• Word Sense Disambiguation: Knowing the correct gram-\nmatical category or part-of-speech is not suﬃcient. Words\ncan have several meanings or senses even within given gram-\nmatical categories. Human languages strike a balance be-\ntween having too many words so that each idea can be ex-\npressed precisely using a speciﬁc word made just for that\npurpose but diﬃcult to remember so many words, and, over-\nloading words with too many diﬀerent senses, thereby mak-\ning it easy to remember and use the words but so much more\ndiﬃcult to resolve the ambiguities and understand them\ncorrectly. A large number of words in any language tend to\nhave several senses. Following Swamy Vivekananda’s ide-\nals in your life, following a thief who is just running away\nwith your purse and following a lecture in a classroom are\nall very diﬀerent kinds of following. Is capital a capital city,\nthe ﬁnancial capital or an upper case letter of the alphabet?\nSimple words like have, give and go have dozens of mean-",
    "with your purse and following a lecture in a classroom are\nall very diﬀerent kinds of following. Is capital a capital city,\nthe ﬁnancial capital or an upper case letter of the alphabet?\nSimple words like have, give and go have dozens of mean-\nings. When we hear ‘Mary had a little lamb’ somehow our\nmind does not even seem to get many of the possible mean-\nings. Contrast with the meanings we get if we place the\nsame sentence in diﬀerent contexts: ‘I had dosa for break-\nfast. Mary had a little lamb!’ or ‘Mary was expecting. Mary\nhad a little lamb!’ Machines tend to get a large number of\npossible solutions and are left clueless as to which one is the\nright one. We may say context helps but characterizing this\nnotion of context in a precise way is a challenge in itself.\nWord Sense Disambiguation has remained a hard nut to\ncrack. People use world knowledge and commonsense. Ma-\nchines have serious diﬃculties with things that are so simple\nand commonplace for us. They have neither commonsense\nnor human like abilities to reason out things.\n• Idioms and Phrases: Idioms and phrases are sources of diﬃ-\nculty for machines. Machines have no clue unless these are\nlisted in a dictionary. Dictionaries of idioms and phrases\nmeant for human users tend to list only those items that\nare potentially confusing to human beings, obvious ones\nmay be left out.\nNothing is obvious for machines.\nHow\ndoes the machine know that tooth powder is not powder of\nthe tooth unless it is told in so many words? How does the\nmachine understand that heavy water is not just water that\n1.8. MACHINE TRANSLATION\n57\nis heavy? How does the machine understand that a water\nmeter cover adjustment screw is a screw meant for adjusting\nthe cover of a meter meant for measuring the ﬂow of water?\nIn what way is water pump diﬀerent from cast iron pump?\nSome soaps say for dry skin and others say for healthy and\nbeautiful skin. A vast majority of all that we say requires\nnon-literal interpretation.",
    "the cover of a meter meant for measuring the ﬂow of water?\nIn what way is water pump diﬀerent from cast iron pump?\nSome soaps say for dry skin and others say for healthy and\nbeautiful skin. A vast majority of all that we say requires\nnon-literal interpretation.\n• Expressions: Non-literal usages and ﬁgures of speech can\nadd to the confusion especially because they vary widely\nacross languages.\nIn Kannada you say something like ‘a\ndream fell to me’ to mean I dreamt a dream. You say some-\nthing like ‘He told words of advise to his legs’ to mean he\nﬂed. You say something like ‘not possible in my hand’ to\nmean I cannot. In English night falls but day breaks. Each\nlanguage has its own set of idiosyncratic usages. Studies\nhave shown that a large percentage of all texts need non-\nliteral interpretation. Traditional dictionaries rarely list all\nsuch usages.\n• Lexical Substitution: Even if the machine is able to get the\ncorrect sense of words in the source language text, selecting\nappropriate target language words is not an easy job. There\nmay be several nearly synonymous words and choice of the\nmost appropriate word requires thought and care. One fa-\nmous example is the English sentence ‘The spirit is willing\nbut the ﬂesh is weak’ which when translated into Russian\nand back to English by a machine came back as ‘The vodka\nis ﬁne but the meat is rotten!’.\nThe spread of senses of\nwords in the source and target languages may show subtle\nvariations and a wrong choice may introduce unintended\ntwists and misinterpretations. We see this in translated ad-\nvertisements and commercials everyday.\nA well meaning\nIndian Language guideline has been translated into English\nas ‘Give way to traﬃc on the right!’ (instead of make way).\nIt is also possible that there is no direct equivalent at all\nin the target language. This happens especially with scien-\ntiﬁc and technical terminology and domain speciﬁc terms.\nMany strategies can be adapted. We may retain the source",
    "as ‘Give way to traﬃc on the right!’ (instead of make way).\nIt is also possible that there is no direct equivalent at all\nin the target language. This happens especially with scien-\ntiﬁc and technical terminology and domain speciﬁc terms.\nMany strategies can be adapted. We may retain the source\nlanguage word as it is (example: phone, car, bus, radio in\nIndian languages). We may borrow and/or extend a word\n58\nCHAPTER 1. THE INFORMATION AGE\nfrom another language such as Sanskrit (example: vidyut\nmeaning lightening in Sanskrit for electricity).\nWe may\ncoin new words using words of another language(example:\naakaas’avaaNi literally meaning sky-speech for broadcast-\ning radio). We may extend or stretch the meaning of avail-\nable words (example: fan is used for electric fans as also for\nthe age-old hand-held fan, motor car, nowadays only car as\nan extension of the horse-carriage, mouse for the pointing\nand selecting device on the computer because it looks like\na mouse and is connected by a long wire like the tail of a\nmouse, notebook to mean a compact note-book sized com-\nputer). We may use acronyms as words (example: LED,\nCD, LASER, RADAR. We may use part of an expression\nto mean the whole (example: radio to mean radio receiver,\ntransistor to mean transistorized radio receiver). We may\ncoin new words by compounding etc. (example: plywood,\nnew-wood, go-cart, laptop). But we must remember that a\ncoined word is like a counterfeit note - it has no currency.\nUntil and unless the word comes into regular usage, it will\ncontinue to aﬀect communication.\nWe may also use various interesting combinations of these\ntechniques(example: The word xray was coined in English\nafter the discovery of this new kind of ray and this word is\noften translated in Indian languages as ksha-kiraNa where\nthe part ray has been translated into its equivalent word ki-\nraNa in the usual way but the preﬁx ksha is not usually used\nto represent an unknown quantity but the English ‘x’ often",
    "often translated in Indian languages as ksha-kiraNa where\nthe part ray has been translated into its equivalent word ki-\nraNa in the usual way but the preﬁx ksha is not usually used\nto represent an unknown quantity but the English ‘x’ often\nis.). The ordinary word window has been borrowed into the\nrealm of graphical user interfaces on computer screens with\nthe idea that each window opens a window to a new world.\nBut notice that opening one window in front of another\nexisting window still shows you the same world through\nboth of these windows, not a new world. Perhaps curtain\ncould have been a much better term to use. Thus while\nmany strategies can be used for any situation demanding a\nword in general and translation in particular, each of these\nstrategies has its own drawbacks. A wrong choice will only\nadd to the confusion. The ultimate question is whether the\ntranslated material is well received and easily understood\nby the readers. Human translators face serious diﬃculties.\n1.8. MACHINE TRANSLATION\n59\nMachines cannot do better.\n• Structural Ambiguities: There are also structural ambigui-\nties in language which are diﬃcult for machines to disam-\nbiguate. Attachment of prepositional phrases and subor-\ndinate clauses is particularly hard without commonsense\nunderstanding. ‘I saw a man on the hill with a telescope’\ncould mean several things. Perhaps there was a telescope\nerected on top of the hill we are talking about. The man\non the hill was possibly carrying a portable telescope in his\nhand. Or the man on the hill was sighted by me using a\ntelescope. Similarly, in ‘I gave the book to the boy who had\ncome home after taking bath’, there are two possible inter-\npretations depending upon whose taking bath we are talk-\ning about. ’mothers with babies 6 months old’ and ’mothers\nwith babies more than 40 years old’ are both correctly un-\nderstood by humans because we know babies cannot be 40\nyears old and mothers cannot be 6 months old. Machines\nhave serious diﬃculties.",
    "ing about. ’mothers with babies 6 months old’ and ’mothers\nwith babies more than 40 years old’ are both correctly un-\nderstood by humans because we know babies cannot be 40\nyears old and mothers cannot be 6 months old. Machines\nhave serious diﬃculties.\n• Syntactic Parsing: There may be substantial diﬀerences\nbetween the structures of source and target language sen-\ntences. Complete syntactic parsing of source language sen-\ntences and appropriate mappings to target language struc-\ntures may be necessary. Suitable mappings are diﬃcult to\nﬁnd at times.\nFurther, source language may encode less\ninformation than what is required in the target language.\nSimilarly the source language may have more explicit infor-\nmation than required for translating into target language.\nShould this extra information be simply thrown out? For\nexample, Hindi requires grammatical gender information\neven for the words that are neuter gender in English.\nThe performance of parser-based translation systems is lim-\nited by the performance of the syntactic parsing systems.\nToday even the best available parsers are not good enough.\nThe situation is much worse in the case of Indian languages.\nThere are no wide coverage computational grammars for\nany Indian language yet. There are no syntactic parsers.\n• Anaphoric References Pronouns such as it, he, her, they,\nthem as also deﬁnite noun phrases such as the second one,\n60\nCHAPTER 1. THE INFORMATION AGE\nthe cover may refer to items already mentioned somewhere\nin the discourse. Referents may be in the same sentence, in\nthe previous sentence, or several sentences before. Resolv-\ning such anaphoric references is essential for understanding\nthe texts but may or may not be essential for translation.\nIn some cases substitution of appropriate pronouns and def-\ninite noun phrases in the target language may carry forward\nidentical interpretations from source language to target lan-\nguage.\n• Discourse Structure It is not suﬃcient to work with one",
    "In some cases substitution of appropriate pronouns and def-\ninite noun phrases in the target language may carry forward\nidentical interpretations from source language to target lan-\nguage.\n• Discourse Structure It is not suﬃcient to work with one\nsentence at a time. Sentence by sentence translation does\nnot always work.\nDiscourse level analysis is essential to\nensure good translation.\nThus machine translation requires many steps and the errors\naccumulate and compound.\nThe performance demands on the\nindividual modules will be very high in order to obtain reasonable\nperformance for MT as a whole. MT remains a tough problem.\nThe best known event in the history of machine translation\nis without doubt the publication in November 1966 of the re-\nport by the Automatic Language Processing Advisory Committee\n(ALPAC 1966). Its eﬀect was to bring to an end the substantial\nfunding for MT research in the United States for some twenty\nyears. More signiﬁcantly, perhaps, was the clear message to the\ngeneral public and the rest of the scientiﬁc community that MT\nwas hopeless. For years afterwards, an interest in MT was some-\nthing to keep quiet about; it was almost shameful. To this day,\nthe “failure” of MT is still repeated by many as an indisputable\nfact.\nThe report said (page 16): “There is no emergency in the ﬁeld\nof translation. The problem is not to meet some nonexistent need\nthrough nonexistent machine translation.\nThere are, however,\nseveral crucial problems of translation. These are quality, speed,\nand cost.” Quality, speed and cost remain the most important\nyardsticks even today.\n1.8.2\nDeploying Machine Translation\nWhenever we talk of machine translation, the ﬁrst thing that\ncomes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never",
    "comes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never\nintended for literary translations. Translation of literary works is\nbest performed by human experts. They can do it, they enjoy\ndoing it and there is no need to replace that with any kind of\nautomatic device. Contrast this situation with, say, Information\nRetrieval from a large collection of documents, a task that human\nbeings just cannot do, nor will they enjoy doing. Machine trans-\nlation is a very diﬀerent kind of an application compared to many\nother areas of language engineering.\nA layman can get direct beneﬁt from an IR system or a sum-\nmarization system. All of us can use these applications in our\ndaily life. But who wants machine translation? Common people\nhave no daily requirement for translation at all. If you are think-\ning in terms of an application meant for direct use by ordinary\npeople, machine translation is a non-existent, imaginary task.\nAutomatic translation makes more sense in those areas where\nit is routine, tedious and boring for people and people really wish\nto avoid doing it manually. Such situations include translation of\nmanuals for scientiﬁc instruments, machinery and consumer prod-\nucts, routine paper work in government oﬃces etc. However, in\nsituations such as these, the people who need translations are not\nexpert translators themselves. Their proﬁciency in language may\nnot be very high. They would therefore be expecting a fully au-\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd",
    "tomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques",
    "diﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua",
    "not easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua\nbut posit a level of analysis and representation that to a\nlarge extent captures the semantics. The representations\nwill still be instantiations in a given language. Thus trans-\nlation involves analyzing the source text to produce an in-\nternal representation of the meaning instantiated with the\nsource language terms, a transfer from this source language\ninstantiation to the corresponding target language instan-\ntiation, and ﬁnally generating target language text from\nthose representations. This approach is called the transfer\napproach is more practical.\n• In principle, deeper the analysis, better the understand-\ning and hence better the translation.\nHowever, in prac-\ntice, analyzing the semantics of the source language is ex-\ntremely hard. It will also be very diﬃcult to generate natu-\nral language sentences from a deep semantic representation.\n64\nCHAPTER 1. THE INFORMATION AGE\nStructure is a function of meaning and in many systems\nonly structural analysis and generation is performed, pre-\nsuming that the structural analysis (that is, parsing) and\ngeneration are good enough to capture and preserve mean-\nings. Some systems do not even do syntactic parsing, only\nsuperﬁcial analysis in terms of morphology and local word\ngrouping is performed. These systems are said to follow a\ndirect approach.\nOne can argue that translation is merely a set of formal oper-\nations to transform from one code system to another and so there\nis no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical",
    "is no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical\nargument but in practice it does not take us anywhere. The only\nknown examples of good translators are the human beings and we\nhuman beings read, understand and only then translate. There\nare no working systems to prove mechanical transformation ideas\nand no one has any clue how to go about building such a system.\n1.8.4\nChallenges in Machine Translation\nGreat progress has been made in the area of machine translation\nand some systems have even be deployed for regular use in lim-\nited domains. Also, crude, ﬁrst-cut translations are available and\nare being used by Search Engines. Nonetheless, do not expect\nhigh quality automatic translations to become available in the\nnear future. Wherever quality is important (and quality is almost\nalways important) you can only expect semi-automatic, human\naided translation systems.\nSome of the major areas of focus in automatic translation to-\nday are 1) detailed analysis of sentence structure 2) identifying\nthe correct sense of words in context, 3) combining human judge-\nment and commonsense with statistical learning techniques and\n4) building high quality lexical resources in large scale.\n1.8.5\nMachine Translation in India\nThere has been a great interest in India in Machine Translation\nand a lot of work has been done over the last 15 years. In fact\n1.8. MACHINE TRANSLATION\n65\nMT has been given so much of attention that many equate NLP to\nMT. Some groups have concentrated on translating from one In-\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now",
    "dian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\nbecoming available but there are still no real life applications.\nThe MAT System\nHere we brieﬂy describe MAT - a small machine assisted trans-\nlation system that was developed by the author for the Govern-\nment of Karnataka, a southern state in India. MAT is used for\ntranslating English texts into Kannada, the oﬃcial language of\nKarnataka. MAT is a parser based translation aid, suitable for\ntranslating between positional languages like English and Indian\nlanguages, which are characterized by a relatively free word or-\nder and a very rich system of morphology. MAT is based on the\nUCSG (Universal Clause Structure Grammar) theory of syntactic\nanalysis developed by the author. The primary goals of the MAT\nproject were to explore the feasibility of parser based translation\nbetween English and Indian languages and to develop a proto-\ntype machine aided translation system for translating the budget\nspeech texts of the Government of Karnataka with the primary\ngoal of saving time and eﬀort. Quality of translation was ulti-\nmately the responsibility of the human translator.\nIt is now fairly clear that fully automatic high quality transla-\ntion is diﬃcult to realize in practice. Either we lose out on quality\nor we will have to involve the human translator in the process\nsomewhere. MAT is a machine assisted translation system that\nprovides for a full spectrum of possibilities - from fully automatic\ngeneration of raw translations suitable for manual post editing,\nthrough semi-automatic translation to almost fully manual trans-\nlation using the facilities provided by the system. The basic idea\nis to make the best of both the human and machine capabili-\nties to achieve good translations with minimum time and eﬀort.",
    "through semi-automatic translation to almost fully manual trans-\nlation using the facilities provided by the system. The basic idea\nis to make the best of both the human and machine capabili-\nties to achieve good translations with minimum time and eﬀort.\nApart from a very powerful post editing tool, MAT also comes\nwith dictionaries, a thesaurus, morphological analyzer/generator\nand several other useful tools.\nMAT is a parser based translation system. Each sentence in\n66\nCHAPTER 1. THE INFORMATION AGE\nthe source text is parsed syntactically using the UCSG frame-\nwork before translating. This makes MAT suitable for translating\nbetween languages that show a signiﬁcant variation in sentence\nstructures. MAT was developed especially for translating between\nEnglish and Indian languages but it can be applied to other lan-\nguages too.\nENGLISH\nSENTENCE\nENG−MORPH\nANALYZER\nENGLISH\nDICTIONARY\nCHUNKER \n FSM GRAMMAR\nFUNCTIONAL\nSTRUCTURE\nANALYZER\nCLAUSE\nSTRUCTURE\nANALYZER\nCLAUSE STRUCTURE\nGRAMMAR (CFG)\nFUNCTIONAL STRUCTURE\nGRAMMAR\nIL SENTENCE\nPLANNER\nIL GRAMMAR\nTRANSLATOR\nIL MORPH\nGENERATOR\nENGLISH−IL\nDICTIONARY\nIL\nSENTENCE\nPOST−EDITING\nTOOL\nIL     SENTENCE\nFIG 1.11 Architecture of the MAT System\nIn MAT, the parser and translator can be run in one of the\nthree modes called non-interactive, interactive and custom modes.\nIn the non- interactive mode, the system neither asks the user for\nany help nor does it pause to display intermediate results. This\nis the simplest and the fastest mode - a full 100 page text can be\nprocessed in less than a couple of minutes on a personal computer.\nIn the interactive mode, the system stops to ask the user for help in\ndealing with unknown words, unresolved ambiguities, etc. It also\nstops to show the parse structure of each sentence. The user can\nselect the correct parse if there are several of them, and translation\nproceeds after getting conﬁrmation from the user. This mode is\nvery useful for researchers and system developers. We can also",
    "stops to show the parse structure of each sentence. The user can\nselect the correct parse if there are several of them, and translation\nproceeds after getting conﬁrmation from the user. This mode is\nvery useful for researchers and system developers. We can also\nexpect better quality of translation.\nIn the custom mode, the\nuser can customize the system interface by specifying the kinds the\nquestions the system may ask the user, the intermediate results\nthat should be displayed and/or sent to log ﬁle etc. This mode\nis ideal for testing and development as also for getting a feel for\nhow exactly the system works. A time limit can be speciﬁed to\ninstruct the system to skip a sentence if it is taking too much\ntime. Performance of the system is displayed continuously and a\nsummary and a histogram are displayed at the end.\n1.8. MACHINE TRANSLATION\n67\nIn MAT, like the parser, the translator also works from whole\nto part, rather than from left to right. After a sentence has been\nparsed by the UCSG parser, we would know the number, type\nand inter-relationships amongst the various clauses in the sen-\ntence and the word groups that take on various functional roles\nin each of these clauses. Keeping this structure of the sentence in\nmind, a suitable structure for the equivalent sentence in the tar-\nget language is ﬁrst developed. Where it is not feasible to make\nmore or less direct transfer of structure, sentence transformation\nrules can be called to restructure the source sentence so that it\nbecomes amenable for translation. This makes it possible to deal\nwith languages with vastly diﬀerent sentence structures.\nAfter having ﬁxed the overall structure of the target language\nsentence, the individual clauses are mapped. Finally, the various\nword groups are mapped. In each word group, the head and the\nmodiﬁers are identiﬁed and transfered, keeping in mind the L-\ngrammar of the target language.\nFor each word, a suitable target language equivalent is ob-",
    "sentence, the individual clauses are mapped. Finally, the various\nword groups are mapped. In each word group, the head and the\nmodiﬁers are identiﬁed and transfered, keeping in mind the L-\ngrammar of the target language.\nFor each word, a suitable target language equivalent is ob-\ntained from the bilingual dictionary. The MAT system provides\nfor incorporating syntactic and some simple kinds of semantic con-\nstraints in the bilingual lexicon for word sense disambiguation. For\nexample, the word ’rise’ is mapped diﬀerently into Kannada in the\nfollowing three sentences:\nI rose\nnaanu eddenu\nThe moon rose\ncaMdra huTTitu\nThe prices are rising\nbelegaLu heccuttive\nKannada, the target language in the current experiments, is\nmorphologically very rich. Words in Kannada often take as many\nas 6 suﬃxes. In many cases, a whole word group in English trans-\nlates into a single highly inﬂected word in Kannada. The MAT\nsystem includes a morphological analyzer/generator for Kannada.\nUsing the Kannada morphological generator, appropriate word\nforms are generated in each case.\nFinally, the target language sentence is generated by placing\nthe clauses and the word groups in appropriate linear order, ac-\ncording to the constraints of the target language grammar. For\nexample, in Kannada the subject has to agree with the verb not\nonly in number but also in gender. So gender feature is extracted\nfrom the subject of the English source sentence and used in the\n68\nCHAPTER 1. THE INFORMATION AGE\nmorphological generation of the Kannada verb so that, for exam-\nple, ’she came’ becomes ’avaLu baMdaLu’ and ’he came’ becomes\n’avanu baMdanu’ in Kannada. See transcripts given at the end\nfor more examples.\nIn case a complete parse with satisfactory rating was not ob-\ntained for whatever reason, the word groups with or without the\nfunctional roles assigned to them are available for semi-automatic\ntranslation. The user picks up parts of the sentence, interactively",
    "for more examples.\nIn case a complete parse with satisfactory rating was not ob-\ntained for whatever reason, the word groups with or without the\nfunctional roles assigned to them are available for semi-automatic\ntranslation. The user picks up parts of the sentence, interactively\ncalls the translator module to translate the part and ﬁnally assists\nthe machine in assembling the target language sentence.\nThe post-editing tool displays the source text and the corre-\nsponding translated target language text one sentence at a time.\nUsing the tool, the translator can move around pieces of text\neasily. He can also delete, insert and edit words at will. More sig-\nniﬁcantly, he can call the thesaurus on line and substitute selected\nwords by their equivalents. Morphological analysis and generation\nare done on the ﬂy so that the correct word forms are substituted\nautomatically. Thus toMdarege can be substituted with kaSTakke\nby mapping the root toMdare to kaSTa, with appropriate mor-\nphology at both ends.\nSubstitution can be called on both the\ntarget and source language words. One can also speciﬁcally call\nthe morphological analyzer, look at and modify the feature list\nand then re-generate a new word form. Further, it is also possible\nto call the translator to translate selected parts of a text. These\nunique advanced features make it possible to translate full sized\nEnglish texts with a minimum of eﬀort. As a last resort the user\ncan manually retype the correct translation. High quality trans-\nlations can be obtained irrespective of the complexity of sentence\nstructures employed and the inherent limitations of the parsing\ntechnology. Overall, the post editing tool makes the life of the\ntranslator so much easier and gives signiﬁcant time savings too.\nThere are separate monolingual dictionaries for English and\nKannada as also a bilingual English-Kannada dictionary. A unique\nfeature of this system is that the bilingual dictionary can be used",
    "translator so much easier and gives signiﬁcant time savings too.\nThere are separate monolingual dictionaries for English and\nKannada as also a bilingual English-Kannada dictionary. A unique\nfeature of this system is that the bilingual dictionary can be used\nas a kind of thesaurus too. You can get all related words for a\ngiven word.\nThere is also a morphological analyzer cum generator for Kan-\nnada. Kannada words can be analyzed for their internal structure.\nSpeciﬁc word forms can also be generated from a given root word.\nFurther, it is possible to get a full paradigm, that is, a system-\n1.8. MACHINE TRANSLATION\n69\natic listing of all the forms of a given word. Both noun and verb\nmorphology are included.\nWhen tested on Budget Speech texts, MAT system 1.0 could\nparse and translate only about 40 percent of the sentences fully\nautomatically. The translations so produced by the machine are\nusually in more or less acceptable form. However, Some sentences\nmay need substantial editing and in a few cases, the outputs may\nhave to be rewritten completely.\nWhere automatic translation\nfails, semi-automatic translation is possible - user selects parts of\nthe sentence, calls the translator to produce translations for the\nparts, and ﬁnally assembles the parts into the complete target\nlanguage sentence. Thus MAT is far from a large scale, robust,\nhighly automatic machine translation system.\nHere are some transcripts from the MAT system 1.0. Clause\nstructure in parse output is indicated through indentation. The\nﬁrst parse produced by the parser has been shown here. Transla-\ntions shown here are raw translations - no post editing has been\ndone. Note how the order of clauses have been changed during\ntranslation as required by the target language grammar. Gloss in\nEnglish has been added manually.\n1 :\nEfforts are being continued to obtain more funds for providing\nrelief to the victims. : 14\nF_Structure 1 :\nSent 1 : Parse No. 1 : Clause 1 : Rating 0 : Passive : efforts are\nbeing",
    "translation as required by the target language grammar. Gloss in\nEnglish has been added manually.\n1 :\nEfforts are being continued to obtain more funds for providing\nrelief to the victims. : 14\nF_Structure 1 :\nSent 1 : Parse No. 1 : Clause 1 : Rating 0 : Passive : efforts are\nbeing\ncontinued\nto obtain more funds for providing relief to the\nvictims\n2 : subj : empty :: EMPTY :: [] :: []\n3 : subj : empty :: EMPTY :: [] :: []\n1 : obj : 2 :: 2 :: [] :: []\n2 : cause :\n3 :: 3 :: [] :: []\n1 : subj : np :: efforts :: [ng,[n,pl,(rt,effort)]] :: []\n1 : vg : vg :: are being continued :: [vgf,[v,aux,be,(rt,be)],\n[v,aux],[v|_]] :: []\n2 : vg : vg :: to obtain :: [vgf,[v]] :: [[(nf2,infinitive)]]\n2 : obj : np :: more funds :: [ng,[a,cmp,(rt,much)],\n[n,pl,(rt,fund)]] :: []\n3 : vg : vg :: for providing :: [vgf,[v]]\n:: [[(nf2,infinitive)]]\n3 : obj : np :: relief to the victims :: [ng,[n],\n[prep,(semf,[space,time])],\n70\nCHAPTER 1. THE INFORMATION AGE\n[det],[n,pl,(rt,victim)]] :: []\nKANNADA:\ntoMdarege\noLagaadavarige\nparihaaravannu\ndifficulty-dat be-subjected-to-people-dat relief-acc\nodagisalu\nheccina nidhigaLannu paDeyalu\nprayatnagaLu\nprovide-inf more\nfund-pl-acc\nobtain-inf effort-pl\nmuMduvarisalaaguttive.\ncontinue-passive2-pres-p3-pl\n2 : The Government knows that the people of Karnataka will welcome\nthis. : 11\nF_Structure 1 :\nSent 2 : Parse No. 1 : Clause 1 : Rating 0 : the government knows\nthat the people of karnataka will welcome this\n1 : obj : 2 :: 2 :: [] :: []\n1 : subj : np :: the government :: [ng,[det],[n]] :: []\n1 : vg : vg :: knows :: [vgf,[v,p3,sl,(rt,know)]] :: []\n2 : cls_link : sentinel :: that :: [] :: []\n2 : subj : np :: the people of karnataka :: [ng,[det],[n,pl],\n[prep],[n,prp]] :: []\n2 : vg : vg :: will welcome :: [vgf,[v,aux,modal],[v]] :: []\n2 : obj : np :: this :: [ng,[pr,sl,nom,acc,dem]] :: []\nKANNADA:\nkarnaaTakada janaru idannu svaagatisuttaare\nKarnataka-of people it-acc welcome-non-past-p3-pl\neMdu\nsarkaara tiLiyuttade.\nthat\ngovt.\nknow-pres-p3-sl",
    "[prep],[n,prp]] :: []\n2 : vg : vg :: will welcome :: [vgf,[v,aux,modal],[v]] :: []\n2 : obj : np :: this :: [ng,[pr,sl,nom,acc,dem]] :: []\nKANNADA:\nkarnaaTakada janaru idannu svaagatisuttaare\nKarnataka-of people it-acc welcome-non-past-p3-pl\neMdu\nsarkaara tiLiyuttade.\nthat\ngovt.\nknow-pres-p3-sl\nThe Status of MT in India\nNow we make some general remarks on the status of MT in In-\ndia. Small successes here and there notwithstanding, the general\npicture is bleak. No systematic survey of the MT needs in our\ncountry has been made. Who are the potential users of MT, what\nkinds of translation tasks do they have, what kinds of challenges\nare involved in those tasks, what kinds of languages and domains\nof specialization are involved, what are the user expectations, how\nwell does the current state of MT technology match the user needs\n- these are all largely unexplored questions. No feasibility studies\n1.9. SPEECH TECHNOLOGIES\n71\nhave been made. Users are not involved. Formal speciﬁcations\nare not written. No formal designs are worked out. There are\nno benchmark standards for testing and evaluation. There are\nno ground truth data for performing a systematic evaluation. In\nmost cases developers themselves do the testing. Testing is done\non training data and training on test data. Experts working on\nmachine translation have no experience in translating. Nor are\nexpert translators interested in machine translation research. No\nwonder we are so weak in both research and product development.\n1.9\nSpeech Technologies\nSpeech is one of the most natural and eﬃcient means of com-\nmunication. Working with texts and graphical user interfaces re-\nquires typing skills and hand-eye coordination, whereas speech is a\nhands-free mode of communication. Even illiterates would be able\nto communicate with computers and reap the beneﬁts of modern\nscience and technology. Future surely lies in speech technologies.\nSpeech technologies are especially relevant for a country like",
    "hands-free mode of communication. Even illiterates would be able\nto communicate with computers and reap the beneﬁts of modern\nscience and technology. Future surely lies in speech technologies.\nSpeech technologies are especially relevant for a country like\nIndia which has an oral tradition that goes back to thousands of\nyears. Entire libraries have been learnt by heart and preserved\naccurately all these years without ever writing them down.\nIn\ncontrast we ﬁnd that even recent writings such as those of Shake-\nspeare are already into several versions. Even after writing was\ninvented, it was considered the sign of a weak mind to write things\ndown in India - only accountants were using writing. The oral tra-\ndition continues even today. If all available copies of the works of\nShakespeare are destroyed today, his works will be lost for ever.\nOn the other hand if all available copies of the vedas are destroyed\nbeyond recovery, nothing will be lost - there are people who know\nthe whole thing by heart. We can simply sit and write them down\nagain.\nThe western notion of illiteracy is questionable. Illiterates are\nnot necessarily uneducated. The greatest scholars in India were\nilliterate and they preferred to be illiterate. It is not really true\nthat speech is transient and ephemeral and writing is permanent\nand safe.\nWhat we have in our minds and ﬁnger tips is what\nactually matters in life. What is the use of all the knowledge that\nresides in a library or the Internet?\n72\nCHAPTER 1. THE INFORMATION AGE\n1.9.1\nAutomatic Speech Recognition\nSpeaking as well as listening and understanding come so very nat-\nurally to human beings that we may tend to think that these are\nsimple and easy tasks.\nIn fact these are extra-ordinarily com-\nplex tasks. Unlike in written text, there are no clear cut word\nboundaries or even sentence boundaries. Speech signals show sub-\nstantial variability across speakers. Even the same word spoken",
    "simple and easy tasks.\nIn fact these are extra-ordinarily com-\nplex tasks. Unlike in written text, there are no clear cut word\nboundaries or even sentence boundaries. Speech signals show sub-\nstantial variability across speakers. Even the same word spoken\nby the same speaker can be very diﬀerent in terms of the signal\ncharacteristics. External noise can cause serious problems. While\nsigniﬁcant progress has been made in automatic speech recogni-\ntion over the last ﬁfty years, we still have a long way to go to\nachieve human like capabilities.\nSpeech understanding requires all aspects of NLP including\nsyntax, semantics and pragmatics and in addition requires the\ncapability to decode information encoded in speech signals.\nA\nspeech signal is a composite of the linguistic message being con-\nveyed, the language and dialect used, style, speakers identiﬁcation\nincluding parameters such as sex, age, health and emotional sta-\ntus, as also background noise which may include speech sounds\nof other people speaking nearby.\nAutomatically extracting the\nlinguistic message while ﬁltering out all other irrelevant aspects\nfrom such a composite signal is not an easy task. While the aim in\nautomatic speech understanding is to understand the meaning and\nintentions conveyed by a given speech utterance, automatic speech\nrecognition tries to look at a supposedly simpler task of extract-\ning the message content and representing it in text form, without\nnecessarily having to understand the meanings or intentions.\nRecognized Message\nEXTRACTION\nFEATURE\nPREPROCESSING\nMODELS\nRECOGNITION\nText Corpora\n(Text)\nSpeech\nSignal\nTraining\nTesting\nFIG 1.12 Speech Recognition\nPeople do not seem to recognize speech to produce a text rep-\nresentation and then process that text in their brains to under-\nstand speech. Understanding seems to be a direct process without\n1.9. SPEECH TECHNOLOGIES\n73\nneed for an intermediate text representation. In fact the very act",
    "People do not seem to recognize speech to produce a text rep-\nresentation and then process that text in their brains to under-\nstand speech. Understanding seems to be a direct process without\n1.9. SPEECH TECHNOLOGIES\n73\nneed for an intermediate text representation. In fact the very act\nof trying to represent speech as text incurs both loss and distortion\nof information. Thus there is no guarantee that speech recogni-\ntion is actually simpler than speech understanding. By saying we\ndo not need to understanding meanings, we are also losing all the\nconstraints that come from semantics. Any way, the decision to\ngo for speech recognition rather than speech understanding is a\npurely practical, engineering decision. Let us now see in broad\nterms how this can be done.\nSpeech sounds are produced by the vibration of the vocal cords\ncaused by the air ﬂow pumped by the lungs, and subsequent ma-\nnipulation of the sounds by the articulatory movements of the\ntongue, lips etc. in the vocal tract to produce diﬀerent kinds of\nsounds. Electronic, mathematical or computational analogues of\nthis source-ﬁlter system have been developed to ﬁnd out how we\ncan distinguish diﬀerent sounds from one another. Finding an ap-\npropriate set of features to characterize and distinguish between\ndiﬀerent speech sounds while accounting for the large inherent\nvariability in natural speech across speakers and contexts is still a\nlargely unsolved problem. It has not been possible so far to build\nspeech recognition systems that handle unlimited vocabulary and\nspontaneous speech of any arbitrary speaker in open domains.\nNevertheless, under restricted conditions, it is possible to develop\nautomatic speech recognition systems that perform fairly well.\nInitial attempts therefore restricted one or more dimensions\nof variability. Isolated word recognition systems did not have to\nworry about identifying word boundaries. Small vocabulary sys-\ntems exploited reduced possibilities of confusing one word with",
    "Initial attempts therefore restricted one or more dimensions\nof variability. Isolated word recognition systems did not have to\nworry about identifying word boundaries. Small vocabulary sys-\ntems exploited reduced possibilities of confusing one word with\nother similar sounding words. Domain speciﬁc systems exploited\nthe limited space of possibilities in structure and meaning within\ncarefully controlled application domains. Speaker dependent sys-\ntems avoided problems due to variations from speaker to speaker.\nSome systems can be easily adapted to the voice of individual\nspeakers and once that is done, they can perform fairly well for\nthat speaker. Speaker adaptive systems have been used as dic-\ntation machines - you can speak into your computer instead of\ntyping from the keyboard. In fact there are applications where\nyou do not want anybody other than the authorized user to have\naccess to the system. You do not want your military aircraft to ac-\ncept voice commands from strangers, right? Thus speaker speciﬁc\n74\nCHAPTER 1. THE INFORMATION AGE\nsystems have their uses too.\nIt is now possible to build speaker independent continuous\nspeech recognition systems with vocabularies going into tens of\nthousands of words. Current systems are, however, still quite brit-\ntle. Performance may deteriorate rapidly due to external noise,\nchange of microphone and other devices used etc. Humans can\nrecognize correctly even in the midst of noise as in a cocktail\nparty or automobile or a factory environment. We can easily han-\ndle multiple speakers and mix up of languages. There is a lot that\nstill needs to be done in the area of speech technologies.\nWe recognize that the main diﬀerences between diﬀerent speech\nsounds are in the frequency components and the way they change\nwith time.\nFeatures based on such observations are extracted\nfrom a large training data collected from many speakers. Inher-\nent variability is handled by building probabilistic models. Since",
    "sounds are in the frequency components and the way they change\nwith time.\nFeatures based on such observations are extracted\nfrom a large training data collected from many speakers. Inher-\nent variability is handled by building probabilistic models. Since\nboundaries between linguistic units such as words and sentences\nare almost impossible to recognize accurately, language models are\nbuilt for sub-word units and then extended for whole sequences\nof words based on the statistical analysis of large text corpora.\nTools to perform all these complex tasks are now freely and pub-\nlicly available. There are also commercial products ﬁne tuned for\nspeciﬁc applications such as dictation machines to replace typing.\nIn future we may see more and more of speech based interfaces\nto computers.\nInformation retrieval via speech may become a\nreality soon. Speech recognition is a complex multi-disciplinary\narea and it is beyond the scope of this book to provide a detailed\naccount. Good books are available for the interested reader.\n1.9.2\nSpeech Synthesis\nMachines that read out given texts, called Text-to-Speech (TTS)\nsystems, are in some sense easier to develop since several dimen-\nsions of variability have already been removed in expressing the\nmessage in text form. Speaker’s voice qualities or external noise\nare no longer relevant when text is given as input. Sentence and\nword boundaries are usually quite clear. However, there are is-\nsues that need to be taken care of. There can be words which are\nwritten with the same spelling but pronounced diﬀerently and the\nother way around - project (n) versus project (v) and their versus,\nthere. 1999 may have to be read as one thousand nine hundred\n1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-",
    "1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-\npropriate pronunciations is an important task. Indian languages\nuse a phonetic writing system and hence there are really no such\nthings as spellings.\nOnly a few relatively simple rules of map-\nping are suﬃcient to take care of deviations from true phonetic\nrepresentation.\nPARAMETERS\nG2P\nDATABASE\nCONCATENATIVE\nSYNTHESIS\nPROSODY\nWAVEFORM\nSYNTHESIS\nNORMALIZATION\nTEXT\nSPEECH OUTPUT\nSPEECH OUTPUT\nOR\nTEXT\nFIG 1.13 Text to Speech Synthesis\nThere are basically two diﬀerent ways speech sounds can be\nactually generated. In waveform generation approaches, a speech\nsignal waveform is actually constructed from features correspond-\ning to the sounds to be produced. Diﬀerent kinds of resonances\n(called formants) are set up in the vocal cavity when we speak\ndiﬀerent sounds. Formant synthesizers produce diﬀerent speech\nsounds by simulating these resonances. Articulatory synthesizers\nattempt to model the speech production processes and synthesize\nspeech using those models.\nAlternatively, one may record bits\nand pieces of sounds and concatenate them to produce whatever\nsequences of sounds need to be generated. Choice of the right\nlevel of recorded units and smoothing at the junctures are two\nimportant considerations. It is found that splitting in the middle\nof linguistic units can be better than splitting at the boundaries\nthemselves, since the middle portions are generally more stable\nand maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE",
    "and maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE\nand intelligently select the most appropriate units.\nTo make the sounds produced by TTS systems more natural,\nit is essential to include prosodic features such as stress, duration\nand intonation. Systems without good prosody sound very dull\nand machine like. Developing high quality, natural sounding TTS\nsystems is in fact quite diﬃcult. Interested readers may refer to\nany of the good books available on the subject.\n1.9.3\nOther Speech Technologies\nWhile speech recognition attempts to extract the linguistic mes-\nsage in the speech signal without regard to who spoke it, speaker\nrecognition focuses on recognition or identiﬁcation of the speaker.\nHere recognizing what exactly he or she said is secondary. It is be-\nlieved that voice of individuals is almost as unique as ﬁnger prints.\nThus signatures, voice prints, ﬁnger prints, iris prints and palm\nprints can be used in conjunction with one another to build very\nrobust biometric systems to identify individuals. A speaker may\nbe asked to speak out pre-speciﬁed passwords or the system may\ndynamically prompt the user each time with a diﬀerent password.\nWe have just have to accept or reject a candidate or we may have\nto ﬁnd the correct identity of the individual. Speaker recognition\nand identiﬁcation have applications in various forms of security\nand access control.\nWe may tend to think that written texts are more authentic\nsince speech sounds themselves are ephemeral.\nHowever, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve",
    "However, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve\nsuch proofs will be of immense value.\nIf speech can be compressed to read faster, it will save space\nand give a quick view. Similarly if speech can be slowed down,\nwe can carefully listen and use that for, say, transcription. Secre-\ntaries need not learn short hand. Note that a speech signal cannot\nbe simply compressed or expanded linearly in time - that would\nchange the speech quality or even render it unintelligible.\nSeparating the speech of individual speakers in a multi-party\nconversation has many uses. You can then store, index and re-\ntrieve diﬀerent speakers’ speech separately.\n1.10. HUMAN AND MACHINE INTELLIGENCE\n77\nIn multi-lingual environments, one may have to ﬁrst recognize\nthe language even before speech recognition is attempted. This is\nvery important in countries like India where people speak many\ndiﬀerent languages.\nLanguage identiﬁcation from short speech\nsamples is an interesting and challenging problem in itself.\nThere are interesting applications of speech technologies to\nmusic recognition, understanding and synthesis. Machines that\ncompose and sing can be built. Machines that teach music can\nbe conceived. Even models of creativity in music can be explored.\nInformation retrieval in the music domain is a fascinating ﬁeld by\nitself.\nSpeech is a technologically complex area requiring insights\nfrom diverse disciplines.\nAcoustics, Signal Processing, Pattern\nClassiﬁcation, Machine Learning, Linguistics, Computer Science\nare all involved. It is beyond the scope of this book to go into the\ndetails of speech technology.\nInterested readers will ﬁnd many\ngood books and articles for further exploration.\n1.10\nHuman and Machine Intelligence",
    "Classiﬁcation, Machine Learning, Linguistics, Computer Science\nare all involved. It is beyond the scope of this book to go into the\ndetails of speech technology.\nInterested readers will ﬁnd many\ngood books and articles for further exploration.\n1.10\nHuman and Machine Intelligence\nThe common belief that machines are dumb, they can only follow\nthe instructions we have given them, and therefore can never be\nmore intelligent or better than us is to a large extent a myth.\nComputers can remember large volumes of data and process the\ndata at great speeds. It is relatively easy to accept that machines\ncan beat us easily when it comes to simple tasks such as counting,\nsearching and sorting.\nCan you remember an entire telephone\ndirectory in your mind and search for entries in a fraction of a\nsecond? It is harder, however, to realize that much more complex\ntasks that appear to require a good deal of intelligence can also\nbe broken down into simple tasks and we can build machines that\noutperform ourselves even in these complex tasks. You might have\nheard of chess playing programs that can beat even grand mas-\nters. These programs derive their “intelligence” from their ability\nto consider much larger number of options and look ahead many\nmore steps than people can do. Playing chess is generally consid-\nered to involve a good deal of intelligence and computers can do\nthis task quite well. It is possible to break this complex task into a\nlarge number of very simple tasks - tasks such as counting, search-\n78\nCHAPTER 1. THE INFORMATION AGE\ning and sorting, and chess playing programs can be built that beat\nthe developers themselves. Sure, the developers have programmed\nthe machine and the machine does “blindly” follow the instruc-\ntions given by the developers. The crucial point is what kind of\ninstructions are given. If you program a machine to play a partic-\nular instance of a chess game, it can play that particular instance",
    "the machine and the machine does “blindly” follow the instruc-\ntions given by the developers. The crucial point is what kind of\ninstructions are given. If you program a machine to play a partic-\nular instance of a chess game, it can play that particular instance\nof the game but pretty much nothing else. On the other hand, if\nyou program the machine to observe how others play and learn\nfrom that, then the machine can actually beat you one day. The\npoint is that it is possible to program machines not just to store\nand routinely process data but also to ask questions, seek answers,\nthink logically, draw inferences, explore possibilities, try alterna-\ntives and evaluate decisions. It is possible to break even these\nsophisticated tasks that appear to require human-like intelligence\ninto simple tasks such as comparison or addition. Computers can\nbe made intelligent.\nComputers can in fact do many more things than we may\ninitially imagine. Text categorization programs can learn to con-\nsider the combined eﬀect of lakhs of words occurring with var-\nious frequencies in diﬀerent categories of documents and auto-\nmatically categorize documents into diﬀerent classes accordingly.\nAutomatic summarization programs use a variety of heuristics to\nselect the most salient parts of a document and produce a sum-\nmary. Information retrieval systems sift through millions of doc-\numents and retrieve relevant documents based on occurrence of\ngiven key words. All of these can be done automatically with no\nmanual intervention, at fairly high levels of performance and at\ngreat speed. Computers are much more than computing or cal-\nculating machines. Is not a wonder that some people continue to\nuse computers as type-writers?\nWe can build intelligent computers. Then why is it that there\nare so many tasks that computers cannot do well today? Building\nintelligent machines requires breaking those complex tasks into\nsimple tasks that computers are good at.\nFor many tasks, we",
    "use computers as type-writers?\nWe can build intelligent computers. Then why is it that there\nare so many tasks that computers cannot do well today? Building\nintelligent machines requires breaking those complex tasks into\nsimple tasks that computers are good at.\nFor many tasks, we\nstill do not know how to break them into simple tasks. We all can\nrecognize somebody by their face but we do not know how exactly\nwe do this in our brain. Our understanding of this process is not\ndetailed and precise enough to express it as a computer program.\nSimilarly, we all can understand spoken and written language and\nwe can think creatively, but we do not know how exactly we do\n1.10. HUMAN AND MACHINE INTELLIGENCE\n79\nsuch things. In fact it is easier to build computers that behave\nlike expert doctors or engineers than to build computers that can\nunderstand language like what every two year old kid can do.\nTasks that are very simple and natural for us can be very hard\nfor computers.\nHuman beings have common sense.\nThey also have world\nknowledge. We can all understand the famous fox and crow story\nbecause we know that the fox is an animal, animals are living\nbeings, living beings need food, if you don’t eat food you feel\nhungry, hunger makes you look for food, if you cannot go to the\nfood then you must make the food come to you, a fox cannot climb\ntrees, a crow normally sits on trees, and a thousand other things\nand we are able to interconnect all these pieces of common sense\nknowledge with logical reasoning. We do all these long chains of\nreasoning so fast and almost unconsciously. Computers are very\nweak in common sense, they do not have world knowledge and\ntheir reasoning abilities are not as robust as ours.\nHuman intelligence is very broad and generic. We can know\nmany things, learn many things and perform many tasks. A single\nperson can be a good singer, a good driver and a good doctor.\nWhile the same computer can be programmed to many things,",
    "their reasoning abilities are not as robust as ours.\nHuman intelligence is very broad and generic. We can know\nmany things, learn many things and perform many tasks. A single\nperson can be a good singer, a good driver and a good doctor.\nWhile the same computer can be programmed to many things,\nif we talk in terms of computer programs, usually one program\nspecializes in only one task. It is built to do that task and we\nonly see how well it can do that particular task.\nWe have knowledge. We also have meta knowledge - we know\nwhat we know and what we do not know well. We know where\nwe are good and where we are weak. We can solve problems and\npuzzles. We can also learn generic strategies and methods to solve\nsuch problems and puzzles. We can theorize, we can “think on\nour own”, we can be creative. Today’s computers are still very\nweak in these terms.\nNevertheless, computers are superior to human beings in many\nways. They can remember vast amounts of uncorrelated data, we\ncannot. They can count, search, sort, perform arithmetic compu-\ntations at lightening speed and great accuracy, we cannot. There\nare inﬁnitely many valid C programs and inﬁnitely many invalid\nC programs but a C compiler can validate the syntax of any ar-\nbitrary C program, understand it and translate it into an equiva-\nlent machine language code, all in a few seconds. Human beings\ncan develop such programs, they can develop the grammars upon\n80\nCHAPTER 1. THE INFORMATION AGE\nwhich these compilers are based, but they cannot match the ma-\nchine when it comes to the task of compilation itself. We must\naccept that there are tasks at which computers are far superior to\nus.\nIt is interesting to explore exactly what kinds of things ma-\nchines can do better than us.\nWe can write a C compiler but\nthe machine does the compilation better. Computers can execute\nprograms with great speed and accuracy, we cannot. We can de-\nvelop new programming languages but we are in fact not very",
    "chines can do better than us.\nWe can write a C compiler but\nthe machine does the compilation better. Computers can execute\nprograms with great speed and accuracy, we cannot. We can de-\nvelop new programming languages but we are in fact not very\ngood at writing programs ourselves. Computer programming is\ndiﬃcult because it requires you to think like a dumb machine and\nit is not easy for intelligent human beings to think like a machine.\nEven experts can rarely write a program correctly the ﬁrst time.\nThis is not because they do not understand clearly what is it\nthat they want to do but it is because writing computer programs\nis an inherently diﬃcult task. The human brain is designed for\ncommunicating with natural languages, not with computer pro-\ngramming languages. It is easy to explain what a program should\ndo in English but is not so easy to express the same thing in a\nprogramming language. It is easy for us to draw an approximate\ncircle but diﬃcult to draw an exact circle. On the other hand it\nis straightforward to write a program to make the machine draw\na perfect circle. It is much more diﬃcult to make it draw an ap-\nproximate circle. The machine does not know what you mean by\n“approximate circle” - it expects you to deﬁne it precisely! The\ncomputer has been of great help to me in writing this book - I\nhave written, erased, placed it back, re-arranged, re-written many\nmany times. I could not have done all that with equal convenience\nand speed sitting with paper and pencil. But the machine could\nnot have written this book by itself. The critical factor, therefore,\nis to understand what kinds of things machines are good at and\nexploit them for our beneﬁt. Machines can indeed do many more\nthings than we expect of them.\nIn terms of speed, automated systems are far superior to man-\nual methods in most tasks. Computers can process thousands,\nlakhs or even millions of documents in the time it takes us to\ngo through just a few pages. In terms of accuracy, automated",
    "things than we expect of them.\nIn terms of speed, automated systems are far superior to man-\nual methods in most tasks. Computers can process thousands,\nlakhs or even millions of documents in the time it takes us to\ngo through just a few pages. In terms of accuracy, automated\nmethods give comparable or superior performance today in cer-\ntain areas such as Information Retrieval, and Text Categorizations\nwhen the data is very large. On the other hand, people are often\n1.11. SHAPE OF THINGS TO COME\n81\nmuch better compared to machines when it comes to tasks such\nas translation or question answering. But we must remember that\neven when automated systems give good performance, their scope\nis often limited and they may be brittle - even a small change\nin the conditions may bring down the performance drastically.\nFor example, today’s speech recognition systems are not robust\nagainst such simple looking variations as change of microphone\nor even moving the head a little away from the microphone while\nspeaking. Human beings are generally more robust than machines\nin most cases.\nThe question therefore is not whether people are better or\nmachines are better. Each of them have their own strengths and\nweaknesses. The challenge is to properly understand the strengths\nand weaknesses of the two and build man-machine synergies that\ncan produce the best results with minimal eﬀort.\n1.11\nShape of Things to Come\nSo far we have seen several interesting and useful applications of\nhuman language technologies. Our treatment has been introduc-\ntory and informal. The focus has been on what kinds of things\ncan be and have been done. In the process we have got some feel\nfor the need for thorough linguistic and statistical analysis of lan-\nguage, techniques for representing and reasoning with knowledge,\ncomputational methodologies for making the machines learn from\ndata, and large scale linguistic data resources that permit eﬀective\nlearning and generalization by machines. We have made progress",
    "guage, techniques for representing and reasoning with knowledge,\ncomputational methodologies for making the machines learn from\ndata, and large scale linguistic data resources that permit eﬀective\nlearning and generalization by machines. We have made progress\nbut there is still a long way to go. There are many serious issues\nand problems yet to be solved. The ﬁeld of NLP or Language\nEngineering is both fascinating and challenging.\nThere are many other possible application areas for NLP. We\nuse natural languages for communication among ourselves but we\ncan also imagine using them one day to communicate with ma-\nchines. It takes a substantial amount of training to learn and use\nartiﬁcial languages such a computer or robot programming lan-\nguages but comparatively little training is required to use human\nlanguages we already know. Natural Languages form a common\nstandard - we all know and use these languages. Communicat-\ning with human languages is more natural and simpler compared\n82\nCHAPTER 1. THE INFORMATION AGE\nto using artiﬁcial, abstract, formal languages. Natural languages\nare, or can be, concise. Compare “who makes more than their\nmanager” to the equivalent query in, say, SQL. Natural languages\nare also complete - we can express almost anything and everything\nwe wish to, whereas programming languages are highly restricted\nin their expressive power. Many of them do not even allow you to\nmake statements or ask questions. We “teach” others, not “pro-\ngram” them. Programming is unnatural, diﬃcult and highly error\nprone. Thus natural languages hold promise, at least in principle,\nas replacements to computer programming languages. Imagine a\nday when you could just talk to your computer and your com-\nputer could understand what you said and answer back in speech\nor written language.\nLet us stretch our imagination further and think of possible\ntechnologies of the future. But beware, today’s science is only yes-",
    "day when you could just talk to your computer and your com-\nputer could understand what you said and answer back in speech\nor written language.\nLet us stretch our imagination further and think of possible\ntechnologies of the future. But beware, today’s science is only yes-\nterday’s science ﬁction. Some of things we are talking about here\nhave already been attempted and you may even ﬁnd limited suc-\ncesses here and there. But by and large they are largely unsolved\nproblems as yet.\nWhat if we could do simultaneous translation into many lan-\nguages in real time? You have seen human experts doing this in\nimportant international meetings. What if we could do speech\nunderstanding rather than speech recognition?\nComputers are\ngetting embedded into all kinds of devices and you can think of\ntalking not only to your computer but also to your refrigerator,\nwashing machine, TV, car or your home robot. What if we could\ncombine speech recognition and speech synthesis with automatic\ntranslation to build speech-to-speech translators? You speak in\nTamil over phone and I would hear it in Hindi. I speak back in\nHindi but you hear the Tamil version. Language barriers would\nmelt away.\nA vast majority of our Indian population who are\ncurrently deprived of the direct beneﬁts of information technol-\nogy would be able to access information through speech in their\nown language. Knowledge of English would no longer be essen-\ntial. You could get information from your phone, there is no need\nto have access to a computer or know how to use it eﬀectively.\nWhat if could combine intelligent information retrieval systems\nwith automatic summarization systems or Information Extraction\nsystems? What if we could integrate these with Expert Systems\nand Decision Support systems? What if we could build intelligent\n1.11. SHAPE OF THINGS TO COME\n83\nassistants and robots for use at oﬃce, home or factory? What if\nyou can instruct your TV, in spoken language, to watch all the",
    "systems? What if we could integrate these with Expert Systems\nand Decision Support systems? What if we could build intelligent\n1.11. SHAPE OF THINGS TO COME\n83\nassistants and robots for use at oﬃce, home or factory? What if\nyou can instruct your TV, in spoken language, to watch all the\nmajor news channels during the day and give you a summary of\nthe important happenings, keeping your own interests in mind,\nwhen you come back home from the oﬃce late in the evening?\nWhat if you could go to your computer and just say, “Hey, this\nman here seems to have had a heart attack. Help!”, and the com-\nputer searches the Internet, ﬁnds out about what to do and what\nnot to do till help arrives, advises you on ﬁrst aid, calls an am-\nbulance, ﬁnds out hospitals where suitable facilities are available,\nselects a few good ones based on distance, cost or whatever and\nﬁxes appointments with good doctors, all on its own? What if the\nembedded computer in your car take over control when you are\ntoo drunk, unconscious or otherwise in bad shape, inform your\nkin or a doctor, and safely drive you home? What if the com-\nputer embedded in your shoe inform you when you over exert or\nautomatically call the doctor when things go really bad?\nPeople grow as per the demands they place on themselves, to\naim low is crime. Students and budding researchers must be en-\ncouraged to think and stretch their imagination. All ideas must\nbe encouraged to start with however foolish or idiotic they may\nappear to be. Some will surely lead to progress. Only one needs to\ncombine imagination with proper knowledge of ground realities.\nOnly then will knowledge really expand and grow. Often scientists\nand engineers glorify some aspects and hide the other side. We\nneed to stop once in a while and take stock. Industrial revolution\nwas supposed to automate manufacturing so that quantity and\nquality improvements can be obtained at reduced cost and the\ntime so saved can be be fruitfully used by people for human de-",
    "need to stop once in a while and take stock. Industrial revolution\nwas supposed to automate manufacturing so that quantity and\nquality improvements can be obtained at reduced cost and the\ntime so saved can be be fruitfully used by people for human de-\nvelopment in general. Don’t we ﬁnd people putting in many more\nhours today than before just to earn their bread? Have dams re-\nally helped us to control ﬂoods and promote irrigation? Farmers\nare committing suicide.\nMere imagination will not help us to move forward. Imagi-\nnation and creativity must be combined with proper knowledge\nand deep understanding. There are real theoretical, conceptual,\nengineering and pragmatic issues and problems. By the time you\nhave completed reading this book, you will hopefully have some\nidea about these issues and hence your own calculated judgements\nabout the shape of things to come.\n84\nCHAPTER 1. THE INFORMATION AGE\nChapter Two will be devoted to the foundations of NLP. We\nwill get to know more about linguistic data resources and the\nanalyses of data from linguistic and statistical points of view. A\nbrief introduction to some of the machine learning techniques is\nincluded.\nChapter Three will take us back to Information Retrieval. In\nthe light of our understanding of language and technology for\nlanguage, we will be able to say more about the issues, problems,\nsolutions and ideas that is shaping Information Retrieval today.\nThis is only an introductory book. Each of the topics included\nhere is a vast area by itself. You will need to dig deeper into areas\nof your interest to get a deeper and more thorough understanding.\nThe aim of this book is only to help you get started.\nChapter 2\nFoundations of Natural\nLanguage Processing\n2.1\nIntroduction\n2.1.1\nLanguage, Communication, Technology\nIf there is one thing that sets human beings apart from all other\nliving creatures it is Language. Language and thought are closely\nrelated. Language enables us to articulate our thoughts and com-",
    "Foundations of Natural\nLanguage Processing\n2.1\nIntroduction\n2.1.1\nLanguage, Communication, Technology\nIf there is one thing that sets human beings apart from all other\nliving creatures it is Language. Language and thought are closely\nrelated. Language enables us to articulate our thoughts and com-\nmunicate with fellow human beings. Man is a social animal. It is\ncommunication through language that binds this society together.\nIt is the language faculty that makes us what we are today. If it\nwere not for language, perhaps the human society could not have\ndeveloped to this extent. To understand the importance of lan-\nguage, try spending just one day without using language in any\nway. Asking somebody to shut up is a big punishment.\nLanguage manifests itself most naturally in the form of speech.\nSpeech is a natural, eﬃcient and direct means of communication\nbetween human beings. Technology such as the Tape Recorder,\nRadio, the Television, the Telephone make it possible for people\nto communicate with one another across space and time - you can\nspeak at one place and at one time and be heard at another place\nor at a later time.\nLanguage can also be codiﬁed in other ways. Speech, by its\nvery nature is transient. Writing, that is creating graphical shapes\n85\n86\nCHAPTER 2. FOUNDATIONS OF NLP\non a two dimensional surface, is more permanent. Scribing, en-\ngraving or embossing on any kind of a surface including stone,\npalm leaves, parchment, metal, cloth or paper qualiﬁes as written\nform of language. Speech is an all-in-one representation, encoding\nas it does the message being conveyed, the language, the speaker’s\nidentity, his or her emotional status, environmental conditions and\nso on. Written text is limited - a lot of useful information is lost\nwhen we write down things. Yet writing has played a major role\nin shaping the destiny of mankind. In fact our knowledge of our\nhistory owes in a large measure to the development of writing.",
    "so on. Written text is limited - a lot of useful information is lost\nwhen we write down things. Yet writing has played a major role\nin shaping the destiny of mankind. In fact our knowledge of our\nhistory owes in a large measure to the development of writing.\nWriting gives us an opportunity to carefully plan, organize and\nstructure our thoughts and thus communicate more eﬀectively.\nSpeech is direct and real-time. The written form of communica-\ntion, on the other hand, is not limited by the real-time processing\nconstraints. Hence writing can be more detailed, more elaborate.\nWritten texts can be preserved for long periods of time, repro-\nduced in large quantities and read by many people at the same\ntime at diﬀerent places.\nPrinting technology enabled eﬀective dissemination of knowl-\nedge and gave a big push to human development. Now the mod-\nern computer and communication technology is making it ever\nso much more easier to create, store, modify, reproduce and dis-\nseminate documents in electronic form. A single CD can store\nthe equivalent of several hundred full length books. Modern tech-\nnology has revolutionized the way we communicate. The impact\ntechnology will have on society at large is diﬃcult to guage.\nTechnology now makes it possible to scan text documents and\ncreate images of these texts.\nThis technology goes a long way\nin preserving ancient manuscripts.\nIt is easier to make several\ncopies of electronic documents and store them in many diﬀerent\nmedia and at many diﬀerent places for ensuring safe custody. Pre-\nserving palm leaves etc. is more diﬃcult. Technology, known as\nOptical Character Recognition also exists for recognizing the writ-\nten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on",
    "ten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on\ntext documents. While there is an increasing trend towards pro-\ncessing of multi-media documents, text processing continues to be\n2.1. INTRODUCTION\n87\nthe major focus. We shall limit ourselves mainly to processing of\ntext documents in this book.\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\nThis chapter deals with the fundamentals of language processing\nthat are essential for realizing Intelligent Information Retrieval\nas well as other applications of language and speech technologies.\nNatural Language Processing (NLP) forms the backbone of every\nhuman language technology application. NLP is concerned with\nnatural or human languages - languages which we human beings\nuse for day to day communications, as against artiﬁcial languages\nsuch as computer and robot programming languages.\nThe terms Natural Language Processing and Computational\nLinguistics have been used interchangeably and we shall do the\nsame here. There is no real diﬀerence between the two except per-\nhaps that linguists sometimes tend to prefer the latter term - in\nComputational Linguistics, Linguistics is the head of the phrase\nand Computational is only an adjectival modiﬁer. Computers, like\nmathematics, are generic and powerful tools and mere use of com-\nputers to carry out linguistic studies will not qualify as a distinct\nbranch of study in itself. Computational Linguistics or NLP is\ndiﬀerent from Linguistics in that the primary aim is to build com-\nputational models of various aspects of human language faculty.\nSuch models need to be simple, elegant and eﬃcient, yet very pre-\ncise, detailed and exhaustive. The models built must stand the\ntest of large scale real life data - language as people use it. NLP",
    "putational models of various aspects of human language faculty.\nSuch models need to be simple, elegant and eﬃcient, yet very pre-\ncise, detailed and exhaustive. The models built must stand the\ntest of large scale real life data - language as people use it. NLP\nalso includes various applications dealing with natural languages\nand the tools and technologies for realizing such applications.\nOf late there is an increasing emphasis on corpus based sta-\ntistical approaches either in conjunction with or as an alternative\nto, purely linguistic analysis. There is a conﬂuence of ideas from\nsuch varied disciplines as Linguistics, Statistics, Mathematics, En-\ngineering, Computer Science, Artiﬁcial Intelligence, Cognitive Sci-\nence, Logic, Philosophy and Psychology. Machine Learning and\nPattern Classiﬁcation Techniques have become the main stream\nmethodologies. Thus the term Language Engineering has become\nmore appropriate. Sometimes we also use the term Language tech-\nnologies. The term Human Language Computing is also in use.\n88\nCHAPTER 2. FOUNDATIONS OF NLP\nNatural Language Processing started oﬀas a part of Artiﬁ-\ncial Intelligence (AI), a discipline with two broad goals - 1) to\nunderstand the nature of human intelligence and 2) to build in-\ntelligent systems. Language faculty is at the very core of human\nintelligence. Whether language precedes and causes thought or\nthe other way round is debatable but the close relation between\nlanguage and thought is indisputable. Without language faculty,\nwe perhaps could not have been intelligent at all. Naturally NLP\nbecame a major focus area within AI. The primary goal of NLP\nwas to understand how exactly we human beings understand, gen-\nerate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-",
    "erate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-\nman languages. Speech perception, understanding and synthesis\nwere also major areas of focus. However, early work in NLP was\nmostly limited to handling written texts and was thus largely dis-\njoint from speech technology research. Of late we can see a closer\nintegration of NLP and speech technologies. There is also a trend\ntowards multi-modal communications involving speech, language\nand gestures.\nWe begin this chapter with elements of linguistic analysis es-\nsential for NLP and take up corpus based statistical approaches in\nthe second half. The treatment is introductory. Interested readers\nwill ﬁnd pointers to more advanced and detailed material in the\nbibliography.\n2.1.3\nNLP: An AI Perspective\nThere are three major concerns in NLP:\n• Natural Language Understanding (NLU): How do we un-\nderstand what we read or hear?\n• Natural Language Generation (NLG): How do we synthesize\nNatural Language utterances to convey whatever we have\nin mind?\n• Natural Language Acquisition (NLA): How do we acquire\nor learn a language?\n2.1. INTRODUCTION\n89\nThe Producer-Comprehender Model\nWe use language to communicate information as also our ideas,\nfeelings, emotions and attitudes.\nThis communication through\nlanguage can only happen between active, cognitive processors\nsuch as human beings and, hopefully, computers. We don’t speak\nto walls, right? There can be no proper communication if one or\nboth the parties are sleeping or otherwise inattentive. We shall\nuse the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive",
    "use the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive\nprocessors. We can model communication between one producer\nand one comprehender without loss of generality - communication\namong several entities can always be modelled in terms of several\none-to-one communications.\nPRODUCER\nAIMS, GOALS\nPLANS, STRATEGIES\nKEN. OF LANGUAGE\nKEN OF CONTEXT\nKEN. OF WORLD\nKEN. OF LANGUAGE\nKEN OF CONTEXT\nKEN OF CONTEXT\nKEN. OF WORLD\nCONSUMER\nAIMS, GOALS\nPLANS, STRATEGIES\nUTTERENCES\n(CODED MESSAGES)\nNLG\nNLU\nFIG 2.1 The Producer-Comprehender Model\nThe producer starts with his own aims, goals, plans and strate-\ngies. If he decides to communicate and use language for that com-\nmunication (he can also communicate through gestures or even\nthrough silence), then he constructs natural language utterances\nand speaks or writes them out. Similarly, the comprehender has\nhis own aims, goals, plans and strategies. He tries to understand\nthe received messages accordingly. The speech or text samples\nthemselves are actually just codes - they encode the messages to\nbe conveyed - information, feelings, attitudes, whatever.\nThus\nlanguage is merely a coding system and language in use is a sim-\nple linear sequences of symbols.\nThe producer generates these\ncodes and the comprehender decodes them.\nFor eﬀective communication, the producer and comprehender\n90\nCHAPTER 2. FOUNDATIONS OF NLP\nmust have a common language, that is, a common set of codes.\nThe knowledge of language includes vocabulary, grammar, etc.\nThe knowledge of language as possessed by any two persons is\nvery unlikely to be exactly the same. I may know some words\nor expressions that you do not know and vice versa. However,\nthere must be a large overlap between the knowledge possessed\nby the two parties. If 90% of the words I use are unknown to you,",
    "very unlikely to be exactly the same. I may know some words\nor expressions that you do not know and vice versa. However,\nthere must be a large overlap between the knowledge possessed\nby the two parties. If 90% of the words I use are unknown to you,\nyou just can’t understand anything. For eﬀective communication,\nwe have to assume that knowledge of language we have is largely\ncommon.\nEﬀective communication through natural language also re-\nquires common sense and world knowledge.\nWe rarely say ev-\nerything and if we try, we will only produce the most boring, dry\nand uninteresting output. Human beings have excellent ability to\nﬁll up the gaps and we only need to state the unusual, special or\ninteresting aspects. We all understand, even kids understand the\nstory “the wily fox tricked the vain crow into dropping the meat\nby praising its lovely voice”. Why did the fox want the meat ﬁrst\nof all? How does praising somebody get you food? If we start say-\ning that fox is an animal, animals need food, meat is food, crows\nsit on trees, foxes cannot climb trees, earth and meat have masses,\ngravity attracts masses, if one mass is very small compared to the\nother, the smaller mass moves towards larger, if the crow opens\nits beaks, the meat falls and so on and on, that would be the\nmost boring story you would have ever heard. But all these and\nmore steps of commonsense reasoning based on world knowledge\nare essential to understand even simple sentences in natural lan-\nguage. A great deal of common sense, world knowledge and ability\nto draw inferences intelligently are essential for natural language\nunderstanding.\nAlso, knowledge of the context is essential for proper com-\nmunication. If you enter a room where people are talking it will\ntake a while before you get to know what they are talking about.\nThe same utterances may mean very diﬀerent things if placed out\nof context or in diﬀerent contexts. Try placing “Mary had a lit-",
    "munication. If you enter a room where people are talking it will\ntake a while before you get to know what they are talking about.\nThe same utterances may mean very diﬀerent things if placed out\nof context or in diﬀerent contexts. Try placing “Mary had a lit-\ntle lamb” in the contexts of “I had sandwiches for breakfast”, or\n“Mary was expecting”!\nIt is important to realize that speakers say and listeners un-\nderstand based on their own aims, plans and strategies. You may\nsay something to literally mean what you said but you often say\n2.1. INTRODUCTION\n91\nsomething with diﬀerent intentions. You may want to tell a lie.\nYou may want to cheat. Your best friend has come home after a\nlong gap and you want him to stay longer but he is in a hurry.\nYou say “it is raining”. Both of you know very well that it is not\nraining. You have communicated your desire to keep your friend\nat home longer and your friend has understood it. You may be\nteaching in a classroom and some students may be trying to un-\nderstand the meaning of what you say. Others may in fact be\nanalyzing your style and mannerisms. Some may be just looking\nfor faults. Some may be thinking of something else. Communica-\ntion through language is much more involved than many think.\nA teacher needs to know what his/her students already know,\ntheir general mental abilities, their proﬁciency in language etc. A\nuniversity teacher may ﬁnd it very diﬃcult to teach school kids\nbecause he/she is used to diﬀerent kinds of models of his listeners.\nEvery speaker has a mental model of the listener. If somebody\nasks “where is Taj Mahal?”, you answer “in India”, “In Agra”,\n“go straight and take a right turn” or whatever depending upon\nyour impression of what the user wants to know and the context\nwhere the question is being asked. When a teacher asks a question\nhe/she may be wanting to test the students’ knowledge and when\na student asks the same question he/she may be genuinely inter-",
    "your impression of what the user wants to know and the context\nwhere the question is being asked. When a teacher asks a question\nhe/she may be wanting to test the students’ knowledge and when\na student asks the same question he/she may be genuinely inter-\nested in knowing the answer. Of course a student may be testing\nthe teacher too. We communicate through language under the\nassumptions of the mental models of the other party.\nNLP is concerned with building models of the minds of the\nproducer and comprehender as also with the mechanisms of coding\nand decoding of language utterances. Mental models include aims,\nobjectives, goals, plans, strategies, attitudes, representation and\nprocessing of knowledge of language, knowledge of the world and\ncommon sense as also reasoning and logic.\nNatural Language Understanding is concerned with the mental\nmodels of the comprehender that explain how the meanings and\nintentions encoded in language utterances are understood.\nNatural Language Generation is concerned with the mental\nmodels of the producer that explain how language utterances are\nproduced to express speciﬁed ideas, emotions, attitudes or pieces\nof information.\nNote that this is not the same as simply gen-\nerating some arbitrary but grammatical sentences from a given\ngrammar. Sometimes linguists use the term “generation” in con-\n92\nCHAPTER 2. FOUNDATIONS OF NLP\nnection with abstract grammars that can generate all and only\nthe grammatically valid sentences.\nWhile we are speaking and listening, we are also simultane-\nously learning new ideas and concepts as also elements of lan-\nguage.\nNatural Language Acquisition is concerned with mental\nmodels of the comprehender that explain how knowledge of lan-\nguage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from",
    "guage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from\nlearning language by merely interacting with the community of\nspeakers of that language. Natural language acquisition deals with\nall forms of language learning.\nThe aim in all cases is to build computational models - models\nthat are amenable for detailed inspection, models that can be\ntested on real life data. The models need to be both exhaustive\nand very detailed and precise. Samples are not suﬃcient.\nAlthough understanding, generation and acquisition have all\nbeen the primary goals of NLP, we ﬁnd that over the last 50 years\nor so maximum attention has been given to natural language un-\nderstanding, generation has received relatively less attention and\nlanguage learning has not received much attention at all. In fact\nan analysis view point is presumed in many cases by default. Thus\nwhile talking about syntax, NLP researchers tend to take the syn-\ntactic analysis view point rather than that of synthesis or gener-\nation. Linguists, on the other hand, are more used to thinking\nof grammars as abstract characterizations of the constraints of a\nlanguage for producing all and only valid structures.\nNLP involves a deep understanding of human cognition and\nintelligence.\nFor the sake of convenience, the language faculty\nis often studied independently of other cognitive tasks but we\nmust remember that this is only a restricted view taken for conve-\nnience and not the complete and accurate picture in itself. NLP is\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is",
    "concerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\nunfortunate that many have given too much of attention to the\ndetails of the code itself rather than to the mechanisms of produc-\ning and interpreting the codes. For example, syntacticians have\n2.1. INTRODUCTION\n93\noften spent all their life analyzing the intricacies of structure of\nsentences rather than worry about what those sentences really\nmean and how particular syntactic structures encode given mean-\nings and intentions. The central element of language is meaning\nand intention. It is unfortunate that phonology, morphology and\nsyntax have come to be considered the “core” of linguistics rather\nthan semantics. A lot of time, eﬀort and energy has been spent on\nsuperﬁcial details rather than the real, core problems and issues.\nThe available body of knowledge in linguistics and NLP today is\nthus heavily skewed and inevitably, this books reﬂects the same.\nWe will be raising some critical issues with semantics but we may\nnot be able to give satisfactory solutions in all cases.\n2.1.4\nNLP Over the Decades\nWe have seen the origins of NLP within Artiﬁcial Intelligence\nduring the mid ﬁfties. The primary aims were a) computational\nmodelling of human language production and comprehension and\nb) building intelligent systems with natural language capabilities.\nMany NLP systems were developed mainly to demonstrate ideas,\nproblems and possible solutions. They were all toy systems by to-\nday’s standards. One of the major application area was machine\ntranslation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP",
    "translation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP\nsystems came to focus. During the seventies and eighties, much\nmore sophisticated systems were built. Linguistic data resources\nhad expanded and tools and technologies had also matured to\nsome extent. Slowly signs of saturation started showing up. There\nwere barriers that were diﬃcult to cross. Performance of systems\ncould not be improved even with great eﬀort. It took great eﬀort\nand time to develop and improve linguistic knowledge but the\nincremental improvements in performance obtained got smaller\nand smaller.\nThe early nineties saw a major paradigm shift. The limita-\ntions of the knowledge based approach had been well understood.\nIt had been shown that natural language understanding and gen-\neration could be done if all the required knowledge could be made\n94\nCHAPTER 2. FOUNDATIONS OF NLP\navailable but knowledge acquisition became the real bottleneck.\nExperts were hard to ﬁnd, hand-crafting knowledge structures was\nextremely tedious, time consuming and costly and after all that,\nsaturation points would be reached beyond which it is diﬃcult to\nprogress. Focus shifted to machine learning techniques to auto-\nmatically ‘learn’ to solve a problem by looking at a large collection\nof examples. Computing power and memory had increased mani-\nfolds and machine costs had drastically come down. What could\nonce be tried out only on big machines could now be attempted on\na desk top PC. Focus shifted to development and statistical anal-\nysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP",
    "ysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\nhad moved from the research laboratories to the market place.\nThe strengths and weaknesses of purely statistical approaches\nhave now been clearly understood. It is now generally believed\nthat a combination of linguistic and statistical approaches may be\nessential. New application areas have emerged. At one point of\ntime NLP had become almost synonymous with Machine Transla-\ntion, especially in our country. Now Information retrieval, Infor-\nmation Extraction, Automatic Categorization, Automatic Sum-\nmarization, Text Mining have all become major application areas\nfor NLP. There is an increasing conﬂuence and synergy between\ntext and speech technologies. People have started talking about\nmulti-lingual and multi-media applications. There is an increased\nawareness of the importance of language and speech technologies\nat all levels. Many large funded research projects have been initi-\nated all over the world. In India the Government has been showing\na keen interest in developing technologies for Indian languages.\nThere are scores of universities and research organizations that\nhave been working on various aspects of language technologies.\nTeething problems have been largely overcome and attention has\nshifted to applications that can have serious impact on our society.\nWe have come a long way. However, if you look back carefully\nand analyze our achievements and failures, you will ﬁnd that al-\nmost all the critical problems have remained unsolved. We have\nnot been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.",
    "not been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.\nSemantics has taken a back seat\neven in linguistics. Not everything can be done by simply count-\ning words. Perhaps the time has come to rethink and re-plan the\nNLP agenda.\n2.1.5\nLinguistics versus NLP\nMany still think that computational linguistics or NLP can be\ndone by putting together a linguist and a programmer. Just as\nChina plus Philosophy does not yield Chinese Philosophy, Compu-\ntational Linguistics or NLP is not merely computers plus linguis-\ntics. Computers are powerful tools, like mathematics, and have\napplications and uses in almost all branches of study. Computa-\ntional, linguistic, statistical and engineering concepts need to be\nclosely integrated in order to synthesize a new ﬁeld of study called\ncomputational linguistics, NLP or language engineering. The pur-\npose of this book is to give a ﬂavor of such an integrated multi-\ndisciplinary ﬁeld. Understanding or at least appreciation of the\nconcerns, aims, assumptions, jargon, deﬁnitions and methodolo-\ngies of various disciplines concerned is important. People who are\ntrained to think and work like that are in short supply. If this\nbook helps in any way to develop such trained manpower, that\nwould be considered a success.\nComputational linguistics or NLP provides a computational\nview point for linguistics. There are many things that are obvious\nfor human beings and the focus will naturally be only on the un-\nusual, special situations when all this is for human consumption.\nPeople have world knowledge and commonsense and this forms a\ntacit assumption when dealing with people. The goal of NLP, on\nthe other hand, is to make the computer, the idiot box that has no\ncommonsense, world knowledge or human-like reasoning power, to",
    "People have world knowledge and commonsense and this forms a\ntacit assumption when dealing with people. The goal of NLP, on\nthe other hand, is to make the computer, the idiot box that has no\ncommonsense, world knowledge or human-like reasoning power, to\nperform language processing tasks with human-like performance.\nA few examples are not enough. Exhaustive sets of data, rules,\nguidelines and principles need to be provided to handle special,\nunusual situations as also the run-of-the-mill cases. Explaining in\nsimple language is not suﬃcient, we will have to write detailed\nand precise program code.\nLet us take a speciﬁc example. One of the primary goals of\nmodern generative linguistics lead by Noam Chomsky is to under-\nstand how children, exposed to such small and imperfect samples\n96\nCHAPTER 2. FOUNDATIONS OF NLP\nof language use, can learn the structure and properties of their\nnative language, whatever that may be, so easily within just a\ncouple of years. From a computational point of view, an accept-\nable solution to this problem would be a computational model of\nchild language acquisition which can be experimentally tested. We\nshould be able to give controlled input and observe the outputs\nas well as the changes that take place in the internal structure of\nthe model to see what exactly constitutes learning and how the\nchild is actually learning with each input sample. We cannot open\nup somebody’s brain and see before and after changes as a sen-\ntence is being processed by the brain. But with a computational\nmodel such a detailed probing would be feasible. For example, if\nwe build a neural network model, we can see the changes taking\nplace in the various synaptic weights with each iteration.\nThe\nindividual numbers may not make much sense but we can surely\nget a global picture of whether the system is learning, at what\nrate it is learning, has a saturation point been reached etc. De-\nspite the great deal of eﬀort that has gone into building linguistic",
    "The\nindividual numbers may not make much sense but we can surely\nget a global picture of whether the system is learning, at what\nrate it is learning, has a saturation point been reached etc. De-\nspite the great deal of eﬀort that has gone into building linguistic\ntheories of human language learning, it has not been possible to\ncome anywhere near a mathematical or computational model or\ntheory of language learning.\nNLP throws upon both challenges and opportunities. NLP\nforces us to go into each aspect in great detail and with great\nprecision. This exercise in itself brings out many interesting prob-\nlems and issues that were never known or considered with any\nseriousness. Linguists, for example, have never taken the problem\nof segmenting a running text into sentences very seriously espe-\ncially for languages such as English where there are fairly clear\ncut sentence boundary markers such as the period, the question\nmark and the exclamation mark. A detailed study shows, con-\ntrary to initial expectation, that segmenting texts into sentences\nand sentence fragments is not a completely trivial task. In fact\nthis leads to a serious discussion and debate on what really con-\nstitutes a sentence and how do we deal with sentence segments.\nA number of sentences, sentence groups, and/or sentence frag-\nments can be embedded inside a given sentence forming a list.\nAn NLP researcher needs to worry about distinguishing between\na full stop, a decimal point and the dot used in acronyms and\nabbreviations whereas linguists rarely ﬁnd such things interesting\nenough to merit scientiﬁc inquiry. Although we have tried to drive\n2.1. INTRODUCTION\n97\nhome the problem by taking a down to earth example here, the\nnature of diﬀerence in the orientation of linguistics and NLP is\nthe same at all levels of abstraction.\nNLP forces linguistic theories, models, ideas and hypotheses\nto be tested on large scale, real world data. This helps to validate",
    "97\nhome the problem by taking a down to earth example here, the\nnature of diﬀerence in the orientation of linguistics and NLP is\nthe same at all levels of abstraction.\nNLP forces linguistic theories, models, ideas and hypotheses\nto be tested on large scale, real world data. This helps to validate\nthe theories, ﬁnd places where they fail, pick up counter examples\nand reﬁne the theory or model accordingly. The NLP approach\nalso makes available large scale data which can be used to build\ntheories and models. Manual methods are tedious, time consum-\ning and hence limited to small data sets. They are also prone to\nhuman biases and misinterpretations. Testing on large scale real\nworld data often throws out new insights.\nAny aspect of human language processing can be modeled and\ntested in NLP as long as the data and rules to handle all cases\nare covered exhaustively in detail and in precise enough terms for\na computer to follow. Priorities for development may be dictated\nby concerns of practical need etc. but there is really no area of\nlinguistics that NLP cannot touch.\nNLP attaches great importance to computational eﬃciency in\nterms of memory space and computation time.\nLinguistics at-\ntaches great importance to simplicity, elegance and economy of\nexpression but eﬃciency in terms of computational space and time\nis usually not a major concern except in certain areas such as psy-\ncholinguistics. The fundamental assumption in AI is that human\nbeings are intelligent and doing things in a simpler, more elegant,\nfaster and more eﬃcient way is an important element of human\nintelligence. In this sense, if an NLP approach shows a much more\neﬃcient method of solving a particular problem, linguists cannot\naﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It",
    "aﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It\nhelps, for example, to make clear distinctions between open ended\npure research, applied research with clear cut aims an objectives\nwith reference to speciﬁed applications and developmental activi-\nties leading to data generation, products, services etc.\n98\nCHAPTER 2. FOUNDATIONS OF NLP\n2.2\nA Layered View of Computational\nLinguistics\nLinguistics recognizes that there are several appropriate levels of\nlinguistic representation such as phonemes, morphemes, words,\nphrases, clauses, sentences and discourse segments. A word is a\nminimal meaningful unit at a particular level of abstraction while\na sentence is also a minimal meaningful unit but at a diﬀerent,\nhigher level of abstraction. Linguistic theories postulate and show\nthat certain kinds of units are appropriate while others are not.\nThus whether words can be understood in terms of lower level\nunits called morphemes would be a fundamental question in lin-\nguistic theory. Linguistics provides tools and techniques of scien-\ntiﬁc inquiry into human languages at each of these appropriate\nlevels of description.\nWe can also cut words or sentences into arbitrary parts but\nsuch parts may not have any linguistic signiﬁcance. For exam-\nple, the individual characters of which a word is made, carries no\nlinguistic signiﬁcance. The length of words expressed in terms of\nnumber of characters it includes or the length of a sentence in\nterms of the number of words in it are generally considered to be\nof no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language",
    "of no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language\nand each level of linguistic description is studied more or less inde-\npendently of other levels. That is, the interactions between layers\nare presumed to be separable from the core of a given level itself so\nthat the core and the interactions with the neighboring levels can\nbe explored separately to characterize any particular level. For\nexample, Chomsky claims “autonomy of syntax” by saying that\nsyntax can be studied more or less independently of other levels of\nlinguistics. Syntax is presumed to be loosely-coupled with other\nlevels, not tightly and inseparably coupled.\nThe above stratiﬁed view of language raises certain basic ques-\ntions. For example, can we understand the meaning of a sentence\nin terms of the meaning of the words in the sentence? Or is a\nsentence an atomic unit of meaning and breaking it into words\nartiﬁcial and unhelpful? Such questions have been scientiﬁcally\n2.2. COMPUTATIONAL LINGUISTICS\n99\nexplored in great depth and detail within the Indian tradition\ndating back to several thousand years. In fact whether there are\nidentiﬁable units such as words and sentences is a question that\ndeserves careful consideration. As opposed to the stratiﬁed model,\none can posit a holistic view of language and claim that we pro-\ncess and understand language all in one go, not layer by layer. Do\nwe perform a dictionary look up and morphological analysis ﬁrst\nand then perform syntactic analysis and ﬁnally get into seman-\ntics in our minds to understand the meaning of an utterance? Or\ndo we do all these things together, in a closely integrated fashion?\nOn the one hand these are questions of implementation strategies.\nAt the same time, these are basic questions relating to layered or",
    "tics in our minds to understand the meaning of an utterance? Or\ndo we do all these things together, in a closely integrated fashion?\nOn the one hand these are questions of implementation strategies.\nAt the same time, these are basic questions relating to layered or\nmodular view as against a holistic view. Most practical systems\ntake a layered view since it is easier to work level by level, from\nsimpler to more complex, rather than try to do everything in one\ngo. Here we will take the layered view and look into the structure\nand function of words and then sentences .\nWe begin our study of language at the level of words and then\nmove on to the level of sentences. Our primary focus is only on\ntexts and so we will not cover phonetics or phonology - the study\nof the basic sound units in a language at the physical and ab-\nstract levels respectively. We will also not get into discourse level\nanalysis. A lot can be done at the level of words and sentences,\ndiscourse level analyses are a lot more complex, our knowledge\nand understanding at the discourse level is relatively less at the\nmoment, and many applications can be developed even without\ngetting into a detailed analysis at the discourse level. However,\nwe will be exploring statistical analyses techniques in the latter\nhalf, some of which work on entire texts.\n2.2.1\nDictionaries\nKnowledge of language is essential for meaningful communica-\ntion through language. Words of a language, and the phonologi-\ncal, morphological, syntactic and semantic information associated\nwith them, forms a very important part of the knowledge of lan-\nguage.\nKnowing the words is an extremely important part of\nknowing a language. Dictionaries are storehouses of such infor-\nmation and therefore, they have a key role to play in NLP. Also,\ntheoretical linguistics has come to assign an increasingly central\n100\nCHAPTER 2. FOUNDATIONS OF NLP\nrole to the lexicon.\nLongman’s dictionary of contemporary English (1987) deﬁnes",
    "mation and therefore, they have a key role to play in NLP. Also,\ntheoretical linguistics has come to assign an increasingly central\n100\nCHAPTER 2. FOUNDATIONS OF NLP\nrole to the lexicon.\nLongman’s dictionary of contemporary English (1987) deﬁnes\na dictionary as ‘a book that gives a list of words in alphabetical\norder with their meanings in the same or another language and\nusually with their pronunciations’. According to Random House\nDictionary of the English Language (College Edition), a dictio-\nnary is ‘a book containing a selection of the words of a language\nusually arranged alphabetically, giving information about their\nmeanings, pronunciations, etymologies etc.; a lexicon’. These def-\ninitions suggest that a dictionary is essentially a list of words of\na language, typically sorted alphabetically, giving the associated\nmeanings. Pronunciation, part-of-speech and other grammatical\nfeatures, etymology, usage, examples, pictures and so on may also\nbe optionally provided.\nDictionaries can be built for speciﬁc purposes and the con-\ntents and organization would vary accordingly. Thus we can talk\nof a dictionary meant for second language learners, a dictionary\nof technical terms in a speciﬁed domain, or a dictionary for peda-\ngogical purposes. We can have a dictionary of phrasal verbs and\na dictionary of idioms. A children’s dictionary may contain only\nthe most frequent and basic words. The term ’lexicon’, on the\nother hand, is used in a technical sense in linguistics. A lexicon\ncontains a complete inventory of all the words in a language along\nwith associated information conforming to the speciﬁcations of a\ngiven theoretical framework. This information would be organized\nas required by the theory. The term lexicon is also used in NLP to\nstand for a dictionary considered to be one of the components of\nan NLP system. Although a lexicon is diﬀerent from a dictionary,\nthese two terms are sometimes used interchangeably.\nElectronic Dictionaries",
    "as required by the theory. The term lexicon is also used in NLP to\nstand for a dictionary considered to be one of the components of\nan NLP system. Although a lexicon is diﬀerent from a dictionary,\nthese two terms are sometimes used interchangeably.\nElectronic Dictionaries\nDictionaries in the form of printed books are not of much direct\nuse in NLP. Only dictionaries in electronic form can be used by\ncomputer programs. Computer programs can be written to search\nand automatically process such electronic dictionaries. Programs\nalso exist for assisting the very development of electronic dic-\ntionaries. Electronic dictionaries greatly enhance the ﬂexibility,\nconvenience and speed of access by human beings too. Under-\nstandably, therefore, the development of large computerized lex-\n2.2. COMPUTATIONAL LINGUISTICS\n101\nical knowledge bases has emerged as probably the most urgent,\nexpensive, and time consuming task facing linguistics and NLP\ntoday. And this is all the more relevant and exigent in the Indian\ncontext where, what is already available is very little compared to\nwhat is needed.\nOne might think of electronic dictionaries as simply electronic\ncounterparts of printed dictionaries.\nThe same information is\nstored in magnetic medium in computer processable forms and\nused in a similar way. People ’look up’ the dictionary by making\nsuitable requests to a computer program and in turn get the pro-\ngram to display on the screen or print onto paper the information\nrequested by the user. The computer can hold huge amounts of\ninformation and can turn the pages for you very fast. While all\nthis is true, electronic dictionaries are in fact much more powerful.\nThe following paragraphs show how.\nWe go to a dictionary to look up the meaning of a word or\nits correct spelling or pronunciation. Sometimes we may be inter-\nested in knowing more about the grammatical properties, usage,\netymology etc.\nWe can use electronic dictionaries for all these",
    "The following paragraphs show how.\nWe go to a dictionary to look up the meaning of a word or\nits correct spelling or pronunciation. Sometimes we may be inter-\nested in knowing more about the grammatical properties, usage,\netymology etc.\nWe can use electronic dictionaries for all these\npurposes. We can also do many more things that would be diﬃ-\ncult with printed dictionaries. How do you extract all words that\nare both nouns and verbs? How do you make a list of 5 letter\nwords? Or bisyllabic words? Or words with a Sanskrit origin?\nWith printed dictionaries one will have no option than to scan\nthrough the whole dictionary manually, a task that can be ex-\ntremely tedious and time consuming. With an electronic lexical\ndatabase, one could script a few lines of code and get all these\ndone in seconds.\nA researcher interested in morphology may like to extract and\nor reorder the words in the dictionary in the right to left (re-\nversed) alphabetical order so that for example, all ’-ing’ ending\nwords would get grouped together and words ending with ’-ed’\nwould form another cluster.\nThis can be done very easily and\nvery fast on a computer. Electronic lexical databases, properly\ndesigned and organized, can also be easily ’reconﬁgured’ to suit\ndiﬀerent needs. For example, one can create graded dictionaries\nfor children, with vocabularies limited to what can be consid-\nered adequate for various age groups. Clearly, large scale lexical\ndatabases are extremely important and useful linguistic resources\nfor any language.\n102\nCHAPTER 2. FOUNDATIONS OF NLP\nElectronic dictionaries are also directly usable by computer\nprograms. Electronic dictionaries form an integral component of\nalmost every activity in computational linguistics and NLP - word\nprocessing and text critiquing systems, spelling error detection\nand correction, grammar checking, oﬃce automation, morpholog-\nical analysis and synthesis, parsing and generation, machine trans-",
    "almost every activity in computational linguistics and NLP - word\nprocessing and text critiquing systems, spelling error detection\nand correction, grammar checking, oﬃce automation, morpholog-\nical analysis and synthesis, parsing and generation, machine trans-\nlation, question answering systems, story understanding, natural\nlanguage interfaces to databases, computer aided instruction, in-\nformation retrieval, speech synthesis, speech recognition, auto-\nmatic indexing and abstracting, concordance and other statistical\nanalyses, vocabulary studies, stylistics, psycholinguistic studies,\ntaxonomical studies etc.\nWe have seen that electronic dictionaries can be very useful\nfor people as also for NLP applications. Yet another dimension\nof the relationship between dictionaries and computers is the use\nof computers in developing dictionaries. Dictionary development\nis a huge and complex task requiring great skill and expertise on\nthe part of the lexicographer. Classical dictionaries have taken\ndecades to develop. At one point of time words used to be writ-\nten down on cards, cards sorted manually, then word frequencies\ncounted manually and duplicate cards removed and so on. Today\ncomputers provide assistance at practically every stage of dictio-\nnary development. To give an idea of this, we may start with large\nand representative collections of electronic texts, extract words\nand perform a type-token analysis (each distinct word form is a\ntype and each occurrence of a type is a token), perform morpho-\nlogical analysis to extract root words, select words, identify parts\nof speech (POS) using POS tagged corpora or through morpholog-\nical analysis, use a KWIC (Key Word in Context) Concordance\nprogram to extract sentences containing a given key word and\ndecide meanings etc.\nfrom these sentences, select examples of\nusage, and format the entries in the dictionary as required. All\nthese steps can be done semi-automatically.\nThe machine per-",
    "program to extract sentences containing a given key word and\ndecide meanings etc.\nfrom these sentences, select examples of\nusage, and format the entries in the dictionary as required. All\nthese steps can be done semi-automatically.\nThe machine per-\nforms more or less routine things like searching, sorting as also\nmore sophisticated tasks such as morphological analysis or word\nsense discrimination.\nHuman experts take intelligent decisions\nusing their common sense and world knowledge.\nA number of\npowerful tools exist to assist lexicographers in their tasks. Good\ndictionaries can be developed much faster today than was possible\nbefore the age of computers.\n2.2. COMPUTATIONAL LINGUISTICS\n103\nThe structure and contents of a dictionary in printed form\nare frozen at the time of its publication. On the other hand, the\ncontent, organization as well as formatting of electronic dictionar-\nies can be changed easily. An electronic dictionary can grow and\nquickly respond to changes. You do not have to wait for the next\nedition of the dictionary to be printed. Users may also be allowed\nto alter the dictionary as needed.\nA variety of tools exist for developing, formatting, printing,\nmaintaining and using electronic dictionaries.\nStatistical tools\nfor corpus analysis are invaluable for developing dictionaries from\ncorpora. The internal physical representations may vary widely\ndepending upon applications and whether the dictionaries are to\nbe used from web site or CDs or as databases stored in the hard\ndisk of a computer. XML has emerged as the preferred exchange\nformat. A DTD (Document Type Deﬁnition) deﬁnes the structure\nof the dictionary so that other applications can use the dictionary.\nWeb enabled dictionaries require server side and/or client side\nscripts to interact with the users, search the dictionary, format\nthe search results for display etc. Eﬃcient algorithms exist for\nfast searching.\nThere are tools for veriﬁcation and validation.",
    "Web enabled dictionaries require server side and/or client side\nscripts to interact with the users, search the dictionary, format\nthe search results for display etc. Eﬃcient algorithms exist for\nfast searching.\nThere are tools for veriﬁcation and validation.\nThese tools make it possible to develop, maintain and use large\ndictionaries and other lexical resources eﬀectively and eﬃciently.\nWe have seen printed dictionaries that include line drawing\nand maps but electronic dictionaries may be enriched in many\nways that printed dictionaries do not allow. We can include mono-\ntone or colour photographs as also animations to illustrate some\naction ‘by doing and showing it’. We can incorporate speech out-\nput to illustrate correct pronunciations. We can include maps that\nyou can zoom into.\nOur focus here will be electronic dictionaries designed for NLP\napplications. There are three main issues in the design of dictio-\nnaries:\n• What kinds of words are listed in the dictionary?\n• What information should the entries provide?\n• How should that information be organized and structured?\nLet us explore each of these in turn.\n104\nCHAPTER 2. FOUNDATIONS OF NLP\nThe Contents of a Dictionary\nA dictionary consists of lexical entries and the associated infor-\nmation. What kinds of lexical entries should we include in a dic-\ntionary? What kind of grammatical, semantic or other kinds of\ninformation should be include for these lexical entries?\nLet us\nlook at these issues one by one.\nThe content of a dictionary and its organization critically de-\npend on the ways in which NLP applications use the dictionary.\nSpell checkers often use merely a list of words. For a syntactic\nparsing program, the grammatical category and other grammati-\ncal features of words may be suﬃcient and the dictionary may not\neven contain information on pronunciation, etymology or mean-\ning descriptions. However, for a full-ﬂedged natural language un-\nderstanding system, meanings and even usage may be extremely",
    "cal features of words may be suﬃcient and the dictionary may not\neven contain information on pronunciation, etymology or mean-\ning descriptions. However, for a full-ﬂedged natural language un-\nderstanding system, meanings and even usage may be extremely\nimportant.\nThis question of content is also closely tied up with theoreti-\ncal frameworks. In LFG, the lexicon is expected to have separate\nentries for ’hand’, ’hands’, ’handed’, and ’handing’, in spite of\nthe fact that these words are morphologically related according to\nvery simple and regular rules. In the Tree Adjoining Grammar,\nthe lexicon contains sets of trees for each entry. Small’s theory\nof word expert parsing places a heavy demand on the dictionary\ntoo. The lexical entries (word experts) are complicated programs\nwith coroutine structure, the speciﬁcation of which requires not\nonly the detailed knowledge of the architecture of the parser, but\nalso acquaintance with a specialized language for writing word ex-\nperts, besides being able to judge what constitutes linguistically\nrelevant information and how to represent that procedurally, and\nreadiness to bring in arbitrary amounts of more general and com-\nmon world knowledge.\nNo division is made between syntactic,\nsemantic or pragmatic knowledge. Connectionist parsing systems\nrequire a qualitatively diﬀerent kind of a lexicon due to the highly\ndistributed nature of their knowledge sources.\nDictionaries are treasuries of words but what exactly is a\nword?\nGenerally speaking, entries in a dictionary may include\nsingle words (man, intelligence, Delhi), sub-word units which are\nless than words, morphemes which can not occur in isolation (-ed,\n-ment, -es), supra-word units consisting of combination of parts of\ntwo or more words such as blends and abbreviations (can’t, TV,\n2.2. COMPUTATIONAL LINGUISTICS\n105\nlaser) and super-word units consisting of compounds and phrases\n(can not, white house, civil war, kick the bucket).",
    "-ment, -es), supra-word units consisting of combination of parts of\ntwo or more words such as blends and abbreviations (can’t, TV,\n2.2. COMPUTATIONAL LINGUISTICS\n105\nlaser) and super-word units consisting of compounds and phrases\n(can not, white house, civil war, kick the bucket).\nShould we store both the words “table” and “tables” in the\ndictionary? All countable nouns have plural forms. Should we list\nsingular and plural forms of all such nouns? To answer such ques-\ntions we may have to turn to psycholinguistic theories or compu-\ntational considerations. It is unlikely that people store all the sin-\ngular and plural forms of nouns in their mental lexicon since plural\nformation in English is quite regular. It would be a more intelli-\ngent strategy to store, say, only the singular forms and derive the\nplural forms using a rule. Suitable mopho-phonemic rules may be\nused so that “car-cars”, “bus-buses”, “mango-mangoes”, “bush-\nbushes” are all handled properly. What about “child-children”?\nHere one may argue that the regular rule does not seem to ap-\nply and it may be best to store the plural form directly in the\ndictionary rather than make a rule that works just for this one\nword. What about “formula-formulae”, “thesis-theses”, “radius-\nradii”, “datum-data”? Are these regular rules applicable to many\ninstances or exceptional cases? How productive should a hypo-\nthetical rule be before we can accept it as a rule? Linguistic theo-\nries have argued in diﬀerent ways and many diﬀerent models have\nbeen proposed. It is not easy to obtain reliable psycholinguistic\nevidence in all cases. The computational bases to answer these\nquestions are perhaps more easy to understand and apply. Mem-\nory space and computation time are the two basic computational\nresources and a strategy is considered intelligent if these resources\nare used eﬃciently. It takes space to store the words and it takes\nspace to store the rules too. It takes time to search for a word in",
    "ory space and computation time are the two basic computational\nresources and a strategy is considered intelligent if these resources\nare used eﬃciently. It takes space to store the words and it takes\nspace to store the rules too. It takes time to search for a word in\nthe dictionary and it takes time to apply a rule. Practical wisdom\ndictates that only the most productive rules need to be used and\nall other irregular forms can be directly stored in the dictionary.\nMany applications store all derived forms directly in the dictionary\nand use only a handful of inﬂectional morphology rules for plurals\nof nouns, past tense of verbs, degrees of comparison for adjectives\netc. in the case of English. The morphology of Indian languages,\nespecially that of Dravidian languages is extremely complex and\nresearch is still going on to ﬁnd optimal solutions.\nWhat seems adequate for human users is not always good\nenough from an NLP point of view. Most dictionaries meant for\nhuman users list only the root words, not the regular inﬂections.\n106\nCHAPTER 2. FOUNDATIONS OF NLP\nFor example, the entry “baker, n” may give a fairly clear idea to\na human user but the fact that “baker” is a noun form obtained\nfrom the verb “bake” is implicit here. Generalizations are lost\nunless somewhere we note that “weaver”, “teacher”, “player” are\nsimilar. How do we understand the meaning of new word, say\n“gorker”? Where do we store the fact that a “sleeper” in the con-\ntext of railways is a piece of wood used below the rails to hold\nthem in place, and not simply one who sleeps. How do we under-\nstand “waiter” in the context of a restaurant? What is common\nto “car driver” and “screw driver” and what makes them diﬀer-\nent? “hand-writing” and “hand-written” are ﬁne but there is no\nsuch thing as “hand-write”. Lexical semantics has somehow not\nreceived the attention it deserves within the NLP community.\nA large dictionary is not always better than a smaller one.",
    "ent? “hand-writing” and “hand-written” are ﬁne but there is no\nsuch thing as “hand-write”. Lexical semantics has somehow not\nreceived the attention it deserves within the NLP community.\nA large dictionary is not always better than a smaller one.\nThink of a dictionary based spelling error detection and correction\nsystem. Any word found in the dictionary will be taken to be a\nvalid word. If a word is not found in the dictionary, it is taken to\nbe a spelling error and similarly spelled words from the dictionary\nare given out as possible corrections. If you type “lave” instead of,\nsay, “leave”, a large dictionary that includes the rare word “lave”\nwill not even detect your mistake, let alone helping you to correct\nit.\nWhat grammatical information should we include?\nThere\nseems to be general agreement about such notions as ’noun’ and\n’verb’. But exactly what information a dictionary should give is to\nsome extent theory bound. For example, should the lexicon give\ntransitivity, case frames or sub-categorization frames, or argument\nstructures for verbs? Should the lexicon say that ’soldier’ is ’+\nhuman ’? What all semantic features should the lexicon include?\nHow should the lexicon denote the selectional restrictions? How\nexactly should the lexicon represent the fact that ’play’ is either\na noun , or a verb in ﬁrst person (singular or plural), or second\nperson (singular or plural), or third person (plural but not singu-\nlar)? How should the lexicon capture the generalization that other\nverbs also behave the same way? What defaults can a lexicon use\nto avoid saying the same thing over and over? The answers to\nsome of these questions depend on the links between the lexicon\nand morphology and other components that may make use of the\nlexicon.\nThe information that we wish to incorporate directly into the\n2.2. COMPUTATIONAL LINGUISTICS\n107\ndictionary is dependent on what kind of programs use this infor-\nmation. English plurals are often formed by adding an ’s’ as in",
    "and morphology and other components that may make use of the\nlexicon.\nThe information that we wish to incorporate directly into the\n2.2. COMPUTATIONAL LINGUISTICS\n107\ndictionary is dependent on what kind of programs use this infor-\nmation. English plurals are often formed by adding an ’s’ as in\n’books’ and ’dogs’. But ’books’ is pronounced with a ﬁnal [s] and\n’dogs’ is pronounced with a ﬁnal [z] sound. Similarly, in Hindi\nfronting of short /a/ when followed by /h/ is a regular and auto-\nmatic as in /kahna/ [k e hna], /pahcan/ [p e hcan] etc. Since these\nphenomena are quite regular and automatic, we might use a gen-\neral rule to handle this and leave no explicit indication whatever in\nthe lexicon. The adjective ’electric’ is pronounced with a ﬁnal /k/\nbut its noun form ’electricity’ is pronounced with an /s/ ending\nas in ’city’. This phenomenon is less regular and non-automatic\n- it might be necessary to indicate explicitly in some way in the\nlexicon, how ’electric’ and ’electricity’ must be pronounced.\nIn fact a dictionary is a list of exceptions. Words are actually\nassociation between sounds and meanings. These associations are\narbitrary and not rule governed. Hence the dictionary lists words\nand their meanings. Spellings are also quite arbitrary in English\nand an English dictionary must spell out each word. If something\nis very regular and users or computer programs can be expected to\nknow it, there is no need to list such things in the dictionary. We\nneed to list those pieces of information which are unpredictable\nand idiosyncratic with respect to the rules or knowledge avail-\nable elsewhere in the system. A dictionary really stores only the\nexceptions.\nThe problems are more complex when we come to meanings.\nFirstly, printed dictionaries meant for human use give meaning de-\nscriptions using natural language sentences. This causes a chicken\nand egg problem in NLP. The purpose of an NLP program may\nbe to ’understand’ natural language sentences but, in order to",
    "Firstly, printed dictionaries meant for human use give meaning de-\nscriptions using natural language sentences. This causes a chicken\nand egg problem in NLP. The purpose of an NLP program may\nbe to ’understand’ natural language sentences but, in order to\ndo so, the NLP system is required to ’understand’ the meaning\ndescriptions given in the lexicon!\nIn monolingual dictionaries, the meanings are given in the\nsame language as the language whose words are being described.\nHence, use of such dictionaries requires that the users be already\nfamiliar with that language to some extent. Bilingual dictionar-\nies provide descriptions and/or give equivalents for words of one\nlanguage in another language. We can also have multilingual dic-\ntionaries where for each word of one language, equivalents may be\ngiven in several other languages. In any case, since today’s com-\nputers do not understand any natural language, meanings given\n108\nCHAPTER 2. FOUNDATIONS OF NLP\nin natural languages are essentially useless for computers.\nMost printed dictionaries adhere to the general principle of\nreducing complex concepts into a composite of primitive ones,\navoiding straight cases like (the apocryphal) “recursion : see re-\ncursion”! Dictionary builders may be simply told to use the sim-\nplest words in describing the meanings of other words or they\nmay even be given a pre-speciﬁed list of simple words to use for\nthe purpose. Many dictionaries claim that they use only a small\nnumber of simpler words (sometimes even giving a listing of such\nwords), that these words have been used only in their ’central’\nmeanings, that only easily understood derivatives are used in the\nmeaning descriptions and syntax used is simple.\nThis reduces\nthe chicken and egg problem to the boot strapping problem. But\nwhere do we start?\nWe still need to be sure that the descrip-\ntion language is ’understandable’. We will have to specify what\nis meant by ‘central’ or ‘core’ meaning and exactly what mor-",
    "This reduces\nthe chicken and egg problem to the boot strapping problem. But\nwhere do we start?\nWe still need to be sure that the descrip-\ntion language is ’understandable’. We will have to specify what\nis meant by ‘central’ or ‘core’ meaning and exactly what mor-\nphologically derived forms are used. One may have to think of\nmapping the meaning descriptions given in natural language sen-\ntences onto some system of formal semantics and represent the\nmeanings in a certain scheme of knowledge representation. Vari-\nous issues in knowledge representation including representational\nadequacy and parsimony arise.\nThese problems remain largely\nunsolved.\nWe may take the stand point that computers need not un-\nderstand the meanings of words or sentences to perform well in\na given task domain.\nA typical example of such a scenario is\nmachine translation. A dictionary meant for machine translation\nsimply lists the source language terms and their equivalents in\nthe target languages.\nElaborate descriptions, explanations and\nexamples are avoided and more or less one-to-one substitutable\nequivalents are listed. Either a single most common, most neu-\ntral, most widely applicable equivalent can be given, hoping that\nit ﬁts fairly well in most contexts, or a list of possible equivalents\ngiven in order of their presumed eﬀectiveness in various contexts.\nMT proceeds by simply substitution of equivalent words hoping\nthat meanings will be preserved.\nMost NLP applications today do not attempt to go to meaning\nlevel at all. Dictionaries need not contain meanings then. Prag-\nmatic knowledge is also usually not included in a dictionary, but\nthe distinction between lexical semantic knowledge and general\n2.2. COMPUTATIONAL LINGUISTICS\n109\npragmatic knowledge is hazy.\nFrom a computational point of view, completeness and consis-\ntency are desirable. We cannot include “November” and exclude\n“December”.\nThe list of function words and other closed-class",
    "2.2. COMPUTATIONAL LINGUISTICS\n109\npragmatic knowledge is hazy.\nFrom a computational point of view, completeness and consis-\ntency are desirable. We cannot include “November” and exclude\n“December”.\nThe list of function words and other closed-class\nwords must be complete. Redundancy should be minimized. Cir-\ncular deﬁnitions must be avoided. A detailed analysis by computer\npoints to many places where such requirements have not been met\nin many good printed dictionaries. For example, in one dictionary\n’body’ is deﬁned as ’the whole of a person’ and used in a diﬀerent\nsense in deﬁning ’parliament’ as ’a law-making body’. In another\ndictionary a ’box’ is deﬁned as ’a container’ and then a ’container’\nas ’a very large, usually metal box’. Circular deﬁnitions of higher\npath lengths are very diﬃcult to detect.\nLay users as well as students of NLP often have a very sim-\nplistic view of dictionaries and lexicography. Developing lexical\nresources is considered a lower job, more akin to typing work, and\nnobody wants to work on these areas. There is always a fancy\nfor big tasks such as machine translation or speech recognition. If\nsigniﬁcant progress has to be made in any of the applications of\nNLP, lexical resources must be given great attention. One of the\npurposes of this section has been to show that there is more to it\nthan meets the eye when it comes to dictionaries.\nStructure and Organization of a Dictionary\nThe most common organization of dictionary entries is a alpha-\nbetically sorted list of entries. Such a dictionary arranged in al-\nphabetical order makes it simple and eﬃcient to look up a given\nword. Eﬃcient data structures and algorithms exist for searching\nin sorted lists of items. Data structures such as height balanced\nbinary search trees (also called AVL Trees), height balanced m-\nary trees (such as B-Trees, B+ Trees and B* trees), splay trees,\nTRIE etc. have been used. Readers may refer to any good text",
    "in sorted lists of items. Data structures such as height balanced\nbinary search trees (also called AVL Trees), height balanced m-\nary trees (such as B-Trees, B+ Trees and B* trees), splay trees,\nTRIE etc. have been used. Readers may refer to any good text\nbook on Data Structures and Algorithms for more details on these\ntopics.\nHowever, alphabetically sorted arrangement is neither essen-\ntial nor possible in all cases. Not all languages use an alphabetic\nsystem of writing. Languages such as Japanese and Chinese follow\nan ideographic writing system where written symbols correspond\nmore or less directly to meanings. For such languages, it would\n110\nCHAPTER 2. FOUNDATIONS OF NLP\nbe natural to classify words into semantic classes and organize\nthe dictionary accordingly. An ancient example of this is the naa-\nmaliMgaanus’aasana also known as amarakoos’a - a collection of\nnouns in Sanskrit organized into 27 major vargas or classes based\non meaning. All words relating to plants are in one group and\nwords relating to earth are in another. If you know something\nabout a word, you can locate it in the dictionary to know more\nabout it as also about many other words semantically related to\nit.\nIt is interesting to note that amarakoos’a is in verse form\nand it was expected that every student gets the whole dictionary\nby-heart!\nIn fact a dictionary organized in terms of semantic classes\ncould be called a thesaurus. We use a thesaurus for searching for\nthe most appropriate word for a given situation. The idea can be\nextended further to build networks of words inter-related along\nmany dimensions of relationships.\nSuch a structure is called a\nWordNet. We will see more on Thesauri and WordNets in later\nsections.\nThe most appropriate organization of a dictionary depends\nupon the ways we intend to use it. If you wish to look up the\ndictionary for spellings, you do not necessarily know the correct\nspellings to start with and hence simple direct search would not",
    "sections.\nThe most appropriate organization of a dictionary depends\nupon the ways we intend to use it. If you wish to look up the\ndictionary for spellings, you do not necessarily know the correct\nspellings to start with and hence simple direct search would not\nwork. You will need a tool that can ﬁnd out words closely related\nto the given word in terms of spellings or pronunciation. Then\nyou could search for ’seperate’ to know the correct spelling as well\nas pronunciation or meaning of the word ’separate’. You could\ntype-in ’paradime’ to locate ’paradigm’. If you expect frequent\nsearches for words of given grammatical categories, organization\nin terms of diﬀerent major grammatical categories may be a better\nchoice. Organization based on frequency of occurrence of words\nmay be better suited in other situations.\nComputationally, we must ensure that searching and process-\ning are eﬃcient in terms of memory space and computation time.\nAlgorithms for searching and other kinds of processing are closely\ntied up with the organization and structures used.\nBuilding electronic dictionaries\nThere are two major sources of information for building electronic\ndictionaries for NLP applications. We may use existing dictionar-\n2.2. COMPUTATIONAL LINGUISTICS\n111\nies as a starting point or we may use corpus based methods. We\nsketch both of these alternatives brieﬂy here.\nOne might think of starting a new lexicographic project to\ncreate an electronic dictionary from scratch. It is obvious that\ncreating a dictionary is an enormous task in terms of manpower,\ntime and ﬁnancial support required. It also takes a great deal of\nlexicographic experience to do a good job of it. If one is interested\nin developing a dictionary for a given NLP application, it may not\nalways be a very good idea to start from scratch. The complexity\nof the dictionary building task is often under-estimated by com-\nputer scientists. It is not merely a typing job.",
    "in developing a dictionary for a given NLP application, it may not\nalways be a very good idea to start from scratch. The complexity\nof the dictionary building task is often under-estimated by com-\nputer scientists. It is not merely a typing job.\nA better alternative is to depend on the dictionaries already\navailable in the book form and attempt to extract and reorganize\nthe required information. This way we are making use of the enor-\nmous amount of thought, research and experience that has gone\ninto making these dictionaries. The information these dictionaries\ncontain is likely to be highly reliable and authentic.\nUsing printed dictionaries requires typing - a time consuming,\ntedious and error prone task. Alternative methods of converting\nprinted pages into electronic texts include OCR (Optical Charac-\nter Recognition) and ASR (Automatic Speech Recognition). You\ncan scan the printed pages and use an OCR system to convert the\nscanned images into editable texts. Similarly, you may read aloud\nthe contents of the printed dictionary and the ASR system will\nrecognize your speech and give you the content as electronic text.\nBoth OCR and ASR systems are far from perfect, especially when\napplied to dictionaries and a substantial amount of eﬀort will go\ninto editing and proof reading. OCR systems have started ap-\npearing and ASR systems are still in the research stage for Indian\nlanguages.\nEarlier, dictionaries were available in “Machine Readable”\nform, typically as typesetting tapes. These tapes could be read\ninto the computer using a suitable device driver program. Ma-\nchine Readable tapes of dictionaries are, however, basically de-\nsigned only for typesetting and printing the dictionaries, not for\nbuilding electronic dictionaries. This makes the process of creating\nan electronic version of the dictionary more complex. Nowadays,\ndictionaries are available in electronic form either for downloading\nor for interactive search from a website or in CD form. Thus there",
    "building electronic dictionaries. This makes the process of creating\nan electronic version of the dictionary more complex. Nowadays,\ndictionaries are available in electronic form either for downloading\nor for interactive search from a website or in CD form. Thus there\nmay not be any need for starting from printed texts.\n112\nCHAPTER 2. FOUNDATIONS OF NLP\nPrinted dictionaries are usually meant for human beings - in-\ntelligent beings with common sense and world knowledge, capable\nof understanding natural languages. Such dictionaries are very\ndiﬀerent from the dictionaries we need for NLP speciﬁc applica-\ntions. A great deal of thought and care is required in extracting\nthe required content and in restructuring and reorganizing this\ncontent. Available printed dictionaries are very useful resources\nbut rarely directly usable. Substantial eﬀort is required to de-\nvelop dictionaries for NLP applications. For example, elaborate\ndescriptions, examples and explanations are not useful for a dic-\ntionary meant for machine translation. Substitutable equivalents\nare required instead.\nIn the case of Indian languages, the lexical databases we have\ntoday are still very meager.\nWe have nearly 150 diﬀerent lan-\nguages in our country and in many cases we do not even have vo-\ncabulary lists, let alone dictionaries. We do not have, even in the\nprinted form, the required monolingual, bilingual or multilingual\ndictionaries. Some of the printed dictionaries available are very\nold and outdated. There are no thesauri even for many major lan-\nguages. It is only in very recent times that electronic dictionaries\nand other lexical resources have started appearing. Large, com-\nprehensive, dependable, uptodate dictionaries in electronic form\nsuitable for NLP applications are badly required today.\nDictionaries can also be developed starting from corpora. Some\nof the best dictionaries for English have been developed using the\ncorpus based methods. Large and representative corpora are use-",
    "suitable for NLP applications are badly required today.\nDictionaries can also be developed starting from corpora. Some\nof the best dictionaries for English have been developed using the\ncorpus based methods. Large and representative corpora are use-\nful at practically all stages of dictionary development. Corpora\nassist in extracting words, grammatical categories and features,\ncontexts, meanings and word senses, usage, etc.\nA number of\ncomputational tools exist for analyzing corpora and extracting,\nﬁltering, sorting and checking the relevant pieces of information.\nCorpora in Indian languages have started appearing. About\n3 Million word corpora are now available for the major languages.\nThese corpora are not suﬃcient for many applications. A 35 Mil-\nlion word corpus now exists for Telugu. Large scale corpora will\nhopefully become available in all the major languages of India\nsoon. There will still be challenges to be overcome before good\ndictionaries can be developed. Indian languages, especially the\nlanguages of the Dravidian family, are characterized by an ex-\ntremely rich system of morphology. What kinds of words must\n2.2. COMPUTATIONAL LINGUISTICS\n113\nbe listed in the dictionary and what kinds of word forms can be\nobtained by rules of morphology is not yet very clear. Exactly\nwhat kind of grammatical information we should provide for a\ngiven word or even the set of POS tags to use are open questions\ntoday.\n2.2.2\nThesauri and WordNets\nThesaurus\nIn very general terms, a thesaurus is a treasury or a storehouse;\nhence, a repository, especially of knowledge; often applied to a\ncomprehensive work, like a dictionary or encyclopedia.\nMore\nspeciﬁcally, a thesaurus is a book containing a classiﬁed list of\nsynonyms, organized to help you ﬁnd the word you want but can-\nnot think of.\nRoget’s thesaurus describes a thesaurus as ‘the opposite of a\ndictionary. You turn to it when you have the meaning already\nbut don’t yet have the word. It may be on the tip of your tongue,",
    "synonyms, organized to help you ﬁnd the word you want but can-\nnot think of.\nRoget’s thesaurus describes a thesaurus as ‘the opposite of a\ndictionary. You turn to it when you have the meaning already\nbut don’t yet have the word. It may be on the tip of your tongue,\nbut what it is you don’t yet know. It is like the missing piece\nof a puzzle. You know well enough that the other words you try\nout won’t do. They say too much or too little. They haven’t the\npunch or have too much. They are too ﬂat or too showy, too kind\nor too cruel. But the word which just ﬁlls the bill won’t come, so\nyou reach for the Thesaurus’.\nWe go to a thesaurus when we have an idea, some concept\nor a meaning in our mind but we are unable to get the just the\nright word that ﬁts our need. We have some word on hand but\nwe somehow feel that there should be a better word, a word that\nsays more precisely what we wish to say, a word that is best for\nthe current context. A thesaurus usually contains an index from\nwhere we can start. We look up the the index for the tentative\nword we have with us, a word that approximates what we wish\nto say but not quite exactly. The index tells us which locations\nin the thesaurus we need to see. We go to those locations and\nhopefully we will get the word that we are looking for. At times,\nwe get more ideas and we may want to continue searching from\nthe words we just got and we may go on several rounds in this\nfashion.\nThe content of a thesaurus is, therefore, very similar to that\n114\nCHAPTER 2. FOUNDATIONS OF NLP\nof a dictionary - words and meanings. A dictionary is typically\norganized in alphabetical order so that you can quickly locate\nthe word of interest and then you can get the correct spelling,\npronunciation, meanings, usage, etymology and other such pieces\nof information associated with that word. A thesaurus, on the\nother hand, is organized in terms of an ontology - a hierarchy of\nconcepts, and the words are structured into groups that convey\na speciﬁc meaning.",
    "pronunciation, meanings, usage, etymology and other such pieces\nof information associated with that word. A thesaurus, on the\nother hand, is organized in terms of an ontology - a hierarchy of\nconcepts, and the words are structured into groups that convey\na speciﬁc meaning.\nThe diﬀerence between a dictionary and a\nthesaurus, therefore, is more of structure and organization rather\nthan that of content. In other words, dictionaries are typically\nsemasiological in direction, i.e., name to notion or form to content,\nwhereas thesauri are typically onomasiological i.e., notion to name\nor content to form.\nBoth the dictionary and the thesaurus contain words of a given\nlanguage and the meanings. Given this, it makes a lot of sense\nto consider a dictionary and a thesaurus as simply two diﬀerent\nviews of the same data, rather than as two entirely diﬀerent enti-\nties. Why should we store the same words, once structured as a\ndictionary, and then again structured in a diﬀerent way as a the-\nsaurus? Especially when we are talking of electronic dictionaries\nand thesauri, it appears to be a good idea to store the words only\nonce and provide two diﬀerent indexing mechanisms, one to use\nthe words as a dictionary, and another to use the same words as\na thesaurus. We have seen that amarakoos’a, the Sanskrit dictio-\nnary, is actually organized in terms of semantic groups. All you\nneed is an eﬃcient indexing mechanism so that you can also look\nup words based on spellings rather than meaning classes.\nConstructing a thesaurus is not an easy job. It requires, like\nother lexicographic works, an in-depth and thorough understand-\ning of words, their meanings and behavior. It also requires a great\ndeal of time and eﬀort even with all the modern information tech-\nnology tools we have today. Developing a suitable ontology is itself\na huge task. Whether we see the world through the language we\nuse or we look at the language in terms of our real world experi-",
    "deal of time and eﬀort even with all the modern information tech-\nnology tools we have today. Developing a suitable ontology is itself\na huge task. Whether we see the world through the language we\nuse or we look at the language in terms of our real world experi-\nences is not a point for debate here but surely the two are closely\nrelated. Any attempt to classify the words of a language and im-\npose a structure on them amounts in some sense to an attempt\nto classify and structure the whole world. There is no single best\nway to do this and whichever way you do this, there is something\nto gain and something to lose. Striking a good trade-of among\n2.2. COMPUTATIONAL LINGUISTICS\n115\nvarious competing possibilities is a diﬃcult challenge. Once the\nontology is designed, selected or adapted, the entire set of words\nin the language needs to be grouped into semantic classes so that\nthey can be placed suitably in the hierarchy. Developing thesauri\ntakes a lot of time and eﬀort. No wonder there are no thesauri at\nall for many languages even till date. For example, Kannada, a\nlanguage spoken by more than ﬁfty million people and with vast\nand rich literature dating back to at least 9th century AD, had\nno thesaurus till date.\nGiven this scenario, technology for automatic construction of\nthesauri is of great current interest. An automatically constructed\nthesaurus may not be as good as one that is carefully handcrafted\nby lexicographers.\nBut it can serve an immediate need.\nAlso,\na thesaurus so generated can be viewed as a raw material for\nlinguists and lexicographers to do further research and develop-\nment. The contents of a thesaurus is similar to the content of\na dictionary, only we also need some way of grouping together\nsemantically related words. Here we show how a thesaurus can\nbe automatically constructed from a suitable bilingual dictionary.\nWe show some examples from a Kannada thesaurus constructed\nby us recently.",
    "a dictionary, only we also need some way of grouping together\nsemantically related words. Here we show how a thesaurus can\nbe automatically constructed from a suitable bilingual dictionary.\nWe show some examples from a Kannada thesaurus constructed\nby us recently.\nTo construct a thesaurus automatically, the data needed in-\nclude the words of the language, the grammatical categories and\nother relevant features, and the meanings. Diﬀerent words may\nhave same spellings and a word may have many meanings. It is\nimportant to keep these things in mind while developing a the-\nsaurus. Perhaps the best single source of all these required pieces\nof information is a bilingual dictionary of equivalents. Target lan-\nguage words given as equivalents to a given source language word\ncould be expected to be semantically closely related if not syn-\nonymous. Thus by a suitable indexing technique, we should be\nable to extract related words and hence form a thesaurus. The\nfollowing skeleton of an algorithm shows how such an indexing\ncan be obtained:\nALGORITHM:\nINPUT: A BILINGUAL DICTIONARY OF EQUIV-\nALENTS\nOUTPUT: A THESAURUS\n#First Create a Reverse Index:\n116\nCHAPTER 2. FOUNDATIONS OF NLP\nFor each entry in the dictionary with head word W\nFor each the categories i = C1, C2, ... Cn\nFor each of the meanings j = M1, M2, ... Mp\nFor each synonym k = S1, S2, ... Sq\nindex(i,j,k) = W\n# Create the thesaurus index:\nFor each word W\nFor all HW = index(i,j,W)\nsynset(i,j,W) = synset(i,j,W) Union (i,j,X) for\nall index(i,j,X) = HW\nNote that the algorithm keeps the synsets separately for each\ncategory and each meaning and thus users should be able to lo-\ncate the word they are looking for without mixing up diﬀerent\ngrammatical categories or diﬀerent senses of a given word. The\nalgorithm can be implemented eﬃciently using suitable data struc-\ntures and hashing techniques. It takes only a few minutes to gen-\nerate the complete thesaurus on a desktop personal computer.",
    "grammatical categories or diﬀerent senses of a given word. The\nalgorithm can be implemented eﬃciently using suitable data struc-\ntures and hashing techniques. It takes only a few minutes to gen-\nerate the complete thesaurus on a desktop personal computer.\nThe thesaurus so generated may then be enhanced, enriched and\nreﬁned further.\nWe show below examples from a thesaurus for Kannada gen-\nerated automatically from an available English-Kannada dictio-\nnary.\nThe dictionary itself was developed for the purposes of\nmachine aided translation and as such gave more or less substi-\ntutable equivalents. Several possible equivalents were listed in the\ndictionary to give maximum choice for the users of the machine\ntranslation system. These semantically related words have been\ngrouped together to construct the thesaurus. No in-depth analy-\nsis has been performed, only simple re-grouping of words.\nnooDu :\nSynset\nCategory\nSense\nparis’iilisu\nv\nLOOK\ndiTTisu\nv\nLOOK\nkaaNu\nv\nLOOK\ntooru\nv\nLOOK\n2.2. COMPUTATIONAL LINGUISTICS\n117\nmane :\nSynset\nCategory\nSense\nkaTTaDa\nn\nBUILDING\nsadana\nn\nHOUSE\ngRha\nn\nHOUSE\nnivaasa\nn\nRESIDENCE\nvaasasthaana\nn\nRESIDENCE\nvaasa\nn\nHABITATION\niruvu\nn\nHABITATION\nbiiDu\nn\nHABITATION\nvasati\nn\nHABITATION\ncikka :\nSynset\nCategory\nSense\nsvalpa\na\nLITTLE\nkoMca\na\nLITTLE\ntusa\na\nLITTLE\ngiDDa\na\nSHORT\nkuLLa\na\nSHORT\nmooTu\na\nSHORT\nsaNNa\na\nSMALL\npuTTa\na\nSMALL\nkiriya\na\nSMALL\nkSudra\na\nSMALL\npuTaaNi\na\nTINY\nkaDime\na\nLESS\neLeya\na\nYOUNG\nhareyada\na\nYOUNG\nyauvanaavastheya\na\nYOUNG\nyuvakanaada\na\nYOUNG\nFIG 2.2 Examples from a Machine Constructed Kannada\nThesaurus\n118\nCHAPTER 2. FOUNDATIONS OF NLP\nHere the diﬀerent meanings and usages are indicated by En-\nglish words. This was a natural choice since the starting point\nwas an English-Kannada dictionary.\nClearly, the words we get from this thesaurus are not always\nexactly synonymous. The whole idea of a thesaurus is to provide a\ntool to the user to explore the semantic space of words by oﬀering",
    "glish words. This was a natural choice since the starting point\nwas an English-Kannada dictionary.\nClearly, the words we get from this thesaurus are not always\nexactly synonymous. The whole idea of a thesaurus is to provide a\ntool to the user to explore the semantic space of words by oﬀering\nterms that are related in some way to the given word. Users are\noften not looking for exact synonyms, they are in fact looking for\nterms that that ﬁt the particular usage on hand.\nWordNet\nWordNet is an on-line lexical reference system whose design is\ninspired by psycholinguistic theories of human lexical memory.\nEnglish nouns, verbs, adjectives and adverbs are organized into\nsynonym sets, each representing one underlying lexical concept.\nDiﬀerent relations link the synonym sets.\nWordNet was devel-\noped by the Cognitive Science Laboratory at Princeton Univer-\nsity. Similar eﬀorts are now going on for many other languages of\nthe world.\nWordNet groups together words into synonym sets called syn-\nsets. Various types of relationships between synsets are depicted.\nSome of these relationships are:\n• Hypernym: The generic term used to designate a whole class\nof speciﬁc instances. Y is a hypernym of X if X is a (kind\nof) Y. A rose is a kind of a ﬂower.\n• Hyponym: The speciﬁc term used to designate a member of\na class. X is a hyponym of Y if X is a (kind of) Y.\n• Meronym: The name of a constituent part of, the substance\nof, or a member of something. X is a meronym of Y if X is\na part of Y. A ﬁnger is a part of a hand.\n• Holonym: The name of the whole of which the meronym\nnames a part. Y is a holonym of X if X is a part of Y.\n• Troponym: A verb expressing a speciﬁc manner elaboration\nof another verb. X is a troponym of Y if X is to Y in some\nmanner. Limping is a kind of walking.\nHere is the search result for the word “table” from WordNet:\n2.2. COMPUTATIONAL LINGUISTICS\n119\nThe noun ”table” has 6 senses in WordNet.\n1. table, tabular array - (a set of data arranged in rows",
    "of another verb. X is a troponym of Y if X is to Y in some\nmanner. Limping is a kind of walking.\nHere is the search result for the word “table” from WordNet:\n2.2. COMPUTATIONAL LINGUISTICS\n119\nThe noun ”table” has 6 senses in WordNet.\n1. table, tabular array - (a set of data arranged in rows\nand columns; ”see table 1”)\n2. table - (a piece of furniture having a smooth ﬂat top\nthat is usually supported by one or more vertical legs;\n”it was a sturdy table”)\n3. table - (a piece of furniture with tableware for a meal\nlaid out on it; ”I reserved a table at my favorite restau-\nrant”)\n4. mesa, table - (ﬂat tableland with steep edges; ”the\ntribe was relatively safe on the mesa but they had to\ndescend into the valley for water”)\n5. table - (a company of people assembled at a table for\na meal or game; ”he entertained the whole table with\nhis witty remarks”)\n6. board, table - (food or meals in general; ”she sets a\nﬁne table”; ”room and board”)\nClearly, WordNets are very useful lexical databases.\nMany\nword sense disambiguation programs use the WordNet as the pri-\nmary basis for deﬁning sense classes as also for determining word\nsenses based on the information and examples contained in this\nlexical database.\nSome work has been initiated for developing\nWordNets in Indian languages too.\nThe more recent Berkeley FrameNet project aims at creat-\ning an on-line lexical resource for English based on frame se-\nmantics and supported by corpus evidence. The aim is to docu-\nment the range of semantic and syntactic combinatory possibilities\n(valences) of each word in each of its senses, through computer-\nassisted annotation of example sentences and automatic tabula-\ntion and display of the annotation results.\nFrameNet is in its name obviously modeled after WordNet,\nand the intention has always been for FrameNet to be both a\ndictionary and a thesaurus, much in the sense that WordNet is.\nA major diﬀerence is that the FrameNet database is founded on",
    "tion and display of the annotation results.\nFrameNet is in its name obviously modeled after WordNet,\nand the intention has always been for FrameNet to be both a\ndictionary and a thesaurus, much in the sense that WordNet is.\nA major diﬀerence is that the FrameNet database is founded on\ncorpus attestations, and that it includes examples, taken from\nthe corpus, of each sense and each grammatical variant of each\n120\nCHAPTER 2. FOUNDATIONS OF NLP\nword in each sense, going far beyond the ‘Someone\ns some-\nthing’ patterns found in WordNet. It also diﬀers from WordNet\nin recognizing relationships among words in a single frame that\nare of diﬀerent parts of speech.\nOntology\nThe term ontology is used in diﬀerent disciplines with somewhat\ndiﬀerent connotations. Philosophers have been using this term for\na long time, and AI researchers have borrowed and used the term\nin a somewhat diﬀerent sense. Ontologies, as used in NLP and\nrelated disciplines is simply an organization of concepts and their\ninter-relationships. The crucial point is that an ontology is deﬁned\nin terms of concepts, not words of a particular language. Concepts\nare generally independent of language and hence a given ontology\nis expected to be useful for various languages.\nOftentimes the\nconcepts are represented using words of a particular language but\nthey are concepts, not words nonetheless.\nThe organization of\nwords in a hierarchical structure in the Roget’s thesaurus could\nbe considered an ontology.\nOne may think of a general ontology of the whole world or\nsmaller, specialized ontologies that are relevant for particular do-\nmains of applications. There is no single perfect way of describing\nthe world, or a sub-world for that matter. There is thus always a\nquestion of which way of structuring the concepts is the best? For\nNLP applications, the criteria would naturally be in terms of the\napplicability and suitability for speciﬁc applications. A structure",
    "the world, or a sub-world for that matter. There is thus always a\nquestion of which way of structuring the concepts is the best? For\nNLP applications, the criteria would naturally be in terms of the\napplicability and suitability for speciﬁc applications. A structure\nrestricts and limits the focus of attention and guides our search.\nIf the structure is well designed, it takes us to the right places\nwith minimal eﬀort and thus useful. If the structure is not good,\nit may lead us away from the goal, confuse us or prevent us from\nreaching the goal quickly. Thus design of ontologies is a complex\nmix of science and art.\n2.2.3\nMorphology\nMorphology is the study of structure, properties and formation\nof words. We are all familiar with the notion of a word - apple,\neat, good are English words.\nWe can similarly name words in\nother languages. Words form one of the most fundamental units\n2.2. COMPUTATIONAL LINGUISTICS\n121\nof linguistic structure. As children we learnt to speak single words\nand as we grew up picked up thousands of words and a variety of\ninformation associated with those words. The list of all the words\nof a given language is referred to as its lexicon.\nWhen we hear somebody speak our language, we can recog-\nnize the words as they speak. When we read printed text, we\ncan see the words neatly laid out on paper separated by spaces.\nExistence of words seems obvious. Yet, if you listen to a language\nthat you do not know, you will hear only a continuous blur of\nsounds and you ﬁnd it hard to recognize individual words. You\ntend to think the speaker is speaking too fast. Recognizing indi-\nvidual words from a continuous stream of sounds when somebody\nspeaks is an extremely complex task.\nThe ability to recognize\nwords constitutes a major part of language comprehension. When\nyou “know” a language, you have mastered the art of recogniz-\ning words without eﬀort. You will have in fact understood many\nproperties associated with words albeit unconsciously.\nWhat is a word?",
    "The ability to recognize\nwords constitutes a major part of language comprehension. When\nyou “know” a language, you have mastered the art of recogniz-\ning words without eﬀort. You will have in fact understood many\nproperties associated with words albeit unconsciously.\nWhat is a word?\nIs there a need at all to give a formal deﬁnition of a word? Let us\nsee. Apple is surely a word but what about apples? A dictionary\nof English language is very likely to list the ’word’ apple but is very\nunlikely to list the ’word’ apples. Should we deﬁne words based\non their listing in a dictionary? Do we then say apples, eating etc.\nare not ’words’? Should we consider compute, computer, comput-\ning, computability, computerize, computerization, computerizable,\ncomputerizability as unrelated and independent words in their own\nright? In that case, are we not losing the inter-connections among\ntheir meanings and thus a good deal of generalization? Is ice-\ncream one word or two? The notion of a word is more complex\nthan we may initially tend to think. Let us try by giving some\ndeﬁnitions for a word and see the implications.\n1. A Word is a Sequence of Characters\nLook at printed text. A word is simply a sequence of charac-\nters separated by spaces. A character is any letter of the alphabet,\na punctuation mark or any other special symbol such as\n’, %, (, $\n122\nCHAPTER 2. FOUNDATIONS OF NLP\nThis deﬁnition is not very precise. What exactly do we mean\nby spaces? In computer typed texts, a space can be a blank space\nor a tab space. We may not actually have a space at the beginning\nor end of a line of text and so the invisible newline character must\nalso be considered a kind of space. The beginning of a text ﬁle and\nthe end of a ﬁle also function as word separators or terminators.\nA good deﬁnition must therefore list the set of possible delimiting\ncharacters.\nWords are usually made up of only alphabetic characters and\nso we may have to treat all punctuation marks and other special",
    "the end of a ﬁle also function as word separators or terminators.\nA good deﬁnition must therefore list the set of possible delimiting\ncharacters.\nWords are usually made up of only alphabetic characters and\nso we may have to treat all punctuation marks and other special\nsymbols as delimiters. Quote marks are not part of the words but\nthen do we consider Rama’s as one word or two? Is ‘s’ a word?\nHyphens may have to be included as part of words but one must\nnot confuse hyphens with dashes or the minus sign!\nIt is also important to make it clear that a word is a sequence\nof non-delimiting characters - otherwise we will be allowing spaces\ninside words. However, if you wish to treat compound words like\nice cream as single words, then spaces must be allowed. When do\nwe allow spaces and when not?\nIt is clear that there can be no ideal or perfect deﬁnition.\nDiﬀerent applications require diﬀerent treatments.\nIf one were\ntokenizing a C program, the set of delimiting and non-delimiting\ncharacters can only be decided based on what constitutes a valid\ntoken in C language. Given a set of delimiting and non-delimiting\ncharacters, however, it is possible to eﬃciently segment texts into\nwords. The following diagram shows how:\nINSIDE\nOUTSIDE\nWORD\nWORD\nDELIM: IGNORE\nNON−DELIM: START−A−WORD\nDELIM: OUTPUT−WORD\nNON−DELIM:\nACCUMULATE−WORD\nFIG 2.3 Tokenization into Words\n2.2. COMPUTATIONAL LINGUISTICS\n123\nFinite State Machines:\nLet us digress a bit into Finite State Machines. A Finite State\nMachine, or a Finite State Automaton as it is also called, is a ﬁnite\nset of states interconnected by directed labelled arcs. One of the\nstates is designated as the start state and some of the states may\nbe designated as terminal states. Such a machine can be used to\nrecognize strings that belong to a particular language. We start\nfrom the start state and for each symbol in the given string, we\nmake a transition from the current state to a new state to which",
    "be designated as terminal states. Such a machine can be used to\nrecognize strings that belong to a particular language. We start\nfrom the start state and for each symbol in the given string, we\nmake a transition from the current state to a new state to which\nthere is an arc labelled with that symbol. When we have ﬁnished\nreading all the symbols in the given string, if we end up in any of\nthe terminal states, we say the machine accepts the given string.\nOtherwise the string is rejected.\nThe machine is therefore an\nabstract characterization of all the strings of a language and can\ntherefore be called a grammar. Every ﬁnite state machine deﬁnes\na grammar. Languages recognized by ﬁnite state machines are\ncalled Regular languages.\nNote that the above diagram for tokenization into words is not\nexactly a ﬁnite state machine. It has a ﬁnite number of states, in\nfact just two states. The starting state is “outside word”. There is\nno terminal state. Thus there are no strings accepted by this ma-\nchine. The arcs are labelled with DELIM and NON-DELIM and\ncan be understood as sets of arcs, one for each speciﬁed delimit-\ning and non-delimiting symbol in the alphabet. The arc labels are\naugmented with actions to be taken while making the transition.\nNo such augmentation is permitted in a Finite State Machine.\nNonetheless, formulating the problem along these lines helps us\nin making good designs and implementations. We may, for ex-\nample, have a single variable called “current-state” and allow it\nto take one of the two values “INSIDE WORD” and “OUTSIDE\nWORD”. The variable is initially assigned the value “OUTSIDE\nWORD”. Each token in the input is either a DELIM or a NON-\nDELIM and we can only be in one of the two states. Thus there\nare only four possibilities to consider. Accordingly state transi-\ntions are made, that is, the value of the current-state variable is\nset. Actions such as START-A-WORD, ACCUMULATE-WORD\nand OUTPUT-WORD can be written as separate functions and",
    "are only four possibilities to consider. Accordingly state transi-\ntions are made, that is, the value of the current-state variable is\nset. Actions such as START-A-WORD, ACCUMULATE-WORD\nand OUTPUT-WORD can be written as separate functions and\ncalled during the appropriate state transitions. We thus have a\nvery simple, neat and generic solution that can be easily under-\n124\nCHAPTER 2. FOUNDATIONS OF NLP\nstood and adapted to a variety of tokenization requirements.\nIt is possible that there is more than one arc going out from\na given state and labelled with the same symbol.\nFurther, we\nmay allow ǫ arcs, arcs that allow us to make a state transition\nwithout consuming any input symbol. The machine is then called\na Non-Deterministic Finite State Automaton (NDFA or NFA). If\nthere is no such non-determinism, the machine is called a Deter-\nministic Finite State Automaton (DFA). For every NFA there is\nan equivalent DFA and hence NFAs are not in any sense more\npowerful than DFAs. We can start with either a DFA or an NFA\nbased on our convenience and convert the NFA into an equivalent\nDFA as and when desired. There is also a procedure by which an\nequivalent minimal DFA (that is, a DFA with minimum number\nof states) can be constructed for any given DFA. Also Regular Ex-\npressions are equivalent to Finite State Machines. We thus have\na powerful set of techniques to help us deﬁne grammars precisely\nand recognize strings of the corresponding languages eﬃciently.\nRecognition of strings using a DFA can be done with linear time\ncomplexity and hence about the fastest one can do.\nFinite State Machines capture Regular Languages. Natural\nlanguages have been shown to be not regular but it is possible\nto divide the problem so that some parts become Regular. Many\nimportant aspects of structure at the level of words and some as-\npects of structure of sentences can be captured using Finite State\nMachines.\nIn particular, linear precedence, repetition, alterna-",
    "to divide the problem so that some parts become Regular. Many\nimportant aspects of structure at the level of words and some as-\npects of structure of sentences can be captured using Finite State\nMachines.\nIn particular, linear precedence, repetition, alterna-\ntives and optional items can be handled elegantly using Finite\nState Machines. Finite State Machines form a sound theoretical\nbasis for dealing with these aspects of structure of natural lan-\nguages. Systems designed and implemented based on the princi-\nples of these Finite State models can be simpler, neater and more\neﬃcient in terms of both grammar development and the recogni-\ntion or parsing algorithms. A lot of work done so far in Indian\nlanguages are completely ad-hoc, based on hoch-poch pieces of\nprogram code. There is no point in going for such ad-hoc pro-\ngramming when good theoretical models exist. Readers are ad-\nvised to refer to books on Theory of Computation for more details\nof Formal Languages, Grammars and Machines.\nFinite State Machines are Regular Expressions are equivalent.\nMany good operating systems and programming languages pro-\nvide support for regular expressions. Many basic tools including\n2.2. COMPUTATIONAL LINGUISTICS\n125\ntext editors allow regular expression based searching, replacement,\nmatching, ﬁltering etc. You can do with a few simple commands\nwhat would otherwise require a lot of programming. All students\nand researchers interested in NLP must be encouraged to master\nthese basic tools. To give just one example, we can take a sentence\nas input and break it into a list of words by using a single simple\ncommand in Perl:\n$words = split(/\\s+/,$line)\nHere the line is split into words using one or more white space\ncharacters as delimiters. You could of course replace the regular\nexpression\ns+ with any other regular expression to perform a diﬀerent split.\nIf something could be done in one line, how do we justify hundreds\nof lines of hoch-poch program code that is not based on any sound",
    "characters as delimiters. You could of course replace the regular\nexpression\ns+ with any other regular expression to perform a diﬀerent split.\nIf something could be done in one line, how do we justify hundreds\nof lines of hoch-poch program code that is not based on any sound\ntheoretical principles? There is too much of ad-hocism in NLP in\nIndia today.\nLet us get back to the question of what constitutes a word.\nThe above deﬁnition of a word based on spelling treats structurally\nand semantically related words as independent words in their own\nright. ‘eat’, ‘eats’, ‘eating’, ‘eaten’, ‘ate’ are all diﬀerent, inde-\npendent words. Signiﬁcant generalizations in language are lost.\nThere are languages where words are written together without\nintervening spaces. Not all languages use alphabetic or syllabic\nwriting systems. Some use ideographic system. The notion of a\ncharacter, then is not as simple as we may initially imagine. As\nsuch, this deﬁnition of word is far from perfect.\n2. A Word is a What is Listed in a Dictionary\nA dictionary lists the words of a language and for each word\nprovides associated information such as pronunciation and mean-\nings. We often refer to a dictionary to know the meaning, pro-\nnunciation or to conﬁrm the spelling.\nDictionaries normally do not list all the inﬂected forms of\nwords. There may be too many of them. Readers are expected to\npossess some knowledge of the morphology of the language. Only\na few peculiar or potentially confusing cases may be explicitly\nmentioned. Morphological generalizations are implicit in a dictio-\nnary. The word ‘baker’ may be listed and its meanings given but\n126\nCHAPTER 2. FOUNDATIONS OF NLP\nthe generalization regarding forming nouns by adding -er to verbs,\nthe nature of semantic relations between the associated words, ex-\nceptions and deviations from this canonical semantic relation etc.\nare not within the scope of dictionaries. A dictionary does not help",
    "CHAPTER 2. FOUNDATIONS OF NLP\nthe generalization regarding forming nouns by adding -er to verbs,\nthe nature of semantic relations between the associated words, ex-\nceptions and deviations from this canonical semantic relation etc.\nare not within the scope of dictionaries. A dictionary does not help\nto guess the meanings of new words either. Words of a language\nare under constant change. New words get in, existing words get\nnew shades of meaning, some words go into the oblivion. Proper\nnames, acronyms etc. may not be listed in the dictionary. No two\ndictionaries agree fully. Using dictionaries as a basis for deﬁning\nwords is also far from perfect.\n3. A Linguist’s Deﬁnition\nThere are many facets to a word. Consider the word apple.\nThe word apple has to be spelled a-p-p-l-e. Only this particular\nstring of alphabets constitutes a valid spelling for the word - any\nother sequence of alphabets is at best a spelling error. The word\napple has a speciﬁed pronunciation, a speciﬁed sequence of sound\nunits. Then there is a meaning. An apple is a kind of fruit with\nso and so properties. A grammatical category may also be associ-\nated. As we shall see, grammatical categories are also meanings,\nonly very broad and general meanings applicable to a whole class\nof words.\nWORD\nMEANING\nSPELLING\nPRONUNCIATION\nFIG 2.4 Words: Meanings, Spellings and Pronunciations\nNote that a few words may have more than one permitted\nspelling. Grey and gray are both acceptable. So are defence and\ndefense. British and American spellings may vary - colour, color\nSometimes words having the same spelling may have com-\npletely diﬀerent meanings: The word bark has two entirely diﬀer-\nent meanings, one as a verb and the other as a noun. The word\n2.2. COMPUTATIONAL LINGUISTICS\n127\nbank has several very diﬀerent meanings - bank of river, bank as\na ﬁnancial institution, a bank of ﬁlters - all of them being nouns.\nProject as a verb and project as a noun mean very diﬀerent things.",
    "ent meanings, one as a verb and the other as a noun. The word\n2.2. COMPUTATIONAL LINGUISTICS\n127\nbank has several very diﬀerent meanings - bank of river, bank as\na ﬁnancial institution, a bank of ﬁlters - all of them being nouns.\nProject as a verb and project as a noun mean very diﬀerent things.\nIn this case in fact the pronunciations are also diﬀerent. Spellings\nare not the most important aspects of words. Spellings are re-\nquired only for writing down words. Words exist even if we never\nwrite them down. Not all languages of the world have a script.\nSounds are more fundamental than spellings. It is worth noting\nthat Indian languages have always given primary importance to\nsounds at all levels, even at the level of scripts.\nThe mapping between spellings and sounds can be highly sys-\ntematic or largely ad-hoc.\nEnglish spellings are quite chaotic.\nAlthough with some experience one can get a general feel for how\nto spell words, there are many exceptions and there is no other go\nthan to simply memorize all cases. Indian scripts are phonetic in\nnature and what we write is to a very large measure the same as\nwhat we speak. Thus there are hardly any spelling rules.\nThere are a few onomatopoeic words in many languages where\nthe sounds themselves signal the meaning - buzz, hiss etc.. Nev-\nertheless, the mapping between sounds and meanings in general\nis also largely arbitrary. Why should a chair be spelled c-h-a-i-\nr or pronounced as chair? Why not as l-i-m-b-a-g-o or r-i-c-h-a?\nThere is no logic or rule or principle. People at some point of time\nhave chosen some arbitrary sounds to represent objects or what-\never they want to talk about and we have to simply accept that.\nLanguage is a means of communication and if we wish to com-\nmunicate eﬀectively, we must all follow a common code. Sounds\nbeing completely arbitrary, we should not consider sounds as the\nmost fundamental aspects of words.\nOf the three basic properties of words namely spellings, pro-",
    "Language is a means of communication and if we wish to com-\nmunicate eﬀectively, we must all follow a common code. Sounds\nbeing completely arbitrary, we should not consider sounds as the\nmost fundamental aspects of words.\nOf the three basic properties of words namely spellings, pro-\nnunciation and meanings, linguists consider meaning to be the\nprimary aspect. Words have totally diﬀerent sounds and spellings\nin diﬀerent languages, yet we are able to ﬁnd equivalences because\nthey signal the same thing, they mean the same thing. Meanings\nform the core aspect of words, not spellings or pronunciations.\nThe most important aspect of the word apple is its meaning. Why\nis apple spelled that way or pronounced that way? Why not some\nother spelling? Why not some other sound sequence? Pronuncia-\ntions and spellings are arbitrary. Linguists therefore deﬁne words\nas (arbitrary) mapping between sounds and meaning.\n128\nCHAPTER 2. FOUNDATIONS OF NLP\nThus it is more appropriate to think that there are several\ndiﬀerent words, with diﬀerent meaning, that so happen to have\nthe same spelling and pronunciation bank. It is not that the word\nbank has two or more diﬀerent meanings, rather two or more diﬀer-\nent words have one spelling. Diﬀerent words with same spelling\nand/or pronunciation are called homonyms (for “same name”).\nFurther, the term ‘homograph’ is used when the spellings are the\nsame and the term ‘homophone’ is used when the pronunciations\nare the same.\nIt is also possible for one word to have many closely related\nmeanings or senses. Consider the word play. This word can mean,\namong other things (sampled from Merriam-Webster Dictionary),\na game or sport; the conduct, course, or action of a game; a partic-\nular act or maneuver in a game as the action during an attempt to\nadvance the ball in football, the action in which cards are played\nafter bidding in a card game, the moving of a piece in a board\ngame (as chess); one’s turn in a game (it’s your play); a recre-",
    "ular act or maneuver in a game as the action during an attempt to\nadvance the ball in football, the action in which cards are played\nafter bidding in a card game, the moving of a piece in a board\ngame (as chess); one’s turn in a game (it’s your play); a recre-\national activity - especially the spontaneous activity of children;\nabsence of serious or harmful intent (said it in play); the act or\nan instance of playing on words or speech sounds; an act, way, or\nmanner of proceeding (that was a play to get your ﬁngerprints);\na operation or activity (other motives surely come into play); a\nbrisk, ﬁtful, or light movement (the gem presented a dazzling play\nof colors); free or unimpeded motion (as of a part of a machine)\nor the length or measure of such motion; scope or opportunity for\naction; the stage presentation of an action or story; a dramatic\ncomposition (drama). There are many meanings but the mean-\nings are all related in some way. Here it would not be appropriate\nto think that there are many diﬀerent words all spelled as p-l-a-\ny. Instead, it is better to think that this one word p-l-a-y has\nso many related meanings. The phenomenon of one word having\nseveral related meanings is termed polysemy. Many of the most\ncommon, simple, frequently used words are highly polysemous.\nLook at any dictionary to ﬁnd out how many meanings these sim-\nple words have. It is a wonder that we are able to get the correct\nsense without any diﬃculty. Computers have serious diﬃculties\nin identifying the correct sense of a word even when the context\nis known.\nWords are arbitrary mapping between sounds and meaning.\nThis deﬁnition is also not entirely satisfactory. Phrases and sen-\n2.2. COMPUTATIONAL LINGUISTICS\n129\ntences are also sound sequences with meaning.\nAlso there are\n’words’ which do not appear to have any deﬁnite meaning. What\nis the ’meaning’ of the word it in it is raining? Should contrac-\ntions be treated as one word or several? What about compound",
    "2.2. COMPUTATIONAL LINGUISTICS\n129\ntences are also sound sequences with meaning.\nAlso there are\n’words’ which do not appear to have any deﬁnite meaning. What\nis the ’meaning’ of the word it in it is raining? Should contrac-\ntions be treated as one word or several? What about compound\nwords? Linguistics oﬀers techniques to answer such questions in\na scientiﬁc and systematic manner. For example, notions such as\ninternal stability and external mobility are used. If it is one word,\nit must not be possible to insert something in between. If it is one\nword, it must always move as a single unit.\nThe concept of a word is not as simple as we may tend to think\nin the beginning. It is generally agreed, however, that there are\nunits called words in most languages of the world. We can talk of\nwords. We can, hopefully, ﬁnd words in spoken and written lan-\nguage. We can work with words as units of structure and meaning.\nWe can consider bigger units made up of several words and sub-\nword units that together make up individual words. Words are a\nreality.\nWords, Phrases, Compounds\nWe have deﬁned words as mappings between sounds and mean-\nings. This deﬁnition does not help us to develop algorithms or\ncomputer programs to break a given sentence into its constituent\nwords. The goal of many NLP applications is to start from given\ntexts and then analyze the text to explicate structure and hence\nmeaning. We do not know the meanings to start with, meanings\nare hopefully what we can get to at the end of the whole process.\nWe need to start by breaking sentences into words, then perform\na dictionary look-up or morphological analysis and may be some\nsyntactic analysis before we can think of meanings. It is com-\nmon practice in NLP, therefore, to use our ﬁrst deﬁnition based\non spaces as delimiters to initially tokenize given sentences into\nwords. Let us call the ‘words’ so obtained as tokens to distinguish\nbetween meaning based deﬁnition of words. If we wish to identify",
    "mon practice in NLP, therefore, to use our ﬁrst deﬁnition based\non spaces as delimiters to initially tokenize given sentences into\nwords. Let us call the ‘words’ so obtained as tokens to distinguish\nbetween meaning based deﬁnition of words. If we wish to identify\nwords as meaning units, we may have to either group together\nseveral such tokens into one or break one into several as required.\nDictionary and morphology provide guidance in this process. A\nclear distinction between phrases and compounds is essential to\nidentify words in texts.\n130\nCHAPTER 2. FOUNDATIONS OF NLP\ni) Compounds:\nSometimes two or more tokens form a single unit of meaning\nand hence a single word. Examples of such multi-token words are\n‘white house’ and ‘high school’. The two tokens in these exam-\nples cannot be treated as independent units in their own right\nbecause the meaning of the group is not simply a combination of\nthe meanings of the individual tokens. A white house is not simply\nany house which is white and a high school is not any school which\nis high in altitude. The two tokens in each of these cases form sin-\ngle units of meaning and hence single words. Such multi-token\nwords are called compound words or simply compounds. A com-\npound is a word that consists of more than one free morpheme.\nThe Sanskrit term for compounds is samaasa. Sanskrit has a very\nproductive mechanism for compound formation. A vast majority\nof words found in any text are complex compounds consisting of\nseveral words. If you simply tokenize a Sanskrit text and look up\nthe tokens in any standard dictionary, you will ﬁnd that a major-\nity of the tokens will not be found in the dictionary at all. Only\nif you can understand the meanings and appropriately break the\ncompounds into the constituent parts, you will be able to locate\nthose parts in a dictionary. There are many types of compounds\nin Sanskrit based on how meanings can be understood. Interested\nreaders may refer to any book on Sanskrit grammar for more de-",
    "compounds into the constituent parts, you will be able to locate\nthose parts in a dictionary. There are many types of compounds\nin Sanskrit based on how meanings can be understood. Interested\nreaders may refer to any book on Sanskrit grammar for more de-\ntails of compound formation in Sanskrit.\nWhether we write the words of a compound as tokens sepa-\nrated by spaces or as one token is a matter of convention. You\nwill generally ﬁnd a lot of variations. Some English compounds\nare written with spaces in between, some are written as single to-\nkens and sometimes the constituent parts are joined by hyphens.\nRowboat, devil-may-care are also compounds, the former written\nas one token and the latter hyphenated.\nThe most important point to note about compounds is that\nthe meaning of the whole is a complex function of the meaning of\nthe constituent parts. If we treat white house as any house that\nis painted white, we have composed the meanings of the words\nhouse and white to get the meaning of the word white house in a\nsimple, straight forward way. Since by white house we normally\nmean a special building where the President resides, not any house\nwhich is painted white, white house is not a simple sequence of two\n2.2. COMPUTATIONAL LINGUISTICS\n131\nindependent words but a single compound word. In compounds,\nmeaning is not compositional.\nIn endocentric compounds, one of the constituents forms the\nhead and the meaning of the whole compound is dictated by the\nmeaning of the head. The grammatical category of the compound\nis also the same as the category of the head.\nA doghouse is a\nhouse, built for a dog. House is the head. Dog is only a modiﬁer.\nIn exocentric compounds, on the other hand, there is no head\nand the meaning of the compound is outside the meaning of its\nconstituent parts.\nIn Sanskrit these exocentric compounds are\ncalled bahuvriihi compounds. bahu (much) + vriihi (rice) means\na rich man, a man owning much rice, so to say.",
    "In exocentric compounds, on the other hand, there is no head\nand the meaning of the compound is outside the meaning of its\nconstituent parts.\nIn Sanskrit these exocentric compounds are\ncalled bahuvriihi compounds. bahu (much) + vriihi (rice) means\na rich man, a man owning much rice, so to say.\nIdentifying the correct meanings of compound words in a lan-\nguage like Sanskrit requires great skill and knowledge of commonly\naccepted conventions. Thus the compound word das’aratha has\nthe two constituent parts das’a meaning ten and ratha meaning\nchariot. How do we combine these two meanings to correctly iden-\ntify the meaning of the compound? Does das’aratha mean one who\npossesses ten rathas? Or does it mean one who built ten rathas?\nOr one who can simultaneously steer ten rathas? It could be any\nof these but the intended and generally accepted meaning is “one\nwho has great expertise in steering a chariot and can steer the\nchariot in all the ten directions - North, East, South, West, the\nfour directions in between these, and up-wards and down-wards”!\nThe compound tiloottama has the parts tila meaning a tiny black\ncoloured oil seed and uttama meaning better.\nDoes this word\nmean one who is slightly better in colour than the dark oil seeds?\nDoes this mean one who is bettered by the black oil seeds? The\nintended meaning is very diﬀerent from any of these. tiloottama\nis a woman who is so beautiful that she is more beautiful by at\nleast a small amount (as indicated by the tiny oil seed) than any\nother highly beautiful lady you can ﬁnd on earth. Essentially it\nmeans the most beautiful lady. The oil seed is only used to imply\na small quantity, an iota. From these examples it should be clear\nthat analyzing the meanings of compounds is a diﬃcult task even\nfor human beings.\nii) Phrases:\nNot all sequences of words are compounds. A red ball can be\n132\nCHAPTER 2. FOUNDATIONS OF NLP\nunderstood as a ball that is red in colour. Meanings are composi-",
    "that analyzing the meanings of compounds is a diﬃcult task even\nfor human beings.\nii) Phrases:\nNot all sequences of words are compounds. A red ball can be\n132\nCHAPTER 2. FOUNDATIONS OF NLP\nunderstood as a ball that is red in colour. Meanings are composi-\ntional and there is no compound formation. Should we then treat\nsentences as simply a single linear sequence of words or are there\nintermediate levels of description between words and sentences?\nIn fact words can be grouped into higher level units called ‘phrases’\nbased on structure or meaning. Phrases are typically contiguous\nsequences of two or more words in a sentence.\nIt is generally accepted that phrases are groups of words which\nbehave as syntactic units. Thus we can talk of noun phrase and\nadjective phrase. All sequences of words and intermediate levels of\ndescription are examples of phrases found in this very paragraph.\nPhrases are syntactic units. Thus a noun phrase may act like the\nsubject or object of a verb in a sentence. It is the whole noun\nphrase, not its individual constituent words, that ﬁll the role of\nthe subject or object. That is why they are treated as units.\nFor the sake of uniformity, we may include a single word also\nin the deﬁnition of a phrase. Thus a phrase is a word or group of\nwords forming a syntactic constituent with a single grammatical\nfunction. The words in a phrase are typically but not necessarily\ncontiguous in the surface sentence.\nThe constituent parts of a phrase are less tightly integrated\nthan the constituents in a compound. Thus it may be possible to\nremove a part of phrase to get another valid phrase and we may\nalso insert some more words in between. Thus all sequences of\nwords is a phrase and so is sequences of words or just sequences.\nIntermediate levels of description is a phrase and so is interme-\ndiate levels of linguistic description. It is much more diﬃcult to\nmanipulate compounds this way.\nEnglish has a large number of phrasal verbs.\n‘Look after’,",
    "words is a phrase and so is sequences of words or just sequences.\nIntermediate levels of description is a phrase and so is interme-\ndiate levels of linguistic description. It is much more diﬃcult to\nmanipulate compounds this way.\nEnglish has a large number of phrasal verbs.\n‘Look after’,\n‘look out’, ‘look into’ and ‘look up’ mean quite diﬀerent things.\n‘Put up with’ has three tokens.\nThe particles used in phrasal\nverbs are usually prepositions too. If ‘Rama went after Sita’ did\nhe simply go after Sita went or did he really go after Sita? How\no we understand ‘he looked up the dictionary’ and ‘he looked up\nthe chimney’?\nDistinguishing between phrasal verbs and verbs\nfollowed by prepositional phrases can be tricky.\nWe must observe that the term phrase has been used in sev-\neral diﬀerent senses in diﬀerent disciplines. Modern syntacticians\nhave used rules such as S →NP\nV P to indicate that a sen-\ntence typically has a Subject and a Predicate and the Subject\n2.2. COMPUTATIONAL LINGUISTICS\n133\ncan be a noun phrase and the Predicate, a verb phrase.\nNote\nhowever that the term phrase as used here can imply complex\nstructures deﬁned recursively. Thus while an NP can be a simple\nnoun phrase consisting of a head noun optionally modiﬁed by one\nor more adjectival modiﬁers and an optional determiner, an NP\ncan also include whole clauses, say, relative clauses. These clauses\nare more like the S above and may in turn consist of their own\nNP and VP. Similarly, the VP is not just a verb optionally modi-\nﬁed by auxiliary verbs but the whole predicate possibly including\nseveral NPs, and other clauses. Phrases and Clauses are diﬀer-\nent levels of description, clauses being higher, bigger, sentence-like\nunits composed of phrases. Mixing up phrases and clauses is un-\nnecessary and unwise. A lot can be gained by separating phrases\nfrom clauses.\nDividing a complex problem into simpler sub-problems is a\nwise move indeed. The division must be into roughly equal sized",
    "units composed of phrases. Mixing up phrases and clauses is un-\nnecessary and unwise. A lot can be gained by separating phrases\nfrom clauses.\nDividing a complex problem into simpler sub-problems is a\nwise move indeed. The division must be into roughly equal sized\nparts and, more importantly, the parts must all of the same type.\nOnly then does this divide-and-conquer strategy give us any real\nbeneﬁts.\nThe assumption is that solving the constituent sub-\nproblems and then combining the results will be more eﬃcient\nthan solving the whole problem in one go. This is a piece of tra-\nditional wisdom universally followed in all engineering disciplines.\nSorting two arrays of 50 items and merging the results can be\nfaster than sorting an array of hundred items. But sorting an ar-\nray of 99 items and another array of one item and then combining\nthe results hardly gives us any improvement. In fact the task of\ncombining the results of the parts will be an added overhead.\nNote that our S →NP\nV P rule violates these basic require-\nments of a good divide-and-conquer strategy. Although widely\nused in linguistic theories today, this treatment of structure of\nsentences appears to be fundamentally ﬂawed. NPs are typically\nmuch smaller than VPs. In fact the VP is the whole of the sen-\ntence, except for the one NP depicted in the rule. Thus the con-\nstituents are rarely roughly equal in size. Note also the VPs may,\nand often do contain NPs but NPs do not contain VPs (except\nindirectly through an S). Hence the NP and VP cannot be of the\nsame type. The NP here typically ﬁlls one functional role - the\nSubject, whereas the VP ﬁlls several - Verb, Object, Indirect Ob-\nject etc. Division of a sentence into one NP and one VP does not\nseem to make much sense at all. In what way is ‘object’ less im-\n134\nCHAPTER 2. FOUNDATIONS OF NLP\nportant than the ‘subject’? Why not treat sentences as ‘subject\nverb’ or ‘subject verb object’?\nObsession for phrase structure rules and tree structures has",
    "seem to make much sense at all. In what way is ‘object’ less im-\n134\nCHAPTER 2. FOUNDATIONS OF NLP\nportant than the ‘subject’? Why not treat sentences as ‘subject\nverb’ or ‘subject verb object’?\nObsession for phrase structure rules and tree structures has\nlead to unnecessarily complicated theories that seem to be pre-\noccupied with solving non-existent, unreal problems. Instead of\nhelping to uncover the underlying meanings, these syntactic the-\nories have only further obscured semantics. There is hierarchical\nstructure in syntax but not where modern generative linguists see.\nA tree structure is helpful to depict the nested structure of part-\nwhole relationships. There is no semantic part-whole relationships\nbetween determiners, adjectives and nouns in a noun phrase. Nei-\nther the determiner is inside the adjective nor the other way round.\nThere is no point in writing trees where one is not a part of the\nother in terms of meaning. On the other hand, there are indeed\nsemantically oriented nested modiﬁer-modiﬁed structures within\nnoun phrases but the syntactic rules that linguists posit do not\neven attempt to capture these semantic relationships.\nOld habits die hard. Linguists will naturally oppose all this\nvehemently. Nevertheless, breaking a sentence into one NP and\none VP is neither essential nor the wisest thing to do.\nThere\nare alternative, perhaps better ways of doing things, if only we\nare ready to unlearn all we have learnt and bring in some fresh\nthinking. To see the truth, often one must learn to think like a\nnon-stake holder. That is not easy. Linguists have been using\nrules of this sort so widely and for so long now that they would\nﬁnd it extremely diﬃcult to think any other way. We are stuck up\nwith NPs and VPs and you may have to learn to live with them\ntoo. But do not hesitate to think independently. What experts\nsay is not always correct. If everyone always follows what elders\nsay, there would be no development.",
    "ﬁnd it extremely diﬃcult to think any other way. We are stuck up\nwith NPs and VPs and you may have to learn to live with them\ntoo. But do not hesitate to think independently. What experts\nsay is not always correct. If everyone always follows what elders\nsay, there would be no development.\nInstead of raising a controversy over the deﬁnition of the term\nphrase NLP researchers simply prefer an alternative, less confus-\ning terminology. Simple, non-recursive, non-nested sequences of\nwords are called chunks or word groups.\nEach chunk or word\ngroup has a head.\nChunks are categorized based on the cate-\ngory of its head. Thus we can talk of noun groups, verb groups\nadjective groups and so on. Note that a noun group will never\ninclude verbs and a verb group will never include nouns or whole\nnoun groups. In simplest terms, a noun group is a noun, option-\nally modiﬁed by adjectival modiﬁers and an optional determiner.\n2.2. COMPUTATIONAL LINGUISTICS\n135\nSimilarly a verb group is simply a main verb optionally modiﬁed\nby auxiliary verbs. A sentence is a sequence (or a set, in the so\ncalled word order free languages) of such chunks or word groups.\nA noun group typically ﬁlls one thematic role such as Subject or\nObject. A verb group ﬁlls the role of the Verb in the sentence or\nclause.\niii) saMdhi or Conﬂation:\nIn many languages of the world, noteworthy among them be-\ning Indian languages, we can join or conﬂate several words into\none for the sake of convenience of pronouncing and writing to-\ngether.\nThis is known as saMdhi or conﬂation.\nThere are no\nchanges in meaning and saMdhi must not be confused with com-\npounding. Words in a saMdhi can be broken into constituent parts\nby saMdhi rules clearly speciﬁed for each language. Thus naanu\n(I) and iiga (now) can be conﬂated to form naaniiga in Kannada.\nThe meaning remains the same. The grammar remains the same.\nOnly the rules of saMdhi formation will have to be incorporated",
    "by saMdhi rules clearly speciﬁed for each language. Thus naanu\n(I) and iiga (now) can be conﬂated to form naaniiga in Kannada.\nThe meaning remains the same. The grammar remains the same.\nOnly the rules of saMdhi formation will have to be incorporated\ninto the system. It is of course possible to combine saMdhi and\nsamaasa.\niv) Multiword expression (MWE):\nMulti-token words and multi-word expressions are of great\ncurrent interest.\nHere is a starter extracted from an on-going\nproject from Stanford University. Any phrase that is not entirely\npredictable on the basis of standard grammar rules and lexical\nentries is termed a ‘Multiword expression (MWE)’. Here are some\nexamples: alive and well, all aboard, all of a sudden, as good as\ngold, at all, beat around the bush, behind someone’s back, bite the\nbullet, bull market, by and large, enough is enough, ﬁrst oﬀ, ﬂy oﬀ\nthe handle, good morning, in eﬀect, in retrospect, kick the bucket,\nkith and kin, let the cat out of the bag, on an even keel, on show,\nout of action, pack one’s bags, plain truth, the be all and end all of\nNP, to and fro, wide awake, you can’t have your cake and eat it too\n. MWEs are characterized by qualities such as Institutionalization\n/ conventionalisation (process of an expression becoming recog-\nnized and accepted as a lexical item, through consistent use over\ntime), Lexicogrammatical ﬁxedness (formal rigidity, preferred lex-\n136\nCHAPTER 2. FOUNDATIONS OF NLP\nical realization, restrictions on aspect, mood, voice, etc.), Seman-\ntic/pragmatic non-compositionality (there is a mismatch between\nthe semantics/pragmatics of the parts and the whole), Syntactic\nirregularity (the expression cannot be parsed based on the simple\nmorphology of the components), Non-identiﬁability (when ﬁrst\nexposed to the expression, the meaning cannot be predicted from\nits surface form), Situatedness (the expression is associated with\na ﬁxed pragmatic point), Figuration (the expression encodes some",
    "morphology of the components), Non-identiﬁability (when ﬁrst\nexposed to the expression, the meaning cannot be predicted from\nits surface form), Situatedness (the expression is associated with\na ﬁxed pragmatic point), Figuration (the expression encodes some\nmetaphor, metonymy, hyperbole, etc, even if the nature thereof is\nunderspeciﬁed), Proverbiality (the expression is used to describe\nand implicitly, to explain a recurrent situation of particular so-\ncial interest by virtue of its resemblance or relation to a scenario\ninvolving homely, concrete things and relations), Informality (the\nexpression is associated with more informal or colloquial registers)\nand Aﬀect (the expression encodes a certain evaluation of aﬀective\nstance towards the thing it denotes). Identifying and analyzing\nmultiword expressions is a topic of current research interest. In-\nterested readers will ﬁnd many good resources on the web.\nTraditionally, linguists discriminate between the following types\nof languages with regard to morphology:\n• Isolating languages (e.g. Mandarin Chinese): there are no\nbound forms, e.g., no aﬃxes that can be attached to a word.\nThe only morphological operation is composition.\n• Agglutinative languages (e.g. Ugro-Finnic and Turkic lan-\nguages): all bound forms are either preﬁxes or suﬃxes, i.e.,\nthey are added to a stem like beads on a string. Every aﬃx\nrepresents a distinct morphological feature. Every feature\nis expressed by exactly one aﬃx.\n• Inﬂectional languages (e.g. Indo-European languages): dis-\ntinct features are merged into a single bound form (a so-\ncalled portmanteau morph). The same underlying feature\nmay be expressed diﬀerently, depending on the paradigm\n• Polysynthetic languages (e.g. Inuit languages): these lan-\nguages express more of syntax in morphology than other\nlanguages, e.g., verb arguments are incorporated into the\nverb.\nThis classiﬁcation is quite artiﬁcial.\nReal languages rarely\nfall cleanly into one of the above classes, e.g., even Mandarin",
    "guages express more of syntax in morphology than other\nlanguages, e.g., verb arguments are incorporated into the\nverb.\nThis classiﬁcation is quite artiﬁcial.\nReal languages rarely\nfall cleanly into one of the above classes, e.g., even Mandarin\n2.2. COMPUTATIONAL LINGUISTICS\n137\nhas a few suﬃxes. Dravidian languages are inﬂectional as well\nas agglutinative. Moreover, this classiﬁcation mixes the aspect of\nwhat is expressed morphologically and the means for expressing\nit.\nThere is a lot more to it than meets the eye:\nBy now you must have understood that even a simple looking\ntask such as breaking sentences into words is really a very complex\ntask. It is easy to build demonstration systems to give a false\nimpression of progress and success. You might have heard people\nclaim that they have solved all problems. Researchers must learn\nto see through the hype and propaganda and get straight to the\nground realities. NLP is not easy. NLP should not be compared\nwith telephones or cars. NLP requires working with meanings,\nNLP requires working with complex issues of human cognition.\nNLP can only be compared with other similar tasks that are also\nlinked to human cognition. Have we been able to understand how\nwe learn? Have we been able to understand how we understand\nspeech? Have we been able to understand how we can recognize\na person by his/her voice, hand-writing or a momentary glance\nat his/her face? May be but in a very limited way. NLP is no\ndiﬀerent. Fantastic results should not be expected. Do not fall\nfor false claims.\nIt is also not a very good idea for every young researcher\nto jump into machine translation, information extraction, speech\nrecognition or other such big tasks. We need to do a lot of ground\nwork before we can see real successes in such large tasks. The\ncomputational dictionaries and morphological analyzers we have\ntoday for Indian languages are all far from perfect, far from ade-\nquate. It takes many years of real hard work to prepare a good",
    "work before we can see real successes in such large tasks. The\ncomputational dictionaries and morphological analyzers we have\ntoday for Indian languages are all far from perfect, far from ade-\nquate. It takes many years of real hard work to prepare a good\ndictionary. Developing a dictionary, for example, may not carry\nthe same kind of appeal and respect that bigger tasks carry but if\nnobody takes up the basic, enabling technologies how can we ever\nachieve success in the big tasks? Let us hope budding researchers\nare able to get out of hype, understand ground realities and put\nin some serious eﬀort on real core problems rather than go after\nname and fame.\n138\nCHAPTER 2. FOUNDATIONS OF NLP\nProperties of Words\nWords are arbitrary pairings of sound sequences and meanings.\nPhonetics and Phonology deal respectively with the physical and\nlogical (or abstract) characterizations of sound units in a language.\nSemantics deals with the meanings of words and other larger units\nof language. Apart from these aspects, words also carry several\nother pieces of information with them. Associated with each word\nis some idea of how that word can be used in phrases and sen-\ntences.\nThe word eat begs questions on who eats and what is\neaten. Syntax deals with the structure of phrases and sentences.\nWords also tell us something about their own internal structure.\nConsider the word unrecognizable. We recognize that this word\nis related to recognize and recognizable. We recognize that there\nis no such word as unrecognize. We have some idea of the parts\nof words and how they are related to one another. Morphology is\nthe study of structure and formation of words. Pragmatics deals\nwith the actual usage of words, expressions and sentences in dif-\nferent contexts. Knowing words forms a major part of knowing a\nlanguage.\nThere are also some properties of words that native speakers\nof a language may not know - how the same words are used to",
    "with the actual usage of words, expressions and sentences in dif-\nferent contexts. Knowing words forms a major part of knowing a\nlanguage.\nThere are also some properties of words that native speakers\nof a language may not know - how the same words are used to\nmean diﬀerent things by diﬀerent groups of people (the words ﬂat\nand bonnet have very diﬀerent meanings in British and Ameri-\ncan English), how words have changed over time (deer used to\nmean any animal), how words of one language are related to words\nof other languages (man, mother, father, brother, daughter, two,\nthree, September, October, November, December etc.\nare related\nto the Sanskrit words maanava, maatR, pitR (hence paternal),\nbhraatR, duhitR, dwi, tri, sapta, aSTa, nava, das’a (hence deca,\ndeci etc. too), bandicoot is from the Telugu word pandikokku, jug-\ngernaut has come from the commotion that goes on during the\nratha yaatra of Lord Jagannath at Puri, bank has come from the\nFrench word bunk meaning a low piece of wooden furniture - be-\nhind which the money lender used to sit, and so on.)\nSometimes we know the meanings but we do not know the\nright words. Many people would have seen the short valance or\nsmall cornice for concealing curtain ﬁxtures but they may not\nknow that this is called pelmet. Similarly we would have come\nacross words and we may even be using them without knowing the\n2.2. COMPUTATIONAL LINGUISTICS\n139\nprecise meanings. Dictionaries and thesauri are excellent sources\nof information about words. We must all inculcate the habit of\nlooking up dictionaries to ﬁnd out the correct meanings of words\nwe use.\nIs there any relation at all between medical case, legal case,\nlower/upper case letters of the alphabet, jewel case, and suit case?\nIs there a primary, nuclear meaning for the word from which we\ncan link up and explain the meanings of other cases? Let us hy-\npothesize. A case is really just a box but not any box. A case is a",
    "lower/upper case letters of the alphabet, jewel case, and suit case?\nIs there a primary, nuclear meaning for the word from which we\ncan link up and explain the meanings of other cases? Let us hy-\npothesize. A case is really just a box but not any box. A case is a\nbox made for a particular purpose. A jewel case is made for keep-\ning jewels just as a suit case is made for keeping a suit. Then we\ncan perhaps explain that legal and medical records are physically\nor metaphorically arranged in diﬀerent cases (boxes, ﬁles, folders),\nand we can locate and access a particular case through a logical\norganization of records in diﬀerent cases/ﬁles/folders. During the\nletter press times, the letters were kept in boxes and the lower\ncase letters were physically kept at a lower height and upper case\nletters at a higher level. Lower case letters are more frequent and\nhence this organization into lower and higher cases was practically\nmore convenient for picking up and composing in the letter press.\nThis is all just a hypothesis, we are not at all asserting here\nthat this is in fact the case. How then do we convince ourselves\nthat we are really right in our hypothesis and not confused by\ncoincidences or imagination? Etymology holds the answer. Ety-\nmology is the scientiﬁc exploration of the history of a linguistic\nform (such as a word) shown by tracing its development since its\nearliest recorded occurrence in the language where it is found, by\ntracing its transmission from one language to another, by ana-\nlyzing it into its component parts, by identifying its cognates in\nother languages, or by tracing it and its cognates to a common\nancestral form in an ancestral language\nSanskrit words, just as in the case of other languages, often\nhave several meanings or senses. Thus the word hari is used to\ndenote Lord Vishnu as also to refer to a lion and to a bee. One\nway of looking at this is that the word hari has several senses\nand we need to use Word Sense Disambiguation techniques to",
    "have several meanings or senses. Thus the word hari is used to\ndenote Lord Vishnu as also to refer to a lion and to a bee. One\nway of looking at this is that the word hari has several senses\nand we need to use Word Sense Disambiguation techniques to\ndisambiguate between the various possible meanings based on the\ncontext in which the word is used. However, this is not the way it\nis looked at in Sanskrit. The word hari is derived from a root har\nthat means to take away. Lord Vishnu takes away all our sins and\n140\nCHAPTER 2. FOUNDATIONS OF NLP\nis therefore called hari. A lion also takes away something, your life.\nThus a lion can also be called hari when used to denote its killing\ninstincts.\nSimilarly a bee takes away nectar from ﬂowers and\nwhen this connotation is intended, we may call a bee hari. Thus\nthe correct understanding of the senses of words is based on an\nappreciation of what the root means and how to interpret that in\na given situation. This is not really a WSD problem as we usually\nformulate it. It is not that one word has several meanings. One\nword has only one meaning but its interpretation varies according\nto context. Exploring words is a fascinating topic in itself.\nTo know one word completely is to know practically every-\nthing. To know the meaning of the word car completely, you will\nneed to know its structure and function. That means you must\nknow about the engine, the body, the parts that make up these,\ntheir structure, function and inter-relationships. To know ignition\nsubsystem you need to know thermodynamics, ﬂuid dynamics and\nchemistry. To understand electrical subsystem you need to know\nall about electricity. To know a car you need to know mechanics,\nphysics, material science, electrical engineering, chemistry, paints,\nplastics, rubber, ..., practically everything in the world. This is\nwhat Indian philosophy says. To know one word completely is to\nknow practically everything. No one knows even one word com-\npletely! Yet we are all so arrogant!",
    "physics, material science, electrical engineering, chemistry, paints,\nplastics, rubber, ..., practically everything in the world. This is\nwhat Indian philosophy says. To know one word completely is to\nknow practically everything. No one knows even one word com-\npletely! Yet we are all so arrogant!\nHow many words?\nWords are ﬁnite but unbounded - new words get coined and some\ngo into the oblivion.\nThe numbers change but not drastically.\nCompare this with sentences. The total number of possible sen-\ntences is potentially inﬁnite. No reasonable limits can be put, no\nreasonably complete list of all possible sentences can be made.\nLists of words can be made. Dictionaries do exactly this.\nThere are several hundred thousand words in English. We use\na small number in daily life. About a 1000 are the most frequent.\nA typical book may contain of the order of 5000 diﬀerent words.\nThe British National Corpus has about 100 Million tokens but\nonly a few lakh types. (Each distinct word form is called a type\nand each occurrence of a type is called a token.) About 1,20,000\nmost frequently occurring types in this corpus give 95% to 98%\n2.2. COMPUTATIONAL LINGUISTICS\n141\ncoverage. That is, given any arbitrary English text, about 95% or\nso of all the words in that text could be generally expected to be\navailable in the speciﬁed word list. Of course if the text on hand\nis a very special kind, the numbers may go down. These are very\nrough ﬁgures, mentioned here only to give you some idea of the\nmagnitude of the numbers involved.\nWe cannot expect any dictionary to give 100% coverage. Any\nreal text is likely to contain proper names, acronyms, loan words\nfrom other languages, special symbols and notations, domain spe-\nciﬁc terminology etc.\nIn many practical applications, standard\ndictionaries have to be supplemented with domain speciﬁc or task\nspeciﬁc dictionaries.\nStandard dictionaries of Indian languages tend to contain much\nsmaller number of words - most dictionaries have only between",
    "ciﬁc terminology etc.\nIn many practical applications, standard\ndictionaries have to be supplemented with domain speciﬁc or task\nspeciﬁc dictionaries.\nStandard dictionaries of Indian languages tend to contain much\nsmaller number of words - most dictionaries have only between\n15,000 and 35,000 entries.\nThese dictionaries tend to include\nwords mostly from literature, not from all the major walks of\nmodern life. You should expect a large mismatch between words\nfound in a standard dictionary and words used in newspapers and\nother media. Many words used in the media today may not be\nfound in the well known dictionaries and many words found in\nthese dictionaries may never be used in the media language. Thus\nbuilding electronic dictionaries for modern language technology\napplications in Indian languages needs a lot of thought and care.\nWe can obtain an idea about the number of types we have in\na given language by performing a growth rate analysis on a text\ncorpus. The ﬁgure below shows the growth rate of types against\ntokens for major Indian languages based on corpora of about 3\nMillion words in each language. These corpora were developed\nwith support from the Department of Electronics, Government\nof India (now called Department of Information Technology) and\ndistributed by the Central Institute of Indian Languages, Mysore.\nThe corpora were divided into equal sized sections and type-token\nanalysis performed on each section. The cumulative number of\ntypes is plotted against the cumulative number of tokens.\n142\nCHAPTER 2. FOUNDATIONS OF NLP\nFIG 2.5 The Type-Token Growth Rate Curves for Indian\nLanguages\nIt can be seen that the curves show clear signs of saturation\n(slowing down, ﬂattening) for Indo Aryan languages whereas the\ncurves for Dravidian languages show rapid growth even at the\nright edge. This shows that the available corpora already include\nmost of the word forms in the case of Indo Aryan languages while\nmany possible word forms have possibly not occurred even once",
    "curves for Dravidian languages show rapid growth even at the\nright edge. This shows that the available corpora already include\nmost of the word forms in the case of Indo Aryan languages while\nmany possible word forms have possibly not occurred even once\nin the corpora of Dravidian languages.\nWe need much bigger\ncorpora for Dravidian languages. It may also be observed from\nthe curves that the number of types in Indo Aryan languages are\nof the order of 1,00,000 to 1,50,000 words, comparable to that\nof English. The number of types in Dravidian languages are at\nleast an order of magnitude bigger. Morphologically, Dravidian\nlanguages are among the most complex languages of the world,\ncomparable only to languages such as Finnish and Turkish.\nSince words are arbitrary mappings from sounds to meanings,\nthey cannot be derived from any more basic representation - words\nwill have to be somehow stored in our mental lexicon. Computers\ndo the same. The numbers seem to be small and today’s comput-\ners should have no diﬃculty in listing all the words and permitting\neﬃcient search. Several good data structures and indexing tech-\n2.2. COMPUTATIONAL LINGUISTICS\n143\nniques exit for eﬃcient indexing of dictionaries.\nHowever, dictionaries may list only the root forms of words\nand expect the morphological component to handle the inﬂected\nwords. Taken together, the dictionary and the morphology must\nbe capable of handling all word forms including inﬂected, derived,\nconﬂated and compound forms in various combinations.\nWhat\naspects are handled by the morphological component and which\ntypes of word forms are directly listed is dictated both by the-\noretical considerations and simplicity and eﬃciency of practical\nimplementations.\nStructure of Words - Morphemes\nLook at the words:\ndog\ndogs\ncat\ncats\ntree\ntrees\ncow\ncows\nWe can see that the words in the second column are plural\nforms of the corresponding words in the ﬁrst column. When we",
    "oretical considerations and simplicity and eﬃciency of practical\nimplementations.\nStructure of Words - Morphemes\nLook at the words:\ndog\ndogs\ncat\ncats\ntree\ntrees\ncow\ncows\nWe can see that the words in the second column are plural\nforms of the corresponding words in the ﬁrst column. When we\nﬁnd a large number of examples of a particular phenomenon, we\ntend to generalize and make a rule - you can add an -s to an\nEnglish noun to make a plural form. The words in the ﬁrst column\ncannot be broken down into smaller parts in any meaningful way,\nthe words in the second column can be considered to be made up\nof two parts - a root and the -s suﬃx. Such minimal units of word\nbuilding are termed ‘morphemes’. Thus dog is a base morpheme\nand -s is a plural morpheme. The meaning of the word dogs is\nin some intuitive sense, related to the morphemes it is made up\nof. Note that arbitrary substrings of words cannot be considered\nas morphemes even if they occur quite frequently. -ceive is not\na morpheme although it occurs with many words such as receive,\nperceive, conceive, deceive.\nMorphemes that can occur independently are called free mor-\nphemes and morphemes which can only occur in conjunction with\nother morphemes are called bound morphemes.\nDog, cat, tree,\ncow are all free morphemes and -s is a bound morpheme. Bound\nmorphemes that occur as preﬁxes or suﬃxes are called aﬃxes.\n144\nCHAPTER 2. FOUNDATIONS OF NLP\nThus -s is a suﬃx and un- as in ’uneasy, unethical and unreal-\nistic’ is a preﬁx. In some languages, an aﬃx may also occur in\nthe middle of another morpheme in which case it is known as an\ninﬁx Contracted forms like ’ll, ’s, ’re are also bound morphemes.\nMorphemes to which aﬃxes are attached are called base or stem\nmorphemes.\nAt one point of time, morphemes were deﬁned to be minimal\nmeaningful units. It has since been realized that the meaning of\na morpheme may vary based on suprasegmental features such as\nstress and tone, as also with the immediate context in which it",
    "morphemes.\nAt one point of time, morphemes were deﬁned to be minimal\nmeaningful units. It has since been realized that the meaning of\na morpheme may vary based on suprasegmental features such as\nstress and tone, as also with the immediate context in which it\noccurs. That is why we have deﬁned morphemes as simply appro-\npriate units for word formation and avoided any direct reference\nto the notion of minimal units of meaning. A morpheme may be\nrealized as one or more morphs. The morphs of a morpheme are\ncalled its allomorphs.\nThe morphological component must include the list of mor-\nphemes allowed in the language, the legal combinations of such\nmorphemes, the changes that take place when morphemes are\njoined to form words, and the associated changes in meaning if\nany. We will then be able to construct complex word forms from\nthe constituent morphemes, break words into the constituent mor-\nphemes, and understand the meaning of the whole in terms of the\nmeanings of the parts.\nThere is some psychological evidence to show that morphemes\nare real.\nWe think in terms of morphemes and the way they\ncombine to form words. For example, when we work with new or\nforeign words, possibly not already stored in our mental lexicon,\nwe seem to be applying the rules of word formation based on\nmorphemes we know. There is also some psychological evidence\nto show that we do not always apply rules to construct or analyze\nwords - we seem to be storing the words directly at times although\nwe could have analyzed those words using the rules we already\nknow. At least the most frequent word forms seem to be directly\nstored in the mental lexicon. One way is to think that the rules\ncome ﬁrst and then all the forms generated by such rules. The\nother way to think is that words are initially simply stored and as\nwe keep seeing many similar examples we naturally generalize by\nlearning the rules. There have also been some studies regarding\nautomatic acquisition of morphology from training data.",
    "other way to think is that words are initially simply stored and as\nwe keep seeing many similar examples we naturally generalize by\nlearning the rules. There have also been some studies regarding\nautomatic acquisition of morphology from training data.\n2.2. COMPUTATIONAL LINGUISTICS\n145\nWord Formation\nWords in a language can be broadly classiﬁed into Open Class and\nClosed Class words. Closed class words include grammatical or\nfunction words such as articles (example: a, an, the), prepositions\n(example: of, in, from, with), conjunctions (example: and, but),\nquantiﬁers (example: all, some, any, each, every), demonstratives\n(example: this, that).\nClosed class words are usually small in\nnumber and ﬁxed - new words cannot be coined or added easily.\nOn the other hand, open class words include the content words -\nnouns, verbs, adjectives and adverbs. There are usually a large\nnumber of words in this class and new words keep getting added\nto language.\nThere are situations where function words are dropped and\nonly content words are retained. This happens in telegraphic lan-\nguage, in early stages of child language, in the speech of people\nwith aphasic brain syndrome, in classiﬁed advertisements, certain\nstyles of poetry, in news headlines and wherever there is need to\nreduce a message to its essentials. Many applications in language\nengineering routinely remove these so called stop words.\nStop\nwords tend to be small and frequent.\nWords connect sound sequences and meanings. New words\ncan be created by inventing new sound sequences and pairing them\nwith meanings, by changing the meaning of existing words, and\nby augmenting or modifying the sound sequence of an existing\nword. Let us now examine some of the mechanisms that languages\nprovide for creating new words.\n• Acronyms (a word formed from the initial letter or letters of\neach of the successive parts or major parts of a compound\nterm) tend to become words in their own right. Examples:",
    "word. Let us now examine some of the mechanisms that languages\nprovide for creating new words.\n• Acronyms (a word formed from the initial letter or letters of\neach of the successive parts or major parts of a compound\nterm) tend to become words in their own right. Examples:\n‘radar’ (radio detection and ranging), ‘laser’ (light ampliﬁ-\ncation by stimulated emission of radiation)\n• Abbreviations such as ‘PC’ (Personal Computer) and ‘CD’\n(Compact Disk) tend to get full word-hood\n• Clippings such as ‘Prof’, ‘Fax’, ‘Photo’ tend to get full word-\nhood\n• Orthographic Abbreviations such as ‘Mr.’, ‘Dr.’, ‘MB’ be-\ncome words. Here the pronunciations may not necessarily\nbe altered.\n146\nCHAPTER 2. FOUNDATIONS OF NLP\n• New words can be created by blending existing words - ‘in-\nfomercial’ from information and commercial, ‘edutainment’\nfrom education and entertainment, ‘brunch’ from breakfast\nand lunch etc.\n• Generiﬁcation: Xerox, the name of a company has now\ncome to mean photocopying.\n• Giving new grammatical categories to words: to skill your\nemployees, to ponytail the girl, to PERT the project\n• Metaphorical extension: Let me chew on those new ideas\n• Compounding:\nCompound words are formed by joining together two (or\nmore) words.\nThe part of speech of a compound is the same as that of\nits head, which in English is usually the rightmost word.\nCompounds are sometimes written as a single orthographic\nword without spaces, sometimes the individual words are\nseparated by hyphens and in other cases the words are writ-\nten with intervening spaces just as in the case of phrases.\nEnglish is not very consistent at this. Hyphens tend to get\nlost over time as the compound sets in ﬁrmly. Multi-token\ncompounds are more often written with spaces in English\nand without spaces in German.\nCompounds diﬀer from phrases along several dimensions.\nSome compounds have a characteristic stress pattern as-\nsociated with them. In noun-noun compounds in English,",
    "compounds are more often written with spaces in English\nand without spaces in German.\nCompounds diﬀer from phrases along several dimensions.\nSome compounds have a characteristic stress pattern as-\nsociated with them. In noun-noun compounds in English,\nthe stress is usually on the ﬁrst word. Thus highchair is a\ncompound meaning a chair specially designed for kids and\nhighchair is a phrase meaning any chair that is high. Words\nin a phrase can be inﬂected - we can say higher chair. We\ncan incorporate modiﬁers into phrases - we can say a very\nhigh chair. Compounds are much more tightly integrated.\nWe cannot say a very white house while referring to the\nresidence of the president.\nIt is not always straightforward to predict the meaning of a\ncompound. Alligator shoes are made of alligator hide and\nhorse shoes are iron shoes worn by horses.The meaning of\nthe head of a compound is usually central to the meaning\nof the whole compound.\n2.2. COMPUTATIONAL LINGUISTICS\n147\nSanskrit has a wide variety of compounds. Compounding is\na very productive mechanism for word formation, so much\nso that in many places compounds are more frequent than\nsimple words. Tokens tend to become very long and com-\nplex (example: nijas’ookas’aMkuutpaaTanakSamamiva =\nnija + s’ooka + s’aMku + utpaaTana + kSamaM + iva).\nA thorough understanding of saMdhi and samaasa is essen-\ntial to understand a Sanskrit text.\n• Derivation:\nNew words can be derived by combining base morphemes\nwith aﬃxes. For example, the suﬃx -er can be added to\nverbs to make new words that have the meaning one who\ndoes X or an instrument that does X where X is the meaning\nof the verb: sing-singer, dance-dancer, write-writer, drive-\ndriver etc. The study of how aﬃxes combine with stems to\nderive new words is known as derivational morphology and\naﬃxes such as the -er agentive suﬃx are known as deriva-\ntional aﬃxes.\nWord formation follows systematic principles of morphol-\nogy. There are rules by which complex words are built from",
    "derive new words is known as derivational morphology and\naﬃxes such as the -er agentive suﬃx are known as deriva-\ntional aﬃxes.\nWord formation follows systematic principles of morphol-\nogy. There are rules by which complex words are built from\nsimpler words and morphemes, and, conversely, these same\nrules can be used to analyze complex words into simpler\nones.\nDerivation is usually associated with\n– Category Change\n– Semantic Change\nSing is a verb and singer is a noun meaning one who sings. -\nable derives adjectives from verbs and the derived adjectives\nmean ’able to be Xed’ where X is the meaning of the verb.\n-ion derives nouns from verbs. The sound often changes to\nsh from t. The stress of the derived word is always on the\nvowel just before the -ion irrespective of where the stress\noriginally was. -able can be combined only with transitive\nverbs. The subject of V+able is always the object of V.\nWord formation rules state predictable information about\ncomplex words. That is how we understand the meanings\n148\nCHAPTER 2. FOUNDATIONS OF NLP\nof novel words.\nBackformation\nThe words pedlar, beggar, hawker, stoker, scavenger, edi-\ntor, swindler, burglar, sculptor existed long before people\nthought they were the agentive nouns derived from verbs\nby adding -er and created the verbs to peddle, to beg, to\nhawk, to stoke, to scavenge, to swindle, to edit, to bur-\ngle, and to sculpt.\nThe words resurrection, preemption,\nvivisection, electrocution, television, emotion and donation\nexisted and people created the verbs to resurrect, to pre-\nempt, to vivisect, to electrocute, to televise, to emote and\nto donate. One ﬁne day, people may take the words hand-\nwriting and handwritten and create a verb to handwrite.\nThis is called backformation, the process of using a word\nformation rule to analyze a morphologically simple word as\nif it were a complex word in order to arrive at a new simpler\nform. Ironically, the word backform has been backformed\nfrom the word backformation!.",
    "This is called backformation, the process of using a word\nformation rule to analyze a morphologically simple word as\nif it were a complex word in order to arrive at a new simpler\nform. Ironically, the word backform has been backformed\nfrom the word backformation!.\nMorphological rules and analyses are not merely abstract the-\nories. Speakers produce and hearers understand new words using\nthese rules and analyses. NLP researchers are naturally concerned\nmore about simplicity and eﬃciency considerations but some un-\nderstanding of the theoretical underpinnings is essential to curtail\nad-hocism and arbitrariness in design and implementation.\nIndian languages exhibit a rich and complex system of word\nformation.\nSanskrit has perhaps the most elaborate system of\nforming words. We will not be able to give a detailed account\nof Sanskrit morphology here, readers are advised to consult any\ngood grammar book. We shall only show through examples how\nknowing the meaning of a small number of roots guides us in\nknowing the meaning of more complex words. The root jnya is\nto know. Jnyaana is knowledge, ajnyaana is ignorance, vijnyaana\nis discrimination (and hence science), prajnya is consciousness,\njnyeeya is the thing that is knowable, jnyaatr is the the knower,\njnyaani is a knowledgeable person, ajnyaani is an ignorant person,\nvignyaani is a scientist. Note how the English equivalents are all\nmorphologically completely unrelated to one another.\n2.2. COMPUTATIONAL LINGUISTICS\n149\nkhaga is a bird because it khee gacchati goes in the sky. paadapa\nis a tree because it paadeena pibati drinks from the foot (absorbs\nfood and water from the roots). s’asa’ is rabbit. s’as’aaMka is one\nwho bears the symbol of rabbit, namely the moon. s’as’aaMkaaMka\nis one who bears the symbol of moon, namely lord s’iva. Even\nsimple words have meanings that can be understood in terms of\neven more simpler components. Many roots are actually single",
    "who bears the symbol of rabbit, namely the moon. s’as’aaMkaaMka\nis one who bears the symbol of moon, namely lord s’iva. Even\nsimple words have meanings that can be understood in terms of\neven more simpler components. Many roots are actually single\nsyllables. In comparison it appears that English requires us to\nblindly memorize a much larger number of arbitrary associations.\nSanskrit sentences are small in size but convey a lot of meaning.\nOne word in Sanskrit equals many words in English.\nOne ex-\nample will suﬃce: agajaananapadmaarkaM (= agaja (aga (a +\ngaM = immovable, that is, mountain) + ja = born) + aanana\n= face + padma = lotus + arkaM = sun) = one who makes the\nface of paarvati (daughter of mountain king) bloom with happiness\nlike the sun does to the lotus, that is Lord gaNees’a, son of s’iva\nand paarvati. English translations are usually much larger in size.\nThey are more like explanations than translations. Some of the\ngreatest Sanskrit books written thousands of years ago and widely\nread and discussed even today are just a few dozens of lines.\nIn India we have the tradition of praying God by chanting\nhis one thousand names (sahasranaama). Giving one thousand\ndiﬀerent names would not make any sense in a language which\ndoes not permit a lot of meaning to be encoded in words containing\njust a few letters. Every one of the thousand names is actually\na prayer in itself, full of meaning. Each name is often just one\ntoken.\nTry John, Peter, Bob, Bill, ...!\nPerhaps the study of\nmorphology and lexical semantics is more interesting and revealing\nthan studies of syntax, at least in Indian languages.\nInﬂectional Morphology\nInﬂectional aﬃxes result in certain changes in grammatical fea-\ntures without signiﬁcantly changing anything else.\nIn English,\nthe main inﬂectional aﬃxes are 1) the plural suﬃx -s and 2) the\npossessive suﬃx ’s added to nouns, 3) the third-person-singular\nsuﬃx -s, 4) the past tense suﬃx -ed, 5) the progressive suﬃx -ing,",
    "tures without signiﬁcantly changing anything else.\nIn English,\nthe main inﬂectional aﬃxes are 1) the plural suﬃx -s and 2) the\npossessive suﬃx ’s added to nouns, 3) the third-person-singular\nsuﬃx -s, 4) the past tense suﬃx -ed, 5) the progressive suﬃx -ing,\nand 6) the past participle suﬃxes -en or -ed for verbs, and, 7) the\ncomparative suﬃx -er and 8) the superlative suﬃx -est for adjec-\ntives. All these aﬃxes are suﬃxes. Some derivational suﬃxes, on\n150\nCHAPTER 2. FOUNDATIONS OF NLP\nthe other hand, are preﬁxes. Derivation often changes the gram-\nmatical category while inﬂection does not.\nInﬂection is added\nafter derivation if any and there can be no more derivation once\nan inﬂection is in place. Meaning changes are minimal, regular\nand highly predictable in inﬂection while this is not always so in\nderivation. A railway sleeper is a piece of wood (or other similar\nobject) placed below the rails to hold the rails together, not just\nsomeone who sleeps. A waiter in a hotel is not just someone who\nwaits (you may have to wait for the waiter instead, especially if\nit is a ﬁve star hotel!). Note the the inﬂectional suﬃx -ing adds\na progressive sense to a verb whereas the derivational suﬃx -ing\nderives a noun from a verb. Inﬂected word forms that are bound\nby grammatical features such as personal endings are called ﬁnite\nforms and can be placed in tabular form of word sets known as\nparadigm tables.\nIndian languages have a richer system of inﬂectional morphol-\nogy than English. Telugu nouns are inﬂected for case, gender and\nnumber. Verbs are inﬂected for aspect, tense, gender, number and\nperson. Apart from these, a Telugu verb may also incorporate as-\npectual auxiliaries, negation, causation etc. Exhaustive studies\nhave not yet been made for many of our languages. Some experts\nhave estimated that there can be as many as 1,80,000 diﬀerent\nword forms that can be obtained from a single verb root in Tel-\nugu!\nMeaning of Complex Words",
    "pectual auxiliaries, negation, causation etc. Exhaustive studies\nhave not yet been made for many of our languages. Some experts\nhave estimated that there can be as many as 1,80,000 diﬀerent\nword forms that can be obtained from a single verb root in Tel-\nugu!\nMeaning of Complex Words\nThe main justiﬁcation for analyzing the structure of words in\nterms of the constituent morphemes comes from the assumption\nthat the meaning of the word as a whole can be understood in\nterms of the meaning of its parts. Let us examine to what ex-\ntent this assumption of compositionality of meaning is actually\nvalid. Curable means able to be cured and inﬂatable means able\nto be inﬂated. Things are not always so simple. If a theory is\nquestionable, it is not just that questions can be asked about that\ntheory. Questions can anyway be asked about any theory. If you\nsay a theory is questionable, you mean it is dubious and suspect.\nIf a bill is payable by so and so date, it means you better pay it\nbefore the speciﬁed data. If a book is readable, it is well writ-\nten. Morphology can only point to the general nature of semantic\n2.2. COMPUTATIONAL LINGUISTICS\n151\nrelationships between a complex word and its constituent parts.\nMorphology is only a guide, not a complete solution. We cannot\nexpect to write down a set of rules that can help us to correctly\nanalyze the meaning of any given word. It is not easy to write\ncomputer programs to analyze the meanings of words. It is not\neasy to automate language understanding.\nNew Meanings for Old Words\nNative Speakers often take an existing word and extend its mean-\ning in a recognizable way. Although no new word is added, the\nlanguage is enriched just the same. For example, many terms used\nin ocean navigation have been extended to the realm of space ex-\nploration - ship, docking, navigation, ﬂoating, captain, crew, deck,\netc. Although the situations are drastically diﬀerent, people see\nenough similarities to make analogical or metaphorical extension.",
    "in ocean navigation have been extended to the realm of space ex-\nploration - ship, docking, navigation, ﬂoating, captain, crew, deck,\netc. Although the situations are drastically diﬀerent, people see\nenough similarities to make analogical or metaphorical extension.\nHere objects, ideas or events from one realm are described with\nwords from a diﬀerent realm of objects, ideas and events. Words\nlike chew, swallow and digest are used in the mental realm. In-\ndeed people show great creativity and imagination in extending\nthe existent language into new realms of human experience.\nWe generally tend to extend from simpler to more complex,\nfrom general to speciﬁc, from old to new, from physical to ab-\nstract. Spacial words have been extended to temporal domain -\nin, on, ahead, before, etc. The terms hot and cool have been ex-\ntended and applied to all kinds of domains. Meanings can also be\nnarrowed down. The meat, which once used to stand for any edi-\nble solid food item, is now used only to refer to edible solid ﬂesh\nof animals. Deer used to mean any animal but now it refer to\njust that one species. (Interestingly, the equivalent Sanskrit word\nmRga is used to denote either the deer or any animal) There are\nalso cases where the connotations reverse over time, from good to\nbad for example, especially in slang words.\nElectrical engineers know bus, conductor and driver although\nno passengers. Computer science uses trees, roots, leaves, nodes,\nbranches, forests but not fruits or ﬂowers. Analogy works to some\nextent but not always. Computer scientists cut oﬀthe root of\na tree to obtain a forest!\nMechanical engineering has adapted\nterms like spring, crank, eccentric, nut, bolt, screw, dog. Civil\nengineers talk of plan and elevation. Every subject has its own\n152\nCHAPTER 2. FOUNDATIONS OF NLP\nset of technical terminology some of which may also exist with\nvery diﬀerent meanings in the common vocabulary of a language.\nLanguages inﬂuence one another.\nWords of a language are",
    "engineers talk of plan and elevation. Every subject has its own\n152\nCHAPTER 2. FOUNDATIONS OF NLP\nset of technical terminology some of which may also exist with\nvery diﬀerent meanings in the common vocabulary of a language.\nLanguages inﬂuence one another.\nWords of a language are\nborrowed by another language and gradually assimilated into the\nlanguage.\nLanguages change with time.\nLanguages change to\nsome extent from place to place too. The English word ‘man’ has\ncome from the Sanskrit word ‘maanava’ or ‘manuja’ meaning born\nfrom ‘manu’ the ﬁrst human being in each cycle of creation. As\nsuch, it is neutral to gender. The words ‘maanava’, ‘manuja’ and\n‘manuSya’ are applicable to both men and women, especially in\nplural. Etymologically, therefore, there is no gender bias in using\nthe word man.\nComputational Morphology\nThere are many competing theories of morphology in linguistics.\nAmong the descriptive models of morphology of the twentieth cen-\ntury, Item and Process model was considered as the precursor of\nmodern generative phonology. Item and Process model believes\nthat all surface stem alternants are derived from either one of the\nexisting surface stem alternants or a hypothetical stem set up. In\nthis model the stem is represented in the lexicon as invariant. In\ncontrast to Item and Process model, the Item and Arrangement\nmodel allows all surface stem alternants to be represented in the\nlexicon. A third model known as Word and Paradigm model al-\nlows full listing of paradigms in the lexicon.\nHere we have only tried to show that morphological rules and\nanalyses seem to be real in the sense people appear to understand\nand use them in practice. It would be wrong, however, to think\nthat there exist simple known rules to handle all kinds of words\nthat we come across in various applications. A waiter in a hotel\nis not just any person who waits (customers wait for food etc.\ntoo). Cooker is not one who cooks but the vessel we use for cook-",
    "that there exist simple known rules to handle all kinds of words\nthat we come across in various applications. A waiter in a hotel\nis not just any person who waits (customers wait for food etc.\ntoo). Cooker is not one who cooks but the vessel we use for cook-\ning. Analysis of words is not always straight forward. Synthesis is\nalso very complex. The noun form of create is creation but noun\nform of state is statement, not station. Computational models\nof morphology aim to produce exhaustive yet simple, elegant and\ncomputationally eﬃcient solutions to morphological analysis and\ngeneration. Computational models do not guarantee correct inter-\npretations of meanings. Instead the aim is to produce appropriate\n2.2. COMPUTATIONAL LINGUISTICS\n153\nanalyses for all words of a given language.\nFrom one point of view, morphology is just a question of stor-\ning all word forms versus deriving some from a smaller, more ba-\nsic set of stored words. Computationally, this translates into the\nusual trade-oﬀbetween time and space. It takes space to store\nall forms of all words and it takes time to search for a given word\nin a large list. It also takes some space to store the rules of mor-\nphology and more importantly, it takes time to select and apply\nthe rules. A practical solution has to ﬁnd a good compromise.\nThe productivity and uniformity of rules is the key factor.\nIn\nthe case of English, where there about 300 aﬃxes for derivation,\none widely used approach is to simply store all derived forms and\nhandle only the few cases of inﬂection through rules. There are\nmany idiosyncrasies in the rules of derivational morphology and\nit is simpler to simply store all the derived forms. Thus friend,\nfriendly and friendliness are all directly stored in the dictionary\nbut friends is not stored.\nSeveral computational models have also been proposed for\nmorphological analysis and generation.\nIn the next section we\nshall brieﬂy sketch one such model.\nMorphology of Indian Languages",
    "friendly and friendliness are all directly stored in the dictionary\nbut friends is not stored.\nSeveral computational models have also been proposed for\nmorphological analysis and generation.\nIn the next section we\nshall brieﬂy sketch one such model.\nMorphology of Indian Languages\nIndian languages in general and Dravidian languages in particular,\nexhibit a very rich system of morphology. While the English verb\neat gives rise to only a few variants such as eats, ate, eaten and\neating, the corresponding verb in Telugu can give rise to a very\nlarge number of variants. It is estimated that a single verb root\nmay give rise to as many as 1,80,000 diﬀerent possible word forms.\nWords in Dravidian languages like Telugu and Kannada are long\nand complex, built up from many aﬃxes that combine with one\nanother according to complex rules of saMdhi. For example, the\nsingle Telugu word nilabeTTukooleekapootunnaaDaa? essentially\nmeans something like “Is it true that he is ﬁnding it diﬃcult to\nhold on to (his words/some such thing)?” Obviously the trade-oﬀs\nare very diﬀerent compared to English.\nMorphology includes inﬂection, derivation, conﬂation (saMdhi)\nand compounding. Compound formation in modern Indian lan-\nguages is not very productive.\nCompounds are largely frozen\nforms and can even be listed in the dictionary. The rules of ex-\n154\nCHAPTER 2. FOUNDATIONS OF NLP\nternal saMdhi are also fairly simple and well known. However,\nthe juncture where saMdhi takes place is unknown and hence a\nsearch is involved in analysis. Nouns may be inﬂected for case and\nnumber and as such not too complex although more complex than\nthat of English. The most challenging part is the verb morphol-\nogy. Verbs may be inﬂected for aspect, tense, gender, number and\nperson. There are also a number of forms which are independent\nof gender-number-person variations. These are called non-ﬁnite\nforms as against the ﬁnite forms that are determined by personal\nendings.",
    "ogy. Verbs may be inﬂected for aspect, tense, gender, number and\nperson. There are also a number of forms which are independent\nof gender-number-person variations. These are called non-ﬁnite\nforms as against the ﬁnite forms that are determined by personal\nendings.\nTo illustrate the nature and complexity of morphology of In-\ndian languages, here we give a brief sketch of Kannada morphol-\nogy using a computational model known as Network and Process\nModel. Kannada is one of the four major literary languages of\nthe Dravidian family, spoken mainly in the state of Karnataka,\nSouth India.\nKannada is mainly an agglutinating language of\nthe suﬃxing type. Nouns are marked for number and case and\nverbs are marked, in most cases, for agreement with the subject\nin number, gender and person. This makes Kannada a relatively\nfree word order language. Here we take examples from the in-\nﬂectional morphology of Kannada verbs. We exclude preﬁxation,\nexternal saMdhi and compounds in our discussions here. We give\na brief account of the Network and Process Model and then give\nexamples of Kannada inﬂection using this model.\nIn the Network and Process model, morphology is divided\ninto two distinct but related components respectively called the\nnetwork and the process. The network component includes three\naspects:\n• the various aﬃxes (morphemes) that take part in the mor-\nphological processes in the language\n• the associations between the aﬃxes (morphemes) and the\ngrammatical features (and hence meaning)\n• constraints on the selection and ordering of aﬃxes (mor-\nphemes) in various combinations\nWhen morphemes combine, often there are complex morpho-\nphonemic changes at the juncture as speciﬁed by the rules of\nsaMdhi. The saMdhi processes are dealt with in a separate com-\nponent called the process. As we shall see soon, this division into\n2.2. COMPUTATIONAL LINGUISTICS\n155\nthe two components oﬀers certain unique advantages over other\npossible approaches.\ni) The Network Component:",
    "saMdhi. The saMdhi processes are dealt with in a separate com-\nponent called the process. As we shall see soon, this division into\n2.2. COMPUTATIONAL LINGUISTICS\n155\nthe two components oﬀers certain unique advantages over other\npossible approaches.\ni) The Network Component:\nConsider the structure of a ﬁnite verb in Kannada. In its sim-\nplest form, a ﬁnite verb form includes the root, a tense suﬃx and\na gender-number-person suﬃx, taken in that order. For example\nmaaDu\n+ utt\n+ aane\nRoot:(do)\nTense:non_past\nGNP: m,sl,p3\n= maaDuttaane\n((he) does)\nIt may be noted that the gender, number and person are all\nencoded into a single atomic suﬃx. Further, note that while Kan-\nnada has three persons, three genders and two numbers, not all\ncombinations have distinct suﬃxes. Thus the suﬃx ’aare’ indi-\ncates third person masculine or feminine plural - it is partly neu-\ntral to gender. Similarly, in ﬁrst and second persons, there are\nno gender distinctions. Also, the gender-number-person suﬃxes\nshow variations across the three tenses - there are three separate\nsets of suﬃxes, one for each tense. For example, the n-sl-p3 suﬃx\nis ’ide’ in the past tense, ’ade’ in the non-past, and ’udu’ in the fu-\nture/habitual. Thus the selection of a particular gender-number-\nperson suﬃx is conditioned by the selection of the tense suﬃx\nand vice versa. Likewise, there are constraints on the selection\nof auxiliary verbs in the non-ﬁnite forms. For example, aspectual\nauxiliaries like biDu (lit. leave or let go), nooDu (lit. see), koDu\n(lit.\ngive) and haaku (lit.\nput) occur only after a past verbal\nparticiple and other aspectual auxiliaries such as aagu (lit. be-\ncome) and toDagu (lit. start) occur only in an inﬁnitival context.\nThe network component provides a simple and eﬃcient scheme for\nincorporating such constraints. Figure 2.6 gives a sample of the\nnetwork for the inﬂectional morphology of Kannada verbs. The\nﬁrst line lists all the states, the second line gives the start state",
    "The network component provides a simple and eﬃcient scheme for\nincorporating such constraints. Figure 2.6 gives a sample of the\nnetwork for the inﬂectional morphology of Kannada verbs. The\nﬁrst line lists all the states, the second line gives the start state\nand the third line lists the set of ﬁnal states. The subsequent lines\nare triplets indicating state transitions. The ﬁrst component gives\nthe source state and the last component gives the destination state\n156\nCHAPTER 2. FOUNDATIONS OF NLP\nfor the transition. The middle component gives the labels and the\nassociated feature bundles.\n0 10 20 40 50 51 52 60 70 100\n0\n100\n0\n- ASPECT:phi,iru(perfective)\n-\n40\n40\n- TENSE:PAST:id\n-\n50\n40\n- TENSE:PRESENT:utt\n-\n51\n40\n- TENSE:FUT:uv\n-\n52\n51\n- AGR:GNP1\n-\n60\n52\n- AGR:GNP2\n-\n60\n53\n- AGR:GNP3\n-\n60\n60\n- CLITIC:(question,doubt,surprise...)\n-\n70\n70\n- VOCATIVE:ammaa,appaa,ayyaa,aNNaa,akkaa,... - 100\n0\n- CONCESSIVE:ruu\n- 100\n0\n- CONDITIONAL:re\n- 100\n0\n- IMPERATIVE:phi\n- 100\n0\n- PLURAL-IMPERATIVE:i\n- 100\n0\n- HORTATIVE:ooNa\n-\n60\n0\n- PAST-VERBAL-PART:i/u\n-\n10\n10\n- ASPECTUAL-AUX1:biDu,nooDu,koDu,haaku...\n-\n0\n0\n- INFINTIVE:alu\n-\n20\n20\n- APSCTUAL-AUX2:aagu,toDagu...\n-\n0\n0\n- CAUSATIVE:isu\n-\n0\nFIG 2.6 Kannada Morphology\nA network consists of a set of states interconnected by labelled\narcs. The states are given labels for convenience of reference. The\nstates in ﬁgure 2.6 are represented by numbers. The arc labels\nare the aﬃxes. Each aﬃx also carries the associated grammatical\nfeature bundles. There is a well deﬁned start state and one or more\nwell deﬁned terminal states. To generate a complete word form\nfrom a given root, we start at the start state and move through a\nsequence of states until we reach one of the terminal states. During\neach state transition, we attach the aﬃx on the corresponding arc\nlabel to the stem. If the aﬃx attachment is done according to\n2.2. COMPUTATIONAL LINGUISTICS\n157\nappropriate saMdhi rules as speciﬁed in the process component,",
    "sequence of states until we reach one of the terminal states. During\neach state transition, we attach the aﬃx on the corresponding arc\nlabel to the stem. If the aﬃx attachment is done according to\n2.2. COMPUTATIONAL LINGUISTICS\n157\nappropriate saMdhi rules as speciﬁed in the process component,\nwe get the correct word form. Some examples are given below to\nillustrate this process of generation:\nnooDu\n+ 0\n+ utt\n+ aane\nRoot:(see) Aspect:0 Tense:non-past GNP:m,sl,p3\n= nooDuttaane\n((he) sees)\ntinnu\n+ i\n+\nhaaku\n+\ni\nRoot:(eat) past_vbl_part. aspectual aux. past_vbl_part.\n+ iru\n+\nid\n+ anu\n= tiMduhaakiddanu\nAspect:Perf. Tense:past GNP:m,sl,p3 ((he) had eaten)\nFor analysis, we start from a terminal state and look for suf-\nﬁxes that match the labels of arcs leading to those states. By a\nseries of aﬃx stripping steps, we get to the root which can then be\nchecked against the lexicon. Computationally, it is usually more\neﬃcient to work from right to left for analysis since the number of\nsuﬃxes is much smaller than the number of roots. The branching\nfactor is much smaller if we work from right to left than the other\nway round. Further, we work directly at the level of aﬃxes and\nstems/roots, not at the level of individual letters. This adds to\nthe eﬃciency of this model.\nFormally, the network component is an extension of the well\nknown Finite Automata. The major extension is in terms of the\nincorporation of a separate process component that combines the\naﬃxes and/or root/stem according rules of saMdhi rather than\nsimply concatenating the strings. We thus have an augmented\nsimple (non-recursive) state transition network. The network used\nhere is a non-deterministic ﬁnite automaton. It is well known that\nfor every non-deterministic ﬁnite automaton, there is an equiva-\nlent deterministic ﬁnite automaton. Recognition/generation algo-\nrithms for deterministic ﬁnite automata are of linear time com-\nplexity and hence about the fastest possible.",
    "for every non-deterministic ﬁnite automaton, there is an equiva-\nlent deterministic ﬁnite automaton. Recognition/generation algo-\nrithms for deterministic ﬁnite automata are of linear time com-\nplexity and hence about the fastest possible.\nThe network component is itself inherently bi-directional and\ncan be used for both analysis and generation. Changes made to\n158\nCHAPTER 2. FOUNDATIONS OF NLP\nthe network are automatically and immediately reﬂected in both\nthe analyzer and the generator, saving the burden of having to\nbuild and update the two separately, ensuring consistency. This\nmakes it possible to build an analyzer, test it on large scale data\nand reﬁne accordingly. The corresponding generator is also au-\ntomatically obtained without any extra eﬀort. Observe that it is\nmuch easier to test an analyzer than a generator on large scale\ndata.\nNote that it is impossible to select, say, the past tense and\na GNP suﬃx from the non-past set. All kinds of selectional con-\nstraints (syllabic pattern, phonological constraints, etc.) are natu-\nrally and elegantly incorporated into the augmented network. An\narc is taken if and only if the conditions speciﬁed therein, if any,\nare satisﬁed. Observe that unlike ATNs, the networks used here\nare non-recursive and hence far simpler and far more eﬃcient.\nNetworks are eﬃcient in representation too. Common sub-\nparts can be collapsed. Loops can be represented easily. For ex-\nample, from state 10 in ﬁgure 2.6 we can go back to state 0. Thus\nwe can generate and analyze word forms such as ’maaDisikoMDu-\nbiTTuhoodanu’ (maaDu + isu + i + koLLu + i + biDu + i +\nhoogu + 0 + id + anu). Also, networks have simple visual rep-\nresentations, making it easy for people to read and understand.\nThis helps in grammar development.\nii) The Process:\nHere we are concerned with the saMdhi processes that take\nplace between aﬃxes and/or root/stem. While in some languages\nthe rules of saMdhi may be fairly simple and straight forward,",
    "resentations, making it easy for people to read and understand.\nThis helps in grammar development.\nii) The Process:\nHere we are concerned with the saMdhi processes that take\nplace between aﬃxes and/or root/stem. While in some languages\nthe rules of saMdhi may be fairly simple and straight forward,\nthere are languages where the saMdhi processes are quite involved.\nThe Network and Process model incorporates a general and pow-\nerful process component.\nThe most common saMdhi process in Kannada is the dele-\ntion(loopa) of ﬁnal vowel. If a vowel initial suﬃx combines with a\nvowel ﬁnal stem/root, the last vowel of the stem/root is deleted.\nThus, maaDu + id →maaDid. It must be noted, however, that\nKannada has really no consonant ending words at all. Even in the\ncase of loan word such as car, an enunciative vowel -u is added to\nmake it ’kaaru’. A large number of Kannada words end with -u.\nHowever, the ﬁnal -u in many words is only an enunciative vowel\n2.2. COMPUTATIONAL LINGUISTICS\n159\nand is not real. Thus the root is ’maaD’ and it is simply concate-\nnated with ’id’ to form ’maaDid’ - there is really no loopa taking\nplace. None the less traditionally the citation forms in dictionary\ninclude the enunciative vowel and a loopa saMdhi may have to be\ncarried out in the process component.\nIn some cases, aagama saMdhi also occurs.\nFor example,\n’bare’ + ’itu’ →’bareyitu’. Kannada does not permit vowel se-\nquences. Hence wherever two vowels join and the ﬁrst one is not\nenunciative and hence not deletable, a glide is inserted between\nthe two vowels. Similarly, an -a ending human noun gets an ’n’ in-\nﬂectional increment: ’huDuga’ + ’annu’ →’huDuganannu’. The\nprocess component checks for the relevant conditions and applies\nthe appropriate saMdhi rules for analysis or generation as the case\nmay be.\nFor example, the dative case suﬃx in Kannada is ’ige’ with\nthe following variations: Neuter nouns ending with -a take ’kke’\nand -e and -i ending words get ’ge’. Thus we get\nmara\n(tree)\n=>",
    "the appropriate saMdhi rules for analysis or generation as the case\nmay be.\nFor example, the dative case suﬃx in Kannada is ’ige’ with\nthe following variations: Neuter nouns ending with -a take ’kke’\nand -e and -i ending words get ’ge’. Thus we get\nmara\n(tree)\n=>\nmarakke\ntaayi\n(mother)\n=>\ntaayige\ntaMde\n(father)\n=>\ntaMdege\nmuugu\n(nose)\n=>\nmuugige\nhuDuga (boy)\n=>\nhuDuganige\nNote that although the suﬃx in question is an vowel initial\nsuﬃx, the ﬁnal vowel in the root is not lost in the last example and\nan inﬂectional increment gets in between. Without the increment,\nwe would have obtained huDugige which confuses with the dative\ncase of huDugi (girl).\nIn the Network and Process model, one has the freedom to\ndivide the work between the network and process components as\nappropriate. For example, the process component can be used as a\nbase for incorporating all exceptions and idiosyncratic variations.\nThis strategy of capturing the variations inside the process com-\nponent has the advantage that the network needs to show only\nthe default, major, unmarked, basic cases and thus simpler and\nmore economical. The rule is segregated from the exceptions.\nThe division of labour between the network and the process\ncomponents seems to be justiﬁed on several counts.\nThe net-\n160\nCHAPTER 2. FOUNDATIONS OF NLP\nwork component is declarative in nature.\nOn the other hand,\nthe processes of making and breaking saMdhi are more procedu-\nral. The order in which the various constraints are checked diﬀers\nfrom analysis to generation. While generating a word form from\na given root, we would know the grammatical and semantic fea-\ntures of the root to start with. When we are analyzing a complete\nword form, these aspects can only be veriﬁed after a tentative\nanalysis is produced and a possible root is hypothesized. Hence\nthe process component needs parallel procedures for making and\nbreaking saMdhi. On the other hand, the network component is\ninherently bidirectional.\nLemmatization and Stemming",
    "analysis is produced and a possible root is hypothesized. Hence\nthe process component needs parallel procedures for making and\nbreaking saMdhi. On the other hand, the network component is\ninherently bidirectional.\nLemmatization and Stemming\nWe have seen that morphology of Indian languages is quite com-\nplex. It has not been possible so far to develop high performance\nmorphological analyzers and generators for many of our languages.\nSystems developed so far are far from adequate. Even simple ap-\nplications such as spelling error detection and correction require\nthe full power of morphological analysis and generation. Is there\na way out?\nFor many applications, it is the root that contains the maxi-\nmum useful semantic content and the rest of morphemes indicate\ngrammatical features etc. which are relatively less relevant. For\nexample, an Information Retrieval system would be substantially\nbeneﬁtted if all words could be replaced with their roots. This\nwould reduce the sparseness introduced by the variations due to\nmorphology. The crucial part is the meaning of the root.\nLemmatization is the process of extracting the root of a given\nword, without necessarily performing full morphological analysis.\nLemmatization involves the reduction of corpus words to their re-\nspective headwords (lemmas). For example, the inﬂected forms\n“speaks” and “speaking” resulting from a combination of a single\nroot with two diﬀerent suﬃxes (-s and -ing) are brought back to\nthe lemma “speak”. Lemmatization is a process wherein the in-\nﬂectional and variant forms of a word are reduced to their lemma\n- their base form, or dictionary look-up form. When one lemma-\ntizes a text, one replaces each individual word in that text with its\nlemma. A text in English which has been lemmatized, then, would\ncontain all forms of a verb represented by its inﬁnitive, all forms\n2.2. COMPUTATIONAL LINGUISTICS\n161\nof a noun by its nominative singular, and so forth. In languages",
    "lemma. A text in English which has been lemmatized, then, would\ncontain all forms of a verb represented by its inﬁnitive, all forms\n2.2. COMPUTATIONAL LINGUISTICS\n161\nof a noun by its nominative singular, and so forth. In languages\nother than English, this process would involve similar, though not\nexactly the same, principles of reduction.\nIdeally, the words “am”, “are”, and “is”, would appear as\n“be”, and the words “car”, “cars”, “car’s” and “cars”’ would ap-\npear as “car”. The phrase “the boy’s cars are diﬀerent colours”\nwould appear in a lemmatized text as “the boy car be diﬀerent\ncolour”. In practice, lemmatization may not, however, observe\nthe rule of the common root in all cases, particularly in the case\nof irregular verbs.\nMorphological analysis naturally leads us to the root but full\nmorphological analysis can be diﬃcult and unnecessary. There\nmay be several morphemes and several levels of saMdhi within a\nword. The aim here is only to extract the root and complete mor-\nphological analysis may not be necessary for that. Thus lemmati-\nzation can be viewed as a short cut to full morphological analysis.\nOne may go a bit further and not even insist that we ob-\ntain correct roots. We will only be looking for common initial\nportions (assuming that the morphology is mainly suﬃxing in na-\nture). Thus we may reduce computer and computing into comput\n- the common initial part, also called the stem. Note that comput\nis not a valid word at all. Yet, by reducing all related morpholog-\nical forms to a common stem, we can expect improvements in the\nperformance of NLP applications such as Information Retrieval\nsystems. Stemming reduces the data sparseness and tightens the\nstatistics we derive from training data.\nMorphological analysis is complex and time consuming and\neven for languages such as English, lemmatization and stemming\nhave been widely used to build practical NLP systems. Porter’s\nstemmer and Lovin’s stemmer are good examples for English.",
    "statistics we derive from training data.\nMorphological analysis is complex and time consuming and\neven for languages such as English, lemmatization and stemming\nhave been widely used to build practical NLP systems. Porter’s\nstemmer and Lovin’s stemmer are good examples for English.\nPractical stemmers are far from perfect.\nOnce a stemmer\nstemmed stockings to stock and when a customer was searching\nfor stocks in the share market, the search engine retrieved a lot of\ndocuments relating to silk stockings!\nAs we have stated already, we do not have comprehensive,\nauthentic, reliable, readily usable electronic dictionaries and the-\nsauri for most of our languages in India. Morphological analyzers\navailable are incomplete, imperfect, untested. Good spell checkers\nare not available. As of this writing, we do not even have good\nlemmatization of stemming systems for many Indian languages.\n162\nCHAPTER 2. FOUNDATIONS OF NLP\nThere are no benchmark standards.\nThere is no standard test\ndata. There are no standard testing and evaluation procedures.\nResearchers would not like to admit but there is a lot that needs\nto be done on a priority basis if we have to reach a state of global\nleadership in language technologies.\n2.2.4\nPOS Tagging\nWe have seen that words may belong to more than one part-\nof-speech (POS) category. A dictionary simply lists all possible\ngrammatical categories for a given word. It does not tell us which\nword is used in which grammatical category in a given context.\nGrammatical categories, as indicated by POS tags, are rough in-\ndicators of meaning. Thus book when used as a noun refers to a\nbound collection of pages whereas the same word used as a verb\nindicates the activity of reserving a ticket, a seat, etc. Shop as a\nnoun refers to the store where we go and buy things and the same\nword when used as a verb refers to the activity of going to a shop,\nbuying things, paying money etc. Thus knowing the grammatical",
    "indicates the activity of reserving a ticket, a seat, etc. Shop as a\nnoun refers to the store where we go and buy things and the same\nword when used as a verb refers to the activity of going to a shop,\nbuying things, paying money etc. Thus knowing the grammatical\ncategory of a word in context is helpful in ultimately determining\nthe meaning.\nOf course words may have several meanings even within given\ngrammatical categories. Bank as a noun may refer to the bank of\na river or the ﬁnancial institution where we deposit money or a\nbank (that is, an array) of batteries, ﬁlters etc. Banking may refer\nto the activity of transacting with a bank or the slope intentionally\nprovided on highways near bends to compensate for the centrifugal\nforce of vehicles while they turn. Simple verbs like have, give and\ngo have dozens of senses. Knowing grammatical categories is not\nsuﬃcient to know the correct meanings or senses of words but it is\na useful ﬁrst step. Grammar is nothing but gross, overall meaning.\nNouns are things, verbs are actions or states and adjectives are\nproperties or attributes of objects. Once a ﬁre broke out in an\narmy camp and the commander shouted “ﬁre” only to ﬁnd all the\nsoldiers immediately picking up their riﬂes and ﬁring! If we can\ndistinguish between broad categories of meanings, even that much\nis useful.\nPOS taggers usually use richer sets of tags than just major\ngrammatical categories. 50 to 80 tags are not uncommon for En-\nglish. Important sub-categorizations can be brought out in deﬁn-\n2.2. COMPUTATIONAL LINGUISTICS\n163\ning the tag set. The Claws-5, Claws-7 and Penn Tree Bank tag\nsets are some of the well known tag sets for English. See Appendix\nfor the C5 tag set used in the example given here.\nPOS tagging is the process of taking plain text as input and\nautomatically marking or tagging each word with the grammati-\ncal category most appropriate to that word in the given context.\nThe following is an example of a piece of BNC (British National",
    "POS tagging is the process of taking plain text as input and\nautomatically marking or tagging each word with the grammati-\ncal category most appropriate to that word in the given context.\nThe following is an example of a piece of BNC (British National\nCorpus) text with c5 part-of-speech markers (taken from Captain\nPugwash and the Huge Reward):\n<s c=\"0000002 002\" n=00001>\nWhen<AVQ-CJS> Captain<NP0> Pugwash<NP0> retires<VVZ>\nfrom<PRP>\nactive<AJ0>\npiracy<NN1>\nhe<PNP> is<VBZ>\namazed<AJ0-VVN> and<CJC>\ndelighted<AJ0-VVN> to<TO0>\nbe<VBI>\noffered<VVN>\na<AT0> Huge<AJ0>\nReward<NN1>\nfor<PRP> what<DTQ> seems<VVZ> to<TO0> be<VBI> a<AT0>\nsimple<AJ0> task<NN1>.<PUN>\n<s c=\"0000005 022\" n=00002>\nLittle<DT0> does<VDZ> he<PNP> realise<VVI> what<DTQ>\nvillainy<NN1>\nand<CJC> treachery<NN1> lurk<NN1-VVB>\nin<PRP>\nthe<AT0>\nlittle<AJ0>\ntown<NN1>\nof<PRF>\nSinkport<NN1-NP0>,<PUN>\nor<CJC>\nwhat<DTQ>\na<AT0>\nhideous<AJ0> fate<NN1>\nmay<VM0> await<VVI> him<PNP>\nthere<AV0>.<PUN>\nPOS taggers are far from perfect. The Claws tagger used to\ntag the BNC corpus gives two-tag combinations such as AVQ-\nCJS when it is not quite sure which of these two is the correct\none. All it says is that the ﬁrst tag in a two-tag combination is\ngenerally more likely to be correct than the second. If the tagger\nis unable to do even that, it issues an UNC (unknown) tag. POS\ntaggers may not be able to distinctions between homonymy and\npolysemy. POS taggers generally may have diﬃculties in handling\ncompounds, phrases etc. Taggers are not perfect and there may\nbe tagging errors. Hence automatically tagged corpora must be\nused with care.\nDictionaries give us allowed sets of tags for words. Choos-\ning one of the possible tags has to be done based on context. In\n164\nCHAPTER 2. FOUNDATIONS OF NLP\nEnglish words that follow the word “the” may be expected to be\nnouns or adjectives. Context imposes restrictions and we must ex-\nploit these restrictions to tag each word with the most likely tag.",
    "ing one of the possible tags has to be done based on context. In\n164\nCHAPTER 2. FOUNDATIONS OF NLP\nEnglish words that follow the word “the” may be expected to be\nnouns or adjectives. Context imposes restrictions and we must ex-\nploit these restrictions to tag each word with the most likely tag.\nYou may think of formulating context based linguistic rules. Can\nyou get “was” after “the”? But a minute’s reﬂection will convince\nyou that there are too many possibilities and hard linguistic con-\nstraints cannot be found in most cases. Purely rule based WSD is\nnot practicable. The only way is to use statistical knowledge. The\ncategory of the current word can be ﬁxed based on the previous\nwords. The technology used for this is based on Hidden Markov\nModels (HMM).\nHMMs are statistical models based on simple notions such\nas probability of a sequence starting with a given symbol, prob-\nability of a symbol following another symbol and probability of\nsymbols arising from diﬀerent states. The probability of a sen-\ntence starting with “the” may be higher than the probability of a\nsentence starting with “been”. Similarly, the probability of a noun\nor an adjective coming after a determiner may be higher than the\nprobability of ﬁnding an auxiliary verb such as “was” after a de-\nterminer. “Pen” can be used as a verb but it is used more often\nas a noun. HMMs combine such basic notions of probability into\na mathematically sound model. One cannot sit down and manu-\nally compute all the required probabilities for all possible words\nand sequences of words in a language. Algorithms exist that can\nautomatically and eﬃciently compute all the required probabili-\nties from training data. Algorithms also exist for evaluating given\nsequences with respect to given models and for determining opti-\nmal state sequences. See section 2.3.3 for a brief introduction to\nHMMs.\nIn the HMM formulation for POS tagging, the sequence of\nwords in a given text would be considered as the observable states",
    "sequences with respect to given models and for determining opti-\nmal state sequences. See section 2.3.3 for a brief introduction to\nHMMs.\nIn the HMM formulation for POS tagging, the sequence of\nwords in a given text would be considered as the observable states\nof the HMM and the sequence of associated POS tags would be\nconsidered to be the hidden states.\nThere is an algorithm to\nﬁnd the optimal state sequence called Viterbi algorithm.\nThis\nalgorithm gives us the best possible tag sequence for a given word\nsequence. Note that whole sentences are tagged in one go, we do\nnot tag one word at a time.\nHMM based POS-taggers have been used quite successfully for\nEnglish and other positional languages. Practical taggers use more\nsophisticated models. For example, tri-tag based models would\n2.2. COMPUTATIONAL LINGUISTICS\n165\nconsider three-tag sequences as states and consider sequences of\nsuch states.\nA HMM model is built from training data.\nTraining data\nhere would consist of dependable POS-tagged text corpora. How\nwould one get a POS-tagged corpus in the ﬁrst place? We may\nhave to tag a small corpus manually. Using this small manually\ntagged corpus as a training corpus, a HMM based POS-tagger can\nbe built. This tagger is run on a larger corpus to produce a larger\nPOS-tagged corpus. Since the tagger was built from small data,\nits performance will perhaps be not very good. Manual checking\nmay be required to obtain accurately tagged corpus that can then\nbe used as training data to build even better taggers. This sort\nof boot-strapping is common to many statistical approaches.\nSuppose we tag every word blindly with the most frequent tag\nfor that word irrespective of the context in which it has occurred.\nFor English you will ﬁnd that the tagging so generated would be\nabout 85% correct! It appears that POS tagging is an easy job.\nThis is just the nature of the language and the task and this level\nof performance can be taken as a base line. Performance of taggers",
    "For English you will ﬁnd that the tagging so generated would be\nabout 85% correct! It appears that POS tagging is an easy job.\nThis is just the nature of the language and the task and this level\nof performance can be taken as a base line. Performance of taggers\nwould be judged relative to this base-line.\nWe have noted earlier that dictionaries usually simply list pos-\nsible grammatical categories without worrying about which cate-\ngory is correct for a given situation. It must also be noted that\ndictionaries usually list only the root forms of words, not all the\ninﬂected forms. Irregular forms may be listed but all the regular\ninﬂections are rarely listed. For languages such as English that\nshow extremely simple inﬂectional morphology and fairly strict\nword order, HMM models are naturally well suited.\nIndian languages, especially the Dravidian languages are char-\nacterized by a very rich system of inﬂectional morphology that is\nalso closely tied up with rich derivational morphology.\nWords\ncarry a great deal of grammatical information within themselves\nand word order is not the main channel for expressing syntac-\ntic constraints. Sanskrit takes this to the extreme - almost any\npermutation of a given sentence is grammatically valid and all per-\nmutations convey exactly the same primary meaning. Sentences\nare actually unordered sets of words, not really sequences. This\ngives tremendous amount of ﬂexibility to the composer in Sanskrit\nand poetry ﬂows out so ﬂuently and eﬀortlessly. In fact it is easier\nto write in verse form than prose in Sanskrit. A large portion of\n166\nCHAPTER 2. FOUNDATIONS OF NLP\nall works in Sanskrit is in verse form. Modern Indian language lie\nin between Sanskrit and English in this regard but perhaps much\ncloser to Sanskrit than to English. Any system that heavily de-\npends on word order would be inappropriate. Morphology should\nbe the major criterion. HMMs are perhaps not the best models\nfor Indian languages.",
    "in between Sanskrit and English in this regard but perhaps much\ncloser to Sanskrit than to English. Any system that heavily de-\npends on word order would be inappropriate. Morphology should\nbe the major criterion. HMMs are perhaps not the best models\nfor Indian languages.\nHMM models may not be very suitable for Indian languages.\nMorphology of Indian languages has not yet been worked out fully.\nPreliminary tag-sets have been deﬁned and used to build taggers\nfor some Indian languages but there is still a long way to go.\nThe grain size of the tag set is itself a major issue. If we include\nonly the major categories that would be too coarse and although\ntaggers may be built easily, tagged corpus so generated will be of\nvery limited use. If, on the other hand, we attempt to capture all\nthe ﬁne variations depicted in the morphology of the words, the\ntag set would become very large and POS tagging will essentially\nboil down to morphological analysis. Should we have a hundred\nthousand tags? If one were to postulate morphological analysis at\nrun time for any given application, generating and storing POS-\ntagged corpora would no longer serve any serious purpose.\nPerhaps a hierarchical design where tags are not simple atomic\nsymbols but composites indicating hierarchical reﬁnements would\nbe most appropriate for Indian languages. This would also enable\ndevelopment of more and more reﬁned tag sets and POS taggers\nin a phased manner. An an example of this concept, the tag v\nfor verb could be revised to include v-i and v-t for v-intransitive\nand v-transitive and this could further be reﬁned to show more\ndetailed sub-categorizations and so on.\n2.2.5\nSyntax: Grammars and Parsers\nLanguages exhibit complex structures and a detailed and system-\natic analysis of the structure of natural language sentences is in-\nvaluable in determining the meaning of the sentences. Form fol-\nlows function. Knowing the structure will, hopefully, help us in",
    "2.2.5\nSyntax: Grammars and Parsers\nLanguages exhibit complex structures and a detailed and system-\natic analysis of the structure of natural language sentences is in-\nvaluable in determining the meaning of the sentences. Form fol-\nlows function. Knowing the structure will, hopefully, help us in\nknowing the meaning. Syntax is a branch of linguistics which deals\nwith the problem of analyzing the structure of natural language\nsentences and producing structural descriptions. These structural\ndescriptions can then be analyzed further by semantic and prag-\nmatic components to obtain the meaning and the intentions be-\n2.2. COMPUTATIONAL LINGUISTICS\n167\nhind the sentence thereby capturing the communicative aspect of\nlanguage in its broadest sense.\nStudy of language can be roughly grouped into three levels -\nword level, sentence level and discourse or text level. Topics such\nas dictionaries, thesauri, WordNets, lexical semantics and mor-\nphology are primarily word level aspects of language. Syntax is\nmainly a sentence level phenomenon. Sentence level semantics is\nalso a very important aspect. Discourse analysis and pragmatics\nrequires going beyond individual sentences. The three levels are\nnot water tight compartments and this grouping is only a very\ngross classiﬁcation.\nThere are certainly important interactions\nbetween levels and the topics may span across the levels. Never-\ntheless this stratiﬁcation helps us to get a feel for where we stand.\nA lot of work has been done in Indian languages at the word level\nalthough we might still be far from perfect or even satisfactory\nlevels of performance. On the other hand, very little has been\ndone in syntax. There are hardly any computational grammars or\nparsers for Indian languages. Even such basic issues as sentence\nboundary identiﬁcation have not been addressed in any serious\nmeasure. The sentence level forms a very important barrier and\nif we do not make an all out eﬀort to pass through this stage at",
    "parsers for Indian languages. Even such basic issues as sentence\nboundary identiﬁcation have not been addressed in any serious\nmeasure. The sentence level forms a very important barrier and\nif we do not make an all out eﬀort to pass through this stage at\nthe earliest, there is a real danger that we will be left behind and\nwe will not be able to compete with others in the world. Com-\nputational syntax has somehow not received the due attention it\ndeserves in India. Given this, our treatment of syntax here will be\na bit more elaborate and detailed. Young researchers are well ad-\nvised to take computational syntax of Indian languages seriously.\nPeople do not communicate using isolated sentences but with\ncoherent pieces of discourse.\nMost syntactic theories, however,\nlimit themselves to analyzing one sentence at a time, isolating\ninter-sentence interactions for a separate detailed study. Firstly,\nthis is possible because sentences have their own internal syntactic\nstructure that is for the most part independent of how and where\nthe sentence is used. Secondly discourse level grammars have not\nmatured to a stage where one can employ discourse grammars\ndirectly in practice for syntactic analysis of whole discourses in\none stretch. We therefore generally presume that the input to the\nsyntactic analyzer is one isolated sentence.\nIt is not easy to provide a precise deﬁnition of what a sentence\nis. Sentences are generally considered to be sequences of words\n168\nCHAPTER 2. FOUNDATIONS OF NLP\nthat give complete meaning. Firstly this presupposes that there\nare identiﬁable things called words. Secondly it is not very clear\nwhat exactly we mean by complete meaning. If you remove some\nwords from a sentence, it will usually appear incomplete but it is\npossible to remove some words such as optional modiﬁers, with-\nout giving a sense of incompleteness. Also, a single sentence taken\nfrom a paragraph surely does not convey the complete meaning",
    "words from a sentence, it will usually appear incomplete but it is\npossible to remove some words such as optional modiﬁers, with-\nout giving a sense of incompleteness. Also, a single sentence taken\nfrom a paragraph surely does not convey the complete meaning\nconveyed by the whole paragraph. What do we mean by complete\nmeaning? Phrases and clauses may also be meaningful sequences\nof words. We cannot perhaps hope to give a very precise deﬁ-\nnition. We have to assume that sentences are identiﬁable units\nof language, larger than words, phrases and clauses but smaller\nthan paragraphs or discourse segments. We have to assume that\nsuch identiﬁable units of linguistics description exist in human\nlanguages. Similar statements have to be made about words.\nBoundaries between sentences are not always explicitly clear.\nIn English, sentences are terminated by a period, a question mark\nor an exclamation mark. However, the dot in decimal numbers is\nnot a period marking a sentence boundary nor are the periods in\nMr. and A. P. J. Abdul Kalam sentence boundaries. Sentences\nstart with upper case letter but proper names also use upper case\nletters.\nLists of proper names and abbreviations can be main-\ntained. If a sentence end with etc. we should have actually used\ntwo dots, one for marking the abbreviation and one for mark-\ning the end of the sentence but we do not do this. Identifying\nsentence boundaries in English is relatively easy but not entirely\ntrivial. There are systems built just for the purpose of breaking\nparagraphs into sentences and 99% plus accuracies have been ob-\ntained for English. There are no such systems for Indian languages\nas yet. There are also languages of the world where there are no\nexplicit sentences boundary markers. Linguistic theories are usu-\nally oblivious to these issues but if one were to build practical\nNLP systems, these questions become very important.\n2.2. COMPUTATIONAL LINGUISTICS\n169\nConsider the following:",
    "explicit sentences boundary markers. Linguistic theories are usu-\nally oblivious to these issues but if one were to build practical\nNLP systems, these questions become very important.\n2.2. COMPUTATIONAL LINGUISTICS\n169\nConsider the following:\nIn order to make good for the revenue deficit I\npropose to increase the tax on the following items:\n1. toilet soaps and tooth paste\n2. ready made garments (pure cotton and khadi shirts\nwill however be spared. Cotton mixed garments will\nbe taxed at rates as shown in table 5.4.)\n3. soft drinks, soft drink concentrates and essence;\nsoda and other aerated drinks (see appendix I\nfor a complete list.)\n4. furniture made up of\na. wood (ply wood, new-wood and particle\nboards are exempted)\nb. wrought iron or steel\nat the rates indicated in table 5.4 below.\nAs can be seen, items in a list may include sentences, sentence\nfragments or several sentences, and several such items may actu-\nally be part of one big sentence introducing the list. What are\nthe sentences here? Where do they begin and where do they end?\nShould we allow nested structures of sentences or should we treat\nparagraphs as simple linear sequences of sentences. Headings are\noften not full sentences nor can be treated as parts of any other\nsentence. Should we allow sentence fragments? How do we deﬁne\nsuch fragments?\nBoundaries between words are also not always explicitly clear.\nIn English, words are usually separated by spaces. However, there\nare compounds written by convention as two or more tokens sep-\narated by spaces but linguistically to be treated as single words\n(example: white house, wrist watch).\nThere are also contrac-\ntions and abbreviations which are written as one token but to be\n170\nCHAPTER 2. FOUNDATIONS OF NLP\ntreated as several (example: laser = light ampliﬁcation by stimu-\nlated emission of radiation, radar = radio detection and ranging,\nwon’t = will not). The situation is more complex for some lan-",
    "tions and abbreviations which are written as one token but to be\n170\nCHAPTER 2. FOUNDATIONS OF NLP\ntreated as several (example: laser = light ampliﬁcation by stimu-\nlated emission of radiation, radar = radio detection and ranging,\nwon’t = will not). The situation is more complex for some lan-\nguages of the world. Languages like Sanskrit and German have\nvery productive compounding processes and tokenizing by spaces\nwill not do.\nThere are also languages where there are no ex-\nplicit word boundary markers at all. Note also that neither word\nboundaries nor sentences boundaries are clearly observable in spo-\nken language even for English. Despite all this, we presume here\nthat units such as words and sentences exist and it is possible to\ndelimit discourse into sentences and view sentences as sequences\nof words. Syntax looks at one sentence at a time. A syntactic\nanalyzer, also known as a parser, takes one sentence at a time as\ninput and produces a description of the internal structure of the\nsentence.\nGRAMMAR\nLEXICON\nSTRUCTURAL DESCRIPTION\nONE SENTENCE\n(TEXT)\nSYNTACTIC\nANALYZER\n(PARSER)\nFIG 2.7 Syntactic Analysis\nSyntax deals with the characterization of the structure of nat-\nural language sentences and as such can be applied for natural\nlanguage understanding as well as for natural language genera-\ntion. In keeping with the tradition, we have taken the analysis\nview point here. Needless to say, the same frameworks can also\nbe applied for generating syntactically valid natural language sen-\ntences.\nIn a sense, the structural descriptions produced by the syn-\ntactic analyzer can themselves be viewed as raw representations\nof meaning, albeit broad and general, only waiting to be reﬁned\nand perfected by subsequent semantic and pragmatic components.\nThese structural descriptions produced by syntax only aim to cap-\nture the raw structural information content in the sentence. The\nstructure of a sentence is analyzed in terms of meaningful parts",
    "and perfected by subsequent semantic and pragmatic components.\nThese structural descriptions produced by syntax only aim to cap-\nture the raw structural information content in the sentence. The\nstructure of a sentence is analyzed in terms of meaningful parts\nand subparts and the relationships among these parts. The small-\n2.2. COMPUTATIONAL LINGUISTICS\n171\nest parts are the words which can be mapped on to grammatical\ncategories.\nAlthough we normally deal with grammatical cate-\ngories like noun and verb as uninterpreted symbols within a syn-\ntactic framework, it should be emphasized that these categories\nare also essentially semantic in nature - nouns represent things,\nverbs represent actions and states, etc. When we say that ‘book’,\n‘beauty’ and ‘London’ are all nouns, we are emphasizing what\nis really common between all these entities - that they are all\nthings in a broad sense. Thus grammatical categories are general\nmeanings, very general of course. When we need to make some-\nwhat ﬁner distinctions, we use the notion of sub-categorization.\nWe divide nouns into proper nouns and common nouns, verbs\ninto intransitive, transitive and bi-transitive verbs and so on, of-\nten making much ﬁner distinctions than these, leading to a more\ndetailed semantic classiﬁcation.\nIn any case, syntactic analysis\nis a kind of understanding wherein very gross, general meaning\nis captured. The syntactic structures that are obtained in such\nan analysis are representations of such very abstract and general\nmeaning. A fundamental requirement of a syntactic analyzer is\nthat it preserves and explicates as much of the meaning in the\ngiven sentence as possible but avoids distortions of meaning at all\ncosts.\nSyntax as an Independent Module\nSyntactic analysis is a step towards the ﬁnal goal of understand-\ning or generation and it is not appropriate to deal with syntax\ncompletely independent of the ﬁnal goals of NLP. What kind of\nanalysis needs to be made and what kind of structural description",
    "costs.\nSyntax as an Independent Module\nSyntactic analysis is a step towards the ﬁnal goal of understand-\ning or generation and it is not appropriate to deal with syntax\ncompletely independent of the ﬁnal goals of NLP. What kind of\nanalysis needs to be made and what kind of structural description\nshould be produced are therefore dictated by the needs of the sub-\nsequent semantic and pragmatic processes. Autonomy of Syntax\nis to be interpreted to mean that syntax can be taken up and\nstudied as an independent component within the overall system,\nnot for its own sake, not as an end in itself.\nSome researchers have argued that human mind does not pro-\ncess sentences in separate levels such as syntax, semantics and\npragmatics and hence a good computational model must also em-\nploy an integrated holistic approach to language understanding.\nOthers have argued and even attempted to provide evidence for\nthe psychological reality of independent modules. It should be\n172\nCHAPTER 2. FOUNDATIONS OF NLP\nnoted that our current understanding of semantics and pragmat-\nics is in no way as clear and complete as our knowledge of syntax\nand hence even if integrated processing may ﬁnally turn out to be\nthe ideal approach, a modular approach may be the best for the\ntime being.\nWe would like to emphasize that integrated or modular pro-\ncessing approaches should not be confused with implementation\nstrategies. The question is not really whether syntax, semantics\nand pragmatics should all be done parallelly or one after the other\n- this is more of an implementation strategy. The real question\nis whether it is reasonable to break the task into separate and\nindependent modules. The degree of interaction between various\nmodules is the dictating factor. The proponents of the integrated\nprocessing approach believe that the diﬀerent parts interact so\nmuch and in so many complicated ways that it is not reasonable\nto break the task into separate modules. Others believe that it is",
    "modules is the dictating factor. The proponents of the integrated\nprocessing approach believe that the diﬀerent parts interact so\nmuch and in so many complicated ways that it is not reasonable\nto break the task into separate modules. Others believe that it is\npossible to break the task into nearly decomposable modules. By\nnearly decomposable we mean that the interactions between the\nmodules are minimal, localized and clearly understood so that we\ncan treat the modules essentially independent of one another and\ntake care of the interactions separately.\nOf course if relevant semantic and pragmatic information was\navailable, it could help the syntactic analyzer. Is this worth the\neﬀort and is it practically possible in all cases? To illustrate the\nnature of this situation, we will use an analogy. Expanding con-\ntractions such as can’t and won’t is essentially a morphological\nlevel (that is word level) process. However to expand the contrac-\ntions in the following sentences\n1)\nHe’s a nice boy\n2)\nHe’s done it!\none has to get into the level of syntax. Analyzing individual\nwords is a step towards analyzing the structure of the whole sen-\ntence. If analyzing words requires syntactic analysis, we are into\na kind of a chicken and egg situation. More interestingly, we will\nhave to get into the level of discourse analysis if we have to deter-\nmine whether the we’re in the following sentence is we are or we\nwere since the tense can be determined in this case only based on\ndiscourse coherence.\n3)\nWe’re eagerly waiting for the arrival of our new car.\nShould we do discourse analysis before, or at the same time\n2.2. COMPUTATIONAL LINGUISTICS\n173\nwhen we are trying to ﬁnd out what the words in a sentence are?\nWe may feel it is much better to simply note down that we’re can\nbe either we are or we were and proceed with morphological and\nsyntactic analyses. If the eﬀort involved in solving a problem is\ntoo much, it is wiser to see if the problem can be left unsolved at",
    "We may feel it is much better to simply note down that we’re can\nbe either we are or we were and proceed with morphological and\nsyntactic analyses. If the eﬀort involved in solving a problem is\ntoo much, it is wiser to see if the problem can be left unsolved at\nthis stage, hoping that at a later stage the necessary information\nwould become more readily available. Postponing the solution of\na problem can make it much more simpler, thereby more than\ncompensating for the extra eﬀort involved in keeping track of the\nunresolved problem all that way along. This is the sense in which\nwe take autonomy of syntax. We should deal with syntax as inde-\npendently as possible and simply postpone decisions that require\nextensive or costly processing at the semantic or pragmatic levels.\nIt is not always possible or desirable to try to obtain all the\nrequired information to resolve an ambiguity then and there. Ob-\ntaining all required information, even when theoretically possible,\ncan be extremely costly in itself thereby more than compensating\nfor the gain in being able to make the current decision correctly.\nConsequently, total determinism as claimed by some researchers\nmay not a feasible goal. Yet it is highly desirable to try to at-\ntain determinism to the maximum extent possible. Determinism\nmeans being able to take the right decisions at the right time and is\ntherefore both theoretically and practically highly desirable. This\nrequires NLP to be viewed in terms of requirements, availability\nand ﬂow of information and breaking the problem into modules\naccordingly.\nIn fact compilers for computer programming languages follow\nthe same strategy. Programming languages are largely context\nfree but there are aspects that are not context free. The require-\nment that there be one to one correspondence between the formal\nand actual parameters in function deﬁnitions and function calls\ntranslates to a cross-serial dependency that context free grammars",
    "free but there are aspects that are not context free. The require-\nment that there be one to one correspondence between the formal\nand actual parameters in function deﬁnitions and function calls\ntranslates to a cross-serial dependency that context free grammars\nand stack based push down automata cannot handle. Similarly,\nthe requirement that variables are declared before they are used\ncannot be captured using context free rules. What compilers do is\nto ignore these non-context free aspects and go ahead with parsing\nusing context free grammars. All the aspects that were ignored\nare taken care of in the next phase which is generally known as\nsemantic analysis (although there is hardly any semantic analysis\ndone in the true sense).\n174\nCHAPTER 2. FOUNDATIONS OF NLP\nCapturing semantics of modiﬁer-modiﬁed relationships, anal-\nysis of anaphoric references to determine exactly what refers to\nwhat and the nature of the semantic relation between the refer-\nence and the referent, attachment of prepositional phrases and\nsubordinate clauses are examples of issues that a purely syntactic\nframework cannot hope to adequately model. These are all issues\nthat lie in the border between syntax and semantics. Syntax often\nprovides biases that can be used as default or preferred solutions\nbut syntax may not be able to give deﬁnite solutions in all cases.\nA syntactic theory must, therefore, either ignore these aspects al-\ntogether or admit that what it provides is at best a partial and\nimperfect solution.\nMost syntactic theories take the modular approach and pro-\npose an independent syntactic module. It is assumed that it is\npossible to deal eﬀectively with the structure of natural language\nutterances using only a bare minimum of semantic and pragmatic\ninformation.\nGrammars\nLanguages manifest as linear sequence of symbols in text or speech\nform. Not all sequences of these symbols are meaningful. We need\nto restrict the set of all possible sequences of symbols and include",
    "utterances using only a bare minimum of semantic and pragmatic\ninformation.\nGrammars\nLanguages manifest as linear sequence of symbols in text or speech\nform. Not all sequences of these symbols are meaningful. We need\nto restrict the set of all possible sequences of symbols and include\nall the valid sequences and only the valid ones. In other words, we\nneed to impose constraints on the possible sequences of symbols.\nSuch a set of constraints, expressed as rules, or principles or what-\never, is called a grammar. A grammar of a language is a formal\nspeciﬁcation of the valid structures in that language. Thus we may\nthink of grammars at various levels - morphology is the grammar\nat word level, syntax is usually concerned with grammars at the\nsentence level, discourse grammars deﬁne valid discourse elements\nand so on. In syntax, we are naturally concerned with grammars\nat the sentence level.\nWe have deﬁned syntactic analysis as a process of mapping a\ngiven sentence to a description of its structure. The inputs to the\nsyntactic analysis process, also called the parsing process, include\nthe given sentence and the knowledge of language, in particu-\nlar, the lexicon, morphology and the syntactic grammar of the\nlanguage. The goal of syntax in linguistics is to characterize a\ngrammar that accepts all valid sentences and only the valid sen-\n2.2. COMPUTATIONAL LINGUISTICS\n175\ntences. The search is for such a grammar that is simple, elegant\nand psychologically plausible.\nComputational grammars, that is grammars meant for auto-\nmatic processing in NLP applications, need to be detailed, precise\nand exhaustive. Syntacticians often give only samples and then\nget into theoretical discussions on the abstract characteristics of\ngrammars. What we need are descriptive grammars that are very\ndetailed and precise so that computer programs can correctly in-\nterpret and apply them. Developing wide coverage grammars has\nproved to be a very diﬃcult task. The best available grammars,",
    "grammars. What we need are descriptive grammars that are very\ndetailed and precise so that computer programs can correctly in-\nterpret and apply them. Developing wide coverage grammars has\nproved to be a very diﬃcult task. The best available grammars,\neven for languages such as English, are far from perfect today.\nWhen it comes to Indian languages, although so much is talked\nabout in theory, there is no computational grammar for any of\nour languages.\nProcess View and the Importance of Eﬃciency\nIt is a central working hypothesis of the linguistic theories that a\nnon-processing characterization is desirable. Grammars can then\nbe developed independent of how they are going to be used by\nparsers and generators.\nDetails of the algorithms can be kept\nseparate from the basic characterization of the knowledge of lan-\nguage. Grammars can be viewed as a speciﬁcation of a space of\ngrammatical possibilities that does not say anything about how\nto search that space. While this division of knowledge into what\nand how is a boon to the grammar writer, who can write his rules\nwithout worrying about the details of the parser, a purely abstract\nspeciﬁcation oriented view of grammar can be a very bad choice\nin NLP.\nWhile linguistics has typically dealt with characterization of\nstructures, the science of computation deals with theories of pro-\ncessing. NLP attempts to build computational models of the pro-\ncesses of understanding and synthesis. Computational paradigm\nis based on the belief that by developing theories of the processes\ninvolved, we will have a clear and revealing way of explaining the\nstructure. The structures follow from the processes. A procedural\nview point is desirable.\nOne should be careful not to confuse between idealization and\nproceduralization. One should not mix up the issue of procedu-\nral versus nonprocedural approach with the distinction between\n176\nCHAPTER 2. FOUNDATIONS OF NLP\ncompetence and performance.\nChomsky argues for a model of",
    "One should be careful not to confuse between idealization and\nproceduralization. One should not mix up the issue of procedu-\nral versus nonprocedural approach with the distinction between\n176\nCHAPTER 2. FOUNDATIONS OF NLP\ncompetence and performance.\nChomsky argues for a model of\ncompetence and against a performance model by talking about\nmemory limitations, changes in intention during speech, and even\nphysical states such as coughing, sleepiness or drunkenness. No-\nbody may want to build a model of performance that includes\ncoughing and sneezing. Idealizations are done in all approaches.\nThis is not any signiﬁcant argument against the procedural view\nat all.\nIgnoring all purely physical and psychological inﬂuences of ac-\ntual performance, the remaining knowledge of language, the char-\nacterization of the mental competence of a speaker, also has a\nprocedural part in it that has got to be dealt with explicitly.\nThe knowledge of language includes not only rules, constraints\nand principles but also the procedural aspects of what rules, con-\nstraints or principles are applied, how, when, in what order, etc.\nWe have seen that when people speak, they start from their aims\nand goals, they build up their strategies, decide the structures\nand words and construct natural language utterances. Similarly\nunderstanding language requires many steps. A procedural view\npoint is both natural and essential for NLP.\nIt should be emphasized that knowledge does not always have\nto be declarative and nonprocedural. The procedure for multiply-\ning two numbers is very much a part of the knowledge of most of\nus. There was a time in the history of AI where the arguments for\nand against procedural and declarative representations of knowl-\nedge had taken almost the shape of a controversy. It is universally\nagreed now that both kinds of representations of knowledge have\ntheir due share. There is no clash between the two, one comple-\nments the other.",
    "and against procedural and declarative representations of knowl-\nedge had taken almost the shape of a controversy. It is universally\nagreed now that both kinds of representations of knowledge have\ntheir due share. There is no clash between the two, one comple-\nments the other.\nThere is no guarantee that a good purely declarative compe-\ntence model automatically leads to eﬃcient performance. A model\ncannot be judged purely based on the structure of the declara-\ntive grammar that it employs. A model would be good only if\nboth the declarative and procedural components are good. We\nall agree that human beings are eﬃcient processors of language\nand linguistics is concerned with making good models of human\nlanguage cognition. How then can linguistic theories ignore the\neﬃciency of performance? It is not enough if grammars are sim-\nple and elegant, they must also be eﬃcient. Grammars must be\ndesigned for eﬃcient processing. Eﬃciency is a term applicable to\n2.2. COMPUTATIONAL LINGUISTICS\n177\nprocedures not abstract declarative knowledge.\nThus our aim in NLP should be to develop simple and elegant\ngrammars that are also amenable for eﬃcient parsing. We cannot\ncharacterize grammars independent of how they are going to be\nused. Parsing techniques and grammars are closely tied up. Data\nstructure and algorithms are two sides of the same coin. We have\nseen that developing grammars is a challenging task. Thus ease\nof grammar development is also a key issue.\nModern linguists within the generative tradition believe that\nthere is a single universal grammar underlying all human lan-\nguages. Children all over the world pick their language with more\nor less equal ease and within more or less the same amount of time.\nHow do we explain this? There must be underlying universal prin-\nciples and all the diﬀerences we see across languages of the world\nmust be superﬁcial diﬀerences that can be handled by setting val-\nues for some parameters. The parameters themselves must also",
    "How do we explain this? There must be underlying universal prin-\nciples and all the diﬀerences we see across languages of the world\nmust be superﬁcial diﬀerences that can be handled by setting val-\nues for some parameters. The parameters themselves must also\nbe universal. A great deal of research has gone on within this tra-\ndition over the last 50 years or so. However, neither the universal\ngrammar itself or its instantiations into any speciﬁc language seem\nto have been developed into an exhaustive, detailed and precise\nenough description which can be applied within a computational\nframework for any given language for NLU or NLG. We will not\nbe getting into details of these theories here. Instead, we will focus\non grammar formalisms that can be implemented more directly\non computers.\nGrammar Formalisms\nEvery grammar deﬁnes a language but a given language can be\ngenerated by many diﬀerent grammars. All possible grammars\nof a language will not in general be equally simple, natural and\namenable for eﬃcient parsing. Also, grammars can be expressed\nin a variety of ways.\nGrammars can be formulated as regular\nexpressions, ﬁnite state automata, phrase structure rules, trees\nand tree operations, principles, constraints, etc. How do we design\ngood grammars?\nWe need a meta language, called a grammar formalism, to\nspecify how exactly the grammar of a language is written.\nA\ngrammar formalism is a meta language - it is a language for ex-\npressing grammars. A grammar formalism speciﬁes the kinds of\n178\nCHAPTER 2. FOUNDATIONS OF NLP\ngrammars that can be written, deﬁnes the structure of grammars\nand hence the formal properties of the syntactic system employing\nthose grammars. A grammar formalism also suggests the nature\nof processing that is involved in syntactic analysis and the kinds of\nstructural descriptions produced. All grammars written within a\ngrammar formalism will have similar properties imposed on them\nby that grammar formalism. Thus instead of talking about the",
    "of processing that is involved in syntactic analysis and the kinds of\nstructural descriptions produced. All grammars written within a\ngrammar formalism will have similar properties imposed on them\nby that grammar formalism. Thus instead of talking about the\ndesirable properties of individual grammars of speciﬁc human lan-\nguages, we can talk about these common properties at the level of\nthe grammar formalism itself. Hence the design of a good gram-\nmar formalism can be taken as the central problem in syntax. Let\nus now see what constitutes a good grammar formalism.\nWhat Constitutes a Good Grammar Formalism?\ni) Simple Grammars:\nNLP requires exhaustive, detailed and precise grammars. Com-\nputers have no commonsense and no rule can be taken for granted,\nhowever simple or obvious it may appear to be for us. Writing\nsuch exhaustive and precise grammars is not a simple task. Com-\nplexity grows exponentially with the size of the grammar. A good\ngrammar formalism requires only a small number of simple gram-\nmar rules thus simplifying the grammar writing task. A simpler\ngrammar would also be a better choice as a cognitive model than\na grammar that employs unnecessarily complex contrivances. For\nexample, a grammar formalism that altogether eliminates the need\nfor long distance dependencies will surely be better than a gram-\nmar formalism that posits some complex mechanism to deal with\nsuch dependencies.\nA smaller and simpler grammar also makes the parser more\neﬃcient because at each step there are only a few alternatives\nto consider. An eﬃcient parser is required not only because of\npractical needs for fast processing but also from theoretical point\nof view.\nAn eﬃcient syntactic system is a better candidate as\na model of human language processing than an ineﬃcient one.\nSince computationally less powerful grammars will in general lead\nto more eﬃcient parsing, the search in NLP is for the least pow-\nerful grammar that is suﬃcient to deal eﬀectively with natural",
    "a model of human language processing than an ineﬃcient one.\nSince computationally less powerful grammars will in general lead\nto more eﬃcient parsing, the search in NLP is for the least pow-\nerful grammar that is suﬃcient to deal eﬀectively with natural\nlanguages. We should not use a bull dozer if that could be done\n2.2. COMPUTATIONAL LINGUISTICS\n179\nwith a hand tool. We should not use context free grammars if that\ncould be done with regular grammars. Some of the early grammar\nformalisms including the transformational grammar proposed by\nNoam Chomsky and the Augmented Transition Network (ATN)\ngrammars proposed by computer scientists were computationally\nmore much more complex than needed. Many of the major gram-\nmar formalisms proposed by linguists continue to be that way even\ntoday. Positing an unnecessarily complex mechanism is unwise\nand wasteful. Computer science provides many powerful tools for\nanalyzing the formal complexity of grammars and the correspond-\ning parsing systems.\nAcademicians and researchers have a tendency to ﬁrst look for\nwhat is most interesting, rather than what is most useful. Intel-\nlectuals are motivated by love of complexity. We take pride and\nderive personal satisfaction in doing complex things.\nWe even\nhave a tendency to do simple things in a complex way. The Red\nQueen in Lewis Carrol’s Alice in Wonderland said, “I could have\ndone it in a much more complicated way”, with immense pride. A\nlot of time and eﬀort is often spent on a few rare, odd exceptional\nexamples rather than lay primary emphasis on simple, commonly\nused structures. Priorities get distorted.\nii) Universality:\nIt has been well recognized that despite superﬁcial diﬀerences,\nhuman languages share certain common underlying principles.\nLinguists are on the look out for a universal grammar - a grammar\nthat uncovers these underlying universal features. Linguists view\nuniversal grammar as an in-born biological endowment of the hu-",
    "human languages share certain common underlying principles.\nLinguists are on the look out for a universal grammar - a grammar\nthat uncovers these underlying universal features. Linguists view\nuniversal grammar as an in-born biological endowment of the hu-\nman mind. They worry about things that are pre-wired into the\nhuman brain and things are learnt after birth. If a grammar could\nbe made universal and language independent, every eﬀort must be\nmade to move towards that end.\nFrom a purely practical point of view, however, we need not\nmake any reference to the human mind at all and deal with lan-\nguage as people use it, not with the hypothetical language or\ngrammar that may be there inside the mind of humans. Use of the\nterm ‘universal’ then only implies the identiﬁcation and separation\nof language independent aspects of grammar from the idiosyn-\ncratic features of individual human languages. A grammar that\n180\nCHAPTER 2. FOUNDATIONS OF NLP\nis largely common across human languages is theoretically very\ndesirable even without any reference to the human mind. Recog-\nnition of these universal common properties of human languages\nhelps us in building grammars and parsers for various human lan-\nguages with a minimum of eﬀort, with a minimum of duplication\nof work. A Grammar developed for one language can be easily\nadapted for another language. Perhaps common grammars can\nbe developed for a whole family of languages.\nA good grammar formalism should lay primary emphasis on\nthe universal aspects of grammar and relegate the treatment of\nidiosyncratic features of speciﬁc languages to a secondary level.\nThe idea is to concentrate more on the rule rather than on the\nexceptions. It is not as important to deal with the idiosyncratic,\npeculiar and infrequent constructions in speciﬁc languages as it is\nto deal with the more basic, common and frequent constructions\nelegantly and eﬃciently. Knowing the rule from the exception is",
    "exceptions. It is not as important to deal with the idiosyncratic,\npeculiar and infrequent constructions in speciﬁc languages as it is\nto deal with the more basic, common and frequent constructions\nelegantly and eﬃciently. Knowing the rule from the exception is\nextremely important for getting insights into the true nature of\nhuman languages and for discovering the universal nature of our\nlanguages.\niii) Good Structural Descriptions:\nLet us now turn to the structural descriptions produced through\nsyntactic analysis. One of the most commonly used representa-\ntions of syntactic structure is the tree. The children of a node in a\ntree are also trees by deﬁnition. Thus trees show the nested, part-\nwhole, or hierarchical relationships between diﬀerent constituents.\nTrees used in NLP and linguistics are ordered trees - the linear\norder of the nodes in the tree is signiﬁcant. Trees thus show lin-\near as well as hierarchical structure of sentences. Many grammar\nformalisms use tree structures to depict the syntactic structure of\nsentences. A tree structure produced by a syntactic analyzer, also\ncalled a parser, is called a parse tree. Here is an example:\n2.2. COMPUTATIONAL LINGUISTICS\n181\nNP\nNBAR\ndet\nadj\nNBAR\nadj\nNBAR\n(a)\n(big)\n(black)\nn\n(elephant)\nFIG 2.8 A Typical Parse Tree\nIn many languages, the linear ordering of nodes is sometimes\nsigniﬁcant and sometimes insigniﬁcant. These languages are called\nrelatively free word order languages. Indian languages belong to\nthis type.\nIn a Telugu sentence, for example, all possible per-\nmutations of nouns are syntactically valid but typically the verb\ncomes at the right end.\nAlso while the nouns in a clause can\ncome in any order, all of them must necessarily come before the\nnouns in the following clause. Linear order is important in some\ncases and insigniﬁcant in others. Trees are either ordered trees\nor unordered trees. Interpreting trees as ordered in some places\nand unordered elsewhere is not easy. Trees originate from phrase",
    "nouns in the following clause. Linear order is important in some\ncases and insigniﬁcant in others. Trees are either ordered trees\nor unordered trees. Interpreting trees as ordered in some places\nand unordered elsewhere is not easy. Trees originate from phrase\nstructure rules which imply strict order or constituents. Phrase\nstructure rules and the tree structures they produce are not always\nthe best choices.\nApart from depicting the linear and hierarchical structure\nof sentences, a syntactic analyzer must also determine the roles\nplayed by the various constituents in the sentence.\nThe func-\ntional structure of a sentence depicts the assignment of functional\nor thematic roles to the various constituents in the sentence. Role\nassignment depends on functional structure constraints such as\nlinear position, morphological inﬂections, agreement, subcatego-\nrization and selectional restrictions. A noun phrase may be the\nsubject of one clause and the object of another embedded clause.\n182\nCHAPTER 2. FOUNDATIONS OF NLP\nTrees cannot depict functional dependencies. So trees are often\nannotated and augmented with special links connecting various\nnodes in order to indicate functional dependencies. The result-\ning structures are really no longer trees but much more complex\nstructures such as graphs.\nTrees have several other demerits. A parse tree is a monolithic\nstructure that includes units at diﬀerent levels. A tree is a mess\nof words, phrases, clauses and the entire sentence. Consequently,\nparts which logically form one group are thrown far apart. All\nthe problems of long distance dependencies originate from this.\nAlso, trees tend to become very large and unwieldy for long and\ncomplex sentences. Every elementary subtree, that is, a subtree\nthat includes just one node and its children, corresponds to one\napplication of a phrase structure rule. Thus trees are closer to\nphrase structure rules than to the structure of the sentences. We",
    "complex sentences. Every elementary subtree, that is, a subtree\nthat includes just one node and its children, corresponds to one\napplication of a phrase structure rule. Thus trees are closer to\nphrase structure rules than to the structure of the sentences. We\nneed structural descriptions which separate out and vividly show\nlinear, hierarchical as well as functional structure inherent in the\ngiven sentence.\nTrees are not the most suitable structures for\ndepicting the structure of natural language sentences, although\nthey are widely used.\nWe will now brieﬂy sketch the merits and demerits of exist-\ning grammar formalisms. Linguistic grammar formalisms can be\nviewed as extensions of basic phrase structure grammars, more\nspeciﬁcally, Context Free Grammars (CFG). Let us ﬁrst under-\nstand the strengths and weaknesses of Context Free Grammars.\nThis would help us in understanding other grammar formalisms\nclearly.\nContext Free Grammars and Natural Languages\nIn the 1950s, there were major developments taking place in the\nﬁeld of computer science, including the development of high level\nprogramming languages. Grammars and parsers needed for com-\npiling these programs received a great deal of importance. In his\npapers in 1956 and 1959, Noam Chomsky laid out the foundations\nof formal properties of grammars. Chomsky classiﬁed grammars\nbased on string rewriting rules into four classes of formal com-\nplexity called Type 0 or Unrestricted Phrase Structure Grammar,\nType 1 or Context Sensitive Grammar, Type 2 or Context Free\nGrammar and Type 3 or Regular Grammar. Context Free Gram-\n2.2. COMPUTATIONAL LINGUISTICS\n183\nmars (CFG), the second least powerful in the Chomsky Hierarchy,\nwere found to be ideal for dealing with programming languages\neﬃciently. Since then a great deal of formal studies have been\nmade on CFGs within computer science and very eﬃcient pars-\ning algorithms have been developed. Linguists often fail to make",
    "were found to be ideal for dealing with programming languages\neﬃciently. Since then a great deal of formal studies have been\nmade on CFGs within computer science and very eﬃcient pars-\ning algorithms have been developed. Linguists often fail to make\nclear distinctions between these diﬀerent formal types of gram-\nmars and simply use the term “phrase structure” rules. What\nthey most often mean are the Context Free grammar rules. See\nbooks on Theory of Computation for more on types of grammars\nand their formal properties.\nContext Free Grammars:\nFormally, a language is nothing but a set of strings built from\na given set of symbols.\nIf the symbols are words, then a lan-\nguage could be viewed as sequences of words, namely sentences.\nA grammar speciﬁes which sequences of words are valid or invalid.\nIn other words, a grammar is nothing but a speciﬁcation of the\nmembership function for the set called language.\nA Context Free Grammar is a 4-tuple (N,T,P,S) where\nN is a ﬁnite set of non-terminal symbols, T is a ﬁnite set\nof terminal symbols, disjoint from N, S is a special, des-\nignated symbol from N called the Start Symbol, and P is\na ﬁnite set of Production Rules (or Productions), of the\nform A →α\nwhere A is any non-terminal symbol and α is any sequence\nof terminal and non-terminal symbols. Unless the language itself\ncontains the empty string, α can be required to be non-empty.\nTerminal symbols are the symbols that actually occur in the lan-\nguage and the non-terminal symbols are used for the purpose of\ndeﬁning the structure of the language. Here is an example gram-\nmar G:\n184\nCHAPTER 2. FOUNDATIONS OF NLP\nNP -> det NBAR\nNP -> NBAR\nNBAR -> adj NBAR\nNBAR -> n\nHere NP and NBAR are non-terminal symbols and det, adj\nand n are terminal symbols. The string det adj adj n can be gen-\nerated from this grammar. The set of all possible strings that can\nbe generated from this grammar G is the language L(G) accepted\nby this grammar. Note that the language accepted by a CFG can",
    "and n are terminal symbols. The string det adj adj n can be gen-\nerated from this grammar. The set of all possible strings that can\nbe generated from this grammar G is the language L(G) accepted\nby this grammar. Note that the language accepted by a CFG can\nbe ﬁnite on inﬁnite.\nWe could have added rules like\ndet -> the\ndet -> a\ndet -> an\nadj -> big\nadj -> small\nadj -> black\nadj -> white\nn -> cat\nn -> dog\nn -> elephant\nto directly generate sentences such as the big black elephant\nor a small cat or dog. However, it is conventional to stop at pre-\nterminal level and insert actual words through a lexical insertion\nprocess using a dictionary. This makes the grammars so much\nsmaller.\nParsing:\nThe process of analyzing the structure of a given sentence us-\ning a given grammar is called parsing. Parsing can be done either\ntop-down or bottom-up. In top-down parsing, we start from the\nstart symbol S and repeatedly substitute non-terminal symbols by\nthe corresponding sequences on the right hand side of matching\nrules until the given sentence is generated. Note that there may\n2.2. COMPUTATIONAL LINGUISTICS\n185\nbe several rules for each non-terminal symbol including S. Thus\nparsing using CFGs is essentially a non-deterministic process. In\nthe bottom-up approach, we start from the given sentence and\nrepeatedly substitute substrings with the corresponding left hand\nsides of the matching rules until the start symbol S is derived.\nNote that there may be many substrings that match the right\nhand sides of rules. Top-down or bottom-up, parsing using CFGs\nis inherently non-deterministic in nature. There are also parsing\ntechniques such as chart parsing where the top-down and bottom-\nup parsing strategies are combined for practical advantage. What-\never the case, parsing in CFGs has an asymptotic worst case time\ncomplexity of O(n3) in general. Deterministic subclasses of CFGs\nexist in some cases for which the time complexity will be linear.",
    "up parsing strategies are combined for practical advantage. What-\never the case, parsing in CFGs has an asymptotic worst case time\ncomplexity of O(n3) in general. Deterministic subclasses of CFGs\nexist in some cases for which the time complexity will be linear.\nSuch special subclasses have been exploited for building grammars\nfor programming languages but they cannot be extended easily to\nnatural languages.\nPhrase structure grammars are string re-writing grammars.\nWe start with a string and repeatedly rewrite the string until we\nderive the string we are interested in. Each application of a rule\nto rewrite a string is called a step of derivation. The parse tree\nshows the end result of this process of derivation. The parse tree\nfor the sentence a big black elephant is shown in ﬁgure 2.8 above.\nNote that the root of a parse tree always corresponds to the start\nsymbol, the intermediate nodes correspond to non-terminal sym-\nbols, the leaf nodes correspond to the terminal symbols and any\nnode with its child nodes constitutes one application of a rule.\nThe node corresponds to the left hand side of the rule and the\nchildren, read left to right, correspond to the right hand side of\nthe rule.\nStrengths and Limitations of CFGs:\nThe merits and demerits of CFGs for dealing with human lan-\nguages are now fairly well understood and it is generally accepted\nthat CFGs do not form a good model for natural languages. Yet\nthe study of CFGs is signiﬁcant in NLP for four reasons. Firstly,\nsigniﬁcant portions of natural languages are context free although\nthe whole may not be. Since our aim is to develop a grammar\nformalism of the least powerful type, it is important that we\nunderstand the strengths and weaknesses of CFGs. We should\n186\nCHAPTER 2. FOUNDATIONS OF NLP\nnot employ more powerful grammars where CFGs suﬃce and we\nshould clearly understand the reasons for using more powerful\ngrammars where really required. Secondly, most of the linguistic",
    "understand the strengths and weaknesses of CFGs. We should\n186\nCHAPTER 2. FOUNDATIONS OF NLP\nnot employ more powerful grammars where CFGs suﬃce and we\nshould clearly understand the reasons for using more powerful\ngrammars where really required. Secondly, most of the linguistic\ngrammar formalisms also incorporate CFGs in some form or the\nother, at some stage or the other. Understanding CFGs helps us\nin understanding these grammar formalisms. Thirdly, some of the\nimportant syntactic phenomena like long distance dependencies,\ncross serial dependencies and parsing of relatively free word order\nlanguages are also directly related to the powers and limitations of\nphrase structure rules in general and CFGs in particular. Lastly,\ndevelopment of other grammar formalisms can be fruitfully viewed\nas attempts to overcome the weaknesses of CFGs and various ways\nof reacting to their limitations. Keeping these points in mind, we\nﬁrst take up a detailed study of CFGs for natural languages.\nThere is a direct relationship between the generative capacity\n- the kinds of sentence structures which a grammar can generate or\nparse, and computational complexity. The most general grammar\ntakes the largest amount of computational resources (processing\ntime and/or memory space), and the least general grammar can\nbe parsed with the least computing resources. It is important to\nnote that these diﬀerences in computational complexity are not\nsmall diﬀerences. They are orders of magnitude diﬀerences. It\nhas been natural in NLP, therefore, to look for the least powerful\ngrammar that is suﬃcient for dealing with natural language sen-\ntences. Using more powerful grammars than required is wasteful,\nineﬃcient, unintelligent and thus unacceptable by both theoretical\nand practical considerations.\nWe might therefore ask ourselves whether regular grammars,\nthe simplest type of grammars within the Chomsky Hierarchy,\ncan be used to describe human languages. Regular grammars can",
    "ineﬃcient, unintelligent and thus unacceptable by both theoretical\nand practical considerations.\nWe might therefore ask ourselves whether regular grammars,\nthe simplest type of grammars within the Chomsky Hierarchy,\ncan be used to describe human languages. Regular grammars can\nhandle all ﬁnite languages and also some inﬁnite languages charac-\nterized by repeated substrings. Natural languages are commonly\ntaken to be inﬁnite, they involve arbitrarily deeply nested struc-\ntures, not mere repetition of subparts. Regular grammars are not\nsuﬃcient to capture the whole of natural languages.\nA formal\nproof exists and is based on the fact regular sets are closed under\nintersection. We will omit the proof here and take it for granted\nthat regular grammars are insuﬃcient for natural languages.\nThe next question would therefore naturally be whether con-\ntext free grammars can be used.\nThere are deterministic sub-\n2.2. COMPUTATIONAL LINGUISTICS\n187\nclasses of CFGs which can be parsed in deterministic linear time.\nEven parsers based on general CFGs have a worst case time com-\nplexity of O(n3) where ‘n’ is the number of words in a sentence.\nThus parsing with CFGs can be quite eﬃcient.\nIn the sixties,\nCFGs were actually widely applied to the study of natural lan-\nguages. Context-free grammars were very popular as models for\nnatural language in spite of formal inadequacies of the model for\nhandling some of the features that occur in natural languages.\nThe question of context freeness of natural languages has received\na lot of attention. There have been many papers arguing for or\nagainst context freeness of natural languages. See the book en-\ntitled “The Formal Complexity of Natural Language” for exam-\nple. As the editors of the book rightly note in the beginning of\npart two of their book, almost all the arguments given to demon-\nstrate the non-context freeness of natural languages have hinged\naround peripheral, rare and certainly not the run-of-the-mill lan-",
    "ple. As the editors of the book rightly note in the beginning of\npart two of their book, almost all the arguments given to demon-\nstrate the non-context freeness of natural languages have hinged\naround peripheral, rare and certainly not the run-of-the-mill lan-\nguage phenomena. Also, as Gazdar and Pullum have pointed out\nin their paper in that book, many of these arguments are neither\ntechnically nor empirically sound. They contend that nobody has\nyet given a real proof that natural languages are not context free.\nWhether natural languages are fully context free or not, is\nnot really very important. The point of importance is, all these\narguments and counter-arguments notwithstanding, natural lan-\nguages are largely context free. We may recall that computer pro-\ngramming languages also have some features that are certainly\nnot context free and yet context free grammars have been applied\nvery fruitfully to deal with these languages very eﬃciently. We\nneed to clearly understand which aspects of the syntax of natural\nlanguages are really context free and which aspects are not. We\nwill then be able to apply more powerful grammars only where\nthey are really required. Parsing with CFGs will be very much\nfaster than parsing with more complex grammars. CFGs have to\nbe used for whatever aspects they are necessary and suﬃcient.\nFrom the history of the theories and models of natural lan-\nguage syntax, it appears that as soon as the limitations of a par-\nticular type of grammar are realized, researchers tend to jump\nto the next higher level of complexity.\nCFGs were insuﬃcient\nand hence computer scientists developed ATN grammars. At the\nsame time linguists developed the Transformational Grammars.\nBoth of these have Type 0 power - much more than needed and\n188\nCHAPTER 2. FOUNDATIONS OF NLP\nmuch more than can be computationally handled eﬃciently in\npractice. A much better strategy to use, perhaps is to think of an\nappropriate way of breaking the problem into subproblems and",
    "Both of these have Type 0 power - much more than needed and\n188\nCHAPTER 2. FOUNDATIONS OF NLP\nmuch more than can be computationally handled eﬃciently in\npractice. A much better strategy to use, perhaps is to think of an\nappropriate way of breaking the problem into subproblems and\nemploying the least powerful grammar that is essential for each.\nNatural languages are largely context free. In fact, there are as-\npects of natural language syntax that do not even require context\nfree power, regular grammars are suﬃcient. Diﬀerent strokes for\ndiﬀerent folks is the right approach.\nIn a formal sense CFGs have suﬃcient weak generative capac-\nity to deal with almost all aspects of natural languages. Natural\nlanguages are largely context free. In a practical sense, however,\nCFGs have several disadvantages. Let us now look at speciﬁc syn-\ntactic phenomena to highlight these limitations of CFGs.\ni) Functional Dependencies:\nLanguages enforce certain dependencies between the consti-\ntuents in a sentence. Examples of this are grammatical agreement\nrequirements and selectional restrictions. Thus 4) is grammatical\nbut 5) is not, 6) is grammatical while 7) is not.\n4)\nThe students write the test\n5)\n*The student write the test\n6)\nThe student read the paper\n7)\n*The paper read the student\nIf we have to enforce these dependencies using (only) CFGs,\nwe will be faced with two problems.\nFor one thing, we will\nbe forced to introduce new categories since CFGs, like all other\nphrase structure rules, can only generate strings of terminal sym-\nbols.\n(Note that we normally take grammatical category sym-\nbols as the terminal symbols when we write grammar rules, as-\nsuming that words are inserted by a lexical insertion process.)\nThere would be singular-nouns and plural-nouns, singular-verbs\nand plural-verbs. There will be nouns that denote things that can\nread and nouns that denote things which can be read. This would\ncause an explosion of rules leading to very large rule sets. There",
    "There would be singular-nouns and plural-nouns, singular-verbs\nand plural-verbs. There will be nouns that denote things that can\nread and nouns that denote things which can be read. This would\ncause an explosion of rules leading to very large rule sets. There\nwould be separate rules for singular sentences and plural sen-\ntences. Computationally the syntactic system becomes extremely\nineﬃcient. Secondly, and more importantly, a simple requirement\nlike agreement between subject and verb, which can be given in\none simple statement in English, is being turned into a multitude\n2.2. COMPUTATIONAL LINGUISTICS\n189\nof otherwise unrelated categories and rules. Simple generalizations\nare lost.\nA word should be associated with a particular category purely\nbased on its own intrinsic properties. A grammatical relation be-\ntween two independent words is an intrinsic property of neither\nof these words. The phrase structure rules have no direct way of\nspecifying these relations. Thus CFGs, why, all types of phrase\nstructure rules for that matter, are unsuitable for dealing with\ndependencies between diﬀerent constituents in a sentence. Also,\nthe structural descriptions which CFG rules naturally generate,\nnamely trees, cannot depict functional dependencies directly. It\nshould be emphasized that we are not asserting that CFGs are\ninsuﬃcient. CFGs do have the necessary generative capacity to\ngenerate valid and only valid strings as far as such dependencies\nare concerned.\nBut that is simply not the right way of doing\nthings. Functional dependencies must be separately and explic-\nitly speciﬁed in the grammar and clearly depicted in the structural\ndescriptions. CFGs and trees are not the best way to do this.\nii) Relatively Free Word Order Languages\nLinear order of words and phrases in a sentence may be signiﬁ-\ncant - changing the order may render the sentence ungrammatical\nor anomalous.I drank ﬁlter coﬀee is not the same as I drank cof-",
    "descriptions. CFGs and trees are not the best way to do this.\nii) Relatively Free Word Order Languages\nLinear order of words and phrases in a sentence may be signiﬁ-\ncant - changing the order may render the sentence ungrammatical\nor anomalous.I drank ﬁlter coﬀee is not the same as I drank cof-\nfee ﬁlter. Grammars must therefore impose constraints on linear\npositions as required in a given language.\nA sentence is a sequence of words in English and it looks\nalmost unimaginable to view a sentence as an unordered set of\nwords. However, there are languages of the world where order of\nwords is the least important aspect of structure. All permutations\nof words in a Sanskrit sentence are grammatically valid and mean\nexactly the same thing. A sentence could be viewed as a set of\nwords, not really a sequence. It is worth noting that English was\nalso a relatively free word order language at one point of time.\nThere are also a number of human languages where there is\nconsiderable, though not unlimited scope for changing the order of\nwords in a sentence without signiﬁcantly altering its basic meaning\n(to be more precise, without altering the functional structure of\nthe sentence). Modern Indian languages are examples of this. All\nof the following Telugu sentences mean essentially the same thing\n190\nCHAPTER 2. FOUNDATIONS OF NLP\n8)\nraamuDu\nbaDiki\nsiitatoo\nveLLaaDu\nRama\nschool-to\nSita-with\nwent\n9)\nbaDiki\nraamuDu\nsiitatoo\nveLLaaDu\nschool-to\nRama\nSita-with\nwent\n10)\nsiitatoo\nraamuDu\nbaDiki\nveLLaaDu\nSita-with\nRama\nschool-to\nwent\nThe strong point of CFG is that it can eﬀectively deal with\nboth the linear and hierarchical structure inherent in human lan-\nguages. This ability to deal with linear structure comes back as a\nweakness when we have to parse sentences in relatively free word\norder languages. Here a variety of word orderings are allowed and\nlinear order is not to be considered signiﬁcant. CFGs however,\nhave no way of getting rid of their hold on linear order. A CFG\nrule such as",
    "weakness when we have to parse sentences in relatively free word\norder languages. Here a variety of word orderings are allowed and\nlinear order is not to be considered signiﬁcant. CFGs however,\nhave no way of getting rid of their hold on linear order. A CFG\nrule such as\ns →np1\nnp2\nvp\nasserts two independent things. Firstly it asserts that s has\nthree constituents named np1, np2 and vp. Secondly, it also as-\nserts, unfortunately, that np1, np2 and vp have to come in that\nlinear order. A CFG rule cannot assert the hierarchical structure\nalone, without implying the order of constituents. Thus the above\nrule is completely diﬀerent from\ns →np2\nnp1\nvp\nSince both of these orders can be equally valid in some lan-\nguages, we must have both the rules in the grammatical system.\nFor rules with more number of symbols in the right hand side,\nmany permutations may be possible and we need to include one\nrule for each possibility. We are in the unhappy state of being\n2.2. COMPUTATIONAL LINGUISTICS\n191\nforced to take something irrelevant seriously. Having one rule for\neach possible permutation is like imposing a serious restriction\nand then having one case for each way of relaxing that restric-\ntion. Context free grammars are not the ideal for all languages.\nWe multiply the number of rules thereby making our parser very\nineﬃcient. Note again that CFGs are suﬃcient in the formal tech-\nnical sense of generative capacity but practically just unusable.\niii) Long Distance Dependencies and Movement:\nLinear order of words and phrases in a sentence may be signiﬁ-\ncant - changing the order may render the sentence ungrammatical\nor anomalous. There are valid syntactic constructions, however,\nwhere there are deviations from the usual order. Therefore these\nformalisms based on CFGs need to posit a normal or unmarked\norder and view syntactic constructs which deviate from the nor-\nmal order as involving movement. The grammar is expected to",
    "where there are deviations from the usual order. Therefore these\nformalisms based on CFGs need to posit a normal or unmarked\norder and view syntactic constructs which deviate from the nor-\nmal order as involving movement. The grammar is expected to\nspecify what constituents move from which place to which place\nand under what conditions.\nSome movements are local or bounded, as in the case of aux-\niliary shift in simple yes-no questions (example 11) and subject-\nobject inversion in passives (example 12).\n11)\nCan the company sell Pentiums now?\n12)\nMy Pentium was replaced by someone\nOther types of movements take place across arbitrarily long\ndistances and are termed long distance dependencies.\nRelative\nclauses (examples 13 and 14) and wh-questions (examples 15, 16,\nand 17) in English are examples. In these constructions a con-\nstituent from within a clause will have been removed from its\nnormal position and moved to a diﬀerent position. Long distance\ndependencies have been one of the serious problems for syntactic\nsystems and various grammar formalisms have developed special\ntechniques for dealing with them.\n13)\nThe machine which the company sold\nto me was de-\nfective\n14)\nThe machine which the company, which I believe every-\none trusts,\nsold\nto me was defective\n15)\nWhich computers does the company sell\n?\n16)\nWhich computers does the company which I believe\n192\nCHAPTER 2. FOUNDATIONS OF NLP\neveryone trusts\nsell\n?\n17)\nWhich computers do you think the company which I\nbelieve everyone\ntrusts sells\n?\nHistorically, the concepts of movement and other kinds of\ntransformations were introduced in order to account for the fact\nthat sentences superﬁcially looking diﬀerent often have roughly\nthe same meaning. So a deep structure, something closer to mean-\ning than the surface structure of a sentence, was taken as the\nstarting point and other related structures were obtained through\ntransformations.\nIt was soon shown that many of the suppos-",
    "the same meaning. So a deep structure, something closer to mean-\ning than the surface structure of a sentence, was taken as the\nstarting point and other related structures were obtained through\ntransformations.\nIt was soon shown that many of the suppos-\nedly meaning preserving transformations did in fact risk meaning\nchanges and linguists had to give up the idea of linking deep struc-\nture to meaning. Linguists forgot the meaning aspect but stuck\nto the notions of deep and surface structures, now renamed D-\nStructure and S-Structure, and movement continued to be the\nprimary mechanism of going from the D structure to the S struc-\nture. In the more recent minimalistic approach, the concepts of\ndeep and surface structures have been completely abandoned with.\nThe notions of movement and other transformations are direct\nconsequences of the employment of phrase structure rules and the\ncorresponding ordered trees for describing the syntax of natural\nlanguages. As we have already seen, a tree is a mess of diﬀerent\nlevels of meaning units - words, phrases, clauses and sentences.\nIn terms of linear position, the parts of a single meaning unit can\nbe widely spaced apart. Long distance dependencies are only one\ntype of functional dependencies and CFGs cannot eﬀectively deal\nwith any kind of dependency between two constituents.\nIf we could view sentences as sets, rather than as sequences\nof words (or phrases or clauses), there would be no question of\nany distance, long or short.\nMeaning of sentences is not best\ndescribed in terms of length or distance, whether expressed in\nterms of number of intervening structures, words or in inches or\nmeters. It looks like we are trying to solve non-existing problems.\nWestern grammar formalisms have been strongly inﬂuenced\nby languages like English where linear order constraints are very\nsigniﬁcant. Linear order has become such an all important and\ndominating core aspect of western linguistic models that these",
    "Western grammar formalisms have been strongly inﬂuenced\nby languages like English where linear order constraints are very\nsigniﬁcant. Linear order has become such an all important and\ndominating core aspect of western linguistic models that these\nmodels have to bring in highly unnatural and unnecessary com-\n2.2. COMPUTATIONAL LINGUISTICS\n193\nplications to deal with languages where word order is least impor-\ntant. If you want to work with Indian languages, the ﬁrst thing\nyou must learn to unlearn is this obsession with phrase structure\nrules and tree structures. Forget about word order. Forget about\nmovement. Forget about distance. Forget trees.\niv) Cross Serial Dependencies:\nCFGs can handle constituents that come linearly one after the\nother as well as constituents that are properly nested one inside\nthe other. By proper nesting we mean that an inner constituent\nmust end before the outer constituent closes. That is a last-in,\nﬁrst-out property must be satisﬁed.\nIn English, we ﬁnd some\nexamples of cross serial dependencies, as illustrated by 18)\n18)\nManmohan\nSingh, A P J\nAbdul\nKalam, and\nRa-\njasekhara Reddy\nare the Prime minister of India, the Pres-\nident of India and\nthe Chief Minister of Andhra Pradesh\nrespectively.\nThe ‘respectively’ construct ‘A,\nB,\nC\nare\nD,\nE,\nF\nrespectively’ links A to D,\nB to E\nand\nC to F\nin a cross\nserial manner:\nrespectively\nA\nB\nC\nare\nD\nE\nF\nCross serial dependencies occur in programming languages\ntoo. One example is in the requirement that the arguments must\nmatch one-to-one in a function deﬁnition and function call. Com-\npilers simply ignore this aspect while doing the syntactic analysis\nof the source program and postpone the checking to a later phase.\nWe could do the same thing for constructs like ‘respectively’ in\nEnglish by simply taking ‘A,B and C’ and ‘D, E and F’ to be\nsingle composite constituents. It would be all right to postpone a\nfew complex aspects to a later stage of analysis. The only thing",
    "We could do the same thing for constructs like ‘respectively’ in\nEnglish by simply taking ‘A,B and C’ and ‘D, E and F’ to be\nsingle composite constituents. It would be all right to postpone a\nfew complex aspects to a later stage of analysis. The only thing\na CFG grammar will not be able to do is to identify the links be-\ntween the items in the two sets, rest of syntactic analysis can go\non without any problem. The ’respectively’ construct is perhaps\nthe only example of cross-serial nesting in English. This is not a\nmajor problem.\n194\nCHAPTER 2. FOUNDATIONS OF NLP\nThere are languages such as Dutch, however, where we come\nacross an inﬁnite set of grammatically correct sentences with cross\nserial nesting. A way of directly dealing with them would therefore\nbe preferable. Let us look at some examples from Dutch. Both the\nexamples and the discussion that follows are based on Bresnan-\net-al, 1982.\nthat  Jan   the  children   see−past     swim−inf\n19) ... dat   Jan   de   kinderen   zag            zwemmen\nthat  Jan  saw   the  children  swim\n20) ... dat   Jan   Piet  de   kinderen   zag          helpen      zwemmen\nthat  Jan   Piet  the  children    see−past  help−inf    swim−inf\nthat  Jan  saw   Piet  help  the  children  swim\nthat Jan  Piet  Marie  the  children  see−past  help−inf   make−inf  swim−inf\nthat  Jan  saw   Piet  help  Marie   make   the  children  swim\n21) ... dat  Jan  Piet  Marie   de   kinderen  zag        helpen     laten      zwemmen\nThere are restrictions on cross serial nestings in Dutch. The\nnumber of nps should match the number of verbs. The ﬁrst np\nand the ﬁrst verb must satisfy the basic agreement requirements.\nThe ﬁnal nps and the last verb must satisfy the subcategorization\nrestrictions.\nOnly a verb that is subcategorized for both a np\nand an inﬁnitival complement without the complementizer ‘te’\ncan be inserted in this cross serial fashion. There are only a ﬁnite\nnumber of such insertable verbs but they can be repeatedly used",
    "restrictions.\nOnly a verb that is subcategorized for both a np\nand an inﬁnitival complement without the complementizer ‘te’\ncan be inserted in this cross serial fashion. There are only a ﬁnite\nnumber of such insertable verbs but they can be repeatedly used\n2.2. COMPUTATIONAL LINGUISTICS\n195\nand hence there is no limit on the depth of such a nesting. It is to\nbe noted that within these restrictions, all arbitrary permutations\nof the nps in the np sequence and all arbitrary permutations of the\nverbs within the verb sequence produce grammatical sentences:\n22) ...dat Jan Marie Piet de kinderen zag helpen laten zwem-\nmen\n...that Jan saw Marie help Piet make the children swim\n23) ...dat Jan Marie de kinderen Piet zag helpen laten zwem-\nmen\n...that Jan saw Marie help the children make Piet swim\n24) ...dat Jan Marie de kinderen Piet zag laten helpen zwem-\nmen\n...that Jan saw Marie make the children help Piet swim\nThe point to note is that only the agreement between the ﬁrst\nnp and the ﬁrst verb and the subcategorization restrictions be-\ntween other nps and the ﬁnal verb are to be encoded in the gram-\nmar. Hence it is possible to develop a CFG which can generate all\nthese constructs. However, such a CFG will not assign linguisti-\ncally correct structural descriptions to these sentences. That is, it\nis possible to get a CFG that suﬃcient to handle Dutch in terms\nof its weak generative capacity but it has been shown that Dutch\nis not strongly context free.\nThis time we have a real restric-\ntion, CFGs are not suﬃcient as far as strong generative capacity\nis concerned.\nIf universality in the theoretical sense is not the main con-\ncern and the set of languages we wish to deal with do not show\ncross-serial dependencies in any serious measure, we may still go\nahead and claim that CFGs are good enough in this limited sense\nof generative capacity.\nv) Unbounded Branching\nCFGs capture repetition through recursion. In many situa-\ntions what we want is mere repetition and not recursion.",
    "cross-serial dependencies in any serious measure, we may still go\nahead and claim that CFGs are good enough in this limited sense\nof generative capacity.\nv) Unbounded Branching\nCFGs capture repetition through recursion. In many situa-\ntions what we want is mere repetition and not recursion.\nFor\nexample, items in conjunction should really be all at the same\nlevel. Since there is no a priori bound on the number of items\nin conjunction, either we have to use recursive rules or make the\nnumber of rules potentially inﬁnite. A sequence of a’s can only be\ncaptured by one of the recursive rules\nA →a A\n196\nCHAPTER 2. FOUNDATIONS OF NLP\nor\nA →A a\nalong with the base rule\nA →a\nor by the potentially inﬁnite set of rules\nA →a\nA →a a\nA →a a a\nand so on. Computational grammar formalisms require that\nthe rule sets be ﬁnite and hence in general imposing nonexistent\nhierarchical structure through recursive rules is inevitable. CFG\nrules are suitable for dealing with recursion but not for handling\nrepetition. It is worth noting that regular expressions, equivalent\nin computational complexity to the much simpler regular gram-\nmars, can directly and eﬀectively deal with repetition.\nTowards Better Grammar Formalisms:\nIn summary, CFGs impose more structure than really exists\nin some cases and less structure than required in others. CFGs\nimpose linear structure when not appropriate, as in the case of\nrelatively free word order languages. They impose too much of\nhierarchical structure in cases such as unbounded branching. As\nfar as dependencies between diﬀerent constituents are concerned,\nCFGs are unable to capture the required structural constraints\neﬀectively.\nWhat then is good about CFGs?\nCFGs are excellent for\nhandling situations where both linear and hierarchical aspects of\nstructure are involved and nothing else is important. If hierar-\nchical structure is not involved, we may not even require CFGs\nand if linear structure is itself not signiﬁcant, CFGs are no good.",
    "CFGs are excellent for\nhandling situations where both linear and hierarchical aspects of\nstructure are involved and nothing else is important. If hierar-\nchical structure is not involved, we may not even require CFGs\nand if linear structure is itself not signiﬁcant, CFGs are no good.\nCFGs are not the best in any situation where any kind of direct\ndependency between diﬀerent constituents is involved. It should\nalso be noted that going beyond CFGs and using more powerful\nphrase structure rules is not necessarily going to solve all these\nproblems.\nGiven these strengths and limitations of CFGs, both linguists\nand computer scientists have endeavored to develop better gram-\nmar formalisms for handling natural languages. It would be in-\nstructive to view each of these grammar formalisms in terms how\n2.2. COMPUTATIONAL LINGUISTICS\n197\nexactly they have attempted to overcome the deﬁciencies of CFGs\nand improve further. Despite the fact that several grammar for-\nmalisms have already been developed, the search for better for-\nmalisms continues. Theoretically, the available grammar forma-\nlisms are not fully satisfactory in explaining the human language\nfaculty in all its ramiﬁcations. Computationally, the belief and\nhope that simpler and more eﬃcient techniques exist, provides\nmotivation for further search for better and better formalisms.\nWe will now make a brief survey of several important grammar\nformalisms that have made a signiﬁcant impact on NLP. We start\nwith Augmented Transition Networks (ATN) and give a sample\ngrammar of English, adapted from Terry Winograd. This should\nserve two purposes. Firstly, readers will get an idea about the na-\nture of a descriptive grammar for a language. Secondly, it should\ngive some idea about the broad nature of issues involved in de-\nveloping and using computational grammars.\nA set of sample\nsentences is given in the Appendix and the readers are strongly\nadvised to try out each of these sentences on the grammar given",
    "give some idea about the broad nature of issues involved in de-\nveloping and using computational grammars.\nA set of sample\nsentences is given in the Appendix and the readers are strongly\nadvised to try out each of these sentences on the grammar given\nhere. In the latter part, we shall brieﬂy summarize the salient\nfeatures of several other grammar formalisms including the Lex-\nical Functional Grammar (LFG), Generalized Phrase Structure\nGrammar (GPSG) and the Tree Adjoining Grammar (TAG). Our\naim here is not to provide an exhaustive and in-depth coverage of\ngrammar formalisms. We will be selective and very brief. Inter-\nested readers will ﬁnd a good deal of literature on all aspects of\nsyntax.\nAugmented Transition Network Grammar\nAugmented Transition Networks were developed in the early 1970s\naround the same time that linguistics was also taking a new shape\nwith Noam Chomsky leading the generative linguistics programme\nwith his Transformational Grammar (TG). While the concepts be-\nhind ATN existed for some time before, it was Woods who gave a\nformal shape to ATN. As is clear from his work, ATN came up as\nan alternative to, and improvement over, Chomsky’s TG. While\nclaiming equivalence to TG in power, Woods argued that ATN\noﬀered a number of advantages including perspicuity, suﬃcient\ngenerative power, eﬃciency of representation, ability to capture\nregularities, eﬃciency of operation and ﬂexibility for experimen-\n198\nCHAPTER 2. FOUNDATIONS OF NLP\ntation. Surprisingly, linguists are hardly aware of ATN grammars.\nWe have seen that Context Free grammars are more power-\nful than Regular Grammars. Regular grammars are equivalent\nto Simple Transition Networks (more commonly known as Finite\nState Machines or Finite State Automata).\nCFGs are in fact\nequivalent to Recursive Transition Networks, state transition net-\nworks which can call one another recursively.\nSome of the arc\nlabels can be labels of other networks.\nATNs are obtained by",
    "State Machines or Finite State Automata).\nCFGs are in fact\nequivalent to Recursive Transition Networks, state transition net-\nworks which can call one another recursively.\nSome of the arc\nlabels can be labels of other networks.\nATNs are obtained by\naugmenting RTNs with Initializations, Conditions, and Actions.\nEach time an arc is taken, initializations linked to that arc are per-\nformed ﬁrst. The arc can be taken only if the speciﬁed conditions\nare met and if an arc is taken, the actions speciﬁed are performed.\nA sample ATN grammar for English given below. There are\nbasically three Finite State Networks, one for the sentence level,\none for noun phrases and one for prepositional phrases. Note that\nthese are not the usual Finite State Machines - some of the arcs are\nlabelled with the names of other networks. Thus the networks are\nrecursive. The arcs are associated with initializations, conditions\nand actions, together referred to as augmentation. The S network\nsets the agreement requirements while processing the subject noun\nphrase and cross-validates these while passing through the verb.\nThis is essentially how ATNs take care of functional dependencies.\nThe recursive transition networks, equivalent to CFGs, capture\nthe linear and hierarchical structure.\nThe augmentation deals\nwith the functional dependencies. Study the networks and the\nassociated arcs carefully.\n2.2. COMPUTATIONAL LINGUISTICS\n199\na\n22:NP\n23:PP\nb\n1:NP\n16:S/x\n21:NP\nq\n19:v\nc\n2:v\n3:v\nd\n4:v\n5:NP\ne\n8:JUMP\n17:S/z\n6:NP\n18:S/z\n7:JUMP\n9:PP\n10:PP\n11:PP\nSEND\n12\nr\n24:REL\n25:NP\n20:NP\nx\ny\n13:prep(for)\nz\n14:NP\n15:Comp(to)\nThe start state is ’a’. Subject, Direct Object, Indirect Object,\nMain Verb, Auxiliaries, Modiﬁers and Question Element are the\nRoles. Voice ([Active] / Passive) and Mood ([Declarative] / Inter-\nrogative / Imperative / Relative / wh-Relative) are the Features.\nFIG 2.9 The S Network\n200\nCHAPTER 2. FOUNDATIONS OF NLP\nf\ng\n1:Det\n2:JUMP\nh\n6:Proper\n5:Pr\nSEND\n9\n3:Adj\n4:N\n7:PP\n10:S/c\n11:S/r\n8",
    "Roles. Voice ([Active] / Passive) and Mood ([Declarative] / Inter-\nrogative / Imperative / Relative / wh-Relative) are the Features.\nFIG 2.9 The S Network\n200\nCHAPTER 2. FOUNDATIONS OF NLP\nf\ng\n1:Det\n2:JUMP\nh\n6:Proper\n5:Pr\nSEND\n9\n3:Adj\n4:N\n7:PP\n10:S/c\n11:S/r\n8\nThe start state is ’f’. Determiner, Head, Describers and Qual-\niﬁers are the Roles. Number (Singular / Plural), Person (First /\nSecond / Third) and Question(Yes / [No]) are the Features.\nFIG 2.10 The NP Network\ni\nj\n1:Prep\nSEND\n4\nk\n2:NP\n3\nThe start state is ’i’. Prep. Object and Preposition are the\nRoles.\nFIG 2.11 The PP Network\nThe augmentation on the arcs are given below. Study each\none carefully.\n2.2. COMPUTATIONAL LINGUISTICS\n201\nArc\nAugmentation\nS −1 :a NPb\nC: Question of * is No and Mood ̸= Interrogative\nA: SUBJ = *\nS −2 :b Vc\nC: (Type of * is MODAL) or (Form of * is Past)\nor (Form of * is 3P-SL-Present and No. of SUBJ\n= SL and Person of SUBJ = 3rd) or\n(Form of * is Present and ((No. of SUBJ = PL)\nor (Person of SUBJ = 1st or 2nd)))\nA: Main-Verb = *\nS −3 :c Vc\nC: Type of Main-Verb = BE/DO/HAVE/MODAL\nA: Append Main-Verb to Auxiliaries;\nMain-Verb = *\nS −4 :c Vd\nC: Form of * is Past Participle and Type of\nMain-Verb is BE\nA: Voice = Passive; Append Main-Verb to\nAuxiliaries; Main-Verb = *; OBJ = SUBJ;\nSUBJ = a Dummy NP\nS −5 :c NPd\nA: OBJ = *\nS −6 :d NPe\nA: OBJ2 = OBJ; OBJ = *\nS −9 :e PPe\nA: Append * to Modiﬁers\nS −10 :e PPe\nC: Voice is Passive and SUBJ is a Dummy NP\nand WORD in Prep of * is ‘by’\nA: SUBJ = Prep-OBJ of *\nS −11 :e PPe\nC: WORD in Prep of * is ‘to’ or ‘for’ and\nOBJ2 is EMPTY\nA: OBJ2 = Prep-OBJ of *\nS-12:\nC: OBJ2 non-EMPTY and Transitivity of\neSEND\nMain-Verb is Bitransitive. OR OBJ2 is EMPTY\nand OBJ is non-EMPTY and Transitivity is\nTransitive. OR OBJ and OBJ2 are EMPTY\nand Transitivity is Intransitive\nS-13:\nC: WORD in * is ‘for’\nxPrep(FOR)y\nS −14 :y NPz\nA: SUBJ = *\nS-15:\nC: WORD in * is ‘to’\nzCOMP(to)c\nA: Main-Verb = a Dummy Verb with\nType = MODAL\nS −16 :a S/xb\nA: SUBJ = *\nS −17 :c S/ze",
    "Transitive. OR OBJ and OBJ2 are EMPTY\nand Transitivity is Intransitive\nS-13:\nC: WORD in * is ‘for’\nxPrep(FOR)y\nS −14 :y NPz\nA: SUBJ = *\nS-15:\nC: WORD in * is ‘to’\nzCOMP(to)c\nA: Main-Verb = a Dummy Verb with\nType = MODAL\nS −16 :a S/xb\nA: SUBJ = *\nS −17 :c S/ze\nI: SUBJ = SUBJ of ↑\nA: OBJ = *\nS −18 :d S/ze\nI: SUBJ = OBJ of ↑\nA: OBJ2 = OBJ; OBJ = *\n202\nCHAPTER 2. FOUNDATIONS OF NLP\nS −19 :a Vq\nC: Type of * ̸= Non-Aux and Mood =\nDeclarative or Interrogative\nA: Main-Verb = *; MOOD = Interrogative\nS −20 :q NPc\nC: SAME AS C FOR S-2 ARC\nA: SUBJ = *\nS −21 :a NPb\nC: Question of * is YES and MOOD is\nDeclarative\nA: SUBJ = *; Question-Element = *;\nMOOD = Interrogative\nS −22 :a NPa\nC: Question of * is YES and MOOD is\nDeclarative\nA: Question-Element = *; HOLD = *;\nMOOD = Interrogative\nS −23 :a PPa\nC: Question of Prep-OBJ of * is YES and\nMOOD is Declarative\nA: Question-Element = Prep-OBJ of *;\nHOLD = *; MOOD = Interrogative\nS −25 :r NPb\nA: SUBJ = *\nNP −1 :f DETg\nA: DET = *; No = No of *;\nQuestion = Question of *\nNP −3 :g ADJg\nA: Append * to Describers\nNP −4 :g Nh\nC: No is EMPTY or No = No of *\nA: Head = *; No = No of *\nNP −5 :f Prh\nA: Head = *; No = No of *;\nPerson = Person of *; Question=Question of *\nNP −6 :f Properh\nA: Head = *; No = No of *\nNP −7 :h PPh\nA: Append * to Qualiﬁers\nNP −9 :f SEND\nC: HOLD is an NP\nA: Empty and Return HOLD\nNP −10 :h S/ch\nI: MOOD = Relative; SUBJ = a copy based\non ↑Main-Verb = a Dummy Node with\nWORD = ‘be’\nA: Append * to Qualiﬁers\nNP −11 :h S/rh\nI: MOOD = Wh-Relative; HOLD =\na copy based on ↑\nA: Append * to Qualiﬁers\nPP −1 :i PREPj\nA: Preposition = *\nPP −2 :j NPk\nA: Prep-OBJ = *\nPP −4 :i SEND\nC: HOLD is a PP\nA: Empty and Return HOLD\nTable 2.0 Augmentation of the ATN Networks\n2.2. COMPUTATIONAL LINGUISTICS\n203\nHere a * refers to the most recent node and ↑refers to the\nnode from which a recursive call was made. HOLD is a special\nregister used to hold information for reuse later. HOLD is used\nto handle long distance dependencies. OBJ is the Direct Object",
    "2.2. COMPUTATIONAL LINGUISTICS\n203\nHere a * refers to the most recent node and ↑refers to the\nnode from which a recursive call was made. HOLD is a special\nregister used to hold information for reuse later. HOLD is used\nto handle long distance dependencies. OBJ is the Direct Object\nand OBJ2 is the Indirect Object. The notation S/x is a call to\nthe S network but starting with node x rather than the default\nstart node of the network. SEND arc indicates the termination.\nThe SEND arc enables ﬁnal checks to be made using the same\nstandard augmentation procedures applicable to any of the arcs.\nNotice how subject-verb agreement, dative shift, passive voice,\nrelative clauses, yes-no questions and wh-questions are handled.\nWe will not give detailed descriptions of all the networks and the\naugmentation here. Readers are advised to apply this grammar to\nthe sentences given in the appendix and see how the whole thing\nworks or why exactly it does not work in cases where it does not.\nATN parses a sentence from left to right, simulating a non-\ndeterministic machine either by using backtracking or through\npseudo-parallelism. Nondeterminism appears in NLP because the\ninformation required to take the right decision may not be locally\navailable at that point of time. Hence we will be left with only\ntwo options - keeping track of all the possibilities simultaneously,\nor making an arbitrary guess, proceeding sequentially and back-\ntracking to earlier choice points if forced later on. In both cases\nextra eﬀort is expended in trying unfruitful paths. ATN blindly\nsearches all the possibilities applying the conditions on the arcs\nto prune search. Since actions on one arc may aﬀect the result of\napplying a condition on some other arc later, wide coverage ATN\ngrammars tend to become very complex and diﬃcult to write.\nATN handles long distance dependencies using its hold mech-\nanism. Constituents which are obtained earlier than required are",
    "applying a condition on some other arc later, wide coverage ATN\ngrammars tend to become very complex and diﬃcult to write.\nATN handles long distance dependencies using its hold mech-\nanism. Constituents which are obtained earlier than required are\nheld in the hold register until a place is reached where that con-\nstituent is required but found to be missing. The ATN grammar\nnever made any cognitive claims and was hardly known in lin-\nguistic circles. However, the hold mechanism seems to promise\npsychological reality also.\nATN, like most other grammar formalisms, takes linear posi-\ntion of constituents in a sentence too seriously. This is natural, as\nATNs are only augmentations of Recursive Transition Networks,\nwhich are themselves equivalent to CFGs. The positional aspect is\n204\nCHAPTER 2. FOUNDATIONS OF NLP\nso much a part of the entire mechanism that it is not very suitable\nfor dealing with relatively free word order languages.\nAn ATN grammar can be viewed as a speciﬁcation of a space\nof grammatical possibilities which says very little about how to\nsearch that space. While this division of knowledge into what and\nhow is a boon to the grammar writer, ATN parsers are suscepti-\nble to becoming very ineﬃcient. Unrestricted augmentation can\nmake ATN as powerful as the Turing Machine. In general ATN is\nextremely powerful, much more powerful than needed, thus run-\nning the risk of becoming extremely ineﬃcient. In this respect,\nATN is not any better than TG or other formalisms proposed by\nlinguists.\nWhile several experimental NLP systems have been built us-\ning ATN and the ATN mechanism is still being used eﬀectively to\nsolve limited parsing problems, ATN is no longer a serious can-\ndidate as a good grammar formalism for NLP. We have given a\nsample grammar for English here only to illustrate the nature of\ncomputational grammars and issues in parsing.\nLexical Functional Grammar\nLexical Functional Grammar was developed in the early 1980s by",
    "didate as a good grammar formalism for NLP. We have given a\nsample grammar for English here only to illustrate the nature of\ncomputational grammars and issues in parsing.\nLexical Functional Grammar\nLexical Functional Grammar was developed in the early 1980s by\nKaplan and Bresnan. As the name suggests, LFG attaches a high\ndegree of importance to the lexicon. While being sensitive to com-\nputational viability, LFG took a distinct linguistic ﬂavour and has\nbeen making serious cognitive claims also. Thus LFG has come\nto be considered seriously in many schools of linguistics and, at\nleast on some points, LFG is deemed to oﬀer serious competition\nto the more recent theories within generative linguistics including\nthe Government and Binding theory.\nLFG incorporates two levels of processing.\nTo begin with,\nphrase structure rules (CFGs) are used to obtain a tree structure.\nThe tree structure captures the linear and hierarchical relation-\nships and is called the constituent structure or C-structure. Then\nthe functional structure or F-structure is obtained by unifying\nfeature bundles.\nThere are parallels between LFG and ATN. LFG shares many\nof the merits and demerits of ATN. The CFG rules that gener-\nate the C-Structure of LFG are equivalent to the unaugmented\npart of the ATN. The Functional Descriptions of LFG parallel the\n2.2. COMPUTATIONAL LINGUISTICS\n205\naugmentation in ATN. Thus in an abstract sense LFGs and ATN\nare equivalent and LFG grammars are as powerful as the ATN\ngrammars, just too powerful.\nLFG uses the less natural and less intuitive “double shafted\narrow” mechanism for dealing with long distance dependencies.\nLFG chose this direct linking of functionally dependent constituents\ninstead of step by step linkage through transitive chains of func-\ntional identiﬁcations under the pretext that this may require the\nintroduction of otherwise unmotivated functions at intermediate\nF-Structure levels.\nLFG uses the single operation of uniﬁcation instead of the two",
    "instead of step by step linkage through transitive chains of func-\ntional identiﬁcations under the pretext that this may require the\nintroduction of otherwise unmotivated functions at intermediate\nF-Structure levels.\nLFG uses the single operation of uniﬁcation instead of the two\nseparate operations of setting and checking the values of the fea-\nture dimensions as in ATN. This makes the LFG grammar some-\nwhat easier to write. However, uniﬁcation in LFG implies that\na large number of feature structures are built and then rejected.\nAlso uniﬁcation itself is a costly operation.\nLFG also takes linear position too seriously and thus fails to\ndeal eﬀectively with relatively free word order languages.\nGeneralized Phrase Structure Grammar\nGPSG was designed to be a notational extension of CFG. GPSG\nis thus weakly equivalent to CFG and under many views of strong\nequivalence, it is even strongly equivalent to it. GPSG uses meta\nrules to accomplish many of the things that are handled by trans-\nformations in the TG framework. In GPSG a grammar is deﬁned\nby giving a ﬁnite set of CFG rules and a ﬁnite set of meta rules\nwhich allow you to extend the list of CFG rules by the applica-\ntion of the meta rules. The meta rule concept is a very useful\nnotational device and it can make the life of the grammar writer\nmuch simpler. There are restrictions on the application of meta\nrules and hence the overall system complexity is limited. Never-\ntheless, GPSG is essentially CFG and hence it carries with it all of\nthe merits and demerits of CFGs except for providing notational\nconvenience.\nTree Adjoining Grammar\nTree Adjoining Grammars have been around since the mid 1970s.\nThe signiﬁcance of TAG is that it directly takes up the issue of\n206\nCHAPTER 2. FOUNDATIONS OF NLP\nthe least powerful grammar that is needed to handle natural lan-\nguages. Taking for granted that CFGs are not suﬃcient, the pro-\nponents go on to show that TAG, which is only mildly context",
    "The signiﬁcance of TAG is that it directly takes up the issue of\n206\nCHAPTER 2. FOUNDATIONS OF NLP\nthe least powerful grammar that is needed to handle natural lan-\nguages. Taking for granted that CFGs are not suﬃcient, the pro-\nponents go on to show that TAG, which is only mildly context\nsensitive, is suﬃcient to deal with natural languages. TAG has\nbeen shown to be suﬃcient to handle both subcategorization de-\npendencies and ﬁller-gap dependencies. In fact TAG grammars\ncan also deal with crossed dependencies which CFG cannot han-\ndle. TAGs permit polynomial time parsing with a worst case time\ncomplexity of O(n4), just n times worse than CFGs at their worst.\nWhile still not the ideal, this is a very signiﬁcant improvement over\nthe worst case complexity of TG, ATN, and LFG. While GPSG\nmay claim a worst case time complexity of O(n3) based on its\nequivalence to CFG, the ∥G∥2 term within the constant factor,\nwhere ∥G∥represents the number of grammar rules, can become\nlarge enough to make it worse on the whole.\nIn other grammar formalisms, recursion in the CFG or equiv-\nalent component ﬁrst builds up structures some of which get ﬁl-\ntered out later when dependency constraints are applied. In TAG,\nhowever, dependencies are deﬁned initially on bounded structures\n(trees) and recursion simply preserves them. This leads to perhaps\none of the most signiﬁcant aspects of TAG namely its elegant way\nof handling long distance dependencies. In a sense there are no\nlong distance dependencies at all since all dependencies originate\nwithin bounded structures which only get expanded later.\nTAGs use ordered trees and thus have the same kinds of prob-\nlems as other grammar formalisms we have seen above when it\ncomes to handling free word order languages. With the more re-\ncent work in TAG however, the new quasi-trees are coming closer\nto the kaaraka charts of the paaNinian grammar.\nDependency Grammar\nA phrase structure grammar, for example, is deﬁned in terms of",
    "comes to handling free word order languages. With the more re-\ncent work in TAG however, the new quasi-trees are coming closer\nto the kaaraka charts of the paaNinian grammar.\nDependency Grammar\nA phrase structure grammar, for example, is deﬁned in terms of\nconstituents. The head of a constituent is not linked directly to\nthe other words in a phrase structure constituent. This makes\nthe word order very important. In Indian languages as also in\nlanguages like Italian and Russian, which allow a lot of variation\nin the word order, a phrase structure grammar is not very appro-\npriate. Dependency grammars attempt to get around the word\norder limitations of constituency based grammar formalisms.\n2.2. COMPUTATIONAL LINGUISTICS\n207\nIn a dependency grammar, each word is linked directly to\nother words that it relates to. A link is a directed relation that\nconnects a dependent word with its head. Thus there is no need\nfor any non-terminals.\nThe term “Dependency Grammar” stands for a diverse collec-\ntion of approaches to natural language syntax sharing the follow-\ning fundamental characteristics: The distinction between heads\nand dependents; the immediate modiﬁcation of a head by a depen-\ndent (i.e. without intervening nonterminals like in phrase struc-\nture grammars); and the naming of the relation between a head\nand a dependent. Approaches can be diﬀerentiated mainly ac-\ncording to whether they consider grammatical or semantic rela-\ntions (example: “subject” vs. “Actor” or “agent”), and whether\nthe grammar describes tree-structures or graphs.\nCategorial Grammar\nCategorial grammar is a term used for a family of formalisms\nin natural language syntax motivated by the principle of com-\npositionality and organized according to the view that syntactic\nconstituents should generally combine as functions or according\nto a function-argument relationship.\nCategorial Grammar is a lexical approach in which expres-\nsions are assigned categories that specify how to combine with",
    "positionality and organized according to the view that syntactic\nconstituents should generally combine as functions or according\nto a function-argument relationship.\nCategorial Grammar is a lexical approach in which expres-\nsions are assigned categories that specify how to combine with\nother expressions to create larger expressions. An analysis of an\nexpression proceeds by inference over the categories assigned to\nits individuatable parts, trying to assign a given goal-category to\nthe expression. In the type-logical variant of categorial grammar,\na semantic representation is built compositionally in parallel to\nthe categorial inference.\nA categorial grammar shares some features with the simply -\ntyped lambda calculus. Whereas the lambda calculus has only one\nfunction type A →B, a categorial grammar typically has more.\nFor example, a simple categorial grammar for English might have\ntwo function types\nA/B\nand\nA\\B,\n208\nCHAPTER 2. FOUNDATIONS OF NLP\ndepending on whether the function takes its argument from the\nleft or the right.\nSuch a grammar would have only two rules:\nleft and right function application. Such a grammar might have\nthree basic categories (N,NP, and S), putting count nouns in the\ncategory N, adjectives in the category\nN/N,\ndeterminers in the category\nNP/N,\nnames in the category NP, intransitive verbs in the category\nS\\NP,\nand transitive verbs in the category\n(S\\NP)/NP.\nCategorial grammars of this form (having only function applica-\ntion rules) are equivalent in generative capacity to context-free\ngrammar and are thus often considered inadequate for theories\nof natural language syntax. Unlike CFGs, categorial grammars\nare lexicalized, meaning that only a small number of (mostly\nlanguage-independent) rules are employed, and all other syntactic\nphenomena derive from the lexical entries of speciﬁc words.\nAnother appealing aspect of categorial grammars is that it\nis often easy to assign them a compositional semantics, by ﬁrst",
    "language-independent) rules are employed, and all other syntactic\nphenomena derive from the lexical entries of speciﬁc words.\nAnother appealing aspect of categorial grammars is that it\nis often easy to assign them a compositional semantics, by ﬁrst\nassigning interpretation types to all the basic categories, and then\nassociating all the derived categories with appropriate function\ntypes. The interpretation of any constituent is then simply the\nvalue of a function at an argument.\nWith some modiﬁcations\nto handle intentionality and quantiﬁcation, this approach can be\nused to cover a wide variety of semantic phenomena.\nIndian Theories of Grammar\nExtensive studies of the structure of languages have been made in\nthe Indian tradition. The most famous of them all is the grammar\nsystem given by paaNini more than 2000 years ago. More than\nmerely writing a grammar of Sanskrit, paaNini has given us a\n2.2. COMPUTATIONAL LINGUISTICS\n209\nwhole science of grammar. Here we can only very brieﬂy mention\nsome of the salient features.\nSyntactic analysis mainly concerns itself with the assignment\nof functional roles, here called kaaraka roles, to the various con-\nstituents in a given sentence. The verb has its expectations or\naakaaMksha and the noun phrases have the required properties\nyoogyata to occupy diﬀerent kaaraka roles. Thus this theory is\nsimilar in its spirit to the Case Grammars as given by Fillmore\nand many others who followed in the west. In a way, Fillmore’s\ncase grammar is a rediscovery of what paaNini and other ancient\nIndian grammarians had already said long before.\nParsing can be viewed as satisfying the constraints of aakaaMk-\nsha and yoogyata. In Indian languages information required for\ndoing this is encoded primarily in the surface case markers, here\ncalled vibhakti. In modern Indian languages, vibhakti is indicated\nby morphology as well as post-positions. The concept of vibhakti\ncan be extended to include linear position, so that positional lan-",
    "doing this is encoded primarily in the surface case markers, here\ncalled vibhakti. In modern Indian languages, vibhakti is indicated\nby morphology as well as post-positions. The concept of vibhakti\ncan be extended to include linear position, so that positional lan-\nguages such as English can also be handled. sannidhi or occurring\ntogether, is also a useful feature.\nIn Sanskrit as also in modern Indian languages grammar is\nmuch more to do with morphology than with syntax. Word or-\nder is not the most important issue and words carry substantial\namounts of morpho-syntactic information with them. Syntax be-\ncomes so much simpler.\npaaNini’s grammar of Sanskrit is so detailed, precise and me-\nthodical that it is almost like an algorithm.\nThe system is so\nperfect, it is extremely diﬃcult to ﬁnd a fault. Everything just\nworks. Perhaps there is no other system anywhere in the world\nthat can match this level of completeness and perfection.\nUCSG\nUCSG (Universal Clause Structure Grammar) was developed by\nthe author during the early nineties. UCSG uses a divide and\nconquer strategy. The grammar is divided into three nearly in-\ndependent modules. We apply the least complex grammars that\nare suﬃcient for each of the modules thereby leading to very ef-\nﬁcient parsing. Also, grammars for the modules can be written\nseparately and grammars become easy to write. By appropriately\ndividing the problem into modules, we also get to see where ex-\n210\nCHAPTER 2. FOUNDATIONS OF NLP\nactly relatively free word order languages diﬀer from positional\nlanguages and what is really common between the two classes.\nThis would enable development of syntactic systems that work\nboth for positional languages such as English and relatively free\nword order languages exempliﬁed by modern Indian languages.\nGrammars can be viewed as sets of constraints on the struc-\nture of sentences. There are constraints on linear position of con-\nstituents, hierarchical nesting of constituents one inside the other",
    "word order languages exempliﬁed by modern Indian languages.\nGrammars can be viewed as sets of constraints on the struc-\nture of sentences. There are constraints on linear position of con-\nstituents, hierarchical nesting of constituents one inside the other\nand functional dependencies between diﬀerent constituents in a\nsentence. We ﬁnd that these three primary kinds of structure in-\nherent in human languages, namely linear, hierarchical and func-\ntional structures, lend themselves naturally for analysis by three\nindependent modules. The three modules in UCSG are called lin-\near (L), hierarchical (H) and functional (F) components. Both\nthe grammar and the parser are divided into L, H and F parts.\nCorrespondingly, there will be three levels of syntactic representa-\ntion called L-Structure, H-Structure and F-Structure. While other\ngrammar formalisms also tackle linear, hierarchical and functional\naspects some way or the other, the division of labour into these\nthree separate modules, especially the introduction of an indepen-\ndent H level is the highlight of the UCSG formalism.\nHierarchical Structure Analyser\nLinear Structure Analyser\nFunctional Structure Analyser\nH Grammar\nF Grammar\nF-Structure\nL Grammar\nInitialised   Chart\nL-Structure\nH-Structure\nSentence\nFIG 2.12 the UCSG Architecture\nF-Structure depicts the assignment of functional roles to the\nvarious participants in a clause. The F part of the grammar pro-\n2.2. COMPUTATIONAL LINGUISTICS\n211\nvides constraints for role assignment. Functional roles should be so\nselected that role assignments can be made based mainly on syn-\ntactic information. We propose a set of functional roles motivated\nby the question-answer point of view. We employ a combination\nof top-down and bottom-up strategies for role assignment. The\nsubcategorization frames associated with verbs provide the top-\ndown expectations and the surface case markers associated with\nthe nouns provide the bottom-up information. This should re-",
    "of top-down and bottom-up strategies for role assignment. The\nsubcategorization frames associated with verbs provide the top-\ndown expectations and the surface case markers associated with\nthe nouns provide the bottom-up information. This should re-\nmind you of our discussions on paaNini’s grammar system above.\nAny set of role assignments that satisfy all the F constraints si-\nmultaneously gives a valid F-Structure.\nEach clause in a sentence corresponds to one atomic predi-\ncation. It is a general principle of syntax that participants of a\nclause do not cross the clause boundaries except in so far as they\nare speciﬁcally licensed by syntax.\nKnowing the clause struc-\nture therefore greatly helps the functional structure analysis since\nfunctional structure analysis can then be localized to the respec-\ntive clauses. The F part of the grammar can be speciﬁed for a\nsingle clause only and the F grammar becomes simpler and eas-\nier to write. Functional role assignment becomes essentially local\nto a clause.\nIn UCSG we obtain the hierarchical structure of\nclauses and the clause boundaries before we attempt functional\nstructure analysis.\nWe show that clause structure can be de-\ntermined before and without applying the functional structure\nconstraints. H-Structure analysis logically precedes F-Structure\nanalysis in UCSG. An independent H level analysis that is done\nbefore and without the aid of F level constraints sets UCSG apart\nfrom all other grammar formalisms.\nWe observe that there are strong constraints on the sequences\nof verbs and certain clause boundary markers called sentinels. We\nexploit these constraints to identify clauses, to determine the hier-\narchical nesting of clauses and even to determine the clause bound-\naries, although only partially. H-Structure depicts this hierarchi-\ncal structure of clauses. The H grammars for diﬀerent languages\nshow only parametric variations even across positional and rela-",
    "archical nesting of clauses and even to determine the clause bound-\naries, although only partially. H-Structure depicts this hierarchi-\ncal structure of clauses. The H grammars for diﬀerent languages\nshow only parametric variations even across positional and rela-\ntively free word order languages. The grammar of clause structure\nis highly language invariant. Hence the name Universal Clause\nStructure Grammar.\nFurther, we observe that the atomic units at both hierarchi-\ncal structure and functional structure level are groups of words\n212\nCHAPTER 2. FOUNDATIONS OF NLP\n(chunks) rather than individual words. We therefore propose that\nall potential word groups be identiﬁed ﬁrst and both hierarchical\nstructure analysis and functional structure analysis be carried out\non sequences of word groups rather than on sequences of words.\nThis reduces the eﬀective length of the sentence and increases the\neﬃciency of parsing. The L part of the grammar provides con-\nstraints for group formation. We show that the L grammar con-\nstraints in both positional and relatively free word order languages\nare primarily constraints of linear position. Hence the name linear\nstructure.\nFunctional dependencies can span across several clauses due\nto nesting of clauses. These dependencies have got something to\ndo with the hierarchical structure of clauses but they have nothing\nto do whatever with distance - long or short. The concept of linear\ndistance is irrelevant here. We get rid of the problems of long dis-\ntance dependencies by working from whole to part instead of work-\ning from left to right. We determine the clause structure ﬁrst and\nthen analyze the functional structure clause by clause. We start\nfrom the main clause and go recursively into the nested clauses.\nWe pass on information and expectations about displaced, shared\nand missing participants down the hierarchy of clauses.\nWith\nthis, functional structure analysis of a clause becomes practically",
    "from the main clause and go recursively into the nested clauses.\nWe pass on information and expectations about displaced, shared\nand missing participants down the hierarchy of clauses.\nWith\nthis, functional structure analysis of a clause becomes practically\ncompletely independent of the functional structure of any other\nclause in the sentence. There will no longer be any long distance\ndependencies.\nThus long distance dependencies, which require\nspecial mechanisms in other grammar formalisms, are a non-issue\nin UCSG.\nIn UCSG we work from whole to part rather than one end\nto the other. We know the hierarchical structure of clauses and\nclause boundaries before we attempt to analyze the functional\nstructure of individual clauses. In all other grammar formalisms\nsentences are processed linearly, say, left to right. At any point\nin this process, decisions are taken based only on the information\ngathered from the part seen so far. We have really no idea of what\nlies ahead but for certain expectations which can be built based\non what we have already seen. Working from whole to part has\nthe advantage that we always have a global picture of what we\nare doing before we ﬁll up the minor details. This has been the\ntraditional wisdom in all branches of engineering. For example,\nmaps are always made from whole to part.\n2.2. COMPUTATIONAL LINGUISTICS\n213\nWe show that all potential word groups in a sentence can be\nobtained using ﬁnite state machine power in linear time. We also\nshow that hierarchical structure analysis can be done using a very\nsmall number of context free grammar rules in cubic time - cubic\nin the number of verb groups and sentinels, which is typically\nmuch smaller than the number of words in the sentence. Finally,\nwe show how the localization of functional structure analysis to\nthe local domains of the respective clauses makes parsing in UCSG\nhighly eﬃcient on the whole.\nIn conclusion we see that the UCSG formalism has several",
    "much smaller than the number of words in the sentence. Finally,\nwe show how the localization of functional structure analysis to\nthe local domains of the respective clauses makes parsing in UCSG\nhighly eﬃcient on the whole.\nIn conclusion we see that the UCSG formalism has several\nmerits. Firstly, we see that the division of work into three in-\ndependent modules helps to discover, and hence exploit, what is\ncommon between a relatively free word order language like Tel-\nugu and a positional language like English. UCSG is equally well\nsuited for parsing positional languages and relatively free word or-\nder languages. In fact even cross serial dependencies as in Dutch\ncan be handled. Secondly, we get an elegant solution to the prob-\nlem of long distance dependencies. Thirdly, grammars can be writ-\nten more easily than with other grammar formalisms. Fourthly,\nwe note that eﬃcient parsers can be written. Lastly, and most\nimportantly, the division of labour into three modules that have\na very close correspondence with the three aspects of structure of\nhuman languages, will hopefully oﬀer new and useful insights into\nthe nature of our languages.\nUCSG grammars have been developed for English and a few\nIndian languages and applied for machine translation between En-\nglish and Indian languages. The UCSG system for English has\nbeen extended to a wide coverage robust partial parsing system\nby combining the basic architecture outlined above with statistical\nparsing techniques. The approach holds promise since grammars\ncan be developed without need for a parsed training corpus. See\nreferences given in the bibliography for more on UCSG.\nPartial Parsing\nAlthough a lot of work has gone into developing syntactic parsers,\nit has not been possible to achieve high performance on unre-\nstricted texts. Full syntactic parsers also have many limitations:\n1) exponential solution space with the attended computational\nineﬃciencies, 2) large and complex grammars 3) greater need for\n214",
    "it has not been possible to achieve high performance on unre-\nstricted texts. Full syntactic parsers also have many limitations:\n1) exponential solution space with the attended computational\nineﬃciencies, 2) large and complex grammars 3) greater need for\n214\nCHAPTER 2. FOUNDATIONS OF NLP\nsemantic and pragmatic contextual knowledge which are not easy\nto provide 4) need to tackle long-distance dependencies, and 5)\naccumulation and multiplication of errors. Given these, there has\nbeen an increased interest in wide coverage and robust but partial\nor shallow parsing systems in the last decade or so.\nShallow parsing is the task of recovering only a limited amount\nof syntactic information from natural language sentences. Most of\ntimes shallow parsing is restricted to ﬁnding phrases in sentences,\nin which case it is also called chunking. Chunking is simply ﬁnding\nsyntactically related non-overlapping group of words. It is the task\nof dividing the text into syntactically non-overlapping phrases.\nHere by non-overlapping we mean one word can become a member\nof only one chunk. Chunks are also referred to as word groups to\ndistinguish them from phrases, a terminology that has come to\nacquire a diﬀerent specialized connotation in linguistics.\nFor example, the sentence “He reckons the current account\ndeﬁcit will narrow to only # 1.8 billion in September” is parsed\nas follows:\n[NP He ] [VP reckons ] [NP the current account deﬁcit ] [VP\nwill narrow ] [PP to ] [NP only # 1.8 billion ] [PP in ] [NP Septem-\nber ].\nNote that the PP-Attachment ambiguities have not been re-\nsolved yet, nor are the thematic roles identiﬁed.\nDeveloping computational grammars is a challenging task.\nThere are broadly two approaches to development of grammars\n- the linguistic approach which depends upon hand-crafted rules,\nand, the machine learning approach where grammars are learnt\nautomatically from a training corpus. Developing hand-crafted\ngrammar rules is a very slow, tedious and diﬃcult task, requiring",
    "- the linguistic approach which depends upon hand-crafted rules,\nand, the machine learning approach where grammars are learnt\nautomatically from a training corpus. Developing hand-crafted\ngrammar rules is a very slow, tedious and diﬃcult task, requiring\nsubstantial knowledge and skill on the part of the linguist. Auto-\nmatic learning of grammars requires, on the other hand, a large,\nrepresentative, parsed training corpus, which is often not avail-\nable. Perhaps only a good combination of the two approaches can\ngive us the best results with minimal eﬀort.\nWhile several grammars and parsing systems exist for English\nand other major languages of the world, Indian language are lag-\nging far behind. There are hardly any substantial computational\ngrammars for any of the Indian languages. Parsed corpora are\nalso not available and hence machine learning approaches cannot\nbe applied right away. How then do we develop wide coverage\n2.2. COMPUTATIONAL LINGUISTICS\n215\ngrammars and robust shallow parsers with minimal time and ef-\nfort? Only an ingenious combination of linguistic and statistical\nmethods with judicious bootstrapping may help us develop large\nscale computational grammars with minimal time and eﬀort. Such\neﬀorts are currently underway and hopefully we will have at least\npartial parsing systems in Indian languages soon.\n2.2.6\nSemantics\nSemantics is the study of meanings - meaning of words, of sen-\ntences and of discourse units. The very purpose of language is\ncommunication - communication of information, intentions, feel-\nings, attitudes. Natural language understanding aims at recover-\ning these information, intentions, feelings and attitudes from the\nlinguistic message that is encoded as temporal sequences of sounds\nin speech and as linear sequence of symbols in the written text.\nNatural language generation aims at encoding given information,\nintentions, feelings and attitudes as linear sequences in speech or",
    "linguistic message that is encoded as temporal sequences of sounds\nin speech and as linear sequence of symbols in the written text.\nNatural language generation aims at encoding given information,\nintentions, feelings and attitudes as linear sequences in speech or\ntext form. Translation, summarization, categorization, informa-\ntion retrieval and all other applications require understanding the\nmeaning of the text to achieve human-like performance. Seman-\ntics is thus at the very core of language technologies. In fact it\nwould be proper to view all other aspects as steps towards un-\nderstanding the meanings. If we cannot understand the meaning,\nthere is only so much we can do. Attempting any language tech-\nnology application without trying to understand meanings will be\nlike driving a car with the eyes blind-folded. You can of course\ndrive but only at a big risk.\nBut semantics is a diﬃcult area. No one knows exactly what\nwe mean by meaning. No one really knows exactly what we mean\nby understanding. Philosophers have been thinking on these top-\nics for centuries. Now that these questions have become pertinent\nto development of technologies, scientists from a variety of other\nspecializations have also been working on semantics and propos-\ning new ideas.\nA lot of new ideas have come up over the last\n60 years. But that does not mean that the problems have been\nsolved. Given the nature of the problems and issues, it would be\napt to say that we know today almost only as much as we knew\ncenturies ago. In fact many of the ideas tried out by technologists\nare superﬁcial and the real problems have not even been addressed\n216\nCHAPTER 2. FOUNDATIONS OF NLP\nproperly.\nNo wonder then, that most of the applications today do not\neven incorporate a semantics component in any serious measure.\nThere is even a claim that a lot can be done without going for\nany in-depth linguistic analysis. And practical experience does\nshow that in some tasks high performance can be achieved by",
    "even incorporate a semantics component in any serious measure.\nThere is even a claim that a lot can be done without going for\nany in-depth linguistic analysis. And practical experience does\nshow that in some tasks high performance can be achieved by\nonly a very superﬁcial analysis provided large scale training data\nis available and the right methods are used for learning from this\ndata. For example, text categorization systems achieve 95% plus\nperformance by using raw words as features, without need for any\ndictionary or morphological analysis, let alone syntax or seman-\ntics. Human beings are also far from perfect and in some tasks\nit is possible to achieve performance comparable to or even some\nwhat better than human performance without need for in-depth\nlinguistic analysis.\nIt would be wrong, however, to conclude that machines have\nreached or are approaching human levels of intelligence. Given\nsuitable, data, sets of features to use, a method to compute and\nweigh the features and a classiﬁcation method, automatic tech-\nniques exist for performing classiﬁcation in an optimal way as\ndeﬁned by some given criteria. Human beings can also do this,\nalthough they may take more time than computers or commit\nmistakes while computing, but they can also do much more. Ma-\nchine learning today is largely restricted to generalization from\nexamples. But we human beings can ’learn’ even new methods of\nmachine learning. We learn to identify discriminative features, we\nlearn to weigh them and we learn the optimization criteria and\nmethods. There is no comparison between machines and human\nbeings. High performance is achievable only in some restricted\ntasks under suitable assumptions. This does not mean that ma-\nchines can understand the meaning of texts nor does it mean that\nunderstanding meanings is not necessary.\nIn the section on syntax, we said form follows function and\nhence knowing the structure greatly facilitates understanding the",
    "tasks under suitable assumptions. This does not mean that ma-\nchines can understand the meaning of texts nor does it mean that\nunderstanding meanings is not necessary.\nIn the section on syntax, we said form follows function and\nhence knowing the structure greatly facilitates understanding the\nmeanings. We even said the structural descriptions produced by\na syntactic analyzer can be viewed as rough descriptions of mean-\ning. This is not to say that meanings directly follow from syntac-\ntic analysis. Sentences and discourse segments that have similar\nstructure may have widely varying semantics. Look at these two\nsets of sentences, due to Katz:\n2.2. COMPUTATIONAL LINGUISTICS\n217\nSET-1:\n1. There is a fire in my kitchen\n2. My kitchen is in my house\n3. Hence, there is a fire in my house\nSET-2:\n1. There is a pain in my foot\n2. My foot is in my shoe\n3. Hence, there is a pain in my shoe\nSentences in natural language do not show up their underlying\nlogical structure explicitly in their surface form. Making logical\nsense out of sentence and discourse structures is a complex task.\nFormal logics help us in eliminating ambiguities and representing\nmeanings in a precise way. However, obtaining logical formulae\nfrom natural language sentences is tough. In fact we have only\nshifted the burden of semantics to the process of mapping given\nutterances into logical formulae.\nMeaning, Reference and Truth\nMeaning, reference and truth are diﬀerent but related concepts.\nThere are several theories about how we make sense of the world,\ninferring meaning. Is the meaning of a word the object it refers to?\nHow do we deal with words that denote objects that do not exist\nat all in the physical world? Should we talk of mental objects?\nDo words have meanings at all or is it we who attribute meanings\nto words? There are several theories. Here we just list the general\ntheories of understanding without getting into details:\n• Attribution Theory: we need to attribute cause, that sup-\nports our ego",
    "Do words have meanings at all or is it we who attribute meanings\nto words? There are several theories. Here we just list the general\ntheories of understanding without getting into details:\n• Attribution Theory: we need to attribute cause, that sup-\nports our ego\n• Constructivism: we use constructs as perceptual categories\n• Framing: mental combinations that aﬀect perception\n218\nCHAPTER 2. FOUNDATIONS OF NLP\n• Schema:\nmental structure to organize and interpret the\nworld\n• Symbolic Interaction Theory: we derive meaning around\nsymbols\n• Objectiﬁcation: we simplify complex things into concrete\nimages\n• Story Model: We piece together complex situations into\nstories to build understanding\n• Speech Act Theory: Speaking is acting. Meaning is context-\ndependent\nThere are also more speciﬁc theories about inferring meaning:\n• Attention: How we pay attention to things around us\n• Belief: What and how we believe\n• Understanding ourselves: How we perceive ourselves\n• Discomfort: How we handle discomfort\n• Understanding others: How we make sense of other people\n• Attribution: How we attribute cause\nFrom this it should be clear that what exactly is the meaning\nof meaning is itself a big and diﬃcult question. One should not,\ntherefore, expect computer programs at this point of time to cor-\nrectly identify meanings and behave like human beings. NLP is\ndiﬃcult. Do not fall for false claims of success.\nIndian Theories of Meaning\nThere is an ocean of knowledge on meaning within the Indian tra-\ndition. There are several schools of thought and continuous scien-\ntiﬁc development of ideas for several thousand years now without\ngaps. It is unfortunate that there are separate worlds and people\nof one world know nothing about the other. That things are in\nSanskrit and we do not know Sanskrit is not an acceptable ex-\ncuse. Can we say today that we do not know English and so we\ndo not care for anything that is published in English? Can we go",
    "of one world know nothing about the other. That things are in\nSanskrit and we do not know Sanskrit is not an acceptable ex-\ncuse. Can we say today that we do not know English and so we\ndo not care for anything that is published in English? Can we go\nahead and re-publish ideas known for centuries as our own new\n2.2. COMPUTATIONAL LINGUISTICS\n219\ndiscoveries? Indians are learning English because they think that\nis where knowledge is. If knowledge is in Sanskrit it should be\nequally natural that seekers of knowledge try to learn Sanskrit.\nIndian theories of meaning make a three tier distinction in\nhow meanings of words are to be considered abhida, lakshaNa and\nvyanjana gradually moving from literal to indicative or suggestive\nand metaphorical.\nThe discipline of s’aabdaboodha is all about meaning of words.\nHere we dig deep and provide a very detailed picture. Consider\nthe simple verb go. Going requires somebody to go, somewhere\nto go and somewhere to go from. It is the act of going that is\nimplied. Thus s’aabdaboodha deﬁnes the verb go as an activity\nthat is favourable for the dis-association of an object to its cur-\nrent location and re-association of that object to another location.\nEvery word is deﬁned with such great detail. It becomes possible\nto derive the meanings of complex constructs through a series of\nlogical deductions.\nThere is also a great scientiﬁc debate, going on for the last few\nthousand years in India, on whether the meaning of a sentence\n(or an utterance) can be constructed from the meanings of the\nwords it is made up of or not. These are all largely unknown in\nthe western world. It is taken for granted that syntax is useful\nfor semantics and so much of time and eﬀort is spent on syntax\nbefore the basic question of its utility for semantics is answered.\nMeanings of words apart, our interpretation and understand-\ning is what ﬁnally matters. In that sense meanings are in our\nminds. Somebody is a freedom-ﬁghter or a terrorist depending",
    "before the basic question of its utility for semantics is answered.\nMeanings of words apart, our interpretation and understand-\ning is what ﬁnally matters. In that sense meanings are in our\nminds. Somebody is a freedom-ﬁghter or a terrorist depending\nupon how you look at it and which side you take. The same thing\ncan mean diﬀerent things to diﬀerent people in diﬀerent contexts.\nThe same thing also changes meaning as you get better and deeper\nunderstanding. There is a limit for purely symbolic and objective\nconsideration of meanings.\nThere is a lot that has been done in semantics but there is\nstill a lot more that needs to be done. It is beyond the scope of\nthis book to get into the depths of semantics. We will only look\nat selected linguistic phenomena with examples.\n220\nCHAPTER 2. FOUNDATIONS OF NLP\nAttachment\nThere are some areas which lie at the boundary between syntax\nand semantics. Attachment of prepositional phrases and subor-\ndinate clauses is one example. Resolution of anaphoric references\nis another. To a small extent these problems can be tacked from\nthe point of syntax but by and large they belong the realm of\nsemantics. Syntax cannot take us too far. Consider the following\nwell known example:\nI saw a man on the hill with a telescope\nThe prepositional phrase on the hill may modify the verb,\nor the object. Accordingly we get two diﬀerent meanings - the\nseeing action took place on the hill (compare: I ate an apple on\nthe hill) or the man was actually on the hill when he was sighted.\nSimilarly, the prepositional phrase with a telescope can modify\neither the verb or the man or the hill.\nPerhaps the telescope\nwas used as an instrument for seeing or the man was carrying\na portable telescope in his hand or a big telescope was mounted\non the hill on which you saw the man. In any given sentence,\none of these possibilities or the other may look more natural and\nsome possibilities may even look odd.\nLook at many diﬀerent",
    "a portable telescope in his hand or a big telescope was mounted\non the hill on which you saw the man. In any given sentence,\none of these possibilities or the other may look more natural and\nsome possibilities may even look odd.\nLook at many diﬀerent\nexamples and you will see that all the possible ways of attaching\nthe prepositional phrases are appropriate for some example or the\nother. Syntax can provide some general biases but it cannot tell\nus exactly how to attach prepositional phrases. This is largely a\nproblem of semantics.\nIf large scale training data is available one could think of using\nstatistics to ﬁnd out which way of attaching may the most likely\nfor the given sentence.\nWords may be replaced with concepts\nusing an ontology and the hierarchy of concepts exploited to make\nbetter judgements. You can think of mounting a telescope on a\nhill but a hill on a telescope makes no sense. Why? because you\nhave never come across any such thing in your life.\nBased on\nthe language experience and real world experience you have so\nfar, such a thing looks impossible. A man carrying a telescope is\nimaginable - perhaps a telescope can be built which is small, light-\nweight and hence portable. Even if you have never seen a telescope\nin your life, you can make certain judgements intelligently. We do\n2.2. COMPUTATIONAL LINGUISTICS\n221\nnot know exactly how. How then do we build machines which can\nbehave like us?\nConsider the following sentence:\n• I saw a man who was reading a newspaper while watching\nTV\n• I killed a man who was reading a newspaper while driving\nmy car\nWas the man reading the newspaper while watching TV or was\nit that I saw such a man while watching TV? Who was driving my\ncar? The problem of attaching subordinate clauses is similar to\nthe problem of prepositional phrase attachment. These are largely\nproblems of semantics and hence not easy to solve.\nQuantiﬁcation\nTerms like all, every, each, some, a are quantiﬁers - they are used",
    "car? The problem of attaching subordinate clauses is similar to\nthe problem of prepositional phrase attachment. These are largely\nproblems of semantics and hence not easy to solve.\nQuantiﬁcation\nTerms like all, every, each, some, a are quantiﬁers - they are used\nfor expressing quantitative aspects through language. Consider\nthe sentences:\n1a. A man saw every dog\n1b. A man saw each dog\nThe ﬁrst sentence means that one particular man saw all the\ndogs whereas the second one permits an interpretation where dif-\nferent dogs were seen by possibly diﬀerent men. That is, we may\nstart looking at the dogs, one by one, and for each, ﬁnd out which\nman saw that dog. We say “each” has wider scope. Identiﬁcation\nof the correct scope of quantiﬁers is essential for understanding\nthe meaning of natural language sentences.\nThe scope of quantiﬁers is speciﬁed by the particular quan-\ntiﬁers used as also by syntactic structure. Features such as deﬁ-\nniteness, voice, position in the sentence seem to aﬀect the relative\nscope of quantiﬁers. Think carefully about each of the following\nexamples and see what kinds of interpretations they permit:\n2a. A man saw every dog\n2b. Every dog saw a man\n222\nCHAPTER 2. FOUNDATIONS OF NLP\n3a. Every dog saw a man\n3b. A man was seen by every dog\n4a. Every dog saw the man\n4b. The man saw every dog\n4c. Each dog saw the man\n5a. Who saw every dog?\n5b. Who saw each dog?\n6. There is a dog that saw every man\nYou may ﬁnd at hard to lay down hard and fast rules but\nyou can surely observe some general tendencies and preferences.\nBased on such observations, we may conclude that the general\nhierarchy of scope of these quantiﬁers could be:\nthe > each > whterms > every, all, some, a\nSyntactic structures such as prepositional phrases and relative\nclauses can also aﬀect scoping. Consider:\n7a. The kitchen in every house had a stove\n7b. The kitchen that was in every house had a stove\nNegative markers appear in diﬀerent positions in sentences",
    "Syntactic structures such as prepositional phrases and relative\nclauses can also aﬀect scoping. Consider:\n7a. The kitchen in every house had a stove\n7b. The kitchen that was in every house had a stove\nNegative markers appear in diﬀerent positions in sentences\nand give diﬀerent meanings. They also interact with quantiﬁers.\nHence, they can be studied in terms of scope phenomena. Con-\nsider:\n8a. Not every boy likes Mary\n8b. Every boy does not like Mary\n8c. Not every boy likes some girl\n9.\nEvery representative from some countries\ndid not speak at the meeting\n2.2. COMPUTATIONAL LINGUISTICS\n223\nAs you can see things can become quite confusing when sev-\neral quantiﬁers are used in a sentence. Like other problems in\nsemantics, determining the exact scope of quantiﬁers is not very\neasy. It would be instructive to study quantiﬁers and scoping phe-\nnomena in your own language. You could consider examples such\nas the following Hindi sentences:\n10a. har aadmi kutte se pyaar kartaa hai\n10b. har aadmi eek kutte se pyaar kartaa hai\n10c. har aadmi kisii kutte se pyaar kartaa hai\n10d. har aadmi kisii na kisii kutte se pyaar\nkartaa hai\n10e. har aadmi eek na eek kutte se pyaar\nkartaa hai\nModiﬁcation\nNouns represent objects and we use adjectives to qualify these\nobjects and specify their properties or attributes. Adjectives may\nspecify additional, special, unusual, unexpected properties or oth-\nerwise modify the basic meaning of the noun in many complex\nways.\nHere we are looking at the problems and issues associ-\nated with such qualiﬁcation or modiﬁcation. Here we shall use\nthe terms qualiﬁcation and modiﬁcation interchangeably in this\ngeneral sense.\nThere are two major issues:\n1. What modiﬁes what?\n2. What is the semantic relationship between the modiﬁer and\nthe modiﬁed?\nIt must ﬁrst be noted that although we may say nouns re-\nfer to objects and adjectives modify the nouns, nouns can act as\nadjectives and modify other nouns. Thus we need to talk about",
    "1. What modiﬁes what?\n2. What is the semantic relationship between the modiﬁer and\nthe modiﬁed?\nIt must ﬁrst be noted that although we may say nouns re-\nfer to objects and adjectives modify the nouns, nouns can act as\nadjectives and modify other nouns. Thus we need to talk about\nnoun-noun modiﬁcation and adjective-noun modiﬁcation.\n224\nCHAPTER 2. FOUNDATIONS OF NLP\nNoun-Noun Modiﬁcation:\nConsider the following examples:\n1. Water Meter Cover Adjustment Screw\n2. Computer Software Development Training Institute\nWe understand “water meter cover adjustment screw” as the\n“adjustment screw of the cover of the meter used for measuring\nthe ﬂow of water”. What kind of a “meter”? “Water Meter”.\nThus “water” modiﬁes “meter”. “Cover” is a part of the “wa-\nter meter”. Thus “cover” is modiﬁed by “water meter”. What\nkind of a “screw”? “Adjustment screw”, not ﬁxing screw. Thus\n“adjustment” is the modiﬁer and “screw” is the modiﬁed. “Wa-\nter meter cover” as a whole modiﬁes “adjustment screw”. Sim-\nilarly “computer software development training institute” is an\n“institute that trains people in developing computer software”.\n“Training” what? “Computer Software Development”, not just\n“Development”. You can think of many more examples of noun\nsequences such as these. Try to analyze and see what modiﬁes\nwhat and how exactly you can say that.\nHow do we know all this?\nWe use our common sense and\nworld knowledge. We know that “water pump” is a pump used\nto pump water and “cast iron pump” is a pump made up of cast\niron. Cast iron pump does not pump cast iron, nor is a water\npump made up of water. It is only world knowledge and common\nsense that can tell this diﬀerence. Computers do not have world\nknowledge or common sense. Hence the challenge.\nNot all sequences of nouns are cases of modiﬁcation. In\nThe cat was hungry. So I gave the cat food\n“cat” does not modify “food” (although elsewhere it can).\nWhat I gave is “food” and to whom I gave food is “cat”.",
    "knowledge or common sense. Hence the challenge.\nNot all sequences of nouns are cases of modiﬁcation. In\nThe cat was hungry. So I gave the cat food\n“cat” does not modify “food” (although elsewhere it can).\nWhat I gave is “food” and to whom I gave food is “cat”.\nIn spoken language, we use prosodic cues such as stress to in-\ndicate modiﬁer-modiﬁed relationships. Thus we can understand\nthat “steel mill” is a mill for manufacturing or rolling steel and\n“steel beam” is a beam made up of steel based on the stress pat-\ntern. Written language is an impoverished version of spoken lan-\nguage and naturally the problems are harder.\nNo simple solutions exist to ﬁnd what modiﬁes what in all\ncases. There are some general preference tendencies in language\n2.2. COMPUTATIONAL LINGUISTICS\n225\nand we can exploit them to make guesses. For example, given\nthree nouns N1, N2 and N3 in sequence, N1 modifying N2 is more\npreferable to N2 modifying N3 and N1 modifying this combined\ngroup. We can also use statistical clues based on the analysis of\na large and representative corpus.\nFinding the nature of semantic relationship between the mod-\niﬁer and modiﬁes is also tough. In “fan blade” blade is a part\nof the fan. In “rubber ball” rubber is the material of which the\nball is made. In “Bush administration” Bush is the head of the\nadministration. In “Maruti car” Maruti is the company that man-\nufactures the car. In “ink pen” ink is the material used to ﬁll the\npen with for writing. In “Akash missile” Akash is the name given\nto the missile. In “Delhi declaration” Delhi is the place where\nsome important declaration was made. In “1999 bombing” 1999\nis the time when the bombing took place. These are not easy for\na dumb machine to understand.\nAdjective-Noun Modiﬁcation:\nSome adjectives are “intersective” in nature. These adjectives\nrestrict the sense of the modiﬁed objects. Thus “red balls” are all\nthose balls which are red. Balls which are not red are excluded.",
    "a dumb machine to understand.\nAdjective-Noun Modiﬁcation:\nSome adjectives are “intersective” in nature. These adjectives\nrestrict the sense of the modiﬁed objects. Thus “red balls” are all\nthose balls which are red. Balls which are not red are excluded.\nAll red balls are balls. They have all the common properties of\nballs. Additionally, they are required to be red in colour. You can\nunderstand “red balls” by making a set of all balls and a set of\nall red objects and ﬁnding an intersection. Red balls are exactly\nthose objects which are balls and which are red in colour. The set\nof red balls is smaller in size than the set of all balls (or equal in\nsize in the extreme case where all balls are red and there are no\nballs of any other colour). Intersective adjectives are thus easier\nto understand.\nHowever, not all adjectives are intersective in nature.\nYou\ncannot understand “slow trains” by making a set of all trains and\na set of all slow moving objects and ﬁnding the intersection. A\nset of all trains makes sense but a set of all slow objects cannot be\nmade. A slow train may actually be far faster than a fast snail.\nSlow and fast are relative. We cannot make sets of all slow objects\nor all fast objects. So are adjectives like large, small, bright, dull,\netc.\nA “toy gun” is not a gun in the ﬁrst place. An “alleged mur-\n226\nCHAPTER 2. FOUNDATIONS OF NLP\nderer” may not be a murderer for all you know. “Average marks”\nmay not be treated as marks because marks may have to be whole\nnumbers while the average may turn out to be a fraction.\nNote that apart from adjectives, proper names, possessive\nnouns, possessive pronouns, past and present participles can all\nact as modiﬁers. There can be several modiﬁers in sequence and\nwhat modiﬁes what can be ambiguous. All the sense ambiguities\nof words add up to the problem. Does “old ladies hostel” mean\na ladies hostel that is old or a hostel for old ladies? Is “light red\nball” a ball which is light in weight and red in colour or a ball",
    "what modiﬁes what can be ambiguous. All the sense ambiguities\nof words add up to the problem. Does “old ladies hostel” mean\na ladies hostel that is old or a hostel for old ladies? Is “light red\nball” a ball which is light in weight and red in colour or a ball\nwhich is light red in colour?\nNote that all comparative adjectives are intersective in na-\nture. Comparison is with respect to some speciﬁed thing and thus\nrelative things become absolute. Thus in “logs heavier than 100\nKg”, we can make a set of all logs and a set of all objects heav-\nier than 100 Kgs. Interestingly, superlatives are non-intersective.\nYou cannot make a set of “largest objects” to interpret “largest\ndog in the pound”.\nWord Sense Disambiguation\nA word can have more than one sense. The sense in which the\nword is used can be determined, most of the times, by the context\nin which the word occurs. The word bank has several senses out\nof which bank as a ﬁnancial institution and bank as a sloping land\nbordering a river can be easily distinguished from the context.\nDistinguishing between the senses of bank as a ﬁnancial institu-\ntion and bank as a building housing such an institution is more\ndiﬃcult. The process of identifying the correct sense of words in\ncontext is called Word Sense Disambiguation (WSD). Homonymy\nand Polysemy must both be considered. Word sense disambigua-\ntion contributes signiﬁcantly to many natural language processing\ntasks such as machine translation and information retrieval.\nMany words have more than one sense. Here are some more\nexamples:\nThe focus of research in WSD is on distinguishing between\nsenses of words within a given syntactic category, since senses\nacross syntactic categories are more easily disambiguated through\nPOS tagging techniques. Many researchers have focused on dis-\nambiguation of selected target words although there is some recent\n2.2. COMPUTATIONAL LINGUISTICS\n227\nSense Deﬁnition\nreadiness to give attention\nquality of causing attention to be given to",
    "POS tagging techniques. Many researchers have focused on dis-\nambiguation of selected target words although there is some recent\n2.2. COMPUTATIONAL LINGUISTICS\n227\nSense Deﬁnition\nreadiness to give attention\nquality of causing attention to be given to\nactivity, etc. that one gives attention to\nadvantage, advancement or favor\na share in a company or business\nmoney paid for the use of money\nTable 2.1: Senses of the word interest - noun\nSense Deﬁnition\nExample\nnot easy - diﬃcult\nit’s hard to be disciplined\nnot soft - metaphoric\nthese are hard times\nnot soft - physical\nthe hard crust\nTable 2.2: Senses of the word hard - adjective\ninterest in unrestricted WSD.\nWSD systems often rely upon sense deﬁnitions in dictionar-\nies, features of senses (for example, box-codes and subject cate-\ngories present in Longman’s Dictionary of Contemporary English\n(LDOCE)), entries in bilingual dictionaries, WordNet etc. Dictio-\nnaries and other sources do not always agree on the number and\nnature of senses for given words. For some tasks the ﬁne gran-\nularity of senses as given in some dictionaries is not required or\nmay even be counter productive and so methods to merge closely\nrelated senses have been explored by some researchers.\nBoth knowledge based and machine learning approaches have\nbeen applied for WSD. Glossaries of senses present in dictionar-\nies are helpful. For example, the sense deﬁnition which has the\nmaximum overlap with the deﬁnitions of the context words may\nbe taken as the correct sense. Machine learning techniques such\nas Bayesian learning, decision lists, and decision trees have been\nused. Machine learning methods require a training corpus. Clus-\ntering techniques have been used to group cases with same sense\ntogether where labelled training data is not available.\nChoice of the right features is often more important than the\n228\nCHAPTER 2. FOUNDATIONS OF NLP\nSense Deﬁnition\nExample\nfunction as something\nserves as yard stick to\nprovide a service",
    "together where labelled training data is not available.\nChoice of the right features is often more important than the\n228\nCHAPTER 2. FOUNDATIONS OF NLP\nSense Deﬁnition\nExample\nfunction as something\nserves as yard stick to\nprovide a service\ndept. will serve select few\nsupply with food/means\nserve dinner\nhold an oﬃce\nserved as head of department\nTable 2.3: Senses of the word serve - verb\nchoice of techniques for classiﬁcation. A variety of features have\nbeen used, including bigrams, surface form of the target word,\ncollocations, POS tags of target and neighboring words and syn-\ntactic features such as heads of phrases and categories of phrases\nin which the target word appears. Some researchers believe that\nlexical features are suﬃcient while others have argued for combin-\ning lexical features with syntactic features.\nNot all words in the context are helpful for determining the\nsense of a target word. Syntax can help in identifying relevant\nparts of the context, thereby eliminating noise. Verb-object, subject-\nverb and noun-modiﬁer relationships can be used. The role of var-\nious kinds of lexical and syntactic features have been studied in\nisolation as well as in various combinations. Ensemble techniques\nhave been proposed to combine the results of diﬀerent classiﬁers\nusing diﬀerent sets of features.\nSome of the crucial issues such as the precise deﬁnition of\nword senses and granularity of sense distinctions have not been\nexplored very well. WordNet and other available lexical resources\nare usually taken as the basis for ﬁxing the possible senses of a\nword. Word sense disambiguation using purely linguistic methods\nwill be very diﬃcult. Recent studies have shown that fairly high\nperformance can be obtained if suﬃcient training data is avail-\nable. Sense tagged data, however, is not readily available. Thus\nautomatic generation of sense tagged data has become a major\nquestion.\nResolution of Anaphora\nLook the following examples (from Graeme Hirst):",
    "performance can be obtained if suﬃcient training data is avail-\nable. Sense tagged data, however, is not readily available. Thus\nautomatic generation of sense tagged data has become a major\nquestion.\nResolution of Anaphora\nLook the following examples (from Graeme Hirst):\n1a. Bill Thought that John would laugh at him\n2.2. COMPUTATIONAL LINGUISTICS\n229\n1b. Bill Thought that John would laugh at himself\n2a. When Sue went to Nadia’s house for dinner,\nshe served sukiyaki augratin\n2b. When Sue went to Nadia’s house for dinner,\nshe ate sukiyaki augratin\n3. Give the bananas to the monkeys\nalthough they are not ripe\nbecause they are hungry\n4. Smoking gives one cancer\nWho is “him” in 1a above and who is “himself” in 1b? Who\nserved sukiyaki augratin and who ate? What are the two “they”s\nin 3 above? Who is this “one” in the last example? Words like\n“him, himself, they, one” refer to some body or something men-\ntioned before.\nThey are references to objects found elsewhere\nin the text. The objects referred to are called referents or an-\ntecedents. Resolving references, that is identifying what refers to\nwhat, is an important problem in linguistics and NLP.\nBecause we ﬁnd it too monotonous and boring to say some-\nthing like “cows give milk. Cows have four legs and two horns.\nCows eat grass. Cows are domestic animals. Cows ...” we say\n“cow” once or twice and then onwards we start referring to this\ncow as “it”. Pronouns have the main purpose of standing in place\nof nouns and referring to them. Pronouns are abbreviated forms of\nnouns, abbreviated in terms of their information content, that is.\nThus “he” refers to any single male human being. This pronoun\ncontains this much of information but the last piece identifying\nthe exact person is missing. In fact any situation where there is\nan abbreviation of information, even ellipses, can be considered\nas a case of reference.\nModern linguistic theories provide a set of principles that ap-",
    "contains this much of information but the last piece identifying\nthe exact person is missing. In fact any situation where there is\nan abbreviation of information, even ellipses, can be considered\nas a case of reference.\nModern linguistic theories provide a set of principles that ap-\nply to references. Anaphors such as “himself” must be bound in\nthe local domain and so we know “himself” in 1b must be John.\nPronouns such as “him” will not be bound inside the local domain\nand so it cannot refer to John in 1a. Since no other possibility\n230\nCHAPTER 2. FOUNDATIONS OF NLP\nexists we may conclude that “him” is Bill. But the principles pro-\nvided in the linguistic theories are far from adequate to resolve all\nreferences in natural language. In fact the other examples given\nabove cannot be solved with the application of linguistic theory\nalone. Of course the reference and the referent point to the same\nobject and hence must share all grammatical and semantic prop-\nerties. Thus agreement in gender, number and other grammatical\nfeatures will be useful. Grammar alone will not be suﬃcient, se-\nmantics is essential as well. We understand sentences like 3 above\nonly because we know that only fruits an be ripe or otherwise and\nonly monkeys can be hungry, not the other way around. Prag-\nmatic knowledge that it is usually the host who serves and the\nguest who eats helps us to understand 2a and 2b. Of course guest\ncan serve, and host eats too. In 4 above “one” must be understood\nas anyone who smokes. No straight forward algorithms exist. We\nmust combine grammatical, semantic and pragmatic knowledge\nwith common sense. We can at best make good guesses.\nIt is possible to refer to parts of objects mentioned before (“the\ncover”), to sets of objects (“the longer ones”), to speciﬁc items of\nan ordered set (“the former”, “the latter”, “the third”), and in\nfact to any object that is semantically related to the referent in\nsome way (“the choice of colours”). World knowledge as well as",
    "cover”), to sets of objects (“the longer ones”), to speciﬁc items of\nan ordered set (“the former”, “the latter”, “the third”), and in\nfact to any object that is semantically related to the referent in\nsome way (“the choice of colours”). World knowledge as well as\nknowledge of the context are essential to resolve the references.\nLook at the following examples:\n1. I drove by our house in my car\na) The windows were dirty\nb) I saw my father’s car. The windows were dirty\nc) I saw my father’s bicycle. The windows were dirty\nd) The windows were dirty. The front door was open\nResolving anaphoric references remains one of the most fasci-\nnating and challenging tasks today.\n2.2. COMPUTATIONAL LINGUISTICS\n231\nConcluding Remarks\nIt is unfortunate that linguistics, like other disciplines, has rede-\nﬁned itself to address mostly problems of a superﬁcial nature to\nthe near total exclusion of the most important and crucial as-\npects of language. Areas like phonetics, phonology, morphology\nand syntax form the “core” of linguistics today while semantics is\nlargely forgotten. Some even may call it “extra-linguistic”! Think\nof language without meaning! There are three factors that may\nbe responsible for this. Firstly, there is growing professionalism\nand one is forced to address only manageable problems since suc-\ncessful results and solutions are expected. Secondly, real problems\nare often hard and there is no incentive for taking up real, hard\nproblems. In fact the very complicated nature of the real prob-\nlems often drives professionals away. Lastly, there is a rise and\neven dominance of empiricism.\nLanguage Engineering is noth-\ning but this empiricism based on large scale corpora and quan-\ntitative methods.\nIf we can get good performance in Informa-\ntion Retrieval, or Text Categorization or Search Engines by doing\nonly the most superﬁcial analysis of language, why bother to get\ndeeper? Many superﬁcial problems can be and have been solved.",
    "titative methods.\nIf we can get good performance in Informa-\ntion Retrieval, or Text Categorization or Search Engines by doing\nonly the most superﬁcial analysis of language, why bother to get\ndeeper? Many superﬁcial problems can be and have been solved.\nReal problems will remain. Application orientation is of course\nvery useful - applications must drive theory as also test and prove\ntheoretical ideas and hypotheses. But undue importance given to\nshort sighted areas where immediate uses can be found, is harmful\nto the true development of human knowledge.\nThis is certainly not to say that there is no work going on in\nsemantics. You will ﬁnd a large number of groups working actively\non various aspects of semantics in many parts of the world, if not\nso much in India. However, in the twentieth century, in contrast\nto earlier days, the choice of research problems and the way we go\nabout doing research has changed a lot. We select areas which are\ndoable, areas where we can publish papers or develop products.\nWe do not select a problem because it is important to solve it.\nKnowledge and scholarship per se are not valued, only the show of\nknowledge is. In the process many fundamental questions remain\nunanswered. We make assumptions and go ahead. Once we have\ncome a long way oﬀ, it becomes diﬃcult even to remember those\nassumptions and appreciate that what all we have obtained is\nsubject to our assumptions being true. Go back to the deﬁnition\n232\nCHAPTER 2. FOUNDATIONS OF NLP\nof NLP, NLU and NLG, go back to the discussion on question-\nanswering systems and check for yourself where we stand today.\nAsk basic questions. Do words exist? Do words have meaning?\nCan we deﬁne the meaning of words precisely? What exactly do\nwe mean by context? What exactly do we mean by sense of a\nword? Can we derive the meaning of a sentence from the meaning\nof the words and the structure of the sentence? Can we derive\nthe meaning of a text in terms of the meaning of the sentences",
    "we mean by context? What exactly do we mean by sense of a\nword? Can we derive the meaning of a sentence from the meaning\nof the words and the structure of the sentence? Can we derive\nthe meaning of a text in terms of the meaning of the sentences\nit is made up of? Suppose the answer to some of these questions\nturns out to be no. Don’t you think a whole lot of all the work\nwe are doing will become irrelevant and useless as far as NLP is\nconcerned?\n2.2.7\nPragmatics\nPragmatics is the study of the aspects of meaning and language\nuse that are dependent on the speaker, the addressee and other\nfeatures of the context of utterance, such as the following: a) The\neﬀect that the following have on the speaker’s choice of expres-\nsion and the addressee’s interpretation of an utterance: Context of\nutterance, Generally observed principles of communication, The\ngoals of the speaker b) Programmatic concerns, such as the treat-\nment of given versus new information (including presupposition),\ndeixis, speech acts (especially illocutionary acts), implicature, and\nthe relations of meaning or function between portions of discourse\nor turns of conversation.\n2.2.8\nOther Areas of Linguistics\nWe have only touched some of the areas of linguistics that are\nperhaps more directly relevant for NLP. There are several other\ninteresting and useful areas. Phonetics and phonology deal with\nthe physical and logical levels of basic sound units that make up\nthe words of a language. As such they are very much relevant for\nspeech technologies. Psycholinguistics addresses questions relat-\ning to how humans process language and use experimental meth-\nods to build and test hypotheses. Sociolinguistics is a rich ﬁeld\nconcerned with social aspects of language. Language is related\nto social status, power and politics. Historical linguistics is con-\ncerned with linguistic genetics and language change over time.\n2.3. STATISTICAL APPROACHES\n233\nLanguage policy is important as it has direct implications for the",
    "concerned with social aspects of language. Language is related\nto social status, power and politics. Historical linguistics is con-\ncerned with linguistic genetics and language change over time.\n2.3. STATISTICAL APPROACHES\n233\nLanguage policy is important as it has direct implications for the\npeople. Should the primary education be in the mother tongue\nor in some other language? The three language formula in our\ncountry is a example of language policy implementation. Teach-\ning language is another aspect that is closely related other areas\nof language and linguistics. Language teaching requires method-\nologies and techniques diﬀerent from those required for teaching,\nsay, mathematics or science. There are even language games that\ncan promote eﬀective language learning. To know the importance\nof language try to spend one day, just one day, without speaking,\nlistening, reading or writing! That would be extremely diﬃcult.\n2.3\nCorpus Based and Statistical Ap-\nproaches\nAt one point of time, experts used to specify what is right or ac-\nceptable and what is not. Grammars used to be prescriptive in\nnature and students could be punished for not following the rules.\nThis strictness was considered essential for maintaining the purity\nand standards. After all language is for communication and we\ncannot eﬀectively communicate if we all do not follow a set of com-\nmonly accepted protocols and standards. Looseness and lightness\nof thought about language is really more dangerous than careless\nuse of language. We often hear people say that grammar is not\nimportant and as long as they can communicate with others, that\nis good enough. But one must remember that we cannot commu-\nnicate eﬀectively unless we take language a bit more seriously. A\nlarge number of day to day problems at home or oﬃce can ac-\ntually be traced to communication problems arising from laxity\nin the use of language. Carelessness about language is harmful.",
    "nicate eﬀectively unless we take language a bit more seriously. A\nlarge number of day to day problems at home or oﬃce can ac-\ntually be traced to communication problems arising from laxity\nin the use of language. Carelessness about language is harmful.\nEﬀective communication is not possible unless language is taken\nseriously. Language and communication skills are essential for all\nwalks of life and it is generally true that those who have come to\nthe top are good communicators. Do not take language lightly.\nThe way we look at language has changed over time. Today\nlinguists believe that what people actually use is the real lan-\nguage. If all of them make a mistake, it is better not to call that\na mistake but a basic property of the language itself. The idea is\nnot to introduce looseness of thought about language but to fo-\n234\nCHAPTER 2. FOUNDATIONS OF NLP\ncus on developing descriptive grammars rather than prescriptive\nones. Grammars must describe language as native speakers of a\nlanguage actually use.\nIn order to take this view seriously, we need to base our lin-\nguistic studies on carefully collected large scale real life data. Such\ncollections of linguistic data are called corpora. We cannot aﬀord\nto sit down, meditate and make a list of diﬀerent types of sentences\nand assert that these types cover almost all the major types used\nby people. Linguists have a great tendency even today to think\nof diﬀerent types of constructs etc. How can we be sure? Studies\nshow that what experts initially think is often falsiﬁed when we\nlook at real data. Our power of imagination is not good enough\nwhen it comes to generating linguistic data.\nConcocted exam-\nples are often quite unrealistic. Linguists often come out which\nstrange and bizarre constructions that may never occur in real\nusage while many important constructs we actually use regularly\nmay be ignored altogether. If a system works well on a large and\nrepresentative corpus, that would be more dependable than saying",
    "strange and bizarre constructions that may never occur in real\nusage while many important constructs we actually use regularly\nmay be ignored altogether. If a system works well on a large and\nrepresentative corpus, that would be more dependable than saying\nthat the system performs very well on a thousand diﬀerent types\nof constructs, carefully selected by expert linguists. Corpus lin-\nguistics uses corpora as the basis for all investigations, hypothesis\ngeneration, testing and validation.\nCorpora are useful, why, almost essential for developing lan-\nguage technology applications. Corpora are used to build lexical\nresources such as word lists, dictionaries, thesauri and ontolo-\ngies. Statistical techniques are available for automatic or semi-\nautomatic development of lexical databases, morphological ana-\nlyzers, POS taggers, syntactic parsers etc. Corpora are used for\ntraining in machine learning algorithms. Corpora are also used\nfor testing and validation. Many of the language technology ap-\nplications possible today owe their existence to the availability of\nlarge scale corpora, aﬀordable storage and computing power and\nthe advancements in machine learning techniques.\nIn this section we shall take a brief look at diﬀerent kinds\nof corpora and techniques for developing, managing and using\ncorpora for language engineering applications.\n2.3. STATISTICAL APPROACHES\n235\n2.3.1\nCorpora\nA corpus is a large and representative collection of linguistic data,\ncarefully collected according to prescribed criteria. It is under-\nstood that the data are in electronic and machine processable\nform. A library housing a collection of printed books is not a cor-\npus. It is not of much direct value for the language engineering\napplications - machines cannot read or process printed books eas-\nily. Linguistic data can be at various levels and we can think of\ntext corpora, speech corpora, corpora of scanned images of texts\netc. The terms “large” and “representative” are qualitative but",
    "applications - machines cannot read or process printed books eas-\nily. Linguistic data can be at various levels and we can think of\ntext corpora, speech corpora, corpora of scanned images of texts\netc. The terms “large” and “representative” are qualitative but\nare intended to be objective and not entirely subjective. Some de-\ngree of objectivity can be introduced by having prescribed criteria.\nFor example, if a text corpus is intended to be used for building\ndictionaries, there must be a reasonable degree of coverage of all\nthe words of the language in the corpus. If we also need to look\nat sentential contexts in which diﬀerent words are used, say for\ndetermining the senses of words in diﬀerent contexts, then vari-\nous possible contexts must be covered. Qualitative criteria such\nas these can be translated into more precise quantitative criteria.\nYet, what is large enough and representative enough are big ques-\ntions that are not easy to answer in all cases. The only assurance\nis the that a corpus is carefully collected with the intention and\nhope of covering the full range of variabilities of concern.\nIt is important to realize that what is large and representative\nenough for one kind of application is not necessarily good enough\nfor a diﬀerent use.\nThere is really nothing like an all-purpose\ncorpus. One must not assume that just because a large corpus\nhas been used, the results and conclusions are universally valid\nand what works well here will also work well elsewhere. The size\nand nature of a corpus callas for careful consideration with regard\nto the particular application on hand.\nNonetheless, large corpora have often been found to be useful\nfor a variety of uses, often unintended and unforeseen at the time\nof collection. A striking example of this is the WordNet - a col-\nlection of English words grouped based on semantic similarities\nand interconnected in various semantic dimensions of relation-\nships. WordNet was initially developed with psychology in mind",
    "of collection. A striking example of this is the WordNet - a col-\nlection of English words grouped based on semantic similarities\nand interconnected in various semantic dimensions of relation-\nships. WordNet was initially developed with psychology in mind\nbut it has been used very extensively in a variety of NLP tasks\nand applications. It is therefore important to be careful and be\n236\nCHAPTER 2. FOUNDATIONS OF NLP\nas general and open as possible and include as many dimensions\nof variability as practically possible, while developing a corpus.\nCorpus development is a time consuming and costly process and\nevery care should be taken to ensure that what is done once can\nbe used for many diﬀerent applications.\nLet us get back to the question of how large is a large cor-\npus. We have seen the growth rate curves for types against to-\nkens for several major Indian languages - refer to Figure 2.5 above.\nThe distinction between Dravidian languages and Indo-Aryan lan-\nguages is striking in this ﬁgure - there are many more word forms\n(types) in Dravidian languages than in the other Indian languages.\nWhile 150,000 to 200,000 word types should be giving a very good\ncoverage for northern languages, Dravidian languages such as Tel-\nugu spoken mainly in the southern parts of India require a much\nlarger number of word forms. And, more importantly, the avail-\nable corpus is not suﬃcient even to get a clear idea of how many\nwords are there in the language. The morphology of these lan-\nguages is so rich, no one so far has an exact idea how many dif-\nferent word forms are there in the language. We have mentioned\nearlier that there can be as many as 1,80,000 diﬀerent types arising\nfrom a single verb root Telugu.\nWhat this shows is that techniques based on these corpora\nwhich work well for Indo-Aryan languages may not be applica-\nble to Dravidian languages. For example, it would be possible to\nsimply list all forms of all words and use this for dictionary based",
    "from a single verb root Telugu.\nWhat this shows is that techniques based on these corpora\nwhich work well for Indo-Aryan languages may not be applica-\nble to Dravidian languages. For example, it would be possible to\nsimply list all forms of all words and use this for dictionary based\nspelling error detection and correction system for Hindi, Punjabi\nor Bengali but such an approach cannot not be expected to pro-\nduce comparable performance results for say, Telugu or Kannada.\nIt would thus be not proper to make out right comparisons of\nperformance of language engineering products across these classes\nof languages. The inherent complexity of the languages must be\nfactored in when making any comparative judgements of perfor-\nmance.\nIf a 3 Million word corpus is insuﬃcient for Telugu, would a 10\nMillion word corpus be big enough for a speciﬁed purpose? Let us\nsee. A corpus of 225 full books adding up-to about 30,000 pages\nand 9.25 Million words has been developed for Telugu. The corpus\nincludes a variety of topics and categories - newspaper articles,\nshort stories, novels, poetry, classical and modern writings etc.\nThe growth rate curve of types against tokens for the total corpus\n2.3. STATISTICAL APPROACHES\n237\nof Telugu now amounting to about 12 Million word is shown below.\nThe curve still does not show any clear signs of saturation. There\nare about 21,00,000 types in this corpus and yet many of the\npossible types have not occurred even once in this corpus. If we\nbuild a large corpus, we should expect to see more new types\nunseen so far.\nFIG 2.13 Type-Token Growth Rate Analysis\nOne must be very careful in developing and using corpora for\nstatistical approaches to NLP. One study has shown that nearly\n35% of words used in a standard newspaper in English are not\navailable in a good printed dictionary and similarly, a large per-\ncentage of words listed in the dictionary are never used in the\nnewspaper. Systems which give good performance on one corpus",
    "35% of words used in a standard newspaper in English are not\navailable in a good printed dictionary and similarly, a large per-\ncentage of words listed in the dictionary are never used in the\nnewspaper. Systems which give good performance on one corpus\nhave often been found to fail on other corpora. Corpus provides\nan extremely useful data resource for all our investigations but\none must be careful in coming to conclusions.\nText Corpora\nTypes of Text Corpora:\nA plain text corpus is simply a collection of electronic text\n238\nCHAPTER 2. FOUNDATIONS OF NLP\ndocuments. The collection may be organized in ﬁles or folders\nand directories. Documents usually contain headers which include\nmeta-data such as title, author, publisher, year of publication, and\nother relevant ﬁelds. Several text corpora including hundreds of\nmillions of words are now available for English. Plain text corpora\nin all the major Indian languages were developed with the support\nof the Department of Electronic (now Department of Information\nTechnology), Government of India and distributed by the Central\nInstitute of Indian Languages at Mysore.\nThese corpora have\nonly about 3 Million words. Recently, larger corpora are being\ndeveloped for many Indian languages. Telugu has a text corpus\nof nearly 38 Million words as of this writing.\nA POS-tagged corpus is a text corpus where each word is\ntagged with the appropriate POS tag. We have already seen that\nwords may belong to several possible POS categories and the dic-\ntionary only lists all the possible categories. The correct POS tag\nfor a given word can only be ascertained from the context where\nit is used. We have seen how POS tagging can be performed using\ntechnologies such as HMMs. Large POS tagged corpora exist for\nEnglish but very little is available for Indian languages.\nA sense-tagged corpus is one in which every word is tagged\nwith the appropriate sense of the word. We have seen that POS\ntags can disambiguate between gross diﬀerences in meaning. To",
    "English but very little is available for Indian languages.\nA sense-tagged corpus is one in which every word is tagged\nwith the appropriate sense of the word. We have seen that POS\ntags can disambiguate between gross diﬀerences in meaning. To\nget a ﬁner distinction among word senses, we need to get into word\nsense disambiguation (WSD). WSD systems need sense-tagged\ncorpora for training. Only limited sense tagged corpora are avail-\nable even for English. The WSD problem remains largely unsolved\ntoday but if large scale sense tagged corpora could somehow be\ngenerated, it may be possible to develop generic high performance\nWSD systems.\nA parsed corpus has sentences that have been syntactically\nanalyzed or parsed using a suitable grammar formalism. The tree\nstructures or other kinds of descriptions of syntactic structures\nare explicitly shown in the corpus. A parsed corpus can be used,\nfor example, to automatically learn grammars using statistical\ntechniques. Small scale parsed corpora are available for English.\nAs we have already seen, there is no computational grammars or\nparsers for Indian languages as yet.\nFull syntactic parsing has proved to be diﬃcult and many\ntimes we resort to shallow or partial parsing. We may just group\n2.3. STATISTICAL APPROACHES\n239\nwords into phrases or chunks. Thus a shallow-parsed corpus will\nexplicitly depict some aspects of syntax but not in full. There is\nhope that large scale shallow parsed corpora can be developed for\nIndian languages using a judicious combination of linguistic and\nstatistical approaches.\nParallel corpora, including texts from one language and equiv-\nalent texts in another language, have been found to be very useful\nfor a variety of applications. Both monolingual and bilingual ap-\nplications can beneﬁt from a parallel corpus. Dictionaries, mor-\nphological analyzers, POS taggers, WSD systems, machine trans-\nlation systems can all beneﬁt from parallel corpora, especially if",
    "for a variety of applications. Both monolingual and bilingual ap-\nplications can beneﬁt from a parallel corpus. Dictionaries, mor-\nphological analyzers, POS taggers, WSD systems, machine trans-\nlation systems can all beneﬁt from parallel corpora, especially if\nthere is a good one-to-one correspondence between the text units\nin the two languages. When such correspondences are weak, we\nprefer the term similar corpora.\nSince the units of texts in the two languages in a parallel cor-\npus may not match one to one in a very straight-forward manner,\nwe need to align the corresponding units of text, be they sentences\nor chunks or words. A parallel corpus becomes very useful once\nit is aligned.\nStatistical techniques for alignment exist but for\ngenerating high quality training corpora, some manual eﬀort is\ninevitable.\nDeveloping Text Corpora:\nText corpora can be developed by typing in printed texts,\nusing OCR or through speech recognition. OCR and speech tech-\nnologies are far from perfect, especially for Indian languages and\nthe only workable method is to key in texts as of today. Of course\none may also make use of texts already available in electronic\nforms such as newspapers. This involves downloading as well as\nformat conversion since many web pages in Indian languages to-\nday are not encoded in any standard text encoding standard such\nas ISCII or UNICODE. It can be expected that fairly large quanti-\nties of plain text corpora will become available soon for the major\nlanguages of India. A more critical question is whether these cor-\npora are balanced.\nIt is often not possible to develop corpora\nbased on pre-set criteria and pre-selected genres simply because\nnot much is available in those genres. You will ﬁnd any amount\nof material on literature but hardly any on scientiﬁc and technical\ndomains. Thus there is a tendency to put together whatever is\n240\nCHAPTER 2. FOUNDATIONS OF NLP\navailable and call it a corpus. Such corpora may not be balanced",
    "not much is available in those genres. You will ﬁnd any amount\nof material on literature but hardly any on scientiﬁc and technical\ndomains. Thus there is a tendency to put together whatever is\n240\nCHAPTER 2. FOUNDATIONS OF NLP\navailable and call it a corpus. Such corpora may not be balanced\nand must be used with great care.\nIndian languages are characterized by rich morphology and\nrelatively free word order. Hence sequence based techniques such\nas HMMs may not be the most suitable for POS tagging of Indian\nlanguage corpora. Also, since there are so many aspects that go\ninto individual word tokens, the design of the tag-sets is itself a\nvery complex task. Too gross a classiﬁcation (such as into the\nbasic grammatical categories) may not be good enough and too\nﬁne grained a classiﬁcation would make POS tagging heavily de-\npendent on morphology. If there are 1,80,000 word forms derived\nfrom a single verb root in Telugu, should we have so many tags\nthen? That would not make much sense. Perhaps a hierarchical\ntagging scheme where we can work at various grain sizes in stages\nmay be a good idea. POS tagged corpora are not yet available in\na big measure in Indian languages today.\nThere are no computational grammars and hence no syntactic\nparsers for any of the Indian languages. The question of a parsed\ncorpus therefore does not arise. Similarly, we do not have a clear\nidea about the number and nature of word sense distinctions that\nmay have to be made. There are no dependable sources for word\nsenses and dictionaries vary quite widely. There is no sense tagged\ncorpora. We have a long way to go.\nWhen we say so and so thing does not exist, we mean large\nscale, properly designed, thoroughly tested, proven, publicly avail-\nable solutions do not exist.\nThere may be some half-hearted,\nshort-sighted, ad-hoc, hoch-poch implementations and student\nprojects here and there. What is the use of such toy, demo sys-\ntems?\nCorpus based approaches require a number of tools for devel-",
    "able solutions do not exist.\nThere may be some half-hearted,\nshort-sighted, ad-hoc, hoch-poch implementations and student\nprojects here and there. What is the use of such toy, demo sys-\ntems?\nCorpus based approaches require a number of tools for devel-\nopment, maintenance and usage. For example, a KWIC concor-\ndance tool locates lines containing a given word so that we can get\nan idea about the usage and range of senses for that word. There\nare tools for formatting, veriﬁcation and validation and for ana-\nlyzing in a variety of ways. Tools for alignment of parallel copora\nare also being developed. Several such tools have been developed\nspeciﬁcally for Indian languages by various centres in India. Now\ntools exist for decoding any unknown font for converting the texts\ninto a standard character encoding scheme such as ISCII or UNI-\nCODE. These tools will help accelerate the development of corpus\n2.3. STATISTICAL APPROACHES\n241\nbased technologies for Indian languages.\nSpeech Corpora:\nThere is an increased interest in speech technologies in recent\ntimes in India. Unlike in the case of text, simply recording and\ndigitizing speech will not constitute a corpus. That is not of much\nuse. A speech corpus needs to be segmented and labelled at the\nlevel of sentences, words and sub-word units such as phones. This\nlatter job is more complex, more time consuming and more tedious\nthan just recording and digitizing. Also, since a speech signal is a\ncomposite of speech message, speaker’s voice, environmental and\nsystem noise etc., a careful technological consideration of a variety\nof factors is essential to build a useful speech corpus. Even simple\nthings like the microphone used, the location and direction of the\nmicrophone with respect to the speaker’s mouth and the recording\nroom conditions can be critical factors. While several centres are\nworking on speech technologies in India, publicly useable speech\ncorpora are not yet available for any of our languages.",
    "microphone with respect to the speaker’s mouth and the recording\nroom conditions can be critical factors. While several centres are\nworking on speech technologies in India, publicly useable speech\ncorpora are not yet available for any of our languages.\nSpeech corpora meant for speech recognition are generally\nrecorded under conditions similar to the conditions where the\nrecognition system may ﬁnally be used, say in an ordinary of-\nﬁce room or at a railway station. Speaker independent systems\nrequire speech data from a large number of speakers of diﬀer-\nent age, sex, speaking rates and styles etc. Spontaneous speech,\nnatural, continuous but carefully articulated speech of coopera-\ntive speakers, or just read speech may be recorded depending on\nneed. For speech synthesis purposes, data from a single speaker\nor a few selected speakers (one male, one female, for example) are\nrecorded under noise-free conditions as in a studio. The future lies\nin speech. we will surely see more and more of speech technologies\nin future.\n2.3.2\nStatistical Approaches to Language\nWe may have diﬃculties in accepting a statistical orientation for\nhuman languages. We feel languages are rule governed and things\nare not a matter of chance or probability. How can simply some\nnumbers characterize something so complex yet highly structured\n242\nCHAPTER 2. FOUNDATIONS OF NLP\nobject as language?\nHow can we interpret numbers and make\nsense out of them? Language manifests regularities and follows\nrules, either it is OK or it is not OK, where is the question of\ngradations? Everything must be either right or wrong, either true\nor false, either zero or one. Where does probability come from?\nLinguistics is all about possibilities. The principles and rules\nof linguistics rule out the impossible combinations and allow valid\ncombinations. The aim is to build theories and models so that all\nand only valid structures can be accepted.\nWe must immediately realize that the notion of probability",
    "of linguistics rule out the impossible combinations and allow valid\ncombinations. The aim is to build theories and models so that all\nand only valid structures can be accepted.\nWe must immediately realize that the notion of probability\nincludes the notion of possibility. If something is impossible it\nwill have a probability of zero, else a non-zero value between zero\nand one. Thus nothing is lost in moving from possibility based\nview to a probability based view. In fact we gain a higher degree\nof control because now we can handle gradations of possibilities\n- from the most likely to the least. Thus at a ideological level,\nthere need not be any concern in applying statistical techniques\nto human languages.\nThen we must realize that languages do show gradations. It\nis not all a matter of yes or no, true or false, right or wrong, zero\nor one. Let us see some examples of gradation in language. We\nhave nouns and we have verbs. Nouns are things and verbs are\nactions or states. Simple, right? Not quite. We also have things\nthat are partly ’nouny’ and partly ’verby’. Look at the following\nexamples:\n1. He saw the accident. He fainted.\n2. He saw the accident and he fainted.\n3. After he saw the accident, he fainted.\n4. After seeing the accident he fainted.\n5. On seeing the accident he fainted.\n6. At the sight of the accident he fainted.\nFIG 2.14 Gradations between Nouns and Verbs\nHere saw is clearly a verb and sight is clearly a noun. seeing is\n2.3. STATISTICAL APPROACHES\n243\nin between - it is a gerund, a verb that acts like a noun, it ﬁlls the\nthematic roles that are normally taken by nouns but it shows up\nits expectations about objects etc. Seeing requires something to\nbe seen and somebody who sees. Thus the accident which is the\nobject of the verb initially, has gradually changed into the object\nof the prepositional phrase indicated by of. After is a preposition\nbut it is also a subordinate conjunction. In comparison, on can",
    "be seen and somebody who sees. Thus the accident which is the\nobject of the verb initially, has gradually changed into the object\nof the prepositional phrase indicated by of. After is a preposition\nbut it is also a subordinate conjunction. In comparison, on can\nonly be a preposition. We have gradually moved from a verby\nstructure to a nouny structure. There are gradations in language.\nThis is just one example to prove this point.\nIn fact lexical categories are not completely rigid and ﬁxed -\nthey vary in usage. Although we may say initially that nouns are\nobjects and adjectives are attributes of such objects, languages\npermit use of nouns as adjectival modiﬁers.\nIn fact adjectives\ncan also function as nouns. In the poor envy the rich, the “poor”\nstands for the poor people and “the rich” stands from the rich\npeople. Thus even at the level of lexical categories, things are not\nvery hard and fast.\nIf basic aspects such as lexical categories can show variations\nand gradations in usage, so will all the complex structures at\nhigher levels of analysis. Grammaticality of sentences is not al-\nways a simple yes/no questions. Syntacticians are familiar with\nthe “question mark” judgements by native speakers of a given\nlanguage. It is very diﬃcult to formulate purely linguistic rules\nfor part-of-speech tagging for positional languages such as English.\nOur treatment of semantics should make it amply clear that prob-\nlems in semantics are much more insidious and nebulous. It would\nnot be easy to develop rules for word sense disambiguation purely\nbased on linguistic constraints. Although languages show elab-\norate structure, it is not true that everything is completely rule\ngoverned.\nRecent studies have shown that even small babies use statisti-\ncal methods when they learn to speak and understand language.\nThe linguistic inputs the child gets are too few and too imperfect\nfor kids to learn symbolic rules. On the other hand, statistical",
    "governed.\nRecent studies have shown that even small babies use statisti-\ncal methods when they learn to speak and understand language.\nThe linguistic inputs the child gets are too few and too imperfect\nfor kids to learn symbolic rules. On the other hand, statistical\ndecision rules based on probabilities can be obtained even from\nsmall scale and imperfect data and gradually reﬁned and revised\nas more and more data becomes available. Contrary to what many\nlinguists believe, statistical techniques seem to have psychological\nreality too.\n244\nCHAPTER 2. FOUNDATIONS OF NLP\nLinguists are usually enthused by the quick identiﬁcation of a\nfew prominent rules that can account for a major part of linguis-\ntic phenomena. But as you expand the scope and look at more\nand more data, rules gradually give way to sub-rules and then\nto exceptions and ﬁnally things become too unwieldy. The 80-20\nrule is largely true with human languages - 80% of the data can\nbe handled with just 20% of the eﬀort but the remaining 20%\nrequire much more eﬀort. This rule applies recursively and it be-\ncomes harder and harder to cross beyond a level. Such saturation\nof linguistic approaches has been clearly observed in many appli-\ncations. Statistical approaches based on large and representative\ndata have many times crossed the point of linguistic saturation\nand hence the interest in these statistical approaches.\nIt is also not true that purely statistical approaches always\nwork very well. Machine learning has largely been limited to gen-\neralization from examples and there are limits on generalizability.\nOften data required becomes too large to be practicable. The en-\ngineering challenge is to use a judicious combination of linguistic\nand statistical approaches to achieve the maximum possible with\nminimum eﬀort.\nThere are several ways we can integrate linguistic and statis-\ntical approaches in a hybrid architecture. Statistical methods can\nbe applied ﬁrst and linguistics used later as a ﬁlter to rule out",
    "and statistical approaches to achieve the maximum possible with\nminimum eﬀort.\nThere are several ways we can integrate linguistic and statis-\ntical approaches in a hybrid architecture. Statistical methods can\nbe applied ﬁrst and linguistics used later as a ﬁlter to rule out\ncombinations that are impossible. Alternatively, linguistic meth-\nods can be applied ﬁrst and statistics used later only for rating\nand raking the possible outputs. Linguistics aims to deal with\nall and only valid constructs and it if often very diﬃcult to take\ncare of both these requirements at the same time. There may be\nseveral right answers but the wrong answers can be simply too\nmany. How do we rule out all of them? So it makes sense to\nrelax one of these constraints initially. For example, a linguistic\nmethod which captures all valid structure but does not necessar-\nily bar all invalid structures is often much easier to construct. If\ninputs can be assumed to be correct in most cases, an assumption\nthat is generally valid in many applications, we can use the lin-\nguistic module to analyze all inputs without worrying about their\nvalidity, and then apply statistical methods to rate, rank and if\nrequired ﬁlter out unacceptable combinations. Apart from these\nloosely-coupled architectures, we can also think of tightly inte-\ngrated architectures. Linguistics often forms the main device for\n2.3. STATISTICAL APPROACHES\n245\nidentifying the features to be used. Statistical methods can then\nbe used for feature weighting, dimensionality reduction, etc.\n2.3.3\nMachine Learning\nIn Machine Learning approaches, a set of training data is given\nand the machine “learns” a general rule or builds a model for\nperforming the intended task. The learning is automatic - there\nwill be no manual intervention. If the training data is good and\neﬀective learning methods are used, good models can be learnt.\nMachine learning methods are basically techniques for gener-",
    "performing the intended task. The learning is automatic - there\nwill be no manual intervention. If the training data is good and\neﬀective learning methods are used, good models can be learnt.\nMachine learning methods are basically techniques for gener-\nalization from Examples. A typical way of using such machine\nlearning techniques is to automatically group together similar ob-\njects or to classify objects into diﬀerent classes or groups based\non the similarities and dissimilarities. Thus the notions of class\nand similarity or dissimilarity are fundamental. Sometimes the\nterm distance is also used to indicate a measure of dissimilar-\nity. Distance can be quantiﬁed in many ways. Think of the road\ndistance, the rail distance and as-the-crow-ﬂies distance between\ntwo points. In any case distances are expressed and quantiﬁed\nin terms the values of one or more features. Thus each object is\nrepresented as a vector of n features if we are working with n dif-\nferent features. Each such feature vector can be mathematically\nviewed as a point in n-dimensional space. Think of distances in\nthis n-dimensional space. If good features and appropriate dis-\ntance measures are used machine learning techniques can learn\neﬀective decision rules for classiﬁcation of given objects.\nFor example, to classify words as OK or not-OK in a spelling\nerror detection system we may use the probability of a word start-\ning with a given letter, the probabilities of two letters occurring\nnext to each other (also called bi-gram probabilities), the proba-\nbility of a word occurring before or after another given word etc.\nas features. These features can, for example, make a good guess\nthat the string “abnidella” is unlikely to be a valid English word\nwithout consulting a dictionary.\nThus machine learning is a purely data driven approach. The\ngreatest merit of this approach therefore is its generality and\nadaptability. All we need to migrate to a new language or a new",
    "that the string “abnidella” is unlikely to be a valid English word\nwithout consulting a dictionary.\nThus machine learning is a purely data driven approach. The\ngreatest merit of this approach therefore is its generality and\nadaptability. All we need to migrate to a new language or a new\napplication is to provide appropriate training data in that lan-\nguage or for that application. The machine unlearns and relearns\n246\nCHAPTER 2. FOUNDATIONS OF NLP\nto automatically to adapt to the new situation. Migrating from\none language to another using a linguistic approach, on the other\nhand, would necessitate extensive manual exploration of the new\nlanguage structures and properties.\nMachine learning can be supervised or unsupervised. In su-\npervised learning, a set of labeled training data is given and the\nmachine learns a general decision rule which can be used for clas-\nsiﬁcation of new data items. In unsupervised learning, a set of\nunlabeled training data is given and the machine learns to group\nsimilar data items into clusters so that new data items can be\nplaced into the right clusters. The number of clusters may or may\nnot be known beforehand.\nWe describe here a selection of machine learning techniques\nbrieﬂy. The purpose is to give an exposure to the basic ideas.\nInterested readers may consult books on machine learning and\npattern classiﬁcation for an in-depth exposition.\nRegression as Classiﬁcation\nRegression analysis is a statistical technique for investigating and\nmodeling the relationship between variables in a system. When\nthere are more than two variables in the system, the term multiple\nregression is employed. Regression is often used as a modeling\ntechnique where the value of one of the selected variables, called\nthe response variable, is determined by the values of the other\nindependent variables, also called the regressors. The modeling\nprocess basically involves determining parameters of the model,",
    "technique where the value of one of the selected variables, called\nthe response variable, is determined by the values of the other\nindependent variables, also called the regressors. The modeling\nprocess basically involves determining parameters of the model,\ni.e. the weights of the regressor variables. The model itself could\nbe linear or non-linear in the parameters. Regression distinguishes\nthe response variable from the regressors and is thus generally\nconsidered to be a non-symmetric technique.\nMultiple Regression can also be used as a two-class classiﬁca-\ntion tool. The regressor variables are the feature vectors extracted\nfrom the training data. Since we are using regression for classiﬁ-\ncation rather than for modeling, no particular feature is selected\nas a response variable or expressed in terms of the other features.\nWe posit a separate decision variable, whose value is determined\nby the class the instance belongs to. The method is thus symmet-\nric in the features. We give below the formulation of the Multiple\nLinear Regression as a classiﬁcation technique.\n2.3. STATISTICAL APPROACHES\n247\nSuppose there are k features. Let xij denote the ith observa-\ntion of feature xj where i = 1,2,...,n and j = 1,2,...,k. Let yi be\nthe ith observed value of the decision variable. Then\nyi = β0 + β1xi1 + β2xi2 + ... + βkxik + εi\n(2.1)\nwhere the parameters βj, j = 0,1,2,...,k are called regression\ncoeﬃcients and εi are called error terms or residuals. The regres-\nsion coeﬃcients are the parameters in the model. Note that the\nequation is linear in the parameters. The aim is to estimate the\nvalues of these parameters from training data. In matrix notation,\nwe have\ny = X β + ε\n(2.2)\nwhere y is an n×1 vector of observations, X is an n×p matrix of\nfeature values, where p is k + 1, β is p×1 vector of the regres-\nsion coeﬃcients, and ε is an n×1 vector of error terms. We may\nestimate the values of the parameters ˆβ using the least square\nmethod. That is, we wish to minimize\nS(β) =\nn",
    "feature values, where p is k + 1, β is p×1 vector of the regres-\nsion coeﬃcients, and ε is an n×1 vector of error terms. We may\nestimate the values of the parameters ˆβ using the least square\nmethod. That is, we wish to minimize\nS(β) =\nn\nX\ni=1\nε2\ni = ε′ε = (y −Xβ)′(y −Xβ)\n(2.3)\nThe least squares estimators must satisfy\n∂S\n∂β |β = −2X′y + 2X′Xˆβ = 0\n(2.4)\nwhich can be simpliﬁed as\nˆβ = (X′X)−1X′y\n(2.5)\n(X′X)−1 exists provided the features are linearly independent.\nIn order to determine the parameters, we need to know the\nvalue of the decision variable on the left hand side of the regression\nequation. Since the decision variable is not a feature in the system\nbut an additional variable introduced to indicate the class for the\npurposes of classiﬁcation, the value of the decision variable can be\nchosen arbitrarily subject to the following constraints. In order to\nensure adequate separation between the two classes, the values for\nthe two classes must be clearly separated. Also, the choice of the\nvalues for the decision variable inﬂuences the range of values for\n248\nCHAPTER 2. FOUNDATIONS OF NLP\nthe parameters - the value chosen must result in reasonable ranges\nof values for the parameters, avoiding overﬂows and underﬂows in\nthe extreme. Finally, choice of symmetric values for the two classes\nin the two-class case makes the decision rule and thresholding for\nrejection simpler. In practice the values are decided after a bit of\nexperimentation with the actual data on hand.\nFor the two-class classiﬁcation problem, we may use diﬀer-\nential features - actual value of each feature is calculated as the\ndiﬀerence between the values of the feature for the two classes.\nValues of the decision variable for the two classes are chosen sym-\nmetrically around zero and the parameters are estimated from the\ntraining data. A test sample can then be classiﬁed as belonging to\nclass C1 or C2 depending upon whether the value of the decision",
    "Values of the decision variable for the two classes are chosen sym-\nmetrically around zero and the parameters are estimated from the\ntraining data. A test sample can then be classiﬁed as belonging to\nclass C1 or C2 depending upon whether the value of the decision\nvariable is positive or negative. It is possible to reject a point if\nthe value of the decision variable is too close to zero, say, closer\nthan a speciﬁed threshold.\nClassiﬁcation performance can be speciﬁed in terms of Pre-\ncision and Recall, or using some combined measure such as the\nF-Measure:\nRecall =\nOk\nTotal ∗100\n(2.6)\nPrecision =\nOk\nTotal −Unknown ∗100\n(2.7)\nF = 2PR\nP + R\n(2.8)\nwhere Ok is the number of test samples that are correctly classi-\nﬁed, Unknown is the number of test samples that are not classiﬁed\nand Total is the total size of the test data. There is usually a trade\noﬀbetween Precision and Recall and a single combined measure is\ntherefore useful for comparison. F-Measure is one such measure.\nThe deﬁnition shown here gives equal weightage for Precision and\nRecall.\nWe have outlined a general method for supervised two-class\nclassiﬁcation using Multiple Linear Regression.\nThe method is\nconceptually simple and based on sound theoretical foundations.\nThe method is symmetric in the features. Although matrix inver-\nsion is required for estimating the values of the parameters, once\n2.3. STATISTICAL APPROACHES\n249\nthe model is built classifying objects is very eﬃcient - only com-\nputation of the linear regression equation and checking the sign\nof the decision variable are required. The technique is thus highly\nsuitable for two-class classiﬁcation problems with a reasonably\nsmall number of features.\nTechniques also exist for validating the adequacy of the model\nfor a given problem and for evaluating the relative signiﬁcance of\nthe various features (which can be used for feature selection).\nWe will illustrate the use of regression as a classiﬁcation tool",
    "small number of features.\nTechniques also exist for validating the adequacy of the model\nfor a given problem and for evaluating the relative signiﬁcance of\nthe various features (which can be used for feature selection).\nWe will illustrate the use of regression as a classiﬁcation tool\nfor identifying language from small text samples in section ???\nbelow.\nNearest Neighbour Classiﬁers\nOne of the conceptually simple and practically quite eﬀective clas-\nsiﬁcation methods is based on the notion of nearest neighbours.\nOnce a collection of objects have been placed in a space of possible\nfeature values, objects of the same kind tend to group together\nprovided the features used are appropriate.\nBirds of the same\nfeather ﬂock together. If you can identify the type of the sur-\nrounding birds, you can guess the type of the bird on hand quite\naccurately. This is the basic idea behind nearest neighbour clas-\nsiﬁcation.\nThe feature values are computed and the k nearest neighbours\nare determined using a suitable distance measure. The value of\nk can be either simply assumed to be some small number, say\n5, or the best value of k can be experimentally determined after\ntrying out several values. The categories of each of nearest neigh-\nbours is checked and the object on hand is assigned the same cat-\negory as the category of the majority of the k-nearest neighbours.\nNote that all the k-nearest neighbours may not be equally distant\nfrom the candidate object. To take the distances of the neigh-\nbours into account, distance-weighted decision rules have been\nproposed. Also, the k-neighbours may or may not show a clear\nmajority. Several advanced ideas have been proposed to improve\nthe classiﬁcation performance.\nNote that each time we need to classify a given object, the k\nnearest neighbours will have to be computed. This involves com-\nputing the distances to all the objects in the collection. Therefore\nthis technique is an instance based classiﬁcation technique.\n250",
    "the classiﬁcation performance.\nNote that each time we need to classify a given object, the k\nnearest neighbours will have to be computed. This involves com-\nputing the distances to all the objects in the collection. Therefore\nthis technique is an instance based classiﬁcation technique.\n250\nCHAPTER 2. FOUNDATIONS OF NLP\nNearest neighbour classiﬁcation techniques work quite well in\nmany application areas and are widely used.\nBayesian Learning Theory\nBayesian Learning is a probabilistic approach to inference based\non the assumption that the quantities of interest are governed by\nprobability distributions and the optimal decision can be made by\nreasoning about these probabilities together with observed data.\nIn corpus based approaches, probabilities are estimated by\ncounting the frequencies of favourable cases and all possible cases\nand taking the ratio of the two.\nFor example, the number of\ndocuments belong to a given category divided by the total number\nof documents would give the probability that a document belongs\nto the speciﬁed category. Thus the probabilities we are talking\nhere are empirical probabilities.\nP(X) denotes the probability of some event X. P(X, Y ) de-\nnotes the joint probability of both the events X and Y occurring\ntogether. P(X|Y ) denotes the probability of X given that Y has\nalready occurred. It is thus a conditional probability. P(X|Y )\ncan be computed as P(X,Y )\nP(Y ) . Similarly, P(Y |X) is P(Y,X)\nP(X) .\nSince P(X, Y ) and P(Y, X) are the same, we can combine these\nlast two equations to get\nP(X|Y ) = P(Y |X)∗P(X)\nP(Y )\nThis is known as Bayes Theorem. To understand Bayes The-\norem let us replace X by “disease” and Y by “symptoms”. Then\nwe get:\nP(disease|symptoms) = P(symptoms|disease)∗P(disease)\nP(symptoms)\nWe are interested in computing the probability that a patient\nhas a particular disease given the symptoms he has. For exam-\nple, we may wish to compute the probability that a patient has",
    "we get:\nP(disease|symptoms) = P(symptoms|disease)∗P(disease)\nP(symptoms)\nWe are interested in computing the probability that a patient\nhas a particular disease given the symptoms he has. For exam-\nple, we may wish to compute the probability that a patient has\nviral fever given that he has head-ache. There is usually no direct\nway to compute this probability. Head-ache can be because of vi-\nral fever or many other possible causes including teeth pain or eye\nstress. Bayes Theorem expresses this probability in terms of other\n2.3. STATISTICAL APPROACHES\n251\nquantities which are usually easier to estimate. Look at the right\nhand side of the equation. It is possible to estimate the probability\nof observing given symptoms given that a patient has the speciﬁed\ndisease. The fraction of the times a patient suﬀering from viral\nfever has head-ache gives the probability of observing head-ache\ngiven viral fever. This probability is called the likelihood. The\nother term in the numerator is called prior probability. Probabil-\nity that any patient has a speciﬁed disease is simply based on the\nknowledge of how rampant that disease is at the given place and\ntime. If viral fever is rampant, this knowledge will and should\ninﬂuence the ﬁnal diagnosis.\nSince this knowledge exists even\nbefore the doctor examines the patient and notes down the symp-\ntoms, it is called prior knowledge. The diagnosis will be based\non the combination of likelihood and prior knowledge - any one\nof them is not as good as the combination. The diagnosis will be\ncorrect to the extent that the symptoms are observed in patients\nwith the given disease and to the extent that the disease itself is\nrampant. The denominator gives the probability of observing the\nsymptoms without regard to any speciﬁed disease. If head-ache\nis a very common symptom then you will be less sure of your\ndiagnosis based on this symptom. On the other hand, if yellow\nurine occurs quite rarely, then using this symptom to diagnose,",
    "symptoms without regard to any speciﬁed disease. If head-ache\nis a very common symptom then you will be less sure of your\ndiagnosis based on this symptom. On the other hand, if yellow\nurine occurs quite rarely, then using this symptom to diagnose,\nsay, jaundice, would be more safe. The probability on the left had\nside of the equation is called the a posteriori probability- it is the\nprobability obtained after suitably reﬁning the prior knowledge\nbased on the observation of speciﬁed symptoms. The beauty of\nBayes theorem is it combines all these aspects into a single equa-\ntion, based on sound mathematical theory, yet very practical for\nestimating probabilities of interest.\nNote that a diagnosis is usually not made based on just one\nsymptom. When we say “symptoms” in the equation above, we\nactually mean one or more symptoms. The probabilities are thus\njoint probabilities of observing all the given symptoms simultane-\nously. What is the probability that the patient has jaundice given\nthat he has yellow eyes, yellow urine, liver enlargement, nausea\nand weakness?\nIn Bayesian Learning methods a maximum a posteriori (MAP)\nprobability is computed using the Bayes Theorem. We use Bayes\nTheorem to estimate the probability of each of the possible classes\ngiven the observed features and select the class for which this\n252\nCHAPTER 2. FOUNDATIONS OF NLP\nposterior probability is the highest. Note that the denominator\non the right hand of the equation will be independent of the class,\nand hence it can be ignored as far as the aim is to ﬁnd the class\nwith the maximum a posteriori probability.\nIn some cases, the prior probabilities of all the hypotheses are\nassumed to be uniform and hence bracketed out. This assumption\nof uniform priors is questionable and has led to criticism of the\nBayesian approaches.\nBayesian method requires the estimation of joint probabilities\nof all the features for each category.\nIn order to simplify this,",
    "assumed to be uniform and hence bracketed out. This assumption\nof uniform priors is questionable and has led to criticism of the\nBayesian approaches.\nBayesian method requires the estimation of joint probabilities\nof all the features for each category.\nIn order to simplify this,\nindependence is often assumed. That is, the conditional probabil-\nity of a feature given a category is assumed to be independent of\nthe conditional probabilities of other features given that category.\nA Bayesian classiﬁer that makes this independence assumption is\ntermed a Naive Bayes Classiﬁer. The Independence assumption\nis rarely valid in real world. Yet the method works quite well and\nis used in practice.\nProbabilities are numbers less than one and multiplying prob-\nabilities makes the numbers smaller and smaller. Form an imple-\nmentation point of view, therefore, it is useful to take logarithms.\nLogarithms also convert multiplications to additions.\nIn order to understand how Bayesian learning actually works,\nlet us apply this to the Text Categorization problem. The basic\nidea is to use the joint probabilities of document terms and cate-\ngories to estimate the probabilities of categories given a document.\nTo categorize a test document dj as belonging to a category Ci,\nthe maximum likelihood is ﬁrst estimated over all categories:\nP(dj|Ci) =\nX\nw∈dj\nlog(P(w|Ci))\n(2.9)\nHere w is a word or a term occurring in a given document.\nThe basic idea is that words occur with varying frequencies in\ndocuments of diﬀerent categories. No single word may be good\nenough to make a decision but the combined eﬀect of all the words\nin the document collection may be good enough to give correct\nclassiﬁcation.\nThe prior probabilities of each category Prior(Ci) are evalu-\nated as the ratios of the number of documents in category Ci to\nthe number of documents in the total collection.\n2.3. STATISTICAL APPROACHES\n253\nFinally, the posterior probabilities of each category are calcu-",
    "classiﬁcation.\nThe prior probabilities of each category Prior(Ci) are evalu-\nated as the ratios of the number of documents in category Ci to\nthe number of documents in the total collection.\n2.3. STATISTICAL APPROACHES\n253\nFinally, the posterior probabilities of each category are calcu-\nlated by adding the log likelihoods to the log priors.\nP(Ci|dj) = log(P(dj|Ci)) + log(Prior(Ci))\n(2.10)\nA test document is assigned the category with the maximum\nposterior probability.\nTo minimize misclassiﬁcation errors due\nto narrow diﬀerences, a threshold value can be used to include\na reject option. Performance can then be measured in terms of\nPrecision, and Recall. In order to capture the Precision-Recall\ntrade-oﬀin a single quantity, a combined measure such as the\nF-measure can be used.\nUnsupervised Learning: Clustering\nWhen we have training data where the instances are not labelled\nfor their correct class, we can still try to group the instances based\non their similarities and dissimilarities. Similarity or dissimilarity\nis measured in terms of features and a suitable distance measure.\nWhat we do is to group together instances that are similar and\nhence close to one another. The idea is to group instances into\nclusters such that the intra-cluster distances are smaller and the\ninter-cluster distances are larger.\nThis is kind of unsupervised\nlearning is called clustering.\nThere are several interesting clustering algorithms but we shall\nsketch only one of them here - the K-Means Clustering Algorithm.\nThe idea is simple. Let us say we have set of instances and we wish\nto group them into K classes. K is assumed to be known. We start\nby arbitrarily picking up K items from the given set of instances\nand treating them as the representatives of the K clusters to be\ndiscovered. Now the distances of each of the instances to these\nK cluster centres are computed and each instance is assigned to\nthe cluster it is closest to. Once this is done, the centroids of the",
    "and treating them as the representatives of the K clusters to be\ndiscovered. Now the distances of each of the instances to these\nK cluster centres are computed and each instance is assigned to\nthe cluster it is closest to. Once this is done, the centroids of the\nclusters are recomputed and they will become the representatives\nof the K clusters for the next iteration. The process is repeated.\nIt can be observed that with each iteration, instances which are\nsimilar tend to get grouped together and after some iterations,\nno instance changes over to another cluster. A stable grouping of\ninstances has been obtained. Even if the initial choice of the K\nrepresentatives was not very good, because we assign instances to\n254\nCHAPTER 2. FOUNDATIONS OF NLP\nnearest clusters and recompute the centroids, we can hope to see\nnatural clusters being formed.\nMarkov Models\nDeﬁnition: Consider a system which may be described at any\ntime as being in one of a set of N distinct states, s1, s2, . . . , sN. At\nregularly spaced discrete times, the system undergoes a change of\nstate (possibly back to the same state) according to a set of prob-\nabilities associated with the state. We denote the time instants\nassociated with state changes as t = 1, 2, . . . , and we denote the\nactual state at time t as qt.\nA full probabilistic description of\nthe above system would, in general, require speciﬁcation of the\ncurrent state (at time t), as well as all the predecessor states. An\nexample of Markov model is shown in the Figure 2.15.\n>\n>\n<\n \n>\n>\n>\n<\n<\n<\n>\n>\n>\n>\n<\na\na\na\na\na\na\na\na\na\na\na\na\na\na\ns\ns\ns\ns\ns\n22\n11\n33\n44\n55\n21\n32\n34\n45\n54\n51\n13\n41\n35\n1\n2\n3\n4\n5\nFIG 2.15 A Markov chain with 5 states with selected state\ntransitions\nFor the sake of mathematical and computational tractability,\nfollowing assumptions are made in the theory of Markov models.\nThe Markov assumption: It is assumed that the current\nstate always depends only on preceding state. This is called the\n2.3. STATISTICAL APPROACHES\n255",
    "transitions\nFor the sake of mathematical and computational tractability,\nfollowing assumptions are made in the theory of Markov models.\nThe Markov assumption: It is assumed that the current\nstate always depends only on preceding state. This is called the\n2.3. STATISTICAL APPROACHES\n255\nﬁrst order Markov assumption and the resulting model obtained\nis called ﬁrst order Markov model. In this case of a discrete, ﬁrst\norder, Markov chain, the probabilistic description is truncated to\njust the current and the predecessor state, i.e.,\nP[qt = j|qt−1 = i, qt−2 = k, . . .] = P[qt = j|qt−1 = i]\n(2.11)\nHowever generally the current state may depend on past kstates\nand it is possible to obtain such a model, called a kth order Markov\nmodel by deﬁning the transition probabilities as follows.\nP[qt = j|qt−1 = jt−1, qt−2 = jt−2, . . .]\n=\nP[qt = j|qt−1 = j, qt−2 = jt−2, . . . , qt−k = jt−k] (2.12)\nIn any case we only peep a limited distance into the past his-\ntory. Hence this assumption is also known as the limited horizon\nassumption.\n• State transition matrix A is:\nA =\n\n\na11\na12\n· · ·\na1j\n· · ·\na1N\na21\na22\n· · ·\na2j\n· · ·\na2N\n...\n...\n· · ·\n...\n· · ·\n...\nai1\nai2\n· · ·\naij\n· · ·\naiN\n...\n...\n· · ·\n...\n· · ·\n...\naN1\naN2\n· · ·\naNj\n· · ·\naNN\n\n\nwhere,\naij = P(qt = j|qt−1 = i)\n1 ≤i, j ≤N\nThe state transition coeﬃcients will have the properties:\naij ≥\n0,\n∀i, j\n(2.13)\nN\nX\nj=1\naij =\n1,\n∀i\n(2.14)\nThe stationarity assumption: It is also assumed that state\ntransition probabilities are independent of the actual time at which\nthe transitions take place. Mathematically,\n256\nCHAPTER 2. FOUNDATIONS OF NLP\nP(qt = j|qt−1 = i) = P(qt+l = j|qt+l−1 = i).\nThe above stochastic process could be called an observable\nMarkov model since the output of the process is the set of states\nat each instant of time, where each state corresponds to a physical\n(observable) event.\nExamples: Consider a simple 3-state Markov model λ of the\nweather. Assuming that once a day (e.g. at noon), the weather is",
    "Markov model since the output of the process is the set of states\nat each instant of time, where each state corresponds to a physical\n(observable) event.\nExamples: Consider a simple 3-state Markov model λ of the\nweather. Assuming that once a day (e.g. at noon), the weather is\nobserved as being one of the following:\nstate 1: Rainy(R)\nstate 2: Cloudy(C)\nstate 3: Sunny(S)\nLet the A matrix be\nA =\n\n\n0.4\n0.3\n0.3\n0.2\n0.6\n0.2\n0.1\n0.1\n0.8\n\n\nWe could now compute the probability of any given sequence\nunder the model. For example, the probability of the sequence:\nO = (S, S, S, R, R, S, C, S) can be computed as follows:\nP(O|λ)\n=\nP(S, S, S, R, R, S, C, S|model)\n=\nP(S|S, S, S, R, R, S, C)P(S, S, S, R, R, S, C)\n(by cond. prob. rule)\n=\nP(S|C)P(S, S, S, R, R, S, C)\n(by I order Markov property)\n...\n=\nP(S)P(S|S)P(S|S)P(R|S)P(R|R)\nP(S|R)P(C|S)P(S|C)\n=\nπ3a33a33a31a11a13a32a23\n=\n(1)(0.8)2(0.1)(0.4)(0.3)(0.1)(0.2)\n=\n1.536X10−4\nwhere we use the notation πi = P(q1 = i) to denote the initial\nstate probabilities. Here we have assumed π to be 1 but in general\nthe π values can be speciﬁed for each state.\n2.3. STATISTICAL APPROACHES\n257\nHidden Markov Models\nSo far we have considered Markov models in which each state cor-\nresponded to an observable (physical) event. Such a model is too\nrestrictive to be applicable to many problems of interest. In the\ncase of Hidden Markov Models the observations are probabilistic\nfunctions of the states i.e., the resulting model is a doubly em-\nbedded stochastic process with an underlying stochastic process\nthat is not observable (hidden), but can only be observed through\nanother set of stochastic processes that produce the sequence of\nobservations.\nA Hidden Markov Model is a ﬁnite set of states, each of which\nis associated with a probability distribution. Transitions among\nthe states are governed by a set of probabilities called transition\nprobabilities. In a particular state an outcome or observation can",
    "observations.\nA Hidden Markov Model is a ﬁnite set of states, each of which\nis associated with a probability distribution. Transitions among\nthe states are governed by a set of probabilities called transition\nprobabilities. In a particular state an outcome or observation can\nbe generated, according to the associated probability distribution.\nIt is only the outcome, not the state, which is visible to an exter-\nnal observer. The states are “hidden”. Hence the name Hidden\nMarkov Model.\nUrn and Ball Model: Assume that there are N glass urns\nin a room. Within each urn there are a large number of colored\nballs. We assume that there are M distinct colors of the balls.\nThe physical process for obtaining observations is as follows. A\ngenie is in the room, and according to some random process, he\n(or she) chooses an initial urn. From this urn, a ball is chosen at\nrandom, and its color is recorded as the observation. The ball is\nthen replaced in the urn from which it was selected. A new urn is\nselected according to the random selection process associated with\nthe current urn, and the ball selection process is repeated. This\nentire process generates a ﬁnite observation sequence of colors,\nwhich we would like to model as the observed output of an HMM.\nIt is clear that the simplest HMM that can respond to the\nurn and ball process is one in which each state corresponds to a\nspeciﬁc urn, and a (ball) color probability is deﬁned for each state.\nThe choice of urns is dictated by the state transition matrix of the\nHMM. Thus HMM is characterized by N urns containing colored\nballs, M distinct colors of balls and each urn having a (possibly)\ndiﬀerent distribution of colors.\n258\nCHAPTER 2. FOUNDATIONS OF NLP\nElements of a Hidden Markov Model:\nAn HMM is characterized by the following:\n1) N, the number of states in the model. Although the states\nare hidden, for many practical applications there is often some\nphysical signiﬁcance attached to the states. In the urn and ball",
    "CHAPTER 2. FOUNDATIONS OF NLP\nElements of a Hidden Markov Model:\nAn HMM is characterized by the following:\n1) N, the number of states in the model. Although the states\nare hidden, for many practical applications there is often some\nphysical signiﬁcance attached to the states. In the urn and ball\nmodel, the states correspond to the urns. Generally states are\ninterconnected in such a way that any state can be reached from\nany other state. We denote the set of states as Q and the state at\ntime t as qt\n2) M, the number of distinct observation symbols associated\nwith a state, i.e., the discrete alphabet set. The observation sym-\nbols correspond to the physical output of the system to be mod-\neled. For the urn and ball model observation symbols were the\ncolors of the balls selected from the urns. We denote the set of\nsymbols as V.\n3) The state transition probability distribution A = aij, where,\naij = P(qt+1 = j|qt = i),\n1 ≤i, j ≤N\n(2.15)\nwith the state transition probabilities satisfying the constraints\naij\n≥\n0\nN\nX\nj=1\naij\n=\n1\n4) The observation symbol probability distribution in state j,\nB = {bj(k)}, where,\nbj(k) = P [vk|qt = j] ,\n1 ≤j ≤N\n1 ≤k ≤M\n(2.16)\nand satisfying the following constraints\nbj(k)\n≥\n0\nM\nX\nk=1\nbj(k)\n=\n1\n5) The initial state distribution π = πi, where\n2.3. STATISTICAL APPROACHES\n259\nπi = P [q1 = i] ,\n1 ≤i ≤N\n(2.17)\nsatisfying the constraint PN\ni=1 πi = 1\nThe entire model λ can be denoted as : λ = (A, B, π)\nA complete speciﬁcation of an HMM requires speciﬁcation of\ntwo model parameters (N and M), speciﬁcation of observation\nsymbols, and the speciﬁcation of the three probability measures\nA,B, and π. For convenience, we use the compact notation\nλ = (A, B, π)\n(2.18)\nGiven appropriate values of N,M,A,B, and π, the HMM can\nbe used as a generator to generate an observation sequence O =\nO1O2 . . . OT (where each observation Ot is one of the symbols\nfrom V, and T is the number of observations in the sequence) as\nfollows:",
    "λ = (A, B, π)\n(2.18)\nGiven appropriate values of N,M,A,B, and π, the HMM can\nbe used as a generator to generate an observation sequence O =\nO1O2 . . . OT (where each observation Ot is one of the symbols\nfrom V, and T is the number of observations in the sequence) as\nfollows:\n1. Choose an initial state q1 = si according to initial state\ndistribution π.\n2. Set t=1.\n3. Choose Ot = vk according to the symbol probability distri-\nbution in state si, i.e., bi(k).\n4. Transit to a new state qt+1 = sj according to the state\ntransition probability distribution for state si, i.e., aij.\n5. Set t=t+1; return to step 3) if t < T; otherwise terminate\nthe procedure.\nThe above procedure can be used as both a generator of ob-\nservations, and as a model for how a given observation sequence\nwas generated by an appropriate HMM.\nThree Basic Problems:\n1. Given the observation sequence O = (O1, O2, . . . , OT ) and a\nmodel λ = (A, B, π), how do we eﬃciently compute P(O|λ),\nthe probability of the observation sequence, given the model?\n260\nCHAPTER 2. FOUNDATIONS OF NLP\n2. Given the observation sequence O = (O1, O2, . . . , OT ) and\nthe model λ, how do we choose a corresponding state se-\nquence Q = (q1, q2, . . . , qt, . . . , qT ) which is optimal in some\nmeaningful sense (“that best explains the observations”)?\n3. Given the observation sequence O = (o1, o2, . . . , oT ), how do\nwe adjust the model parameters λ = (A, B, π) to maximize\nP(O|λ)?\nProblem 1 is the evaluation problem, namely given a model\nand a sequence of observations, how do we compute the prob-\nability that the observed sequence was produced by the model.\nWe can also view the problem as one of scoring how well a given\nmodel matches a given observation sequence. For example, if we\nconsider the case in which we are trying to choose among several\ncompeting models, the solution to problem 1 allows us to choose\nthe model which best matches the observations. Enumerating all",
    "model matches a given observation sequence. For example, if we\nconsider the case in which we are trying to choose among several\ncompeting models, the solution to problem 1 allows us to choose\nthe model which best matches the observations. Enumerating all\npossible state sequences and then computing the probability of\nobserving the given sequence using that state sequence is com-\nputationally too expensive. There are two algorithms called the\nForward Algorithm and the Backward Algorithm which can do the\nsame computation more eﬃciently.\nProblem 2 is the one in which we attempt to uncover the\nhidden part of the model, i.e., to ﬁnd the “best” state sequence.\nWe need to use some optimality criterion. There are several rea-\nsonable optimality criteria that can be imposed, and hence the\nchoice of criterion is dependent on the intended use for the uncov-\nered state sequence. The solution to this problem two is given by\nViterbi Algorithm. In the case of POS tagging, we could model\nstates as POS tags and words as observation symbols. POS tag-\nging would then correspond to identifying an optimal state se-\nquence using this Viterbi algorithm.\nIn problem 3 we attempt to optimize the model parameters\nso as to best describe a given observation sequence. The obser-\nvation sequence used to adjust the model parameters is called\na training sequence.\nThe training problem is a crucial one for\nmost applications of HMMs, since it allows us to optimally adapt\nmodel parameters to observed training data. The Baum-Welch\nAlgorithm provides a method for doing this. This is basically a\nparameter re-estimating technique. Instead of simply computing\nempirical probabilities from training corpora, we can start with\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n261\nan initial model and iteratively reﬁne the parameters as we get to\nsee more and more training data. The Baum-Welch algorithm is\nactually an application of the EM algorithm.\nWe avoid the details of the algorithms here. The algorithms",
    "2.4. INDIAN LANGUAGE TECHNOLOGIES\n261\nan initial model and iteratively reﬁne the parameters as we get to\nsee more and more training data. The Baum-Welch algorithm is\nactually an application of the EM algorithm.\nWe avoid the details of the algorithms here. The algorithms\nare readily available and can be easily accessed by the interested\nreaders.\nOther Techniques in Machine Learning\nThere are many learning techniques and many of them have been\napplied to language technology problems. Decision Trees and De-\ncision Lists can be learnt automatically from training data. Sev-\neral techniques exist to classify objects where the decision bound-\naries are non-linear. Support Vector Machines (SVMs), which are\nbased on the principle of structural risk minimization, have been\nexplored widely in recent times.\nNeural networks and genetic\nalgorithms have been applied. Computational Learning Theory\nhas started providing answers to fundamental relations between\nthe training data size, the classiﬁcation accuracy levels and the\nassociated conﬁdence levels. Machine learning is a large and fas-\ncinating ﬁeld of study in itself. Interested readers will ﬁnd many\ngood books and other reference materials.\n2.4\nTechnology Development in Indian\nLanguages\nIndia is a land of One Billion people - about one sixth of the\nwhole world. Our civilization dates back to many thousands of\nyears. India is a land of many religions, many cultures and many\nlanguages. A life time is not suﬃcient to get even a glimpse of\neverything that is Indian.\nThe Language Scene in India:\nThe language scene in India is quite unique. Most of higher\neducation is imparted through the medium of English. Most peo-\nple prefer to send their children to English medium schools. The\nquality of books and teachers in local languages are generally con-\n262\nCHAPTER 2. FOUNDATIONS OF NLP\nsidered to be inferior, there is relatively little scientiﬁc and tech-\nnical material available in these languages and scope for gainful",
    "quality of books and teachers in local languages are generally con-\n262\nCHAPTER 2. FOUNDATIONS OF NLP\nsidered to be inferior, there is relatively little scientiﬁc and tech-\nnical material available in these languages and scope for gainful\nemployment are relatively less. English is the language of choice in\nbusiness, medicine, law and even the government, although there\nare eﬀorts to encourage the use of our own languages. In other\nnon-English speaking countries, people use their own language for\nprofessional communicating among themselves and use English\nonly to deal with outsiders. In India all business and professional\ntransactions across the country are done in English. English has\na greater role than our own languages.\nThe situation described above is actually true more of urban\nareas than in villages. A majority of Indians live in villages. Re-\ncent surveys show that a vast majority of our population do not\nknow English and those who know a bit are not comfortable in\nthe usage of English for day to day communications. The ﬁgures\nvary from 85% to 95% of the population. Thus a vast majority\nof our people are deprived of the direct beneﬁts of all the modern\ninformation technology. People cannot be expected to know En-\nglish, have typing skills and all the technical expertise required to\nuse computers for retrieving relevant and useful pieces of informa-\ntion from the Internet or whatever. Indian language technologies,\nespecially with voice input and output would go a long way in\nempowering people by providing them access to information.\nMany people speak their own mother tongue at home but\nuse English for everything else. With each generation, there is a\nslow decay in the use of our languages. Your grand parents were\nwell read scholars, your parents studied your language up to high\nschool and were able to read and write, you can speak but you\ncannot read and write, and your children will perhaps ﬁnd it dif-",
    "slow decay in the use of our languages. Your grand parents were\nwell read scholars, your parents studied your language up to high\nschool and were able to read and write, you can speak but you\ncannot read and write, and your children will perhaps ﬁnd it dif-\nﬁcult even to speak your own language ﬂuently. Language is a\nmirror of human life. Languages are also treasuries of human civ-\nilization. Everything that we know today is encoded in language\nsomewhere. Everything we have learnt over the last thousands\nof years is hidden somewhere in our languages. As the languages\nslowly go into oblivion, so do these vast treasures of knowledge -\nknowledge of medicine, knowledge of astronomy, of mathematics,\nof mythology, of human values, of traditions and customs. Tech-\nnology development for Indian languages has the potential to slow\ndown and perhaps even reverse this trend, at least to some extent.\nAlmost all of the commercial and business transactions are\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n263\ndone in English not only for interacting with people abroad but\neven within the country. A large part of government and legal\ntransactions are also performed in English.\nAlmost all of the\nhigher education is in the medium of English. Thus centres of\npower and money do not often see any great urgency for Indian\nlanguage technologies. Private companies in Information technol-\nogy sector recruit only those who know English. Companies need\npeople who know English to interact with their foreign counter-\nparts and it is easy to ﬁnd people who are good in the relevant\nskills as also in English. Since everybody in the companies knows\nEnglish, even the thought of using other local languages never\narises. Groups trying to promote Indian language computing com-\nplain of lack of market. It is only recently that the importance\nof local language computing is getting realized more and more\nand Government of India has started promoting Indian language\ntechnologies in a big way.",
    "arises. Groups trying to promote Indian language computing com-\nplain of lack of market. It is only recently that the importance\nof local language computing is getting realized more and more\nand Government of India has started promoting Indian language\ntechnologies in a big way.\nThe three language formula has worked well in some cases but\nhas not worked very well in others. Nevertheless, a large number\nof people know two, three or even more number of languages. This\nis a kind of natural multi-lingualism, very diﬀerent from the kind\nof multi-lingualism you will ﬁnd in countries that are essentially\nmono-lingual. You will ﬁnd that more or less free mixing of several\nlanguages is very common. Many documents are required to be in\nmore than one language. With so many languages in use, correctly\nidentifying the language used is itself an critical step in many\nlanguage technology applications.\nAlthough there are many languages and people from one part\nof the country sometimes feel almost like foreigners in other parts\nof the same country, it is not true that there are any serious lan-\nguage barriers. People have been travelling all over the country\nfrom ancient times and nobody has complained of language bar-\nriers. People know that everybody does not speak their language\nand there is a sense of tolerance, cooperation and understanding.\nLearn more languages. Earn more friends is the motto. Common-\nness of social culture, traditions and linguistic features also helps.\nMedia and increased travel and mixing of people have helped too.\nEnglish and Hindi act like national link languages. Thus there\nare no serious language barriers for simple day to day commu-\nnications. However, it is still diﬃcult for a vast majority of the\npeople to use computers, access information from the Internet, etc.\n264\nCHAPTER 2. FOUNDATIONS OF NLP\nTechnology for Indian languages is developing fast and hopefully\npeople will be able to overcome these problems soon.",
    "nications. However, it is still diﬃcult for a vast majority of the\npeople to use computers, access information from the Internet, etc.\n264\nCHAPTER 2. FOUNDATIONS OF NLP\nTechnology for Indian languages is developing fast and hopefully\npeople will be able to overcome these problems soon.\nWe have heard people, especially from other countries, saying\nthat India has one language called Hindi and about 20 or so di-\nalects. Our own elders and policy makers often think that there\nare about 20 languages and hundreds of dialects. The fact is that\nthere are about 150 diﬀerent languages spoken in India, of which\n22 have been given constitutional recognition and are considered\nto be the major languages. The status of languages is closely re-\nlated to power politics and there are always ﬁghts to get higher and\nhigher status for one’s own language. Scientiﬁc analysis, however,\nis not based on power politics but on sound linguistic principles.\nDialects are regional or social varieties of a given language - they\nall share a great degree of commonness in terms of vocabulary,\ngrammar, pronunciation etc. If you hear somebody speaking a\ndiﬀerent dialect, you may ﬁnd a few words, expressions, accent\netc. a bit strange or even incomprehensible in the beginning but\noverall, it is your own language and you understand almost ev-\nerything without diﬃculty. If the diﬀerences are so much that a\nperson knowing one language cannot understand the other at all,\nwe will have to consider them as separate languages. Although\ndrawing the dividing line between language and dialects is a bit\ntricky, linguists have devised robust and reliable techniques for\ndeciding if two varieties are dialects of the same language or two\ndiﬀerent languages. There is hardly anything common between\nHindi and Kannada, for example, not even the words, and so it\nwould be completely unacceptable to consider Kannada as a di-\nalect of Hindi or the other way around. A Kannada speaker can",
    "diﬀerent languages. There is hardly anything common between\nHindi and Kannada, for example, not even the words, and so it\nwould be completely unacceptable to consider Kannada as a di-\nalect of Hindi or the other way around. A Kannada speaker can\nunderstand the diﬀerent dialects of Kannada but not Hindi, unless\nhe or she has learnt Hindi. Kannada and Hindi as as distinct as\nEnglish and French are. After careful studies, linguists have come\nto the conclusion that there are about 150 diﬀerent languages spo-\nken in India today. These are not dialects of one another. People\ncoming from monolingual countries naturally ﬁnd it diﬃcult to ac-\ncept this high degree of variation within a given country but those\nwho know India well will have no diﬃculty. A survey conducted\nabout 20 years ago showed that every 8 to 10 km we ﬁnd some\nnoticeable change in language, food, dress, customs and traditions\netc. India is a land of great diversity.\nIndian languages encompass four language families - the Indo-\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n265\nAryan, the Dravidian, the Tibeto-Burman and the Austro-Asiatic.\nSome of these languages have extensive literature going back to\nas early as 9th century AD. (The level of maturity seen in these\nearly works prove beyond doubt that the languages must have\nbeen highly developed over many centuries before. Since we have\nnot been able to locate any of the earlier works, we will have to\ngo by the earliest available works to date our languages.) Many\nlanguages also exhibit a very rich oral ’literature’. The major In-\ndian languages are among the most widely spoken languages of the\nworld. We have an extraordinarily systematic and scientiﬁc lin-\nguistic tradition for more than 2000 years now. Phonology, Mor-\nphology, Syntax, Semantics, Logic, Pragmatics have been studied\nextensively over the past several thousand years. It is a challenge\neven to make a survey of all the works and the various schools",
    "guistic tradition for more than 2000 years now. Phonology, Mor-\nphology, Syntax, Semantics, Logic, Pragmatics have been studied\nextensively over the past several thousand years. It is a challenge\neven to make a survey of all the works and the various schools\nof thought that have originated, grown, changed and evolved in\nIndia over a continuum of thousands of years.\nWith such a rich, varied, ancient and systematic and scientiﬁc\nbackground in human languages, we Indians should have been\nworld the leaders in language technology.\nInstead, we are lag-\nging far behind not only the western languages but also the lan-\nguages of the far east. We are still struggling to use computers\nas type-writers - “type, compose and print”!\nThe idea here is\nnot to belittle the commendable work being done by many groups\nacross the country.\nThe point is, compared to what we could\nhave achieved and what we should have achieved by now, what is\nactually achieved so far is meager.\nWe have stated above that there are about 150 diﬀerent lan-\nguages spoken in India. This is what linguists generally believe\n- we do not even have as yet an exact list of our languages. It\ntakes a tremendous amount of eﬀort to analyze each of these lan-\nguages and the large number of dialects associated with each of\nthem and study the vocabulary, the morphology, the syntax and\nthe semantics. A majority of the recent work, especially in terms\nof technology, are limited to the 20 or so major languages.\nEven in these major languages, resources available for technol-\nogy development are scarce. Electronic dictionaries are becoming\navailable only recently. There is no concept of a thesaurus in many\nlanguages. There is no computational grammar for any of these\nlanguages even today. Even the morphology has not been ana-\nlyzed in enough depth and detail. It is not easy for an automatic\n266\nCHAPTER 2. FOUNDATIONS OF NLP\nsystem to say whether a given sequence of symbols is a valid word",
    "languages. There is no computational grammar for any of these\nlanguages even today. Even the morphology has not been ana-\nlyzed in enough depth and detail. It is not easy for an automatic\n266\nCHAPTER 2. FOUNDATIONS OF NLP\nsystem to say whether a given sequence of symbols is a valid word\nin a given language or not. In fact we do not know exactly how\nmany words are there in many of our languages. There are no\ngood spell checkers as yet in many of our languages.\nThe least that somebody would expect in today’s times is a\ncollection of texts in electronic form. Such a large and represen-\ntative collection of texts, called a corpus, has immense value for\nstatistical and linguistic analysis and for developing technology at\nall levels right from dictionaries and spell checkers to intelligent\ninformation retrieval, automatic categorization, automatic sum-\nmarization and automatic translation. Unfortunately, even large\nplain text corpora are not yet available in may languages. Only\nabout 3 Million word corpora are available for most of the major\nlanguages. These corpora have not be thoroughly proof-read and\nhence are not very dependable.\nThere are relatively few web-sites and web pages in Indian\nlanguages. Most of them are not indexed by search engines be-\ncause standard encoding schemes are not followed. Some sites use\npictures instead of text. Many use font-encoded pages and either\ndepend on local availability of fonts at client side or dynamic font\ntechnology and plug-in’s. Most of the newspapers, magazines and\nbooks will be in electronic form at some point of time or other\nduring production but most often in completely non-standard,\nproprietary and secret encoding schemes and are thus useless for\nany further processing or analysis. In most cases, the electronic\nforms of the documents are never archieved.\nSpeech technologies are especially important for a country like\nIndia with many languages and high levels of illiteracy. There are",
    "any further processing or analysis. In most cases, the electronic\nforms of the documents are never archieved.\nSpeech technologies are especially important for a country like\nIndia with many languages and high levels of illiteracy. There are\nagain certain characteristics of Indian languages which are quite\ndistinct from English. For example, stress is relatively less impor-\ntant and other prosodic features such as duration are more signiﬁ-\ncant. Aspiration is a contrastive feature. A deeper understanding\nof characteristics of our languages is essential and technology de-\nveloped for other languages cannot be simply borrowed. There is\na lot of quantitative work that needs to be done. Very little has\nbeen done so far. We do not even have large speech corpora.\nHowever, things are changing fast. There is a much higher de-\ngree of understanding and appreciation of the language technology\nissues at all levels. There is deﬁnite trend towards standardiza-\ntion. Several serious large scale eﬀorts have been initiated. There\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n267\nis a corpus of about 35 Million words for Telugu today. Auto-\nmatic text categorization systems have been developed. Language\nidentiﬁcation across Indian languages is now possible. OCR sys-\ntems have started appearing for Indian scripts. Major initiatives\nhave been taken in machine translation and speech technologies.\nSearch engines specialized for Indian languages have been devel-\noped. There is progress on Information retrieval and extraction\nsystems as well. There is hope that Indian language technologies\nwill develop very fast over the next few years.\nHowever, we should add a word of caution to our note of op-\ntimism. Whatever has been done so far in terms of technology\ndevelopment is largely ad-hoc, hoch-poch, untested, unﬁnished\nand essentially unusable.\nSpeciﬁcations are not written down.\nSystems are not designed carefully. Instead we jump to imple-\nmentation right away. Bench mark standards and standard test",
    "development is largely ad-hoc, hoch-poch, untested, unﬁnished\nand essentially unusable.\nSpeciﬁcations are not written down.\nSystems are not designed carefully. Instead we jump to imple-\nmentation right away. Bench mark standards and standard test\ndata are lacking. Standard testing and evaluation metrics, meth-\nods and tools are absent. Developers themselves double as testers.\nTraining is done on test data and testing is done on training data.\nEven false claims are made at times. Standards are not respected.\nPeer review is discouraged. Teams are not built. There is too\nmuch of unnecessary competition. Everybody wants to do every-\nthing. A dozen groups are working on speech synthesis and each\ngroup develops its own text normalization techniques. Why can-\nnot one group take up one small item like this, do a good job of\nit and make it available for all others to use? If there are com-\npeting ideas and there are enough people and resources to try out\nthe various possibilities, then competition is good. Duplication of\neﬀort for want of coordination, lack of team spirit or due to igno-\nrance is not welcome. Everybody wants to do big things and the\nfoundations and ground realities are neglected. Everybody talks\nof all these issues and everybody wants to be the leader - ‘you\ncome and work for me’ is the attitude. We cannot go on like this\nfor ever. If we have achieved so little compared to what we could\nhave, these are the reasons.\nApprehensions and lack of trust with regard to Intellectual\nProperty Rights are perhaps another major set of reasons for the\ncurrent scenario in India. Researchers are not fully aware of IPR\nissues, external support is inadequate, legal protection is poor,\nIPR is a costly issue and everybody is confused. Should we give\nout our products for free or should we transfer the technology to\n268\nCHAPTER 2. FOUNDATIONS OF NLP\na company or should we sell them ourselves? In many cases there\nare no companies which are ready to take a technology and take",
    "IPR is a costly issue and everybody is confused. Should we give\nout our products for free or should we transfer the technology to\n268\nCHAPTER 2. FOUNDATIONS OF NLP\na company or should we sell them ourselves? In many cases there\nare no companies which are ready to take a technology and take\nit further. There are agents and traders looking for ready-made\nproducts which they can directly sell and make money but there\nare often no companies that are capable of absorbing a technology\nin the true sense, making a product out of that and market and\nmaintain the product. Technology absorption requires substantial\ntechnical expertise and research capabilities. There is always a\nﬁght, right inside our own minds, whether we should give our\nproducts for free or make money out of it. Should we give out\nthe source code? Should we take a patent? It makes no sense to\ntake only an Indian patent. Getting a US patent takes a lot of\ntime and money. Who wants to work with lawyers? There is a\ndilemma between use and misuse. There is a ﬁght between putting\nthe products of research into good use for the beneﬁt of the people\nand preventing misuse and exploitation by unscrupulous elements.\nIn the end a lot of good work done remains unknown and unused.\nWe have the capability. We can be world leaders. Only we\nmust learn to rise above politics and personal ego and get more\nprofessional. We must learn grow beyond funds, power, name and\nfame and start looking at research from research point of view.\nWe can then certainly achieve the status of global leadership in\nlanguage technologies.\nThe Nature of Indian Languages:\nAnother reason for the relatively slow progress of Indian lan-\nguage technology is the very nature of our languages.\nIndian\nlanguages are characterized by several unique features that make\nthem very diﬀerent from other major languages of the world. Thus\nthe technologies developed for English or Japanese cannot always\nbe borrowed in a more or less direct fashion and applied to our",
    "Indian\nlanguages are characterized by several unique features that make\nthem very diﬀerent from other major languages of the world. Thus\nthe technologies developed for English or Japanese cannot always\nbe borrowed in a more or less direct fashion and applied to our\nlanguages. This is a double-edged sword. There are some nice\nproperties of Indian languages which we can exploit and build\nsimpler, more elegant and more eﬃcient solutions. There are also\ncertain characteristics that make Indian language processing more\ncomplex.\nHence a clear understanding and appreciation of the\nsalient characteristics of Indian languages is very essential. Here\nwe give a brief sketch of some these salient characteristics.\nOurs is primarily a oral culture. Vast amounts of knowledge\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n269\nhas been passed on from generation to generation, for thousands\nof years, only through the word of mouth. An exceptionally elabo-\nrate system was used to ensure that the knowledge so transmitted\ndoes not get corrupted or inﬂuenced by local conditions. If every\ncopy of a traditional text were to be destroyed, nothing will have\nbeen lost - there are people who know the whole stuﬀby heart\nand the book can simply be written down again. Even after writ-\ning came into use, writing was looked down upon - you write it\ndown only if you are not capable or conﬁdent of retaining it in\nyour memory. It is said that we normally use only a very small\npart of our brain’s capacity and our ancestors believed more in\ntheir own mental abilities than in external physical devices. To-\nday the ﬁrst thing we do is to take a photocopy. Copies lying in\nour bookshelves do not constitute our knowledge. Our knowledge\nis what we have in our ﬁnger tips. To this day, many of our tribal\nlanguages have no script. But they do have a very rich oral ‘liter-\nature’. Thus not everything we have is available in written form.\nThe greatest scholars in India have always been “illiterate” and",
    "is what we have in our ﬁnger tips. To this day, many of our tribal\nlanguages have no script. But they do have a very rich oral ‘liter-\nature’. Thus not everything we have is available in written form.\nThe greatest scholars in India have always been “illiterate” and\nthey preferred to remain so even after writing became possible.\nThe western notion of illiteracy is thus not applicable to India.\nIlliterates are not necessarily uneducated or ignorant.\nThis book, however, is focussed on computer processing of\nwritten texts. We shall therefore start with a study of our writing\nsystems and the text processing environment. Our aim here will\nbe to a get a feel for the nature of Indian languages and issues\nin technology development for Indian languages. We shall brieﬂy\nsurvey a few selected technologies.\n2.4.1\nThe Text Processing Environment:\n(Text Processing Tools)\nKEYBOARD\n(Keyboard Driver)\n(Font Encoding)\n(Character Encoding Standard)\nFILE\nPRINTER\n(Rendering Engine)\nDISPLAY\nFONTS\nThe Text Processing Environment\nFIG 2.16 The Text Processing Environment\n270\nCHAPTER 2. FOUNDATIONS OF NLP\nWhen you type in some text from a computer keyboard, a\npiece of software called the keyboard driver checks which keys are\npressed and sends out corresponding numerical codes. The text\nediting software converts these codes into a possibly diﬀerent set\nof codes corresponding to the character encoding scheme used.\nA character is a letter of the alphabet, a punctuation mark or a\nsymbol such as ‘#’ or ‘+’. For example, if ASCII is the character\nencoding scheme used, the upper case letter A is coded as the\nnumber 65.\nWhat is actually stored in a text ﬁle is just this\nnumber 65. All the software programs ‘know’ how to deal with\nthis number 65. For example, a text editor would display it as the\nletter ‘A’. A sorting program would know that ‘A’ comes before\n‘B’. If we are using the ISCII character encoding standard, the\n1991 BIS standard (IS 13194:1991), and you type the character",
    "this number 65. For example, a text editor would display it as the\nletter ‘A’. A sorting program would know that ‘A’ comes before\n‘B’. If we are using the ISCII character encoding standard, the\n1991 BIS standard (IS 13194:1991), and you type the character\n‘ka’, the number 179 will be stored. UNICODE is yet another\ncharacter encoding scheme - it speciﬁes how exactly texts will be\ncoded into numbers and stored in a text ﬁle. ASCII is suitable\nfor languages using the English alphabet. ISCII can be used for\nIndian Language texts, including English. UNICODE is intended\nto be useful for all languages.\nA character encoding scheme speciﬁes how texts in a given\nlanguage are encoded and stored in ﬁles. Presumably, all the text\nprocessing operations operate on this known, public, established\nstandard. Note that the emphasis is on storage representation,\nnot on how the characters appear when displayed or printed.\nWords are simply linear sequences of letters. Writing on paper\nas well as displaying on the computer screen and printing through\na computer printer are simple processes where the appropriate\nfont shapes are selected and placed one next to the other in a left\nto right fashion by a piece of software called a rendering engine.\nBy selecting appropriate fonts and style variations within that,\nwe can get the various appearances of the same letter, say ‘A’.\nIn English there is a near one to one correspondence between\nthe characters and the shapes stored in a font ﬁle.\nThus it is\nconceivable that characters and font shapes can be given the same\nnumerical encoding. Diﬀerent fonts for the same character can\nalso be given the same numerical encoding. The encoding for the\nletter ‘A’ is 65 irrespective of which font is used for display. It\nis the rendering engine which actually constructs the appropriate\nimages on the computer screen or in a print-out as required.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n271\nA character encoding scheme speciﬁes the character set, im-",
    "letter ‘A’ is 65 irrespective of which font is used for display. It\nis the rendering engine which actually constructs the appropriate\nimages on the computer screen or in a print-out as required.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n271\nA character encoding scheme speciﬁes the character set, im-\nposes a default collating sequence on the characters in the set and\nmaps these characters to numerical codes. Text ﬁles simply con-\ntain these codes. Only when displayed or printed by a rendering\nengine using appropriate fonts will you start seeing the content of\nsuch text ﬁles in a readable form.\nThings are diﬀerent in Indian languages. The very notion of\na character is diﬀerent. A thorough understanding of the basics\nof characters and character encoding schemes is invaluable. Let\nus start by looking at the alphabets.\n2.4.2\nThe Alphabet\nEnglish uses an alphabetic writing system. There are 26 letters\nin the alphabet and all words in English language are simply se-\nquences of these 26 letters. Knowing A-B-C-D is not enough to\nread English - you must also learn spellings. Spelling rules are\nquite arbitrary. If P-U-T is put why is B-U-T but? One letter is\nused to give diﬀerent sounds and diﬀerent letter combinations are\nused to give the same sound: (fat - fate, met - mete, kit - kite,\nput - but, case - chess - cess), (meet - meat - mete, bore - boar).\nBernard Shaw once remarked that “ﬁsh” can be spelled as “goat”\nin English, by borrowing the ’g’ for the ’f’ sound from ’rough’, ’o’\nfor the ’i’ sound from ’women’ and ’t’ for ’sh’ from ’station’!\nSpelling based alphabetic writing systems necessitate an ad-\nditional level of processing in many applications. For example, a\nText-To-Speech system must have the rules to map the spellings\nto sounds so that texts can be read out correctly.\nOn the other hand, every nursery kid in India starts by a sys-\ntematic study of phonetics, the science of sounds. The equivalent\nof the alphabet chart, called varNamaala shows a systematic and",
    "to sounds so that texts can be read out correctly.\nOn the other hand, every nursery kid in India starts by a sys-\ntematic study of phonetics, the science of sounds. The equivalent\nof the alphabet chart, called varNamaala shows a systematic and\nscientiﬁc classiﬁcation of diﬀerent sounds in the language. Since\nsounds are largely universal and language independent, the chart\nhas the same structure for all languages. The written shapes dif-\nfer but the overall scheme is the same. There are only very minor\ndeviations from language to language. Let us look at a slightly\nabstracted version of such a chart so that it will be applicable to\nalmost all Indian languages:\n272\nCHAPTER 2. FOUNDATIONS OF NLP\nvarNamaala\na\naa\ni\nii\nu\nuu\ne\nee\nai\no\noo\nau\nM\nH\nk\nkh\ng\ngh\nng\nc\nch\nj\njh\njnya\nT\nTh\nD\nDh\nN\nt\nth\nd\ndh\nn\np\nph\nb\nbh\nm\ny\nr\nl\nv\ns’\nS\ns\nh\nFIG 2.17 The “Alphabets” of Indian Scripts\nNote how vowels and consonants are clearly separated. First\ncome the vowels, then the consonants. First the back vowel ’a’,\nthen the mid vowel ’i’ and ﬁnally the vowel ’u’ produced by lip\nrounding. The ’a’ vowel is the most basic and one of the simplest\nvowel sound in any language of the world. Short and long vowels\nare paired nicely. The diphthongs ’ai’ and ’au’ come towards the\nend. The ﬁrst line ends with the semi-vowels ’M’ and ’H’. The con-\nsonants come in two parts, ﬁrst 25 of them are grouped into 5 rows\nof 5 columns each and then the rest are listed. Each row stands\nfor a place of articulation and each column a manner of articula-\ntion. The rows represent velars, palatals, alveolars, dentals and\nlabials, systematically moving from the throat towards the lips.\nThe columns represent unvoiced-unaspirated, unvoiced-aspirated,\nvoiced-unsapirated, voiced-aspirated, an ﬁnally the nasals. The\nsecond part includes the semi-vowels, liquids and fricative sounds.\nSuch a nice phonetically based orthographic system makes our\nlanguages free of spellings - what we write is what we speak. The",
    "voiced-unsapirated, voiced-aspirated, an ﬁnally the nasals. The\nsecond part includes the semi-vowels, liquids and fricative sounds.\nSuch a nice phonetically based orthographic system makes our\nlanguages free of spellings - what we write is what we speak. The\nchild only learns the shapes of the letters and how they combine\nto depict complex sound clusters. Reading and writing then come\nnaturally.\nThere are of course small deviations and variations from lan-\nguage to language and script to script but our aim here is to\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n273\ndiscern the simplicity and beauty of this scheme. A lot more can\nbe said about our varNamaala but we shall avoid digressing too\nmuch into those issues. It is believed that Lord s’iva himself gave\nthe sound system to mankind through his Damaruga and that is\nhow we human beings got this great gift of speech. The s’iva su-\nutras form the basis for all of our grammars. Readers are urged\nto explore these further.\n2.4.3\nThe Script Grammar\nAnother unique characteristic of Indian languages is a grammar\nat the level of scripts.\nWe can talk of grammatically valid an\ninvalid sequences, which is not the same as valid or invalid spelling.\nA word which is not spelled correctly in English could still be\nconceived of as a possible proper name or abbreviation or acronym\nor a domain speciﬁc term. On the other hand, a sequence that\nis ungrammatical can never occur in any language, even in the\nfuture, not even as an acronym or proper name.\nEnglish and many other western languages use an alphabetic\nwriting system. Any word is simply a (linear) sequence of the\nletters of the alphabet. Thus words have spellings. Spelling rules\nmay be highly regular or quite irregular and largely arbitrary as\nin English, forcing us to remember the spellings of all the words.\nOn the other hand, ideographic languages such as Chinese and\nJapanese use pictures to depict meanings. In contrast to these,\nIndian languages use a syllabic writing system.",
    "in English, forcing us to remember the spellings of all the words.\nOn the other hand, ideographic languages such as Chinese and\nJapanese use pictures to depict meanings. In contrast to these,\nIndian languages use a syllabic writing system.\nSpeaking and listening come naturally whereas writing and\nreading come much later. Written language needs to be taught\nand learnt. Not all are capable of reading and writing - there are\nilliterates. In fact language is speech - writing is an artifact. It is\nnoteworthy that Indian scripts are based on sound units. There\nare really no spellings at all.\nIndian scripts are directly based on phonetics - the units of\northography exhibit a more or less one to one correspondence\nwith the spoken sounds. The units of orthography are essentially\nC*V syllables (more accurately called aksharas) where C denotes\na consonant sound and V a vowel sound. C* segments are also\nallowed and hence the term ’syllable’, although often used, is not\naccurate. We shall use the term akshara. Aksharas are the atomic\nunits of writing. Parts of an akshara do not necessarily constitute\n274\nCHAPTER 2. FOUNDATIONS OF NLP\nvalid linguistic units.\nSince aksharas are of the general form C*V or C* and there\ncan be several consonants in consonant clusters, the total number\nof possible aksharas is very large. There are examples of conso-\nnant clusters consisting of as many as ﬁve consonants (example,\nkaartsnya, Sanskrit).\nIf all possible ﬁve consonant clusters are\nallowed, we get more than 2 Billion aksharas, assuming about 35\nconsonants, about 15 vowels and three vowel modiﬁers!\nOf course all these mathematically possible aksharas may not\noccur in any given language and all those that can occur do not\noccur equally frequently.\nStudies have shown that about 5000\naksharas cover more than 99% of all words in all the major Indian\nlanguages taken together. Even ﬁve thousand is a large number -\nwe cannot possibly treat them as atomic units. Can we have 5000",
    "occur equally frequently.\nStudies have shown that about 5000\naksharas cover more than 99% of all words in all the major Indian\nlanguages taken together. Even ﬁve thousand is a large number -\nwe cannot possibly treat them as atomic units. Can we have 5000\nkeys on a type-writer or a computer keyboard? Typing in, editing,\nstoring, processing, displaying and printing of Indian languages\nall seem to be much more complex. Methods used for western\nlanguages are not suitable.\nFive to ten thousand aksharas are all that we use frequently\nbut to ensure completeness, we need to be able to represent every\npossible akshara while ruling out all invalid ones. It is clearly not\nfeasible to give a separate symbol to each of the possible aksharas.\nThat would make the orthography too complex to be practicable.\nIt would make a lot of sense to consider consonants and vowels\nas basic units an compose aksharas out of them. Consonants and\nvowels are reasonably small in number, about 40 consonants and\nabout 15 vowels in all. However we need some more issues relating\nto orthography to be taken into consideration before we come up\nwith a good scheme.\nPure consonant sounds cannot in general be pronounced inde-\npendently, they need the help of a vowel sound to pronounce. Of\nall the vowels we can think of ’a’ (as in English word ’but’) is the\nsimplest, the most frequent and the most universal vowel sound.\nIn fact ’a’ and ’m’ sounds are so basic that these are the sounds\nbabies learn to speak ﬁrst in almost all languages. It is no won-\nder that the word for mother is based on ’a’ and/or ’m’ sounds in\nmost languages of the world. So when we learn the basic alphabet,\nthe varNamaala, as kids, we are told to read out the consonant\nsounds with the help of the ’a’ vowel. Consonants combined with\nvowels are more frequent than pure consonants and consonants\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n275\ncombined with ’a’ vowel are generally the most frequent. It there-",
    "the varNamaala, as kids, we are told to read out the consonant\nsounds with the help of the ’a’ vowel. Consonants combined with\nvowels are more frequent than pure consonants and consonants\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n275\ncombined with ’a’ vowel are generally the most frequent. It there-\nfore makes sense to consider consonants with an implied ’a’ vowel\nas basic units and explicitly remove the implied ’a’ vowel only\nwhen needed. In this scheme, a symbol called halaMt meaning\nconsonant-ending is explicitly added to obtain a pure consonant.\nOne may of course argue that we can treat pure consonants as\nbasic and add the ’a’ vowel as and when needed. Without getting\ninto debates on which of these methods is really superior, let us\nadapt the former scheme for our purpose here. Every consonant\nhas an implied ’a’ vowel sound in it.\nOf course consonants may combine with vowels other than ’a’\nand we need to specify the particular vowel in such cases. It will\nbe assumed that the implicit ’a’ is removed and the speciﬁed vowel\nis used in its place. We also need a mechanism to distinguish be-\ntween a consonant sound with an implied ’a’ vowel followed by\nanother diﬀerent vowel such as ’i’, from the case where we need\nthe combination of the consonant with the ’i’ vowel sound - (“ka-i”\nis diﬀerent from “ki”). Further, the written shapes for vowels oc-\ncurring independently in their own right and vowels that combine\nwith consonants to form single aksharas are substantially diﬀer-\nent in many scripts. Given these two factors, it makes good sense\nto distinguish between independent vowels and vowel sounds that\ncombine with consonants. The latter are called vowel maatras.\nFinally, we also have a couple of semi-vowels, namely ’m’ and\n’h’ (called anusvaara and visarga in Sanskrit), which can be ap-\npended to the vowel sounds occurring either independently or with\none or more consonants. In some languages we also have a half-\nanusvaara. These are called vowel modiﬁers.",
    "Finally, we also have a couple of semi-vowels, namely ’m’ and\n’h’ (called anusvaara and visarga in Sanskrit), which can be ap-\npended to the vowel sounds occurring either independently or with\none or more consonants. In some languages we also have a half-\nanusvaara. These are called vowel modiﬁers.\nThe only other mechanism we need is to allow a pure conso-\nnant without automatically combining it with following consonant\nto form a cluster. This occurs relatively rarely as in the case of En-\nglish and other foreign language words when written in our scripts.\nA simple escape mechanism is provided - use two halaMts in se-\nquence to terminate the akshara and prevent combination with\nfollowing consonants if any.\nNow that we have all the required ingredients we can start\ndeﬁning aksharas very precisely. Note that the basic units of ak-\nshara construction are consonants, vowels, halaMt, vowel maatras\nand vowel modiﬁers. Note also that not all possible sequences of\nsuch building blocks are valid. For example, vowel maatras can\n276\nCHAPTER 2. FOUNDATIONS OF NLP\nonly post-modify consonants, they cannot occur before or inde-\npendently. You can have only one maatra, not more than one.\nThus there are valid and invalid sequences. We are ready to give\na grammar, a grammar of scripts that accepts all valid aksharas\nand only valid aksharas. Any sequence which is invalid can never\never occur in any language. The grammar is complete, it is not\na sample. Every one of the billions of possible valid aksharas is\nhandled. There is no compromise. Here is the grammar:\nV\nD\nH\n1\n2\n7\n4\n6\n3\nV: Vowel\nD Vowel Modifier\nM: Vowel Matra\nC: Consonant\nH: Halant\nC\nC\nM\nH\nD\nScript Grammar\nFIG 2.18 Script Grammar for Indian Languages\nThis extremely simple grammar is complete and consistent. It\nallows all valid aksharas, potentially inﬁnite in number, without\nregard to their frequency of occurrence in any particular language\nor domain. It is primarily based on sound units and hence uni-",
    "This extremely simple grammar is complete and consistent. It\nallows all valid aksharas, potentially inﬁnite in number, without\nregard to their frequency of occurrence in any particular language\nor domain. It is primarily based on sound units and hence uni-\nversal and language independent. We have a common grammar\nfor all languages and all scripts. What an excellent solution!\nNote that the grammar speciﬁed here is a Finite State Ma-\nchine. It is the simplest kind of grammar we can think of and the\nmost eﬃcient. All aksharas in a given word can be identiﬁed in a\nsingle scan in linear time. The grammar can be used to identify\ninvalid aksharas as a kind of spell-checking process, to break a\nword into its constituent aksharas and for other similar purpose.\nThe script grammar is sometimes given as phrase structure\nrules but one must recognize that the power of Context Free\nGrammars is not required. Finite State power is suﬃcient. Thus\ngiving phrase structure rules can be misleading.\nAlthough the above grammar for scripts is well accepted, there\ncould be alternative schemes. For example, we may treat conso-\nnants as pure consonants without any implicit vowel. This would\nmake the halaMt unnecessary but ’a’ vowel needs to be explicitly\nshown where needed. This would also eliminate the need to as-\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n277\nsume that the implicit ’a’ is removed when a consonant combines\nwith some other vowel.\nAlso note that this grammar is not entirely phonetic based. If\nit were, we would not be treating vowels and vowel maatras as sep-\narate elements since they represent the same phonetic items. But\nremember that our purpose here is not to do a phonetic transcrip-\ntion of speech, our purpose is to develop a grammar for scripts. In\nterms of written symbols, many Indian languages show substan-\ntial variations in shape - independent vowels and vowel maatras\nlook very diﬀerent. Our choice here is not vary bad then. Vowels",
    "tion of speech, our purpose is to develop a grammar for scripts. In\nterms of written symbols, many Indian languages show substan-\ntial variations in shape - independent vowels and vowel maatras\nlook very diﬀerent. Our choice here is not vary bad then. Vowels\nand vowel maatras look diﬀerent and we need some mechanism to\ndistinguish between the two.\n2.4.4\nFonts, Glyphs and Encoding Standards\nThe same letter ‘A’ may appear diﬀerently on the computer screen\nand in print-outs depending upon which ‘font’ is used.\nIn lay\nman’s terms, fonts are diﬀerent styles of writing characters. A\nfont ﬁle speciﬁes the exact shapes of characters and the various\nshapes are given numerical codes. Within a given font it is often\npossible to show ﬁner variations such as size, boldface, italics, un-\nderline, strike-through etc. The ﬁgure below shows some examples\nof English fonts and variations within these.\nA A\nA A\nA\nA\nFIG 2.19 English Fonts\nIn Indian scripts, the number of aksharas is very large and it\nis also not practicable to have one font shape for each akshara.\nJust as aksharas themselves were composed of more basic elements\nsuch as vowels and consonants, font shapes need to be composed\nof more basic elements. However, the primary aim here would\nnot be phonetics or grammaticality, the primary aim would be\nease of composition. A font should be designed to have a small\nnumber of basic shapes from which all valid aksharas can be easily\ncomposed. Such basic shapes used to deﬁne fonts are called glyphs.\nHere are some examples of glyphs and how they combine to form\nfull aksharas:\n278\nCHAPTER 2. FOUNDATIONS OF NLP\nFIG 2.20 Telugu Script: Vowels and Consonants\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n279\nFIG 2.20 Contd. Glyph to Character Mappings\nThe shapes we use in deﬁning a font should be selected based\non the simplicity of their being composed to obtain combined\nshapes for displaying full aksharas. The shapes are quite diﬀerent\nfrom script to script. The script grammar, on the other hand,",
    "279\nFIG 2.20 Contd. Glyph to Character Mappings\nThe shapes we use in deﬁning a font should be selected based\non the simplicity of their being composed to obtain combined\nshapes for displaying full aksharas. The shapes are quite diﬀerent\nfrom script to script. The script grammar, on the other hand,\nwas based on phonetics and the aim was to develop a simple and\neﬃcient grammar to deﬁne all valid aksharas in a language inde-\npendent manner. From this, it should be clear that there may not\nbe any one-to-one correspondence between glyphs used in deﬁn-\ning fonts and the elements of which the script grammar itself was\ndeveloped. Diﬀerent scripts look diﬀerently and hence the fonts\nand the glyphs of which these fonts are made of, also diﬀer.\n280\nCHAPTER 2. FOUNDATIONS OF NLP\nIn English there is a near one to one correspondence between\nletters of the alphabet and glyphs used for rendering them. Words\nare simply sequences of letters and letters themselves are small in\nnumber. Words are written left to right by simply writing the\nletters one after the other. There is no need to compose letters\ninto any intermediate level of description. Fonts use one glyph\nper letter and there is no need for any elaborate composition. The\nglyphs are simply laid out in a line from left to right. Indian scripts\nare more complex. Glyphs must be composed in both horizontal\nan vertical direction - glyphs may need to be scaled and placed in\nvarious positions relative to each other - to the right, on the top,\nto the right-bottom, at the bottom, to the left-bottom etc. Thus\nunderstanding the concept of glyphs is very important.\nA few hundred glyphs are needed to eﬀectively render all pos-\nsible aksharas in a given Indian language. The glyphs are variable\nin size - not all characters are of the same width. Exactly what\nset of shapes to use is not ﬁxed by any standard. It is left to\nthe font designers to select the basic shape set they feel necessary",
    "sible aksharas in a given Indian language. The glyphs are variable\nin size - not all characters are of the same width. Exactly what\nset of shapes to use is not ﬁxed by any standard. It is left to\nthe font designers to select the basic shape set they feel necessary\nto render the various aksharas eﬃciently and aesthetically. Font\ndesigners design glyphs based on the ease of composition into vari-\nous possible aksharas. Further, to make the aksharas so composed\nlook neat and aesthetic, it becomes necessary to develop several\nequivalent glyphs and use the appropriate ones based on the con-\ntext. For example, a glyph to represent a particular vowel maatra\nmay come in several shapes, sizes and locations - one for narrow\nconsonants, one for wide consonants, one for some other complex\ncombination of consonants, etc. Diﬀerent fonts for the same lan-\nguage may, and often do, diﬀer in terms of the glyph sets they\nemploy. Fonts also vary in terms of the position in the code table,\nthat is, what numerical codes are given to each of the glyphs they\nencompass. Each font uses a possibly diﬀerent set of glyphs and\npositions them in possibly diﬀerent ways in the code table. In fact\nthere is no glyph encoding standard. There is even some opposi-\ntion to the idea of developing a glyph encoding standard fearing\nthat such a standard may curtail the creativity of font designers\nin producing aesthetic fonts.\nThis total lack of standardization at the level of glyphs and\nfonts has far reaching consequences. For example, simply select-\ning a piece of text and changing the font can render the text junk.\nFonts have been treated like pieces of art and they remain propri-\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n281\netary, non-standard and incompatible with one another. Of late\nthere is some realization that standards are useful if not essential\nand there is hope that some sort of standards will develop and get\nwidely accepted. Tamil is perhaps the only language where there",
    "281\netary, non-standard and incompatible with one another. Of late\nthere is some realization that standards are useful if not essential\nand there is hope that some sort of standards will develop and get\nwidely accepted. Tamil is perhaps the only language where there\nhas been a great deal of eﬀort in overcoming teething problems of\nthis kind and standards have been developed and widely accepted,\nalthough keeping only Tamil language in mind.\nIn fact many commercial software vendors have promoted the\nuse of their own proprietary, non-standard and secret fonts. Texts\ntyped from these software are encoded in terms of these propri-\netary fonts, rather than in terms of any standard character en-\ncoding scheme. A large number of electronic texts in Indian lan-\nguages have been encoded in proprietary fonts and thus not di-\nrectly useful for any language technology application. Text typed\nin through one commercial software is often not useable in an-\nother software. A great deal of time, eﬀort and money goes into\ndeveloping inter-conversion tools between diﬀerent fonts. Perhaps\nIndia could have been in a much more advanced state today if\nstandards had been respected and commercial companies not al-\nlowed to take the whole country for a ride. In spite of the fact\nthat almost every publisher is using electronic typing and compos-\ning of some kind or the other, developing even plain text corpora\nin Indian languages has been such an arduous task. It is only\nvery recently that corpora crossing 30 Million words have started\nappearing at least for some languages.\n2.4.5\nCharacter Encoding Standards\nIn alphabetic writing systems such as English, there are a small\nnumber of letters which can be directly encoded in a character\nencoding standard such as ASCII. Letters of the alphabet, along\nwith other special symbols such as punctuation marks, numerals,\nparenthesis, quote marks etc, are called characters.\nA charac-\nter encoding scheme speciﬁes the allowed set of characters and",
    "encoding standard such as ASCII. Letters of the alphabet, along\nwith other special symbols such as punctuation marks, numerals,\nparenthesis, quote marks etc, are called characters.\nA charac-\nter encoding scheme speciﬁes the allowed set of characters and\nplaces them in a particular order. This order deﬁnes the collating\nsequence. Usually collating sequence is so selected that sorting\nin alphabetical order becomes straight forward. Each position is\ngiven a numerical code and the characters are represented inside\ncomputers by these numbers.\nThus the character ’A’ is repre-\nsented as number 65 in ASCII. Since there is a one-to-one corre-\n282\nCHAPTER 2. FOUNDATIONS OF NLP\nspondence between letters and glyphs, fonts can also use the same\nnumerical codes for placing the corresponding glyphs. Thus ’A’\ncan be represented as 65 in ASCII character encoding standard\nas also in any of the fonts. This makes the whole scheme very\nsimple and neat. The encoded text remains the same irrespective\nof which fonts are used.\nWe need a character encoding scheme for Indian languages\ntoo. But we must ﬁrst understand what we mean by a character.\nAksharas are the basic units of writing but there are too many\nof them. We cannot encode each possible akshara by giving it a\nnumerical code. We must get into more basic units and encode\nthem.\nThus aksharas will need to be composed of more basic\nunits. The script grammar we have seen above forms an excellent\nscheme to deﬁne a character encoding standard.\nAny character encoding scheme for Indian language scripts\nmust ideally deﬁne a script grammar and implement it by spec-\nifying the numerical codes for the consonants, vowels and other\nsymbols used in the grammar. In spite of the fact that the num-\nber of valid aksharas is potentially inﬁnite and practically very\nlarge, a script grammar makes it possible to encode texts using\na fairly small number of diﬀerent codes. The texts encoded in\nsuch a scheme would be a sequence of such numeric codes. The",
    "ber of valid aksharas is potentially inﬁnite and practically very\nlarge, a script grammar makes it possible to encode texts using\na fairly small number of diﬀerent codes. The texts encoded in\nsuch a scheme would be a sequence of such numeric codes. The\nappropriate units of reading, displaying, printing and otherwise\nmanipulating Indian languages are aksharas, not individual sym-\nbols such as vowels, consonants, maatras etc. It is straight for-\nward to synthesize aksharas as also to break running texts into\nsequences of aksharas using the script grammar.\nAksharas are\nthe atoms of Indian writing systems. Any method that attempts\nto directly deal with sub-atomic units such as vowels, consonants\nand maatras are to be rejected as they are fundamentally ﬂawed.\nSuch designs result in improper and unacceptable breaking of ak-\nsharas. How often do we see a consonant at the end of one line\nand the associated maatra in the beginning of the next line!\nISCII\nISCII is a National Standard for character encoding of major\nscripts of Indian languages. A brief description of the ISCII char-\nacter encoding schemes given below. Refer to the 1991 BIS stan-\ndard (IS 13194:1991) for more information.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n283\nIndia is a multi-lingual country.\nOf the major languages,\nUrdu, Sindhi and Kashmiri are primarily written in Perso-Arabic\nscripts, although Devanagari script is also used. Sindhi is also\nwritten in Gujarati script. All the remaining languages are writ-\nten in 10 diﬀerent scripts, all of which have the origins in the an-\ncient braahmii script. These scripts are Devanagari, Gurumukhi\n(Punjabi), Gujarati, Oriya, Bengali, Assamese, Telugu, Kannada,\nMalayalam and Tamil. Many of these languages are written in\nmore than one script. Sanskrit is written in almost all the scripts.\nAlso, a given script may be used for writing several languages.\nSanskrit, Hindi and Marathi are all written using the Devanagari",
    "Malayalam and Tamil. Many of these languages are written in\nmore than one script. Sanskrit is written in almost all the scripts.\nAlso, a given script may be used for writing several languages.\nSanskrit, Hindi and Marathi are all written using the Devanagari\nscript. The association between languages and scripts is not one\nto one. People who come from the west often have diﬃculties in\nunderstanding this situation.\nAll the 10 braahmii based scripts have the same phonetic\nstructure.\nLinguists have also shown that India is a linguistic\narea and thanks to great similarities and commonness in the cul-\ntural and social settings across India, Indian languages have many\nthings in common at various levels of linguistic description rang-\ning from the aksharas through word and sentence structure and\nmeaning to expressions and idioms. There is a unique blend of\nsimilarity and commonness, along with distinctive and contrastive\ncharacteristics. It is therefore most appropriate to have a common\nscript code for all the 10 braahmii based scripts, as indeed ISCII\ndoes.\nWe have seen that the varNamaala for Indian languages is a\nvery systematic and scientiﬁc organization of the basic sounds -\nthe vowels and the consonants. We have also seen that units of\nwriting are aksharas and there is a script grammar that deﬁnes\nthe valid aksharas in a simple and eﬃcient manner. The script\ngrammar is common to all the ten braahmii based scripts. ISCII\ntakes advantage of these facts and encodes 15 vowels, 38 conso-\nnants, 3 vowel modiﬁers, a nukta symbol to indicate alternative\nforms of certain selected consonants, thereby forming a slight su-\nperset of all the elements used in these ten braahmii based scripts.\nThese are placed in the code table in such a manner that correct\nsorting is automatically obtained for ISCII encoded texts. Since\nthe total number of items to be encoded is less than 128, ISCII\nhas chosen to use the second half of the ASCII table, retaining",
    "These are placed in the code table in such a manner that correct\nsorting is automatically obtained for ISCII encoded texts. Since\nthe total number of items to be encoded is less than 128, ISCII\nhas chosen to use the second half of the ASCII table, retaining\nthe ﬁrst 128 codes as they are. This makes mixing of English and\n284\nCHAPTER 2. FOUNDATIONS OF NLP\nIndian languages, a very common phenomenon in India, trivial.\nThus ISCII includes all of the basic ASCII characters without any\nchange.\nUsing a common script code for diﬀerent scripts makes translit-\neration between these scripts trivial.\nWhat we encode are the\nsounds and the sounds are the same in all languages although the\nshapes used in writing these sounds may be very diﬀerent. It must\nbe noted that transliterating texts from one script to another is\nvery common in India. The very nature of the language situation\nin India makes this necessary. We use words from other languages\nbut write them in our script. Transliteration also makes it possible\nto ’read’ texts in a language we do not know. Actually, because\nof the linguistic and social similarities, we can understand other\nlanguages to some limited extent provided we can ’read’ them by\ntransliterating the texts into a script we know. In ISCII translit-\neration is trivial - the encoding is exactly the same in all scripts\nand all we really need to see the text in another script is to sim-\nply render the same text in the required script. Note that the ﬂip\nside of the coin is that there can be no multi-lingual (multi-script)\nplain text in pure ISCII. Language and/or script can be indicated\nonly using annotations or markup.\nISCII encodes sound units and sound units are largely com-\nmon across the languages of the world. It is therefore conceivable\nthat ISCII can be extended to encode all languages of the world.\nWhat is common to diﬀerent languages of the world is the set of\nsound units. Really nothing else is so universal. The ISCII scheme",
    "mon across the languages of the world. It is therefore conceivable\nthat ISCII can be extended to encode all languages of the world.\nWhat is common to diﬀerent languages of the world is the set of\nsound units. Really nothing else is so universal. The ISCII scheme\nwould perhaps be the best scheme for developing universal charac-\nter encoding scheme for all the languages and scripts of the world.\nIf we can do it for so many diverse languages and scripts of India,\nwhy not for the whole world?\nAlthough the ISCII standard is now almost 14 years old, it\nis not very widely known or used. Commercial companies have\npromoted their own proprietary font encoded texts and very few\nof them have even export-import facilities to ISCII. ISCII is not\na registered standard and is not supported by many Operating\nSystems and Web Browsers. ISCII only deﬁnes the character en-\ncoding scheme, it does not provide any standards for glyph encod-\ning, nor does it provide a set of compatible fonts. This makes it\nnecessary for application software to handle all issues of keyboard\ndrivers, ISCII-to-Font mapping and rendering. Perhaps this is the\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n285\nmain reason why ISCII has not picked up as much as it should\nhave done and we see proliferation of non-standard and propri-\netary schemes even today.\nThe ISCII standard does recommend keyboard layouts for dif-\nferent scripts under the name INSCRIPT. The inscript keyboards\nare well designed and reasonably eﬃcient once you learn trying in\ninscript. Nevertheless, there are diﬀerent views and again we ﬁnd\na proliferation of several keyboarding schemes. Some ﬁnd it more\nconvenient to type in Indian scripts using the Roman transliter-\nations. There are again several roman phonetic schemes. Thus\none who is proﬁcient in typing in a particular scheme will ﬁnd it\ndiﬃcult to use anther. Not all software vendors support all the\navailable schemes.\nISCII standard speciﬁes the script grammar. The ISCII stan-",
    "ations. There are again several roman phonetic schemes. Thus\none who is proﬁcient in typing in a particular scheme will ﬁnd it\ndiﬃcult to use anther. Not all software vendors support all the\navailable schemes.\nISCII standard speciﬁes the script grammar. The ISCII stan-\ndard however uses phrase structure rules instead of the simpler\nFinite State Machine notation.\nIt must be made clear that ﬁ-\nnite state power is suﬃcient, we do not need the power of context\nfree grammars. It should also be noted that ISCII standard only\n‘speciﬁes’ the grammar of scripts. It is the duty of the software\nsystems to ‘enforce’ this grammar and allow only grammatically\nvalid aksharas. Many of the commercial software packages do not\nenforce the script grammar properly and the texts you create using\nsuch software will be ungrammatical. Often there will be multiple\nspellings (sequences of encoded bytes) for the same word. What\nyou see and what is stored in the ﬁle will be diﬀerent. Processing\nsuch texts will be unnecessarily more complex than needed.\nISCII is based on the script grammar we have seen above.\nConsonants are assumed to have an implicit ’a’ vowel in them.\nThis scheme gives a one byte code for ‘ka’ and two byte codes for\n‘ki’, ‘kii’ etc., making the text processing algorithms a triﬂe bit\nmore complex than would have been otherwise necessary. Vowels\nand vowel maatras are encoded separately although they repre-\nsent the same sounds - a deviation of the basic philosophy that\nwhat we speak is what we write. However, this design makes it\neasy to distinguish between a consonant followed by a vowel from\na consonant that combines with a vowel sound to form a single\nakshara.\nISCII makes a super set of sounds over a set of languages. This\nis theoretically objectionable as each language has its own char-\nacteristic set of sounds (phonemes) and foreign sounds cannot be\n286\nCHAPTER 2. FOUNDATIONS OF NLP\nincluded. However it makes a lot of practical engineering sense to",
    "ISCII makes a super set of sounds over a set of languages. This\nis theoretically objectionable as each language has its own char-\nacteristic set of sounds (phonemes) and foreign sounds cannot be\n286\nCHAPTER 2. FOUNDATIONS OF NLP\nincluded. However it makes a lot of practical engineering sense to\ndevelop a common scheme when the commonness overwhelmingly\nshadows minor diﬀerences.\nISCII also errs by including special sequences and separate\ncodes for minor variations in shapes of certain rendered characters.\nFonts specify how exactly the characters appear when displayed or\nprinted, a non-issue as far as character encoding schemes are con-\ncerned. We should not be concerned with allographs, with minor\nvariations in shapes of rendered characters that do not change the\nword in question. As a character encoding standard, ISCII should\nhave worried only about encoding characters and not about fonts\nor rendering.\nIt must be understood that there can be no multi-lingual (that\nis multi-script) plain text in ISCII since the codes are same for\nall scripts. Sounds are the same in all languages and thus ISCII\ncarries no information about language or script. Attributes are es-\nsential to explicate language and script. The best way to handle\nvariations is to use attributes. The best way to encode attributes\nis to use open, public, visible XML style attributes, not invisible\ncontrol characters. ISCII attempts to specify some of the com-\nmonly used attributes.\nAttributes are application speciﬁc and\nopen ended. It would have been much better for ISCII to provide\na general scheme for annotation rather than attempt to specify\nattribute and extension codes. The attribute and extension codes\nspeciﬁed in ISCII standard are not uniformly followed and soft-\nware vendors make their own non-standard extensions and modi-\nﬁcations. They even use “prohibited” codes. Government of India\nhas come out with a very good standard for character encoding.",
    "speciﬁed in ISCII standard are not uniformly followed and soft-\nware vendors make their own non-standard extensions and modi-\nﬁcations. They even use “prohibited” codes. Government of India\nhas come out with a very good standard for character encoding.\nHowever, the standard has not been enforced or widely accepted\nand properly used. We could have saved a lot of time, eﬀort and\nmoney had we simply respected the ISCII standard from day one.\nIf a standard is imperfect, we may work towards bringing out a\nrevised and improved standard. We cannot use that as an excuse\nfor not using the standard at all. An imperfect standard is better\nthan no standard at all.\nA plain ISCII document has no explicit indication of language.\nGiven the multi-lingual nature of our country, automatically iden-\ntifying language from small text samples is therefore very impor-\ntant. Think of a text containing Sanskrit verses and explanatory\nnotes in Marathi or Hindi. Everything is in Devanagari script,\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n287\npossibly in the same font but a spell checker operating on such a\ndocument needs to know which parts are in Sanskrit and which\nparts are in Marathi or Hindi.\nUNICODE\nWith increasing globalization, the need for handling various lan-\nguages of the world is becoming more and more important. UNI-\nCODE has come out of the desire to handle all languages of the\nworld in a uniform way. It aims to provide a separate code space\nfor each language. There is however, a bit of confusion between\nlanguages and scripts and sometimes code spaces are assigned to\nscript-wise rather than language-wise.\nUNICODE for Indian languages is based on (an older, out-\ndated version of) ISCII - each language has a separate code space\nbut the codes mainly diﬀer only in terms of the starting oﬀset.\nThus the foundations for script coding remain identical to ISCII\nand the main diﬀerence is only the use of two bytes for each char-\nacter, the ﬁrst byte indicating the language and the second byte",
    "but the codes mainly diﬀer only in terms of the starting oﬀset.\nThus the foundations for script coding remain identical to ISCII\nand the main diﬀerence is only the use of two bytes for each char-\nacter, the ﬁrst byte indicating the language and the second byte\nbeing the same as in ISCII. There is really no major change at\nall. Texts become larger due to the use of multi-byte characters\nbut this is not such a crucial issue. ISCII makes transliteration\nbetween Indian scripts trivial and it is quite straight forward to\ndo this in UNICODE too. There was no multi-lingual plain ISCII\ntext possible but with UNICODE the language/script identiﬁca-\ntion is built in.\nWe still have some teething problems in using Indian lan-\nguages on computers.\nIt is not always easy to do even simple\nthings like searching and sorting. Part of the reason for this is\nthe proliferation of proprietary and non-standard technologies by\ncommercial companies. Also, ISCII is not a registered standard,\nand it is not directly supported by operating systems, browsers\netc. There are no standard ISCII compatible fonts and standard\nmapping tools between ISCII and the fonts. While things have\nbeen improving of late, UNICODE is slowly becoming more pop-\nular. UNICODE has provision for fonts with embedded the map-\nping rules. Thus it will be easier to render UNICODE texts for\ndisplay and printing purposes. With improving system level sup-\nport, some of the teething problems can be avoided or overcome.\nUNICODE is likely to be widely supported by many operating\n288\nCHAPTER 2. FOUNDATIONS OF NLP\nsystems, Browsers and other standard software. Although there\nare many problems and issues yet to be sorted out with UNICODE\nand related fonts for Indian languages, UNICODE is slowly pick-\ning up and may become the dominant encoding scheme. Some\nthink switching to Unicode is inevitable. For some time at least\nISCII and UNICODE will co-exist as standard character encoding",
    "and related fonts for Indian languages, UNICODE is slowly pick-\ning up and may become the dominant encoding scheme. Some\nthink switching to Unicode is inevitable. For some time at least\nISCII and UNICODE will co-exist as standard character encoding\nschemes for Indian languages. Migrating from ISCII to UNICODE\nwill anyway not be much diﬃcult.\nFrom Characters to Fonts:\nWe have seen that many commercial software packages encode\ndocuments in their own proprietary font encodings. A font en-\ncoded document is really not text at all. Fonts are made up of\nglyphs - graphic shapes which have no one-to-one correspondence\nwith any appropriate level of linguistic description. A glyph can be\na full vowel or a consonant but it can also be part of a consonant, a\nconsonant combined with a particular vowel maatra, just a vowel\nmaatra or any other arbitrary unit that is designed purely for the\nsake of graphical composition and rendering into aksharas. A dot\nthat appears in the middle of some aksharas in Telugu could be\na glyph. How can we interpret or process documents encoded as\nsequences of such arbitrary symbols? Even basic operations such\nas searching and sorting cannot be performed on such documents\nusing standard tools. Glyphs do not correspond to aksharas or to\nmore basic elements such as vowels ans consonants, glyphs can-\nnot be placed in a collating sequence that permits natural sorting.\nFont encoded documents are not texts at all. You may be able\nto type, compose and print documents using a proprietary com-\nmercial software but you cannot use such documents like normal\ntexts in other languages.\nAll texts, Indian language texts included, must be encoded in\nsome character encoding standard such as ISCII or UNICODE.\nBut unlike in the case of English, an ISCII encoded text cannot\nbe directly displayed or printed because ISCII encodes texts in\nterms of sound units, not shape units required for visual rendering.\nISCII is designed for the ears, not eyes. We have already seen that",
    "But unlike in the case of English, an ISCII encoded text cannot\nbe directly displayed or printed because ISCII encodes texts in\nterms of sound units, not shape units required for visual rendering.\nISCII is designed for the ears, not eyes. We have already seen that\nfonts are made up of glyphs and glyphs may not have one-to-one\ncorrespondence with the elements of the script grammar on which\nISCII is based. This necessitates that we posit a level of processing\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n289\nthat translates an ISCII encoded text into a selected font for the\npurposes of display and printing.\nIt must be emphasized that\nfonts are required only for display and printing. There is no other\nrole for fonts. All processing is best done on a character encoded\ntext, not on a font encoded document. For example, if you want\nto develop a spell checker or a grammar checker, an automatic\nsummarization system or a text categorization system, character\nencoded texts can be processed in a uniform and eﬃcient way\nwhile font encoded pages will be idiosyncratic to speciﬁc fonts\nused and thus not generic and universal. If you just change over\nto a diﬀerent font tomorrow, all your programs may fail.\nTexts must be encoded in a character encoding standard based\non the script grammar and the glyphs in a font do not necessarily\ncorrespond to aksharas or to the basic elements of which aksharas\nare built. Aksharas are too numerous for a one-glyph-per-akshara\ndesign. Thus converting from a character encoding such as ISCII\nor UNICODE into and from a given font encoding is an additional\nstep that is essential for Indian languages. However, there is as\nyet no ‘standard’ way of mapping from ISCII into various fonts -\nthere is no standard ‘grammar’ to specify this mapping. There are\nvery few free fonts and most software vendors have their own pro-\nprietary fonts and ad-hoc methods of mapping if any. Some have\nused table-look-up, some use context free grammars and some use",
    "there is no standard ‘grammar’ to specify this mapping. There are\nvery few free fonts and most software vendors have their own pro-\nprietary fonts and ad-hoc methods of mapping if any. Some have\nused table-look-up, some use context free grammars and some use\nad-hoc pieces of code. These solutions are usually not complete\nor consistent. Often a poorly designed commercial software can\nbe crashed by simply typing in a particular sequence of characters\nwhich the system is unable to process.\nGiven this situation, most commercial vendors of Indian lan-\nguage software have chosen to store texts directly in terms of\nthe font encoding scheme for their own, mostly proprietary, fonts,\nrather than using ISCII which has been in existence for over a\ndecade as a National standard.\nThis makes it diﬃcult to take\ntexts typed using one software and use it on another software sys-\ntem. In fact simply selecting a block of text and changing the\nfont can render the block junk. No standard text processing al-\ngorithms can be run on such documents. Searching, sorting, spell\nchecking, dictionary look up, frequency analysis, and all other text\nprocessing applications will have to be tuned to work with spec-\niﬁed fonts and the same processes cannot be guaranteed to work\ncorrectly with other documents or documents encoded in other\n290\nCHAPTER 2. FOUNDATIONS OF NLP\nfonts.\nCommercial softwares are also often extremely slow be-\ncause they try to directly work with the text as it would be ﬁnally\nrendered. Font encoded texts are not texts at all by any standard.\nThey are at best suitable only for displaying on the screen and\nprinting on paper. It is completely unacceptable to encode texts\nin any scheme other than National or International character en-\ncoding standards such as ISCII or UNICODE. We need to grow\nbeyond using computers merely as type-writers. Of what use is\na text which is encoded in some non-standard, proprietary, secret\nsystem of codes? Vast amounts of texts in Indian languages are",
    "coding standards such as ISCII or UNICODE. We need to grow\nbeyond using computers merely as type-writers. Of what use is\na text which is encoded in some non-standard, proprietary, secret\nsystem of codes? Vast amounts of texts in Indian languages are\nbeing developed for some purpose or the other and in most cases\nthe documents are in electronic form at some stage or the other.\nAll this eﬀort goes waste when it comes to re-using these texts for\nany research and development activity.\nThus there is a need to develop a simple, uniform and eﬃcient\nscheme for mapping ISCII encoded pages into a selected font for\nthe purposes of visualization.\nThe scheme should be provably\ncomplete and consistent. It should be easy to adapt the scheme\ndeveloped for one font for other fonts with minimal eﬀort.\nConverting from ISCII to font is best done at the level of ak-\nsharas. ISCII deﬁnes the script grammar and texts can be easily\nbroken into aksharas using this grammar. Each akshara must then\nbe mapped to the appropriate glyph sequence. Note that there\nis no grammar for fonts and hence mapping from font encoded\ndocuments back into ISCII would be a bit more complex. In the\ncase of complex scripts such as Telugu, a non-deterministic solu-\ntion requiring search may be necessary. In any case, it would be\nmuch better to map between ISCII and fonts rather than attempt\nto map directly from one font to another.\nDirect font to font\nconverters are ad-hoc solutions, not generic, one-time, re-usable\nmodules.\nThings are changing slowly now. Standard text editors are\nbecoming available. People have started demanding more. Text\nprocessing applications will get widely used. Ministry of Infor-\nmation Technology, Government of India, is a voting member of\nthe UNICODE consortium and can inﬂuence decision making in\nUNICODE. UNICODE is still in the making. It has hardly come\nto be used for Indian languages in any signiﬁcant way. This is the",
    "mation Technology, Government of India, is a voting member of\nthe UNICODE consortium and can inﬂuence decision making in\nUNICODE. UNICODE is still in the making. It has hardly come\nto be used for Indian languages in any signiﬁcant way. This is the\nright time to understand all the issues carefully and take the right\nsteps in the right direction.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n291\nIssues in Character Encoding Standards:\nThere is a lot of confusion regarding character encoding standards\nand their relation with fonts, keyboard drivers, rendering engines\netc.\nPeople think in terms of what they see on the computer\nscreen or on a printed paper.\nSeeing is not believing when it\ncomes to encoding standards. The reality is not what appears on\nthe screen but what is actually stored in the ﬁle. We have already\nseen that ISCII encodes sound units and thus an ISCII document\nis independent of language and script used, let alone fonts. What\nwe see on the screen is not the main concern here. We must think\nin terms of what is stored in ﬁles and how to process such text\nﬁles. The following points are included here to clearly bring out\nthe major concerns in designing character encoding standards for\nIndian languages.\n• Since character encoding schemes deal with scripts, inherent\nproperties of scripts must be part of character encoding\nstandards themselves. They cannot be relegated to the level\nof fonts or software implementations. One such issue is the\nscript grammar. There are other important issues as well.\nFor example, Telugu and Kannada scripts diﬀer from say,\nDevanagari script in terms of how consonant clusters are\nrepresented. To represent a C1C2 cluster, Devanagari uses\na half C1 and a full C2 shapes whereas Telugu and Kannada\nwould use a full C1 and a half C2 shape. The best place to\nmake this assertion is at the script level.\n• Further, although the relation between languages and scripts\nis not necessarily one-to-one and character encoding schemes",
    "a half C1 and a full C2 shapes whereas Telugu and Kannada\nwould use a full C1 and a half C2 shape. The best place to\nmake this assertion is at the script level.\n• Further, although the relation between languages and scripts\nis not necessarily one-to-one and character encoding schemes\nworry about scripts and not languages, there are certain\nbasic properties of scripts in relation to languages that use\nthem and such properties must be mentioned in the stan-\ndard. For example, in Hindi the last consonant in a word is\ntaken as a pure consonant even without any explicit halaMt\nwhereas in Sanskrit it is not so.\n• Any character encoding scheme for Indian languages must\ninclude a grammar of aksharas.\nISCII standard already\ndoes so. Characters are aksharas.\n• Every software developed for Indian languages must include\nand enforce the grammar of aksharas. Aksharas are atomic\n292\nCHAPTER 2. FOUNDATIONS OF NLP\nunits of our writing systems and aksharas cannot be arbi-\ntrarily split at any cost. Operating systems, text editors\nand word processors, web browsers must all treat aksharas\nas indivisible atomic units. As of today, this is not so. You\nmight have seen pages of text where a consonant appears\nat the end of a line and the associated maatra appears in\nthe beginning of the next line!. You will also ﬁnd compo-\nnents of a single akshara separated by spaces. Web browsers\nare not yet ISCII-aware and UNICODE-aware browsers are\nonly beginning to come. It is not enough for a browser to\n‘know’ UNICODE, it must have the grammar of aksharas\nbuilt-in too.\nChecking for ungrammatical aksharas must\nbe an integral part of every software.\nMost commercial\nsoftwares today do not check or enforce grammaticality at\nakshara level - they permit ungrammatical combinations to\nbe typed in, they do not necessarily show this up in render-\ning, and hence it is not possible to check and correct these\nmistakes manually by looking at the screen. What you see",
    "softwares today do not check or enforce grammaticality at\nakshara level - they permit ungrammatical combinations to\nbe typed in, they do not necessarily show this up in render-\ning, and hence it is not possible to check and correct these\nmistakes manually by looking at the screen. What you see\non the screen is not what is stored in the document ﬁle.\nNo wonder some of the text corpora developed using such\nsoftwares is full of errors.\n• Every software system must support all grammatically valid\naksharas.\nCompleteness is essential.\nGood character-to-\nfont mapping schemes need to be developed.\n• It is also not correct to split words at arbitrary akshara\nboundaries. Hyphenation rules will have to be developed\nfor Indian languages so that justiﬁcation of texts in nar-\nrow columns etc. can be done properly. This is especially\nimportant in Indian languages since the average length of\nwords is much larger than in the case of English.\n• A font developer may choose his own set of glyphs and place\nthem anywhere in the font code table according to his own\nneeds. There is no real need to enforce a glyph encoding\nstandard. However, every font must be accompanied by a\nmapping system that can map every grammatically possible\nakshara onto the corresponding sequence of glyphs and vice\nversa. In case any special rules are required for selection of\nglyphs or for correct rendering, they must be included as\npart of the package.\nIt is not suﬃcient to give just the\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n293\nfont ﬁle. The mapping system must be provably complete,\nconsistent and computationally eﬃcient in terms of storage\nand processing time. No grammatical akshara must be in-\ncapable of being used because of font, mapping or rendering\nlimitations.\n• Of course if glyph encoding standards can be developed and\naccepted and used, issues of compatibility between fonts\nand character to font mappings can be taken care of once\nfor all.\n• The over all user experience depends on all the components",
    "limitations.\n• Of course if glyph encoding standards can be developed and\naccepted and used, issues of compatibility between fonts\nand character to font mappings can be taken care of once\nfor all.\n• The over all user experience depends on all the components\nof a text processing environment and their inter-compati-\nbility. Character encoding standards, fonts and rendering\nengines must all be developed carefully to provide the ap-\npropriate user experience on the whole. Quality of rendered\ntexts depend on the fonts, the glyph sets used in the font,\nthe mapping rules used and glyph selections made, and the\ndetails of the rendering engine. However, character encod-\ning is not directly concerned with the appearance of rendered\ntexts at all. Decisions on character encoding issues should\nnever be taken based solely on how rendered texts appear to\nthe eye.\nCharacter encoding has nothing to do with aes-\nthetics. All issues relating to character encoding standards\nmust be based solely on how the texts would be coded and\nhow text processing algorithms would treat these texts. Will\nthere be one or several ’spellings’ for a given word? Will a\ndefault sorting algorithm sort words in the same order as\nin a dictionary? How can we design simple, eﬃcient and\nuniform sorting methods that can sort texts in diﬀerent\nlanguages? If we carry out a type-token analysis on a cor-\npus will frequencies of words get counted correctly? What\nabout other statistical analyses such as n-grams or Markov\nanalysis? What happens when we transliterate texts from\none language to another? The ‘spelling’ of words is thus the\nonly important aspect that should dictate character encod-\ning issues.\nTo give a speciﬁc example, whether ‘kSa’ should be encoded\ndirectly in a character encoding standard such as ISCII or\nUNICODE should be decided solely on whether dictionar-\nies treat ‘kSa’ diﬀerently from the cluster formed from the\n294\nCHAPTER 2. FOUNDATIONS OF NLP\nconsonants ‘ka’ and ‘Sa’.",
    "To give a speciﬁc example, whether ‘kSa’ should be encoded\ndirectly in a character encoding standard such as ISCII or\nUNICODE should be decided solely on whether dictionar-\nies treat ‘kSa’ diﬀerently from the cluster formed from the\n294\nCHAPTER 2. FOUNDATIONS OF NLP\nconsonants ‘ka’ and ‘Sa’.\nAre there two diﬀerent words,\notherwise identical, one using ‘kSa’ as a single unit and the\nother using a cluster of ‘ka’ and ‘Sa’? That ‘kSa’ appears\ndiﬀerently on the screen is no reason to argue for a separate\ncode. One should not also go by what is given in primary\nschool text books. The purpose there is to teach the chil-\ndren to read and write. If something looks a bit diﬀerent, it\nis useful to point that out and teach it as a separate entity.\nWhen it comes to usage on a computer, the fonts and the\nrendering engine can be and must be designed to give you\nexactly what you want to see on the screen and in printed\nmatter even if ‘kSa’ is not encoded as a separate ‘charac-\nter’ in the character encoding scheme.\nThere is no need\nto compromise on what the users really want to see on the\nscreen and in print-outs - users shall see exactly what they\nlike to see in rendered texts. But this does not require sep-\narate encoding of alternative shapes. Encoding need not be\nand should not be compromised. Users can still see exactly\nwhat they wish to see.\n• It is important to clearly distinguish between basic charac-\nters and special symbols. Basic characters are those that\nform the words of a given language, as found in a dictio-\nnary and in various inﬂected forms in running texts. Basic\ncharacters dictate the ‘spellings’, the sorting order and so\non. Special symbols such as the currency symbols, religious\nsymbols, symbols for playing cards, symbols used in music\netc. may have to be carefully incorporated into character\nencoding schemes but every care must be taken to avoid\nproliferation of multiple ‘spellings’ for words. As a speciﬁc",
    "on. Special symbols such as the currency symbols, religious\nsymbols, symbols for playing cards, symbols used in music\netc. may have to be carefully incorporated into character\nencoding schemes but every care must be taken to avoid\nproliferation of multiple ‘spellings’ for words. As a speciﬁc\nexample, currency is indicated in Kannada by simply writ-\ning ‘ruu.’ the standard way. Adding a new special symbol\nwill make it possible to write the same thing in two diﬀer-\nent ways. This is highly undesirable. New symbols must be\nadded with great care. Multiple spellings must be avoided\nat all costs.\n• While we agree that sounds are more basic than written\nsymbols, arguments about sounds must not be carried too\nfar. The items in the varNamaala are just names of the\ncorresponding characters which are abstract in nature. ‘a’\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n295\nis the name of a vowel sound rather than an accurate rep-\nresentation of any vowel sound, just as ‘ef’ and ‘em’ are\nnames of the consonants ‘f’ and ‘m’ respectively in English.\nCharacter encoding schemes are used only for writing and\nwhat we write is at best an attempt to approximate what\nwe speak, even in Indian languages.\nMinor (allophonic)\nvariations in pronunciation cannot be and need not be de-\npicted in the writing system. We need to make a distinction\nif and only where the variations are signiﬁcant enough to\nconfuse one word for the other. If there is no ambiguity,\nthere is no need to show the variations in the orthogra-\nphy.\nCharacter encoding schemes must not be concerned\nwith allophonic variations. A character encoding scheme\nis primarily concerned with scripts and orthography, not\nwith pronunciations or meanings. As a speciﬁc example, in\nKannada there are no vowel sound as in the English word\n‘cat’. In Telugu, this particular sound does occur in speech\nbut is not distinguished from the normal ‘a’ sound (as in\nthe English word ‘but’) in writing. This does not introduce",
    "with pronunciations or meanings. As a speciﬁc example, in\nKannada there are no vowel sound as in the English word\n‘cat’. In Telugu, this particular sound does occur in speech\nbut is not distinguished from the normal ‘a’ sound (as in\nthe English word ‘but’) in writing. This does not introduce\nany ambiguities. There is no confusion as to which word is\nmeant. Hence there is no need to introduce a new character\ncorresponding to the vowel sound as in ‘cat’ as far as na-\ntive words are concerned. Adding new character codes will\nonly add to the confusion and the proliferation of spellings.\nThe primary goal is to write words of our own language,\nnot words of foreign languages! We are we so much wor-\nried about accurate depiction of ‘cat’, ‘cot’ and ‘caught’ in\nIndian scripts?\nSome consonants such as ‘ka’ and ’Da’ have signiﬁcant vari-\nations in pronunciation in some languages and we usually\nput a dot underneath these characters when writing on pa-\nper. We can take care of these variations by adding a sepa-\nrate code called the ‘Nukta’. Whether to encode these vari-\nations or not is primarily a question of whether these are\nallophonic variations or linguistically signiﬁcant variations.\nHowever, one must also keep in mind the possibility that\nthese variations have already been encoded and depicted in\nthe scripts by convention in the past.\n• ISCII has already been extended to include the vowel sounds\n296\nCHAPTER 2. FOUNDATIONS OF NLP\nas in the English words ‘cat’, ‘caught’ and ‘cot’ as distin-\nguished from the normal ‘a’ sound in Sanskrit and other\nIndian languages. These sounds may not be used in native\nwords - they are used only for transliterating English and\nother foreign language words into our languages. They are\nuseful for distinguishing between cases like ‘cat’, ‘caught’\nand ‘cot’ when transliterated into Indian scripts. If we have\nto accommodate these new codes at all, we must at least do\nit uniformly for all Indian languages. We must uniformly",
    "other foreign language words into our languages. They are\nuseful for distinguishing between cases like ‘cat’, ‘caught’\nand ‘cot’ when transliterated into Indian scripts. If we have\nto accommodate these new codes at all, we must at least do\nit uniformly for all Indian languages. We must uniformly\nadd these new characters AYE and AWE into the UNI-\nCODE code table for each of the Indian languages and add\nannotations saying that these characters shall be used only\nfor transliterated foreign words and they shall never be used\nfor native words.\nWe must however be careful to understand that ordinary\nusers may not read or follow the annotations in code ta-\nbles and other documentation associated with standards.\nThey just use whatever suits them from the set of available\ncodes. Thus there is always an element of risk of proliferat-\ning multiple ‘spellings’ for words. This would surely happen\nfor quite some time, since many languages already have well\nestablished conventions. In Kannada, for example, we write\n‘kyaat’, ‘byaank’, and ‘myaap’ to depict the English words\n‘cat’, ‘bank’ and ‘map’. Diﬀerent languages have diﬀerent\nsets of conventions. Some of these have been in use for a\nlong time and well stabilized. While we may hope that in\nfuture people will shift over to the new characters provided,\nthere is no guarantee that the intended shift will take place\ncompletely. Hence decisions to add new characters must be\ntaken with utmost care. If something can be avoided, not\nadding it is the best.\n• It will be a good idea to place all special symbols in a sep-\narate code space rather than duplicate them in the code\ntable for each language. Also, the space available within\nthe code tables for speciﬁc languages may not be suﬃcient\nto incorporate all the required special symbols. Punctua-\ntion marks, diacritics and many special symbols are used\nin all or most languages and are therefore best treated in-\ndependent of any speciﬁc language. ISCII does not disturb",
    "the code tables for speciﬁc languages may not be suﬃcient\nto incorporate all the required special symbols. Punctua-\ntion marks, diacritics and many special symbols are used\nin all or most languages and are therefore best treated in-\ndependent of any speciﬁc language. ISCII does not disturb\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n297\nthe ﬁrst half of the ASCII table and thus at least the ba-\nsic punctuation marks etc. are uniformly available for all\nlanguages.\n• We will have to live with multiple standards. ISCII and\nUNICODE will co-exist. Other standards such as those for\nPerso-Arabic scripts may also need to be integrated. Inter-\ncompatibility amongst the various standards is therefore a\nvery important consideration. The various standards must\ndevelop and grow together. Migration is costly and time\nconsuming.\n• One possible idea is to use ISCII as the internal base for all\nsoftwares for Indian languages and provide export/import\nto/from UNICODE for compatibility during the transitional\nperiod.\n• It is advisable to use a standard keyboarding scheme such as\nINSCRIPT. There are however, several keyboarding schemes\nin popular use. It is alright. It must be ensured however,\nthat the keyboard layouts and the drivers are complete and\nconsistent with respect to the character encoding scheme\nused. There should not be any character which cannot be\ntyped in! Software systems must strive to provide for all\npopular schemes and make the schemes easily adaptable or\ncustomizable by user himself.\n• Only a small number of aksharas with three consonant clus-\nters are in frequent use in Indian languages.\nConsonant\nclusters with four or more consonants are relatively rare.\nCare must be taken, however, to consider clusters required\nto write texts transliterated from some foreign language.\nIt is pragmatic, nevertheless, to allow splitting of conso-\nnant clusters with more than, say, three consonants. We\nmay take a uniform national level decision to invariably",
    "Care must be taken, however, to consider clusters required\nto write texts transliterated from some foreign language.\nIt is pragmatic, nevertheless, to allow splitting of conso-\nnant clusters with more than, say, three consonants. We\nmay take a uniform national level decision to invariably\nsplit consonant clusters with more than three consonants\nby introducing an extra HalaMt. A uniform policy of this\nkind would go a long way in ensuring compatibility between\nsoftware systems and text documents prepared or processed\nusing them. This would drastically reduce the set of gram-\nmatical aksharas too.\n298\nCHAPTER 2. FOUNDATIONS OF NLP\n• A small set of fonts suitable for headings and running texts\nmust be made freely available for all Indian languages. We\nare currently in a ridiculous situation where if you buy a\ncomputer in India you automatically get A, B, C, D but not\n‘a’, ‘aa’, ‘i’, ‘ii’. There is no harm if special and fancy fonts\nare developed and sold on commercial basis but a small set\nof free fonts must be freely available to all and available\nfor free. Fonts without character-to-font mappings are of\nno use - the fonts must be readily useable on any platform,\nwith any software whatever.\n• There are now few UNICODE compliant fonts for Indian\nlanguages. Unless urgent steps are taken to develop free\nUNICODE fonts, we will not be able to take advantage of\nthe technological developments. It is also ridiculous to ask\neach user to buy commercial fonts for a price. A small set\nof UNICODE compliant fonts must be developed and made\navailable freely.\n• It is neither fair nor economically sound to insist that all\nIndians use software compatible with any one commercial\nvendor. All issues of standardization must be made very\ncarefully so that they are applicable across hardware and\nsoftware platforms. This is very important when develop-\ning software tools and even more so when developing fonts,\nsince there are several diﬀerent technologies used for speci-",
    "vendor. All issues of standardization must be made very\ncarefully so that they are applicable across hardware and\nsoftware platforms. This is very important when develop-\ning software tools and even more so when developing fonts,\nsince there are several diﬀerent technologies used for speci-\nfying font shapes (Ex. ttf, metafont, open-type.\n• Revising the UNICODE standard is all the more critical\nsince the UNICODE consortium does not allow characters\nto be deleted or their primary properties altered as a matter\nof policy and principle. A mistake committed once will stay\nfor ever. It is better to go slow on contentious addition of\nnew symbols.\n• A character encoding scheme automatically speciﬁes the de-\nfault sorting order - words get sorted in the order of the\nnumerical codes used to represent them. Hence every care\nmust be taken to ensure that the positions alloted for the\nvarious characters and special symbols give us the correct\nsorting order. However, we need not go too far on this - uni-\nversal sorting algorithms exist and the sorting order can be\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n299\neasily speciﬁed through map ﬁles to meet our exact require-\nments even if the default sorting order is not acceptable.\n• Sounds are more basic than orthography (the writing sys-\ntem). We learn to speak words much before we start read-\ning and writing. Speaking is learnt naturally, writing - with\nconscious eﬀort. Sounds are basic, natural, intrinsic prop-\nerties of a languages. Orthography is an artifact. Not all\nare able to read and write - there are illiterates too. Or-\nthography is at best an approximate mapping of the sounds\nthat we use in a language onto a graphical format, even in\nIndian languages which are basically phonetic in nature.\nCharacter encoding schemes are best designed based on the\nstructure of sounds in a language rather than based on the\nway words are written. The varNamaala reﬂects an arrange-\nment of characters based on the nature of sounds. ISCII is",
    "Indian languages which are basically phonetic in nature.\nCharacter encoding schemes are best designed based on the\nstructure of sounds in a language rather than based on the\nway words are written. The varNamaala reﬂects an arrange-\nment of characters based on the nature of sounds. ISCII is\nbased on this principle too. Since the sounds are more or\nless the same in all Indian languages, ISCII uses the same\ncode space for all these languages. UNICODE, on the other\nhand, places each Indian language in a separate code space.\nHowever, UNICODE is not completely opposed to idea of\nproving a common code space as can be seen from the fol-\nlowing quote from the UNICODE Technical Documenta-\ntion:\n”Duplicate encoding of characters is avoided by unifying\ncharacters within scripts across languages; characters that\nare equivalent in form are given a single code.\nChinese\n/ Japanese / Korean (CJK) consolidation is achieved by\nassigning a single code for each ideograph that is common\nto more than one of these languages.\nThis is instead of\nproviding a separate code for the ideograph each time it\nappears in a diﬀerent language.\n(These three languages\nshare many thousands of identical characters because their\nideograph sets evolved from the same source.)”\nWhy not then a common code space for all Indian lan-\nguages? UNICODE for Indian languages is still on its way,\nit is not already ﬁrmly established and so widely used that\nchanges would do great harm. There is no harm in incor-\nporating signiﬁcant changes in the way UNICODE is devel-\noping. In fact this is the right opportunity to correct any\n300\nCHAPTER 2. FOUNDATIONS OF NLP\nmistakes of the past and improve upon the present. All ef-\nforts must therefore be made to set further development of\nUNICODE on the best path.\n• Fonts must also include punctuation marks, diacritics and\nother special symbols as required. As of today, many fonts\ndo not provide even the basic punctuation marks.\nThus",
    "forts must therefore be made to set further development of\nUNICODE on the best path.\n• Fonts must also include punctuation marks, diacritics and\nother special symbols as required. As of today, many fonts\ndo not provide even the basic punctuation marks.\nThus\nusers are forced to switch to English even to insert a comma\nor a quote mark. Since size and other parameters of fonts\ndo not map one to one between English and Indian language\nfonts, there are problems associated with this need to switch\nlanguages merely for using punctuation marks.\n• Any new standardization attempt must give careful thought\nto local standardization and ‘rationalization’ eﬀorts already\nimplemented or currently being debated and discussed. Peo-\nple should not feel that we are going back-wards. In some\ncases the reform processes may have been completed and\nwell accepted conventions established and uniformly fol-\nlowed. In some cases, we may actually be passing through\nthe processes of change. The guiding principle in all cases\nmust be to respect the present and move towards the fu-\nture, rather than dogmatically stick to what was only used\nlong ago. At the same time, it would be prudent to carry\nalong the legacy as long as they are still in vogue.\nWe\nmust however be careful to add annotations stating that\nthese characters and symbols are provided only for back-\nward compatibility and shall not be used for current writ-\ning.\n• Every care must be taken to ensure that basic characters as\nwell as special symbols are placed in the code table in corre-\nsponding positions across various Indian languages so that\ntransliteration remains simple. At the same time placement\nin the code table must also ensure correct sorting order.\n• Englishmen may not even think of writing Kannada words\nusing the English alphabet but it is much more natural for\nKannada people to write English words in Kannada script.\nTransliteration is commonplace in Indian languages.\nWe",
    "in the code table must also ensure correct sorting order.\n• Englishmen may not even think of writing Kannada words\nusing the English alphabet but it is much more natural for\nKannada people to write English words in Kannada script.\nTransliteration is commonplace in Indian languages.\nWe\nfreely write Sanskrit in many diﬀerent scripts. Some lan-\nguages are written in more than one script. We often write\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n301\nEnglish and other foreign words in our scripts.\nMusical\ncompositions of Saint tyaagaraaja are written by Kannadi-\ngas in Kannada script and by the Telugu people in the\nTelugu script so that they can learn and sing these com-\npositions.\nTransliteration is a basic operation that must\nbe accounted for.\nAll eﬀorts must therefore be made to\nensure straight forward transliteration across Indian lan-\nguages. Yet, if there is a clash between the requirements of\na given language and the needs of easy transliteration to or\nfrom other languages, the basic needs of the given language\nper se must have the ﬁnal word. After all the code space\nfor a language is primarily meant for that language.\n• While transliteration among Indian languages is very im-\nportant, 100% ﬁdelity in transliteration among Indian lan-\nguages may not be possible.\nWhat would happen if the\nsource language uses a particular character that the target\nlanguage does not have at all? Should we create artiﬁcial\ncharacters just to make a language compatible with other\nlanguages? What do native speakers of that language do\nwith such unknown symbols? Arguments about transliter-\nation must not be carried too far.\n• Some Indian languages have their own numerals. These are\nin regular use in some languages while in others they are\nnot used these days. A careful decision must accordingly\nbe taken. Roman numerals must be permitted in any case.\nFrom these discussions, it must be clear that there are con-\nfusions and unresolved issues. Users must learn to live with con-",
    "in regular use in some languages while in others they are\nnot used these days. A careful decision must accordingly\nbe taken. Roman numerals must be permitted in any case.\nFrom these discussions, it must be clear that there are con-\nfusions and unresolved issues. Users must learn to live with con-\nfusions and imperfect solutions for some time. All the teething\nproblems are being addressed and hopefully we will be able to use\nIndian languages on computers as simply and as easily as English.\n2.4.6\nRomanization\nWe have seen examples of Sanskrit, Hindi, Telugu and Kannada\nwords written using the Roman alphabet.\nThe unique multi-\nlingual environment in India makes it necessary to write one lan-\nguage using the script of another language. Sanskrit is often writ-\nten in the scripts of local languages. For example, the verses from\n302\nCHAPTER 2. FOUNDATIONS OF NLP\nbhagavadgiita may be given along with the commentaries writ-\nten in Telugu language, both of which are written in the Telugu\nscript for the beneﬁt of the Telugu knowing people. Now that\nEnglish has become a very important link language, it also be-\ncomes necessary to write Indian languages in the Roman script.\nEnglish newspapers carry advertisements of Hindi movies. The\nmovie names etc. are naturally written in the Roman script. Thus\nthere is a regular, routine need for writing Indian languages in the\nRoman script, not just for those special meta occasions when we\nare talking about languages per se. An Englishman may never\neven think of writing English in Kannada script but it is natural\nand essential to write Kannada in the Roman script.\nHowever, there seem to be no serious concerted eﬀort to come\nup with a good standard, widely accepted scheme for Roman\ntransliteration. There are several schemes but most people use\nsome ad-hoc scheme of their own creation. Let us understand the\nnature of the task ﬁrst.\nIt must be recognized at the outset that English is an alpha-",
    "up with a good standard, widely accepted scheme for Roman\ntransliteration. There are several schemes but most people use\nsome ad-hoc scheme of their own creation. Let us understand the\nnature of the task ﬁrst.\nIt must be recognized at the outset that English is an alpha-\nbetic writing system based on the 26 letters of the alphabet and\nspelling rules while Indian scripts are phonetically based and syl-\nlabic in nature. Even if we were to count the basic units such\nas vowels and consonants instead of full aksharas, we ﬁnd that\nthe numbers are larger than the 26 letters we have. One common\nsolution is to use diacritic marks but this scheme is inconvenient\nfor direct use on the computer. Typing in diacritic marks from\nthe computer keyboard is not convenient. Another common so-\nlution is to mix lower case and upper case letters.\nIn English\ncapital letters are used for proper names and acronyms as also to\nmark the beginning of a sentence. Here we need to use capital\nletters instead for denoting retroﬂex consonants etc. Multi-byte\nequivalents are also required as there are more symbols in Indian\nscripts than 52. We need to handle long and short vowels, aspi-\nrated and unaspirated consonants, retroﬂex sounds, etc. Based\non these ideas, several schemes have been proposed including the\niTRANS scheme and the Rice University scheme.\nThe worst part of the story is that people use completely\nad-hoc transliterations without consistently sticking to any one\nscheme. Each person seems to have his/her own standard. News-\npapers need to transliterate Indian languages into Roman on a\ndaily basis but each time it is left to the whims and fancies of\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n303\nindividual journalists and editors. Advertisements are worse. It\ntakes a substantial amount of time, patience as well as knowledge\nof the language concerned to understand these transliterations.\nThe problem is complicated by the fact that there are many lan-",
    "2.4. INDIAN LANGUAGE TECHNOLOGIES\n303\nindividual journalists and editors. Advertisements are worse. It\ntakes a substantial amount of time, patience as well as knowledge\nof the language concerned to understand these transliterations.\nThe problem is complicated by the fact that there are many lan-\nguages and we need to perform language identiﬁcation as well.\nThere are several issues in the design of a good Romain translit-\neration scheme. Some systems are case sensitive while others are\nnot. Thus mixing of upper and lower cases has its own merits and\ndemerits. This mixing also makes the text look a bit awkward as\nupper case letters generally appear bigger. Some researchers have\nworried about storage space and suggest the use of upper case\nletters even for long vowels. The IIT(Kanpur) group has taken\nthis idea of economy of representation to the extreme and they\npropose highly unnatural mappings such as ’w’ for the ’th’ sound\nas in ’thesis’ and ’x’ for the ’d’ sound as in ’Daddy’. A few bytes\nmight be saved but the texts now look really ugly and diﬃcult to\nread. Also, keying in letters of diﬀerent cases requires frequent\nuse of SHIFT or CAPS-LOCK keys. Some use ’aa’ for long ’a’.\nThen we might expect that the same logic be universally applied\nand we use ’ii’ for the long vowel sound as in ’feet’ and ’ee’ for\nthe long vowel sound in ’ate’. However, while ’ee’ is frequently\nused in English spellings, ’ii’ is never used. Some argue that we\nmust use those sequences which are more natural and frequent in\nEnglish while others are upset that English spellings should form\nthe basis for writing Indian languages. English spelling rules are\nghastly and English phone set is diﬀerent from the set we use for\nour languages. English spellings cannot answer all our concerns.\nYet readability is important too. Any scheme that is systematic\nand uniform is more likely to be accepted and picked up easily\ncompared to ad-hoc schemes. Why should Romanization of In-",
    "our languages. English spellings cannot answer all our concerns.\nYet readability is important too. Any scheme that is systematic\nand uniform is more likely to be accepted and picked up easily\ncompared to ad-hoc schemes. Why should Romanization of In-\ndian scripts be ad-hoc just because English happens to be ad-hoc?\nIn the long run, people will ﬁnd it easier to get used to a system-\natic, uniform scheme than a hoch-poch scheme based on a foreign\nlanguage. Should English knowing people decide things for Indian\nscripts or should we keep Indians speaking Indian languages and\nknowing Indian scripts as the base? Some have proposed the use\nof special symbols such as the colon to suggest vowel lengthen-\ning but others say special symbols will interfere with processing\nof such texts. Whether we use special symbols or not, there will\nalways be a need for an escape mechanism so that literal inter-\n304\nCHAPTER 2. FOUNDATIONS OF NLP\npretations can be permitted. Some are worried about single or\neven round trip transliteration among various scripts. With the\nincreasing use of multi-lingual and multi-script documents, prob-\nlems will only increase. It is high time a well thought out standard\nis prepared and brought into widespread use.\nIn the absence of a single widely accepted and widely known\ntransliteration scheme, in this book we have avoided taking a\nstrong position based on any one ideology and used what we con-\nsider a fairly simple and straight forward scheme in this book. It\nis generally acceptable to add a ‘h’ to indicate aspirated sounds\nand use capitals for retroﬂex sounds. Long vowels have been in-\ndicated by doublets. Proper names and sentence initial words are\nnot capitalized.\n2.4.7\nSpell Checkers\nSpell checkers form one of the most basic technologies for any given\nlanguage. It may be surprising then, that good spell checkers are\nnot available for many Indian languages even today. Let us see\nwhy. We take examples from Kannada language. We start with",
    "not capitalized.\n2.4.7\nSpell Checkers\nSpell checkers form one of the most basic technologies for any given\nlanguage. It may be surprising then, that good spell checkers are\nnot available for many Indian languages even today. Let us see\nwhy. We take examples from Kannada language. We start with\ngeneral issues in the design of spell checkers.\nSpelling Error Detection and Correction\nA spell checker is a computer program that deals with the detec-\ntion and correction of spelling errors in texts. An ideal detector\nshould pin point all and only those words in the text that are\nwrongly spelled. An ideal corrector should automatically correct\nall such errors. In practice, neither detection nor correction can\nbe perfect. Most practical spell checkers do not even attempt to\nautomatically correct spelling errors - instead they only oﬀer list\nof suggested alternatives for the user to choose from. The user\nhas several options: he may select one of the suggestions provided\nby the system, he may direct the program to accept the speciﬁc\ninstance or even all instances of current spelling in the text, he\nmay direct the system to add this “new” word to the custom dic-\ntionary, or he may choose to edit the word manually.\nA good spell checker must detect all or most of the spelling\nerrors and at the same time minimize false alarms. It must oﬀer\neither the single right suggestion or a small set of suggestions for\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n305\ncorrection in which the right suggestion is included. The sugges-\ntions oﬀered must be ranked and the right suggestion must occur\nin the top of the list in most cases.\nA good spell checker may be required to handle loan words,\nsplit and merged tokens, archaic usage, dialectal variations, ac-\ncepted cases of spelling variations, colloquial forms, domain spe-\nciﬁc terminology, acronyms and abbreviations, proper names etc.\nClearly no practical spell checker can be expected to handle all\nthese cases perfectly.",
    "split and merged tokens, archaic usage, dialectal variations, ac-\ncepted cases of spelling variations, colloquial forms, domain spe-\nciﬁc terminology, acronyms and abbreviations, proper names etc.\nClearly no practical spell checker can be expected to handle all\nthese cases perfectly.\nMost practical spell checkers work one word at a time and\nhence cannot even detect real word errors - mistakes that result\nin some other valid word in the language, as against non-word\nerrors - mistakes that result in an invalid word.\nFor example,\nmost spell checkers will not ﬁnd anything wrong in the sentence\n“One plus one is tow”. Catching real word errors requires context\nbased processing.\nIdeally, full syntactic parsing, semantic and\npragmatic analysis may be required. The context must somehow\nbe brought to bear to decide whether there is any spelling error\nor not. Both the textual context in which the word occurs and\nthe situational context and topic of the discourse are important.\nPractical spell checkers are far from ideal.\nDetection and correction can both be done using dictionaries\nand morphology and other linguistic tools, using statistical tech-\nniques, or using a combination of both.\nIn a dictionary based\napproach, a word not found in the dictionary is considered to\nbe due to a spelling error and other words from the dictionary\nwhich are close to the input word in terms of spelling are given\nout as suggestions for correction. In a simple statistical approach,\nthe probability of a particular alphabet/word sequence in the lan-\nguage is used instead as the basis. Interested readers may look at\na survey paper by Karen Kuckich for a variety of techniques for\ndetection and correction of spelling errors.\nA large dictionary is also not necessarily the best. A large\ndictionary includes many infrequent words which may be confused\nfor other words with a real-word spelling error. A user who wanted\nto type ‘leave’ actually types ‘lave’ by mistake but the system",
    "A large dictionary is also not necessarily the best. A large\ndictionary includes many infrequent words which may be confused\nfor other words with a real-word spelling error. A user who wanted\nto type ‘leave’ actually types ‘lave’ by mistake but the system\nquietly accepts this as a valid word form!\n306\nCHAPTER 2. FOUNDATIONS OF NLP\nWhat is a spelling error?\nAll through the previous section, we used the term word to mean\nsimply a sequence of alphabets separated by spaces. Thus a word\nis seen here from the perspective of its spelling rather than from\nthe perspective of pronunciation or meaning. The mapping from\nspelling to meaning is mostly arbitrary.\nIn languages like English, the mapping from spellings to pro-\nnunciation is also quite ad-hoc. The same alphabet gives rise to\ndiﬀerent sounds in diﬀerent contexts and the same sounds can be\nrealized using a variety of spelling combinations. Thus spellings\nhave to be learnt and carefully remembered and people naturally\ntend to make mistakes. The origin of spelling errors can thus be\ncognitive. Of course phonetic and typographical errors are also\npossible.\nIndian scripts, on the other hand, are primarily phonetic in\nnature - the orthography reﬂects the phonetics to a large extent.\nThus there is really no such thing as spelling. Nevertheless, there\nis scope for mistakes of a variety of kinds and techniques of spell\nchecking can be applied just the same.\nAll sequences of vowels, consonants, maatras and vowel mod-\niﬁers are not valid aksharas. Thus it is not possible to have an\nisolated maatra, an akshara beginning with a maatra or an ak-\nshara with more than one maatra. The script grammar speciﬁes\nvalid combinations. Thus one level of checking for correctness is\nbuilt into our scripts.\nOne common mistake in many Indian languages is the use of\nan aspirate for the corresponding non-aspirate or vice versa - is it\nsaMbaMdha or saMbhaMda (relationship) ?. Modern Indian lan-",
    "valid combinations. Thus one level of checking for correctness is\nbuilt into our scripts.\nOne common mistake in many Indian languages is the use of\nan aspirate for the corresponding non-aspirate or vice versa - is it\nsaMbaMdha or saMbhaMda (relationship) ?. Modern Indian lan-\nguages have many words borrowed from Sanskrit. While Sanskrit\nuses aspirates and non-aspirates distinctively, the distinction may\nbe inherently less prominent or absent in the native language and\nhence the confusion. There is large scale hyper-correction: aadhi\n(ﬁrst, orginal) s’aMkara, vidhyaa (knowledge) gaNapati. Most of\nthe sign boards you see have one or more mistakes. These are all\ncognitive errors.\nPhonetic errors are also not inconceivable. aDugemane (kitchen)\nis quite often pronounced and written as aDigemane. huDugi (girl)\noften becomes huDigi. Non-initial vowels appear to be less impor-\ntant.\nFrequent and regular usages like huDgi especially in the\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n307\nspoken variety only substantiate this view. Even huDagi may be\nacceptable. This results in variations in spelling. Word initial ha\nis often pronounced as a - aasana (seat or chair) for haasana - the\nname of a place. One may become conscious of this error and may\neven resort to hyper-correction. These eﬀects in pronunciation can\nbe reﬂected in orthography leading to spelling errors.\nTypos are also possible for typed text although it is possible\nto design editors that disallow right away certain invalid combi-\nnations of symbols. For example, an editor may prevent an acci-\ndental second maatra for a consonant. Nonetheless, it will not be\npossible to prevent typos in all cases.\nFurther, there are often variations in spellings, some of which\nmay be more acceptable than others for speciﬁc purposes and thus\nspelling error correction can be viewed as more of a normalization\neﬀort. In Kannada, the spoken and written varieties of language\nare quite distinct (example - barutteene vs. baruttiini (I come)).",
    "may be more acceptable than others for speciﬁc purposes and thus\nspelling error correction can be viewed as more of a normalization\neﬀort. In Kannada, the spoken and written varieties of language\nare quite distinct (example - barutteene vs. baruttiini (I come)).\nDialectal variations also exist. There are loan words whose native\nsounds cannot be represented directly in orthography. There is\nno phonetically accurate way to write bank in Kannada and nat-\nurally new conventions have come to use. A common rendering is\nbyaank. There is no representation for the ‘fa’ sound and a con-\nvention that has come to widespread use is to write two dots (a\nspecial kind of nukta) below the ‘pa’ letter - as in kaapi (coﬀee).\nHowever, these conventions are not followed uniformly in all cases\nthus leading to variations in spelling. The half consonant ‘ra’ in\nconsonant clusters as in muurti (image/idol) is written in Kannada\nas a special symbol called arkaavattu and is written by conven-\ntion after the latter consonant in the cluster. However, in recent\nwritings, the default method of writing consonant clusters with\nthe ’ra’ consonant is used quite often instead of the ‘arkaavattu’.\nIn Telugu, words are split at arbitrary points and very often the\nsame author writes the same word in two diﬀerent ways within a\nsingle page. In eﬀect, there are many variations. Normalization is\nhighly desirable where the texts are intended to be subjected to\nautomatic language processing tools. Otherwise there would be\ndiﬃculties in searching, sorting and many other such processes.\nNormalization is not to be confused with standardization.\nStandardization is a sensitive issue.\nWe do not mean deﬁning\none or the other alternative form as the standard and imposing it\non everybody. That is not the point. The idea is two fold. Firstly,\n308\nCHAPTER 2. FOUNDATIONS OF NLP\na spell checker must provide options for the user to normalize the\ngiven texts as per his/her requirements. If a user wants all col-",
    "on everybody. That is not the point. The idea is two fold. Firstly,\n308\nCHAPTER 2. FOUNDATIONS OF NLP\na spell checker must provide options for the user to normalize the\ngiven texts as per his/her requirements. If a user wants all col-\nloquial forms to be replaced with formal varieties, a spell checker\ncan be designed to assist in the process. A user must be able to\nspecify which dialects are to be accepted and which ones are to\nbe treated as errors and substituted. Many of our languages have\na long history and there are archaic forms and modern forms. If\na user desires that he better avoid archaic forms, a spell checker\nmust assist him/her in that. Secondly, unwanted variability in the\ntext can be reduced by normalization of spellings. This will be\nvery useful for text processing applications in Indian languages.\nSpell checkers for Indian languages can and must be much more\nthan the usual spell checkers for other languages.\nSpell Checking in Morphologically Rich Languages:\nAlthough English has a rich and complex system of derivational\nmorphology, inﬂectional morphology is quite simple and straight\nforward. Most spell checkers for English therefore store the de-\nrived forms directly in the lexicon and apply rules of morphology\nonly for a few cases where the rules are simple and highly produc-\ntive. This approach is practically feasible and reasonably eﬃcient\nfor languages such as English.\nMorphological analysis and generation in Indian languages is\nquite an involved and complex task. Often there are six or seven\nlevels of aﬃxation, there being several possible aﬃxes at each level\nThus tiMduhaakiddanu ((he) had eaten) can be analyzed as\ntinnu\ni\nhaaku\ni\niru\nid\nanu\nRt:(eat)\npt-part.\nasp.aux.\npt-part.\nPerf.\nPt\nm,sl,p3\nIn languages such as Kannada, a verb root may give rise to\nseveral hundred thousands of complete words.\nDravidian lan-\nguages, especially Kannada and Telugu, are among the most com-\nplex languages of the world, comparable only to languages such",
    "Rt:(eat)\npt-part.\nasp.aux.\npt-part.\nPerf.\nPt\nm,sl,p3\nIn languages such as Kannada, a verb root may give rise to\nseveral hundred thousands of complete words.\nDravidian lan-\nguages, especially Kannada and Telugu, are among the most com-\nplex languages of the world, comparable only to languages such\nas Finnish and Turkish. A verb form may include several aspec-\ntual auxiliaries, clitics, particles and vocatives, apart from tense,\ngender, number and person suﬃxes.\nA word in Kannada of-\nten corresponds to a whole phrase in English.\nThus ‘although\n(I/we/you/he/she/it/they) had certainly wanted to come’ would\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n309\nbe just one word in Kannada - roughly ‘baraleebeekeMdukoMDid-\ndaruu’. Inter-word saMdhi and compounds add to the problem.\nIt is practically not feasible to store all forms of all words in the\ndictionary. A detailed morphological component is essential for\ndeveloping a good spell checker for languages such as Kannada.\nIn order to detect spelling errors, every word in the text has\nto be morphologically analyzed and checked with a dictionary.\nOnly then can we accept all and only the valid words and ﬂag\nthe others as erroneous. Spelling errors may occur in the roots,\nin the various aﬃxes or in the internal saMdhi that glues these\nparts into the whole word. Once the source of the error is found,\nappropriate suggestions for correcting that part may be generated\nusing a variety of techniques. Finally, the morphological generator\nwould be called to put together the correct parts to re-build the\ncomplete word form. Thus ‘hugaLannu’ will be analyzed as ‘hu\n+ gaLu + annu’ (noun + plural + accusative), ‘hu’ corrected to\n’huu’ (ﬂower)’, and then the correct form ‘huugaLannu’ generated.\nSimilarly, ‘huugalannu’ can be corrected as ’huugaLannu’. Com-\nplexity increases if there are multiple errors in a word. However,\nmultiple errors are relatively less frequent.\nThe block diagram below gives the overall structure of a typi-",
    "Similarly, ‘huugalannu’ can be corrected as ’huugaLannu’. Com-\nplexity increases if there are multiple errors in a word. However,\nmultiple errors are relatively less frequent.\nThe block diagram below gives the overall structure of a typi-\ncal spell checker for a morphologically rich language such as Kan-\nnada. The techniques used here can also be applied fruitfully for\nother languages. The morphological analyzer and generator can\nbe implemented in the Network and Process model.\nAs we have seen, morphological analysis and generation are\ncomplex processes and the performance of current systems is not\nvery high. We may therefore wish to apply hybrid techniques.\nType-token analysis is performed on a large and representative\ncorpus and the most frequent word forms are identiﬁed. A suitable\nthreshold can be determined which optimizes the spell checker\nperformance in terms of both false alarms and missed detections.\nHigh frequency word forms can be directly stored in the dictionary\nand morphology applied only in those cases where the word is not\nlisted in the dictionary. There are many advanced techniques for\nspell checker design. Interested readers will ﬁnd good papers.\n310\nCHAPTER 2. FOUNDATIONS OF NLP\nInput Text\nIdentify Erroneous Part(s)\nCorrection Algorithms\nMorphological Generator\nNot OK\nGet Next Word\nMorphological Analyser\nOK\nSuggestions        for Correcting Parts\nMake Corrections\nand Correction System\nIsolated Word  Error Detection\nCorrected Text\nFIG 2.21 A Typical Dictionary-Morphology based Spell\nChecker\nThere are no bench mark data for testing and evaluating the\nperformance of spell checkers in many Indian languages.\nMost\nof the spell checkers available today have not been thoroughly\nevaluated.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n311\n2.4.8\nOptical Character Recognition\nAn Optical Character Recognition (OCR) system converts a scann-\ned image of a text document into electronic text just as if the text\nmatter was typed-in by somebody.\nScanned images are much",
    "evaluated.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n311\n2.4.8\nOptical Character Recognition\nAn Optical Character Recognition (OCR) system converts a scann-\ned image of a text document into electronic text just as if the text\nmatter was typed-in by somebody.\nScanned images are much\nlarger in size compared to corresponding text ﬁles.\nThe state-\nment “A picture is worth one thousand words” is literally true\nhere. Texts occupy less storage space and less network bandwidth\nwhen sent across a network. Converting images into texts makes\nit possible to edit and process the contents as normal text. OCR\nsystems are therefore very useful.\nOCR systems can be used to convert available paper docu-\nments into electronic texts without typing.\nSince OCR engine\ncan be run day and night on several computers parallelly, we can\ngenerate large scale corpora with less time and eﬀort. OCR en-\ngines can also be used for a variety of other applications. OCR\nsystems have just started appearing for Indian scripts. Most of\nthe current OCR systems for Indian languages are designed only\nfor printed texts and perform well only on reasonably good quality\ndocuments. Research work is going on for handling hand-written\ndocuments. Handling old manuscripts is more complex. The pa-\nper or other base materials used would have deteriorated, coloured\nand noisy. Also the character shapes used may be more complex\nand quite diﬀerent from the shapes used in modern digital fonts.\nSystem Overview\nAn OCR system typically contains three stages: preprocessing\nstage, recognition stage and post-processing stage. Binarization,\nseparation of image regions into textual and graphical regions,\nmulti-column detection and skew correction are some of the ma-\njor tasks performed in preprocessing phase. Separation of text\ninto glyphs, characters, words and lines, and recognition of indi-\nvidual glyphs are tasks of the recognition stage. Post-processing\ncomprises combining the recognized glyphs into valid aksharas",
    "jor tasks performed in preprocessing phase. Separation of text\ninto glyphs, characters, words and lines, and recognition of indi-\nvidual glyphs are tasks of the recognition stage. Post-processing\ncomprises combining the recognized glyphs into valid aksharas\nand words, spell-checking, etc. Optical character recognition is a\nvast ﬁeld and there are a large number of alternative technolo-\ngies at every level. The following paragraphs sketch some of the\nsimple techniques that have been eﬀectively used in OCR. The\ntreatment here is not intended to be comprehensive or suggestive\nof the best or recommended methods. OCR is currently an active\n312\nCHAPTER 2. FOUNDATIONS OF NLP\narea of research and the interested readers will ﬁnd a vast body\nof literature giving more technical details.\nPreprocessing Stage\nBinarization: refers to the conversion of a scanned gray level im-\nage into a two-tone or binary (pure black and white) image. A\nbinary image is appropriate for OCR work as the image docu-\nment contains only two useful classes of data — the background,\nsay the paper, and the foreground, the printed text. It is common\nto represent the background paper colour by white-coloured pixels\nand the text by black-coloured pixels. In image processing jargon,\nthe background pixels have a value of 1 and the foreground pixels\nhave a value of 0. Binarization has a signiﬁcant impact because it\nprovides input to every other stage of an OCR system. All pixels\ndarker than a threshold are mapped to pure black and the rest\nof the pixels are mapped to pure white. Several strategies can\nbe used for binarization to achieve desired performance on diﬀer-\nent types of scanned documents and scanners. Global, percentile\nbased and iterative methods have been applied to identify the best\nthreshold.\nSkew Detection and Correction: stages deal with improper\nalignment of a document while it is scanned. The usual eﬀect of\nskew could be that the lines of text or no longer horizontal but at",
    "based and iterative methods have been applied to identify the best\nthreshold.\nSkew Detection and Correction: stages deal with improper\nalignment of a document while it is scanned. The usual eﬀect of\nskew could be that the lines of text or no longer horizontal but at\nan angle, called the skew angle. Documents with skew cause line,\nword and character breaking routines to fail. Skew also causes\nreduction in recognition accuracy. Skew detection and correction\ncan be done by, say, maximizing the variance in horizontal pro-\njection proﬁle.\nText and Graphics Separation: refers to the process of identify-\ning which regions of the document image contain text and which\nregions contain pictures and other non-text information that is\nnot relevant to the OCR system. Horizontal and vertical projec-\ntion proﬁles can be used for such separation as well as for many\nother preprocessing operations (see below). A horizontal proﬁle\nis obtained by counting and plotting the number of text or black\npixels in each row of the image. A vertical proﬁle is obtained by\ncounting the black pixels in each column of the image. Horizontal\nproﬁles show distinct peaks that correspond to lines of text and\nvalleys that result from inter-line gaps. A line of text is revealed\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n313\nby a peak in the horizontal proﬁle whose width is approximately\nthe font size. A graphic object, in contrast, is much larger. The\nactual shape of the peak is also diﬀerent because of higher den-\nsity of black pixels in a graphics block. Thus, the proﬁle shapes\ndiscriminate between text and graphical blocks.\nMulti-column Text Detection: can be done using Recursive X-\nY Cuts technique. It is based on recursively splitting a document\ninto rectangular regions using vertical and horizontal projection\nproﬁles alternately.\nThe use of horizontal and vertical projection proﬁles for all\nthe major preprocessing tasks minimizes system complexity and\nallows faster processing of documents.\nRecognition Stage",
    "into rectangular regions using vertical and horizontal projection\nproﬁles alternately.\nThe use of horizontal and vertical projection proﬁles for all\nthe major preprocessing tasks minimizes system complexity and\nallows faster processing of documents.\nRecognition Stage\nLine, Word, Character and Glyph Separation: is a very impor-\ntant task as the recognition engine processes only one glyph at\na time.\nWord and glyph separation are the key steps.\nIn one\nrecent successful system, word segmentation has been done using\na combination of Run-Length Smearing Algorithm (RLSA) and\nConnected-Component Labelling. Words are combined into lines\nusing simple heuristics based on their locations. The performance\nof RLSA in accurately segmenting words is very high on good qual-\nity text but drops in the presence of complex layouts and tightly\npacked text that is sometimes seen in magazines. A variety of zon-\ning techniques have also be used. Words can be decomposed into\nglyphs by running the connected component labelling algorithm\nagain. The method is conceptually simple and glyph separation\ncan be very accurate.\nRecognition: There are broadly two approaches to recognition\n- template matching and classiﬁcation based on features. Direct\nmatching rarely works but reﬁned template matching algorithms\ncan actually give fairly good recognition performance. One such\ntemplate matching technique that has been used very success-\nfully is based on the notion of fringe distance where the distance\nbetween two images is a function of the distance of each black\npixel in one image to the nearest black pixel in the other image.\nA database is created from standard glyph shapes and any new\nglyph image can be matched against the templates stored in the\ndatabase and the reference template that matches best could be\n314\nCHAPTER 2. FOUNDATIONS OF NLP\ngiven out as the recognized output.\nThe two images need to be scaled and normalized before such\na comparison can be made. We may use a linear scaling algo-",
    "database and the reference template that matches best could be\n314\nCHAPTER 2. FOUNDATIONS OF NLP\ngiven out as the recognized output.\nThe two images need to be scaled and normalized before such\na comparison can be made. We may use a linear scaling algo-\nrithm that uniformly scales all parts of the image to the required\nsize. Linear scaling is fast but suﬀers from problems with complex\nshaped glyphs at large font sizes and with small glyphs at small\nfont sizes. Non-linear normalization can improve performance by\nselectively scaling regions of low curvature. Punctuation marks,\nwhich are easily distorted because of their small sizes, are usually\nhandled separately. For example, heuristics based on location and\nstacking can be used to handle these punctuation marks.\nThere is a large variety of features that can be used to discrim-\ninate between various glyphs, a large variety of distance measures\nto compute the distance or dissimilarity and a variety of classi-\nﬁcation techniques to ﬁnally classify a given glyph assign a class\nlabel. Interested readers will ﬁnd more details in the published\npapers.\nPost-processing Stage\nAssembling Glyphs into aksharas: is much more challenging than\nit appears in the case of Indian scripts. For example, in Telugu\nscript, glyphs can be scaled and placed on top, to the left bottom,\ndirectly below, to the right bottom or to the right side of another\nbase glyph. The glyphs may not be recognized in the same order\nin which they are required to assemble aksharas correctly. Glyph\nshapes may be same but distinctions may have to be made based\non relative size and relative location. The end result of this com-\nplex assembly process is the text encoded in a suitable character\nencoding scheme such as ISCII or UNICODE.\nSpell-Checker: OCR works glyph by glyph and there can be\nerrors of omission and commission. The kinds of errors made by an\nOCR engine are quite diﬀerent from the kinds of spelling mistakes",
    "plex assembly process is the text encoded in a suitable character\nencoding scheme such as ISCII or UNICODE.\nSpell-Checker: OCR works glyph by glyph and there can be\nerrors of omission and commission. The kinds of errors made by an\nOCR engine are quite diﬀerent from the kinds of spelling mistakes\nwe make when we type from a computer keyboard. Statistical\napproaches are generally preferred for building spell checkers for\nOCR systems.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n315\nFIG 2.22 Assembling Glyphs into Aksharas\nOCR systems, like other systems in language engineering and\nNLP, need to be tested against standard benchmark data and\n316\nCHAPTER 2. FOUNDATIONS OF NLP\nstandard test procedures. It is traditional in OCR literature to\nspecify recognition accuracies in terms of glyphs or characters but\nwhat is more meaningful for end users is accuracy expressed, say,\nin terms of words. Benchmark standards, ground truth data and\ntools for developing such ground truth data are all being developed\nfor Indian scripts. OCR systems can play a very important role\nin the coming years.\n2.4.9\nLanguage Identiﬁcation\nThere is an increasing need to deal with multi-lingual documents\ntoday. Most of language technology applications in both the text\nand speech domains are, however, inherently language speciﬁc.\nA spell checker designed for Hindi cannot be applied directly on\nMarathi. It becomes necessary, therefore, to ﬁrst segment docu-\nments language-wise. Then the Hindi spell checker can be used\nfor the Hindi parts and the Marathi spell checker applied to the\nMarathi parts. Language Identiﬁcation has been incorporated or\nintegrated into many applications including Text Categorization\nand Text Retrieval.\nAs we have seen already, there are a large number of languages\nand scripts used in India. English, Hindi and other local languages\nare often mixed as a matter of policy and practice. Mixing San-\nskrit and local languages in a single text is also very common. In",
    "and Text Retrieval.\nAs we have seen already, there are a large number of languages\nand scripts used in India. English, Hindi and other local languages\nare often mixed as a matter of policy and practice. Mixing San-\nskrit and local languages in a single text is also very common. In\nmost cases it is a case of frequent code switching but code mix-\ning is also observed. Therefore Language Identiﬁcation is all the\nmore relevant in our country. Interestingly, the correspondence\nbetween languages and scripts is not strictly one to one - some\nscripts are used for writing several languages and some languages\nare written in more than one script. Devanagari script is used to\nwrite Sanskrit, Hindi, Marathi, Konkani and Sindhi. Sanskrit is\nwritten in almost all the diﬀerent scripts. Therefore mere script\nidentiﬁcation is not suﬃcient. It is important to be able to identify\nlanguage irrespective of the script or font being used.\nWe have seen that ISCII encodes texts in terms of sound units\nand a plain ISCII document has no explicit indication of language.\nAutomatically identifying language from small text samples in\nISCII texts is therefore very important. It may also be noted that\nswitching to UNICODE will not solve the problem - UNICODE\nprovides code spaces for scripts, not necessarily one for each lan-\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n317\nguage. Devanagari script is used for several languages including\nSanskrit, Hindi, Marathi, Nepali and Konkani. The Bengali script\nis also used for Assamese.\nInstead of viewing Language Identiﬁcation as a document seg-\nmentation problem, it is possible to take the somewhat simpler\nview of classifying a given small segment of text into one of the\ngiven set of languages.\nLanguage Identiﬁcation can be viewed\nas a generic machine learning problem, a supervised classiﬁcation\ntask in which features extracted from a training corpus are used\nfor classiﬁcation. Any of the machine learning techniques can be\nused.",
    "given set of languages.\nLanguage Identiﬁcation can be viewed\nas a generic machine learning problem, a supervised classiﬁcation\ntask in which features extracted from a training corpus are used\nfor classiﬁcation. Any of the machine learning techniques can be\nused.\nIn the last decade or so, corpus based Machine Learning ap-\nproaches have become predominant in language engineering over\nthe Knowledge Based approaches which use explicit rules hand-\ncrafted by domain experts. Recent research on Language Identi-\nﬁcation has been limited almost exclusively to Machine Learning\napproaches.\nA Machine Learning system is expected to be generic and it is\nunderstood that training is based only on the intrinsic properties\nof the data, as expressed through a set of “features”. Extraneous\nindicators such as clues from scripts or fonts used, header infor-\nmation or explicit markup tags in the document structure cannot\nbe used.\nThe basic idea is that each language uses a unique or a very\ncharacteristic alphabet, and the letters of the alphabet appear\nwith surprisingly consistent frequencies in any statistically signif-\nicant text. In addition, the frequency of occurrence of sequences\nof two, three, four and more letters are characteristically stable\nwithin, but diverse among diﬀerent natural languages. The most\nfrequent 3-grams, 4-grams etc. have been used for language iden-\ntiﬁcation. A crucial part of the recognition system is the identiﬁ-\ncation of the set of most distinctive, most frequently encountered\nsequences of characters (that is, bigrams, trigrams, etc.) that can\nbe associated with each language. Distinctiveness implies that the\nfrequency of a letter combination for a given language should be\nhigh relative to the frequency of occurrence in other languages.\nIn alphabetic writing systems such as those used for English\nand other European languages, a character is simply a letter of\nthe alphabet (or a punctuation mark, a digit or other special sym-",
    "high relative to the frequency of occurrence in other languages.\nIn alphabetic writing systems such as those used for English\nand other European languages, a character is simply a letter of\nthe alphabet (or a punctuation mark, a digit or other special sym-\nbol), which is typically represented as a single byte in a character\n318\nCHAPTER 2. FOUNDATIONS OF NLP\nencoding scheme such as ASCII. Researchers dealing with such\nlanguages have naturally chosen a byte as the basic unit of text.\nFeatures such as n-grams are deﬁned in terms of bytes. Indian\nscripts are not alphabetic but rather syllabic in nature. Aksharas\nare the atomic units of writing - individual bytes have no sig-\nniﬁcance in Indian scripts. Texts are to be treated sequences of\naksharas.\nSome researchers have used lists of frequent words to distin-\nguish one language from the other. Comparing with stored lists\nof frequent words can be very eﬀective for language identiﬁca-\ntion. Our experiments using word lists with Indian Languages,\nnot described here, also conﬁrm this point. However, there are\nseveral objections to the use of lists of words, aﬃxes etc. As test\nsamples become smaller, chances of ﬁnding full words reduce. In\nsmall samples, words may be cut and storing lists of full words\nwill be of no use.\nThe most frequent words are usually closed\nclass grammatical words such as determiners, prepositions and\nconjunctions and carry little semantic information.\nSmall text\nsamples exacerbate the bursty nature of texts where such closed\nclass words surround pockets of less common words. It is these\nless common words that may in fact be more useful for language\nidentiﬁcation between certain languages than the small function\nwords. Which words to include in a word list is therefore an open\nquestion. Lastly, statistical features such as n-grams in any case\ninclude the information contained in small, frequent words, aﬃxes\netc. Given these facts and the desire to build generic, trainable",
    "words. Which words to include in a word list is therefore an open\nquestion. Lastly, statistical features such as n-grams in any case\ninclude the information contained in small, frequent words, aﬃxes\netc. Given these facts and the desire to build generic, trainable\nlanguage identiﬁcation systems, machine learning approaches that\ndepend solely on features extracted from data are preferred.\nA number of language identiﬁcation systems have been built\nfor various language groups of the world. Despite its great rele-\nvance, Language Identiﬁcation had, however remained a largely\nunexplored area for Indian languages. Recently, Multiple Linear\nRegression has been successfully applied to develop high perfor-\nmance pair-wise language identiﬁcation among the major Indian\nlanguages. An F-Measure of 98% plus has been achieved for In-\ndian languages when test samples were about 10 aksharas in size\nand the performance went up to nearly 100% when the sample size\nwas increased to about 25 aksharas. Note that aksharas are ap-\npropriate units of text for Indian scripts, not bytes. Experimental\nresults also corroborate this claim.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n319\nTexts are treated a sequences of aksharas. The script gram-\nmar is used to segment texts into aksharas. Aksharas are smaller\nunits than full words. The number of frequently used aksharas is\nalso smaller than number of words. The number of distinct words\nis of the order of hundreds of thousands whereas aksharas in com-\nmon use are in thousands. Studies have shown that about 5000\naksharas account for more than 99% of all words in all the major\nIndian languages. Thus the size of the training corpus required\nwill be much smaller if we use akshara based features. There is\nno need to talk in terms of words or morphemes.\nMonograms, bigrams and trigrams of aksharas can be used as\nfeatures. We may also consider positional features - word initial,\nword medial and word ﬁnal n-grams may be distinguished. It is",
    "will be much smaller if we use akshara based features. There is\nno need to talk in terms of words or morphemes.\nMonograms, bigrams and trigrams of aksharas can be used as\nfeatures. We may also consider positional features - word initial,\nword medial and word ﬁnal n-grams may be distinguished. It is\nuseful to use diﬀerential features - n-grams that occur frequently\nin one language but not in the other can be extracted from a\ncorpus and listed in a table.\nA two level feature extraction process may be followed. In the\nﬁrst level, text corpora are used to extract akshara level mono-\ngrams, bigrams and trigrams. Only the most frequently occurring\nand diﬀerential features are retained. The result is a set of ta-\nbles for each pair of languages. The tables simply list the akshara\nlevel monograms, bigrams and trigrams that occur frequently in\nthe ﬁrst language but not in the second, and vice versa. The fre-\nquencies themselves are not stored. After this step, full corpora\nare never used again.\nIn the second phase, training samples are extracted randomly\nfrom the corpora and used for estimating the parameters of the\nregression model. A training data set consists of random samples\ncontaining only a small number of aksharas. Since the features\nare deﬁned in a diﬀerential manner, the actual feature values are\nobtained by simply counting the occurrences of these features in\nthe training samples. For example, all possible trigrams are ex-\ntracted from a training sample and each is checked in the feature\ntables obtained in the ﬁrst phase. The value of the trigram fea-\nture for language L1 is the total number of these trigrams found\nfrequently in L1 but not in L2. Thus the feature values are all\nintegers.\nNote that the feature values are not computed directly and\nsolely from the training samples. Instead, they are expressed in\nterms of the prior knowledge obtained from corpora as encapsu-\n320\nCHAPTER 2. FOUNDATIONS OF NLP",
    "frequently in L1 but not in L2. Thus the feature values are all\nintegers.\nNote that the feature values are not computed directly and\nsolely from the training samples. Instead, they are expressed in\nterms of the prior knowledge obtained from corpora as encapsu-\n320\nCHAPTER 2. FOUNDATIONS OF NLP\nlated in the tables obtained in the ﬁrst phase. Since all we need is\nplain text corpora and small corpora are suﬃcient, this two-stage\nfeature extraction is feasible and practicable. The features so ex-\ntracted can be expected to be more robust and more reliable than\nfeatures extracted directly from small training samples.\nIf we are using the MLR technique, the parameters of the\nregression equation are estimated from the training samples ran-\ndomly extracted from the corpora. Then testing can be carried\nout on test data, also extracted randomly from the rest of the cor-\npus. Each sample is analyzed into aksharas and the diﬀerential\nfeature values are obtained. The value of the decision variable\nis computed and the sample classiﬁed accordingly. The perfor-\nmance is measured in terms of Precision, Recall and F-Measure.\nExperiments may be conducted to ascertain the eﬀect of number\nof training samples, size of training samples, size of test samples,\nrelative signiﬁcance of features in various combinations etc. Ex-\nperiments can also be repeated for purposes of cross-validation.\nExperiments can also be performed with diﬀerent values for\nthe threshold in order to explore the trade-oﬀbetween Precision\nand Recall. When encountered with the task of identifying the\nlanguage of a small piece of text, it is possible to initially look for\na high-precision, low-recall solution and reduce the threshold value\niteratively in case identiﬁcation fails until a solution is obtained.\nThe diﬀerences between the within-language-family and across-\nlanguage-family cases may be explored. We can see the degree of\n“closeness” between various language pairs. Hindi and Punjabi",
    "iteratively in case identiﬁcation fails until a solution is obtained.\nThe diﬀerences between the within-language-family and across-\nlanguage-family cases may be explored. We can see the degree of\n“closeness” between various language pairs. Hindi and Punjabi\nmay show up to be closer than, say, Oriya and Punjabi. When\nwe need to recognize languages that are closer to one another, we\nmay need more sophisticated features or larger data to get the\nsame level of performance. This idea can be extended further to\nstudy other kinds of variations among languages or language fam-\nilies as also to uncover universal, language-invariant features in a\nquantitative way.\nNote that machine learning methods are completely generic\nand hence the same program can be used for identiﬁcation be-\ntween any two languages - all that we need is suitable training\ncorpora and a few minutes of time to retrain the system on the\nnew training data. We do not need any word lists or other lin-\nguistic information about the languages being distinguished. This\ngeneric nature and adaptability is the most important merit of\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n321\nmachine learning techniques. Linguistic approaches, on the other\nhand, normally require careful linguistic study for each language\nunder consideration and hence a lot of time and eﬀort. Machine\nlearning techniques can be adapted to new languages within min-\nutes and without any manual eﬀort.\n2.4.10\nOthers Technologies for Indian Languages\nMachine translation has been taken quite seriously in India and\na lot of progress has been made. Major focus has been laid on\ntranslating between Indian languages, exploiting the commonness\nof these languages along the linguistic and cultural dimensions.\nRecently, there is also an increased emphasis on translation be-\ntween English and Indian languages. While demonstration level\nsystems have been built, there is a long way to go before machine\ntranslation can be applied in real life situations.",
    "Recently, there is also an increased emphasis on translation be-\ntween English and Indian languages. While demonstration level\nsystems have been built, there is a long way to go before machine\ntranslation can be applied in real life situations.\nThere is some scattered work on Information Retrieval, Infor-\nmation Extraction, Text summarization, Text and Web Mining,\netc. Strong, focussed, long term eﬀorts are rarely seen.\nThere is an acute paucity of lexical resources including plain\nand annotated corpora, parallel corpora, dictionaries, thesauri,\nWordNets etc. Greater emphasis is being given to development of\nlexical resources and hopefully things would be much better soon.\nIn spite of being a country with many languages, high degree\nof illiteracy and basically an oral tradition, speech has taken a\nback seat. Only a couple of centres have been active in speech\nfor a long time. There is now a realization that the future lies in\nspeech. There is even some thought on speech based cross-lingual\ninformation access and speech-to-speech translation. Closer inte-\ngration of NLP and speech technologies is the need of the hour.\n2.4.11\nNLP and Sanskrit\nThere is a common misconception that Sanskrit is the best lan-\nguage for NLP. What does this mean?\nShould we stop using\nall other languages and start using only Sanskrit for everything?\nThat would not make much sense.\nMany people would surely\nwant to learn Sanskrit but not for the sake of giving up their own\nmother tongue, not because computers would start demanding\nSanskrit.\n322\nCHAPTER 2. FOUNDATIONS OF NLP\nIt is also not true that Sanskrit is a lot more amenable for\nautomatic processing by the computer. True, Sanskrit has an ex-\ntremely systematic and scientiﬁc way of dealing with all aspects\nof language starting from the alphabet through words, phonetics,\nphonology, morphology and syntax to semantics and proper in-\nterpretation of meanings. Yet all this is designed for intelligent",
    "tremely systematic and scientiﬁc way of dealing with all aspects\nof language starting from the alphabet through words, phonetics,\nphonology, morphology and syntax to semantics and proper in-\nterpretation of meanings. Yet all this is designed for intelligent\nhuman beings with common-sense, not for dumb machines.\nA\nsubstantial degree of common sense and world knowledge is re-\nquired to process and understand any language for that matter\nand perhaps it is only more so in the case of Sanskrit. We have\nseen for example that compounds are very common in Sanskrit\nand interpreting compounds often requires human intelligence. It\nis also very common to leave out portions that we can ﬁll up\nourselves. Sanskrit statements are often highly elliptic. Handing\nellipses is still a very diﬃcult task for computers. Further, if one\nwere to look at Sanskrit as it is actually used, you will very of-\nten ﬁnd highly abstract and cryptic statements, calling for expert\ninterpretation and explanation from a Guru. Knowing the literal\nmeanings is of little use. If you come across the statement You\nare That what sense will you make out of this?\nYet knowing Sanskrit may perhaps help in two ways. Firstly,\nmodern Indian languages have a lot of things borrowed from San-\nskrit and partly or wholly assimilated into them. Knowing San-\nskrit helps one to get a deeper insight into the structure and\nmeaning of words and sentences in our languages. One can start\nunderstanding the beauty of language in a better way. We will\nget to know how diﬀerent languages have adapted, assimilated,\ndeveloped and grown in their own ways. We can begin to under-\nstand and appreciate even those languages which we knew noth-\ning about. You become a bit more broad minded. Secondly, and\nmuch more importantly, knowing Sanskrit helps us to get a more\nscientiﬁc and systematic way of dealing with language at all lev-\nels. Sanskrit not only has an excellent grammar, one that is hard",
    "ing about. You become a bit more broad minded. Secondly, and\nmuch more importantly, knowing Sanskrit helps us to get a more\nscientiﬁc and systematic way of dealing with language at all lev-\nels. Sanskrit not only has an excellent grammar, one that is hard\nto ﬁnd fault with, there is a whole science of grammar of which\nthe Sanskrit grammar is just an instance. It is not the Sanskrit\ngrammar itself that is so very important, it is the underlying sci-\nence of grammar that is crucial. The same is true of phonetics,\nphonology, morphology and semantics. Linguists cannot aﬀord to\nbe ignorant of Sanskrit.\nA whole ocean of knowledge exists in Sanskrit on almost all\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n323\naspects of language, meaning, logic and understanding. However,\nsimply knowing Sanskrit is not suﬃcient to make full use of all\nthis knowledge. Today we are unable to leverage this wealth of\nknowledge and experience not just because the number of people\nwho know Sanskrit is less. This is due largely to the very na-\nture of these traditional knowledge sources. According to Indian\ntradition, knowledge was not meant for all - in fact every eﬀort\nwas made to ensure that knowledge does not reach the hands of\nthe “un-deserving”. (If everybody is taught how to make bombs\nyou know what happens. There are no “right” hands for bombs\n- if you are “right” you will not need a bomb at all. The only\nhands interested in bombs are the “wrong hands”) It is foolish\nto publish all knowledge and then worry about it boomeranging\nback on you. Hence Gurus were extremely choosy about whom\nthey will teach and how much they will teach. Knowledge was\nmeant only for those who are extremely serious - those who con-\nsider seeking knowledge as the main goal of life and those who are\nadjudged to be capable of correct interpretation and proper appli-\ncation of that knowledge for righteous purposes. Given these, it\nwas expected that a seeker of knowledge should search for the right",
    "sider seeking knowledge as the main goal of life and those who are\nadjudged to be capable of correct interpretation and proper appli-\ncation of that knowledge for righteous purposes. Given these, it\nwas expected that a seeker of knowledge should search for the right\nteacher (guru) and learn from him. Knowledge was not “pushed”\nonto everybody, one had to seek and “pull” knowledge with eﬀort.\nPublic knowledge was thus limited. An extremely cryptic style is\nfollowed - often in the form of suutras (aphorisms, formulae). It\nis the nature of the tradition that commentaries are written on\nthe original texts to make them easier to understand and com-\nmentaries are written on such commentaries! To this day, getting\nan in-depth understanding of these works requires spending years\nof your life with a guru. The number of serious students within\nthe Indian tradition is steadily decreasing and so it is becoming\nmore and more diﬃcult even to ﬁnd a good guru. Added to all\nthis is the diﬃculty in communicating with traditional scholars in\na parlance that make sense to a modern language engineer. Fur-\nther, the original purpose of these works were very diﬀerent from\nthe purposes for which we wish to use them today. By and large,\nthis wealth of traditional knowledge has remained dormant and\nlargely unused. Our future lies in our past.\n324\nCHAPTER 2. FOUNDATIONS OF NLP\n2.4.12\nEpilogue\nWe have noted that developments in technology for Indian lan-\nguages has been painfully slow and the major reasons for this\ninclude non-availability of large scale data resources and slow de-\nvelopment of basic tools and technologies, partly due to inherent\ncomplexity of our languages.\nLanguage technology issues are not widely known or appreci-\nated in India. These technologies are not part of the curriculum\nand trained manpower is in short supply. It is also not easy to\nmotivate people to work in this area. Lack of adequate manpower\nis one of the main reasons for the slow progress.",
    "ated in India. These technologies are not part of the curriculum\nand trained manpower is in short supply. It is also not easy to\nmotivate people to work in this area. Lack of adequate manpower\nis one of the main reasons for the slow progress.\nThere is a great apathy towards standards and good engineer-\ning practices. The number of people working on language tech-\nnologies is so small and there is so much work to do that there is a\ngreat tendency to take short cuts. In many cases there are neither\nany written speciﬁcations nor design documents. Researchers get\ndown to writing code straight away. There are no benchmark data\nor established standards for testing and evaluation. There are no\nseparate teams for evaluation and the developers themselves dou-\nble as evaluators of their own software. At times training is done\non test data and testing on the training data. Thus results claimed\nby researchers cannot always be taken on the face value.\nPartly due to lack of standards and proper engineering prac-\ntices and partly due to the feeling that there is so much to do\nbut not enough hands, researchers tend to take “big” ideas and\ntry to become Jack of all. As an example, there are many teams\nworking on Text-to-Speech systems and each team develops its\nown text normalization tools. Would it not be better if each team\ntakes up one small idea at a time and does a thorough job of it\nso that everybody can just take and use that one small module?\nThese are matters of perceptions. If you are doing TTS and Ma-\nchine Translation then you are doing some big thing but if you are\ndeveloping a good dictionary or a morphological analyzer that is\nnot going to catch anybody’s attention in a big way. Most teams\ntry to develop everything from scratch on their own rather than\nteam up with others. This may be partly due to lack of mutual\ntrust and understanding. As a country, we have the potential to\nbecome world leaders in language technologies. But we need to\nlearn many basic lessons ﬁrst.",
    "try to develop everything from scratch on their own rather than\nteam up with others. This may be partly due to lack of mutual\ntrust and understanding. As a country, we have the potential to\nbecome world leaders in language technologies. But we need to\nlearn many basic lessons ﬁrst.\n2.5. CONCLUSIONS\n325\n2.5\nConclusions\nBy now you will have realized that NLP is a vast and highly\ninter-disciplinary subject. It is not possible to cover everything in\ndetail in a single book. The purpose of this book is only to give\nyou an initial idea about the nature of problems and issues, kinds\nof approaches that can be and have been taken, some idea about\nthe current status and challenges yet to be overcome and some\ngeneral idea about the shape of things to come. Each of the topics\ncovered here is a vast subject in itself and the readers interested\nin pursuing any of the areas in more depth will ﬁnd many good\nbooks and research articles. We have seen that although a great\ndeal of progress has been made over the last ﬁve decades, many of\nthe core problems remain unsolved. There are many interesting\nand useful applications and there is a lot that needs to be done. If\nthis book helps beginners to get interested and to kick start serious\nexplorations and research, it would have served its purpose.\nWe have been looking at NLP both as an independent ﬁeld of\nstudy and in the context of Information retrieval and other closely\nrelated applications. We will get back to IR in the next chapter\nand look at some of the advances that are being made in that\nﬁeld.\n326\nCHAPTER 2. FOUNDATIONS OF NLP\nChapter 3\nAdvances in\nInformation Retrieval\nInformation Retrieval is all about eﬃcient storage and retrieval\nof documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.",
    "of documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nIn this chapter we will look at some the recent advances in\nthe ﬁeld of Information Retrieval. We begin with a brief historical\nsketch and then give the basic IR model. Then we look at various\ndirections of development of the IR ﬁeld. The idea of Intelligent\nInformation Retrieval will be introduced and the need for deeper\nlinguistic analysis will be highlighted.\n3.1\nA Brief History of Information Re-\ntrieval\nInitial explorations of text retrieval systems from small collections\nof scientiﬁc abstracts, legal and business documents etc. began\nin the nineteen sixties and seventies. Foundations of Boolean and\nvector-space models were developed. During these early days, doc-\numents were studied and brief descriptors or lists of index terms\nwere manually prepared for each document.\n327\n328\nCHAPTER 3. ADVANCES IN IR\nDuring the eighties, large scale document databases started\nappearing.\nLexis-Nexis, Dialog and MEDLINE are noteworthy\nexamples of such databases. The need for eﬃcient retrieval from\nlarge collections was increasingly felt. This gave a big push to IR\nresearch and development.\nDuring the nineties, focus shifted to searching FTP-able doc-\numents on Internet (example: Archie, WAIS) and searching the\nWorld Wide Web (example: Lycos, Yahoo, Altavista). Organized\ncompetitions such as NIST TREC were held. Recommender sys-\ntems (example: Ringo, Amazon, NetPerceptions) appeared. Au-\ntomated text categorization and clustering systems were devel-\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information",
    "oped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nretrieval. There is an increasing cross-fertilization and integration\nof related technologies including speech.\nCurrently indexing is performed automatically on full texts.\nManual processing is slow, costly and may be inconsistent. On the\nother hand, while automatic processing can be very fast, it lacks\nthe commonsense and human judgement of manual methods and\ncan therefore be somewhat inferior in quality. The challenge today\nis to keep the superior speed factor and yet achieve near-human\nperformance through automatic methods.\nIt is clear that IR today is closely related to many other dis-\nciplines.\nIt interfaces with Database Management systems, Li-\nbrary and Information sciences, Artiﬁcial Intelligence, Natural\nLanguage Processing and Machine Learning. Database Manage-\nment systems focus on structured data stored in tables and eﬃ-\ncient processing of precisely deﬁned queries expressed in a formal\nlanguage such as SQL. The syntax and semantics of the data as\nwell as the query are clear. Recent trends towards semi-structured\ndata such as XML brings it closer to IR. Library and Information\nScience has focused on the human user aspects such as human-\ncomputer interaction, user interfaces and visualization. Eﬀective\ncategorization of human knowledge is a primary goal. Citation\nanalysis and bibliometrics are focus areas. Recent work in digital\nlibraries is bringing library science closer to computer science and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and",
    "information retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a",
    "rise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\ndatabase or an index of web pages. The task of creating an in-\ndex is made complicated by the large size of the data as also the\nhighly dynamic nature of the web, lack of any centralized con-\ntrol and the heterogeneity of the document types and formats.\nToday IR means retrieving relevant documents from very large\ncollections such as from the web. Full text indexing is taken for\ngranted.\n330\nCHAPTER 3. ADVANCES IN IR\n3.2\nBasic IR Models\nIn ad-hoc retrieval, the most common view of IR, an unaided\nuser expresses his need through a short query. The IR system\nmatches the query against the documents in the collection and\nreturns the documents that match. The returned documents may\nbe given in a ranked order.\nThe document collection is ﬁxed\nand a one time indexing on the whole collection is performed to\ncreate and store an index. Matching is performed on the index,\nnot on the original documents. Each time a user issues a query, a\nmatch operation is performed and the results returned. Document\nFiltering provides an alternative view. Here the query may be\nconsidered ﬁxed and a stream of documents need to be checked.\nThe query expresses what exactly a particular user wants or does\nnot want. Documents are ﬁltered out accordingly. There is no\nscope to perform indexing of the whole collection of documents\nsince the collection is not available beforehand and documents\nkeep coming. Here the decision is usually Boolean - include or\nexclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models",
    "exclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models\nAn IR model must specify four things:\n• Document Representation\n• Query Representation\n• Deﬁnition of Relevance\n• Matching/Retrieval Function\nThe simplest way one can think of matching documents and\nquery is to perform direct keyword search - search for the given\nquery string verbatim in the documents and return the documents\nthat contain the given query string. This is too rigid. Computer\nwill not match Computers or even computer if the search is case\nsensitive. So some kind of soft match may be introduced. Even\nwith that extension the search is too rigid.\nSuppose you want\ndocuments containing the words Sun, Moon, orbit, eclipse but not\n3.2. BASIC IR MODELS\n331\nnecessarily all at one place and not necessarily exactly in the order\ngiven. It would then be helpful to consider documents and queries\nas unordered collections of words. This is called that bag-of-words\nrepresentation. A bag is an unordered collection of items just like\na set, only multiple occurrences are allowed. Some early systems\nused pre-speciﬁed sets of index terms. Current systems tend to\nprefer full text indexing. It is also possible to exploit structure\nof documents and meta-data ( such as URL, title, anchor string,\nauthor and other meta tags, font sizes, capitalization and position\nin the document). Thus the HTML/XML structure in hypertext\ndocuments could be made use of.\nThe Boolean Retrieval Model\nThe Boolean model is based on set theoretic principles. Here doc-\numents are treated as sets of keywords.\nNote that a set is an\nunordered collection of items. Frequency of occurrence is not rel-\nevant. In a set an item is either present or absent. Items cannot\nrepeat. Queries are treated as Boolean expressions of keywords,",
    "uments are treated as sets of keywords.\nNote that a set is an\nunordered collection of items. Frequency of occurrence is not rel-\nevant. In a set an item is either present or absent. Items cannot\nrepeat. Queries are treated as Boolean expressions of keywords,\nconnected by logical operators such as AND, OR and NOT. Brack-\nets can be used to explicate scope or for over-riding the default\nscope of operators. Thus we could search for:\n(Hotel && (Three Star || Five Star)) && !Sheraton\nAny given document will either match or not match. There\nare no partial matches. There are no ranks.\nThe Boolean model is simple, easy to understand and use,\nclean and eﬃcient. It is possible to strictly enforce which terms\nshould or should not be present. Boolean models were quite widely\nused at one point of time. However, these models are very rigid.\nAND means ALL and OR means ANY. All matched documents\nwill be retrieved - there may be too many or too few, there is\nno way to control the number.\nThis is especially problematic\nwhen the document collections are very large, as in the case of\nthe Internet. It is diﬃcult to rank the matching documents. It\nis diﬃcult to express complex requirements. Ideas such as query\nreformulation and relevance feedback are diﬃcult to incorporate.\nBecause of all these factors, Boolean models are no longer the\npreferred retrieval models for IR.\n332\nCHAPTER 3. ADVANCES IN IR\nThe Vector Space Retrieval Model\nThe vector space model treats documents as bags of words - un-\nordered collections of terms, repetitions allowed, so we can talk of\nfrequencies. Documents as well as queries are treated as vectors\n(arrays) of features. Each term (a word, a phrase or other similar\nentity) is a possible feature. Features are given numerical values.\nThus feature vectors can be geometrically visualized as points in\nn-dimensional space or as vectors connecting the origin to these\npoints. The spatial similarity between such vectors is used as a",
    "entity) is a possible feature. Features are given numerical values.\nThus feature vectors can be geometrically visualized as points in\nn-dimensional space or as vectors connecting the origin to these\npoints. The spatial similarity between such vectors is used as a\nmetaphor to deﬁne the similarity between documents and queries.\nRetrieved documents are ranked based on the degree of similarity.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 3.1 The Vector Space Model in Two Dimensions\nThere are several issues in the design of a vector space model.\nHow do we determine which terms in a given document are impor-\ntant? What about word sense? Should we take the surface form\nof words or the root forms? What deﬁnition of word do we use?\nHow do we take care of multi-token words, compounds, idioms\nand phrases? What is the relationship between the importance of\na given term for a given document in relation to its importance\nfor other documents in the collection or for the whole collection?\nHow exactly do we compute the similarity between a query and\na document? In the case of a large hyperlinked and highly dy-\nnamic collection such as the world wide web, what exactly is the\ncollection and what are the eﬀects of the hyperlinks, formatting\ninformation and meta data available? There are no perfect an-\nswers to all the questions but we will be able to give some ideas\nas we proceed.\n3.2. BASIC IR MODELS\n333\nVector space models based on the bag-of-words representa-\ntion ignores syntax (word order, phrase structure, proximity, and\nmore) and semantics (word senses, scope of quantiﬁers and nega-\ntion, anaphoric references, synonymy and other kinds of relation-\nships between words and between words and concepts). “Restau-\nrant” will not match with “cafe”. “PRC” will not match with\n“China”.\nBut “Bat” as in cricket may match with the mam-\nmal called bat. “Apple” will treat the apple fruit and the Apple\ncomputer without distinction. Vector space models lack the tight",
    "rant” will not match with “cafe”. “PRC” will not match with\n“China”.\nBut “Bat” as in cricket may match with the mam-\nmal called bat. “Apple” will treat the apple fruit and the Apple\ncomputer without distinction. Vector space models lack the tight\ncontrol that Boolean models permit. Given a query “A B”, vector\nspace models may prefer a document containing A frequently but\nnot B, or B frequently but not A, over documents that contain\nboth A and B but less frequently. Is this what we really want in\nall situations?\nIn spite of the fact that there are still many open questions,\nvector space models have worked fairly well in practice, especially\nfor large collections such as the web. They are simple, based on\nprinciples of mathematics and statistics and amenable for eﬃcient\nimplementations. They are more ﬂexible than Boolean Models -\npartial matching is allowed and the retrieved documents can be\nranked and ordered. The vector space model based on the bag-\nof-words representation is the mainstream approach in IR today.\n3.2.2\nTerm Weighting: tf-idf\nIt may be argued that the number of times a term occurs in a\ndocument is a stronger indicator of its semantic content as com-\npared to mere presence or absence of terms. Thus feature values\ncan be taken as Term Frequency (TF) rather than as Boolean.\nAn inverted index can also be maintained, giving for each word,\nthe frequencies and the positions of the word for each document.\nThe inverted index makes it easy to locate documents containing\nparticular words or phrases. Thus if the terms Batting, Bowling,\nFielding, LBW, Run-Out, Boundary, Catch, Run etc. occur fre-\nquently, we may think that that the document must be related to\nCricket.\nOne cannot be so sure that the document is about football\nif the word goal appears frequently because the word goal can\nappear in many diﬀerent kinds of documents with diﬀerent mean-\nings. Similarly, if some words, say, important, however, also, re-\n334\nCHAPTER 3. ADVANCES IN IR",
    "Cricket.\nOne cannot be so sure that the document is about football\nif the word goal appears frequently because the word goal can\nappear in many diﬀerent kinds of documents with diﬀerent mean-\nings. Similarly, if some words, say, important, however, also, re-\n334\nCHAPTER 3. ADVANCES IN IR\ncently, although, change etc. occur frequently in some document,\nwe may not be able to say much about the semantic content of\nthe document. In fact these words may occur frequently in many\ndocuments on a variety of topics. Hence it makes sense to look\nat the number of diﬀerent documents in the collection in which\na particular term appears. If a term occurs in many, or almost\nall the documents, it is less useful as an indicator of any particu-\nlar topic. The Inverse Document Frequency (IDF) is the ratio of\nthe total number of documents in the collection to the number of\ndocuments in the collection in which the speciﬁed term occurs.\nThe term frequency is often dampened - √tf or 1 + log(tf) is\nused. This is because a document containing a term more often\nis more signiﬁcant but may not be by as much as the frequency\nitself suggests. If a term occurs once in d1 and thrice in d2, d2 is\nsurely more signiﬁcant than d1 for the given term but may not be\nreally three times as signiﬁcant. Also, there are length eﬀects. A\nlarge document and a small document should not be treated on\npar. The occurrence of a rare term in a small document is perhaps\nmore signiﬁcant than the occurrence of the same term in a large\ndocument.\nThus the tf values are often normalized for length\nby dividing them by the size of the document or, perhaps better\nstill, by the frequency of the most frequent word in the document.\nAnother formula that has been proposed to reﬁne the raw tf is\n0.5 + (0.5 ∗tf)/(maxtf).\nSince the total number of documents in a collection may be\nlarge, it is usual to squash the raw idf value also by taking loga-\nrithms.\nThe product of Term Frequency and Inverse Document Fre-",
    "Another formula that has been proposed to reﬁne the raw tf is\n0.5 + (0.5 ∗tf)/(maxtf).\nSince the total number of documents in a collection may be\nlarge, it is usual to squash the raw idf value also by taking loga-\nrithms.\nThe product of Term Frequency and Inverse Document Fre-\nquency, abbreviated as tf.idf, is one of most basic term weighing\nschemes. In fact tf.idf is a class of term weighting schemes. There\nare several combinations, some of the commonly used ones are\nshown below:\n3.2. BASIC IR MODELS\n335\nTerm Frequency\nIDF\nNormal-\n-ization.\nn: natural: tf\nn:natural: idf:\nN\nni\nn:no norm.\nl: log: 1 + log(tf)\nt:log( N\nni )\nc:cosine\na: augmented: 0.5 +\n0.5∗tf\nmax tf\nmax tf: freq. of the most\nfrequent term in the doc.\nVarious schemes of tf.idf weighting are denoted using the la-\nbels in the above table. Thus ltc.ann refers to logarithmic terms\nfrequency, logarithmic idf and cosine normalization for the docu-\nments and augmented term frequency, plain idf and no normal-\nization for the query terms. Weights for terms that do not occur\nin a given document can be taken as zero.\nThese tf.idf schemes consider both the local and global eﬀects\nof term frequencies. They are simple and eﬀective, they are widely\nused.\nHowever, such schemes for term weighting are ad hoc and lack\na proper mathematical basis. Schemes based on term distribution\nprobability models have been proposed. Some of the commonly\nused schemes are based on 1) Poisson distribution, 2) two-Poisson\nmodel and 3) Katz’s K-Mixture model.\nThe terms in the query are usually taken to be equally weighted\nbut it is conceivable that users specify the relative weights of the\nquery terms. There will still be no logical connectives and the\nmatching algorithms will continue to be statistical. It is not easy\nfor users to think of appropriate ways of weighting the query terms\nand hence this idea is diﬃcult to use.\n3.2.3\nSimilarity Measures\nHow do we quantify the similarity between two vectors? We have",
    "matching algorithms will continue to be statistical. It is not easy\nfor users to think of appropriate ways of weighting the query terms\nand hence this idea is diﬃcult to use.\n3.2.3\nSimilarity Measures\nHow do we quantify the similarity between two vectors? We have\nseen that directions between the two vectors can be taken as a\nmeasure of similarity between the two vectors.\n(Sometimes it\nis more convenient to talk in terms of dissimilarity rather than\nsimilarity. Greater the distance between two quantities, higher is\nthe dissimilarity. Hence the name Distance Measures.) It is usual\nto measure the Cosine of the angle between the two vectors rather\nthan the angle itself. The cosine will be 1 when the two vectors\n336\nCHAPTER 3. ADVANCES IN IR\ncoincide and 0 when they are completely disjoint (orthogonal).\nThe cosine measure is given by:\ncosine(q, d) =\nPn\ni=1 qi∗di\n√Pn\ni=1 q2\ni\n∗√Pn\ni=1 d2\ni\nIt is appropriate to normalize the lengths of all vectors to\n1. Otherwise, longer vectors (corresponding to longer documents)\nwould have an unfair advantage and the corresponding documents\ntend to get ranked higher than the shorter documents.\nIf the\nvector lengths are all normalized to 1, the denominator becomes 1\nand so the cosine is simply the dot product of the two vectors. It\ncan be shown that the cosine measure and the Euclidean distance\nproduce identical rankings.\n3.2.4\nThe Probability Ranking Principle\nIf the user does not ﬁnd many relevant documents in the ﬁrst\npage of returned results, he is often willing to look at the next\npage, thereby trading Precision for Recall. The Probability Rank-\ning Principle states that ranking documents in order of decreasing\nprobability of relevance is optimal. Retrieval is viewed as a greedy\nsearch where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the",
    "search where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the\nviolation of this assumption is when there are duplicates in the\ncollection. If an ambiguous word such as capital is included in the\nquery, an optimal system may be expected to retrieve and present\nthe documents so that the user sees this ambiguity but the PRP\nprinciple would give documents that are maximally relevant for\neither of the two senses of the ambiguous word. Further, relevance\nis diﬃcult to quantify and measure accurately. At best we make\ngood estimates. It may be worth looking at the variance of these\nestimates and prefer those decisions with lower variance.\n3.2.5\nPerformance Evaluation\nAs we have already seen, Precision is the percentage of relevant\ndocuments in the returned set and Recall is the percentage of\nall relevant documents in the collection that is in the returned\nset. However, most IR systems produce a ranked list of returned\n3.2. BASIC IR MODELS\n337\ndocuments and a case where the last three of the ten documents\nreturned are relevant cannot be equated with the case where the\nﬁrst three out of ten are relevant. Most users scan the returned\ndocuments from top to bottom and would like to see many relevant\ndocuments right at the top. Thus by measuring the precision at\nseveral initial segments of the ranked list, one may obtain a good\nimpression of how well the system ranks relevant documents. We\nmay therefore measure the Precision at speciﬁed cutoﬀlevels such\nas 5, 10, 20 or 100 from the top. By considering all the documents\nabove a relevant document and computing Precision and then by\naveraging all such Precision values for each of the relevant docu-\nment retrieved, one gets an uninterpolated average Precision. This\naverage Precision will be 1 if all the relevant documents are at the",
    "above a relevant document and computing Precision and then by\naveraging all such Precision values for each of the relevant docu-\nment retrieved, one gets an uninterpolated average Precision. This\naverage Precision will be 1 if all the relevant documents are at the\ntop of the list. Documents further down in the list are also con-\nsidered and Precision for relevant documents beyond the returned\nlist is taken as zero. Therefore this quantity actually indirectly\nmeasures recall. On the other hand interpolated average Precision\nis more directly based on the Recall. Precision values are calcu-\nlated for each speciﬁed levels of Recall and then averaged. In the\ncase of the widely used 11-point average, for example, Precision\nis measured at Recall levels of 0%, 10%, 20%, 30%, 40%, 50%,\n60%, 70%, 80%, 90% and 100%. One point to note is that if the\nPrecision value at any Recall level is lower than a Precision value\nat a higher Recall value, the higher value is used. This is based on\nthe idea that if the Precision is increasing, people will be willing\nto go down further. This process is called interpolation. Curves\nwhich show the variation between Precision and Recall are called\nPrecision-Recall curves. Average Precision is one way to compute\na combined measure indicating the trade-oﬀbetween Precision\nand Recall. the F-Measure is another commonly used combined\nmeasure. The F-Measure is given by\nF =\n1\nα 1\nP +(1−α) 1\nR\nwhere α gives the relative weights for Precision P and Recall\nR. If equal weightage is given for the two, then\nF = 2∗P∗R\nP+R\n338\nCHAPTER 3. ADVANCES IN IR\n3.3\nTowards Intelligent Information Re-\ntrieval\nLet us now take stock of where we stand, where we would like\nto go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant",
    "to go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant\nto the query. As we have already seen, the most common approach\nis to view documents and queries are bags of words and represent\nthem as feature vectors where each feature corresponds to one\nword. Similarity between feature vectors is quantiﬁed in terms\nof the orientations of these vectors. Performance is measured in\nterms of Precision and Recall.\nIntelligent IR, on the other hand, requires that we consider the\nsyntax as well as semantics of documents and queries, we adapt\nto users based on direct or indirect feedback and learning, and we\ntake care of authority and dependability of documents. An ideal\ninformation retrieval system is one that can perform like a human\nassistant. This is really the software grand challenge. Obviously,\nwe are far from such an ideal.\nLet us now see what kinds of\nimprovements and enhancements can be or have been made in\nthe ﬁeld of information retrieval.\nMere presence or absence of keywords is clearly too naive a\nview of a document. Can we say I have a bad head ache and Now\nI am free from head ache mean the same thing and both match\nthe query head ache equally well? Is India beat Australia same as\nAustralia beat India? Can we equate I like Govinda’s movies and\nI like Govinda’s movies as much as I would like a burning stove if\nI were sitting on it? Most current IR systems continue to use the\nbag-of-words representation while examples like this clearly show\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate",
    "the inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nthe correct sense of the word can we be sure that we are looking\nat the right document. We may try to take into account the order\nof words in the query. A full syntactic analysis may be carried\n3.3. TOWARDS INTELLIGENT IR\n339\nout. We may try to adapt to the user based on direct or indirect\nfeedback. We may try to take into account the authority of the\nsource.\nWe have already seen that many of the NLP tasks are inher-\nently diﬃcult in themselves and performance of current systems\nis limited. Thus full syntactic analysis is not only time consuming\nbut also limited in performance - current grammars and parsers\nfail on a signiﬁcant percentage of cases. Unrestricted WSD sys-\ntems are still at a research level. The real challenge, therefore, is\nto integrate the best of available NLP technologies with IR models\nwithout sacriﬁcing the simplicity and eﬃciency of IR models. We\nwill explore below some the ideas and techniques that have been\nused to build better IR systems.\n3.3.1\nImproving User Queries - Relevance Feed-\nback\nA big question is how do we assess the relevance of a retrieved\ndocument for a given query? Relevance is a subjective judgment\nand may include being on the proper subject, being timely (re-\ncent information), being authoritative (from a trusted source),\nsatisfying the goals of the user and his/her intended use of the\ninformation (information need), etc. Relevance is not an yes/no\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying",
    "question, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\nassumptions in practice. For example, we may use the simplest\nnotion of relevance that the query string appears verbatim in the\ndocument. A slightly less strict notion could be that the words in\nthe query appear frequently in the document, in any order (bag\nof words).\nInstead of looking for improving the performance of retrieval\nfor given queries, we may invert the problem and see how the\nquery itself may be improved so that performance is maximized.\nThis makes sense because it is not always very easy for a user\nto specify exactly what he wants as a query. An ideal query is\none that expresses the user’s requirements precisely in relation to\nthe documents in the collection.\nBut ﬁnding an ideal query is\ndiﬃcult unless we already know exactly what the documents in\n340\nCHAPTER 3. ADVANCES IN IR\nthe collection contain. We may therefore start with a good guess\nand hope to improve after we see some results. We make the as-\nsumption that documents which are relevant to a given query are\nsimilar. Therefore a query can be improved by making it closer\nto relevant documents retrieved and farther from the irrelevant\ndocuments retrieved. Thus by looking at the returned results and\njudging them as relevant or otherwise, we may obtain an improved\nquery. More terms from the relevant documents retrieved can be\nadded and weights for the terms can also be adjusted based on\nthe frequency of occurrence of those terms in the relevant and ir-\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.",
    "relevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nInstead of interactively improving the query, one may assume\nthat the top few (say, 20) hits are all actually relevant and au-\ntomatically improve the query without asking users to judge any\nretrieved documents. This technique is termed Pseudo-relevance\nFeedback.\n3.3.2\nPage Ranking\nRetrieving the documents that best match the given query does\nnot necessarily give us the best overall performance. There are a\nvery large number of documents on the Internet and too many of\nthem may match equally well. Simply listing a large number of\nrelevant documents is not very good. How do we help the users\nthen? We should use not only the terms in the documents and\nqueries for matching and ranking the documents, but also some\ngeneral measure of goodness of various documents. It would be\nnice if we could somehow compute authenticity or dependability\nof documents but there is no simple way to do that. What search\nengines such as Google do is to instead use the popularity of the\ndocuments as a measure of goodness. If many people are look-\ning at a document perhaps there is something important or useful\nabout it. But how do we ﬁnd out who is looking at which docu-\nment and for what purpose? One thing that is clearly computable\nfrom the web of documents on the Internet is the hyperlink struc-\nture that links up various documents. We could look at the links\n3.3. TOWARDS INTELLIGENT IR\n341\ngoing out from a given document but perhaps the links coming\ninto a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.",
    "into a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nAlso we must not allow people to deliberately manipulate these\nranks by artiﬁcially creating a large number of dummy documents\nand make them all point to a given document to increase its rank.\nGoogle works by looking at all the documents that point to a\ngiven document and weight these pointers by the ranks of those\ndocuments. Thus a document gets a high rank if it has many links\ncoming from high ranked documents.\nOf course this deﬁnition\nis recursive but there is a simple iterative algorithm that keeps\nupdating the page ranks. It is not easy to fool the system for\nlong - even if dummy documents are created to enhance the page\nrank of some document, those dummy documents would soon get\nvery low ranks as no other high ranked documents would be really\nlinking to them. The web may appear to be a totally unorganized\nand uncontrolled mess but simple ideas like this can bring some\norder to the chaos.\nThe Google Page Rank (PR) computation formula is given\nbelow:\nPR(A) = (1 −d) + d(PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))\nwhere Tis are citations of document A and C(T) gives the total\nnumber of outgoing links from document T. Here d is a factor that\nspeciﬁes the relative importance of the current document to the\ndocuments which cite it.\n3.3.3\nRole of Linguistics\nStop Words\nWords that seem to be useless for searching are termed stop words.\nThese are mainly grammatical or function words such as articles,\nprepositions and conjunctions.\nWords such as the, from, could\nhave important grammatical function in sentences but are unlikely\nto help us in retrieving relevant documents using the bag-of-words\nrepresentation. Function words may be extracted from an elec-\n342\nCHAPTER 3. ADVANCES IN IR",
    "prepositions and conjunctions.\nWords such as the, from, could\nhave important grammatical function in sentences but are unlikely\nto help us in retrieving relevant documents using the bag-of-words\nrepresentation. Function words may be extracted from an elec-\n342\nCHAPTER 3. ADVANCES IN IR\ntronic dictionary. Stop words are usually high frequency words\nand small in size.\nSo we may also make an initial list of stop\nwords by looking at the frequency and/or length.\nEliminating\nstop words reduces the feature vector size. However, removing\nstop words has its own negative eﬀects. How do we handle the\nphrase when and where if all these three words are eliminated as\nstop words?\nInstead of using ad-hoc methods to remove some terms as stop\nwords, we may try to measure the signiﬁcance of words and then\nremove those with low signiﬁcance. For example, terms with very\nlow tf.idf values can be removed.\nMorphology\nThe simple bag-of-words representation described so far would\ntreat compute, computer, computing etc. as individual, unrelated\nwords in their own right, although all these words are semantically\nclosely related. A morphological analyzer can analyze full words\ninto roots and aﬃxes according to valid rules of morphophonemics.\nHowever, developing a full morphological analyzer is not always\neasy, especially for languages with a very rich system of morphol-\nogy. We know that Indian languages fall into this category. Most\nIR systems take a short cut and apply simpliﬁed versions of an-\nalyzers called stemmers. Lovin and Porter stemmer are widely\nused for English. A stemming algorithm removes aﬃxes accord-\ning to some rules. Thus compute, computer, computing will all be\nreduced to comput-. Note that comput is not even a valid word in\nEnglish! Stemming algorithms can reduce semantically unrelated\nwords into a common stem. Thus stocks and stockings may both\nget reduced to stock- causing serious damage. Also reducing both",
    "reduced to comput-. Note that comput is not even a valid word in\nEnglish! Stemming algorithms can reduce semantically unrelated\nwords into a common stem. Thus stocks and stockings may both\nget reduced to stock- causing serious damage. Also reducing both\ngallery and gall to gall- makes it diﬃcult for people to understand\nwhat is happening.\nPhrases\nIf you are looking for interest rate you cannot simply go by doc-\numents containing the words interest and rate anywhere in them.\nThus identifying phrases and treating them as single terms is im-\nportant. However, identifying phrases is not as simple as it may\nappear. How do we take care of interest rate, interest rates and\n3.3. TOWARDS INTELLIGENT IR\n343\nrate of interest as all meaning essentially the same thing? How do\nwe distinguish between phrases and compounds? How do we dis-\ntinguish between words that occur in sequence merely by chance\nwith word sequences that have a unitary meaning?\nHandling\nphrases is much more complex than we may think. Most current\nIR systems follow a simple and crude yet quite eﬀective method.\nThey simply identify ordered pairs of words (called bigrams) that\noccur frequently and treat them as individual terms. Of course\nthis is very restrictive - phrases with more than two words are not\nat all considered. Some systems have a separate phrase identiﬁca-\ntion module and attempt more sophisticated techniques, similar\nto techniques used for identifying collocations.\nSyntax\nSince Godse killed Gandhi and Gandhi killed Godse mean very\ndiﬀerent things, the bag-of-words representation is too crude. A\ngood IR system must really consider the relative positions of words\nand analyze the structure of sentences in depth. However, syn-\ntactic analysis is itself a very complex and challenging task. Most\ncurrent parsing systems are limited in their performance. Also,\nattempts to use deeper analysis have given mixed results - perfor-\nmance does not necessarily improve. There is now an increasing",
    "tactic analysis is itself a very complex and challenging task. Most\ncurrent parsing systems are limited in their performance. Also,\nattempts to use deeper analysis have given mixed results - perfor-\nmance does not necessarily improve. There is now an increasing\ninterest in partial or shallow parsing systems. Wide coverage ro-\nbust partial parsing systems are becoming available.\nWord Sense Disambiguation\nWords have several meanings and simply counting the frequency\nof the word match without knowing whether is is used to mean a\ncricket match or a matrimonial match or just a match box is no\ngood. the role played by Homonymy, Polysemy and Synonymy is\ncomplex. If a query contains the term house and no word sense\ndisambiguation is done, the system returns documents containing\nthe term house in any of its senses including an ordinary dwelling\nplace and a parliamentary house, thereby reducing the real Preci-\nsion. At the same time, it may miss out other relevant documents\nthat may use synonymous or similar meaning words such as ’res-\nidence’ or ’home’. Thus Recall is also reduced. This shows how\nimportant it is to identify the correct senses of words in context.\n344\nCHAPTER 3. ADVANCES IN IR\nYet Word Sense Disambiguation is itself a diﬃcult area of re-\nsearch and as of now, most IR systems do not bother to look at\nthe meanings of words at all. We will look at a technique later\nby which word sense disambiguation is indirectly taken care of to\nsome extent.\nAnaphora Resolution\nPronouns are often discarded as stop words but they actually\nstand for some nouns.\nReferences are informationally abbrevi-\nated forms of the referents. The references and the referents point\nto the same thing - they must be treated as equals. Resolution of\nanaphoric references can therefore help in improving the perfor-\nmance of IR systems.\nDiscourse Segmentation\nIn large documents, the topic keeps shifting and the topics of\nsmaller identiﬁable units such as sections and paragraphs may be",
    "anaphoric references can therefore help in improving the perfor-\nmance of IR systems.\nDiscourse Segmentation\nIn large documents, the topic keeps shifting and the topics of\nsmaller identiﬁable units such as sections and paragraphs may be\nmore relevant than the topic of entire documents. For example, if\nyou are looking for Malaria a paragraph in which the term malaria\noccurs several times could be more informative and useful to you\nthan a large document in which the word occurs scattered all\nover, even if the total frequency of occurrence is higher. Discourse\nsegmentation can therefore be useful.\nText Tiling\nThere are techniques for automatically identifying discourse seg-\nments within a text. One method, called Text Tiling, breaks texts\ninto ﬁxed size blocks and assumes that they are separated by hypo-\nthetical ’gaps’. A Cohesion Scorer measures the topic continuity\nacross gaps. A Depth Scorer checks how low the cohesion of a\ngap is relative to neighboring gaps. Cohesion is relative. Finally\na Boundary Selector ﬁnds the segmentation points. Once a doc-\nument is segmented into discourse segments, we could think of\nperforming IR on discourse segments rather than on whole docu-\nments.\nCurrent views on use of NLP in IR are mixed. While the limi-\ntations of current IR models are well understood and the need for\n3.3. TOWARDS INTELLIGENT IR\n345\ndeeper linguistic analysis is accepted in principle, practical stud-\nies and experiments including NLP components have not consis-\ntently given better performance.\nThis could be partly because\nof the nature and limitations of the NLP modules considered in\nthese experiments. Even if speed is sacriﬁced to some extent, if\nwe can show that accuracies can be signiﬁcantly and consistently\nimproved, then the role of NLP could be better appreciated. User\nperspectives matter a lot. Many users may be happy that they\nare able to get something useful from the Internet and not both-\nered too much about the low precision and recall of the current",
    "improved, then the role of NLP could be better appreciated. User\nperspectives matter a lot. Many users may be happy that they\nare able to get something useful from the Internet and not both-\nered too much about the low precision and recall of the current\nsystems. As user expectations grow, and IR is applied for more\nserious and highly speciﬁc retrieval tasks, the need for moving\ntowards more intelligent models would be felt more and more.\n3.3.4\nLatent Semantic Indexing\nThe bag-of-words representation treats words and phrases as iso-\nlated occurrences. In fact the co-occurrence of terms has a sig-\nniﬁcant implication for the semantic content of documents. Co-\noccurrence also implicitly disambiguates words. For example, if\nthe word bank occurs frequently with words such as cheque, with-\ndraw, deposit, balance, interest, etc. then it indicates the ﬁnancial\ninstitution sense, not the bank of a river. Latent Semantic Index-\ning (LSI) is a technique by which co-occurring terms are grouped\ntogether and treated similarly.\nLSI actually projects from the\noriginal n-dimensional vector space with n diﬀerent terms onto a\nlower dimensional space such that co-occurring terms fall along\nsame dimensions and non-co-occurring terms are projected onto\ndiﬀerent dimensions. The idea is that the latent, that is hidden,\n’true’ semantic space of possibilities is obtained from the surface\nrepresentation where each term is treated as a separate entity and\nthe semantic relationships between the terms are implicit. In the\nlatent semantic space, a query and a document can have high sim-\nilarity even if they do not share any terms, as long as the terms\nare semantically similar according to the co-occurrence analysis.\nLSI can thus be viewed as a similarity metric and an alternative\nto word overlap measures such as tf.idf. The latent semantic space\nhas fewer dimensions and thus LSI can also be looked upon as a\ndimensionality reduction technique.",
    "are semantically similar according to the co-occurrence analysis.\nLSI can thus be viewed as a similarity metric and an alternative\nto word overlap measures such as tf.idf. The latent semantic space\nhas fewer dimensions and thus LSI can also be looked upon as a\ndimensionality reduction technique.\nLSI chooses an optimal mapping among various possible map-\n346\nCHAPTER 3. ADVANCES IN IR\npings to a lower dimension. This is achieved by using a mathe-\nmatical technique known as Singular Value Decomposition (SVD).\nLSI is basically an application of SVD to the word-by-document\nmatrix. The dimensions of the reduced space correspond to the\naxes of greatest variation.\nThe idea is to capture as much of\nthe variation in the data as possible. SVD can be viewed as a\nmethod to rotate the axes of on an n-dimensional space so that\nthe ﬁrst axis runs along the direction of largest variation, the sec-\nond axis runs along the direction of the second largest variation,\nand so on. Thus SVD is similar to Principle Component Analysis\n(PCA). PCA is applicable only to square matrices whereas SVD\ncan be applied to any matrix. SVD is a least square method. SVD\ntakes a matrix A and represents it as ˆA in a lower dimensional\nspace such that the “distance” between the two as measured by\nthe 2-norm is minimized:\n△=∥A −ˆA ∥2\nThe 2-norm is for matrices what Euclidean distance is for\nvectors.\nSVD, and therefore LSI, assumes normal distribution which\nis seldom true for term distributions. Also SVD can be slower.\nHence the use of LSI is justiﬁed only when the performance im-\nprovements are very signiﬁcant.\n3.3.5\nMeta Search Engines\nWith millions of pages being added to the World Wide Web every\nyear it was realized that no search engine could possibly index all\nthe information on the web and be reasonably eﬃcient at the same\ntime. The web is extremely large. To get an idea, consider these\n1996 ﬁgures for the number of pages indexed by various search",
    "year it was realized that no search engine could possibly index all\nthe information on the web and be reasonably eﬃcient at the same\ntime. The web is extremely large. To get an idea, consider these\n1996 ﬁgures for the number of pages indexed by various search\nengines: Excite - 1.5 Million pages, Lycos - 19 Million unique\nURLs including ftps and gopher and Altavista - 166 Million pages.\nThese look like toys by today’s standards. Yet no search engine\nindexes all the available web pages. As the web keeps growing\nfaster and larger this need would only become more and more\napparent. While interacting with search engines, it was realized\nthat the user who had a varied set of interests started gradually\ntranscending a single search engine and was forced to submit and\nresubmit his queries to several search engines. This was a major\nbottleneck in World Wide Web searching and from this need for\n3.3. TOWARDS INTELLIGENT IR\n347\nreducing the amount of user time spent in submitting queries to\nmultiple search engines arose the concept of a meta search engine.\nA meta search engine takes a user query and submits the\nquery to many public domain search engines either in parallel\nor sequentially, collects results returned by each search engine\nand returns the compiled result to the user. Apart from this, a\nmeta search engine may provide other facilities such as removing\nduplicates and ﬁltering the results. Meta search engines are not\nintended to replace search engines - search engines are the service\nproviders to the meta search engines. Search engines are likely to\nbe increasingly used by meta search engines rather than by the\nend users directly. If Internet keeps growing at the phenomenal\nrate it is doing now, the amount of user time spent in locating\nrelevant information will also keep growing. Hence we need to\nlook for alternative architectures for searching and information\nretrieval from the web.\nMeta search engines have several advantages over direct use",
    "rate it is doing now, the amount of user time spent in locating\nrelevant information will also keep growing. Hence we need to\nlook for alternative architectures for searching and information\nretrieval from the web.\nMeta search engines have several advantages over direct use\nof search engines. Unlike search engines, meta search engines can\nreside locally and can be customized or adapted to the needs of\na speciﬁc user or a user group.\nA meta search engine can be\nlinked to a local database so that web search can be replaced with\nlocal search in some situations and frequently downloaded pages\ncan be locally archived. Advanced techniques for automatically\nbuilding and adapting user models are of great interest. It has\nbeen seen that users generally restrict themselves to a particular\nsubset of topics when they initiate a web search.\nThis can be\nused to construct a user proﬁle which should be extremely use-\nful in search optimization. More detailed understanding of the\nuser’s needs becomes possible as the meta search engine is local\nand customizable. For example, you may specify whether you are\nseriously looking for an answer to a very speciﬁc question or you\nare generally exploring what all is available. Search engines treat\neach query as a fresh task and they have no idea of what you have\nalready searched, what all you already know and what exactly\nyou are now looking for. Meta search engines can be customized\nto work in the background mode so that user’s time in waiting\nfor results can be minimized. A meta search engine can moni-\ntor the network bandwidth dynamically and adjust the ﬁring of\nthe various search engines accordingly. Users need to learn a sin-\ngle query interface since the system automatically formulates the\n348\nCHAPTER 3. ADVANCES IN IR\nuser’s query as required by various search engines. The system\ncan also learn statistical models of search engines and the pages\nthey index so that it becomes possible to choose the right search",
    "gle query interface since the system automatically formulates the\n348\nCHAPTER 3. ADVANCES IN IR\nuser’s query as required by various search engines. The system\ncan also learn statistical models of search engines and the pages\nthey index so that it becomes possible to choose the right search\nengines for the right query. Such models can be used to predict,\nfor example, the expected results for a given search. The meta\nsearch engine can “know”, even before searching, that there will\nbe too many hits and the query needs to be tightened up or that\nthere may not be many relevant documents. Overall performance\ncan also be optimized by choosing the search engines and ﬁring\nthem judiciously.\nUsers spend a lot of time trying to locate what they want from\nthe Internet. Even with technology improvements and increasing\nbandwidths, eﬀective bandwidth is getting oﬀset by increasing\nresources and the number of users. On the whole availability of\nsuﬃcient bandwidth will remain an imminent problem in many\nparts of the world. Meta search engines oﬀer one good solution to\nsaving user time.\nThere are several meta search engines in use. The Metacrawler\nand the Savvy search (http://www.savvy.com) are two such search\nservices on the web. The Metacrawler utilizes several search en-\ngines to which it submits its queries. The service removes dupli-\ncates from the results returned and presents it to the user in a\nclick-able format. The All in One Page (http://www.allone.com)\nis a meta search service which provides results from multiple re-\nsource applications which include gopher, Archie, HTTP and oth-\ners.\nHere we shall brieﬂy look at a Meta Search Engine called\nPersonal Search Assistant (PSA) developed by us sometime back.\nThe PSA system has been implemented using CGI scripts written\nin Perl which run under the Apache Web Server. PSA is modular.\nLet us take a quick look at each of the major modules:\n• The Collector Module submits queries to several search en-",
    "The PSA system has been implemented using CGI scripts written\nin Perl which run under the Apache Web Server. PSA is modular.\nLet us take a quick look at each of the major modules:\n• The Collector Module submits queries to several search en-\ngines in parallel and collects the results in local ﬁles for\nfurther processing. It can be conﬁgured to look for spec-\niﬁed kinds of information for focussed search, say URLs,\nemail ids, or ftp sites relating to a given topic. It can work\nin the background. It monitors the network bandwidth and\nadjusts the parallel ﬁring of search engines accordingly.\n• The Update Module interprets the search results given by\n3.3. TOWARDS INTELLIGENT IR\n349\ndiﬀerent search engines, takes care of formatting variations,\nand updates one or more local databases. It may also ini-\ntiate its own collection phase to refresh outdated database\nentries.\n• The Local Database Handler interfaces with various modules\nin the system and allows the user to search, view and modify\ndiﬀerent parts of the various local databases.\n• The History Module maintains logs of previous search oper-\nations. The log includes information such as search speciﬁc\nuser login for the search, the search term/terms used, links\nfollowed by the user, links followed by others, search engine\nstatistics for each search, the speciﬁc search engines utilized\nby the search, engine speed (= number of hits/time spent),\nnumber of search engine links followed and total number\nof times a particular search engine is chosen. PSA com-\nputes values of certain parameters including average search\nengine speed, search engine popularity and uses these val-\nues to make suggestions and recommendations to the user,\nfor modelling search engines and for user modelling. The\nPSA history subsystem works during and after the collect-\ning phase and interacts with the LDB handler to update the\nlocal database with the history details of a previous search.\nThe history subsystem keeps track of search queries, for-",
    "for modelling search engines and for user modelling. The\nPSA history subsystem works during and after the collect-\ning phase and interacts with the LDB handler to update the\nlocal database with the history details of a previous search.\nThe history subsystem keeps track of search queries, for-\nmats in which they are submitted, who submitted them,\nhow far the results were appropriate in relation to the search\nqueries as perceived by the user and user preferences in fol-\nlowing the returned links.\n• The Personal Agent module uses these pieces of informa-\ntion to construct a user proﬁle. The user need not spend\nany time on the global search phase as it can be a non-\ninteractive, background phase. The user can interact with\nthe local database at any time during or after the search is\nover.\n3.3.6\nSemantic Web\nWe have seen that one of the major issues in IR is the need to\nunderstand the documents in the collection. Instead of looking\nfor more and more sophisticated techniques for unearthing hidden\n350\nCHAPTER 3. ADVANCES IN IR\nsemantic information from plain text documents, why not think\nof embedding semantics explicitly into the documents themselves?\nThat would give a big push to all automatic document processing\nsystems including IR systems. The big question however is, how\ndo we incorporate all this semantics into the documents?\nTim Berners-Lee, the inventor of the WWW (World Wide\nWeb), URI (Uniform Resource Identiﬁer), HTTP (HyperText Tran-\nsfer Protocol), and HTML (HyperText Markup Language), says\n“The Semantic Web is an extension of the current web in which\ninformation is given well-deﬁned meaning, better enabling com-\nputers and people to work in cooperation.”. The Semantic Web\nprovides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers",
    "provides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers\nand industrial partners. It is based on the Resource Description\nFramework (RDF), which integrates a variety of applications us-\ning XML for syntax and URIs for naming. See Scientiﬁc American\nMay 2001 issue for an interesting article by Tim Berners-Lee.\n3.3.7\nInformation Retrieval is Diﬃcult\nIR is an inherently diﬃcult task. Can you search millions of doc-\numents and accurately suggest the most relevant documents for a\ngiven query in a fraction of a second? In IR we are asking com-\nputers to do what we human beings cannot do. And we want IR\nto be fully automatic - there is no scope for human intervention.\nWhat makes IR diﬃcult? There are three major issues:\n• Understanding user needs: Understanding exactly what the\nuser is looking for is not easy. Key words do not tell us\nwhat is the purpose of the current search, what all the user\nalready knows, what all he has already searched or what\nlevel of abstraction would suit his level of knowledge and\nexpertise. Social and cultural contexts are important. It\nlooks strange that we set forth on a grand searching and\nretrieval operation without understanding exactly what we\nare looking for!\n• Understanding the Documents: Unless you know exactly\nwhat the documents contain, what they pertain to and what\n3.3. TOWARDS INTELLIGENT IR\n351\nall things they include, for whom it is written, what back-\nground is assumed etc., how can we say which documents\nwill suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient",
    "will suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient\nindexing of extremely large and highly dynamic collections\nsuch as the world wide web is a technological challenge.\nSearching and matching in such large collections is also not\neasy.\nClever use of data structures and algorithms must\nbe combined with distributed and parallel architectures in-\nvolving thousands of computers spread across the globe.\nHardware may fail, network services may be disrupted, but\nsearch engines must deliver under all circumstances. Build-\ning in such a high degree of robustness and fault-tolerance\nare technological challenges.\nGiven all this, is it not a great thing that today’s search en-\ngines are doing quite well, if not fully satisfactorily? The fact that\npeople use search engines routinely shows that the today’s IR tech-\nnologies are useable and useful. Nevertheless, dissatisfaction is the\nstarting point for innovation and design. We want IR to deliver\nmore. Improvements in IR are likely to come as much from clever\nheuristics and tricks as from in depth linguistic analysis. Statis-\ntical analysis and modelling techniques are likely to play a major\nrole too. We can expect greater cross-fertilization, synergy and\nintegration of IR with other areas of language engineering.\n3.3.8\nConclusions\nIntelligent IR requires that we consider the syntax and semantics\nof documents and queries, we adapt to users based on direct or\nindirect feedback and learning, and we take care of authority and\ndependability of documents. Developing an information retrieval\nsystem that can perform like a human assistant has been dubbed\nas the software grand challenge. It is natural, therefore, that we\nhave talked more of problems and broad ideas rather than concrete",
    "dependability of documents. Developing an information retrieval\nsystem that can perform like a human assistant has been dubbed\nas the software grand challenge. It is natural, therefore, that we\nhave talked more of problems and broad ideas rather than concrete\nsolutions.\n352\nCHAPTER 3. ADVANCES IN IR\nOn the one hand we feel the need for deeper linguistic analysis\nand NLP. On the other hand we want great speed and robustness.\nThe challenge is to use deeper linguistic analysis without losing\nthe advantages of speed, ﬂexibility and robustness.\nLinguistic\nanalysis is usually quite language speciﬁc whereas statistical tech-\nniques and machine learning algorithms can be adapted to a wide\nvariety of languages without much manual eﬀort.Linguists have\nbeen talking of universal grammars for a long time now but there\ndo not seem to be any such universal grammars that can be used\nright away.\nDevelopments in IR must be understood in the context of de-\nvelopment in related ﬁelds such as Information Extraction, Text\nCategorization and Automatic Summarization as also NLP tech-\nnologies such as WordNets, stemming algorithms, partial parsing\nsystems and WSD. With the rapid developments in each of these\nﬁelds and increasing synergy between the various areas, we can ex-\npect IR systems to become much more sophisticated in the years\nto come.\nDocument in Indian languages have slowly started growing.\nThere are still some teething problems with regard to non-standard\nencoding schemes. We can hope that the Indian language content\nwill grow fast in the near future. The need for indexing and search-\ning pages in Indian languages will grow. The tools and techniques\nwhich have been been ﬁne tuned for English do not necessarily\nwork well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages",
    "work well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages\nnot in, but on Indian languages and Indian tradition and culture.\nMulti-lingual and multi-media extensions will become increasingly\nimportant.\nBibliography\n1. Akshar Bharati, Vineet Chaitanya and Rajeev Sangal, “Nat-\nural Language Processing: A Paninian Perspective”, Prentice-\nHall of India, 1995\n2. Andrian Akmajian, Richard A Demers and Robert M Har-\nnish, “Linguistics: An Introduction to Language and Com-\nmunication”, The MIT Press, Second Edition, 1984\n3. Baker C L, “English Syntax”, MIT Press, 1989\n4. Ben Gold and Nelson Morgan, “Speech an Audio Signal\nProcessing”,John Wiley and Sons Inc., 2002\n5. Charniak E, “Statistical Language Learning”, MIT Press,\n1993\n6. Christopher D Manning and Hinrich Shutze, “Foundations\nof Statistical Natural Language Processing”, The MIT Press,\n2000\n7. Daniel Jurafsky and James H Martin, “Speech and Lan-\nguage Processing”, Pearson Education, 2002\n8. Douglas O’Shaughnessy, “Speech Communications - Hu-\nman and Machine”, Second Edition, Universities Press, 2001\n9. D C Montgomery, E A Peck and G G Vining, “Introduction\nto Linear Regression Analysis”, John Wiley and Sons, INC.,\n2001\n10. George W Smith, “Computers and Human Language”, Ox-\nford University Press, 1991\n353\n11. Hopcroft J.E. and Ullman J.D., “Introduction to Automata\nTheory, Languages, and Computation”, Addison-Wesley,\n1979\n12. Inderjeet Mani and Mark T Maybury (Eds), “Advances in\nAutomatic Text Summarization”, the MIT Press, 1999\n13. James Allen,”Natural Language Understanding”, Second\nEdition, Pearson Education, 2003\n14. Lawrence Rabiner and Biing-Hwang Juang, “Fundamentals\nof Speech Recognition”, Pearson Education, 2003\n15. Michael G Dyer, “In-Depth Understanding”, MIT Press,1983\n16. Nigel Fabb, “Sentence Structure”, Routledge",
    "Edition, Pearson Education, 2003\n14. Lawrence Rabiner and Biing-Hwang Juang, “Fundamentals\nof Speech Recognition”, Pearson Education, 2003\n15. Michael G Dyer, “In-Depth Understanding”, MIT Press,1983\n16. Nigel Fabb, “Sentence Structure”, Routledge\n17. Ralph Grishman, “Computational Linguistics: An Intro-\nduction”, Cambridge University Press, 1999\n18. Richard Coates, “Word Structure”, Routledge, 1999\n19. Richard Hudson, “Word Meaning”, Routledge\n20. Richard O Duda, Peter E Hart and David G Stork, “Pattern\nClassiﬁcation”, John Wiley and Sons Inc., 2001\n21. Richard Sproat, “Morphology and Computation”, MIT Press,\n1992\n22. Roche E.and Schabes Y. (Eds), “Finite State Language Pro-\ncessing”, MIT Press, 1997\n23. Tom M Mitchell, “Machine Learning”, The McGraw-Hill\nCompanies Inc., 1997\n24. The Unicode Standard Version 4.0, Addison-Wesley, 2004\n354\nAppendix 1: C5 Tag\nSet\n1. AJ0 adjective (unmarked) (e.g. GOOD, OLD)\n2. AJC comparative adjective (e.g. BETTER, OLDER)\n3. AJS superlative adjective (e.g. BEST, OLDEST)\n4. AT0 article (e.g. THE, A, AN)\n5. AV0 adverb (unmarked) (e.g. OFTEN, WELL, LONGER,\nFURTHEST)\n6. AVP adverb particle (e.g. UP, OFF, OUT)\n7. AVQ wh-adverb (e.g. WHEN, HOW, WHY)\n8. CJC coordinating conjunction (e.g. AND, OR)\n9. CJS subordinating conjunction (e.g. ALTHOUGH, WHEN)\n10. CJT the conjunction THAT\n11. CRD cardinal numeral (e.g. 3, FIFTY-FIVE, 6609)\n12. DPS possessive determiner form (e.g. YOUR, THEIR)\n13. DT0 general determiner (e.g. THESE, SOME)\n14. DTQ wh-determiner (e.g. WHOSE, WHICH)\n15. EX0 existential THERE\n16. ITJ interjection or other isolate (e.g. OH, YES, MHM)\n17. NN0 noun (neutral for number) (e.g. AIRCRAFT, DATA)\n355\n18. NN1 singular noun (e.g. PENCIL, GOOSE)\n19. NN2 plural noun (e.g. PENCILS, GEESE)\n20. NP0 proper noun (e.g. LONDON, MICHAEL, MARS)\n21. ORD ordinal (e.g. SIXTH, 77TH, LAST)\n22. PNI indeﬁnite pronoun (e.g. NONE, EVERYTHING)\n23. PNP personal pronoun (e.g. YOU, THEM, OURS)\n24. PNQ wh-pronoun (e.g. WHO, WHOEVER)",
    "19. NN2 plural noun (e.g. PENCILS, GEESE)\n20. NP0 proper noun (e.g. LONDON, MICHAEL, MARS)\n21. ORD ordinal (e.g. SIXTH, 77TH, LAST)\n22. PNI indeﬁnite pronoun (e.g. NONE, EVERYTHING)\n23. PNP personal pronoun (e.g. YOU, THEM, OURS)\n24. PNQ wh-pronoun (e.g. WHO, WHOEVER)\n25. PNX reﬂexive pronoun (e.g. ITSELF, OURSELVES)\n26. POS the possessive (or genitive morpheme) ’S or ’\n27. PRF the preposition OF\n28. PRP preposition (except for OF) (e.g. FOR, ABOVE, TO)\n29. PUL punctuation - left bracket (i.e. ( or [ )\n30. PUN punctuation - general mark (i.e. . ! , : ; - ? ... )\n31. PUQ punctuation - quotation mark (i.e. ‘ ’ ” )\n32. PUR punctuation - right bracket (i.e. ) or ] )\n33. TO0 inﬁnitive marker TO\n34. UNC ”unclassiﬁed” items which are not words of the En-\nglish lexicon\n35. VBB the ”base forms” of the verb ”BE” (except the inﬁni-\ntive), i.e. AM, ARE\n36. VBD past form of the verb ”BE”, i.e. WAS, WERE\n37. VBG -ing form of the verb ”BE”, i.e. BEING\n38. VBI inﬁnitive of the verb ”BE”\n39. VBN past participle of the verb ”BE”, i.e. BEEN\n40. VBZ -s form of the verb ”BE”, i.e. IS, ’S\n41. VDB base form of the verb ”DO” (except the inﬁnitive),\ni.e. ”DO”\n42. VDD past form of the verb ”DO”, i.e. DID\n356\n43. VDG -ing form of the verb ”DO”, i.e. DOING\n44. VDI inﬁnitive of the verb ”DO”\n45. VDN past participle of the verb ”DO”, i.e. DONE\n46. VDZ -s form of the verb ”DO”, i.e. DOES\n47. VHB base form of the verb ”HAVE” (except the inﬁnitive),\ni.e. HAVE\n48. VHD past tense form of the verb ”HAVE”, i.e. HAD, ’D\n49. VHG -ing form of the verb ”HAVE”, i.e. HAVING\n50. VHI inﬁnitive of the verb ”HAVE”\n51. VHN past participle of the verb ”HAVE”, i.e. HAD\n52. VHZ -s form of the verb ”HAVE”, i.e. HAS, ’S\n53. VM0 modal auxiliary verb (e.g. CAN, COULD, WILL, ’LL)\n54. VVB base form of lexical verb (except the inﬁnitive)(e.g.\nTAKE, LIVE)\n55. VVD past tense form of lexical verb (e.g. TOOK, LIVED)\n56. VVG -ing form of lexical verb (e.g. TAKING, LIVING)\n57. VVI inﬁnitive of lexical verb",
    "53. VM0 modal auxiliary verb (e.g. CAN, COULD, WILL, ’LL)\n54. VVB base form of lexical verb (except the inﬁnitive)(e.g.\nTAKE, LIVE)\n55. VVD past tense form of lexical verb (e.g. TOOK, LIVED)\n56. VVG -ing form of lexical verb (e.g. TAKING, LIVING)\n57. VVI inﬁnitive of lexical verb\n58. VVN past participle form of lex. verb (e.g. TAKEN, LIVED)\n59. VVZ -s form of lexical verb (e.g. TAKES, LIVES)\n60. XX0 the negative NOT or N’T\n61. ZZ0 alphabetical symbol (e.g. A, B, c, d)\n357\n358\nAppendix 2: Sample\nSentences\nReaders may ﬁnd it instructive to work out these sentences (adapted\nfrom Terry Winograd and other sources) using the ATN grammar\ngiven in this book. A star indicates an ungrammatical sentence\nand a question mark indicates a sentences that some may consider\nungrammatical. This list is not intended to provide a comprehen-\nsive coverage of all the important syntactic phenomena.\n1. I slept\n2. * I slept the baby\n3. * The baby has been slept\n4. * I slept him the blanket\n5. * I like\n6. I like you\n7. * I like her a book\n8. I bought her a rose\n9. * I bought a rose her\n10. I wrote a letter to my mother\n11. * I ate an apple to my mother\n12. I wrote a letter\n13. I wrote\n14. The wood was burned by the ﬁre\n359\n15. The wood was stored by the ﬁre\n16. The ﬁsh have been caught\n17. We gave them the ﬁsh\n18. We gave the ﬁsh to them\n19. ? We gave the ﬁsh them\n20. Sita wanted to sing\n21. Sita wanted Rama to sing\n22. For her to argue would have upset him\n23. Sita wanted to be entertained\n24. There are three windows in this room\n25. It was Rama who asked Lakshmana not to leave Sita alone\n26. Did you catch a ﬁsh?\n27. Which swimmer caught a ﬁsh?\n28. Which ﬁsh did the swimmer catch?\n29. What river was it caught in?\n30. Whom did you want Rama to tell to catch a ﬁsh?\n31. * In what river they ﬁshed?\n32. Whom was the ﬁsh caught by?\n33. By whom was the ﬁsh caught?\n34. Whom was the ﬁsh expected to be given to?\n35. An engineer reading the newspaper in the balcony got angry",
    "29. What river was it caught in?\n30. Whom did you want Rama to tell to catch a ﬁsh?\n31. * In what river they ﬁshed?\n32. Whom was the ﬁsh caught by?\n33. By whom was the ﬁsh caught?\n34. Whom was the ﬁsh expected to be given to?\n35. An engineer reading the newspaper in the balcony got angry\n36. They found the answer that they were looking for\n37. Yesterday I ate a cake the likes of which I had never seen\n38. The deity in whose image we were depicted was unknown\nto many\n39. The windows broken in the scuﬄe have been replaced\n40. I saw a baby being given a bath in the open\n41. The ﬁsh that they thought you had told me not to bother\nwith was very small\n42. The horse raced past the barn fell\n360\nAppendix 3: ISCII\nCharacter Set\nCode\nCharacter Name\n161\nVowel-Modiﬁer caMdrabiMdu\n162\nVowel-Modiﬁer anusvaara\n163\nVowel-Modiﬁer visarga\n164\nVowel a\n165\nVowel aa\n166\nVowel i\n167\nVowel ii\n168\nVowel u\n169\nVowel uu\n170\nVowel R\n171\nVowel e (Southern Scripts)\n172\nVowel ee\n173\nVowel ai\n174\nVowel aye (deevanaagari Script)\n175\nVowel o (Southern Scripts)\n176\nVowel oo\n177\nVowel au\n178\nVowel awe (deevanaagari Script)\n179\nConsonant ka\n180\nConsonant kha\n181\nConsonant ga\n182\nConsonant gha\n183\nConsonant nga\n361\nCode\nCharacter Name\n184\nConsonant ca\n185\nConsonant cha\n186\nConsonant ja\n187\nConsonant jha\n188\nConsonant jnya\n189\nConsonant Ta\n190\nConsonant Tha\n191\nConsonant Da\n192\nConsonant Dha\n193\nConsonant Na\n194\nConsonant ta\n195\nConsonant tha\n196\nConsonant da\n197\nConsonant dha\n198\nConsonant na\n199\nConsonant na (Tamil)\n200\nConsonant pa\n201\nConsonant pha\n202\nConsonant ba\n203\nConsonant bha\n204\nConsonant ma\n205\nConsonant ya\n206\nConsonant jya (Assamese, Bangla, Oriya Scripts)\n207\nConsonant ra\n208\nConsonant Hard ra (Southern Scripts)\n209\nConsonant la\n210\nConsonant La\n211\nConsonant zha (Tamil, Malayalam Scripts)\n212\nConsonant va\n213\nConsonant s’a\n214\nConsonant Sa\n215\nConsonant sa\n216\nConsonant ha\n217\nConsonant INVISIBLE\n362\nCode\nCharacter Name\n218\nVowel Sign aa\n219\nVowel Sign i\n220\nVowel Sign ii\n221",
    "209\nConsonant la\n210\nConsonant La\n211\nConsonant zha (Tamil, Malayalam Scripts)\n212\nConsonant va\n213\nConsonant s’a\n214\nConsonant Sa\n215\nConsonant sa\n216\nConsonant ha\n217\nConsonant INVISIBLE\n362\nCode\nCharacter Name\n218\nVowel Sign aa\n219\nVowel Sign i\n220\nVowel Sign ii\n221\nVowel Sign u\n222\nVowel Sign uu\n223\nVowel Sign R\n224\nVowel Sign e (Southern Scripts)\n225\nVowel Sign ee\n226\nVowel Sign ai\n227\nVowel Sign aye (deevanaagari Script)\n228\nVowel Sign o (Southern Scripts)\n229\nVowel Sign oo\n230\nVowel Sign au\n231\nVowel Sign awe (deevanaagari Script)\n232\nVowel Omission Sign (halaMt)\n233\nDiacritic Sign (nukta)\n234\nFull Stop (viraam, Northern Scripts)\n235\nReserved\n236\nReserved\n237\nReserved\n238\nReserved\n239\nAttribute Code\n240\nExtension Code\n241\nDigit 0\n242\nDigit 1\n243\nDigit 2\n244\nDigit 3\n245\nDigit 4\n246\nDigit 5\n247\nDigit 6\n248\nDigit 7\n249\nDigit 8\n250\nDigit 9\n251\nReserved\n252\nReserved\n253\nReserved\n254\nReserved\nBIS Standard IS 13194:1991\n363\n364\nIndex\naakaamksha 209\naboutness 32\nad-hoc retrieval 29\naﬃx 143\nagglutinative language 136\nakshara 273, 302, 306, 311\nallomorph 144\nAlta-Vista 40\namarakoos’a 110\nanaphoric reference 59\nresolution of 228, 344\nantecedent 229\na posteriori probability 251\nASCII 270, 281, 283, 297, 318\nATN - see augmented transition\nnetwork\nattachment 220\naugmented transition network 197\nautomatic speech recognition 72\nconnected word 73\ncontinuous 73\nisolated word 73\nsmall/large vocabulary 73\nspeaker (in)dependent 73\nspontaneous 73\nautomatic summarization - see\ntext summarization\nautomatic translation 7, 54\napproaches to 62\nchallenges 64\ndeploying 60\ndirect 64\nexample based 63\nhuman aided 63\nin India 64, 70\ninter-lingua 63\nis hard 55\nrule based 63\nstatistical 63\ntransfer 63\nbackformation 148\nbackward algorithm 260\nbag of words 30\nbase line 165\nBaum-Welch algorithm 260\nBayesian learning 53, 250\nBayes theorem 250\nbigrams 317\nBORIS 24\nbrowser 4\ncategorial grammar 207\ncharacter encoding standards 281\nissues in 291\nchunking 51, 134, 214\nCICERO 38\nclustering 253",
    "transfer 63\nbackformation 148\nbackward algorithm 260\nbag of words 30\nbase line 165\nBaum-Welch algorithm 260\nBayesian learning 53, 250\nBayes theorem 250\nbigrams 317\nBORIS 24\nbrowser 4\ncategorial grammar 207\ncharacter encoding standards 281\nissues in 291\nchunking 51, 134, 214\nCICERO 38\nclustering 253\ncommon sense 26\ncompound 129, 130\ncomputational linguistics 87\nconceptual dependency 21\nconﬂation 135\ncoreference 37\n365\ncorpora - see corpus\ncorpus 235\nannotated 238\ndeveloping 239\nspeech 241\ntext 237\ncorpus linguistics 233\ncross-serial dependency 193\ncurse of dimensionality 51\ndependency grammar 206\ndictionary 99\nbilingual 68, 107, 112\ncontents 104\nelectronic 100\nmonolingual 68, 107, 112\nstructure and organization\n109\ndimensionality reduction 51, 54,\n245, 345\ndiphone 75\ndiscourse structure 60\nsegmentation 344\ndissimilarity 53\ndistance 53\ndocument ﬁltering 5\ndocument routing 6\neekavaakyata 46\nELIZA 9\nexpert system 5\nfeature 48\nweighting 51\nﬁnite state automaton - see ﬁ-\nnite state machine\nﬁnite state machine 123\nDFA 124\nNFA 124\nF-measure 37, 248, 337\nfont 277\nforward algorithm 260\nfree word order 189\nfunctional dependency 188\ngeneralized phrase structure gram-\nmar 205\ngenescene 38\nglyph 277\nGPSG - see generalized phrase\nstructure grammar\ngrammar 166, 174\ncontext free 182\nformalism 177\ntypes of 182\nuniversality 179\nhidden Markov model 164, 257\nHMM - see hidden Markov model\nholonym 118\nhomonymy 128, 163, 226, 343\nhuman and machine intelligence\n77\nhuman language computing 87\nhypernym 118\nhyponym 118\nidf - see inverse document fre-\nquency\nidioms and phrases 56\nin-depth understanding 23\ninﬂectional language 136\ninformation extraction 6, 25\narchitecture 38\ndeﬁnition 35\ntasks 36\ninformation overload 2\ninformation retrieval 2, 28\nBoolean model 331\ndeﬁnition 29\nhistory 327\nintelligent 338\nmodels 330\nvector-space model 332\nwithout NLP 33\nINSCRIPT 285\n366\ninterpolated average precision 337\ninverse category frequency 52\ninverse document frequency 52,\n333\ninverted index 333",
    "information overload 2\ninformation retrieval 2, 28\nBoolean model 331\ndeﬁnition 29\nhistory 327\nintelligent 338\nmodels 330\nvector-space model 332\nwithout NLP 33\nINSCRIPT 285\n366\ninterpolated average precision 337\ninverse category frequency 52\ninverse document frequency 52,\n333\ninverted index 333\nISCII 270, 282, 360\nisolating language 136\nkaaraka 209\nkey word 12\nkNN - see nearest neighbour clas-\nsiﬁer\nKWIC 102\nlanguage and cognition 16\nlanguage engineering 81, 87\nlanguage identiﬁcation 77, 316\nlatent semantic indexing 345\nlemmatization 160\nlexical ambiguity 55\nlexical functional grammar 204\nlexicon 100\nLFG - see lexical functional gram-\nmar\nlikelihood 251\nLOLITA 38\nlong distance dependency 191\nLSI - see latent semantic index-\ning\nmaatra 275, 306\nmachine aided translation - see\nMAT\nmachine learning 48, 245\nmachine translation - see auto-\nmatic translation\nMarkov model 254\nMAT 65\nmeronym 118\nmeta search engine 5, 346\nmodiﬁcation 223\nadjective-noun 225\nnoun-noun 224\nmorpheme 143\nbound 143\nfree 143\nmorphology 51, 120, 342\nanalysis 68\ncomputational 152\nderivation 147\ngeneration 68\nIndian language 153\ninﬂection 149\nmovement 191\nMUC 36\nmulti-word-expression 135\nmutual information 51\nnaive Bayes classiﬁer 252\nnamed entity 36\nnatural language acquisition 88\nnatural language generation 88\nnatural language processing\nvis-a-vis linguistics 95\nnatural language understanding\n88\nn-dimensional space 30, 245, 332,\n345\nnearest neighbour classiﬁer 249\nnetwork and process model 154\nn-grams 293, 318\nNLP and Sanskrit 321\nnon-word error 305\nnormalization\ncosine 53\nlength 53\nOCR - see optical character recog-\nnition\nontology 120\noptical character recognition 86,\n311\npaaNinian grammar 208\npage ranking 340\nparse tree 181\n367\nparsing 59, 166, 170, 184\neﬃciency of 175\nprocess view 175\npartial parsing 213\npart-of-speech 102\ntagging 162\npattern matching 13\npersonal search assistant 348\nphrase 129, 131, 342\nplans and goals 21\npolysemy 163, 226, 343\npolysynthetic language 136",
    "page ranking 340\nparse tree 181\n367\nparsing 59, 166, 170, 184\neﬃciency of 175\nprocess view 175\npartial parsing 213\npart-of-speech 102\ntagging 162\npattern matching 13\npersonal search assistant 348\nphrase 129, 131, 342\nplans and goals 21\npolysemy 163, 226, 343\npolysynthetic language 136\nPOS - see part-of-speech\npragmatics 232\nprecision 31, 37, 248\nprecision-recall curves 337\npreﬁx 143\nprincipal component analysis 51\nprior probability 251\nprobability ranking principle 336\nproducer-comprehender model 89\nProteus 38\npseudo-relevance feedback 340\nPSP2 17\nquantiﬁcation 221\nquestion-answering 4\nto measure understanding 8\nreal word error 305\nrecall 31, 37, 248\nrecommender system 5\nreferent 229\nregression 246\nregular expression 124\nregular language 124\nrelevance 32\nfeedback 339\nRogerian therapy 9\nromanization 301\ns’aabdaboodha 219\nsamaasa 130\nsaMdhi 135\nsannidhi 209\nscript 21\nscript grammar 273\nscripts, plans and goals 21\nsearch engine 4\nsemantics 215\nsemantic web 349\nsentence 167\nshallow parsing 214\nSIFT 38\nsimilarity measure 335\nsingular value decomposition 51,\n346\nSIR 15\nspeaker identiﬁcation 76\nspeaker recognition 76\nspeech synthesis 74\narticulatory 75\nconcatenative 75\nspeech technology 71\nspeech understanding 72\nspell checker - see spelling error\ndetection/correction\nspelling error 20\ncorrection 304\ndeﬁnition of 306\ndetection 304\nstationarity 255\nstemmer 342\nstemming 51, 160, 342\nstop word 51, 341\nstory understanding 21\nstructural ambiguity 59\nSTUDENT 13\nsuﬃx 143\nsupervised learning 246\nSVD - see singular value decom-\nposition\nsynonymy 343\n368\nsyntactic analysis - see parsing\nsyntax 59, 166, 343\nautonomy of 171\nTAG - see tree adjoining gram-\nmar\ntag-set 355\ntemplate matching 313\nterm attribute 51\nterm frequency 52, 333\nterm weighting 333\ntext attribute 51\ntext categorization 6, 47\napproaches to 47\ncategory pivoted 49\ndocument pivoted 49\nhard 50\nmulti-label 50\nranking 50\ntext classiﬁcation 50, 53\ntext clustering 50, 53\ntext mining 94\ntext representation 50",
    "term attribute 51\nterm frequency 52, 333\nterm weighting 333\ntext attribute 51\ntext categorization 6, 47\napproaches to 47\ncategory pivoted 49\ndocument pivoted 49\nhard 50\nmulti-label 50\nranking 50\ntext classiﬁcation 50, 53\ntext clustering 50, 53\ntext mining 94\ntext representation 50\ntext summarization 6, 39\napproaches to 41\nevaluation of 45\nIndian tradition 45\nrelation to IE 44\ntext tiling 344\ntext-to-speech - see speech syn-\nthesis\ntf-idf 51\ntf - see term frequency\ntheories of meaning 217\nIndian 218\nthesaurus 113\ntree adjoining grammar 205\ntrigrams 317\ntroponym 118\nTTS - see speech synthesis\nTuring test 27\ntype-token analysis 142, 237\nUCSG - see universal clause struc-\nture grammar\nunderstanding in context 20\nungrammaticality 20\nUNICODE 270, 287\nuniversal clause structure gram-\nmar 209\nunsupervised learning 246, 253\nvarNamaala 272\nvector space model 30\nViterbi algorithm 260\nword 121\ndeﬁnition 121\nformation 145\nhow many 140\nproperties 138\nstructure 143\nword group 134\nwordnet 113\nword sense disambiguation 56,\n226, 343\nworld knowledge 16, 26\nWSD - see word sense disam-\nbiguation\nyoogyata 209\n369\nFor More Information Visit\n202.41.85.68\n370\nAbout the Author\nKavi Narayana Murthy obtained his bachelors degree in Me-\nchanical Engineering from Bangalore University in the year 1983,\nMaster of Technology in Artiﬁcial Intelligence and Robotics from\nUniversity of Hyderabad in the year 1988, and PhD in Computer\nScience from University of Hyderabad in the year 1996. He has\nover 5 years of experience in reputed private and public industries\nand over 15 years of post graduate teaching and research expe-\nrience at the department of computer and information sciences,\nUniversity of Hyderabad. His research interests include Natural\nLanguage Processing and Speech Recognition.\nHe has over 40\npublications.\nHe has traveled widely and delivered lectures in\nmany prestigious academic institutions as well as industry. He\nknows several languages. His general interests include Sanskrit",
    "Language Processing and Speech Recognition.\nHe has over 40\npublications.\nHe has traveled widely and delivered lectures in\nmany prestigious academic institutions as well as industry. He\nknows several languages. His general interests include Sanskrit\nand Indian traditional knowledge systems.\n371"
]