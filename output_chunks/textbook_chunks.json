[
    "LINEAR ALGEBRA \nKENNETH HOFFMAN \nProfessor of Mathematics \nMassachusetts Institute of Technology \nRAY KUNZE \nProfessor of Mathematics \nUniversity of California, Irvine \nSecond Edition \nPRENTICE-HALL, INC., Englewood Cliffs, New Jersey \n© 1971, 1961 by \nPrentice-Hall, Inc. \nEnglewood Cliffs, New Jersey \nAll rights reserved. No part of this book may be \nreproduced in any form or by any means without \npermission in writing from the publisher. \nPRENTICE-HALL INTERNATIONAL, INC., London \nPRENTICE-HALL OF AUSTRALIA, P'l'Y. W'D., Sydney \nPRENTICE-HALL OF CANADA, LTD., Toronto \nPRENTICE-HALL OF INDIA PRIVATE LIMITED, New Delhi \nPRENTICE-HALL OF JAPAN, INC., Tokyo \nCurrent printing (last digit): \n10 \n9 \n8 \n7 \n6 \nLibrary of Congress Catalog Card No. 75-142120 \nPrinted in the United States of America \nPreface \nOur original purpose in writing this book was to provide a text for the under­\ngraduate linear algebra course at the Massachusetts Institute of Technology. This \ncourse was designed for mathematics majors at the junior level, although three­\nfourths of the students were drawn from other scientific and technological disciplines \nand ranged from freshmen through graduate students. This description of the \nM.LT. audience for the text remains generally accurate today. The ten years since \nthe first edition have seen the proliferation of linear algebra courses throughout \nthe country and have afforded one of the authors the opportunity to teach the \nbasic material to a variety of groups at Brandeis University, Washington Univer­\nsity (St. Louis), and the University of California (Irvine). \nOur principal aim in revising Linear Algebra has been to increase the variety \nof courses which can easily be taught from it. On one hand, we have structured the \nchapters, especially the more difficult ones, so that there are several natural stop­\nping points along the way, allowing the instructor in a one-quarter or one-semester",
    "of courses which can easily be taught from it. On one hand, we have structured the \nchapters, especially the more difficult ones, so that there are several natural stop­\nping points along the way, allowing the instructor in a one-quarter or one-semester \ncourse to exercise a considerable amount of choice in the subject matter. On the \nother hand, we have increased the amount of material in the text, so that it can be \nused for a rather comprehensive one-year course in linear algebra and even as a \nreference book for mathematicians. \nThe major changes have been in our treatments of canonical forms and inner \nproduct spaces. In Chapter 6 we no longer begin with the general spatial theory \nwhich underlies the theory of canonical forms. We first handle characteristic values \nin relation to triangulation and diagonalization theorems and then build our way \nup to the general theory. We have split Chapter 8 so that the basic material on \ninner product spaces and unitary diagonalization is followed by a Chapter 9 which \ntreats sesqui-linear forms and the more sophisticated properties of normal opera­\ntors, including normal operators on real inner product spaces. \nWe have also made a number of small changes and improvements from the \nfirst edition. But the basic philosophy behind the text is unchanged. \nWe have made no particular concession to the fact that the majority of the \nstudents may not be primarily interested in mathematics. For we believe a mathe­\nmatics course should not give science, engineering, or social science students a \nhodgepodge of techniques, but should provide them with an understanding of \nbasic mathematical concepts. \niii \niv \nPreface \nOn the other hand, we have been keenly aware of the wide range of back­\ngrounds which the students may possess and, in particular, of the fact that the \nstudents have had very little experience with abstract mathematical reasoning. \nFor this reason, we have avoided the introduction of too many abstract ideas at",
    "grounds which the students may possess and, in particular, of the fact that the \nstudents have had very little experience with abstract mathematical reasoning. \nFor this reason, we have avoided the introduction of too many abstract ideas at \nthe very beginning of the book. In addition, we have included an Appendix which \npresents such basic ideas as set, function, and equivalence relation. We have found \nit most profitable not to dwell on these ideas independently, but to advise the \nstudents to read the Appendix when these ideas arise. \nThroughout the book we have included a great variety of examples of the \nimportant conccpts which occur. The study of such examples is of fundamental \nimportance and tends to minimize the number of students who can repeat defini­\ntion, theorem, proof in logical order without grasping the meaning of the abstract \nconcepts. The book also contains a wide variety of graded exercises (about six \nhundred), ranging from routine applications to ones which will extend the very \nbest students. These exercises are intended to be an important part of the text. \nChapter 1 deals with systems of linear equations and their solution by means \nof elementary row operations on matrices. It has been our practice to spend about \nsix lectures on this material. It provides the student with some picture of the \norigins of linear algebra and with the computational technique necessary to under­\nstand examples of the more abstract ideas occurring in the later chapters. Chap­\nter 2 deals with vector spaces, subspaces, bases, and dimension. Chapter 3 treats \nlinear transformations, their algebra, their representation by matrices, as well as \nisomorphism, linear functionals, and dual spaces. Chapter 4 defines the algebra of \npolynomials over a field, the ideals in that algebra, and the prime factorization of \na polynomial. It also deals with roots, Taylor's formula, and the Lagrange inter­",
    "isomorphism, linear functionals, and dual spaces. Chapter 4 defines the algebra of \npolynomials over a field, the ideals in that algebra, and the prime factorization of \na polynomial. It also deals with roots, Taylor's formula, and the Lagrange inter­\npolation formula. Chapter 5 develops determinants of square matrices, the deter­\nminant being viewed as an alternating n-linear function of the rows of a matrix, \nand then proceeds to multilinear functions on modules as well as the Grassman ring. \nThe material on modules places the concept of determinant in a wider and more \ncomprehensive setting than is usually found in elementary textbooks. Chapters 6 \nand 7 contain a discussion of the concepts which are basic to the analysis of a single \nlinear transformation on a finite-dimensional vector space; the analysis of charac­\nteristic (eigen) values, triangulable and diagonalizable transformations; the con­\ncepts of the diagonalizable and nilpotent parts of a more general transformation, \nand the rational and Jordan canonical forms. The primary and cyclic decomposition \ntheorems play a central role, the latter being arrived at through the study of \nadmissible subspaces. Chapter 7 includes a discussion of matrices over a polynomial \ndomain, the computation of invariant factors and elementary divisors of a matrix, \nand the development of the Smith canonical form. The chapter ends with a dis­\ncussion of semi-simple operators, to round out the analysis of a single operator. \nChapter 8 treats finite-dimensional inner product spaces in some detail. It covers \nthe basic geometry, relating orthogonalization to the idea of 'best approximation \nto a vector' and leading to the concepts of the orthogonal projection of a vector \nonto a subspace and the orthogonal complement of a subspace. The chapter treats \nunitary operators and culminates in the diagonalization of self-adjoint and normal \noperators. Chapter 9 introduces sesqui-linear forms, relates them to positive and",
    "onto a subspace and the orthogonal complement of a subspace. The chapter treats \nunitary operators and culminates in the diagonalization of self-adjoint and normal \noperators. Chapter 9 introduces sesqui-linear forms, relates them to positive and \nself-adjoint operators on an inner product space, moves on to the spectral theory \nof normal operators and then to more sophisticated results concerning normal \noperators on real or complex inner product spaces. Chapter 10 discusses bilinear \nforms, emphasizing canonical forms for symmetric and skew-symmetric forms, as \nwell as groups preserving non-degenerate forms, especially the orthogonal, unitary, \npseudo-orthogonal and Lorentz groups. \n' \nWe feel that any course which uses this text should cover Chapters 1, 2, and 3 \nPreflUJe \nv \nthoroughly, possibly excluding Sections 3.6 and 3.7 which deal with the double dual \nand the transpose of a linear transformation. Chapters 4 and 5, on polynomials and \ndeterminants, may be treated with varying degrees of thoroughness. In fact, \npolynomial ideals and basic properties of determinants may be covered quite \nsketchily without serious damage to the flow of the logic in the text; however, our \ninclination is to deal with these chapters carefully (except the results on modules), \nbecause the material illustrates so well the basic ideas of linear algebra. An ele­\nmentary course may now be concluded nicely with the first four sections of Chap­\nter 6, together with (the new) Chapter 8. If the rational and Jordan forms arc to \nbe included, a more extensive coverage of Chapter 6 is necessary. \nOur indebtedness remains to those who contributed to the first edition, espe­\ncially to Professors Harry Furstenberg, Louis Howard, Daniel Kan, Edward Thorp, \nto Mrs. Judith Bowers, Mrs. Betty Ann (Sargent) Rose and Miss Phyllis Ruby. \nIn addition, we would like to thank the many students and colleagues whose per­\nceptive comments led to this revision, and the staff of Prentice-Hall for their",
    "to Mrs. Judith Bowers, Mrs. Betty Ann (Sargent) Rose and Miss Phyllis Ruby. \nIn addition, we would like to thank the many students and colleagues whose per­\nceptive comments led to this revision, and the staff of Prentice-Hall for their \npatience in dealing with two authors caught in the throes of academic administra­\ntion. Lastly, special thanks are due to Mrs. Sophia Koulouras for both her skill \nand her tireless efforts in typing the revised manuscript. \nK. M. H. / R. A. K. \nContents \nChapter 1. Linear Equations \n1 \n1.1. \nFields \n1 \n1.2. \nSystems of Linear Equations \n3 \n1.3. \nMatrices and Elementary Row Operations \n6 \n1.4. \nRow-Reduced Echelon Matrices \n11 \n1.5. \nMatrix Multiplication \n16 \n1.6. \nInvertible Matrices \n21 \nChapter 2. Vector Spaces \n28 \n2.1. \nVector Spaces \n28 \n2.2. \nSubspaces \n34 \n2.3. \nBases and Dimension \n40 \n2.4. \nCoordinates \n49 \n2.5. \nSummary of Row-Equivalence \n55 \n2.6. \nComputations Concerning Subspaces \n58 \nChapter 3. Linear Transformations \n67 \n3.1. \nLinear Transformations \n67 \n3.2. \nThe Algebra of Linear Transformations \n74 \n3.3. \nIsomorphism \n84 \n3.4. \nRepresentation of Transformations by Matrices \n86 \n3.5. \nLinear Functionals \n97 \n3.6. \nThe Double Dual \n107 \n3.7. \nThe Transpose of a Linear Transformation \n111 \nvi \nContents \nChapter 4. Polynomials \n4.1. \nAlgebras \n117 \n117 \n4.2. \nThe Algebra of Polynomials \n4.3. \nLagrange Interpolation \n4.4. \nPolynomial Ideals \n4.5. \nThe Prime Factorization of a Polynomial \n119 \n124 \n127 \n134 \nChapter 5. Determinants \n140 \nChapter 6. \nChapter 7. \nChapter 8. \n5.1. \nCommutative Rings \n140 \n5.2. \nDeterminant Functions \n141 \n5.3. \nPermutations and the Uniqueness of Determinants \n150 \n5.4. \nAdditional Properties of Determinants \n156 \n5.5. \nModules \n164 \n5.6. \nMultilinear Functions \n166 \n5.7. \nThe Grassman Ring \n173 \nElementary Canonical Forms \n181 \n6.1. \nIntroduction \n181 \n6.2. \nCharacteristic Values \n182 \n6.3. \nAnnihilating Polynomials \n190 \n6.4. \nInvariant Subspaces \n198 \n6.5.",
    "Additional Properties of Determinants \n156 \n5.5. \nModules \n164 \n5.6. \nMultilinear Functions \n166 \n5.7. \nThe Grassman Ring \n173 \nElementary Canonical Forms \n181 \n6.1. \nIntroduction \n181 \n6.2. \nCharacteristic Values \n182 \n6.3. \nAnnihilating Polynomials \n190 \n6.4. \nInvariant Subspaces \n198 \n6.5. \nSimultaneous Triangulation; Simultaneous \nDiagonaliza tion \n206 \n6.6. \nDirect-Sum Decompositions \n209 \n6.7. \nInvariant Direct Sums \n213 \n6.8. \nThe Primary Decomposition Theorem \n219 \nThe Rational and Jordan Forms \n227 \n7.1. \nCyclic Subspaces and Annihilators \n227 \n7.2. \nCyclic Decompositions and the Rational Form \n231 \n7.3. \nThe Jordan Form \n244 \n7.4. \nComputation of Invariant Factors \n251 \n7.5. \nSummary; Semi-Simple Operators \n262 \nInner Product Spaces \n270 \n8.1. \nInner Products \n270 \n8.2. \nInner Product Spaces \n277 \n8.3. \nLinear Functionals and Adjoints \n290 \n8.4. \nUnitary Operators \n299 \n8.5. \nNormal Operators \n311 \nvii \nviii \nContents \nChapter 9. Operators on Inner Product Spaces \n319 \n9.1. Introduction \n319 \n9.2. Forms on Inner Product Spaces \n320 \n9.3. Positive Forms \n325 \n9.4. More on Forms \n332 \n9.5. Spectral Theory \n335 \n9.6. Further Properties of Normal Operators \n349 \nChapter 10. Bilinear Forms \n359 \n10.1. Bilinear Forms \n359 \n10.2. Symmetric Bilinear Forms \n367 \n10.3. \nSkew-Symmetric Bilinear Forms \n375 \n10.4 \nGroups Preserving Bilinear Forms \n379 \nAppendix \n386 \nA.I. Sets \n387 \nA.2. \nFunctions \n388 \nA.3. \nEquivalence Relations \n391 \nAA. Quotient Spaces \n394 \nA.5. \nEquivalence Relations in Linear Algebra \n397 \nA.6. \nThe Axiom of Choice \n399 \nBibliography \n400 \nIndex \n401 \n1. Linear Equations \n1 .1. Fields \nWe assume that the reader is familiar with the elementary algebra of \nreal and complex numbers. For a large portion of this book the algebraic \nproperties of numbers which we shall use are easily deduced from the \nfollowing brief list of properties of addition and multiplication. We let F \ndenote either the set of real numbers or the set of complex numbers.",
    "real and complex numbers. For a large portion of this book the algebraic \nproperties of numbers which we shall use are easily deduced from the \nfollowing brief list of properties of addition and multiplication. We let F \ndenote either the set of real numbers or the set of complex numbers. \n1. Addition is commutative, \nx+y \ny+x \nfor all x and y in F. \n2. Addition is associative, \nx + (y + z) = (x + y) + z \nfor all x, y, and z in F. \n3. There is a unique element 0 (zero) in F such that x + 0 = x, for \nevery x in F. \n4. To each x in F there corresponds a unique element (-x) in F such \nthat x + (-x) \n= O. \n5. Multiplication is commutative, \nxy = yx \nfor all x and y in F. \n6. Multiplication is associative, \nx(yz) = (xy)z \nfor all x, y, and z in F. \n1 \n2 \nLinear Equations \nChap. 1 \n7. There is a unique non-zero element 1 (one) in F such that xl = x, \nfor every x in F. \n8. To each nOll-zero x in F there corresponds a unique element X-I \n(or 1/x) in F such that xx-1 = 1. \n9. Multiplication distributes over addition; that is, x(y + z) \n= \nxy + xz, for all x, y, and z in F. \nSuppose one has a set F of objects x, y, z, . . .  and two operations on \nthe elements of F as follows. The first operation, called addition, asso­\nciates with each pair of elements x, y in F an element (x + y) in F; the \nsecond operation, called multiplication, associates with each pair x, y an \nelement xy in F; and these two operations satisfy conditions (1)-(9) above. \nThe set F, together with these two operations, is then called a field. \nRoughly speaking, a field is a set together with some operations on the \nobjects in that set which behave like ordinary addition, subtraction, \nmultiplication, and division of numbers in the sense that they obey the \nnine rules of algebra listed above. With the usual operations of addition \nand multiplication, the set C of complex numbers is a field, as is the set R \nof real numbers. \nFor most of this book the 'numbers' we use may as well be the ele­",
    "nine rules of algebra listed above. With the usual operations of addition \nand multiplication, the set C of complex numbers is a field, as is the set R \nof real numbers. \nFor most of this book the 'numbers' we use may as well be the ele­\nments from any field F. To allow for this generality, we shall use the \nword 'scalar' rather than 'number.' Not much will be lost to the reader \nif he always assumes that the field of scalars is a subfield of the field of \ncomplex numbers. A subfield of the field C is a set F of complex numbers \nwhich is itself a field under the usual operations of addition and multi­\nplication of complex numbers. This means that ° and 1 are in the set F, \nand that if x and y are elements of F, so are (x + y), -x, xy, and X-I \n(if x x 0). An example of such a subfield is the field R of real numbers; \nfor, if we identify the real numbers with the complex numbers (a + ib) \nfor which b = 0, the ° and 1 of the complex field are real numbers, and \nif x and y are real, so are (x + y), -x, xy, and X-I (if x x 0). We shall \ngive other examples below. The point of our discussing sub fields is essen­\ntially this: If we are working with scalars from a certain sub field of C, \nthen the performance of the operations of addition, subtraction, multi­\nplication, or division on these scalars does not take us out of the given \nsubfield. \nEXAMPLE 1 .  The set of positive integers: 1, 2, 3, . . .  , is not a sub­\nfield of C, for a variety of reasons. For example, ° is not a positive integer; \nfor no positive integer n is -n a positive integer; for no positive integer n \nexcept 1 is lin a positive integer. \nEXAMPLE 2. The set of integers: .\n. . , \n2, -1, 0, 1, 2, . . .  , is not a \nsub field of C, because for an integer n, lin is not an integer unless n is 1 or \nSec. 1.2 \nSystems of Linear Equations \n-1. With the usual operations of addition and multiplication, the set of \nintegers satisfies all of the conditions (1)-(9) except condition (8).",
    "sub field of C, because for an integer n, lin is not an integer unless n is 1 or \nSec. 1.2 \nSystems of Linear Equations \n-1. With the usual operations of addition and multiplication, the set of \nintegers satisfies all of the conditions (1)-(9) except condition (8). \nEXAMl'LE 3. The set of rational numbers, that is, numbers of the \nform p/q, where p and q are integers and q ;t. 0, is a subfield of the field \nof complex numbers. The division which is not possible within the set of \nintegers is possible within the set of rational numbers. The interested \nreader should verify that any sub field of C must contain every rational \nnumber. \nEXAMPLE 4. The set of all complex numbers of the form x + y V2, \nwhere x and y are rational, is a subfield of C. We leave it to the reader to \nverify this. \nIn the examples and exercises of this book, the reader should assume \nthat the field involved is a sub field of the complex numbers, unless it is \nexpressly stated that the field is more general. We do not want to dwell \non this point; however, we should indicate why we adopt such a conven­\ntion. If F is a field, it may be possible to add the unit 1 to itself a finite \nnumber of times and obtain 0 (see Exercise 5 following Section 1.2) : \n1 + 1 + . . .  + 1 \nO. \nThat does not happen in the complex number field (or in any subfield \nthereof). If it does happen in F, then the least n such that the sum of n \nl's is 0 is called the characteristic of the field F. If it does not happen \nin F, then (for some strange reason) F is called a field of characteristic \nzero. Often, when we assume F is a subfield of C, what we want to guaran­\ntee is that F is a field of characteristic zero; but, in a first expmmre to \nlinear algebra, it is usually better not to worry too much about charac­\nteristics of fields. \n3 \n1 .2. Systems of Linear Equations \nSuppose F is a field. We consider tbe problem of finding n scalars \n(elements of F) XI, •\n•\n•\n , Xn which satisfy the conditions",
    "linear algebra, it is usually better not to worry too much about charac­\nteristics of fields. \n3 \n1 .2. Systems of Linear Equations \nSuppose F is a field. We consider tbe problem of finding n scalars \n(elements of F) XI, •\n•\n•\n , Xn which satisfy the conditions \nAnXl + A12X2 + . . . + A1nxn \nYI \n(1-1) \nA21Xl + A22X2 + . . .  + A2nxn = Y2 \nwhere y I, . . .  ,Ym and Aij, 1 ::; i ::; m, 1 ::; j ::; n, are given elements \nof F. We call (1-1) a system of m linear equations in n unknowns. \nAny n-tuple (XI, .\n.\n•\n , xn) of elements of F which satisfies each of the \n4 \nLinear Equations \nChap. 1 \nequations in (1-1) is called a solution of the system. If YI = Y2 = \n.\n.\n.\n \nYm \n0, we say that the system is homogeneous, or that each of the \nequations is homogeneous. \nPerhaps the most fundamental technique for finding the solutions \nof a system of linear equations is the technique of elimination. We can \nillustrate this technique on the homogeneous system \n2XI -\nX2 + Xa = 0 \nXl + 3X2 + 4xa = O. \nIf we add (-2) times the second equation to the first equation, we obtain \n-7X2 - txa = 0 \nor, X2 \n-X3. If we add 3 times the first equation to the second equation, \nwe obtain \n7XI + txa = 0 \nor, Xl = -Xa. So we conclude that if (Xl, X2, Xa) is a solution then Xl \n= X2 = \n-Xa. Conversely, one can readily verify that any such triple is a solution. \nThus the set of solutions consists of all triples ( \na, \na, a). \nWe found the solutions to this system of equations by 'eliminating \nunknowns,' that is, by multiplying equations by scalars and then adding \nto produce equations in which some of the Xj were not present. We wish \nto formalize this process slightly so that we may understand why it works, \nand so that we may carry out the computations necessary to solve a \nsystem in an organized manner. \nFor the general system (1-1), suppose we select m scalars CI, .\n•\n•\n , Cm, \nmultiply the jth equation by Cj and then add. We obtain the equation",
    "and so that we may carry out the computations necessary to solve a \nsystem in an organized manner. \nFor the general system (1-1), suppose we select m scalars CI, .\n•\n•\n , Cm, \nmultiply the jth equation by Cj and then add. We obtain the equation \n(ClAn + ... + CmAml)Xl + . . .  + (clAln + . . .  + cmAmn)xn \n= CIYI + . . . + CmYm. \nSuch an equation we shall call a linear combination of the equations in \n(1-1). Evidently, any solution of the entire system of equations (1-1) will \nalso be a solution of this new equation. This is the fundamental idea of \nthe elimination process. If we have another system of linear equations \n0-2) \nBnXl + ... + B1nxn = Zl \nBklxl + . . . + Bknxn \nZk \nin which each of the k equations is a linear combination of the equations \nin (1-1), then every solution of (1-1) is a solution of this new system. Of \ncourse it may happen that some solutions of (1-2) are not solutions of \n(1-1). This clearly does not happen if each equation in the original system \nis a linear combination of the equations in the new system. Let us say \nthat two systems of linear equations are equivalent if each equation \nin each system is a linear combination of the equations in the other system. \nWe can then formally state our observations as follows. \nSec. 1.2 \nSystems of Linear Equations \nTheorem 1. Equivalent systems of linear equations have exactly the \nsame solutions. \nIf the elimination process is to be effective in finding the solutions of \na system like (1-1), then one must see how, by forming linear combina­\ntions of the given equations, to produce an equivalent system of equations \nwhich is easier to solve. In the next section we shall discuss one method \nof doing this. \nExercises \n1. Verify that the set of complex numbers described in Example 4 is a sub­\nfield of C. \n2. Let F be the field of complex numbers. Are the following two systems of linear \nequations equivalent? If so, express each equation in each system as a linear",
    "of doing this. \nExercises \n1. Verify that the set of complex numbers described in Example 4 is a sub­\nfield of C. \n2. Let F be the field of complex numbers. Are the following two systems of linear \nequations equivalent? If so, express each equation in each system as a linear \ncombination of the equations in the other system. \n3Xl + X2 = 0 \nXl + X2 \n0 \n3. Test the following systems of equations as in Exercise 2. \n-Xl + X2 + 4xa \n0 \nXl + 3X2 + 8xa = 0 \nĵXl + X2 + fXa = 0 \nXl \n-\nXa \n0 \nX2 + 3xa = 0 \n4. Test the following systems as in Exercise 2. \n(1 + ȳ) Xl + 8X2 - iXa -\nX4 = 0 \n5. Let F be a set which contains exactly two elements, 0 and 1. Define an addition \nand multiplication by the tables: \n+ \n0 \n1 \no \n0 \n1 \n1 \n1 \n0 \no \n1 \no \n0 0 \n1 \n0 \n1 \nVerify that the set F, together with these two operations, is a field. \n6. Prove that if two homogeneous systems of linear equations in two unknowns \nhave the same solutions, then they are equivalent. \n7. Prove that each subfield of the field of complex numbers contains every \nrational number. \n8. Prove that each field of characteristic zero contains a copy of the rational \nnumber field. \n5 \n6 \nLinear Equations \nChap. 1 \n1.3. Matrices and Elementary \nRow Operations \nOne cannot fail to notice that in forming linear combinations of \nlinear equations there is no need to continue writing the 'unknowns' \nXl, . . .  , Xn, since one actually computes only with the coefficients A Ii and \nthe scalars Yi. We shall now abbreviate the system (1-1) by \nAX = Y \nwhere \nWe call A the lllatrix of coefficients of the system. Strictly speaking, \nthe rectangular array displayed above is not a matrix, but is a repre­\nsentation of a matrix. An m X n matrix over the field F is a function \nA from the set of pairs of integers (i, j), 1 ::; i ::; m, 1 ::; j ::; n, into the \nfield F. The entries of the matrix A are the scalars A (i, j) = Aij, and \nquite often it is most convenient to describe the matrix by displaying its",
    "A from the set of pairs of integers (i, j), 1 ::; i ::; m, 1 ::; j ::; n, into the \nfield F. The entries of the matrix A are the scalars A (i, j) = Aij, and \nquite often it is most convenient to describe the matrix by displaying its \nentries in a rectangular array having m rows and n columns, as above. \nThus X (above) is, or defines, an n X 1 matrix and Y is an m X 1 matrix. \nFor the time being, AX = Y is nothing more than a shorthand notation \nfor our system of linear equations. Later, when we have defined a multi­\nplication for matrices, it will mean that Y is the product of A and X. \nWe wish now to consider operations on the rows of the matrix A \nwhich correspond to forming linear combinations of the equations in \nthe system AX = Y. We restrict our attention to three elelllentary row \noperations on an m X n matrix A over the field F: \n1. multiplication of one row of A by a non-zero scalar Cj \n2. replacement of the rth row of A by row r plus C times row 8, C any \nscalar and r y s; \n3. interchange of two rows of A. \nAn elementary row operation is thus a special type of function (rule) e \nwhich associated with each m X n matrix A an m X n matrix e(A). One \ncan precisely describe e in the three cases as follows: \n1. e(A)ij = Aij if i x r, \ne(A)rj = cArj. \n2. e(A)ii = Aij if i x r, \ne(A)rj = Arj + cA.j. \n3. e(A)ij = Aij if i is different from both r and 8, \ne(A)rj = A8;, \ne(A)8j = Arj. \nSec. 1.3 \nMatrices and Elementary Row Operations \nIn defining e(A), it is not really important how many columns A has, but \nthe number of rows of A is crucial. For example, one must worry a little \nto decide what is meant by interchanging rows 5 and 6 of a 5 X 5 matrix. \nTo avoid any such complications, we shall agree that an elementary row \noperation e is defined on the class of all m X n matrices over F, for some \nfixed m but any n. In other words, a particular e is defined on the class of \nall m-rowed matrices over F'.",
    "To avoid any such complications, we shall agree that an elementary row \noperation e is defined on the class of all m X n matrices over F, for some \nfixed m but any n. In other words, a particular e is defined on the class of \nall m-rowed matrices over F'. \nOne reason that we restrict ourselves to these three simple types of \nrow operations is that, having performed such an operation e on a matrix \nA, we can recapture A by performing a similar operation on e(A). \nJ'heorem 2. 1'0 each elementary row operation e there corresponds an \nelementary row operation e1, of the same type as e, such that el(e(A» \n= \ne(e1(A» \nA for each A. In other words, the inverse operation (junction) of \nan elementary row operation exists and is an elementary row operation of the \nsame type. \nProof. (1) Suppose e is the operation which multiplies the rth row \nof a matrix by the non-zero scalar c. Let e1 be the operation which multi­\nplies row r by c-1• (2) Suppose e is the operation which replaces row r by \nrow r plus c times row s, r x s. Let el be the operation which replaces row r \nby row r plus ( c) times row s. (3) If e interchanges rows r and s, let el \ne. \nIn each of these three cases we clearly have el(e(A» \ne(e1(A» \n= A for \neach A. I \nDefinition. If A and B are m X n matrices over the field F, we say that \nB is row-equivalent to A if B can be obtained from A by a finite sequence \nof elementary row operations. \nUsing Theorem 2, the reader should find it easy to verify the following. \nEach matrix is row-equivalent to itself; if B is ro>v-equivalent to A, then A \nis row-equivalent to B ;  if B is row-equivalent to A and C is row-equivalent \nto B, then C is row-equivalent to A. In other words, row-equivalence is \nan equivalence relation (see Appendix). \nTheorem 3. If A and B are row-equivalent m X n matrices, the homo­\ngeneous systems of linear equations AX = 0 and B X  = 0 have exactly the \nsame solutions. \nProof. Suppose we pass from A to B by a finite sequence of",
    "an equivalence relation (see Appendix). \nTheorem 3. If A and B are row-equivalent m X n matrices, the homo­\ngeneous systems of linear equations AX = 0 and B X  = 0 have exactly the \nsame solutions. \nProof. Suppose we pass from A to B by a finite sequence of \nelementary row operations: \nA \nAo -+ A1-+ ... -+ Ak \nB. \nIt is enough to prove that the systems AjX = 0 and Ai+1X = 0 have the \nsame solutions, i.e., that one elementary row operation does not disturb \nthe set of solutions. \n7 \n8 \nLinear Equations \nChap. 1 \nSo suppose that B is obtained from A by a single elementary row \noperation. No matter which of the three types the operation is, (1), (2), \nor (3), each equation in the system BX \n0 will be a linear combination \nof the equations in the system AX = O. Since the inverse of an elementary \nrow operation is an elementary row operation, each equation in AX = 0 \nwill also be a linear combination of the equations in BX \nO. Hence these \ntwo systems are equivalent, and by Theorem 1 they have the same \nsolutions. \nI \nEXAMPLE 5. Suppose F is the field of rational numbers, and \nA 7D \n-: J -U \nWe shall perform a finite sequence of elementary row operations on A, \nindicating by numbers in parentheses the type of operation performed. \nD \n- 1  \n3 \n4 \n0 \n6 \n1 \n[! \n-9 \n3 \n4 \n0 \n-2 - 1  \n[! \n-9 \n3 \n0 \n-2 \n1 \n1 2\" \n[! \n0 \n1 \n0 \n-2 \nI \n1 2\" \n-n љ[! \n-n [! \nI;JŪ[ū \n-2\" \n0 \n-EJ \n[! \n[! \n0 1 \n0 0 \n1 0 \n-9 \n3 -ŨJũ \n4 \n0 \n6 \n1 \n5 \n-9 \n3 =!J \n4 \n0 \n1 \n1 \n2\" \n0 \n15 -lJŬ \n-:r \n0 \n-2 \n1 \n1 \n-i \n2\" \n0 \n1 -¥J \n0 \n0 \n-V-\n1 \n1 \n-i \n2\" \n_,lJ \n17 \n-a-\n-i \nThe row-equivalence of A with the final matrix in the above sequence \ntells us in particular that the solutions of \nand \n2Xl \nX2 + 3xa + 2X4 \n0 \nXl + 4X2 \n-\nX4 \n= 0 \n2Xl + 6X2 -\nXa + 5X4 \n0 \nXa - J.\";-X4 = 0 \nXl \n+ ¥X4 = 0 \nX2 \niX4 = 0 \nare exactly the same. In the second system it is apparent that if we assign \nSec. 1.3 \nMatrices and Elementary Row Operations \nany rational value c to X4 we obtain a solution (--¥c, %, J\".!c, c), and also",
    "Xl + 4X2 \n-\nX4 \n= 0 \n2Xl + 6X2 -\nXa + 5X4 \n0 \nXa - J.\";-X4 = 0 \nXl \n+ ¥X4 = 0 \nX2 \niX4 = 0 \nare exactly the same. In the second system it is apparent that if we assign \nSec. 1.3 \nMatrices and Elementary Row Operations \nany rational value c to X4 we obtain a solution (--¥c, %, J\".!c, c), and also \nthat every solution is of this form. \nEXAMPLE 6. Suppose F is the field of complex numbers and \n[-1 iJ \nA \n= -Ȳ ȱ. \nIn performing row operations it is often convenient to combine several \noperations of type (2). With this in mind \n[=ŧ iJ-\"C[_ ȰŦ ;\niJ\nƼ[^ \n3i\n2iJ\n-\"C[_ n \nThus the system of equations -Xl + iX2 \n= 0 \n-iXI + 3X2 \n= 0 \nXl + 2X2 \n= 0 \nhas only the trivial solution Xl = X2 = o. \nIn Examples 5 and 6 we were obviously not performing row opera­\ntions at random. Our choice of row operations was motivated by a desire \nto simplify the coefficient matrix in a manner analogous to 'eliminating \nunknowns' in the system of linear equations. Let us now make a formal \ndefinition of the type of matrix at which we were attempting to arrive. \nDefinition. An m X n matrix R is called row-reduced if: \n(a) the first non-zero entry in each non-zero row of R is equal to 1; \n(b) each column of R which contains the leading non-zero entry of some \nrow has all its other entries o. \nEXAMPLE 7. One example of a row-reduced matrix is the n X n \n(square) identity matrix I. This is the n X n matrix defined by \n{I, if i = j \nIij = Oij = 0 'f \n. --\" . \n, \n1 \nI r J. \nThis is the first of many occasions on which vve shall use the Kronecker \ndelta (5). \nIn Examples 5 and 6, the final matrices in the sequences exhibited \nthere are row-reduced matrices. Two examples of matrices which are not \nrow-reduced are: [1 0 \no 1 o 0 o OJ \n-1 0 , \n1 0 \n9 \n10 \nLinear Equations \nChap. 1 \nThe second matrix fails to satisfy condition (a), because the leading non­\nzero entry of the first row is not 1. The first matrix does satisfy condition \n(a), but fails to satisfy condition (b) in column 3.",
    "row-reduced are: [1 0 \no 1 o 0 o OJ \n-1 0 , \n1 0 \n9 \n10 \nLinear Equations \nChap. 1 \nThe second matrix fails to satisfy condition (a), because the leading non­\nzero entry of the first row is not 1. The first matrix does satisfy condition \n(a), but fails to satisfy condition (b) in column 3. \nWe shall now prove that we can pass from any given matrix to a row­\nreduced matrix, by means of a finite number of elementary row oper­\ntions. In combination with Theorem 3, this will provide us with an effec­\ntive tool for solving systems of linear equations. \nTheQrem 4. Every m X n matrix over the field F is row-equivalent to \na row-reduced matrix. \nProof. Let A be an m X n matrix over F. If every entry in the \nfirst row of A is 0, then condition (a) is satisfied in so far as row 1 is con­\ncerned. If row 1 has a non-zero entry, let k be the smallest positive integer \nj for which Ali x 0. Multiply row 1 by Ali/, and then condition (a) is \nsatisfied with regard to row 1 .  Now for each i 2: 2, add (-Aik) times row \n1 to row ʎ'. Now the leading non-zero entry of row 1 occurs in column le, \nthat entry is 1, and every other entry in column le is 0. \nNow consider the matrix which has resulted from above. If every \nentry in row 2 is 0, we do nothing to row 2. If some entry in row 2 is dif­\nferent from 0, we multiply row 2 by a scalar so that the leading non-zero \nentry is 1 .  In the event that row 1 had a leading non-zero entry in column \nk, this leading non-zero entry of row 2 cannot occur in column k; say it \noccurs in column kT x k. By adding suitable multiples of row 2 to the \nvarious rows, we can arrange that all entries in column k' are 0, except \nthe 1 in row 2. The important thing to notice is this: In carrying out these \nlast operations, we will not change the entries of row 1 in columns 1, . . .  , k; \nnor will we change any entry of column k. Of course, if row 1 was iden­\ntically 0, the operations with row 2 will not affect row 1 .",
    "the 1 in row 2. The important thing to notice is this: In carrying out these \nlast operations, we will not change the entries of row 1 in columns 1, . . .  , k; \nnor will we change any entry of column k. Of course, if row 1 was iden­\ntically 0, the operations with row 2 will not affect row 1 .  \nWorking with one row at a time in the above manner, it i s  clear that \nin a finite number of steps we will arrive at a row-reduced matrix. \nI \nExercises \n1. Find all solutions to the system of equations \n2. If \n(1 - i)Xl - iX2 = 0 \n2Xl + (1 \ni)X2 \nO. \nfind all solutions of AX = 0 by row-reducing A. \nSec. 1.4 \n3. If \nA = [ X =Y ZJ \n- 1 \n° \n3 \nRow-Reduced Echelon Matrices \nfind all solutions of AX = 2X and all solutions of AX = 3X. (The symbol eX \ndenotes the matrix each entry of which is c times the corresponding entry of X.) \n4. Find a row-reduced matrix which is row-equivalent to \nA \n= [l \n(1 + i) \n-2 \n2i ťJ' \n-1 \n5. Prove that the following two matrices are not row-equivalent: \n[a -! `J [-b _ -rJ \n6. Let \nA \n= [; =J \nbe a 2 X 2 matrix with complex entries. Suppose that A is row-reduced and also \nthat a + b + c + d \n0. Prove that there are exactly three such matrices. \n7. Prove that the interchange of two rows of a matrix can be accomplished by a \nfinite sequence of elementary row operations of the other two types. \n8. Consider the system of equations AX = ° where \nA \n= [; ȮJ \nis a 2 X 2 matrix over the field F. Prove the following. \n(a) If every entry of A is 0, then every pair (Xl, X2) is a solution of AX = 0. \n(b) If ad - be ¢ 0, the system AX \n° has only the trivial solution Xl \nX2 = 0. \n(c) If ad - be = ° and some entry of A is different from 0, then there is a \nsolution (x«, xg) such that (Xl, X2) is a solution if and only if there is a scalar y \nsuch that Xl = yx?, X2 = yxg. \n11 \n1 .4. Row-Reduced Echelon Matrices \nUntil now, our work with systems of linear equations was motivated \nby an attempt to find the solutions of such a system. In Section 1.3 we",
    "solution (x«, xg) such that (Xl, X2) is a solution if and only if there is a scalar y \nsuch that Xl = yx?, X2 = yxg. \n11 \n1 .4. Row-Reduced Echelon Matrices \nUntil now, our work with systems of linear equations was motivated \nby an attempt to find the solutions of such a system. In Section 1.3 we \nestablished a standardized technique for finding these solutions. We wish \nnow to acquire some information which is slightly more theoretical, and \nfor that purpose it is convenient to go a little beyond row-reduced matrices. \nDefinition. An m X n rnatrix R is called a row-reduced echelon \nlllatrix if: \n12 \nLinear Equations \nChap. 1 \n(a) R is row-reduced; \n(b) every row of R which has all its entries 0 occurs below every row \nwhich has a non-zero entry; \n(c) 7f rows 1, . . . , r are the non-zero rows of R, and if the leading non­\nzero entry of \nTOW i occurs in column ki, i = 1, .\n.\n. , r, then kl < \nk2 < . .\n. < kr • \nOne can also describe an m X n row-reduced echelon matrix R as \nfollows. Either every entry in R is 0, or there exists a positive integer r, \n1 ::; r ::; m, and r positive integers kl' ... , kr with 1 ::; k; ::; n and \n(a) Rij \n0 for i > r, and Rij = 0 if j < ki• \n(b) Rikj \n= Oij, 1 ::; i ::; r, 1 ::; j ::; r. \n(c) kl < . .\n. < kr• \nEXAMPLE 8. Two examples of row-reduced echelon matrices are the \nn X n identity matrix, and the m X n zero matrix om,n, in which all \nentries are O. The reader should have no difficulty in making other ex­\namples, but we should like to give one non-trivial one: \n[0 1 \no 0 \no 0 \n-3 0 !] \no \n1 \n2 ·  \n0\n0\n0\n \nTheorem 5. Every m X n matrix A is row-equivalent to a row-reduced \nechelon matrix. \nProof. We know that A is row-equivalent to a row-reduced \nmatrix. All that we need observe is that by performing a finite number of \nrow interchanges on a row-reduced matrix we can bring it to row-reduced \nechelon form. I \nIn Examples 5 and 6, we saw the significance of row-reduced matrices",
    "Proof. We know that A is row-equivalent to a row-reduced \nmatrix. All that we need observe is that by performing a finite number of \nrow interchanges on a row-reduced matrix we can bring it to row-reduced \nechelon form. I \nIn Examples 5 and 6, we saw the significance of row-reduced matrices \nin solving homogeneous systems of linear equations. Let us now discuss \nbriefly the system RX = 0, when R is a row-reduced echelon matrix. Let \nrows 1, .\n. . , r be the non-zero rows of R, and suppose that the leading \nnon-zero entry of row i occurs in column ki• The system RX = 0 then \nconsists of r non-trivial equations. Also the unknown Xk, will occur (with \nnon-zero coefficient) only in the ith equation. If we let U1, •\n•\n.\n , Un-r denote \nthe (n - r) unknowns which are different from Xkll ' .• , Xk\" then the \nr non-trivial equations in RX \n0 are of the form \n(1-3) \nn -r \nXk, + ძ C1jUj = 0 \nj-l \nn-r \nXk, + ძ CrjUj = O. \ni-I \nSec. 1.4 \nRow-Reduced Echelon M a/rices \nAll the solutions to the system of equations RX \n0 are obtained by \nassigning any values whatsoever to Ul, •\n•\n.\n , Un-T and then computing the \ncorresponding values of Xk\" •\n•\n•\n , Xk, from (1-3). For example, if R is the \nmatrix displayed in Example 8, then r = 2, kl = 2, k2 = 4, and the two \nnon-trivial equations in the system RX = 0 are \nX2 \n3xa \ntX5 \n0 or X2 \n3xa \nX4 + 2X5 = 0 or X4 = -2X5. \nSO we may assign any values to Xl, X3, and X5, say Xl = a, Xa = b, X5 = c, \nand obtain the solution (a, 3b - c, b, -2c, c). \nLet us observe one thing more in connection with the system of \nequations RX = O. If the number r of non-zero rows in R is less than n, \nthen the system RX = 0 has a non-trivial solution, that is, a solution \n(Xl, .\n•\n.\n , Xn) in which not every Xj is O. For, since r < n, we can choose \nsome Xj which is not among the r unknowns Xk\" •\n•\n•\n , Xk\" and we can then \nconstruct a solution as above in which this Xj is 1. This observation leads",
    "(Xl, .\n•\n.\n , Xn) in which not every Xj is O. For, since r < n, we can choose \nsome Xj which is not among the r unknowns Xk\" •\n•\n•\n , Xk\" and we can then \nconstruct a solution as above in which this Xj is 1. This observation leads \nus to one of the most fundamental facts concerning systems of homoge­\nneous linear equations. \nTheorem 6. If A is an m X n matrix and m < n, then the homo-\ngeneous system of linear equations AX \n0 has a non-trivial solution. \nProof. Let R be a row-reduced echelon matrix which is row­\nequivalent to A. Then the systems AX = 0 and RX = 0 have the same \nsolutions by Theorem 3. If r is the number of non-zero rows in R, then \ncertainly r ::; m, and since m < n, we have r < n. It follows immediately \nfrom our remarks above that AX = 0 has a non-trivial solution. I \nTheorem 7. If A is an n X n (square) matrix, then A is row-equivalent \nto the n X n identity matrix if and only if the system of equations AX \n0 \nhas only the trivial sol1tt'ion. \nProof. If A is row-equivalent to I, then AX = 0 and IX = 0 \nhave the same solutions. Conversely, suppose AX = 0 has only the trivial \nsolution X \nO. Let R be an n X n row-reduced echelon matrix which is \nrow-equivalent to A, and let r be the number of non-zero rows of R. Then \nRX = 0 has no non-trivial solution. Thus r s n. But since R has n rows, \ncertainly r ::; n, and we have r \nn. Since this means that R actually has \na leading non-zero entry of 1 in each of its n rows, and since these l's \noccur each in a different one of the n columns, R must be the n X n identity \nmatrix. \nI \nLet us now ask what elementary row operations do toward solving \na system of linear equations AX = Y which is not homogeneous. At the \noutset, one must observe one basic difference between this and the homo­\ngeneous case, namely, that while the homogeneous system always has the \n13 \n14 \nLinear Equation8 \ntrivial solution Xl = \nno solution at all. \nChap. 1 \n= Xn = 0, an inhomogeneous system need have",
    "outset, one must observe one basic difference between this and the homo­\ngeneous case, namely, that while the homogeneous system always has the \n13 \n14 \nLinear Equation8 \ntrivial solution Xl = \nno solution at all. \nChap. 1 \n= Xn = 0, an inhomogeneous system need have \nWe form the augmented matrix A' of the system AX = Y. This \nis the m X (n + 1) matrix whose first n columns are the columns of A \nand whose last column is Y. More precisely, \nA;j = Aij, if j ::;  n \nAi(n+l) = Yi. \nSuppose we perform a sequence of elementary row operations on A, \narriving at a row-reduced echelon matrix R. If we perform this same \nsequence of row operations on the augmented matrix A', we will arrive \nat a matrix R' whose first n columns are the columns of R and whose last \ncolumn contains certain scalars Zl, \n•\n•\n•\n , Zm. The scalars Zi are the entries \nof the m X 1 matrix \nz[:J \nwhich results from applying the sequence of row operations to the matrix \nY. It should be clear to the reader that, just as in the proof of Theorem 3, \nthe systems AX = Y and RX = Z are equivalent and hence have the \nsame solutions. It is very easy to determine whether the system RX \nZ \nhas any solutions and to determine all the solutions if any exist. For, if R \nhas r non-zero rows, with the leading non-zero entry of row i occurring \nin column ki' i\nI, .\n. . , r, then the first r oquations of RX \nZ effec­\ntively express Xk\" •\n•\n•\n , Xk, in terms of the (n \nr) remaining Xj and the \nscalars Zl, .\n•\n•\n , Zr. The last (m - r) equations are \no \n= Zr+l \no \nZm \nand accordingly the condition for the system to have a solution is Zi \n0 \nfor i > r. If this condition is satisfied, all solutions to the system are \nfound just as in the homogeneous case, by assigning arbitrary values to \n(n - r) of the Xj and then computing Xk, from the ith equation. \nEXAMPLE 9. Let F be the field of rational numbers and \n-2 \n1 \n5 \nand suppose that we wish to solve the system AX = Y for some Yl, Yz,",
    "found just as in the homogeneous case, by assigning arbitrary values to \n(n - r) of the Xj and then computing Xk, from the ith equation. \nEXAMPLE 9. Let F be the field of rational numbers and \n-2 \n1 \n5 \nand suppose that we wish to solve the system AX = Y for some Yl, Yz, \nand Ya. Let us perform a sequence of row operations on the augmented \nmatrix A '  which row-reduces A :  \nSec. 1.4 \n[Ť \n-2 \n1 \n1 \n1 \n5 \n-1 \n[g \n-2 \n1 \n5 \n-1 \n0 \n0 \ny,] [1 \nY2 Ģ \n0 \nYa \n0 \n-2 \n5 \n5 \n1 \n-1 \n-1 \ny, ] [\n1 \n) \n(1) \n(Y2 - 2YI \nʍ \n0 \n(Ya -\nY2 + 2YI) \n0 \nRow-Reduced Echelon Matrices \n(Y2 - 2YI) Ģ \ny, ] \nYa \n-2 \n1 \n1 \n-t \n0 \n0 \ny, ] \n1 ( \n2) \n(2) \n5\" \nY2 -\nYI \n--> \n(Ya -\nY2 + 2Yl) \n[g \n0 \nܴ \n!(y, + 2y,) ] \n5 \n1 \n1 \nHY2 - 2YI) \n. \n-5 \n0 \n0 \n(Ya -\nY2 + 2YI) \nThe condition that the system AX = Y have a solution is thus \n2Yl -\nY2 + \nYa = 0 \nand if the given scalars \nYi satisfy this condition, all solutions are obtained \nby assigning a value c to Xa and then computing \nXl = - !c + HYI + 2Y2) \nX2 = \n!c + HY2 - 2YI). \nLet us observe one final thing about the system AX = Y. Suppose \nthe entries of the matrix A and the scalars \nYI, ... , \nYm happen to lie in a \nsub field FI of the field F. If the system of equations AX \n= Y has a solu­\ntion with Xl, •\n.\n.\n , Xn in F, it has a solution with Xl, •\n•\n•\n , Xn in Fl. For, \nover either field, the condition for the system to have a solution is that \ncertain relations hold between \nYI, ... , \nYrn in FI (the relations Zi = 0 for \ni > r, above). For example, if AX = Y is a system of linear equations \nin which the scalars \nYk and Aij are real numbers, and if there is a solution \nin which Xl, . .\n. , Xn are complex numbers, then there is a solution with \nXl, .\n•\n.\n , Xn real numbers. \nExercises \n1. Find all solutions to the following system of equations by row-reducing the \ncoefficient matrix: \ntXI + 2X2 -\n6X3 = 0 \n-4XI \n+ 5xs = 0 \n-3XI + 6X2 - 13xs = 0 \n-tXl + 2X2 -\n!xs = 0 \n2. Find a row-reduced echelon matrix which is row-equivalent to",
    "Xl, .\n•\n.\n , Xn real numbers. \nExercises \n1. Find all solutions to the following system of equations by row-reducing the \ncoefficient matrix: \ntXI + 2X2 -\n6X3 = 0 \n-4XI \n+ 5xs = 0 \n-3XI + 6X2 - 13xs = 0 \n-tXl + 2X2 -\n!xs = 0 \n2. Find a row-reduced echelon matrix which is row-equivalent to \nWhat are the solutions of AX = O? \n15 \n16 \nLinear Equations \n3. Describe explicitly all 2 X 2 row-reduced echelon matrices. \n4. Consider the system of equations \nXl \nX2 + 2xa = 1 \n2Xl \n+ 2X8 = 1 \nXl - 3X2 + 4xa = 2. \nDoes this system have a solution? If so, describe explicitly all solutions. \nChap. 1 \n5. Give an example of a system of two linear equations in two unknowns which \nhas no solution. \n6. Show that the system \nhas no solution. \nXl - 2X2 + Xa + 2X4 = 1 \nXl + X2 \nXa + X4 = 2 \nXl + 7 X2 \n5xa \nX4 = 3 \n7. Find all solutions of \n8. Let \n2Xl - 3X2 - 7 Xa + 5X4 + 2X5 = \n-2 \nXl - 2X2 - 4xa + 3X4 + Xs = -2 \n2Xl \n- 4xa + 2X4 + Xs = \n3 \nXl - 5X2 - 7xa + 6X4 + 2X5 = \n-7 . \nA = [; -x ]J' \n1 \n- 3  0 \nFor which triples (Yl, Y2, Ya) does the system AX \nY have a solution? \n9. Let \nA = [- -Ȭ < -]. \no \n0 1 \n1 \n1 \n-2 1 \n0 \nFor which (YI, Y2, Ya, Y4) does the system of equations A X  \n= Y have a solution? \n10. Suppose R and R' are 2 X 3 row-reduced echelon matrices and that the \nsystems R X  0 and R 'X \n0 have exactly the same solutions. Prove that R \nR'. \n1,,), Matrix Multiplication \nIt is apparent (or should be, at any rate) that the process of forming \nlinear combinations of the rows of a matrix is a fundamental one. For this \nreason it is advantageous to introduce a systematic scheme for indicating \njust what operations are to be performed. More specifically, suppose B \nis an n X p mat,rix over a field F with rows {h, . . .  , f3n and that from B we \nconstruct a matrix C with rows 'Yt, .\n•\n•\n , 'Ym by forming certain linear \ncombinations \n(1-4) \nSec. 1.5 \nMatrix Multiplication \nThe rows of G are determined by the mn scalars A ii which are themselves",
    "is an n X p mat,rix over a field F with rows {h, . . .  , f3n and that from B we \nconstruct a matrix C with rows 'Yt, .\n•\n•\n , 'Ym by forming certain linear \ncombinations \n(1-4) \nSec. 1.5 \nMatrix Multiplication \nThe rows of G are determined by the mn scalars A ii which are themselves \nthe entries of an m X n matrix A. If (1-4) is expanded to \nn \n(Gil' .. Gip) = : (AirBrl•• • AirBrp) \nr=1 \nwe see that the entries of G are given by \nn \nGij = : AirBrj. \nrCl \nDefinition. Let A be an m X n matrix over the field F and let B be an \nn X p matrix over F. The product AB is the m X p matrix C whose i, j \nentry is \nn \nCij \n: AirBrj. \nr=1 \nEXAMPLE 10. Here are some products of matrices with rational entries. \n(a) \nHere \n(b) \nHere \n1'1 = (5 \n1'2 = (0 \n[ \n1 OJ [\n5 \n-1 2] \n-3 1 \n15 \n4 8 \n1 2) = 1 . (5 \n1 2) + ° . (15 4 8) \n7 2) = -3(5 \n1 2) + 1 . (15 4 8) \n1ȫ -] \n= [-; Ȫ][O 6 IJ \n62 -3 \n5 4 \n3 8 -2 \n8 \n-2 \n0 1 \n1'2 = ( 9  12 -8) = -2(0 6 1) + 3(3 8 -2) \n1'3 = (12 62 -3) = \n5(0 6 1) + 4(3 8 -2) \n(c) \n[2ǨJ \n= [ǩ !J [!J \n(d) \n[-Ȧ ǪǫJ [ nJ[2 4J \nHere \n(e) \n(f) \n(g) \n1'2 \n(6 12) \n3(2 4) \n[2 4J [ nJ = [1OJ \nH] \nH] \n17 \n18 \nLinear Equations \nChap. 1 \nIt is important to observe that the product of two matrices need not \nbe defined; the product is defined if and only if the number of columns in \nthe first matrix coincides with the number of rows in the second matrix. \nThus it is meaningless to interchange the order of the factors in (a), (b), \nand (c) above. Frequently we shall write products such as AB without \nexplicitly mentioning the sizes of the factors and in such cases it will be \nunderstood that the product is defined. From (d), (e), (f), (g) we find that \neven when the products AB and BA are both defined it need not be true \nthat AB \nBA; in other words, matrix multiplication is not commutative. \nEXAMPLE 11. \nCa) If I is the m X m identity matrix and A is an m X n matrix, \nfA = A. \n(b) If f is the n X n identity matrix and A is an m X n matrix, \nAI = A.",
    "that AB \nBA; in other words, matrix multiplication is not commutative. \nEXAMPLE 11. \nCa) If I is the m X m identity matrix and A is an m X n matrix, \nfA = A. \n(b) If f is the n X n identity matrix and A is an m X n matrix, \nAI = A. \n(c) If Ok,m is the k X m zero matrix, Ok,,, = Ok,mA. Similarly, \nAOn,p = Om,p. \nEXAMPLE 12. Let A be an m X n matrix over F. Our earlier short­\nhand notation, AX = Y, for systems of linear equations is consistent \nwith our definition of matrix products. For if \nwith Xi in F, then AX is the m X 1 matrix \nY = [t;] \nYm \nsuch that Yi = AilXI + Ai2X2 + \n.\n. . + Ainxn. \nThe use of column matrices suggests a notation which is frequently \nuseful. If B is an n X p matrix, the columns of B are the 1 X n matrices \nBl, •\n•\n•\n ,Bp defined by [Bli] \nB· = \n: \nJ \n.\n, \nBnj \n1 ::; j ::; p. \nThe matrix B is the succession of these columns: \nB \n= [BI' . . . , Bp]. \nThe i, j entry of the product matrix AB is formed from the ith row of A \nSec. 1.5 \nM atrix Multiplication \nand the jth column of B. The reader should verify that the jth column of \nAB is ABj: \nAB = [ABl' . .. , ABp]. \nIn spite of the fact that a product of matrices depends upon the \norder in which the factors are written, it is independent of the way in \nwhich they are associated, as the next theorem shows. \nTheorem 8. If A, B, C are matrices over the field F such that the prod­\nucts BC and A(BC) are defined, then so are the products AB, (AB)C and \nA(BC) = (AB)C. \nProof. Suppose B is an n X p matrix. Since BC is defined, C is \na matrix with p rows, and BC has n rows. Because A (BC) is defined we \nmay assume A is an m X n matrix. Thus the product AB exists and is an \nm X p matrix, from which it follows that the product (AB)C exists. To \nshow that A (BC) = (AB)C means to show that \n[A(BC)]ij = [(AB)C]ij \nfor each i, j. By definition \n[A(BC)]ij = : Air(BC)rj \n• \n= 2; Ai. 2; Br,C8j \nr \n8 \nr 8 \n= 1: 1: AirB.sC.j \n8 • \n= 2; (2; AirB .. )C.j \n8 \n• \n= 2; (AB) isC sj \n8 \n= [(AB)C]ij. I",
    "show that A (BC) = (AB)C means to show that \n[A(BC)]ij = [(AB)C]ij \nfor each i, j. By definition \n[A(BC)]ij = : Air(BC)rj \n• \n= 2; Ai. 2; Br,C8j \nr \n8 \nr 8 \n= 1: 1: AirB.sC.j \n8 • \n= 2; (2; AirB .. )C.j \n8 \n• \n= 2; (AB) isC sj \n8 \n= [(AB)C]ij. I \nWhen A is an n X n (square) matrix, the product AA is defined. \nWe shall denote this matrix by A 2. By Theorem 8, (AA)A = A (AA) or \nA 2A = AA 2, so that the product AAA is unambiguously defined. This \nproduct we denote by A 3. In general, the product AA ... A (k times) is \nunambiguously defined, and we shall denote this product by A k. \nNote that the relation A(BC) \n= (AB)C implies among other things \nthat linear combinations of linear combinations of the rows of C are again \nlinear combinations of the rows of C. \nIf B is a given matrix and C is obtained from B by means of an ele­\nmentary row operation, then each row of C is a linear combination of the \nrows of B, and hence there is a matrix A such that AB = C. In general \nthere are many such matrices A, and among all such it is convenient and \n19 \n20 \nLinear Equations \nChap. 1 \npossible to choose one having a number of special properties. Before going \ninto this we need to introduce a class of matrices. \nDefinition. An m X n matrix is said to be an elementary matrix if \nit can be obtained from the m X m identity matrix by means of a single ele­\nmentary row operation. \nEXAMPLE 13. A 2 X 2 elementary matrix is necessarily one of the \nfollowing; \n[\u0006 Ȟ} [! \u001fJ \nc ¡ 0, [\u0006 ȟ} \nc Y 0. \nTheorem 9. Let e be an elementary row operation and let E be the \nm X m elementary matrix E = e(I). Then, for every m X n matrix A, \ne(A) \n= EA. \nProof. The point of the proof is that the entry in the ith row \nand jth column of the product matrix EA is obtained from the ith row of \nE and the jth column of A.  The three types of elementary row operations \nshould be taken up separately. We shall give a detailed proof for an oper­",
    "e(A) \n= EA. \nProof. The point of the proof is that the entry in the ith row \nand jth column of the product matrix EA is obtained from the ith row of \nE and the jth column of A.  The three types of elementary row operations \nshould be taken up separately. We shall give a detailed proof for an oper­\nation of type (ii). The other two cases are even easier to handle than this \none and will be left as exercises. Suppose r ¡ s and e is the operation \n'replacement of row r by row r plus c times row s.' Then \nTherefore, \nE \n_ {Oik' i ¡ r \nik - Ork + CO.k, i = r. \nIn other words EA \ne(A). I \nCorollary. Let A and B be m X n matrices over the field F. Then B \nis row-equivalent to A if and only if B \nP A, where P is a product of m X m \nelementary matrices. \nProof. Suppose B \nPA where P \nE. · · ·  E2El and the Ei are \nm X m elementary matrices. Then EIA is row-equivalent to A , and \nE2(ElA) is row-equivalent to EIA. So E2ElA IS row-equivalent to A ;  and \ncontinuing ill this way we see that (E •\n. . .  El)A is row-equivalent to A .  \nNow suppose that B is row-equivalent to A. Let El, E2, •\n•\n•\n , E. be \nthe elementary matrices corresponding to some sequence of elementary \nrow operations which carries A into B. Then B \n(E •\n. . .  El)A . \nI \nSec. 1.6 \n1. Let \nA = [< -\\ i} \nCompute ABC and CAB. \n2. Let \nA = [Ţ -[ :} \nVerify directly that A(AB) = A2B. \nInvertible Matrices \nExercises \nC \n[1 \n1]. \n[2 -2] \nB = !  ! .  \n3. Find two different 2 X 2 matrices A. such that A 2 \n0 but A ;;6. O. \n4. For the matrix A of .Exercise 2, find elementary matrices El, E2, \n•\n•\n•\n , Ek \nsuch that \n5. Let \nA [\\ -\\], \n1 \n0 \nIs there a matrix C such that CA = B? \n6. Let A be an m X n matrix and B an n X k matrix. Show that the columns of \nC = AB are linear combinations of the columns of A. If ai, . . . , an are the columns \nof A and '1'1, •\n•\n•\n , 'Yk are the columns of C, then \nn \n'Yi \nძ Briar. \nr = l  \n7. Let A and B be 2 X 2 matrices such that AB \nI. Prove that BA = I. \n8. Let \nC = [Cll Cu] \nC21",
    "C = AB are linear combinations of the columns of A. If ai, . . . , an are the columns \nof A and '1'1, •\n•\n•\n , 'Yk are the columns of C, then \nn \n'Yi \nძ Briar. \nr = l  \n7. Let A and B be 2 X 2 matrices such that AB \nI. Prove that BA = I. \n8. Let \nC = [Cll Cu] \nC21 \nC22 \nbe a 2 X 2 matrix. We inquire when it is possible to find 2 X 2 matrices A and B \nsuch that C \nAB - BA. Prove that such matrices can be found if and only if \nCll + C?/l, = O. \n21 \n1 .6. Invertible Matrices \nSuppose P is an m X m matrix which is a product of elementary \nmatrices. For each m X n matrix A, the matrix B \nPA is row-equivalent \nto A ;  hence A is row-equivalent to B and there is a product Q of elemen­\ntary matrices such that A = QB. In particular this is true when A is the \n22 \nLinear Equations \nChap. 1 \nm X m identity matrix. In other words, there is an m X m matrix Q, \nwhich is itself a product of elementary matrices. such that QP = I. As \nwe shall soon see, the existence of a Q with QP = I is equivalent to the \nfact that P is a product of elementary matrices. \nDefinition. Let A be an n X n (square) matrix over the field F. An \nn X n matr£x B such that BA = I 1:S called a left inverse of A; an n X n \nmatrix B such that AB = I is called a right inverse of A. If AB = BA = I, \nthen B is called a two-sided inverse of A and A is said to be invertible. \nLemma. If A has a left inverse B and a right inverse C, then B = C. \nProof. Suppose BA = I and AC =\" I. Then \nB = BI = B(AC) = (BA)C = IC = C. I \nThus if A has a left and a right inverse, A is invertible and has a \nunique two-sided inverse, which we shall denote by A -I and simply call \nthe inverse of A. \nTheorem 10. Let A and B be n X n matrices over F. \n(i) If A is invertible, so is A-I and (A \n-1) -1 \nA. \n(ii) If both A and B are invertible, so is AB, and (AB)-1 \nB-IA-I. \nProof. The first statement is evident from the symmetry of the \ndefinition. The second follows upon verification of the relations \n(AB) (B-IA -1) = (B-IA -1) (AB) \n= I.",
    "(i) If A is invertible, so is A-I and (A \n-1) -1 \nA. \n(ii) If both A and B are invertible, so is AB, and (AB)-1 \nB-IA-I. \nProof. The first statement is evident from the symmetry of the \ndefinition. The second follows upon verification of the relations \n(AB) (B-IA -1) = (B-IA -1) (AB) \n= I. \nI \nCorollary. A product of invertible matrices is invertible. \nTheorem 11. An elementary matrix is invertible. \nProof. Let E be an elementary matrix corresponding to the \nelementary row operation e. If el is the inverse operation of e (Theorem 2) \nand El = el (I), then \nand \nEdt) = el(E) = el(e(I» \n= I \nso that E is invertible and El = E-l. I \nEXAMPLE 14. \n(a) \n(b) \n[Ƞ sJI \n= [t \u0006J \n[\u0006 n-1 \n= [s -n \nSec. 1.6 \nI nvertible Matrices \n(c) \n[1 0J\n-l \n= [ 1 \nOJ \nc \n1 \n-c 1 \n(d) When c Y 0, \nI [1 ° J\n. \n° c-l \nTheorem 12. If A is an n X n matrix, the following are equivalent. \n(i) A is invertible. \n(ii) A is row-equivalent to the n X n identity matrix. \n(iii) A is a product of elementary matrices. \nProof. Let R be a row-reduced echelon matrix which IS row­\nequivalent to A. By Theorem 9 (or its corollary), \nR \nEk • •• E2EIA \nwhere EJ, . . . , Ek are elementary matrices. Each Ej is invertible, and so \nA = K1 I . . .  Ek\" IR. \nSince products of invertible matrices are invertible, we see that A is in­\nvertible if and only if R is invertible. Since R is a (square) row-reduced \nechelon matrix, R is invertible if and only if each row of R contains a \nnon-zero entry, that is, if and only if R \nI. We have now shown that A \nis invertible if and only if R \nI, and if R = I then A = Ek\" l . . .  El l. \nIt should now be apparent that (i), (ii), and (iii) are equivalent statements \nabout A. \nI \nCorollary. If A is an invertible n X n matrix and if a sequence of \nelementary row operations reduces A to the identity, then that same sequence \nof operations when applied to I yields A-I. \nCorollary. Let A and B be m X n matrices. Then B is row-equivalent \nto A if and only if B",
    "about A. \nI \nCorollary. If A is an invertible n X n matrix and if a sequence of \nelementary row operations reduces A to the identity, then that same sequence \nof operations when applied to I yields A-I. \nCorollary. Let A and B be m X n matrices. Then B is row-equivalent \nto A if and only if B \nP A where P is an invertible m X m matrix. \nTheorem 1.1. For an n X n matrix A, the following are equivalent. \n(i) A is invertible. \n(ii) The homogeneous system AX = 0 has only the trivial solution \nX \nO. \n(iii) The system of equations AX = Y has a solution X for each n X 1 \nmatrix Y. \nProof. According to Theorem 7, condition (ii) is equivalent to \nthe fact that A is row-equivalent to the identity matrix. By Theorem 12, \n(i) and (ii) are therefore equivalent. If A is invertible, the solution of \nAX = Y is X = A-IY. Conversely, suppose AX = Y has a solution for \neach given Y. Let R be a row-reduced echelon matrix which is row-\n23 \n24 \nLinear Equations \nChap. 1 \nequivalent to A. We wish to show that R = I. That amounts to showing \nthat the last row of R is not (identically) O. Let \nIf the system RX \nE can be solved for X, the last row of R cannot be O. \nWe know that R = PA, where P is invertible. Thus RX = E if and only \nif AX = P-IE. According to (iii), the latter system has a solution. I \nCorollary. A square matrix with either a left or right inverse is in­\nvertible. \nProof. Let A be an n X n matrix. Suppose A has a left inverse, \ni.e., a matrix B such that BA \nI. Then AX \n0 has only the trivial \nsolution, because X = IX = B(AX). Therefore A is invertible. On the \nother hand, suppose A has a right inverse, i.e., a matrix C such that \nAC = I. Then C has a left inverse and is therefore invertible. It then \nfollows that A = C-I and so A is invertible with inverse C. \nI \nCorollary. Let A = AIA2 . . .  Ak, where AI . .\n. , Ak are n X n (square) \nmatrices. Then A is invertible if and only if each Aj is invertible. \nProof. We have already shown that the product of two invertible",
    "follows that A = C-I and so A is invertible with inverse C. \nI \nCorollary. Let A = AIA2 . . .  Ak, where AI . .\n. , Ak are n X n (square) \nmatrices. Then A is invertible if and only if each Aj is invertible. \nProof. We have already shown that the product of two invertible \nmatrices is invertible. From this one sees easily that if each Aj is invertible \nthen A is invertible. \nSuppose now that A is invertible. We first prove that Ak is in­\nvertible. Suppose X is an n X 1 matrix and AkX = O. Then AX = \n(AI ' . .  Ak_I)AkX = O. Since A is invertible we must have X = O. The \nsystem of equations AkX \n0 thus has no non-trivial solution, so Ak is \ninvertible. But now Al . . .  Ak-l = AAk-1 is invertible. By the preceding \nargument, Ak-l is invertible. Continuing in this way, we conclude that \neach A j is invertible. I \nWe should like to make one final comment about the solution of \nlinear equations. Suppose A is an m X n matrix and we wish to solve the \nsystem of equations AX \nY. If R is a row-reduced echelon matrix which \nis row-equivalent to A ,  then R = P A where P is an rn X m invertible \nmatrix. The solutions of the system A4\" = Y are exactly the same as the \nsolutions of the system RX \nPY ( \nZ). In practice, it is not much more \ndifficult to find the matrix P than it is to row-reduce A to R. For, suppose \nwe form the augmented matrix A' of the system AX = Y, with arbitrary \nscalars YI, . . .  , Ym occurring in the last column. If we then perform on A' \na sequence of elementary row operations which leads from A to R, it will \nSec. 1 .6 \nI nvertible Matrices \nbecome evident what the matrix P is. (The reader should refer to Ex­\nample 9 where we essentially carried out this process.) In particular, if A \nis a square matrix, this process will make it clear whether or not A is \ninvertible and if A is invertible what the inverse P is. Since we have \nalready given the nucleus of one example of such a computation, we shall \ncontent ourselves with a 2 X 2 example.",
    "is a square matrix, this process will make it clear whether or not A is \ninvertible and if A is invertible what the inverse P is. Since we have \nalready given the nucleus of one example of such a computation, we shall \ncontent ourselves with a 2 X 2 example. \nEXAMPLE 15. Suppose F is the field of rational numbers and \nThen \n[r \n- 1  YI] [ȝ \n3 \n3 Y2 \n-;- 1  \nA = [r \n-!} \nY2] \nYI \n[\u0006 \n3 \n1 \n[\u0006 \n3 \nY2 ] \n-7 YI - 2Y2 \nY2 \n] (2) [\n1 \nt(2Y2 - YI) \n---t \n0 \nfrom which it is clear that A is invertible and \nA-I = [ t Ȝl \n_ l  \n7 \n0 t(Y2 + 3YI)] \n1 \nt(2Y2 \nYI) \nIt mlly seem cumbersome to continue writing the arbitrary scalars \nYI, Y2, . . .  in the computation of inverses. Some people find it less awkward \nto carry along two sequences of matrices, one describing the reduction of \nA to the identity and the other recording the effect of the same sequence \nof operations starting from the identity. The reader may judge for him­\nself which is a neater form of bookkeeping. \nEXAMPLE 16. Let us find the inverse of \n[I \n1 \n'] \n2 \n3\" \nA \nt 1 . \nI \n\".1 \n5 \n[1 \nt tJ [ \n1 \n0 n \nJ \n0 \n1 \n3 \n4 \n' \ni \nt \n0 \n0 \n[] \n1 ;J [ \n1 \n0 n \n2 \n1 \n1 \n2 \n1 \n- .1  \n0 \nT\"2 \n4 5  \n3 \n[] \n1 It:} [ \n1 \n0 ^] \n2 \n1 \n1 \n2 \n0 \n1 \n- 1  \n6 \n[š \n1 n [ \n1 \n0 IJ] \n2 \n1 \n-6 \n12 \n0 \n30 \n180 \n25 \n26 \nLinear Equations \nChap. 1 \n[ţ \n} n [ -9 60 -60J \n1 \n-țȚ 192 -180 \n0 \n-180 180 \n[\\ 0 n [ -3: -36 30J \n1 \n192 -180 . \n0 \n30 180 180 \nIt must have occurred to the reader that we have carried on a lengthy \ndiscussion of the rows of matrices and have said little about the columns. \nWe focused our attention on the rows because this seemed more natural \nfrom the point of view of linear equations. Since there is obviously nothing \nsacred about rows, the discussion in the last sections could have been \ncarried on using columns rather than rows. If one defines an elementary \ncolumn operation and column-equivalence in a manner analogous to that \nof elementary row operation and row-equivalence, it is clear that each",
    "sacred about rows, the discussion in the last sections could have been \ncarried on using columns rather than rows. If one defines an elementary \ncolumn operation and column-equivalence in a manner analogous to that \nof elementary row operation and row-equivalence, it is clear that each \nm X n matrix will be column-equivalent to a 'column-reduced echelon' \nmatrix. Also each elementary column operation will be of the form \nA -t AE, where E is an n X n elementary matrix-and so on. \nExercises \n1. Let \nA = [-111 ¢ £ ŞJ' \n-2 1 \n1 \nFind a row-reduced echelon matrix R which is row-equivalent to A and an in-\nvertible 3 X 3 matrix P such that R \nPA. \n2. Do Exercise 1, \nbut with \nA = [1 -¤ -no \n3. For each of the two matrices \n[2 \n5 -lJ \n4 -1 2 ,  \n6 4 1  [1 -1 2J \n3 2 4  \no 1 -2 \nuse elementary row operations to discover whether it is invertible, and to find the \ninverse in case it is. \n4. Let \nSec. 1.6 \nFor which X does there exist a scalar c such that AX = eX? \n5. Discover whether \nis invertible, and find A -1 if it exists. \n3 \n4] \n3 \n4 \n3 \n4 \no \n4 \nInvertible 11 atrices \n6. Suppose A is a 2 X 1 matrix and that B is a 1 X 2 matrix. Prove that C = AB \nis not invertible. \n7. Let A be an n X n (square) matrix. Prove the following two statements: \n(a) If A is invertible and AB \n0 for some n X n matrix B, then B \nO. \n(b) If A. is not invertible, then there exists an n X n matrix B such that \nAB \n0 but B ;& O. \n8. Let \nA [; =} \nProve, using elemcntary row opcrations, that A is invertible if and only if \n(ad \nbe) ;& O. \n9. An n X n matrix A is called upper-triangular if Aij \n0 for i > j, that is, \nif every entry below the main diagonal is O. Prove that an upper-triangular (square) \nmatrix is invertible if and only if every entry on its main diagonal is different \nfrom O. \n10. Prove the following generalization of Exercise 6. If A is an m X n matrix, \nB is an n X m matrix and n < m, then AB is not invertible.",
    "matrix is invertible if and only if every entry on its main diagonal is different \nfrom O. \n10. Prove the following generalization of Exercise 6. If A is an m X n matrix, \nB is an n X m matrix and n < m, then AB is not invertible. \nn. Let A be an m X n matrix. Show that by means of a finite number of elemen­\ntary row and/or column operations one can pass from it to a matrix R which \nis both 'row-reduced echelon' and 'column-reduced echelon,' i.e., Rij = 0 if i ;& j, \nRii = 1, 1 S i S  r, Rii = 0 if i > r. Show that R \nPAQ, where P is an in­\nvertible m X m matrix and Q is an invertible n X n matrix. \n12. The result of Example 16 suggests that perhaps the matrix \nA =  \n1 \n1 \n-\n2 \n1 \n1 \n2 \n3 \n1 \n1 \nn n \n1 \nn \n1 \nis invertible and A -1 has integer entries. Can you prove that? \n27 \n2. Vector Spaces \n2.1 . Vector Spaces \nIn various parts of mathematics, one is confronted with a set, such \nthat it is both meaningful and interesting to deal with 'linear combina­\ntions' of the objects in that set. For example, in our study of linear equa­\ntions we found it quite natural to consider linear combinations of the \nrows of a matrix. It is likely that the reader has studied calculus and has \ndealt there with linear combinations of functions; certainly this is so if \nhe has studied differential equations. Perhaps the reader has had some \nexperience with vectors in three-dimensional Euclidean space, and in \nparticular, with linear combinations of such vectors. \nLoosely speaking, linear algcbra is that branch of mathematics which \ntreats the common properties of algebraic systems which consist of a set, \ntogether with a reasonable notion of a 'linear combination' of elements \nin the set. In this section we shall define the mathematical object which \nexperience has shown to be the most useful abstraction of this type of \nalgebraic system. \nDefinition. A vector spacc (or linear space) consists of the following: \n1. a field F of scalars; \n2. a set V of objects, called vectors;",
    "experience has shown to be the most useful abstraction of this type of \nalgebraic system. \nDefinition. A vector spacc (or linear space) consists of the following: \n1. a field F of scalars; \n2. a set V of objects, called vectors; \n3. a rule (or operation), called vector addition, which associates with \neach pair of vectors 0;, (3 in V a vector 0; + (3 in V, called the sum of 0; and (3, \nin su,ch a way that \n(a) addition is commutative, 0; + (3 = (3 + 0;; \n(b) addition is associative, 0; + «(3 + 'Y) \n(0; + (3) + 'Y ;  \n28 \nSec. 2.1 \nVector Spaces \n(c) there is a unique vector 0 in V, called the zero vector, such that \na + 0 \na for all a in V; \n(d) for each vector a in V there is a unique vector -a in V such that \na + (- a) = 0; \n4. a rule (or operation), called scalar multiplication, which associates \nwith each scalar c in F and vector a in V a vector Ca in V, called the product \nof c and a, in such a way that \n(a) la = a for every a in V; \n(b) (clc2)a = Cl(C2a) ; \n(c) c(a + (3) \nCa + c(3; \n(d) (Cl + c2)a = CIa + C2a. \nIt is important to observe, as the definition states, that a vector \nspace is a composite object consisting of a field, a set of 'vectors,' and \ntwo operations with certain special properties. The same set of vectors \nmay be part of a number of distinct vector spaces (see Example 5 below). \nWhen there is no chance of confusion, we may simply refer to the vector \nspace as V, or when it is desirable to specify the field, we shall say V is \na vector space over the field F. The name 'vector' is applied to the \nelements of the set V largely as a matter of convenience. The origin of \nthe name is to be found in Example 1 below, but one should not attach \ntoo much significance to the name, since the variety of objects occurring \nas the vectors in V may not bear much resemblance to any preassigned \nconcept of vector which the reader has. We shall try to indicate this \nvariety by a list of examples; our list will be enlarged considerably as we",
    "too much significance to the name, since the variety of objects occurring \nas the vectors in V may not bear much resemblance to any preassigned \nconcept of vector which the reader has. We shall try to indicate this \nvariety by a list of examples; our list will be enlarged considerably as we \nbegin to study vector spaces. \nEXAMPLE 1 .  The n-tuple space, Fn. Let F be any field, and let V be \nthe set of all n-tuples a \n= (Xl, X2, . . .  , xn) of scalars Xi in F. If (3 \n= \n(Yl, Y2, . . . , Yn) with Yi in F, the sum of a and (3 is defined by \n(2-1) \nThe product of a scalar c and vector a is defined by \n(2-2) \nCa \nThe fact that this vector addition and scalar multiplication satisfy con­\nditions (3) and (4) is easy to verify, using the similar properties of addi­\ntion and multiplication of elements of F. \nEXAMPLE 2. The space of m X n m'ltrices, FmXn. Let F be any \nfield and let m and n be positive integers. Let FmXn be the set of all m X n \nmatrices over the field F. The sum of two vectors A and B in FmXn is de­\nfined by \n(2-3) \n29 \n30 \nVector Spaces \nThe product of a scalar c and the matrix A is defined by \n(2-4) \n(cA),j = cA,j' \nNote that FiX\" = Fn. \nOhap. 2 \nEXAMPLE 3. The space of functions from a set to a field. Let F be \nany field and let S be any non-empty set. Let V be the set of all functions \nfrom the set S into F. The sum of two vectors f and g in V is the vector \nf + g, i.e., the function from S into F, defined by \n(2-5) \n(f + g) (s) = f(s) + g(s). \nThe product of the scalar c and the function f is the function cf defined by \n(2-6) \n(cf) (s) = cf(s). \nThe preceding examples are special cases of this one. For an n-tuple of \nelements of F may be regarded as a function from the set S of integers \n1, . . .  , n into F. Similarly, an m X n matrix over the field F is a function \nfrom the set S of pairs of integers, (i, j), 1 ᄄ i \u001a m, 1 ᄄ j ᄄ n, into the \nfield F. For this third example we shall indicate how one verifies that the",
    "1, . . .  , n into F. Similarly, an m X n matrix over the field F is a function \nfrom the set S of pairs of integers, (i, j), 1 ᄄ i \u001a m, 1 ᄄ j ᄄ n, into the \nfield F. For this third example we shall indicate how one verifies that the \noperations we have defined satisfy conditions (3) and (4). For vector \naddition: \n(a) Since addition in F is commutative, \nf(s) + g(s) = g(s) + f(s) \nfor each s in S, so the functions f + g and g + f are identical. \n(b) Since addition in F is associative, \nf(s) + [g(s) + h(s)] \n[f(s) + g(s)] + h(s) \nfor each s, so f + (g + h) is the same function as (f + g) + h. \n(c) The unique zero vector is the zero function which assigns to each \nelement of S the scalar 0 in F. \n(d) For each f in V, (-f) is the function which is given by \n( -f) (s) \n-f(s). \nThe reader should find it easy to verify that scalar multiplication \nsatisfies the conditions of (4), by arguing as we did with the vector addition. \nEXAMPLE 4. The space of polynomial functions over a field F. \nLet F be a field and let V be the set of all functions f from F into F which \nhave a rule of the form \n(2-7) \nf(x) \nCo + CiX + . . .  + c\"xn \nwhere Co, Cl, •\n•\n•\n , Cn are fixed scalars in F (independent of x). A func­\ntion of this type is called a polynomial function on F. Let addition \nand scalar multiplication be defined as in Example 3. One must observe \nhere that if j and g are polynomial functions and C is in F, then j + g and \ncf are again polynomial functions. \nSec. 2.1 \nVector Spaces \nEXAMPLE 5. The field C of complex numbers may be regarded as a \nvector space over the field It of real numbers. More generally, let P be the \nfield of real numbers and let V be the set of n-tuples a \n(Xl, . . . , Xn) \nwhere Xl> \n•\n•\n•\n , Xn are complex numbers. Define addition of vectors and \nscalar multiplication by (2-1) and (2-2), as in Example 1. In this way we \nobtain a vector space over the field R which is quite different from the \nspace Cn and the space Rn.",
    "(Xl, . . . , Xn) \nwhere Xl> \n•\n•\n•\n , Xn are complex numbers. Define addition of vectors and \nscalar multiplication by (2-1) and (2-2), as in Example 1. In this way we \nobtain a vector space over the field R which is quite different from the \nspace Cn and the space Rn. \nThere are a few simple facts which follow almost immediately from \nthe definition of a vector space, and we proceed to derive these. If c is \na scalar and 0 is the zero vector, then by 3(c) and 4(c) \ncO = c(O + 0) = cO + cO. \nAdding - (cO) and using 3(d), we obtain \n(2-8) \ncO = O. \nSimilarly, for the scalar 0 and any vector a we find that \n(2-9) \nOa = O. \nIf c is a non-zero scalar and a is a vector such that Ca \n0, then by (2-8), \nc-1(ca) \nO. But \nhence, a \nO. Thus we see that if c is a scalar and a a vector such that \nca = 0, then either c is the zero scalar or a is the zero vector. \nIf a is any vector in V, then \no = Oa = (1 \nfrom which it follows that \n(2-10) \nl)a = la + ( - l)a = a + ( - l)a \nl)a = \na. \nFinally, the associative and commutative properties of vector addition \nimply that a sum involving a number of vectors is independent of the way \nin which these veetors are combined and associated. For example, if \nai, a2, as, a4 are vectors in V, then \nand such a sum may be written without confusion as \nal + a2 + as + a4. \nDefinition. A vector (J in V is said to be a linear combination of the \nvectors ai, •\n•\n•\n , an in V provided there exist scalars Cl, . . .  , Cn in F such that \n(J = Cial + . . . + cnan \nn \n= ::t; Ciaj. \n; = 1  \n31 \n32 \nVector Spaces \nChap. 2 \nOther extensions of the associative property of vector addition and \nthe distributive properties 4(c) and 4(d) of scalar multiplication apply \nto linear combinations: \nn \nn \n: Ciai + : d,a, \ni = 1  \ni = 1  \nn \n: (C, + di)ai \n; = 1  \nn \nn \nC : Ciai \n: (CCi)ai. \ni = 1  \n. = 1  \nCertain parts of linear algebra are intimately related to geometry. \nThe very word 'space' suggests something geometrical, as does the word",
    "to linear combinations: \nn \nn \n: Ciai + : d,a, \ni = 1  \ni = 1  \nn \n: (C, + di)ai \n; = 1  \nn \nn \nC : Ciai \n: (CCi)ai. \ni = 1  \n. = 1  \nCertain parts of linear algebra are intimately related to geometry. \nThe very word 'space' suggests something geometrical, as does the word \n'vector' to most people. As we proceed with our study of vector spaces, \nthe reader will observe that much of the terminology has a geometrical \nconnotation. Before concluding this introductory section on vector spaces, \nwe shall consider the relation of vector spaces to geometry to an extent \nwhich will at least indicate the origin of the name 'vector space.' This \nwill be a brief intuitive discussion. \nLet us consider the vector space [la. In analytic geometry, one iden­\ntifies triples (Xl, X2, Xa) of real numbers with the points in three-dimensional \nEuclidean space. In that context, a vector is usually defined as a directed \nline segment PQ, from a point P in the space to another point Q. This \namounts to a careful formulation of the idea of the 'arrow' from P to Q. \nAs vectors are used, it is intended that they should be determined by \ntheir length and direction. Thus one must identify two directed line seg­\nments if they have the same length and the same direction. \nThe directed line segment PQ, from the point P = (Xl, X2, X3) to the \npoint Q = (YI, Y2, Y3), has the same length and direction as the directed \nline segment from the origin 0 = (0, 0, 0) to the point (YI \nXl, Y2 - X2, \nY3 \nX3). Furthermore, this is the only segment emanating from the origin \nwhich has the same length and direction as PQ. Thus, if one agrees to \ntreat only vectors which emanate from the origin, there is exactly one \nvector associated with each given length and direction. \nThe vector OP, from the origin to P = (x!, X2, X3), is completely de­\ntermined by P, and it is therefore possible to identify this.vector with the \npoint P. In our definition of the vector space R3, the vectors are simply",
    "vector associated with each given length and direction. \nThe vector OP, from the origin to P = (x!, X2, X3), is completely de­\ntermined by P, and it is therefore possible to identify this.vector with the \npoint P. In our definition of the vector space R3, the vectors are simply \ndefined to be the triples (Xl, X2, X3). \nGiven points P = (Xl, X2, X3) and Q = (Y], Y2, Y3), the definition of \nthe sum of the vectors OP and OQ can be given geometrically. If the \nvectors are not parallel, then the segments OP and OQ determine a plane \nand these segments are two of the edges of a parallelogram in that plane \n(see Figure 1). One diagonal of this parallelogram extends from 0 to a \npoint S, and the sum of OP and OQ is defined to be the vector OS. The \ncoordinates of the point S are (Xl + YI, X2 + Y2, X3 + Y3) and hence this \ngeometrical definition of vector addition is equivalent to the algebraic \ndefinition of Example 1. \nSec. 2.1 \nVector Spaces \nFIGURE 1 \nScalar multiplication has a simpler geometric interpretation. If c is \na real number, then the product of e and the vector OP is the vector from \nthe origin with length lei times the length of OP and a direction which \nagrees with the direction of OP if e > 0, and which is opposite to the \ndirection of OP if c < 0. This scalar multiplication just yields the vector \nOT where T \n(exJ, eX2, eX3), and is therefore consistent with the algebraic \ndefinition given for R3. \nFrom time to time, the reader will probably find it helpful to 'think \ngeometrically' about vector spaces, that is, to draw pictures for his own \nbenefit to illustrate and motivate some of the ideas. Indeed, he should do \nthis. However, in forming such illustrations he must bear in mind that, \nbecause we are dealing with vector spaces as algebraic systems, all proofs \nwe give will be of an algebraic nature. \nExercises \n1. If F is a field, verify that Fn (as defined in Example 1) is a vector space over \nthe field F.",
    "this. However, in forming such illustrations he must bear in mind that, \nbecause we are dealing with vector spaces as algebraic systems, all proofs \nwe give will be of an algebraic nature. \nExercises \n1. If F is a field, verify that Fn (as defined in Example 1) is a vector space over \nthe field F. \n2. If V is a vector space over the field F, verify that \n(aJ + a2) + (a3 + a4) \n[a2 + (a3 + al)J + a4 \nfor all vectors aI, a2, a3, and a4 in V. \n3. If C is the field of complex numbers, which vectors in C3 are linear combina­\ntions of (1, 0, - 1), (0, I, 1), and (1, 1, I)? \n33 \n34 \nVector Spaces \nChap. 2 \n4. Let V be the set of all pairs (x, y) of real numbers, and let F be the field of \nreal numbers. Define \n(x, y) + (Xl, YI) = (X + Xl, Y + Yl) \nc(x, y) = (CX, y). \nIs V, with these operations, a vector space over the field of real numbers? \n5. On Rn, define two operations \na ffi{3 = a - {3 \nc ·  a = -ca. \nThe operations on the right are the usual ones. Which of the axioms for a vector \nspace are satisfied by (Rn, ffi, . )? \n6. Let V be the set of all complex-valued functions f on the real line such that \n(for all t in R) \nf( -t) = f(t). \nThe bar denotes complex conjugation. Show that V, with the operations \n(f + g) (t) = f(t) + get) \n(cf)(t) = cf(t) \nis a vector space over the field of real numbers. Give an example of a function in V \nwhich is not real-valued. \n7. Let V be the set of pairs (x, y) of real numbers and let F be the field of real \nnumbers. Define \n(X, y) + (Xl, Yl) = (X + Xl, 0) \nc(x, y) = (ex, 0). \nIs V, with these operations, a vector space? \n2.2. Subspaces \nIn this section we shall introduce some of the basic concepts in the \nstudy of vector spaces. \nDefinition. Let V be a vector space over the field F. A subspace of V \nis a subset W of V which is itself a vector space over F with the operations of \nvector addition and scalar multiplication on V. \nA direct check of the axioms for a vector space shows that the subset",
    "study of vector spaces. \nDefinition. Let V be a vector space over the field F. A subspace of V \nis a subset W of V which is itself a vector space over F with the operations of \nvector addition and scalar multiplication on V. \nA direct check of the axioms for a vector space shows that the subset \nW of V is a subspace if for each a and {3 in W the vector a + {3 is again \nin W; the 0 vector is in W; for each a in W the vector (- a) is in W; for \neach a in W and each scalar c the vector Ca is in W. The commutativity \nand associativity of vector addition, and the properties (4) (a), (b), (c), \nand (d) of scalar multiplication do not need to be checked, since these \nare properties of the operations on V. One can simplify things still further. \nSec. 2.2 \nSubspaces \nTheorem 1 .  A non-empty subset W of V is a subspace of V if and only \nif for each pair of vectors a, (3 in W and each scalar c in F the vector Co' + (3 \nis again in W. \nProof. Suppose that W is a non-empty subset of V such that \nCo' + (3 belongs to W for all vectors a, (3 in W and all scalars c in F. Since \nW is non-empty, there is a vector p in W, and hence (- l)p + p = 0 is \nin W. Then if a is any vector in W and c any scalar, the vector Co' = Co' + 0 \nis in W. In particular, ( - 1)0' \na is in W. Finally, if a and (3 are in W, \nthen 0' +  (3 = 10' + (3 is in W. Thus W is a subspace of V. \nConversely, if W is a subspace of V, a and (3 are in W, and c is a scalar, \ncertainly Co' + (3 is in W. I \nSome people prefer to use the Co' + (3 property in Theorem 1 as the \ndefinition of a subspace. It makes little difference. The important point \nis that, if W is a non-empty subset of V such that Co' + (3 is in V for all a, \n(3 in W and all c in F, then (with the operations inherited from V) W is a \nvector space. This provides us with many new examples of vector spaces. \nEXAMPLE 6. \n(a) If V is any vector space, V is a subspace of V ;  the subset con­",
    "(3 in W and all c in F, then (with the operations inherited from V) W is a \nvector space. This provides us with many new examples of vector spaces. \nEXAMPLE 6. \n(a) If V is any vector space, V is a subspace of V ;  the subset con­\nsisting of the zero vector alone is a subspace of V, called the zero sub­\nspace of V. \n(b) In Fn, the set of n-tuples (Xl, . . .  , xn) with XI = 0 is a subspace; \nhowever, the set of n-tuples with Xl \n1 + X2 is not a subspace (n s 2). \n(c) The space of polynomial functions over the field F is a subspace \nof the space of all functions from F into F. \n(d) An n X n (square) matrix A over the field Ii' is symmetric if \nAij \nAj,: for each i and i The symmetrie matrices form a subspace of \nthe space of all n X n matrices over F. \n(e) An n X n (square) matrix A over the field C of complex num­\nbers is Herllitian (or self-adjoint) if \nAjk = Akj \nfor each j, k, the bar denoting complex conjugation. A 2 X 2 matrix is \nHermitian if and only if it has the form \n[ \nz\n.\n x + iY] \nx - zy \nw \nwhere x, y, Z, and w are real numbers. The set of all Hermitian matrices \nis not a subspace of the space of all n X n matrices over C. For if A is \nHermitian, its diagonal entries An, A22) •\n•\n•\n , are all real numbers, but the \ndiagonal entries of iA are in general not real. On the other hand, it is easily \nverified that the set of n X n complex Hermitian matrices is a vector \nspace over the field R of real numbers (with the usual operations). \n35 \n36 \nVector Spaces \nChap. 2 \nEXAMPLE 7. The solution space of a system of homogeneous \nlinear equations. Let A be an m X n matrix over F. Then the set of all \nn X 1 (column) matrices X over F such that AX = 0 is a subspace of the \nspace of all n X 1 matrices over F. To prove this we must show that \nA (cX + Y) = ° when AX = 0, A Y = 0, and c is an arbitrary scalar in F. \nThis follows immediately from the following general fact. \nLemma. If A is an m X n matrix over F and B, C are n X p matrices \nover F then \n(2-11)",
    "space of all n X 1 matrices over F. To prove this we must show that \nA (cX + Y) = ° when AX = 0, A Y = 0, and c is an arbitrary scalar in F. \nThis follows immediately from the following general fact. \nLemma. If A is an m X n matrix over F and B, C are n X p matrices \nover F then \n(2-11) \nA(dB + C) = d(AB) + AC \nfa?' each scalar d 1:n F. \nProof. [A (dB + C)]ii = }; Aik(dB + Chi \nk \n= }; (dAikBki + AikCd \nk \nd };  AikBkj + }; AikCkj \nk \nIe \nd(AB);j + (AC);i \n[d(AB) + AC]ij. I \nSimilarly one can show that (dB + C)A = d(BA) + CA, if the \nmatrix sums and products are defined. \nTheorem 2. Let V be a vector space over the field F. The intersection \nof any collection of subspaces of V is a subspace of V. \nProof. Let {Wa} be a collection of subspaces of V, and let W \nn Wa be their intersection. Recall that W is defined as the set of all ele-\na \nments belonging to every Wa (see Appendix). Since each Wa is a subspace, \neach contains the zero vector. Thus the zero vector is in the intersection \nW, and W is non-empty. Let a and (3 be vectors in W and let c be a scalar. \nBy definition of W, both a and (3 belong to each Wa, and because each Wa \nis a subspace, the vector (ca + (3) is in every Wa. Thus (ca + (3) is again \nin W. By Theorem 1, W is a subspace of V. I \nFrom Theorem 2 it follows that if S is any collection of vectors in V, \nthen there is a smallest subspace of V which contains S, that is, a sub­\nspace which contains S and which is contained in every other subspace \ncontaining S. \nDefinition. Let S be a set of vectors in a vector space V. The subspace \nspanned by S is defined to be the intersection W of all subspaces of V which \ncontain S. When S is a finite set of vectors, S = {al' a2, . . .  , an} , we shall \nsimply call W the subspace spanned by the vectors al, a2, . . .  , an. \nSec. 2.2 \nSubspaces \nTheorem 3. The subspace spanned by a non-empty subset 8 of a vector \nspace V is the set of all linear combinations of vectors in 8.",
    "simply call W the subspace spanned by the vectors al, a2, . . .  , an. \nSec. 2.2 \nSubspaces \nTheorem 3. The subspace spanned by a non-empty subset 8 of a vector \nspace V is the set of all linear combinations of vectors in 8. \nProof. Let W be the subspace spanned by S. Then each linear \ncombination \na \nXlal + X2a2 + . . . + Xmam \nof vectors aI, a2, .\n.\n•\n , am in S is clearly in W. Thus W contains the set L \nof all linear combinations of vectors in S. The set L, on the other hand, \ncontains S and is non-empty. If a, (3 belong to L then a is a linear \ncombination, \na\nXIal + X2a2 + . . . + Xmam \nof vectors ai in S, and (3 is a linear combination, \n(3 = Yl(31 + Y2(32 + . .  . \nYn(3n \nof vectors (3j in S. For each scalar c, \nm \n\" \nCa + (3 = z (cxi)ai + z Yj(3j. \ni - I  \nj- l \nHence Ca + (3 belongs to L. Thus L is a subspace of V. \nNow we have shown that L is a subspace of V which contains S, and \nalso that any subspace which contains S contains L. It follows that L is \nthe intersection of all subspaces containing S, i.e., that L is the subspace \nspanned by the set S. I \nDefinition. If 81, 82, •\n•\n•\n , 8k are subsets of a vector space V, the set of \nall sums \nal + a2 + . . . + ak \nof vectors aj in Sj is called the SUIn of the subsets SI, S2, . . .  , Sk and is de­\nnoted by \nor by \nk \nz Sj. \ni - 1  \nIf WI, W2, •\n•\n•\n , Wk are subspaces of V, then the sum \nis easily seen to be a subspace of V which contains each of the subspaces \nWi. From this it follows, as in the proof of Theorem 3, that W is the sub­\nspace spanried by the union of WI, W2, •\n•\n•\n , Wk. \nEXAMPLE 8. Let F be a sub field of the field C of complex numbers. \nSuppose \n37 \n38 \nVector Spaces \nal \n(1, 2, 0, 3, 0) \na2 = (0, 0, 1, 4, 0) \nas \n(0, 0, 0, 0, 1). \nChap. 2 \nBy Theorem 3, a vector a is in the subspace W of Fa spanned by aI, a2, as \nif and only if there exist scalars Cl, C2, C3 in F such that \na = Clal + C2a2 + C3ag. \nThus W consists of all vectors of the form \na = (Cl' 2cr, C2, 3CI + 4C2, C3)",
    "a2 = (0, 0, 1, 4, 0) \nas \n(0, 0, 0, 0, 1). \nChap. 2 \nBy Theorem 3, a vector a is in the subspace W of Fa spanned by aI, a2, as \nif and only if there exist scalars Cl, C2, C3 in F such that \na = Clal + C2a2 + C3ag. \nThus W consists of all vectors of the form \na = (Cl' 2cr, C2, 3CI + 4C2, C3) \nwhere Cl, C2, C3 are arbitrary scalars in F. Alternatively, W can be described \nas the set of all 5-tuples \nwith Xi in F such that \nX2 = 2Xl \nX4 = 3Xl + 4xa. \nThus (-3, -6, 1, \n5, 2) is in W, whereas (2, 4, 6, 7, 8) is not. \nEXAMPLE 9. Let F be a subfield of the field C of complex numbers, \nand let V be the vector space of all 2 X 2 matrices over F. Let WI be the \nsubset of V consisting of all matrices of the form \nwhere X, y, z are arbitrary scalars in F. Finally, let W2 be the subset of V \nconsisting of all matrices of the form \n[Ȋ ȋJ \nwhere X and y are arbitrary scalars in F. Then WI and W2 are subspaces \nof V. Also \nbecause \nThe subspace WI n W2 consists of all matrices of the form \nEXAMPLE 10. Let A be an m X n matrix over a field F. The row \nvectors of A are the vectors in Fn given by ai = (A iI, •\n•\n•\n , A in), i = 1, . . . , \nm. The subspace of Fn spanned by the row vectors of A is called the row \nSec. 2.2 \nSubspaces \nspace of A. The subspace considered in Example 8 is the row space of the \nmatrix \n[1 2 0 3 \nA =  0 0 1 4  \n0 0 0  \n° \nIt is also the row space of the matrix \nB = [ \\ -4 2 0 \no 1 \n° 0 \n-8 1 \n3 0] \n4 0 \no 1 . \n-8 ° \nEXAMPLE 11. Let V be the space of all polynomial functions over F. \nLet S be the subset of V consisting of the polynomial functions fo, fl' 12, . . .  \ndefined by \nn = 0, 1,2, .... \nThen V is the subspace spanned by the set S. \nExercises \n1. Which of the following sets of vectors a \n(aI, . . .  , an) in Rn are subspaces \nof Rn (n 9 3)? \n(a) all a such that al 9 0; \n(b) all a such that al + 3a2 = a3; \n(c) all a such that a2 \nai; \n(d) all a such that ala2 = 0; \n(e) all a such that a2 is rational.",
    "Exercises \n1. Which of the following sets of vectors a \n(aI, . . .  , an) in Rn are subspaces \nof Rn (n 9 3)? \n(a) all a such that al 9 0; \n(b) all a such that al + 3a2 = a3; \n(c) all a such that a2 \nai; \n(d) all a such that ala2 = 0; \n(e) all a such that a2 is rational. \n2. Let V be the (real) vector space of all functions 1 from R into R. Which of the \nfollowing sets of functions are subspaces of V? \n(a) all f such that f(x2) = l(x)2; \n(b) all 1 such that fCO) = f(I) ; \n(c) all 1 such that 1(3) \n1 + 1( -5) ;  \n(d) all f such that f(- l) = 0; \n(e) all 1 which are continuous. \n3. Is the vector (3, - 1, 0, - 1) in the subspace of R5 spanned by the vectors \n(2, -1, 3, 2), ( - 1, 1, 1, -3), and (1, 1, 9, -5)? \n4. Let W be the set of all (Xl, X2, X3, X4, x.) in R5 which satisfy \no \nX5 \n0 \n9XI - 3X2 + 6xa - 3X4 - 3xs = O. \nFind a finite set of vectors which spans W. \n39 \n40 \nVector Spaces \nChap. 2 \n5. Let F be a field and let n be a positive integer (n ;:: 2). Let V be the vector \nspace of all n X n matrices over P. Which of the following sets of matrices A in V \nare subspaces of V? \n(a) all invertible A ;  \n(b) all non-invertible A ;  \n(c) all A such that A B  = BA, where B is some fixed matrix in V; \n(d) all A such that A 2 \nA .  \n6. (a) Prove that the only subspaces of RI are RI and the zero subspace. \n(b) Prove that a subspace of R2 is R2, or the zero subspace, or consists of all \nscalar multiples of some fixed vector in R2. (Thfl last type of subspace is, intuitively, \na straight line through the origin.) \n(c) Can you describe the subspaces of R3? \n7. Let WI and Wz be subspaces of a vector space V such that the set-theoretic \nunion of WI and Wz is also a subspace. Prove that one of the spaces Wi is contained \nin the other. \n8. Let V be the vector space of all functions from R into R; let V. be the \nsubset of even functions, f( -x) = f(x) ;  let Vo be the subset of odd functions, \nf( -x) \n-f(x). \n(a) Prove that V. and Vo are subspaces of V.",
    "in the other. \n8. Let V be the vector space of all functions from R into R; let V. be the \nsubset of even functions, f( -x) = f(x) ;  let Vo be the subset of odd functions, \nf( -x) \n-f(x). \n(a) Prove that V. and Vo are subspaces of V. \n(b) Prove that V, + Vo = V. \n(c) Prove that Vd n Vo = {O} . \n9. Let WI and H'z be subspaces of a vee tor space V such that WI + Wz = V \nand WI n W2 \n{O} . Prove that for each vector a in V there are 'Unique vectors \nal in WI and a2 in W2 such that a = al + a2. \n2.3. Bases and Dimension \nWe turn now to the task of assigning a dimension to certain vector \nspaces. Although we usually associate 'dimension' with something geomet­\nrical, we must find a suitable algebraic definition of the dimension of a \nvector space. This will be done through the concept of a basis for the space. \nDefinition. Let V be a vector space over F. A subset S of V is said to \nbe linearly dependent (01' simply, dependent) if there exist distinct vectors \naJ, a2, . . .  , an in S and scalars C1, C2, \n•\n.\n.\n , Cn in F, not all oj which are 0, \nsuch that \nA set which is not linearly dependent is called linearly independent. If \nthe set S contains only finitely ma:ny vectors al, a2, . . .  , an, we sometimes say \nthat al, a2, . . . , an are dependent (or' independent) instead of saying S is \ndependent (or independent). \nSec. 2.3 \nBases and Dimension \nThe following are easy consequences of the definition. \n1. Any set which contains a linearly dependent set is linearly de­\npendent. \n2. Any subset of a linearly independent set is linearly independent. \n3. Any set which contains the ° vector is linearly dependent; for \n1 · 0  = O. \n4. A set 8 of vectors is linearly independent if and only if each finite \nsubset of S is linearly independent, i.e., if and only if for any distinct \nvectors ai, .\n.\n.\n , an of 8, Clal + . . . + Cna\" = ° implies each Ci = O. \nDefinition. Let V be a vector space. A basis for V is a linearly inde­",
    "4. A set 8 of vectors is linearly independent if and only if each finite \nsubset of S is linearly independent, i.e., if and only if for any distinct \nvectors ai, .\n.\n.\n , an of 8, Clal + . . . + Cna\" = ° implies each Ci = O. \nDefinition. Let V be a vector space. A basis for V is a linearly inde­\npendent set of vectors in V which spans the space V. The space V is finite­\ndimensional if it has a fim:te basis. \nEXAMPLE 12. Let F be a subfield of the complex numbers. In Fa the \nvectors \nal = ( 3, 0, -3) \na2 = (- 1, 1, \n2) \nas \n(\n4, 2, -2) \na4 = ( 2, 1, \n1) \nare linearly dependent, since \n2al + 2az - aa + 0 . a4 = O. \nThe vectors \n€1 \n(1, 0, 0) \nE2 \n(0, 1, 0) \nfa = (0, 0, 1) \nare linearly independent \nEXAMPLE 13. Let F be a field and in Fn let S be the subset consisting \nof the vectors EI, E2, \n•\n•\n•\n , En defined by \nEI \n= (1, 0, 0, . . .  , 0) \nE2 \n(0, 1, 0, . . .  , 0) \nEn = (0, 0, 0, . . .  , 1). \nLet Xl, X2, •\n•\n• , X n  be scalars in F and put a = XIEI + X2€2 + . . .  + XnE\". \nThen \n(2-12) \na \nThis shows that EI, \n•\n•\n•\n , En span Fn. Since a \n0 if and only if Xl \nX2 = . . . = Xn \n0, the vectors EJ, •\n•\n•\n , En are linearly independent. The \nset 8 = {EI' \n.\n•\n•\n , En} is accordingly a basis for Fn. We shall call this par­\nticular basis the standard basis of FlO. \n4-1 \nVector Spaces \nChap. 2 \nEXAMPLE 14. Let P be an invertible n X n matrix with entries in \nthe field F. Then PI, . . .  , P fl '  the columns of P, form a basis for the space \nof column matrices, FnXI. We see that as follows. If X is a column matrix, \nthen \nSince PX = ° has only the trivial solution X = 0, it follows that \n{PI, . . .  , Pn} is a linearly independent set. Why does it span FnXI? Let Y \nbe any column matrix. If X = P-IY, then Y = PX, that is, \nY = XIPI + . . .  + xnP n. \nSo {PI, . . . , Pn} is a basis for FnXI. \nEXAMPI,E 15. Let A be an m X n matrix and let S be the solution \nspace for the homogeneous system AX \n0 (Example 7). Let R be a row­",
    "be any column matrix. If X = P-IY, then Y = PX, that is, \nY = XIPI + . . .  + xnP n. \nSo {PI, . . . , Pn} is a basis for FnXI. \nEXAMPI,E 15. Let A be an m X n matrix and let S be the solution \nspace for the homogeneous system AX \n0 (Example 7). Let R be a row­\nreduced echelon matrix which is row-equivalent to A. Then S is also the \nsolution space for the system RX = O. If R has r non-zero rows, then the \nsystem of equations RX = ° simply expresses r of the unknowns XI, .\n.\n•\n , Xn \nin terms of the remaining (n - r) unknowns Xj. Suppose that the leading \nnon-zero entries of the non-zero rows occur in columns kl' . . .  , kr• Let J \nbe the set consisting of the n - r indices different from kl' . . .  , kr: \nJ = {I, . . .  , n} \n- {kl, • • •  , kr} . \nThe system RX = 0 has the form \nXkl + z CljXj = 0 \nJ \nwhere the Gij are certain scalars. All solutions are obtained by assigning \n(arbitrary) values to those x/s with j in J and computing the correspond­\ning values of Xk\" •\n•\n.\n , Xk,. For each j in J, let Ej be the solution obtained \nby setting Xj \n1 and Xi \n0 for all other i in J. We assert that the en \n1') \nvectors Ej, j in J, form a basis for the solution space. \nSince the column matrix Ej has a 1 in row j and zeros in the rows \nindexed by other elements of J, the reasoning of Example 13 shows us \nthat the set of these vectors is linearly independent. That set spans the \nsolution space, for this reason. If the column matrix T, with entries \ntl, •\n•\n•\n , tn, is in the solution space, the matrix \nN = z tjEj \nJ \nis also in the solution space and is a solution such that Xi = tf for each \nj in J. The solution with that property is unique; hence, N = T and T is \nin the span of the vectors Ej• \nSec. 2.3 \nBases and Dimension \nEXAMPLE 16. We shall now give an example of an infinite basis. Let \nF be a sub field of the complex numbers and let V be the space of poly­\nnomial functions over F. Itecall that these functions are the functions",
    "in the span of the vectors Ej• \nSec. 2.3 \nBases and Dimension \nEXAMPLE 16. We shall now give an example of an infinite basis. Let \nF be a sub field of the complex numbers and let V be the space of poly­\nnomial functions over F. Itecall that these functions are the functions \nfrom F into F which have a rule of the form \nf(x) \n== Co + CIX + . . . + cnxn. \nLet fk(X) = Xk, k = 0, 1, 2, . . . . The (infinite) set {fo, iI, f2' . . .  } is a basis \nfor V. Clearly the set spans V, because the function f (above) is \nf = cofo + Cdi \ncn/n• \nThe reader should see that this is virtually a repetition of the definition \nof polynomial function, that is, a function f from F into F is a polynomial \nfunction if and only if there exists an integer n and scalars Co, . . . , Cn such \nthat f = cofo + . . .  + cnfn. Why are the functions independent? To show \nthat the set {fO' !I, f2' . . .  } is independent means to show that each finite \nsubset of it is independent. It will suffice to show that, for each n, the set \n{fo, . . . , fn} is independent. Suppose that \nThis says that \ncolo + . . . + cJn = 0. \nCo \nCIX + . . .  + cnxn = ° \nfor every x in F; in other words, every x in F is a root of the polynomial \nf(x) \nCo + CIX + \n. . .  + CnJ`n. We assume that the reader knows that a \npolynomial of degree n with complex coefficients cannot have more than n \ndistinct roots. It follows that Co = c] = . . .  = Cn = O. \nWe have exhibited an infinite basis for V. Does that mean that V is \nnot finite-dimensional? As It matter of fact it does; however, that is not \nimmediate from the definition, because for all we know V might also have \nIt finite basis. That possibility is easily eliminated. (We shall eliminate it \nin general in the next theorem.) Suppose that we have a finite number of \npolynomial functions gl, . . . , gr. There will be a largest power of x which \nappears (with non-zero coefficient) in gl(X), . . .  , gr(X). If that power is k, \nclearly fk+l(X)",
    "in general in the next theorem.) Suppose that we have a finite number of \npolynomial functions gl, . . . , gr. There will be a largest power of x which \nappears (with non-zero coefficient) in gl(X), . . .  , gr(X). If that power is k, \nclearly fk+l(X) \nXk+1 is not in the linear span of gl, . . .  , gr. So V is not \nfinite-dimensional. \nA final remark about this example is in order. Infinite bases have \nnothing to do with 'infinite linear combinations.' The reader who feels an \nirresistible urge to inject power series \ninto this example should study the example carefully again. If that does \nnot effect a cure, he should consider restricting his attention to finite­\ndimensional spaces from now on. \n44 \nVector Spaces \nOhap. 2 \nTheorem 4. Let V be a vector space which is spanned by a finite set of \nvectors {31, {32, . . . , 13m. Then any independent set of vectors in V is finite and \ncontains no more than m elements. \nProof. To prove the theorem it suffices to show that every subset \nS of V which contains more than m vectors is linearly dependent. Let S be \nsuch a set. In S there are distinct vectors al, a2, . . .  , an where n > m. \nSince 131, . . . , 13m span V, there exist scalars Aij in F such that \nm \naj \nz Aij/3i• \ni - I  \nFor any n scalars Xl, X2, •\n.\n.\n , X\" we have \nn \nz Xjaj \ni - l \nn \nm \n= z Xj z AdJ; \nj = 1  \ni = 1  \n= 0 (i; AijXi)f3i. \ni = 1 \nj = 1  \nSince n > m, Theorem 6 of Chapter 1 implies that there exist scalars \nXl, X2, •\n•\n•\n , Xn not all 0 such that \nn \n: A ijXj \n0, \n1 S i S m. \ni = 1  \nHence Xlt'X1 + X2a2 + . . .  + Xnt'X\" = O. This shows that S IS a linearly \ndependent set. I \nCorollary 1. If V is a finite-dimensional vector space, then any two \nbases of V have the same (finite) number of elements. \nProof. Since V is finite-dimensional, it has a finite basis \n{{31, f32, . . . , f3m} . \nBy Theorem 4 every basis of V is finite and contains no more than m \nelements. Thus if {at, a2, . . .  , an} is a basis, n S m. By the same argu­",
    "bases of V have the same (finite) number of elements. \nProof. Since V is finite-dimensional, it has a finite basis \n{{31, f32, . . . , f3m} . \nBy Theorem 4 every basis of V is finite and contains no more than m \nelements. Thus if {at, a2, . . .  , an} is a basis, n S m. By the same argu­\nment, m S n. Hence m = n. I \nThis corollary allows us to define the dimension of a finite-dimensional \n. vector space as the number of elements in a basis for V. We shall denote \nthe dimension of a finite-dimensional space V by dim V. This allows us \nto reformulate Theorem 4 as follows. \nCorollary 2. Let V be a finite-dimensional vector space and let n \ndim V. Then \nSec. 2.3 \nBases and Dimension \n(a) any subset of V which contains more than n vectors is linearly \ndependent; \n(b) no subset of V which contains fewer than n vectors can span V. \nEXAMPLE 17. If F is a field, the dimension of Fn is n, because the \nstandard basis for Fn contains n vectors. The matrix space FmXn has \ndimension mn. That should be clear by analogy with the case of Fn, be­\ncause the mn matrices which have a 1 in the i, j place with zeros elsewhere \nform a basis for FmXn. If A is an rn X n matrix, then the solution space \nfor A has dimension n - r, where r is the number of non-zero rows in a \nrow-reduced echelon matrix which is rowᄃequivalent to A .  See Example 15. \nIf V is any vector space over F, the zero subspace of V is spanned by \nthe vector 0, but {O} is a linearly dependent set and not a basis. For this \nreason, we shall agree that the zero subspace has dimension 0. Alterna­\ntively, we could reach the same conclusion by arguing that the empty set \nis a basis for the zero subspace. The empty set spans {O}, because the \nintersection of all subspaces containing the empty set is {O} , and the \nempty set is linearly independent because it contains no vectors. \nLemma. Let S be a linearly independent subset of a vector space V. \n8uppose fJ is a vector in V which is not in the subspace spanned by S. Then",
    "intersection of all subspaces containing the empty set is {O} , and the \nempty set is linearly independent because it contains no vectors. \nLemma. Let S be a linearly independent subset of a vector space V. \n8uppose fJ is a vector in V which is not in the subspace spanned by S. Then \nthe set obtained by adjoining f3 to S is linearly independent. \nProof. Suppose ah .\n•\n.\n , am are distinct vectors in 8 and that \nClal + . . . + cmam + b{3 = O. \nThen b = 0; for otherwise, \nand f3 is in the subspace spanned by 8. Thus Clal + . . . + Crnam \n0, and \nsince 8 is a linearly independent set each Ci \nO. \nI \nTheorem 5. If W is a subspace of a fi·nite-dirnensional vector space V, \nevery linearly independent subset of W is finite and is part of a (finite) basis \nfor W. \nProof. Suppose 80 is a linearly independent subset of W. If 8 is \na :inearly independent subset of W containing 80, then 8 is also a linearly \nindependent subset of V; since V is finite-dimensional, 8 contains no more \nthan dim V elements. \nWe extend 80 to a basis for W, as follows. If 80 spans W, then 80 is a \nbasis for W and we are done. If 80 does not span W, we use the preceding \nlemma to find a vector fJl in W such that the set 81 \n80 U {fJ1} is inde­\npendent. If 81 spans W, fine. If not, apply the lemma to obtain a vector fJ2 \n45 \n46 \nVector Spaces \nChap. 2 \nin W such that 82 = 81 U {J32} is independent. If we continue in this way, \nthen (in not more than dim V steps) we reach a set \nSm \nSo U {J31, •\n.\n. , (jm} \nwhich is a basis for W. \nI \nCorollary 1. If W is a proper subspace of a finite-dimensional vector \nspace V, then W is finite-dimensional and dim W < dim V. \nProof. We may suppose W contains a vector a \"¢ O. By Theorem \n5 and its proof, there is a basis of W containing a which contains no more \nthan dim V elements. Hence W is finite-dimensional, and dim W ::; dim V. \nSince W is a proper subspace, there is a vector J3 in V which is not in W.",
    "Proof. We may suppose W contains a vector a \"¢ O. By Theorem \n5 and its proof, there is a basis of W containing a which contains no more \nthan dim V elements. Hence W is finite-dimensional, and dim W ::; dim V. \nSince W is a proper subspace, there is a vector J3 in V which is not in W. \nAdjoining J3 to any basis of W, we obtain a linearly independent subset \nof V. Thus dim W < dim V. I \nCorollary 2. In a finite-dimensional vector space V every non-empty \nlinearly independent set of vectors is part of a basis. \nCorollary 3. Let A be an n X n matrix over a field F, and suppose the \nrow vectors of A form a linearly independent set of vectors in Fn. Then A is \ninvertible. \nProof. Let ai, a2, .\n. . , an be the row vectors of A, and suppose \nW is the subspace of Fn spanned by aJ, a2, .\n. . , an. Since ai, a2, . . . , an \nare linearly independent, the dimension of W is n. Corollary 1 now shows \nthat W = F\". Hence there exist scalars Bij in F such that \nn \nfi \nz BiJ<\"Xj, \nj=1 \nwhere {EI' E2, .. . , En} is the standard basis of Fn. Thus for the matrix B \nwith entries Bij we have \nBA \n= I. \nI \nTheorem 6. If WI and Wz are finite-dimensional subspaces of a vector \nspace V, then WI + Wz is finite-dimensional and \ndim WI + dim W2 = dim (WI Ii Wz) + dim (WI + Wz). \nProof. By Theorem 5 and its corollaries, WI Ii W2 has a finite \nbasis {ai, .\n. . , O!k} which is part of a basis \nand part of a basis \n{ai, .\n.\n. , ak, J31, . .\n. , J3m} \nfor \nWI \n{alt · . .  , ak, \n'YJ, \"\n\" 'Yn} \nfor \nWz. \nThe subspace WI + W2 is spanned by the vectors \n'Yl, •\n.\n.\n , 'Yn \nSec. 2.3 \nBases and Dimension \nand these vectors form an independent set. For suppose \nz Xiai + z Yj{3j + z Zr'Yr \nO. \nThen \n- z Zr'Yr = z Xiai + z Yj{3j \nwhich shows that z Zr'Yr belongs to WI' As z Zr'Yr also belongs to W2 it \nfollows that \n{ Zr'Yr = z Ciai \nfor certain scalars CI, . . . , Ck. Because the set \nis independent, each of the scalars Zr = 0. Thus \nz Xiai + z Yj{3j = ° \nand since \nis also an independent set, each Xi",
    "which shows that z Zr'Yr belongs to WI' As z Zr'Yr also belongs to W2 it \nfollows that \n{ Zr'Yr = z Ciai \nfor certain scalars CI, . . . , Ck. Because the set \nis independent, each of the scalars Zr = 0. Thus \nz Xiai + z Yj{3j = ° \nand since \nis also an independent set, each Xi \n0 and each Yj \nO. Thus, \nis a basis for WI + Wz. Finally \ndim WI + dim Wz = (k + m) + (k + n) \nk + (m + k + n) \ndim (WI n W2) + dim (WI + W2). \nI \nLet us close this section with a remark about linear independence \nand dependence. We defined these concepts for sets of vectors. It is useful \nto have them defined for finite sequences (ordered n-tuples) of vectors: \nai, . . .  , an. We say that the vectors al, . . . , an are linearly dependent \nif there exist scalars Cl1 .\n.\n.\n , Cn, not all 0, such that Clal + . . . + cnan \nO. \nThis is all so natural that the reader may find that he has been using this \nterminology already. What is the difference between a finite sequence \nai, . . .  , an and a set {al, . . .  , an} ?  There are two differences, identity \nand order. \nIf we discuss the set {a!, . . .  , an} ,  usually it is presumed that no \ntwo of the vectors a!, . . .  , an are identical. In a sequence ai, . . .  , an all \nthe a/s may be the same vector. If ai \naj for some i ;:z!: j, then the se­\nquence ai, .\n. . , an is linearly dependent: \nai + ( - l)aj \nO. \nThus, if ai, . . .  , an are linearly independent, they are distinct and we \nmay talk about the set {ai, . . . , an} and know that it has n vectors in it. \nSo, clearly, no confusion will arise in discussing bases and dimension. The \ndimension of a finite-dimensional space V is the largest n such that some \nn-tuple of vectors in V is linearly independent-and so on. The reader \n47 \n48 \nVector Spaces \nChap. 2 \nwho feels that this paragraph is much ado about nothing might ask him­\nself whether the vectors \nare linearly independent in R2. \n(e,,/2, 1) \n(ș, 1) \nThe elements of a sequence are enumerated in a specific order. A set",
    "47 \n48 \nVector Spaces \nChap. 2 \nwho feels that this paragraph is much ado about nothing might ask him­\nself whether the vectors \nare linearly independent in R2. \n(e,,/2, 1) \n(ș, 1) \nThe elements of a sequence are enumerated in a specific order. A set \nis a collection of objects, with no specified arrangement or order. Of \ncourse, to describe the set we may list its members, and that requires \nchoosing an order. But, the order is not part of the set. The sets {I, 2, 3, 4} \nand {4, 3, 2, I} are identical, whereas 1, 2, 3, 4 is quite a different sequence \nfrom 4, 3, 2, 1. The order aspect of sequences has no bearing on ques­\ntions of independence, dependence, etc., because dependence (as defined) \nis not affected by the order. The sequence an, . . .  , al is dependent if and \nonly if the sequence aI, . . .  , an is dependent. In the next section, order \nwill be important. \nExercises \n1. Prove that if two vectors are linearly dependent, one of them is a scalar \nmultiple of the other. \n2. Are the vectors \nal = (1, 1, 2, 4), \na2 = (2, - 1, -5, 2) \naa \n(1, - 1, -4, 0), \na4 \n(2, 1, 1, 6) \nlinearly independent in R4? \n3. Find a basis for the subspace of R4 spanned by the four vectors of Exercise 2. \n4. Show that the vectors \nal = (1, 0, - 1), \na2 = (1, 2, 1), \naa = (0, -3, 2) \nform a basis for R3. Express each of the standard basis vectors as linear combina­\ntions of aI, a2, and aa. \n5. Find three vectors in R3 which are linearly dependent, and are such that \nany two of them are linearly independent. \n6. Let V be the vector space of all 2 X 2 matrices over the field F. Prove that V \nhas dimension 4 by exhibiting a basis for V which has four elements. \n7. Let V be the vector space of Exercise 6. Let WI be the set of matrices of the \nform \nand let W2 be the set of matrices of the form \nSec. 2.4 \nCoordinates \n(a) Prove that Wl and W2 are subspaces of V. \n(b) Find the dimensions of WI> W2, Wl + W2, and Wl n W2•",
    "7. Let V be the vector space of Exercise 6. Let WI be the set of matrices of the \nform \nand let W2 be the set of matrices of the form \nSec. 2.4 \nCoordinates \n(a) Prove that Wl and W2 are subspaces of V. \n(b) Find the dimensions of WI> W2, Wl + W2, and Wl n W2• \n8. Again let V be the space of 2 X 2 matrices over F. Find a basis {Al' A2, Aa, A4} \nfor V such that A ;  \nAi for each j. \n9. Let V be a vector space over a subfield F of the complex numbers. Suppose \na, (3, and l' are linearly independent vectors in V. Prove that (a + (3), ((3 + 1'), \nand (1' + a) are linearly independent. \n10. Let V be a vector space over the field F. Suppose there are a finite number \nof vectors aI, . . .  , ar in V which span V. Prove that V is finite-dimensional. \nn. Let V be the set of all 2 X 2 matrices A with complex entries which satisfy \nAll + A22 = O. \n(a) Show that V is a vector space over the field of real numbers, with the \nusual operations of matrix addition and multiplication of a matrix by a scalar. \n(b) Find a basis for this vector space. \n(c) Let W be the set of all matrices A in V such that A21 \n-A12 (the bar \ndenotes complex conjugation). Prove that W is a subspace of V and find a basis \nfor W. \n12. Prove that the space of all m X n matrices over the field F has dimension mn, \nby exhibiting a basis for this space. \n13. Discuss Exercise 9, when V is a vector space over the field with two elements \ndescribed in Exercise 5, Section 1.1. \n14. Let V be the set of real numbers. Regard V as a vector space over the field \nof rational numbers, with the usual operations. Prove that this vector space is not \nfinite-dimensional. \n49 \n2.4. Coordinates \nOne of the useful features of a basis <B in an n-dimensional space V is \nthat it essentially enables one to introduce coordinates in V analogous to \nthe 'natural coordinates' Xi of a vector ex = (Xl, . . .  , Xn) in the space Fn. \nIn this scheme, the coordinates of a vector a in V relative to the basis <B",
    "that it essentially enables one to introduce coordinates in V analogous to \nthe 'natural coordinates' Xi of a vector ex = (Xl, . . .  , Xn) in the space Fn. \nIn this scheme, the coordinates of a vector a in V relative to the basis <B \nwill be the scalars which serve to express a as a linear combination of the \nvectors in the basis. Thus, we should like to regard the natural coordinates \nof a vector a in Fn as being defined by a and the standard basis for Fn; \nhowever, in adopting this point of view we must exercise a certain amount \nof care. If \nand <B is the standard basis for Fn, just how are the coordinates of ex deter­\nmined by <B and a? One way to phrase the answer is this. A given vector a \nhas a unique expression as a linear combination of the standard basis \nvectors, and the ith coordinate Xi of ex is the coefficient of fi in this expres­\nsion. From this point of view we are able to say which is the ith coordinate \n50 \nVector Spaces \nChap. 2 \nbecause we have a 'natural' ordering of the vectors in the standard basis, \nthat is, we have a rule for determining which is the 'first' vector in the \nbasis, which is the 'second,' and so on. If CB is an arbitrary basis of the \nn-dimensional space V, we shall probably have no natural ordering of the \nvectors in CB, and it will therefore be necessary for us to impose some \norder on these vectors before we can define 'the ith coordinate of a rela­\ntive to CB.' To put it another way, coordinates will be defined relative to \nsequences of vectors rather than sets of vectors. \nDefinition. If V is a finite-dimensional vector space, an ordered basis \nfor V is a finite sequence of vectors which is linearly independent and spans V. \nIf the sequence ai, . . .  , an is an ordered basis for V, then the set \n{ai, . . .  , an} is a basis for V. The ordered basis is the set, together with \nthe specified ordering. We shall engage in a slight abuse of notation and \ndescribe all that by saying that \nCB \n{ai, . . .  , an}",
    "If the sequence ai, . . .  , an is an ordered basis for V, then the set \n{ai, . . .  , an} is a basis for V. The ordered basis is the set, together with \nthe specified ordering. We shall engage in a slight abuse of notation and \ndescribe all that by saying that \nCB \n{ai, . . .  , an} \nis an ordered basis for V. \nNow suppose V is a finite-dimensional vector space over the field F \nand that \nCB = {al, . . . , an} \nis an ordered basis for V. Given a in V, there IS a unique n-tuple \n(Xl, . . .  , Xn) of scalars such that \nn \na \nz Xiai. \ni = 1  \nThe n-tuple is unique, because if we also have \nthen \nn \nz (Xi - Zi)ai \n0 \n. = 1  \nand the linear independence of the ai tells us that Xi \nZi = 0 for each i. \nWe shall call Xi the ith coordinate of a relative to the ordered basis \nIf \nthen \nCB = {alJ . . .  , an} .  \nn \n(3 = z Yiai \n. = 1  \nn \na + (3 = z (Xi + Yi)ai \n;=1 \nso that the ith coordinate of (a + (3) in this ordered basis is (Xi + Yi). \nSec. 2.4 \nCoordinates \nSimilarly, the ith coordinate of (ea) is ex;. One should also note that every \nn-tuple (Xl, . . . , Xn) in Fn is the n-tuple of coordinates of some vector in \nV, namely the vector \nn \nz Xiai· \n; = 1  \nTo summarIze, each ordered basis for V determines a one-one \ncorrespondence \na -t (Xl, . . .  , Xn) \nbetween the set of all vectors in V and the set of all n-tuples in Fn. This \ncorrespondence has the property that the correspondent of (a + (3) is the \nsum in Fn of the correspondents of a and {3, and that the correspondent \nof (ca) is the product in Fn of the scalar e and the correspondent of a. \nOne might wonder at this point why we do not simply select some \nordered basis for V and describe each vector in V by its corresponding \nn-tuple of coordinates, since we would then have the convenience of oper­\nating only with n-tuples. This would defeat our purpose, for two reasons. \nFirst, as our axiomatic definition of vector space indicates, we are attempt­",
    "ordered basis for V and describe each vector in V by its corresponding \nn-tuple of coordinates, since we would then have the convenience of oper­\nating only with n-tuples. This would defeat our purpose, for two reasons. \nFirst, as our axiomatic definition of vector space indicates, we are attempt­\ning to learn to reason with vector spaces as abstract algebraic systems. \nSecond, even in those situations in which we use coordinates, the signifi­\ncant results follow from our ability to change the coordinate system, i.e., \nto change the ordered basis. \nFrequently, it will be more convenient for us to use the coordinate \nmatrix of a relative to the ordered basis CB :  \nx m \nrather than the n-tuple (Xl, . . .  , xn) of coordinates. To indicate the de­\npendence of this coordinate matrix on the basis, we shall use the symbol \n[alB \nfor the coordinate matrix of the vector a relative to the ordered basis CB. \nThis notation will be particularly useful as we now proceed to describe \nwhat happens to the coordinates of a vector a as we change from one \nordered basis to another. \nSuppose then that V is n-dimensional and that \nCB = {ai, . . .  , an} \nand CB' = {ai, . . .  , aK} \nare two ordered bases for V. There are unique scalars P ij such that \n(2-13) \nn \na; = z Pijai, \ni = 1  \n1 S j S n. \nLet xL .\n.\n.\n , xӨ be the coordinates of a given vector a in the ordered basis \nCB'. Then \n51 \n52 \nVector Spaces \na = x{a{ + . . .  + x?a? \nn \nn \n= z xj z Pijai \ni = 1  \ni = 1  \nn \nn \nz z (Pijxj)ai \nj = 1  i = 1  \n= \u000f (\u000f PiiX@) ai· \ni= 1 \ni - I  \nThus we obtain the relation \n(2-14) \na = \u000f (\u000f PijXi) ai. \ni - I  \nj = 1  \nChap. 2 \nSince the coordinates Xl, X2, •\n•\n•\n , Xn of a in the ordered basis ffi are uniquely \ndetermined, it follows from (2-14) that \n(2-15) \nn \nXi = z Pijxj, \nj = 1  \n1 ::s: i ::S:  n. \nLet P be the n X n matrix whose i, j entry is the scalar P;j, and let X and \nX' be the coordinate matrices of the vector a in the ordered bases ffi and",
    "•\n , Xn of a in the ordered basis ffi are uniquely \ndetermined, it follows from (2-14) that \n(2-15) \nn \nXi = z Pijxj, \nj = 1  \n1 ::s: i ::S:  n. \nLet P be the n X n matrix whose i, j entry is the scalar P;j, and let X and \nX' be the coordinate matrices of the vector a in the ordered bases ffi and \nffi'. Then we may reformulate (2-15) as \n(2-16) \nX = PX'. \nSince ffi and ffi' are linearly independent sets, X = 0 if and only if XI = O. \nThus from (2-16) and Theorem 7 of Chapter 1, it follows that P is invertible. \nHence \n(2-17) \nX' = P-lX. \nIf we use the notation introduced above for the coordinate matrix of a \nvector relative to an ordered basis, then (2-16) and (2-17) say \n[a](B = P[a](B' \n[a]<l.\\' = p-l[aJ<l.\\. \nThus the preceding discussion may be summarized as follows. \nTheorem 7. Let V be an n-dimensional vector space over the field F, \nand let ffi and ffi' be two ordered bases of V. Then there is a unique, necessarily \ninvertible, n X n matrix P with entries in F such that \n(i) \n(ii) \n[aJ(B = P [a](B' \n[a](B' = P-l[a](B \nfor every vector a in V. The columns of P are given by \n1, . . .  , n. \nSec. 2.4 \nCoordinates \nTo complete the above analysis we shall also prove the following \nresult. \nTheorem 8. Suppose P is an n X n invertible matrix over F. Let V \nbe an n-dimensional vector space over F, and let (B be an ordered basis of V. \nThen there is a unique ordered basis (B' of V such that \nW \nèk = P éê \n(ii) \n[alB' = P-I [a](B \nfor every vector a in V. \nProof. Let (B consist of the vectors al, •\n.\n.\n , an. If (B' = \n{a;, .\n.\n.\n , aӨ} is an ordered basis of V for which (i) is valid, it is clear that \nn \naj = z Pijai' \ni = l  \nThus we need only show that the vectors aj, defined by these equations, \nform a basis. Let Q = P-I. Then \nz Qjkaq = z Qjk z Pijai \nj \nj \ni \n= z z PiiQjk ai \nj i \nThus the subspace spanned by the set \n(B' = {a;, . . .  , a} \ncontains (B and hence equals V. Thus (B' is a basis, and from its definition",
    "form a basis. Let Q = P-I. Then \nz Qjkaq = z Qjk z Pijai \nj \nj \ni \n= z z PiiQjk ai \nj i \nThus the subspace spanned by the set \n(B' = {a;, . . .  , a} \ncontains (B and hence equals V. Thus (B' is a basis, and from its definition \nand Theorem 7, it is clear that (i) is valid and hence also (ii). \nI \nEXAMPLE 18. Let F be a field and let \nbe a vector in Fit. If (B is the standard ordered basis of Fn, \n(B \n{EI' . . .  , En} \nthe coordinate matrix of the vector a in the basis (B is given by \nEXAMPLE 19. Let R be the field of the real numbers and let e be a \nfixed real number. The matrix \n- Sin e] \ncos e \n53 \n54 \nVector Spaces \nChap. 2 \nis invertible with inverse, \np-l \n= [ C?S 0 sin OJ. \n-sm 0 cos 0 \nThus for each 0 the set eE' consisting of the vectors (cos 0, sin 0), ( - sin e, \ncos e) is a basis for R2; intuitively this basis may be described as the one \nobtained by rotating the standard basis through the angle e. If a is the \nvector (Xli X2), then \nor \nEXAMPLE 20. \n[\nCOS e sin e] [Xl] \n[aJr1\\' = -sin e cos (J \nX2 \nxi \nXl cos (J + X2 sin (J \nx+ = \n-Xl sin (J + X2 cos (J. \nLet F be a subfield of the complex numbers. The matrix \np [-Š i -n \nis invertible with inverse \nThus the vectors \nai \n1, \n0, 0) \naჾ = \n4, \n2, 0) \naჿ \n5, -3, 8) \nform a basis eE' of Fa. The coordinates xL xј, X3 of the vector a = (Xl, X2, xa) \nin the basis eE' are given by \n[X¡] _ [-x\\ + 2X2 ;- -Y-xa] \n_ [-1 : Y] [\nXl] \nX2 \n-\nZ\"X2 + T1rXa \n-\n0 \nZ\" \nnr \nX2 ' \nX3 \ntXa \n0 0 \nt \nXa \nIn particular, \n(3, 2, -8) = - lOaf - !aᄀ - a3. \nExercises \n1. Show that the vectors \nal \n(1, 1, 0, 0), \nas = (1, 0, 0, 4), \na2 \n(0, 0, 1, 1) \na4 = (0, 0, 0, 2) \nform a basis for R4. Find the coordinates of each of the standard basis vectors \nin the ordered basis {ai, a2, as, a4} . \nSec. 2.5 \nSummary of Row-Equivalence \n2. Find the coordinate matrix of the vector (1, 0, 1) in the basis of C3 consisting \nof the vectors (2i, 1, 0), (2, - 1, 1), (0, 1 + i, 1 - i), in that order. \n3. Let il",
    "in the ordered basis {ai, a2, as, a4} . \nSec. 2.5 \nSummary of Row-Equivalence \n2. Find the coordinate matrix of the vector (1, 0, 1) in the basis of C3 consisting \nof the vectors (2i, 1, 0), (2, - 1, 1), (0, 1 + i, 1 - i), in that order. \n3. Let il \n{ai, a2, as} be the ordered basis for R3 consisting of \nal = (1, 0, - 1), \na2 = (1, 1, 1), \naa = (1, 0, 0). \nWhat arc the coordinates of the vector (a, b, c) in the ordered basis il? \n4. Let W be thc subspace of C3 spanned by al \n(1, 0, i) and a2 \n(1 + i, 1, - 1). \n(a) Show that al and a2 form a basis for W. \n(b) Show that the vectors {31 \n(1, 1, 0) and {32 = (1, i, 1 + i) are in W and \nform another basis for W. \n(c) What are the coordinates of al and a2 in the ordered basis {{31, {32} for W? \n5. Let a = (XI, X2) and {3 = (YI, Y2) be vectors in R2 such that \nXIYI + X2Y2 = 0, \nxi + xĳ = yi + yĴ = 1. \nProve that il = {a, {3} is a basis for R2. Find the coordinates of the vector (a, b) \nin the ordered basis il \n{a, {3} . (The conditions on a and {3 say, geometrically, \nthat a and (3 are perpendicular and each has length 1.) \n6. Let V be the vector space over the complex numbers of all functions from R \ninto C, i.e., the space of all complex-valued fnnctions on the real line. Letfl(x) \n1, \nfz(x) = eix, fa(x) = e-ix• \n(a) Prove that fl' f2, and fa are linearly independent. \n(b) Let gl(x) \n1, g2(X) \ncos x, ga(X) \nsin x. Find an invertible 3 X 3 matrix \nP such that \n7. Let V be the (real) vector space of all polynomial functions from R into R \nof degree 2 or less, i.e., the space of all functions f of the form \nf(x) = Co + CIX + C2X2. \nLet t be a fixed real number and define \ngl(X) = 1, \ng2(X) = X + t, \nga(X) = (x + t)2. \nProve that il \n{gl, g2, ga} is a basis for V. If \nf(x) = Co + CIX + C2X2 \nwhat are the coordinates of f in this ordered basis il? \n55 \n2 . .  '). Summary of Row-Equivalence \nIn this section we shall utilize some elementary facts on bases and",
    "gl(X) = 1, \ng2(X) = X + t, \nga(X) = (x + t)2. \nProve that il \n{gl, g2, ga} is a basis for V. If \nf(x) = Co + CIX + C2X2 \nwhat are the coordinates of f in this ordered basis il? \n55 \n2 . .  '). Summary of Row-Equivalence \nIn this section we shall utilize some elementary facts on bases and \ndimension in finite-dimensional vector spaces to complete our discussion \nof row-equivalence of matrices. We recall that if A is an m X n matrix \nover the field F the row vectors of A are the vectors all .\n•\n.\n , am in Fn \ndefined by \n56 \nVector Spaces \nOhap. 2 \nand that the row space of A is the subspace of F\" spanned by these vectors. \nThe row rank of A is the dimension of the row space of A .  \nIf P is a k X m matrix over F, then the product B = PA is a k X n \nmatrix whose row vectors /31, . . .  , /3k are linear combinations \n/3i \nPilal + . . .  + Pimam \nof the row vectors of A. Thus the row space of B is a subspace of the row \nspace of A. If P is an m X m invertible matrix, then B is row-equivalent \nto A so that the symmetry of row-equivalence, or the equation A = P-lB, \nimplies that the row space of A is ::d80 a subspace of the row space of B. \nTheorem 9. Row-equivalent matrices have the same row space. \nThus we see that to study the rOw space of A we may as well study \nthe row space of a row-reduced echelon matrix which is row-equivalent \nto A. This we proceed to do. \nTheorem 10. Let R be a non-zero row-reduced echelon matrix. rPhen \nthe non-zero row vectors of R form a basis for the row space of R. \nProof. Let PI, •\n•\n•\n , PT be the non-zero row vectors of R :  \nPi = (Ril' . . .  , Rin) . \nCertainly these vectors span the row space of R; we need only prove they \nare linearly independent. Since R is a row-reduced echelon matrix, there \nare positive integers kl' . . . , kT such that, for i \u001a r \n(a) R(i, j) \n° if j < ki \n(2-18) \n(b) R(i, kJ = Oij \n(c) kl < . . .  < kT• \nSuppose /3 \n(2-19) \n(bJ, •\n•\n•\n , bn) is a vector in the row space of R: \nf3 = ClPl + . . . + CrPT'",
    "are positive integers kl' . . . , kT such that, for i \u001a r \n(a) R(i, j) \n° if j < ki \n(2-18) \n(b) R(i, kJ = Oij \n(c) kl < . . .  < kT• \nSuppose /3 \n(2-19) \n(bJ, •\n•\n•\n , bn) is a vector in the row space of R: \nf3 = ClPl + . . . + CrPT' \nThen we claim that Cj = bk;. Por, by (2-18) \nr \n(2-20) \nbk; = : CiR(i, kj) \ni = 1  \n= Cj. \nIn particular, if f3 = 0, i.e., if CIPI + . . . + CTPT = 0, then Cj must be the \nkjth coordinate of the zero vector so that Ci \n0, j \n1, . . . , r. Thus \nPI, •\n•\n•\n , PT are linearly independent. I \nTheorem 11. Let m and n be positive integers and let P be a field. \nSuppose W is a subspace of Fn and dim W \u001a m. Then there is precisely one \nm X n row-reduced echelon matrix over F which has W as its row space. \nSec. 2.5 \nSummary of Row-Equivalence \nProof. There is at least one m X n row-reduced echelon matrix \nwith row space W. Since dim W ::;  m, we can select some m vectors \nai, .\n.\n.\n , am in W which span W. Let A be the m X n matrix with row \nvectors ai, .\n.\n.\n , am and let R be a row-reduced echelon matrix which is \nrow-equivalent to A .  Then the row space of R is W. \nNow let R be any row-reduced echelon matrix which has W as its row \nspace. Let PI, . . .  , Pr be the non-zero row vectors of R and suppose that \nthe leading non-zero entry of Pi occurs in column ki, i \n1, . . .  , r. The \nvectors PI, .\n•\n•\n , Pr form a basis for W. In the proof of Theorem 10, we \nobserved that if fJ \n(bl, . . .  , bn) is in W, then \n{3 \nCIPI + . , . + CrPr, \nand Ci = bk,; in other words, the unique expression for {3 as it linear com­\nbination of PI, . . .  , Pr is \n(2-21) \nr \n{3 = 7 bk,Pi. \n; = 1  \nThus any vector {3 is determined if one knows the coordinates bk., i = 1, . . . , \nr. For example, P8 is the unique vector in W which has k8th coordinate 1 \nand kith coordinate 0 for i r!' 8. \nSuppose {3 is in W and {3 r!' O. We claim the first non-zero coordinate \nof {3 occurs in one of the columns k \n•. Since \nr \n{3 = 7 bk,Pi \ni = l  \nand {3 ;;6. 0, we can write",
    "r. For example, P8 is the unique vector in W which has k8th coordinate 1 \nand kith coordinate 0 for i r!' 8. \nSuppose {3 is in W and {3 r!' O. We claim the first non-zero coordinate \nof {3 occurs in one of the columns k \n•. Since \nr \n{3 = 7 bk,Pi \ni = l  \nand {3 ;;6. 0, we can write \n(2-22) \nFrom the conditions (2-18) one has Rij \n0 if i > 8 and j ::; k •. Thus \n(3 \n(0, . . .  , 0, bk., •\n•\n• , bn), \nbk• r!' 0 \nand the first non-zero coordinate of {3 occurs in column k •. Note also that \nfor each Ie., 8 = 1, . . . , r, there exists a vector in W which has a non-zero \nk.th coordinate, namely ps. \nIt is now clear that R is uniquely determined by W. The description \nof R in terms of W is as follows. We consider all vectors {3 = (blJ •\n•\n•\n , bn) \nin W. If {3 ;;6. 0, then the first non-zero coordinate of {3 must occur in some \ncolumn t: \n{3 = (0, . . .  , 0, bt, •\n•\n•\n , bn), \nbt ;;6. O. \nLet kl' . . .  , kr be those positive integers t such that there is some {3 ;;6. 0 \nin W, the first non-zero coordinate of which occurs in column t. Arrange \nkl' . . .  , kr in the order kl < le2 < . . , < ler. For each of the positive \nintegers k. there will be one and only one vector p. in W such that the \nksth coordinate of P. is 1 and the kith coordinate of p. is 0 for i ;;6. 8. Then \nR is the m X n matrix which has row vectors PI, . . .  , pr, 0, . . .  , 0. I \n57 \n58 \nVector Spaces \nChap. 2 \nCorollary. Each m X n matrix A is row-equi1!alent to one and only \none row-reduced echelon matrix. \nProof. We know that A is row-equivalent to at least one row­\nreduced echelon matrix R. If A is row-equivalent to another such matrix \nR', then R is row-equivalent to R'; hence, R and R' have the same row \nspace and must be identical. I \nCorollary. Let A and B be m X n matrices over the field F. Then A \nand B are row-equivalent if and only if they have the same row space. \nProof. We know that if A and B are row-equivalent, then they",
    "space and must be identical. I \nCorollary. Let A and B be m X n matrices over the field F. Then A \nand B are row-equivalent if and only if they have the same row space. \nProof. We know that if A and B are row-equivalent, then they \nhave the same row space. So suppose that A and B have the same row \nspace. Kow A is row-equivalent to a row-reduced echelon matrix R and \nB is row-equivalent to a row-reduced echelon matrix R'. Since A and B \nhave the same row space, R and R' have the same row space. Thus R \nR' \nand A is row-equivalent to B. I \nTo summarize-if A and B are m X n matrices over the field F, the \nfollowing statements are equivalent: \n1. A and B are row-equivalent. \n2. A and B have the same row space. \n3. B \nPA, where P is an invertible m X m matrix. \nA fourth equivalent statement is that the homogeneous systems \nAX = 0 and BX = 0 have the same solutions; however, although we \nknow that the row-equivalence of A and B implies that these systems \nhave the same solutions, it seems best to leave the proof of the converse \nuntil later. \n2.6. Computations Concerning Subspaces \nWe should like now to show how elementary row operations provide \na standardized method of answering certain concrete questions concerning \nsubspaces of Fn. We have already derived the facts we shall need. They \nare gathered here for the convenience of the reader. The discussion applies \nto any n-dimensional vector space over the field F, if one selects a fixed \nordered basis CB and describes each vector a in V by the n-tuple (Xl, . . .  , Xn) \nwhich gives the coordinates of a in the ordered basis il. \nSuppose we are given m vectors ai, .\n.\n.\n , am in Fn. We consider the \nfollowing questions. \n1. How does one determine if the vectors ai, \n.\n. . , am are linearly \nindependent? More generally, how does one find the dimension of the \nsubspace TV spanned by these vectors? \nSee. 2.6 \nComputations Concerning Subspaces \n2. Given (3 in Fn, how does one determine whether (3 is a linear com­",
    "1. How does one determine if the vectors ai, \n.\n. . , am are linearly \nindependent? More generally, how does one find the dimension of the \nsubspace TV spanned by these vectors? \nSee. 2.6 \nComputations Concerning Subspaces \n2. Given (3 in Fn, how does one determine whether (3 is a linear com­\nbination of aI, . . .  , am, i.e., whether (3 is in the subspace W? \n3. How can one give an explicit description of the subspace W? \nThe third question is a little vague, since it does not specify what is \nmeant by an 'explicit description' ; however, we shall clear up this point \nby giving the sort of description we have in mind. With this description, \nquestions (1) and (2) can be nnswered immediately. \nLet A be the m X n matrix with row vectors ai: \nai = (Ail) . . . , Ai,.). \nPerform a sequence of elementary row operations, starting with A and \nterminating with a row-reduced echelon matrix R. We have previously \ndescribed how to do this. At this point, the dimension of W (the row space \nof A) is apparent, since this dimension is simply the number of non-zero \nrow vectors of R. If PI, . . .  , Pr are the nOll-zero row vectors of R, then \nCB \n{PI' . . .  , Pr} is a basis for W. If the first non-zero coordinate of Pi is \nthe kith one, then we have for i S r \n(a) \nR(i, j) = 0, if j < ki \n(b) \nR(i, kj) = Oij \n( C ) \nkl < . . . < kr• \nThe subspace W consists of aU vectors \n(3 \nCIPI + . . .  + CrPr \nr \n= 7 ci(Ril, •\n•\n•\n , Rin). \niܱl \nThe coordinates bl, . . .  , bn of such a vector (3 are then \n(2-23) \nr \nbj \nz ciR.ij. \ni - I  \nIn particular, bk; \nCil and so if (:J \n(bb . . . , br.) is a linear combination \nof the Pi, it must be the particular linear combination \n(2-24) \nThe conditions on (3 that (2-24) should hold are \nT \n(2-25) \nbj = 7 bk,Rih \nj = 1, . . .  , n. \n; = 1  \n.r\\ow (2-25) is the explicit description of the subspace W spanned by \nai, .\n.\n.\n , am, that is, the subspa,ce consists of all vectors (3 in Fn whose co­",
    "(2-24) \nThe conditions on (3 that (2-24) should hold are \nT \n(2-25) \nbj = 7 bk,Rih \nj = 1, . . .  , n. \n; = 1  \n.r\\ow (2-25) is the explicit description of the subspace W spanned by \nai, .\n.\n.\n , am, that is, the subspa,ce consists of all vectors (3 in Fn whose co­\nordinates satisfy (2-25). What kind of description is (2-25)? In the first \nplace it describes W as all solutions (3 = (bl, . . •  , bn) of the system of \nhomogeneous linear equations (2-25). This system of equations is of a \nvery special nature, because it expresses (n - r) of the coordinates as \n59 \n60 \nVector Spaces \nChap. 2 \nlinear combinations of the r distinguished coordinates bkll •\n•\n•\n , bk,. One \nhas complete freedom of choice in the coordinates bkil that is, if CI, •\n•\n•\n , Cr \nare any r scalars, there is one and only one vector (3 in W which has Ci as \nits kith coordinate. \nThe significant point here is this: Given the vectors a;, row-reduction \nis a straightforward method of determining the integers r, kl' . . .  , kr and \nthe scalars Rij which give the description (2-25) of the subspace spanned \nby ai, . . .  , am. One should observe as we did in Theorem 11 that every \nsubspace W of Fn has a description of the type (2-25). We should also \npoint out some things about question (2). We have already stated how \none can find an invertible m X m matrix P such that R = PA, in Section \n1.4. The knowledge of P enables one to find the scalars Xl, •\n•\n•\n , Xm such \nthat \n(3 \nXlal + . . .  + Xmam \nwhen this is possible. For the row vectors of R are given by \nm \nPi = }; Pijaj \nj= 1  \nSO that if (3 i s  a linear combination of the ail we have \nand thus \nr \n(3 = }; bk,Pi \n; = 1  \nr \nm \n= }; bk• }; P ijaj \ni = 1  \nj = 1  \nm \nr \n}; }; bk,P ijaj \nj = l i = 1  \nr \nXj \n}; bk,P ii \ni = 1  \nis one possible choice for the Xj (there may be many). \nThe question of whether (3 \n(bJ, •\n•\n•\n , bn) is a linear combination of \nthe ai, and if so, what the scalars Xi are, can also be looked at by asking",
    "i = 1  \nj = 1  \nm \nr \n}; }; bk,P ijaj \nj = l i = 1  \nr \nXj \n}; bk,P ii \ni = 1  \nis one possible choice for the Xj (there may be many). \nThe question of whether (3 \n(bJ, •\n•\n•\n , bn) is a linear combination of \nthe ai, and if so, what the scalars Xi are, can also be looked at by asking \nwhether the system of equations \nm \n}; Aijx; \nbil \nj = 1, . .\n. , n \ni = 1  \nhas a solution and what the solutions are. The coefficient matrix of this \nsystem of equations is the n X m matrix E with column vectors ai, . . .  , am. \nIn Chapter 1 we discussed the use of elementary row operations in solving \na system of equations EX = Y. Let us consider one example in which we \nadopt both points of view in answering questions about subspaces of Fn. \nEXAMPLE 21. Let us pose the following problem. Let W be the sub­\nspace of R4 spanned by the vectors \nSec. 2.6 \nComputations Concerning Subspaces \nal = (1, 2, 2, 1) \na2 \n(0, 2, 0, 1) \nas = (-2, 0, -4, 3). \n(a) Prove that aI, a2, as form a basis for W, i.e., that these vectors \nare linearly independent. \n(b) Let {3 = (bl, b2, ba, b4) be a vector in W. What are the coordinates \nof {3 relative to the ordered basis {ai, a2, aa} ? \n(c) Let \nai = (1, 0, 2, 0) \naᄁ = (0, 2, 0, 1) \naჿ = (0, 0, 0, 3). \nShow that ai, aᄀ, aᄂ form a basis for W. \n(d) If {3 is in W, let X denote the coordinate matrix of (3 relative to \nthe a-basis and X' the coordinate matrix of (3 relative to the a'-basis. Find \nthe 3 X 3 matrix P such that X = PX' for every such {3. \nTo answer these questions by the first method we form the matrix A \nwith row vectors ai, a2, aa, find the row-reduced echelon matrix R which \nis row-equivalent to A and simultaneously perform the same operations \non the identity to obtain the invertible matrix Q such that R = QA : \n[ Ș Ȗ ȗ ŜJ \n-'t R = [ś \u001f ȩ ŚJ \n-2 ° \n-4 3 \n° ° ° 1 \n[1 ° 0J \no 1 ° -'t Q  \n0\n0\n1\n \n6 \n5 \n-4 ŝJ \n(a) Clearly R has rank 3, so ai, a2 and aa are independent. \n(b) Which vectors {3 = (bJ, b2, ba, b4) are in W? We have the basis",
    "on the identity to obtain the invertible matrix Q such that R = QA : \n[ Ș Ȗ ȗ ŜJ \n-'t R = [ś \u001f ȩ ŚJ \n-2 ° \n-4 3 \n° ° ° 1 \n[1 ° 0J \no 1 ° -'t Q  \n0\n0\n1\n \n6 \n5 \n-4 ŝJ \n(a) Clearly R has rank 3, so ai, a2 and aa are independent. \n(b) Which vectors {3 = (bJ, b2, ba, b4) are in W? We have the basis \nfor W given by PI, P2, pa, the row vectors of R. One can see at a glance that \nthe span of PI, P2, P3 consists of the vectors (3 for which ba \n2bl. For such \na {3 we have \n(2-26) \n(3 = blPl + b2P2 + b4Pa \n= [bb b2, b4]R \n[bl b2 b4]QA \n= Xl al + X2a2 + Xaaa \nb1 - tb2 + ib4 \n-b1 \nlb2 \nib4 \n- -Ab2 + tb4• \n61 \n62 \nVector Spaces \nChap. 2 \n(c) The vectors ai, a\u001d, a\u001e are all of the form (Yl, Y2, Ya, Y4) with Y3 \n2Yl \nand thus they are in W. One can see at a glance that they are independent. \n(d) The matrix P has for its columns \nPj = [ajJ(J! \nwhere <B = {ai, a2, as} . The equations (2-26) tell us how to find the co­\nordinate matrices for ai, a\u001f, a . For example with (3 = a( we have bl = 1, \nb2 0, \nba = 2, b4 0, and \nXl 1 HO) + HO) 1 \nX2 = \nI\nHO) \nHO) = 1 \nXa = \n- HO) + HO) = \nO. \nThus a( = al - 0:2. Similarly we obtain af = 0:2 and o:ᄂ = 20:1 - 2a2 + ag. \nHence \nNow let us see how we would answer the questions by the second \nmethod which we described. We form the 4 X 3 matrix B with column \nvectors ai, a2, a3 : \nB [1 0 -2] \n2 \n2 0 \n2 0 -4 \n1 1 \n3 \nWe inquire for which YI, Y2, Ya, Y4 the system BX \n= Y has a solution. \n[1 0 \no 0 \no 1 \no 0 \n[1 0 \n2 \n2 \n2 0 1 1 \n-2 Y'] [1 \no Y2 0 \n-4 Ya ---> 0 \n3 Y4 0 \n-2 \nY' ] [1 \n-  Y2 - 2Y4 \n--> 0 \n.) \nY4 \nYl 0 \no \nY3 - 2YI 0 \n0 -2 \nY' ] \n2 \n4 \nY2 \n2YI \n0 \no \nYa \n2Yl\n---> \n1 \n5 \nY4 - YI \n0 0 Y. - !y, + tY, ] \n0 1 \nH2Y4 - Y2) \n1 0 -YI + !Y2 - fY4 \n0 0 \nY3 - 2YI \nThus the condition that the system BX \n= Y have a solution is Ya \n2YI' \nSo (3 = (b1, b2, b3, b4) is in W if and only if ba = 2bl• If (3 is in W, then the \ncoordinates (Xl, X2, Xa) in the ordered basis {aI, a2, as} can be read off from",
    "0 1 \nH2Y4 - Y2) \n1 0 -YI + !Y2 - fY4 \n0 0 \nY3 - 2YI \nThus the condition that the system BX \n= Y have a solution is Ya \n2YI' \nSo (3 = (b1, b2, b3, b4) is in W if and only if ba = 2bl• If (3 is in W, then the \ncoordinates (Xl, X2, Xa) in the ordered basis {aI, a2, as} can be read off from \nthe last matrix above. We obtain once again the formulas (2-26) for those \ncoordinates. \nThe questions (c) and (d) are now answered as before. \nSec. 2.6 \nComputations Concerning Subspaces \nEXAMPLE 22. \nWe consider the 5 X 5 matrix \n[1 2 0 \n1 2 -1 \nA = 0 0 1 \n2 4 1 \no 0 0 \nand the following problems concerning A \n-! Al 10 1 \no 1 \n(a) Find an invertible matrix P such that PA IS a row-reduced \nechelon matrix R. \n(b) Find a basis for the row space W of A. \n(c) Say which vectors (bl, b2, b3, b4, bs) are in W. \n(d) Find the coordinate matrix of each vector (b1, b2, b3, b4, bs) in W \nin the ordered basis chosen in (b). \n(e) Write each vector (bb b2, ba, b4, b5) in W as a linear combination \nof the rows of A. \n(f) Give an explicit description of the vector space V of all 5 X 1 \ncolumn matrices X such that AX \n= O. \n(g) Find a basis for V. \n(h) For what 5 X 1 \ncolumn matrices Y does the equation AX = Y \nhave solutions X? \nTo solve these problems we form the augmented matrix A I of the \nsystem AX = Y and apply an appropriate sequence of row operations \nto A'. \n[? \n2 0 2 1 0 1 \n4 1 0 0 \n[@ \n2 0 3 0 1 4 0 0 0 \n0 0 0 \n0 0 0 \n1 0 Yz 0 0 \n4 0 Ya \n----> 0 0 \n3 0 Y] [ 2  \n10 1 Y4 0 0 \n0 \no \n1 \nY5 0 0 \n0 \nYt \nYz \n0 \nY. 1 \nYl + Y2 + Ya \n----> \n1 -3Yl + Y2 + Y4 \n1 \nY5 \n[B \n2 0 0 0 0 \n0 3 1 -4 1 4 1 4 0 0 \n0 3 0 \n1 4 0 \n0 0 1 \n0 0 0 \n0 0 0 \n0 0 0 1 1 \ny. \n1 \n-Yl + Y2 \nYs \n--+ \n-2Yt + Y4 \nYs \nYl \nY2 \nYs \n- Yl + Y2 + ya \nY. 1 \n- 3Yl + Yz + Y4 - Ys \n63 \n64 \nVector Spaces \nChap. 2 \n(a) If \nfor all Y, then \np = [ i \n- 1  \n-3 \nhence PA is the row-reduced echelon matrix \n[1 \n2 0 3 0] \no 0 1 \n4 0 \nR =\nO\nO\nO\nO\nI · \no 0 0 0 0 \no 0 0 0 0 \nIt should be stressed that the matrix P is not unique. There are, in fact,",
    "Y. 1 \n- 3Yl + Yz + Y4 - Ys \n63 \n64 \nVector Spaces \nChap. 2 \n(a) If \nfor all Y, then \np = [ i \n- 1  \n-3 \nhence PA is the row-reduced echelon matrix \n[1 \n2 0 3 0] \no 0 1 \n4 0 \nR =\nO\nO\nO\nO\nI · \no 0 0 0 0 \no 0 0 0 0 \nIt should be stressed that the matrix P is not unique. There are, in fact, \nmany invertible matrices P (which arise from different choices for the \noperations used to reduce A ') such that P A = R. \n(b) As a basis for W we may take the non-zero rows \nPI \n(1 2 0 3 0) \nP2 = (0 0 1 \n4 0) \nP3 = (0 0 0 0 \n1) \nof R. \n(c) The row-space W consists of all vectors of the form \n(3 \nCIPI + C2P2 + C3P3 \n= (CI, 2CI, C2, 3CI + 4C2, Ca) \nwhere CI, C2, C3 are arbitrary scalars. Thus (bl, b2, ba, b4, b5) is in W if and \nonly if \n(bt, b2, b3, b4, b5) \nblPI + baP2 + b.P3 \nwhich is true if and only if \nb2 = 2b1 \nb4 = 3bI + 4b3• \nThese equations are instances of the general system (2-25), and using \nthem we may tell at a glance whether a given vector lies in W. Thus \n(-5, - 10, 1, - 11, 20) is a linear combination of the rows of A, but \n(1, 2, 3, 4, 5) is not. \n(d) The coordinate matrix of the vector (bl, 2bt, ba, 3bI + 4b31 b5) in \nthe basis {PI, P2, pa} is evidently \nSec. 2.6 \nComputations Concerning Subspaces \n[tJ \n(e) There are many ways to write the vectors in TV as linear combi­\nnations of the rows of A. \nPerhaps the easiest method is to follow the first \nprocedure indicated before Example 21 : \nf3 \n(bl, 2b1, ba, 3b) \n4ba, b6) \n= [b1, ba, b5, 0, OJ . R \n= [b), ba, bu, 0, OJ . PA \n[bb b\" bo, 0, o{ i \n0 0 0 !]A \n1 0 0 \n0 0 0 \n- 1  \n1 \n1 \n0 \n-3 \n1 \n° \n1 \n- 1  \n[b1 + ba, -ba, 0, 0, b5J . A. \nIn particular, with f3 \n( - {5, - 10, 1, \n11, 20) we have \n{3 = (-4, - 1, 0, 0, 20) [i ř : H l· \n2 4 \n1 \n10 \n1 \no 0 \n0\n0\n1 \n(f) The equations in the system RX \n° are \nXl \n2X2 + 3X4 \n0 \nX3 + 4X4 = 0 \nX5 = O. \nThus V consists of all columns of the form \nwhere X2 and X4 are arbitrary. \n(g) The columns \nDl [=fl \nform a basis of V. This is an example of the basis described in Example 15. \n65",
    "2 4 \n1 \n10 \n1 \no 0 \n0\n0\n1 \n(f) The equations in the system RX \n° are \nXl \n2X2 + 3X4 \n0 \nX3 + 4X4 = 0 \nX5 = O. \nThus V consists of all columns of the form \nwhere X2 and X4 are arbitrary. \n(g) The columns \nDl [=fl \nform a basis of V. This is an example of the basis described in Example 15. \n65 \n66 \nVector Spaces \n(h) The equation AX = Y has solutions X if and only if \nExercises \n-YI + Y2 + Ys \n= 0 \n-3YI + Y2 + Y4 \nY6 = o. \nChap. 2 \n1. Let 8 < n and A an s X n matrix with entries in the field F. Use Theorem 4 \n(not its proof) to show that there is a non-zero X in FnXI such that AX \n0. \n2. Let \nLet \nal \n(1, 1, -2, 1), \na2 \n(3, 0, 4, - 1), \naa = (- 1, 2, 5, 2). \na = (4, -5, 9, -7), \n{3 = (3, 1, -4, 4), \n\"Y = (-1, 1, 0, 1). \n(a) Which of the vectors a, {3, \"Y are in the subspace of R4 spanned by the a;'? \n(b) Which of the vectors a, {3, \"Y are in the subspace of C4 spanned by the ai? \n(c) Does this suggest a theorem? \n3. Consider the vectors in R4 defined by \nal = (-1, 0, 1, 2), \na2 = (3, 4, -2, 5), \naa = (1, 4, 0, 9). \nFind a system of homogeneous linear equations for which the space of solutions \nis exactly the subspace of R4 spanned by the three given vectors. \n4. In ca, let \nal = (1, 0, -i), \naz = (1 + i, 1 - i, 1), \naa = (i, i, i). \nProve that these vectors form a basis for Ca. What are the coordinates of the \nvector (a, b, c) in this basis? \n5. Give an explicit description of the type (2-25) for the vectors \n{3 \n(bl, b2, ba, b4, b6) \nin W which are linear combinations of the vectors \nal = (1, 0, 2, 1, - 1), \naa = (2, - 1, 5, 2, 1), \na2 = (-1, 2, -4, 2, 0) \na4 = (2, 1, 3, 5, 2). \n6. Let V be the real vector space spanned by the rows of the matrix \n= \n1 \n7 - 1  -2 - 1  . \n[3 21 \n° \n9 \n0] \nA \n2 14 \n° \n6 \n1 \n6 42 - 1  \n13 \n° \n(a) Find a basis for V. \n(b) Tell which vectors (Xl, X2, Xa, X4, X5) are elements of V. \n(c) If (Xl, Xz, Xa, X4, X5) is in V what are its coordinates in the basis chosen in \npart (a)?",
    "= \n1 \n7 - 1  -2 - 1  . \n[3 21 \n° \n9 \n0] \nA \n2 14 \n° \n6 \n1 \n6 42 - 1  \n13 \n° \n(a) Find a basis for V. \n(b) Tell which vectors (Xl, X2, Xa, X4, X5) are elements of V. \n(c) If (Xl, Xz, Xa, X4, X5) is in V what are its coordinates in the basis chosen in \npart (a)? \n7. Let A be an m X n matrix over the field F, and consider the system of equa­\ntions AX = Y. Prove that this system of equations has a solution if and only if \nthe row rank of A is equal to the row rank of the augmented matrix of the system. \n3. Linear Transformations \n3.1 . Linear Transformations \nWe shall now introduce linear transformations, the objects which we \nshall study in most of the remainder of this book. The reader may find it \nhelpful to read (or reread) the discussion of functions in the Appendix, \nsince we shall freely use the terminology of that discussion. \nDefinition. Let V and W be vector spaces over the field F. A linear \ntransformation from V into W is a function T from V into W such that \nT(ca + (3) = c(Ta) + T{3 \nfor all a and {3 in V and all scalars c in F. \nEXAMPLE 1. If V is any vector space, the identity transformation \nI, defined by Ia \na, is a linear transformation from V into V. The \nzero transformation 0, defined by Oa = 0, is a linear transformation \nfrom V into V. \nEXAMPLE 2. Let F be a field and let V be the space of polynomial \nfunctions f from F into F, given by \nf(x) = Co + C1X + . . .  + CkXk. \nLet \n(Df)(x) \nCl + 2C2X + . . . \nThen D is a linear transformation from V into V -the differentiation \ntransformation. \n67 \n68 \nLinear Transformations \nChap. 3 \nEXAMPLE 3. Let A be a fixed m X n matrix with entries in the field F. \nThe function T defined by T(X) = AX is a linear transformation from \nFnXl into FmXI. The function U defined by U(a) \naA is a linear trans­\nformation from Fm into Fn. \nEXAMPLE 4. Let P be a fixed m X m matrix with entries in the field ]i' \nand let Q be a fixed n X n matrix over F. Define a function T from the \nspace FmXn into itself by T(A)",
    "FnXl into FmXI. The function U defined by U(a) \naA is a linear trans­\nformation from Fm into Fn. \nEXAMPLE 4. Let P be a fixed m X m matrix with entries in the field ]i' \nand let Q be a fixed n X n matrix over F. Define a function T from the \nspace FmXn into itself by T(A) \nPAQ. Then T is a linear transformation \nfrom ]i'mXn into Fmxn, because \nT(cA + B) \nP(cA + B)Q \n== (cPA + PB)Q \n== cPAQ + PBQ \ncT(A) + T(B). \nEXAMPLE 5. Let R be the field of real numbers and let V be the space \nof all functions from R into R which are continuous. Define T by \n(TJ)(x) = 10'\" J(t) dt. \nThen T is a linear transformation from V into V. The function TJ is \nnot only continuous but has a continuous first derivative. The linearity \nof integration is one of its fundamental properties. \nThe reader should have no difficulty in verifying that the transfor­\nmations defined in Examples 1, 2, 3, and 5 are linear transformations. We \nshall expand our list of examples considerably as we learn more about \nlinear transformations. \nIt is important to note that if T is a linear transformation from V \ninto W, then T(O) \n== 0; one can see this from the definition because \nT(O) = T(O + 0) \nT(O) + T(O). \nThis point is often confusing to the person who is studying linear algebra \nfor the first time, since he probably has been exposed to a slightly different \nuse of the term 'linear function.' A brief comment should clear up the \nconfusion. Suppose V is the vector space RI. A linear transformation from \nV into V is then a particular type of real-valued function on the real line R. \nIn a calculus course, one would probably call such a function linear if its \ngraph is a straight line. A linear transformation from RI into Rl, according \nto our definition, will be a function from R into R, the graph of which is a \nstraight line passing through the origin. \nIn addition to the property T(O) = 0, let us point out another property \nof the general linear transformation T. Such a transformation 'preserves'",
    "to our definition, will be a function from R into R, the graph of which is a \nstraight line passing through the origin. \nIn addition to the property T(O) = 0, let us point out another property \nof the general linear transformation T. Such a transformation 'preserves' \nlinear combinations; that is, if ai, . . .  , an are vectors in V and Cl, •\n•\n.\n , Cn \nare scalars, then \nT(clal + . . .  + cnan) \n== cI(Tal) + . . . + cn(Tan). \nSec. 3.1 \nLinear Transformations \nThis follows readily from the definition. For example, \nT(clal + C2(2) = cl(Tal) + T(c2a2) \n= cI(Tal) + c2(Taz). \nTheorem 1. Let V be a finite-dimensional vector space over the field F \nand let {ai, . . . , an} be an ordered basis for V. Let W be a vector space over the \nsame field F and let (31, •\n.\n.\n , (3n be any vectors in W. Then there i.s precisely \none linear transformation T from V into W such that \nTaj = (3i, \nj = 1, . . .  , n. \nProof. To prove there is some linear transformation T with Taj = \n(3j we proceed as follows. Given a in V, there is a unique n-tuple (Xl, . .\n. , xn) \nsuch that \nFor this vector a we define \nTa = XI(31 + . . .  + xn(3n. \nThen T is a well-defined rule for associating with each vector a in V a \nvector Ta in W. From the definition it is clear that Taj \n(3j for each j. \nTo see that T is linear, let \n(3 = Ylal + . . .  + Ynan \nbe in V and let c be any scalar. Now \nca + (3 = (CXI + YI)al + \nand so by definition \nT(ca + (3) \nOn the other hand, \nand thus \nn \nn \nc(Ta) + T(3 = c z Xi(3i + z Yi(3i \n;=1 \n;=1 \nn \n= z (CXi + Yi)(3i \n;=1 \nT(ca + (3) \nc(Ta) + T(3. \nIf U is a linear transformation from V into W with Ua; \n(31) j \nn \n1, .\n. . , n, then for the vector a \nz Xiai we have \n;=1 \nUa = U (.i Xiai) \n,=1 \nn \nz Xi(Uai) \n;=1 \nn \n= z X;(3i \n;=1 \n69 \n70 \nLinear Transformations \nChap. 3 \nso that U is exactly the rule T which we defined above. This shows that the \nlinear transformation l' with 1'aj \n(3j is unique. \nI \nTheorem 1 is quite elementary; however, it is so basic that we have",
    ",=1 \nn \nz Xi(Uai) \n;=1 \nn \n= z X;(3i \n;=1 \n69 \n70 \nLinear Transformations \nChap. 3 \nso that U is exactly the rule T which we defined above. This shows that the \nlinear transformation l' with 1'aj \n(3j is unique. \nI \nTheorem 1 is quite elementary; however, it is so basic that we have \nstated it formally. The concept of function is very general. If V and W are \n(non-zero) vector spaces, there is a multitude of functions from V into W. \nTheorem 1 helps to underscore the fact that the functions which are linear \nare extremely special. \nEXAMPLE 6. The vectors \nal = (1, 2) \na2 = (3, 4) \nare linearly independent and therefore form a basis for R2. According to \nTheorem 1, there is a unique linear transformation from R2 into R3 such \nthat \nTal = (3, 2, 1) \nTaz = (6, 5, 4). \nIf so, we must be able to find 1'(tl)' We find scalars ClJ Cz such that tJ = \nClal + C2a2 and then we know that Ttl = Cl1'al + C21'a2. If (1, 0) \nCl(1, 2) + c2(3, 4) then CI \n-2 and C2 \n1. Thus \nT(l, O) \n-2(3, 2, 1) \n(6, 5, 4) \n= (0, 1, 2). \nEXAMPLE 7. Let l' be a linear transformation from the m-tuple space \nFm into the n-tuple space Fn. Theorem 1 tells us that l' is uniquely de­\ntermined by the sequence of vectors (31, .\n•\n•\n , (3m where \ni = 1, . .\n. , m. \nIn short, l' is uniquely determined by the images of the standard basis \nvectors. The determination is \na \n(Xl, . . .  , X\",) \n1'a \nXl(31 + ' \" \nxm(3m. \nIf B is the m X n matrix which has row vectors (31, . . .  , f3m, this says that \n1'a \naBo \nIn other words, if f3i \n(Bil' . . .  , Bin), then \nThis is a very explicit description of the linear transformation. In Section \n3.4 we shall make a serious study of the relationship between linear trans-\nSec. 3.1 \nLinear Transformations \nformations and matrices. We shall not pursue the particular description \nTa \naB because it has the matrix B OIl the right of the vector a, and that \ncan lead to some confusion. The point of this example is to show that we",
    "Sec. 3.1 \nLinear Transformations \nformations and matrices. We shall not pursue the particular description \nTa \naB because it has the matrix B OIl the right of the vector a, and that \ncan lead to some confusion. The point of this example is to show that we \ncan give an explicit and reasonably simple description of all linear trans­\nformations from pm into pn. \nIf '1' is a linear transformation from V into W, then the range of '1' is \nnot only a subset of W ;  it is a subspace of W. Let RT be the range of T, that \nis, the set of all vectors /3 in W such that /3 \nTa for some a in V. Let /31 \nand /32 be in RT and let c be a scalar. There are vectors al and a2 in V such \nthat Tal = /31 and Ta2 = /32, Since '1' is linear \nT(cal + (2) = cTal + Ta2 \nC/31 + /32, \nwhich shows that C/31 + /32 is also in RT• \nAnother interesting subspace associated with the linear transformation \nT is the set N consisting of the vectors a in V such that Ta = 0. It is a \nsubspace of V because \n(a) T(O) = 0, so that N is non-empty; \n(b) if Tal \nTa2 \n0, then \nT(cal + (2) = cTa! \nTa2 \nso that cal + a2 is in N. \n= cO \n° \n= 0  \nDefinition. Let V and W be vector spaces over the field F and let T \nbe a linear transformation from V into W. The null space of T is the set \nof all vectors a in V such that Ta \nO. \nIf V is finite-dimensional, the rank of T is the dimension of the range \nof T and the nullity of T 1:S the dimension of the null space of T. \nThe following is one of the most important results in linear algebra. \nTheorem 2. Let V and W be vector spaces over the field F and let T be \na linear transformation from V into W. Suppose that V is finite-dimensional. \nThen \nrank (T) + nullity (T) = dim V. \nProof. Let {ai, . . .  , ak} be a basis for N, the null space of T. \nThere are vectors ak+l, . . .  , an in V such that {al, . . .  , an} is a basis for V. \nWe shall now prove that {Tak+l, . . .  , Tan} is a basis for the range of T.",
    "Then \nrank (T) + nullity (T) = dim V. \nProof. Let {ai, . . .  , ak} be a basis for N, the null space of T. \nThere are vectors ak+l, . . .  , an in V such that {al, . . .  , an} is a basis for V. \nWe shall now prove that {Tak+l, . . .  , Tan} is a basis for the range of T. \nThe vectors Tal, . . . , Tan certainly span the range of T, and since Taj = 0, \nfor j S k, we see that Tak+l, . . .  , Tan span the range. To see that these \nvectors are independent, suppose we have scalars Ci such that \nn \nz \nci(Tai) = O. \ni=k+l \n71 \n72 \nLinear Transformations \nOhap. 3 \nThis says that \nT ( £ Ciai) = 0 \ni =k+1 \nn \nand accordingly the vector a \nؒ \nCiai is in the null space of T. Since \ni =k+! \naI, . . . , ak form a basis for N, there must be scalars bt, . . .  , bk such that \nThus \nk \na = : b,ai . \n• = 1  \nIe \nn \n: biai -\nz \nCiai = 0 \ni - 1  \ni-k+! \nand since al, . . .  , an are linearly independent we must have \nIf r is the rank of T, the fact that Tak+l, . . .  , Tan form a basis for \nthe range of T tells us that r = n - k. Since k is the nullity of T and n is \nthe dimension of V, we are done. I \nTheorem 3. If A is an m X n matrix with entries in the field F, then \nrow rank (A) = column rank (A). \nProof. Let T be the linear transformation from FnXl into FmXl \ndefined by T(X) \nAX. The null space of T is the solution space for the \nsystem AX \n0, i.e., the set of all column matrices X such that AX \nO. \nThe range of T is the set of all m X 1 column matrices Y such that AX = \nY has a solution for X. If At, . . . , An are the columns of A, then \nAX = x1A1 + . , .  + xnAn \nso that the range of T is the subspace spanned by the columns of A .  In \nother words, the range of '1' is the column space of A. Therefore, \nrank (T) \ncolumn rank (A). \nTheorem 2 tells us that if S is the solution space for the system AX = 0, \nthen \ndim S + column rank (A) = n. \nWe now refer to Example 15 of Chapter 2. Our deliberations there",
    "other words, the range of '1' is the column space of A. Therefore, \nrank (T) \ncolumn rank (A). \nTheorem 2 tells us that if S is the solution space for the system AX = 0, \nthen \ndim S + column rank (A) = n. \nWe now refer to Example 15 of Chapter 2. Our deliberations there \nshowed that, if r is the dimension of the row space of A, then the solution \nspace S has a basis consisting of n - r vectors: \ndim S = n - row rank (A). \nIt is now apparent that \nrow rank (A) = column rank (A). I \nThe proof of Theorem 3 which we have just given depends upon \nSec. 3.1 \nLinear Transformations \nexplicit calculations concerning systems of linear equations. There is a \nmore conceptual proof which does not rely on such calculations. We shall \ngive such a proof in Section 3.7. \nExercises \n1. Which of the following functions ']' from R2 into R2 are linear transformations? \n(a) T(Xl' X2) = (1 + Xl, X2) ; \n(b) T(XI' xz) = (Xz, Xl) ; \n(c) T(XI, X2) = (Xl, X2) ; \n(d) T(XI, X2) \n(sin Xl, xz) ; \n(e) T(Xl' XZ) \n(Xl - X2, 0). \n2. Find the range, rank, null space, and nullity for the zero transformation and \nthe identity transformation on a finite-dimensional space V. \n3. Describe the range and the null space for the differentiation transformation \nof Example 2. Do the same for the integration transformation of Example 5. \n4. Is there a linear transformation T from R3 into R2 such that T(l, - 1, 1) \n(1, 0) and T(l, 1, 1) = (0, I)? \n5. If \nal \n(1, - 1), \na2 = (2, - 1), \na3 = (-3, 2), \n(31 = (1, 0) \n(32 = (0, 1) \n(33 \n(1, 1) \nis there a linear transformation T from RZ into RZ such that Tai \n(3. for i \n1, 2 \nand 3? \n6. Describe explicitly (as in Exercises 1 and 2) the linear transformation T from \nF2 into F2 such that Tml = (a, b), Tn2 = (e, d). \n7. Let F be a subfield of the complex numbers and let T be the function from \nF3 into Fa defined by \nT(XI, X2, xa) = (Xl \nXz + 2xa, 2xI + X2, -XI - 2xz + 2xa). \n(a) Verify that T is a linear transformation.",
    "F2 into F2 such that Tml = (a, b), Tn2 = (e, d). \n7. Let F be a subfield of the complex numbers and let T be the function from \nF3 into Fa defined by \nT(XI, X2, xa) = (Xl \nXz + 2xa, 2xI + X2, -XI - 2xz + 2xa). \n(a) Verify that T is a linear transformation. \n(b) If (a, b, c) is a vector in Fa, what are the conditions on a, b, and c that \nthe vector be in the range of T? What is the rank of T? \n(c) What are the conditions on a, b, and c that (a, b, c) be in the null space \nof T? What is the nullity of T? \n8. Describe explicitly a lineal' transformation from R3 into R3 which has as its \nrange the subspace spanned by (1, 0, - 1) and (1 , 2, 2). \n9. Let V be the vector space of all n X n matrices over the field F, and let B \nbe a fixed n X n matrix. If \nT(A) = AB - BA \nverify that T is a linear transformation from V into V. \n10. Let V be the set of all complex numbers regarded as a vector space over the \n73 \n74 \nLinear Transformations \nChap. 3 \nfield of real numbers (usual operations). Find a function from V into V which is \na linear transformation on the above vector space, but which is not a linear trans­\nformation on GI, i.e., which is not complex linear. \nn. Let V be the space of n X 1 matrices over F and let W be the space of m X 1 \nmatrices over F. Let A be a fixed m X n matrix over F and let T be the linear \ntransformation from V into W defined by T(X) = AX. Prove that T is the zero \ntransformation if and only if A is the zero matrix. \n12. Let V be an n-dimensional vector space over the field F and let T be a linear \ntransformation from V into II such that the range and null space of T are identical. \nProve that n is even. (Can you give an example of such a linear transformation T?) \n13. Let V be a vector space and T a linear transformation from V into V. Prove \nthat the following two statements about T are equivalent. \n(a) The intersection of the range of T and the null space of T is the zero \nsubspace of V. \n(b) If T(Ta) = 0, then Ta = 0.",
    "13. Let V be a vector space and T a linear transformation from V into V. Prove \nthat the following two statements about T are equivalent. \n(a) The intersection of the range of T and the null space of T is the zero \nsubspace of V. \n(b) If T(Ta) = 0, then Ta = 0. \n3.2. The Algebra of Linear Transformations \nIn the study of linear transformations from V into W, it is of funda­\nmental importance that the set of these transformations inherits a natural \nvector space structure. The set of linear transformations from a space V \ninto itself has even more algebraic structure, because ordinary composition \nof functions provides a 'multiplication' of such transformations. We shall \nexplore these ideas in this section. \nTheorem 4. Let V and W be vector spaces over the field F. Let T and \nU be linear transformations from V into W. The function (T + U) defined by \n(T + U)(a) = Ta + Ua \nis a linear tmnsformationfrom V into W. If c is any element of F, the function \n(cT) defined by \n(cT)(a) = c(Ta) \nis a linear transformation from V into W. The set of all linear transformations \nfrom V into W, together with the addition and scalar multiplication defined \nabove, is a vector space over the field F. \nProof. Suppose T and U are linear transformations from V into \nW and that we define (T + U) as above. Then \n(T \nU)(ca + m \n= T(ca + (3) + U(ca + m \n= c(Ta) + T{3 + c(Ua) + U{3 \n= c(Ta + Ua) + (T{3 + U(3) \n= c(T + U)(a) + (T + U)(m \nwhich shows that (T + U) is a linear transformation. Similarly, \nSec. 3.2 \nThe Algebra of Linear Transformations \n(cT)(da + (3) = c[T(da + (3)] \n= c[d(Ta) \nT{1] \n= cd(Ta) \nc(T{1) \n= d[c(Ta)] + c(T(3) \n= d[(cT)a] + (cT){1 \nwhich shows that (cT) is a linear transformation. \nTo verify that the set of linear transformations of V into W (together \nwith these operations) is a vector space, one must directly check each of \nthe conditions on the vector addition and scalar multiplication. We leave",
    "= d[(cT)a] + (cT){1 \nwhich shows that (cT) is a linear transformation. \nTo verify that the set of linear transformations of V into W (together \nwith these operations) is a vector space, one must directly check each of \nthe conditions on the vector addition and scalar multiplication. We leave \nthe bulk of this to the reader, and content ourselves with this comment: \nThe zero vector in this space will be the zero transformation, which sends \nevery vector of V into the zero vector in W; each of the properties of the \ntwo operations follows from the corresponding property of the operations \nin the space W. I \nWe should perhaps mention another way of looking at this theorem. \nIf one defines sum and scalar multiple as we did above, then the set of \nall functions from V into W becomes a vector space over the field F. This \nhas nothing to do with the fact that V is a vector space, only that V is a \nnon-empty set. When V is a vector space we can define a linear transforma­\ntion from V into W, and Theorem 4 says that the linear transformations \nare a subspace of the space of all functions from V into W. \nWe shall denote the space of linear transformations from V into W \nby L(V, W). We remind the reader that L(V, W) is defined only when V \nand W are vector spaces over the same field. \nTheorem 5. Let V be an n-dimensional vector space over the field F, \nand let W be an m-dimensional vector space over F. Then the space L(V, W) \nis finite-dimensional and has dimension mn. \nProof. Let \nbe ordered bases for V and W, respectively. For each pair of integers (p, q) \nwith 1 ყ p ყ m and 1 ყ q ყ n, we define a linear transformation EM \nfrom V into W by \nEp,q(ai) = {a, \n{1p, \nOiq{1p. \nif i rf- q  \nif i \nq \nAccording to Theorem 1, there is a unique linear transformation from V \ninto W satisfying these conditions. The claim is that the mn transforma­\ntions EM form a basis for L(V, W). \nLet T be a linear transformation from V into W. For each j, 1 ყ j ყ n, \n75 \n76 \nLinear Transformations",
    "if i \nq \nAccording to Theorem 1, there is a unique linear transformation from V \ninto W satisfying these conditions. The claim is that the mn transforma­\ntions EM form a basis for L(V, W). \nLet T be a linear transformation from V into W. For each j, 1 ყ j ყ n, \n75 \n76 \nLinear Transformations \nChap. 3 \nlet Ai;' . . . , Amj be the coordinates of the vector Taj in the ordered basis \nre', i.e., \n(3-1) \nWe wish to show that \n(3-2) \nm \nn \nT = 2: \n2: ApqEp,q. \np = l q = l  \nLet U be the linear transformation in the right-hand member of (3-2) . \nThen for each j \nUaj \n2:: 2:: ApqEp,q(aj) \np q \n2: 2: A pqlJjq{3p \np \nq \nm \n2: Apj{3p \np = l  \n= Taj \nand consequently U \nT. Now (3-2) shows that the Ep,q span L(V, W) ; \nwe must prove that they are independent, But this is clear from what \nwe did above; for, if the transformation \nU = 2: 2: ApqEp,q \np \nq \nis the zero transformation, then Uaj \n0 for each j, so \nm \n2: Apj{3p = 0 \np = l  \nand the independence of the (3p implies that Apj = 0 for every p and j. I \nTheorem 6. Let V, W, and Z be vector spaces over the field F. Let T \nbe a linear transformation from V into W and U a linear transformation \nfrom W into Z. Then the composed function UT defined by (UT)(a) \nU(T(a» is a linear transformation from V into Z. \nProof. \n(UT)(ca + (3) = U[T(ca + (3)] \n= U(cTa + T(3) \nc[U(1Ta)] + U(T{3) \nc(UT)(a) + (UT)({3). I \nIn what follows, we shall be primarily concerned with linear trans­\nformation of a vector space into itself. Since we would so often have to \nwrite 'T is a linear transformation from V into V,' we shall replace this \nwith 'T is a linear operator on V.' \nDefinition. [fV is a vector space over the field F, a linear operator on \nV is a linear transformation from V into V. \nSec. 3.2 \nThe Algebra of Linear Transformations \nIn the case of Theorem 6 when V \nW \nZ, so that U and T are \nlinear operators on the space V, we see that the composition UT is again \na linear operator on V. Thus the space L(V, V) has a 'multiplication'",
    "V is a linear transformation from V into V. \nSec. 3.2 \nThe Algebra of Linear Transformations \nIn the case of Theorem 6 when V \nW \nZ, so that U and T are \nlinear operators on the space V, we see that the composition UT is again \na linear operator on V. Thus the space L(V, V) has a 'multiplication' \ndefined on it by composition. In this case the operator TU is also defined, \nand one should note that in general UT r= TU, i.e., UT \nTU r= O. We \nshould take special note of the fact that if T is a linear operator on V then \nwe can compose T with T. We shall use the notation T2 = TT, and in \ngeneral Tn = T . . . T (n times) for n = 1, 2, 3, . . . . We define TO \nI if \nT r= O. \nLemma. Let V be a vector space over the field F; let U, 1\\ and T2 be \nlinear operators on V; let c be an element of F. \n(a) IU = UI = U ;  \n(b) U(TI + T2) \nUTI + UT2; (Tl + T2)U = TIU + T2U; \n(c) c(UT1) \n(CU)Tl \nU(cT1). \nProof. (a) This property of the identity function is obvious. We \nhave stated it here merely for emphasis. \n(b) \n[U(TI + T2)] (a) = U[(TI + T2) (a)] \nU(Tla + T2a) \nU(TIa) \nU(T2a) \n= ( UTI)(a) + ( UT2) (a) \nso that U(TI + T2) = UTI + UT2. Also \n[(TI + T2) U](a) = (TI + T2)( Ua) \nTI(Ua) + T2(Ua) \n(TIU) (a) \n(T2U)(a) \nso that (Tl + T2) U = Tl U + T2 U. (The reader may note that the proofs \nof these two distributive laws do not use the fact that TI and T2 are linear, \nand the proof of the second one does not use the fact that U is linear either.) \n(c) We leave the proof of part (c) to the reader. \nI \nThe contents of this lemma and a portion of Theorem 5 tell us that \nthe vector space L(V, V), together with the composition operation, is \nwhat is known as a linear algebra with identity. We shall discuss this in \nChapter 4. \nEXAMPLE 8. If A is an m X n matrix with entries in F, we have the \nlinear transformation T defined by T(X) = AX, from FnXl into FmXl. If \nB is a p X m matrix, we have the linear transformation U from FmXl into \nFpXl defined by U(Y)",
    "Chapter 4. \nEXAMPLE 8. If A is an m X n matrix with entries in F, we have the \nlinear transformation T defined by T(X) = AX, from FnXl into FmXl. If \nB is a p X m matrix, we have the linear transformation U from FmXl into \nFpXl defined by U(Y) \nBY. The composition UT is easily described: \n( UT)(X) = U(T(X)) \n= U(AX) \n= B(AX) \n(BA)X. \nThus UT is 'left multiplication by the product matrix BA.' \n77 \n78 \nLinear Transformations \nChap, 3 \nEXAMPLE 9. Let P be a field and V the vector space of all polynomial \nfunctions from F into P. Let D be the differentiation operator defined in \nExample 2, and let T be the linear operator 'multiplication by x' : \n(Tf)(x) \n= xf(x). \nThen DT  TD. In fact, the reader should find it easy to verify that \nDT \nTD = I, the identity operator. \nEven though the 'multiplication' we have on L(V, V) is not commu­\ntative, it is nicely related to the vector space operations of L(V, V). \nEXAMPLE 10. Let CB \n{at, . . .  , an} be an ordered basis for a vector \nspace V. Consider the linear operators Ep,q which arose in the proof of \nTheorem 5 :  \nEp,q(ai) = oiqap. \nThese n2 linear operators form a basis for the space of linear operators on V. \nWhat is Ep,qEr,,? We have \nTherefore, \n(Ep,qEr,8) (ai) = Ep,q(Oisar) \n= OisEp,q(ar) \n= oisOrqap-\nEp,qEr,s = \n' \n. \n{o if r  q \nEp,s, If \nq = r. \nLet T be a linear operator on V. We showed in the proof of Theorem 5 \nthat if \nthen \nIf \nAi = [TaJ:B \nA = [A 1, •\n•\n•\n , An] \nT = Y Y ApqEp,q. \np q \nr \n8 \nis another linear operator on V, then the last lemma tells us that \nTU \n(Y Y  ApqEp,q) (Y Y BrsEr,,) \np q \np q r 8 \nr 8 \nAs we have noted, the only terms which survive in this huge sum are the \nterms where q = r, and since Ep,rEr .• = Ep,·, we have \nTU = Y Y (Y A prBra)Ep,8 \nP \n• \nT \nP \n• \nThus, the effect of composing T and U is to multiply the matrices A and B. \nSec. 3.2 \nThe Algebra of Linear Transformations \nIn our discussion of algebraic operations with linear transformations",
    "terms where q = r, and since Ep,rEr .• = Ep,·, we have \nTU = Y Y (Y A prBra)Ep,8 \nP \n• \nT \nP \n• \nThus, the effect of composing T and U is to multiply the matrices A and B. \nSec. 3.2 \nThe Algebra of Linear Transformations \nIn our discussion of algebraic operations with linear transformations \nwe have not yet said anything about invertibility. One specific question of \ninterest is this. For which linear operators l' on the space V does there \nexist a linear operator T-I such that TT-I = T-IT \nI? \nThe function l' from V into W is called invertible if there exists a \nfunction U from W into V such that U1' is the identity function on V and \nTU is the identity function OIl W. If T is invertible, the function U is \nunique and is denoted by T-I. (See Appendix.) Furthermore, T is invertible \nif and only if \n1. T is 1 :  1, that is, Ta = T(3 implies a = (3; \n2. T is onto, that is, the range of T is (all of) W. \nTheorem 7. Let V and W be vector spaces over the field F and let T \nbe a linear transformation from V into W. If T is invertible, then the inverse \nfunction T-I is a linear transformation from W onto V. \nProof. We repeat ourselves in order to underscore a point. When \n'P is one-one and onto, there is a uniquely determined inverse function 1'-1 \nwhich maps W onto V such that T-IT is the identity function on V, and \nTT-I is the identity function on W. What we are proving here is that if a \nlinear function l' is invertible, then the inverse 1'-1 is also linear. \nLet (31 and (32 be vectors in W and let c be a scalar. We wish to show \nthat \nT-1(c(31 + (32) = cT-1(31 + T-I(32. \nLet ai \nT-1(3;, i = 1, 2, that is, let ai be the unique vector in V such that \nTa; = (:3i. Since T is linear, \nT(cal + (2) = CTal + Ta2 \n= C{:31 + {:32. \nThus Cal + a2 is the unique vector in V which is sent by T into C{:3l + (:32, \nand so \nT-I(c{:31 + (:32) \nCal + a2 \nand T-1 is linear. I \n= C(T-I(3l) + T-l{:32 \nSuppose that we have an invertible linear transformation T from V",
    "T(cal + (2) = CTal + Ta2 \n= C{:31 + {:32. \nThus Cal + a2 is the unique vector in V which is sent by T into C{:3l + (:32, \nand so \nT-I(c{:31 + (:32) \nCal + a2 \nand T-1 is linear. I \n= C(T-I(3l) + T-l{:32 \nSuppose that we have an invertible linear transformation T from V \nonto W and an invertible linear transformation U from W onto Z. Then UT \nis invertible and (U1')-l = T-I U-I. That conclusion does not require the \nlinearity nor does it involve checking separately that UT is 1 :  1 and onto. \nAll it involves is verifying that T-1U-1 is both a left and a right inverse for \nUT. \nIf T is linear, then TCa \n(:3) \nTa \nT{:3; hence, Ta \nT{:3 if and only \nif T(a - (:3) = O. This simplifies enormously the verification that l' is 1 :  1. \nLet us call a linear transformation T non-singular if T'Y = 0 implies \n79 \n80 \nLinear Transformations \nChap. 3 \n'Y = 0, i.e., if the null space of T is {O} . Evidently, T is 1 :  1 if and only if T \nis non-singular. The extension of this remark is that non-singular linear \ntransformations are those which preserve linear independence. \nTheorem 8. Let T be a linear transformation from V into W. Then \nT is non-singular if and only if T carries each linearly independent subset of \nV onto a linearly independent subset of w. \nProof. First suppose that T is non-singular. Let S be a linearly \nindependent subset of V. If lXI, •\n•\n•\n , lXk are vectors in S, then the vectors \nTlXl, •\n•\n.\n , TlXk are linearly independent; for if \nthen \nT(clal + \nand since T is non-singular \ncia I + . . .  + CklXk = ° \nfrom which it follows that each Ci = ° because S is an independent set. \nThis argument shows that the image of S under T is independent. \nSuppose that ']' carries independent subsets onto independent subsets. \nLet lX be a non-zero vector in V. Then the set S consisting of the one vector \nlX is independent. The image of S is the set consisting of the one vector TlX, \nand this set is independent. Therefore TlX -;if 0, because the set consisting",
    "Let lX be a non-zero vector in V. Then the set S consisting of the one vector \nlX is independent. The image of S is the set consisting of the one vector TlX, \nand this set is independent. Therefore TlX -;if 0, because the set consisting \nof the zero vector alone is dependent. This shows that the null space of T is \nthe zero subspace, i.e., T is non-singular. \nI \nEXAMPLE 11. Let F be a sub field of the complex numbers (or a field of \ncharacteristic zero) and let V be the space of polynomial functions over F. \nConsider the differentiation operator D and the 'multiplication by x' \noperator T, from Example 9. Since D sends all constants into 0, D is \nsingular; however, V is not finite dimensional, the range of D is all of V, \nand it is possible to define a right inverse for D. For example, if E is the \nindefinite integral operator : \n1 \n1 \nE(co + ClX + . . .  + C xn) = cox + - ClX2 + . . .  + -- C xn+l \nn \n2 \nn + 1 n \nthen E is a linear operator on V and DE \nI. On the other hand, ED -;if I \nbecause ED sends the constants into 0. The operator T is in what we might \ncall the reverse situation. If xf(x) = ° for all x, then f = 0. Thus T is non­\nsingular and it is possible to find a left inverse for T. For example if U is \nthe operation 'remove the constant term and divide by x' : \nU(co + ClX + . . .  + cnxn) = Cl + C2X + . . .  + cnXn-1 \nthen U is a linear operator on V and UT = I. But TU -;if I since every \nSec. 3.2 \nThe Algebra of Linear Transformations \nfunction in the range of TU is in the range of T, which is the space of \npolynomial functions f such that f(O) == o. \nEXAMPLE 12. Let F be a field and let T be the linear operator on [!'2 \ndefined by \nThen T is non-singular, because if TeXl, X2) \n0 we have \nXl + X2 \n0 \nXl = 0 \nso that Xl = X2 = O. We also see that T is onto; for, let (ZI' Z2) be any \nvector in [!'2. To show that (Zl' \n22) is in the range of T we must find scalars \nXl and X2 such that \nand the obvious solution is Xl = Z2, X2",
    "Then T is non-singular, because if TeXl, X2) \n0 we have \nXl + X2 \n0 \nXl = 0 \nso that Xl = X2 = O. We also see that T is onto; for, let (ZI' Z2) be any \nvector in [!'2. To show that (Zl' \n22) is in the range of T we must find scalars \nXl and X2 such that \nand the obvious solution is Xl = Z2, X2 \nZl - Z2. This last computation \ngives us an explicit formula for T-l, namely, \nT-l(ZI, Z2) \n(Z2, Zl \nZ2). \nWe have seen in Example 11 that a linear transformation may be \nnon-singular without being onto and may be onto without being non­\nsingular. The present example illustrates an important case in which that \ncannot happen. \nTheorem 9. Let V and W be finite-dimensional vector spaces over the \nfield F such that dirn V \ndirn W. If T is a linear transforrnation frorn V into \nW, the following are equivalent: \n(i) T is invertible. \n(ii) T is non-singular. \n(iii) T is onto, that is, the range of T is W. \nProof. Let n = dim V = dim W. From Theorem 2 we know that \nrank (T) \nnullity (T) = n. \nNow T is non-singular if and only if nullity (T) = 0, and (since n = dim \nW) the range of T is W if and only if rank (T) \nn. Since the rank plus the \nnullity is n, the nullity is 0 precisely when the rank is n. Therefore T is \nnon-singular if and only if T(V) = W. So, if either condition (ii) or (iii) \nholds, the other is satisfied as well and T is invertible. I \nWe caution the reader not to apply Theorem 9 except in the presence \nof finite-dimensionality and with dim V \ndim W. Under the hypotheses \nof Theorem 9, the conditions (i), (ii), and (iii) are also equivalent to these. \n(iv) If {ai, . . .  , an} is basis for V, then {Tal, . . .  , Tan} is a basis for \nW. \n81 \n82 \nLinear Transformations \nChap. 3 \n(v) There is some basis {ai, . . .  , an} for V such that {Tal, . . . , Tan} \nis a basis for W. \nWe shall give a proof of the equivalence of the five conditions which \ncontains a different proof that (i), (ii), and (iii) are equivalent.",
    "W. \n81 \n82 \nLinear Transformations \nChap. 3 \n(v) There is some basis {ai, . . .  , an} for V such that {Tal, . . . , Tan} \nis a basis for W. \nWe shall give a proof of the equivalence of the five conditions which \ncontains a different proof that (i), (ii), and (iii) are equivalent. \n(i) -+ (ii). If T is invertible, T is non-singular. (ii) -+ (iii). Suppose \nT is non-singular. Let {al, . . . , a,,} be a basis for V. By Theorem 8, \n{Tal, . . .  , Tan} is a linearly independent set of vectors in W, and since \nthe dimension of W is also n, this set of vectors is a basis for W. Now let fJ \nbe any vector in W. There are scalars CI, •\n•\n•\n , c\" such that \nfJ = CI(Tal) + . . .  + c.,(Tan) \nT(cla! \n. . . + c,.an) \nwhich shows that fJ is in the range of T. (iii) -+ (iv). We now assume that \nT is onto. If {ai, . . .  , an} is any basis for V, the vectors Tal, . . . , Ta\" \nspan the range of T, which is all of W by assumption. Since the dimension \nof W is n, these n vectors must be linearly independent, that is, must comprise \na basis for W. (iv) -+ (v). This requires no comment. (v) -+ (i). Suppose \nthere is some basis {ab ' . .  , an} for V such that {Tab ' . .  , Tan} is a \nbasis for W. Since the Ta; span W, it is clear that the range of T is all of W. \nIf a = cia! + . . . + c.,an is in the null space of T, then \nor \nT(clal \n. . . + c\"an) = 0 \nCI(Tal) + . . .  + c,,(Ta,,) = 0 \nand since the Tai are independent each Ci = 0, and thus a = 0. We have \nshown that the range of T is W, and that T is non-singular, hence T is \ninvertible. \nThe set of invertible linear operators on a space V, with the operation \nof composition, provides a nice example of what is known in algebra as \na 'group.' Although we shall not have time to discuss groups in any detail, \nwe shall at least give the definition. \nDefinition. A group consists of the following. \n1. A set G; \n2. A rule (or operation) which associates with each pair of elements x, \ny in G an element xy in G in such a way that",
    "a 'group.' Although we shall not have time to discuss groups in any detail, \nwe shall at least give the definition. \nDefinition. A group consists of the following. \n1. A set G; \n2. A rule (or operation) which associates with each pair of elements x, \ny in G an element xy in G in such a way that \n(a) x(yz) = (xy)z, for all x, y, and z in G (associativity); \n(b) there is an element e in G such that ex \nxe = x, for every x in G; \n(c) to each element x in G there corresponds an element X-I in G such \nthat xx-l = X-IX = e. \nWe have seen that composition ( U, T) -+ UT associates with each \npair of invertible linear operators on a space V another invertible operator \non V. Composition is an associative operation. The identity operator I \nSec. 3.2 \nThe Algebra of Linear Transformations \nsatisfies IT = T/ for each T, and for an invertible T there is (by Theorem \n7) an invertible linear operator ']'-1 such that T1'-l = 1'-11' = I. Thus the \nset of invertible linear operators on V, together with this operation, is a \ngroup. The set of invertible n X n matrices with matrix multiplica­\ntion as the operation is another example of a group. A group is called \ncommutative if it satisfies the condition xy \nyx for each x and y. The \ntwo examples we gave above are not commutative groups, in generaL One \noften writes the operation in a commutative group as (x, y) -t x + y, \nrather than (x, y) -t xy, and then uses the symbol 0 for the 'identity' \nelement e. The set of vectors in a vector space, together with the operation \nof vector addition, is a commutative group. A field can be described as a \nset with two operations, called addition and multiplication, which is a \ncommutative group under addition, and in which the non-zero elements \nform a commutative group under multiplication, with the distributive \nlaw x(y + z) = xy + xz holding. \n1. Let l' and U be the linear operators on R2 defined by \nT(x[, X2) \n(Xz, Xl) and U(Xl, X2) \n(x!, 0).",
    "commutative group under addition, and in which the non-zero elements \nform a commutative group under multiplication, with the distributive \nlaw x(y + z) = xy + xz holding. \n1. Let l' and U be the linear operators on R2 defined by \nT(x[, X2) \n(Xz, Xl) and U(Xl, X2) \n(x!, 0). \n(a) How would you describe T and U geometrically? \n(b) Give rules like the ones defining T and U for each of the transformations \n( U  + 7'), UT, TU, T2, U2. \n2. Let l' be the (unique) linear operator on CS for whieh \nT€l \n(1, 0, i), \nT€2 \n(0, 1, 1), \nT€a \n(i, 1, 0). \nIs l' invertible? \n3. Let T be the linear operator on R3 defined by \nT(x[, X2, xa) = (3Xl' Xl - Xz, 2XI + X2 + xa). \nIs T invertible? If so, find a rule for 1'-1 like the one which defines T. \n4. For the linear operator T of Exercise 3, prove that \n(T2 - 1)(T - 31) = o. \n5. Let C2X2 be the complex vector space of 2 X 2 matrices with complex entries. \nLet \nB [ 1 -IJ \n-4 \n4 \nand let T be the linear operator on C2X2 defined by T(A) = BA. What is the \nrank of T? Can you describe T2? \n6. Let l' be a linear transformation from RS into R2, and let U be a linear trans­\nformation from R2 into R3. Prove that the transformation UT is not invertible. \nGeneralize the theorem. \n83 \n84 \nLinear Transformations \nChap. 3 \n7. Find two linear operators T and U on R2 such that TU = 0 but UT ;e. O. \n8. Let V be a vector space over the field F and T a linear operator on V. If T2 = 0, \nwhat can you say about the relation of the range of T to the null space of T? \nGive an example of a linear operator T on R2 such that T2 = 0 but T ;e. O. \n9. Let T be a linear operator on the finite-dimensional space V. Suppose there \nis a linear operator U on V such that TU \nI. Prove that T is invertible and \nU = T-I. Give an example which shows that this is false when V is not finite­\ndimensional. (Hint: Let T = D, the differentiation operator on the space of poly­\nnomial functions.) \n10. Let A be an m X n matrix with entries in F and let T be the linear transforma­",
    "I. Prove that T is invertible and \nU = T-I. Give an example which shows that this is false when V is not finite­\ndimensional. (Hint: Let T = D, the differentiation operator on the space of poly­\nnomial functions.) \n10. Let A be an m X n matrix with entries in F and let T be the linear transforma­\ntion from FnXI into FmXI defined by T(X) = AX. Show that if m < n it may \nhappen that T is onto without being non-singular. Similarly, show that if m > n \nwe may have T non-singular but not onto. \nn. Let V be a finite-dimensional vector space and let T be a linear operator on V. \nSuppose that rank (T2) = rank (T). Prove that the range and null space of T are \ndisjoint, i.e., have only the zero vector in common. \n12. Let p, m, and n be positive integers and F a field. Let V be the space of 'In X n \nmatrices over F and W the space of p X n matrices over F. Let B be a fixed p X m \nmatrix and let T be the linear transformation from V into W defined by \nT(A) = BA. Prove that T is invertible if and only if p = m and B is an invertible \nm X m matrix. \n3.3. Isomorphism \nIf V and W are vector spaces over the field F, any one-one linear \ntransformation T of V onto W is called an iSOInorphism of V onto W. \nIf there exists an isomorphism of V onto W, we say that V is isomorphic \nto W. \nNote that V is trivially isomorphic to V, the identity operator being \nan isomorphism of V onto V. Also, if V is isomorphic to W via an iso­\nmorphism T, then W is isomorphic to V, because T-I is an isomorphism \nof W onto V. The reader should find it easy to verify that if V is iso­\nmorphic to W and W is isomorphic to Z, then V is isomorphic to Z. Briefly, \nisomorphism is an equivalence relation on the class of vector spaces. If \nthere exists an isomorphism of V onto W, we may sometimes say that V \nand W are isomorphic, rather than V is isomorphic to W. This will cause \nno confusion because V is isomorphic to W if and only if W is isomorphic \nto V.",
    "isomorphism is an equivalence relation on the class of vector spaces. If \nthere exists an isomorphism of V onto W, we may sometimes say that V \nand W are isomorphic, rather than V is isomorphic to W. This will cause \nno confusion because V is isomorphic to W if and only if W is isomorphic \nto V. \nTheorem 10. Every n-dimensional vector space over the field F is iso­\nmorphic to the space Fn. \nProof. Let V be an n-dimensional space over the field F and let \n<B = {al, . . .  , an} be an ordered basis for V. We define a function T \nSec. 3.3 \nIsomorphism \nfrom V into Fn, as follows: If a is in V, let Ta be the n-tuple (Xl, . . .  , Xn) \nof coordinates of a relative to the ordered basis il, i.e., the rHuple such \nthat \na = Xlal + . . .  + xnan. \nIn our discussion of coordinates in Chapter 2, we verified that this '1' is \nlinear, one-one, and maps V onto Fn. \nI \nFor many purposes one often regards isomorphic vector spaces as \nbeing 'the same,' although the vectors and operations in the spaces may \nbe quite different, that is, one often identifies isomorphic spaces. We \nshall not attempt a lengthy discussion of this idea at present but shall \nlet the understanding of isomorphism and the sense in which isomorphic \nspaces are 'the same' grow as we continue our study of vector spaces. \nWe shall make a few brief comments. Suppose T is an isomorphism \nof V onto W. If S is a subset of V, then Theorem 8 tells us that S is linearly \nindependent if and only if the set T(S) in W is independent. Thus in \ndeciding whether S is independent it doesn't matter whether we look at S \nor T(S). From this one sees that an isomorphism is 'dimension preserving,' \nthat is, any finite-dimensional subspace of V has the same dimension as its \nimage under T. Here is a very simple illustration of this idea. Suppose A \nis an m X n matrix over the field F. We have really given two definitions \nof the solution space of the matrix A.  The first is the set of all n-tuples",
    "image under T. Here is a very simple illustration of this idea. Suppose A \nis an m X n matrix over the field F. We have really given two definitions \nof the solution space of the matrix A.  The first is the set of all n-tuples \n(Xl, . . .  , xn) in Fn which satisfy each of the equations in the system AX \nO. The second is the set of all n X 1 column matrices X such that AX = O. \nThe first solution space is thus a subspace of Fn and the second is a subspace \nof the space of all n X 1 matrices over F. Now there is a completely \nobvious isomorphism between Fn and FnxI, namely, \n(x\" \n, x.) ʌ DJ \nUnder this isomorphism, the first solution space of A is carried onto the \nsecond solution space. These spaces have the same dimension, and so \nif we want to prove a theorem about the dimension of the solution space, \nit is immaterial which space we choose to discuss. In fact, the reader \nwould probably not balk if we chose to identify Fn and the space of n X 1 \nmatrices. We may do this when it is convenient, and when it is not con­\nvenient we shall not. \nExercises \n1. Let V be the set of complex numbers and let F be the field of real numbers. \nWith the usual operations, V is a vector space over F. Describe explicitly an iso­\nmorphism of this space onto R2. \n85 \n86 \nLinear Transformations \nChap. 3 \n2. Let V be a vector space over the field of complex numbers, and suppose there \nis an isomorphism T of V onto C3. Let ai, a2, a3, a4 be vectors in V such that \nTal \n(1, 0, i), \nTa2 \n(-2, 1 + i, 0), \nTaa \n(- 1 ,  1, 1), \nTa4 = (V2, i, 3). \n(a) Is al in the subspace spanned by a2 and aa? \n(b) Let WI be the subspace spanned by al and a2, and let W2 be the subspace \nspanned by a3 and a4. What is the intersection of WI and W2? \n(c) Find a basis for the subspace of V spanned by the four vectors ai' \n3. Let W be the set of all 2 X 2 complex Hermitian matrices, that is, the set \nof 2 X 2 complex matrices A such that Aij \nA ii (the bar denoting complex",
    "spanned by a3 and a4. What is the intersection of WI and W2? \n(c) Find a basis for the subspace of V spanned by the four vectors ai' \n3. Let W be the set of all 2 X 2 complex Hermitian matrices, that is, the set \nof 2 X 2 complex matrices A such that Aij \nA ii (the bar denoting complex \nconjugation). As we pointed out in Example 6 of Chapter 2, W is a vector space \nover the field of real numbers, under the usual operations. Verify that \n[I + x \ny + iZ] \n(x, y, z, t) --> \n• \nt _ x \ny \nu \nis an isomorphism of R4 onto W. \n4. Show that FmXn is isomorphic to Fmn. \n5. Let V be the set of complex numbers regarded as a vector space over the \nfield of real numbers (Exercise 1). We define a function T from V into the space \nof 2 X 2 real matrices, as follows. If z = x + iy with x and y real numbers, then \nT(z) = [X + 7y 5Y ]. \n- lOy \nx - 7y \n(a) Verify that T is a one-one (real) linear transformation of V into the \nspace of 2 X 2 real matrices. \n(b) Verify that T(ZIZ2) = T(ZI) T(Z2). \n(c) How would you describe the range of T? \n6. Let V and W be finite-dimensional vector spaces over the field F. Prove that \nV and W are isomorphic if and only if dim V \ndim W. \n7. Let V and W be vector spaces over the field F and let U be an isomorphism \nof Y onto W. Prove that T -+ UTU-I is an isomorphism of L(V, 1') onto L(W, W). \n3.4. Representation of Transformations \nby Matrices \nLet V be an n-dimensional vector space over the field F and let W \nbe an m-dimensional vector space over F. Let il = {al, . . .  , an} be an \nordered basis for V and il' \n{13b .\n. . , 13m} an ordered basis for W. If T \nis any linear transformation from V into W, then T is determined by its \naction on the vectors aj. Each of the n vectors Taj is uniquely expressible \nas a linear combination \n(3-3) \nm \nTaj = 7 Aiif3i \n; - 1  \nSec. 3.4 \nRepresentation of Transformations by Matrices \nof the (3i, the scalars Ali. . . .  , Ami being the coordinates of 1'aj in the",
    "action on the vectors aj. Each of the n vectors Taj is uniquely expressible \nas a linear combination \n(3-3) \nm \nTaj = 7 Aiif3i \n; - 1  \nSec. 3.4 \nRepresentation of Transformations by Matrices \nof the (3i, the scalars Ali. . . .  , Ami being the coordinates of 1'aj in the \nordered basis il'. Accordingly, the transformation l' is determined by \nthe mn scalars A ij via the formulas (3-3). The m X n matrix A defined \nby A (i, j) \n= Aij is called the matrix of l' relative to the pair of ordered \nbases ill and il'. Our immediate task is to understand explicitly how \nthe matrix A determines the linear transformation 1'. \nIf a = xlal + . . .  + Xnan is a vector in V, then \n'I'a = 'I' (.\u000f Xjaj) \n)=1 \nn \nY Xj(Taj) \nj=l \nn \nm \nY Xj Y A ij(3i \nj=l \ni=l \nIf X is the coordinate matrix of a in the ordered basis il, then the com­\nputation above shows that AX is the coordinate matrix of the vector Ta \nin the ordered basis il', because the scalar \nn \nY A ijxj \nj=l \nis the entry in the ith row of the column matrix A X. Let us also observe \nthat if A is any m X n matrix over the field F, then \n(3-4) \nT (.\u000f Xiai) \n./ (.ȕ A iiXi) f3i \n)=1 \n,=1 ) =1 \ndefines a linear transformation T from V into W, the matrix of which is \nA, relative to il, ill'. We summarize formally : \nJ'heorem 11. Let V be an n-dimensional veetor space over the field F \nand W an m-dimensional vector space over F. Let il be an ordered basis for \nV and il' an ordered basis for W. For each linear transformation T from V \ninto W, there is an m X n matrix A with entries in F such that \n[Tali\\' = A[a]ffi \nfor every vector a in V. Furthermore, T -t A is a one-one correspondence \nbetween the set of all linear tran,ƻformations from V into W and the set of \nall m X n matrices over the field F. \nThe matrix A which is associated with l' in Theorem 1 1  is called the \nmatrix of l' relative to the ordered bases ill, il'. Note that Equation \n(3-3) says that A is the matrix whose columns A!, . . .  , An are given by",
    "all m X n matrices over the field F. \nThe matrix A which is associated with l' in Theorem 1 1  is called the \nmatrix of l' relative to the ordered bases ill, il'. Note that Equation \n(3-3) says that A is the matrix whose columns A!, . . .  , An are given by \nj = 1, . . . , n. \n87 \n88 \nLinear Transformations \nChap. 3 \nIf U is another linear transformation from V into W and B = [BI' . . .  , Bn] \nis the matrix of U relative to the ordered bases il, il' then cA + B is the \nmatrix of cT + U relative to il, il'. That is clear because \ncA, + Bj = c[TaiJru' + [UaiJru, \n[cTaj + Uaj]ru' \n= [(cT + U)aj]ru,. \nTheorem 12. Let V be an n-dimensional vector space over the field F \nand let W be an m-dimensional vedor space over F. For each pair of ordered \nbases il, il' for V and W respectively, the junction which assigns to a linear \ntransformation T its matrix relative to il, il' is an isomorphism between the \nspace L(V, W) and the space of all m X n matrices over the field F. \nProof. We observed above that the function in question is linear, \nand as stated in Theorem 11, this function is one-one and maps L(V, W) \nonto the set of m X n matrices. I \nWe shall be particularly interested in the representation by matrices \nof linear transformations of a space into itself, i.e., linear operators on a \nspace V. In this CtiSe it is most convenient to use the same ordered basis \nin each case, that is, to take il \nil'. We shall then call the representing \nmatrix simply the matrix of T relative to the ordered basis il. Since \nthis concept will be so important to us, we shall review its definition. If T \nis a linear operator on the finite-dimensional vector space V and il = \n{aI, . . .  , an} is an ordered basis for V, the matrix of T relative to il (or, the \nmatrix of T in the ordered basis il) is the n X n matrix A whose entries \nAij are defined by the equations \n(3-5) \nn \nTaj = 2\": A;jai, \ni=l \nj = 1, . . . , n. \nOne must always remember that this matrix representing T depends upon",
    "matrix of T in the ordered basis il) is the n X n matrix A whose entries \nAij are defined by the equations \n(3-5) \nn \nTaj = 2\": A;jai, \ni=l \nj = 1, . . . , n. \nOne must always remember that this matrix representing T depends upon \nthe ordered basis il, and that there is a representing matrix for 7' in each \nordered basis for V. (For transformations of one space into another the \nmatrix depends upon two ordered bases, one for V and one for W.) In order \nthat we shall not forget this dependence, we shall use the notation \n[T]ru \nfor the matrix of the linear operator T in the ordered basis il. The manner \nin which this matrix and the ordered basis describe T is that for each a in V \nEXAMPLE 13. Let V be the space of n X 1 column matrices over the \nfield F; let W be the space of m X 1 matrices over F; and let A be a fixed \nm X n matrix over F. Let T be the linear transformation of V into W \ndefined by T(X) = AX. Let ill be the ordered basis for V analogous to the \nSec. 3.4 \nRepresentation of Transformations by Matrices \nstandard basis in Fn, i.e., the ith vector in (B in the n X 1 matrix Xi with \na 1 in row i and all other entries O. Let (B' be the corresponding ordered \nbasis for W, i.e., thejth vector in (B' is the m X 1 \nmatrix Yj with a 1 \nin row \nJ' and all other entries O. Then the matrix of T relative to the pair (B, (B' is \nthe matrix A itself. This is clear because the matrix AX! is the jth column \nof A .  \nEXAMPLE 14. Let F be a field and let T be the operator on F2 defined by \nT(x!, X2) \n= (Xl, 0). \nIt is easy to see that T is a linear operator on F2. Let (B be the standard \nordered basis for F2, (B \n= {EI' E2} . Now \nTEl = T(l, 0) (1, 0) \n= lei 0€2 \nTE2 \nT(O, 1) \n= (0, 0) \n= OEI \nOE2 \nso the matrix of T in the ordered basis (B is \n[TJm = [9 3l \nEXAMPLE 15. Let V be the space of all polynomial functions from R \ninto R of the form \nf(x) = Co + CIX + C2X2 + Caxa \nthat is, the space of polynomial functions of degree three or less. The",
    "= lei 0€2 \nTE2 \nT(O, 1) \n= (0, 0) \n= OEI \nOE2 \nso the matrix of T in the ordered basis (B is \n[TJm = [9 3l \nEXAMPLE 15. Let V be the space of all polynomial functions from R \ninto R of the form \nf(x) = Co + CIX + C2X2 + Caxa \nthat is, the space of polynomial functions of degree three or less. The \ndifferentiation operator D of Example 2 maps V into V, since D is 'degree \ndecreasing.' Let (B be the ordered basis for V consisting of the four functions \nfl' f2, fa, h defined by hex) = xH. Then \n(Dfl) (X) = 0, \nDfl = 0fI + Of2 + Ofa + Oh \n(Dfa)(x) = \n1, \nDfa \n= 1f1 + Of2 + 0fa \nOh \n(Dfa) (x) \n2x, \nDfs \n% \n2fa + Ofs + Oh \n(Df4)(X) = 3x2, \nDf4 = Ofl + Of2 + 3f3 + Oh \nso that the matrix of D in the ordered basis (B is \n[0 1 0 0] \n° ° 2 0 \n[DJm = \n° 0 ° 3 \n. \no 0 0 0 \nWe have seen what happens to representing matrices when transfor­\nmations are added, namely, that the matrices add. We should now like \nto ask what happens when we compose transformations. More specifically, \nlet V, W, and Z be vector spaces over the field F of respective dimensions \nn, m, and p. Let T be a linear transformation from V into W and U a linear \ntransformation from W into Z. Suppose we have ordered bases \n(B \n= {ai, . . . , an}, \n(B' = {131, . . .  , 13m}, \n(B\" \n= hh ' . . , 'j'p} \n89 \n90 \nLinear Transformations \nChap. 3 \nfor the respective spaces V, W, and Z. Let A be the matrix of T relative \nto the pair CB, CB' and let B be the matrix of U relative to the pair CB', CB\". \nIt is then easy to see that the matrix C of the transformation UT relative \nto the pair CB, CB\" is the product of B and A ;  for, if a is any vector in V \n[TaJ(\\,\\, \nA [aJ(\\ \n[U(Ta)J(\\\" = B[TaJ(\\, \nand so \n[(UT) (0')](\\\" \nBA[a]m \nand hence, by the definition and uniqueness of the representing matrix, \nwe must have C = BA. One can also see this by carrying out the computa­\ntion \n(UT)(aj) \nso that we must have \nm \np \nz Akj z Bik'Yi \nk = l  \ni = l  \nm \n(3-6) \nCij = z BikAkj. \nk = l",
    "and so \n[(UT) (0')](\\\" \nBA[a]m \nand hence, by the definition and uniqueness of the representing matrix, \nwe must have C = BA. One can also see this by carrying out the computa­\ntion \n(UT)(aj) \nso that we must have \nm \np \nz Akj z Bik'Yi \nk = l  \ni = l  \nm \n(3-6) \nCij = z BikAkj. \nk = l  \nWe motivated the definition (3-6) of matrix multiplication via operations \non the rows of a matrix. One sees here that a very strong motivation for \nthe definition is to be found in composing linear transformations. Let us \nsummarize formally. \nTheorem 13. Let V, W, and Z be finite-dimensional vector spaces over \nthe field F; let T be a linear transformation from V into W and U a linear \ntransformation from W into Z. If CB, CB', and CB\" are ordered bases for the \nspaces V, W, and Z, respectively, if A is the matrix of T relative to the pair \nCB, CB', and E is the matrix of U relative to the pair CB', CB\", then the matrix \nof the composition UT relative to the pair CB, CB\" is the product matrix C \nEA. \nWe remark that Theorem 13 gives a proof that matrix multiplication \nis associative--a proof which requires no calculations and is independent \nof the proof we gave in Chapter 1. We should also point out that we proved \na special case of Theorem 13 in Example 12. \nIt is important to note that if T and U are linear operators on a \nspace V and we are representing by a single ordered basis il, then Theorem \n13 assumes the simple form [UTJm = [U]<l\\[7'](\\. Thus in this case, the \nSec. 3.4 \nRepresentation of Transformations by Matrices \ncorrespondence which ffi determines between operators and matrices is not \nonly a vector space isomorphism but also preserves products. A simple \nconsequence of this is that the linear operator T is invertible if and only if \n[TJ<1\\ is an invertible matrix. For, the identity operator I is represented by \nthe identity matrix in any ordered basis, and thus \nis equivalent to \nUT \nTU \nI \n[U]<1\\[T]<1\\ = [T]<1\\[U]<1\\ = I. \nOf course, when T is invertible",
    "[TJ<1\\ is an invertible matrix. For, the identity operator I is represented by \nthe identity matrix in any ordered basis, and thus \nis equivalent to \nUT \nTU \nI \n[U]<1\\[T]<1\\ = [T]<1\\[U]<1\\ = I. \nOf course, when T is invertible \n[T-l]<1\\ = [T]ci1• \nNow we should like to inquire what happens to representing matrices \nwhen the ordered basis is changed. For the sake of simplicity, we shall \nconsider this question only for linear operators on a space V, so that we \ncan use a single ordered basis. The specific question is this. Let T be a \nlinear operator on the finite-dimensional space V, and let \nffi = {al, . . .  , an} \nand ffi' = {a:, . . .  , a²} \nbe two ordered bases for V. How are the matrices [TJ<1\\ and [T]<1\\1 related? \nAs we observed in Chapter 2, there is a unique (invertible) n X n matrix P \nsuch that \n(3-7) \nfor every vector a in V. It is the matrix P = [PI, . . . , Pn] where Pi = \n[0';]<1\\. By definition \n(3-8) \nApplying (3-7) to the vector TO', we have \n(3-9) \nCombining (3-7), (3-8), and (3-9), we obtain \n[T](\\lP[aJ(\\l1 = P[TaJ(\\l1 \nor \nand so it must be that \n(3-10) \nThis answers our question. \nBefore stating this result formally, let us observe the following. There \nis a unique linear operator U which carries ffi onto ffi', defined by \nJ \n1, .\n. . , n. \nThis operator U is invertible since it carries a basis for V onto a basis for \n91 \n92 \nLinear Transformations \nChap. 3 \nV. The matrix P (above) is precisely the matrix of the operator U in the \nordered basis (B. For, P is defined by \nn \naj = 7 Pijai \ni = l  \nand since U aj = aj, this equation can be written \nn \nUaj = Y Pijai. \ni = l  \nSo P = [UJCl, by definitioll. \nTheorem 14. Let V be a finite-dimensional vector space over the field F, \nand let \n(B = {a\" . . .  , an} and <B' = {ai, . . .  , a} \nbe ordered bases for V. Suppose T is a linear operator on V. If P = \nPn] is the n X n matrix with columns Pj \n[ajJm, then \n[T]m, \nP-I[TJClP. \nAlternatively, if U is the invertible operator on V defined by Uaj",
    "and let \n(B = {a\" . . .  , an} and <B' = {ai, . . .  , a} \nbe ordered bases for V. Suppose T is a linear operator on V. If P = \nPn] is the n X n matrix with columns Pj \n[ajJm, then \n[T]m, \nP-I[TJClP. \nAlternatively, if U is the invertible operator on V defined by Uaj \nai, j \n1, .\n.\n.\n , Il, then \nEXAMPLE 16. Let T be the linear operator on R2 defined by T(XI, X2) = \n(Xl, 0). In Example 14 we showed that the matrix of T in the standard \nordered basis (B = {EI' E2} is \n[TJCl = [\u0006 ul \nSuppose (B' is the ordered basis for R2 consisting of the vectors e; = (1, 1), \nE2 = (2, 1). Then \nso that P is the matrix \nBy a short computation \nThus \ne; = tl + t2 \nE৞ = 2El + E2 \nP = G ql \np-l = [-v :J \n1 \n= [ 1 \n= [ Ȕ \nȡJ [\u0006 uJ [v :J \n-:J [\u0006 ȢJ \n-ȓJ \nSec. 3.4 \nRepresentation of Transformations by Matrices \nWe can easily check that this is correct because \nTei = (1,0) \n= \n- ei + e! \nTE\u001d = (2, 0) \n== \n-2\"i + #\u001d. \nEXAMPLE 17. Let V be the space of polynomial functions from R into \nR which have 'degree' less than or equal to 3. As in Example 15, let D be \nthe differentiation operator on V, and let \nCB = {h, f2, fa, f4} \nbe the ordered basis for V defined by fi(X) = Xi-I. Let t be a real number \nand define g;(x) = (x \n+ t)i-l, that is \ngi = fl \ng2 = tfl + f2 \nga = t2fI + 2tf2 + fa \ng4 \n= t3fI + 3t2f2 + 3tfa + f4. \nSince the matrix \np \ne l \nt t2 \n\"J \n1 2t 3t2 \n0 1 \n3t \n0 0 1 \nis easily seen to be invertible with \nIH\nؐ [H \n-t t2 \n- ''] \n1 -2t 3t2 \n0 1 -3t \n0 0 1 \nit follows that CB' = {gI, g2, ga, g4} is an ordered basis for V. In Example 15, \nwe found that the matrix of D in the ordered basis CB is \nlo 1 0 O\nJ \no 0 2 0 \n[DJl == 0 0 0 3 . \no 0 0 0 \nThe matrix of D in the ordered basis CB' is thus \n[\n1 -, \nP-l[DJmP = 3 9 \no 0 \ne [ \n-t 1 0 0 \n­ [J \n1 0 0 0 \nt2 -2t 1 0 t2 -2t 1 0 \n0 \nn \n2 0 0 \n'I \n3t2 0 \n-3t 0 \n1 0 \n'I \n3t2 0 \n-3t 0 \n1 0 \n1 0 0 2 0 0 0 0 1 2t 0 2 0 0 0 0 \nmI \nt t2 \n''J \n1 2t 3t2 \n0 1 3t \n0 0 1 \n3\n''] 6t 3 0 \n93 \n94 \nLinear Tranlformations \nChap. S",
    "[\n1 -, \nP-l[DJmP = 3 9 \no 0 \ne [ \n-t 1 0 0 \n­ [J \n1 0 0 0 \nt2 -2t 1 0 t2 -2t 1 0 \n0 \nn \n2 0 0 \n'I \n3t2 0 \n-3t 0 \n1 0 \n'I \n3t2 0 \n-3t 0 \n1 0 \n1 0 0 2 0 0 0 0 1 2t 0 2 0 0 0 0 \nmI \nt t2 \n''J \n1 2t 3t2 \n0 1 3t \n0 0 1 \n3\n''] 6t 3 0 \n93 \n94 \nLinear Tranlformations \nChap. S \nThus D is represented by the same matrix in the ordered bases CB and CB'. \nOf course, one can see this somewhat more directly since \nDg1 \n0 \nDg2 = gl \nDga = 2g2 \nDg4 \n3ga. \nThis example illustrates a good point. If one knows the matrix of a linear \noperator in some ordered basis CB and wishes to find the matrix in another \nordered basis CB', it is often most convenient to perform the coordinate \nchange using the invertible matrix P; however, it may be a much simpler \ntask to find the representing matrix by a direct appeal to its definition. \nDefinition. Let A and B be n X n (square) matrices over the field F. \nWe say that B is shnilar to A over F if there is an invertible n X It matrix \nP over F such that B = P-IAP. \nAccording to Theorem 14, we have the following: If V is an n-dimen­\nsional vector space over F and CB and CB' are two ordered bases for V, \nthen for each linear operator T on V the matrix B = [T](J,\"\\' is similar to \nthe matrix A = [TJ(J,I. The argument also goes in the other direction. \nSuppose A and B are n X n matrices and that B is similar to A .  Let \nV be any n-dimensional space over F and let CB be an ordered basis for V. \nLet T be the linear operator on V which is represented in the basis CB by \nA. If B \nP-IAP, let CB' be the ordered basis for V obtained from CB by P, \nI.e., \nn \nex} = 2; P ijai­\ni-1 \nThen the matrix of T in the ordered basis CB' will be B. \nThus the statement that B is similar to A means that on each n­\ndimensional space over F the matrices A and B represent the same linear \ntransformation in two (possibly) different ordered bases. \nNote that each n X n matrix A is similar to itself, using P\nI; if",
    "Thus the statement that B is similar to A means that on each n­\ndimensional space over F the matrices A and B represent the same linear \ntransformation in two (possibly) different ordered bases. \nNote that each n X n matrix A is similar to itself, using P\nI; if \nB is similar to A, then A is similar to B, for B = P-IAP implies that \nA = (P-l)-lBP-l; if B is similar to A and C is similar to B, then C is similar \nto A, for B \nP-IAP and C = Q-IBQ imply that C = (PQ)-lA(PQ). \nThus, similarity is an equivalence relation on the set of n X n matrices \nover the field F. Also note that the only matrix similar to the identity \nmatrix I is I itself, and that the only matrix similar to the zero matrix is \nthe zero matrix itself. \nSec. 3.4 \nRepresentation of Transformations by Matrices \nExercises \n1 .  Let T be the linear operator on C2 defined by T(Xl' X2) = (Xl, 0). Let CB be \nthe standard ordered basis for C2 and let CB' \n{at, a2} be the ordered basis defined \nby at = (1, i), a2 = (-i, 2). \n(a) What is the matrix of T relative to the pair CB, CB'? \n(b) What is the matrix of T relative to the pair CB', CB? \n(c) What is the matrix of T in the ordered basis CB'? \n(d) What is the matrix of T in the ordered basis {a2, al} ? \n2. Let T be the linear transformation from R3 into R2 defined by \nT(Xl, X2, Xa) \n(Xl + X2, 2xa - XI)' \n(a) If CB is the standard ordered basis for RS and CB' is the standard ordered \nbasis for R2, what is the matrix of T relative to the pair il, CB'? \n(b) If CB \n{ai, a2, as} and CB' \n{{31, {32}, where \nal \n(1, 0, - 1), a2 \n(1, 1, 1), a3 \n(1, 0, 0), {3l \n(0, 1), {32 \n(1, 0) \nwhat is the matrix of T relative to the pair il, ill\"? \n3. Let T be a linear operator on Fn, let A be the matrix of T in the standard \nordered basis for Fn, and let W be the subspace of Fn spanned by the column \nvectors of A. What does W have to do with T? \n4. Let V be a two-dimensional vector space over the field F, and let CB be an \nordered basis for V. If T is a linear operator on V and",
    "ordered basis for Fn, and let W be the subspace of Fn spanned by the column \nvectors of A. What does W have to do with T? \n4. Let V be a two-dimensional vector space over the field F, and let CB be an \nordered basis for V. If T is a linear operator on V and \n[TJili = [; =J \nprove that T2 - (a + d) T + (ad - be)! = O. \n5. Let T be the linear operator on R3, the matrix of which in the standard ordered \nbasis is \nA = [ ȭ < ZJ' \n- 1  3 4 \nFind a basis for the range of T and a basis for the null space of T. \n6. Let T be the linear operator on R2 defined by \nT(XI' X2) \n(-X2, XI)' \n(a) What is the matrix of T in the standard ordered basis for R2? \n(b) What is the matrix of T in the ordered basis CB = {ai, a2} , where al = (1, 2) \nand a2 = (1, - I)? \n(c) Prove that for every real number e the operator (T - cI) is invertible. \n(d) Prove that if CB is any ordered basis for R2 and [TJili \nA, then A12A21 Z O. \n7. Let T be the linear operator on R3 defined by \nT(XI' X2, Xa) = (3Xl + Xa, -2xI + X2, -Xl + 2X2 + 4Xa). \n(a) What is the matrix of T in the standard ordered basis for R3? \n95 \n96 \nLinear Transformations \n(b) What is the matrix of T in the ordered basis \n{ai, az, aa} \nwhere al \n(1, 0, 1), az = (-1, 2, 1), and aa \n(2, 1, I)? \nOhap. 3 \n(c) Prove that T is invertible and give a rule for T-I like the one which de­\nfines T. \n8. Let () be a real number. Prove that the following two matrices are similar \nover the field of complex numbers: \n[COS 0 \n-sin OJ, [eie \n0 J \nsin ° \ncos () \n0 \ne-i9 \n(Hint: Let T be the linear operator on C2 which is represented by the first matrix \nin the standard ordered basis. Then find vectors al and a2 such that Tal = elear, \nTaz = e-i8a2, and {ai, a2} is a basis.) \n9. Let V be a finite-dimensional vector space over the field P and let 8 and T \nbe linear operators on V. We ask: When do there exist ordered bases (B and (B' \nfor V such that [8]ClI = [TJClI'? Prove that such bases exist if and only if there is",
    "Taz = e-i8a2, and {ai, a2} is a basis.) \n9. Let V be a finite-dimensional vector space over the field P and let 8 and T \nbe linear operators on V. We ask: When do there exist ordered bases (B and (B' \nfor V such that [8]ClI = [TJClI'? Prove that such bases exist if and only if there is \nan invertible linear operator U on V such that T \nU8U-I, (Outline of proof: \nIf [8]ClI = [T]ClI', let U be the operator which earries (B onto (B' and show that \n8 = UTU-I. Oonversely, if T = U8U-l for some invertible U, let (B be any \nordered basis for V and let (B' be its image under U. Then show that [8]ClI \n[T]ClI'.) \n10. We have seen that the linear operator T on R2 defined by T(Xl, X2) = (Xl, 0) \nis represented in the standard ordered basis by the matrix \nA = [; wl \nThis operator satisfies T2 = T. Prove that if 8 is a linear operator on R2 such that \n82 = 8, then 8 = 0, or 8 = I, or there is an ordered basis (B for R2 such that \n[8]m = A (above). \nn. Let W be the space of all n X 1 column matrices over a field P. If A is an \nn X n matrix over P, then A defines a linear operator LA on W through left \nmultiplication : LA(X) \nAX. Prove that every linear operator on W is left multi­\nplication by some n X n matrix, Le., is LA for some A. \nNow suppose V is an n-dimensional vector space over the field P, and let (B \nbe an ordered basis for V. For each a in V, define Ua \n[a]ClI. Prove that U is an \nisomorphism of V onto W. If T is a linear operator on V, then UTU-l is a linear \noperator on W. Accordingly, UTU-l is left multiplication by some n X n matrix A. \nWhat is A? \n12. Let V be an n-dimensional vector space over the field P, and let (B \n{ai, . . .  , an} be an ordered basis for V. \n(a) According to Theorem 1, there is a unique linear operator T on V such that \nj = 1, . . .  , n - 1, \nWhat is the matrix A of T in the ordered basis (B? \n(b) Prove that Tn = 0 but Tn-l =r!' O. \nTan = O. \n(c) Let 8 be any linear operator on V such that 8n = 0 but 8n-l =r!' O. Prove",
    "(a) According to Theorem 1, there is a unique linear operator T on V such that \nj = 1, . . .  , n - 1, \nWhat is the matrix A of T in the ordered basis (B? \n(b) Prove that Tn = 0 but Tn-l =r!' O. \nTan = O. \n(c) Let 8 be any linear operator on V such that 8n = 0 but 8n-l =r!' O. Prove \nthat there is an ordered basis (B' for V such that the matrix of 8 in the ordered \nbasis (B' is the matrix A of part (a). \nSec. 3.5 \nLinear Functionals \nCd) Prove that if M and N are n X n matrices over F such that M\" = N\" = 0 \nbut M,,-l \"\" 0 \"\" Nn-l, then M and N are similar. \n13. Let V and W be finite-dimensional vector spaees over the field F and let T \nbe a linear transformation from V into W. If \n(B \n{ai, . . . , an} \nand (B' \n{{31, •\n.\n•\n , 13m} \nare ordered bases for V and W, respectively, define the linear transformations EM \nas in the proof of Theorem 5 :  Ep,q(ai) = Oiq{3P' Then the EM, 1 ::; P ::; m, \n1 ::; q ::; n, form a basis for L(V, W), and so \nm \n11 \nT \n7 \n7 ApqEp,q \npؑl q=l \nfor certain scalars Apq (the coordinates of T in this basis for L(V, W» . Show that \nthe matrix A with entries A(p, q) \n= Apq is precisely the matrix of T relative to \nthe pair (B, (B'. \n97 \n3.5. Linear Functionals \nIf V is a vector space over the field F, a linear transformationf from V \ninto the scalar field F is also called a linear functional on V. If we start \nfrom scratch, this means that f is a function from V into F such that \nf(ca + (3) = cf(a) + f«(3) \nfor all vectors a and (3 in V and all scalars c in F. The concept of linear \nfunctional is important in the study of finite-dimensional spaces because \nit helps to organize and clarify the discussion of subspaces, linear equations, \nand coordinates. \nEXAMPLE 18. Let F be a field and let al, . . . , an be scalars in F. Define \na function f on Fn by \nf(Xh . . . , xn) = alXt + . . .  + anxn• \nThen f is a linear functional on FT!. It is the linear functional which is \nrepresented by the matrix [at ' \"  an] relative to the standard ordered",
    "EXAMPLE 18. Let F be a field and let al, . . . , an be scalars in F. Define \na function f on Fn by \nf(Xh . . . , xn) = alXt + . . .  + anxn• \nThen f is a linear functional on FT!. It is the linear functional which is \nrepresented by the matrix [at ' \"  an] relative to the standard ordered \nbasis for Fn and the basis {I} for F :  \naj \nf(€,), \nJ \n1 ,  . . .  , n. \nEvery linear functional on Fn is of this form, for some scalars ai, . . . , an. \nThat is immediate from the definition of linear functional because we define \naj = f( E;) and use the linearity \nf(Xl, .\n. . , xn) = f (:1 XjEj) \n= 7 xif(€j) \nj \n98 \nLinear Transformations \nChap. 3 \nEXAMPLE 19. Here is an important example of a linear functional. \nLet n be a positive integer and F a field. If A is an n X n matrix with \nentries in F, the trace of A is the scalar \ntr A = An + A22 + . . .  + Ann. \nThe trace function is a linear functional on the matrix space FnXn because \nn \ntr (cA + B) = k (cA;; + Bii) \ni-I \nn \nn \n= C k A;; + k B;i \ni-I \ni-I \n= c tr A + tr B. \nEXAMPLE 20. Let V be the space of all polynomial functions from the \nfield F into itself. Let t be an element of F. If we define \nLt(p) = pet) \nthen Lt is a linear functional on V. One usually describes this by saying \nthat, for each t, 'evaluation at t' is a linear functional on the space of \npolynomial functions. Perhaps we should remark that the fact that the \nfunctions are polynomials plays no role in this example. Evaluation at t \nis a linear functional on the space of all functions from P into P. \nEXAMPLE 21. This may be the most important linear functional in \nmathematics. Let [a, bJ be a closed interval on the real line and let C([a, bJ) \nbe the space of continuous real-valued functions on [a, bJ. Then \nL(g) = lab get) dt \ndefines a linear functional L on C([a, bJ). \nIf V is a vector space, the collection of all linear functionals on V \nforms a vector space in a natural ,yay. It is the space L(V, F). We denote",
    "be the space of continuous real-valued functions on [a, bJ. Then \nL(g) = lab get) dt \ndefines a linear functional L on C([a, bJ). \nIf V is a vector space, the collection of all linear functionals on V \nforms a vector space in a natural ,yay. It is the space L(V, F). We denote \nthis space by V* and call it the dual space of V :  \nV* = L(V, F). \nIf V is finite-dimensional, we can obtain a rather explicit description \nof the dual space V*. From Theorem 5 we know something about the \nspace V*, namely that \ndim V* = dim V. \nLet CB \n{aI, . . .  , an} be a basis for V. According to Theorem 1, there \nis (for each i) a unique linear functional Ii on V such that \n(3-11) \nIn this way we obtain from CB a set of n distinct linear functionalsib . . .  , In \non V. These functionals are also linearly independent. For, suppose \nSec. 3.5 \n(3-12) \nThen \nLinear Functionals \nCj. \nIn particular, if f is the zero functional, f(aj) = 0 for each j and hence \nthe scalars Cj are all O. Now fl' \n•\n•\n•\n , fn are n linearly independent func­\ntionals, and since we know that V* has dimension n, it must be that \nCB* = {!I, . . .  , fn} is a basis for V*. This basis is called the dual basis \nof CB. \nTheorem 15. Let V be a finite-dimensional vector space over the field F, \nand let CB = {aI, . . .  , an} be a basis for V. Then there is a unique dual \nbasis CB* = {fl' . . .  , fn} for V* such that f;(aj) = Olj. For each linear func-\ntional f on V we have \nn \n(3-13) \nf = 7 f(al)£; \ni = l  \nand for each vector a in V we have \n(3-14) \nn \na \n7 fl(a)aj. \ni = l  \nProof. We have shown above that there is a unique basis which is \n'dual' to CB. Iff is a linear functional on V, thenf is some linear combination \n(3-12) of the!;, and as we observed after (3-12) the scalars Cj must be given \nby Cj = f(aj). Similarly, if \nis a vector in V, then \nn \na = \n7 Xiai \n;=1 \nn \nfiCa) = 7 Xi!;(ai) \ni = l  \nn \n= Y XiOij \nire:; 1 \n= Xj \nso that the unique expression for a as a linear combination of the ai is \nn \na",
    "(3-12) of the!;, and as we observed after (3-12) the scalars Cj must be given \nby Cj = f(aj). Similarly, if \nis a vector in V, then \nn \na = \n7 Xiai \n;=1 \nn \nfiCa) = 7 Xi!;(ai) \ni = l  \nn \n= Y XiOij \nire:; 1 \n= Xj \nso that the unique expression for a as a linear combination of the ai is \nn \na \nY !.(a)ai. I \ni 0 l  \nEquation (3-14) provides us with a nice way of describing what the \ndual basis is. It says, if CB = {al, ' . .  , an} is an ordered basis for V and \n99 \n100 \nLinear Transformations \nChap. 3 \nCB* = {fl, . . . , fn} is the dual basis, then fi is precisely the function \nwhich assigns to each vector a in V the ith coordinate of a relative to the \nordered basis CB. Thus we may also call the 1; the coordinate functions for \nCB. The formula (3-13), when combined with (3-14) tells us the following: \nIf f is in V*, and we let f(ai) = ai, then when \na = Xla! + . . . + x\"a\" \nwe have \n(3-15) \nIn other words, if we choose an ordered basis il for V and describe each \nvector in V by its n-tuple of coordinates (Xl, . . .  , x,,) relative to CB, then \nevery linear functional on V has the form (3-15). This is the natural \ngeneralization of Example 18, which is the special case V = Fn and il = \n{Ell '  . .  , En}. \nEXAMPLE 22. Let V be the vector space of all polynomial functions \nfrom R into R which have degree less than or equal to 2. Let tIl t21 and ta \nbe any three distinct real numbers, and let \nLi(P) \npet;). \nThen L1, L2, and La are linear functionals on V. These functionals are \nlinearly independent; for, suppose \nL = clLl + c2L2 + CaLa. \nIf L = 0, i.e., if L(p) = 0 for each p in V, then applying L to the particular \npolynomial 'functions' 1, x, X2, we obtain \nCl + C2 + C3 = 0  \ntlCl + t2C2 + taca = 0 \ntt Cl \ntӦC2 + tӧC8 \n0 \nFrom this it follows that Cl \nC2 \nC3 \n0, because (as a short computation \nshows) the matrix \n1J \nts \ntӧ \nis invertible when tl, t2, and ts are distinct. Now the L; are independent,",
    "polynomial 'functions' 1, x, X2, we obtain \nCl + C2 + C3 = 0  \ntlCl + t2C2 + taca = 0 \ntt Cl \ntӦC2 + tӧC8 \n0 \nFrom this it follows that Cl \nC2 \nC3 \n0, because (as a short computation \nshows) the matrix \n1J \nts \ntӧ \nis invertible when tl, t2, and ts are distinct. Now the L; are independent, \nand since V has dimension 3, these functionals form a basis for V*. What \nis the basis for V, of which this is the dual? Such a basis {PI, P2, Pa} for V \nmust satisfy \nor \nPi(ti) = a;j. \nThese polynomial functions are rather easily seen to be \nSec. 3.5 \n( )  \n(x - t2) (x - ta) \nPI X = (tl - t2)(tl - ta) \n( ) \n(x - tl) (x - ta) \nP2 x = (t2 - h) (t2 - ta) \n( )  \n(x - h) (x - t2) \npa x = \n. \n(ta - tl)(ta - t2) \nLinear Functionals \nThe basis {Pl, P2, pa} for V is interesting, because according to (3-14) we \nhave for each P in V \nThus, if Cl, C2, and Ca are any real numbers, there is exactly one polynomial \nfunction p over R which has degree at most 2 and satisfies p(tj) = c}, j = \n1, 2, 3. This polynomial function is p = ClPl + C2P2 + Capa. \nNow let us discuss the relationship between linear functionals and \nsubspaces. If f is a non-zero linear functional, then the rank of f is 1 because \nthe range of f is a non-zero subspace of the scalar field and must (therefore) \nbe the scalar field. If the underlying space V is finite-dimensional, the rank \nplus nullity theorem (Theorem 2) tells us that the null space N, has \ndimension \ndim N, = dim V - 1. \nIn a vector space of dimension n, a subspace of dimension n - 1 is called \na hyperspace. Such spaces are sometimes called hyperplanes or subspaces \nof codimension 1. Is every hyperspace the null space of a linear functional? \nThe answer is easily seen to be yes. It is not much more difficult to show \nthat each d-dimensional subspace of an n-dimensional space is the inter­\nsection of the null spaces of (n - d) linear functionals (Theorem 16 below). \nDefinition. If V is a vector space over the field F and S is a subset of V,",
    "that each d-dimensional subspace of an n-dimensional space is the inter­\nsection of the null spaces of (n - d) linear functionals (Theorem 16 below). \nDefinition. If V is a vector space over the field F and S is a subset of V, \nthe annihilator of S is the set So of linear functionals f on V such that \nf(a) = 0 for every a in S. \nIt should be clear to the reader that So is a subspace of V*, whether \nS is a subspace of V or not. If S is the set consisting of the zero vector \nalone, then So = V*. If S = V, then SO is the zero subspace of V*. (This is \neasy to see when V is finite-dimensional.) \nTheorem 16. Let V be a finite-dimensional vector space over the field F, \nand let W be a subspace of V. Then \ndim W + dim WO = dim V. \nProof. Let k be the dimension of W and {al, . . . , ak} a basis for \nW. Choose vectors <Xk+l, •\n•\n•\n , an in V such that {al, . . .  , an} is a basis for \nV. Let {iI, . . . , fn} be the basis for V* which is dual to this basis for V. \n101 \n102 \nLinear Transformations \nChap. 3 \nThe claim is that {fk+1J . . .  , fn} is a basis for the annihilator Woo Certainly \nfi belongs to Wo for i :2: k + 1, because \nfi(aj) = /iij \nand /iij = 0 if i :2: k + 1 and} S; k;  from this it follows that, for i :2: k + 1 ,  \nfiCa) = 0 whenever a is a linear combination of al, . . .  , ak. The functionals \nfk+l' . . .  , fn are independent, so all we must show is that they span Woo \nSuppose f is in V*. Now \nn \nf \n8 f(ai)fi \n; = 1  \nso that if f is in WO we have f(ai) = 0 for i S; k and \nn \nf = \n8 f(ai)f;· \ni =k+ 1  \nWe have shown that if dim W \nk and dim V = n then dim WO = \nn \nk. I \nCorollary. If W is a k-dimensional subspace of an n-dimensional vector \nspace V, then W is the intersection of (n \nk) hyperspaces in V. \nProof. This is a corollary of the proof of Theorem 16 rather than \nits statement. In the notation of the proof, W is exactly the set of vectors a \nsuch that fiCa) = 0, i = k + 1, . . .  , n. In case k = n - 1, W is the null \nspace of fn. \nI",
    "k) hyperspaces in V. \nProof. This is a corollary of the proof of Theorem 16 rather than \nits statement. In the notation of the proof, W is exactly the set of vectors a \nsuch that fiCa) = 0, i = k + 1, . . .  , n. In case k = n - 1, W is the null \nspace of fn. \nI \nCorollary. If Wl and W2 are subspaces of a finite-dimensional vector \nspace, then Wl = W2 if and only if W? \nW2. \nProof. If WI = W2, then of course W7 \nW3. If WI ;;!. W2, then \none of the two subspaces contains a vector which is not in the other. \nSuppose there is a vector a which is in W2 but not in Wl. By the previous \ncorollaries (or the proof of Theorem 16) there is a linear functional f such \nthat f({3) \n0 for all (3 in W, but f(a) ;;!. O. Then J is in W$ but not in Wg \nand WY ;;!. Wg. \nI \nIn the next section we shall give different proofs for these two corol­\nlaries. The first corollary says that, if we select some ordered basis for the \nspace, each Ie-dimensional subspace can be described by specifying (n - k) \nhomogeneous linear conditions on the coordinates relative to that basis. \nLet us look briefly at systems of homogeneous linear equations from \nthe point of view of linear functionals. Suppose we have a system of linear \nequations, \nSec. 3.5 \nLinear Funetionals \nfor which we wish to find the solutions. If we let j;, i = 1, . . . , m, be the \nlinear functional on pn defined by \nfi(Xl, . . .  , xn) = A ilXl + . . .  + A inxn \nthen we are seeking the subspace of pn of all a such that \nfi(a) = 0, \ni = 1, .\n.\n. , m. \nIn other words, we are seeking the subspace annihilated by f1' .\n. . , f  m. \nRow-reduction of the coefficient matrix provides us with a systematic \nmethod of finding this subspace. The n-tuple (A il, .\n.\n.\n , Ain) gives the \ncoordinates of the linear functional fi relative to the basis which is dual \nto the standard basis for P\". The row space of the coefficient matrix may \nthus be regarded as the space of linear functionals spanned by iI, . . . , im.",
    ".\n.\n , Ain) gives the \ncoordinates of the linear functional fi relative to the basis which is dual \nto the standard basis for P\". The row space of the coefficient matrix may \nthus be regarded as the space of linear functionals spanned by iI, . . . , im. \nThe solution space is the subspace annihilated by this space of functionals. \nNow Olle may look at the system of equations from the 'dual' point \nof view. That is, suppose that we are given rn vectors in F\" \nai \n(A il, .\n.\n•\n , Ai,,) \nand we wish to find the annihilator of the subspace spanned by these \nvectors. Since a typical linear functional on pn has the form \nf(X1, . . .  , xn) = \nC1X1 + . . .  + CnXn \nthe condition that f be in this annihilator is that \nn \n7 Aijc; = 0, \nj= l  \ni = 1, \n. . .  , m \nthat is, that (el, . . .  , Cn) be a solution of the system AX O. From this \npoint of view, row-reduction gives us a systematic method of finding the \nannihilator of the subspace spanned by a given finite set of vectors in pn. \nEXAMPLE 23. Here are three linear functionals on R4: \nfl(Xl, X2, Xa, X4) \nXl + 2X2 + 2Xa \nX4 \nf2(Xl, X2, X3, X4) \n2X2 + X4 \n!a(Xl, X2, Xa, X4) = -2X1 - 4xa + 3X4. \nThe subspace which they annihilate may be found explicitly by finding the \nrow-reduced echelon form of the matrix \nA [ 1 2  \n° 2 \n-2 ° 2 IJ \n° 1 . \n- 4  3 \nA short calculation, or a peek at Example 21 of Chapter 2, shows that \n[1 ° 2 OJ \nR= O I 0 0 · \n0 0 0  1 \n103 \n104 \nLinear Transformations \nTherefore, the linear functionals \ngl(XI, X2, Xa, X4) = Xl + 2xa \ng2(XI, X2, Xa, X4) \nX2 \nga(XI, X2, Xa, X4) \nX4 \nChap. 3 \nspan the same subspace of (R4)* and annihilate the same subspace of R4 \nas do fl' f2' fa. The subspace annihilated consists of the vectors with \nXl = -2Xa \nX2 \nX4 =- o. \nEXAMPLE 24. Let W be the subspace of R5 which is spanned by the \nvectors \na1 \n(2, -2, 3, 4, - 1), \na2 \n(\n1, 1, 2, 5, 2), \nas = (0, 0, - 1, -2, 3) \na4 = (1, \n1, 2, 3, 0). \nHow does one describe WO, the annihilator of W? Let us form the 4 X 5",
    "Xl = -2Xa \nX2 \nX4 =- o. \nEXAMPLE 24. Let W be the subspace of R5 which is spanned by the \nvectors \na1 \n(2, -2, 3, 4, - 1), \na2 \n(\n1, 1, 2, 5, 2), \nas = (0, 0, - 1, -2, 3) \na4 = (1, \n1, 2, 3, 0). \nHow does one describe WO, the annihilator of W? Let us form the 4 X 5 \nmatrix A with row vectors aI, a2, as, a4, and find the row-reduced echelon \nmatrix R which is row-equivalent to A :  \nɲ [- \n-2 \n3 \nA \n1 \n2 \n0 \n- 1  \n- 1  \n2 \n4 \n5 \n2 \n3 \n- 1] [1 \n2 ---. R = \n0 \n3 \n0 \no \n0 \nIf f is a linear functional on R5: \n5 \nf(XI, . . .  , X5) \n2; CjXj \ni= 1 \n- 1  0 \n- 1  \nU \n0 1 \n2 \n0 0 \n0 \n0 0 \n0 \nthen ! is in WO if and only if !(IXi) = 0, i = 1, 2, 3, 4, i.e., if and only if \nThis is equivalent to \nor \n5 \n2; Aijc; \n0, \n; =1 \n5 \n7 RijCj = 0, \n; - 1  \n1 ȍ i ყ 4. \nCI - C2 - C4 = 0 \nCs + 2C4 = 0 \nC5 \nO. \nWe obtain all such linear functionals ! by assigning arbitrary values to \nC2 and C4, say C2 = a and C4 = b, and then finding the corresponding C1 \na + b, Cs = - 2b, C5 = O. So WO consists of all linear functionals f of the \nform \nSec. 3.5 \nLinear Functionals \nThe dimension of WO is 2 and a basis {iI, h} for WO can be found by first \ntaking a \n1, b \n° and then a \n0, b \n1 :  \nfl(xl, . . .  , XS) \nXl \nX2 \nh(XI, . . . , X5) \n= Xl - 2xa + X4. \nThe above general f in WO is f = afl + b!2. \n1. In R3, let al \n(1, 0, 1), a2 \n(0, 1, -2), as \n( - 1, - 1, 0). \n(a) If f is a linear functional on Ra such that \nf(al) = 1, \nfCaz) = - 1, \nf(aa) = 3, \nand if a = (a, b, c), find f(a). \n(b) Describe explicitly a linear functional f on R3 such that \nf(al) \nf(a2) \n° but I(aa) 3 0. \n(c) Let f be any linear functional such that \nf(al) = f(a2) = ° and f(aa) 3 0. \nIf a = (2, 3, - 1), show thatf(a) 3 0. \n2. Let CB = {at, az, as} be the basis for CS defined by \nal = (1, 0, - 1), \naz = (1, 1, 1), \naa = (2, 2, 0). \nFind the dual basis of CB. \nExercises \n3. If A and B are n X n matrices over the field P, show that trace (AB) \ntrace \n(BA). Now show that similar matrices have the same trace.",
    "2. Let CB = {at, az, as} be the basis for CS defined by \nal = (1, 0, - 1), \naz = (1, 1, 1), \naa = (2, 2, 0). \nFind the dual basis of CB. \nExercises \n3. If A and B are n X n matrices over the field P, show that trace (AB) \ntrace \n(BA). Now show that similar matrices have the same trace. \n4. Let V be the vector space of all polynomial funetions p from R into R which \nhave degree 2 or less: \np(x) \nCo + CIX + C2XZ\n• \nDefine three linear functionals on V by \n!1(P) = 101 p(x) dx, /2(p) = 102 p(x) dx, fa(p) = 1o-1p(x) dx. \nShow that {fl, fz, fa} is a basis for V* by exhibiting the basis for V of which it is \nthe dual. \n5. If A and B are n X n complex matrices, show that AB \nBA \nI is im-\npossible. \n6. Let m and n be positive integers and F a field. Let II, .\n. . , fm be linear func­\ntionals on pn. For a in Fn define \nTa = C/ICa), . . .  , fm(a» . \nShow that T is a linear transformation from Fn into Fm. Then show that every \nlinear transformation from pn into Fm is of the above form, for some It, . . . , fm. \n7. Let al = (1, 0, -1, 2) and a2 = (2, 3, 1, 1), and let W be the subspace of R4 \nspanned by al and a2. Which linear functionals f: \n105 \n106 \nLinear Transformations \nf(xl, X2, Xa, X4) = CIXI + C2X2 + CaXa + C4X4 \nare in the annihilator of W? \n8. Let W be the subspace of R5 which is spanned by the vectors \nal = €I + 2e2 + fa, \na2 = e2 + 3ea + 3e4 + e5 \naa \n€I + 4€2 + 6Ea + 4€4 + €S. \nFind a basis for Woo \nChap. 3 \n9. Let V be the vector space of all 2 X 2 matrices over the field of real numbers, \nand let \nB = [_U -U} \nLet W be the subspace of V consisting of all A such that AB = O. Let f be a linear \nfunctional on V which is in the annihilator of W. Suppose that f(I) = 0 and \nf(C) \n3, where I is the 2 X 2 identity matrix and \nC [V W} \nFind feB). \n10. Let F be a subfield of the complex numbers. We define n linear functionals \non Fn (n 2: 2) by \nn \nfk(XI, . • •  , xn) = 7 (k - j)Xi' \nj= l  \nl S k S n.",
    "f(C) \n3, where I is the 2 X 2 identity matrix and \nC [V W} \nFind feB). \n10. Let F be a subfield of the complex numbers. We define n linear functionals \non Fn (n 2: 2) by \nn \nfk(XI, . • •  , xn) = 7 (k - j)Xi' \nj= l  \nl S k S n. \nWhat is the dimension of the subspace annihilated by /I, . . .  , fn? \nII. Let WI and W2 be subspaces of a finite-dimensional vector space V. \n(a) Prove that (WI + W2)O = m n wg. \n(b) Prove that (WI n Wz)O \nw؊ + wg. \n12. Let V be a finite-dimensional vector space over the field F and let W be a \nsubspace of V. Iff is a linear functional on W, prove that there is a linear functional \ng on V such that g(a) \nf(a) for each a in the subspace W. \n13. Let F be a subfield of the field of complex numbers and let V be any vector \nspace over F. Suppose that f and g are linear functionals on V such that the func­\ntion h defined by h(a) = f(a)g(a) is also a lineal' functional on V. Prove that \neither f \n0 or g \nO. \n14. Let F be a field of characteristic zero and let V be a finite-dimensional vector \nspace over F. If al, . . .  , am are finitely many vectors in V, each different from the \nzero vector, prove that there is a linear functional f on V such that \nf(ai) ,e 0, \ni = 1, . .\n. , m. \n15. According to Exercise 3, similar matrices have the same trace. Thus we can \ndefine the tracc of a linear operator on a finite-dimensional space to be the trace \nof any matrix which represents the operator in an ordered basis. This is well­\ndefined since all such representing matrices for one operator are similar. \nNow let V be the space of all 2 X 2 matrices over the field F and let P be a \nfixed 2 X 2 matrix. Let T be the linear operator on V defined by T(A) = P A. \nProve that trace (T) \n2 trace (P). \nSec. 3.6 \nThe Double Dual \n16. Show that the trace functional on n X n matrices is unique in the following \nsense. If W is the space of n X n matrices over the field F and if f is a linear func­\ntional on W such that f(AB)",
    "Prove that trace (T) \n2 trace (P). \nSec. 3.6 \nThe Double Dual \n16. Show that the trace functional on n X n matrices is unique in the following \nsense. If W is the space of n X n matrices over the field F and if f is a linear func­\ntional on W such that f(AB) \nf(BA) for each A and B in W, then f is a scalar \nmultiple of the trace function. If, in addition, 1(1) \nn, then 1 is the trace function. \n17. Let W be the space of n X n matrices over the field F, and let Wo be the sub­\nspace spanned by the matrices C of the form C = AB - BA. Prove that Wo is \nexactly the subspace of matrices which have trace zero. (Hint: What is the dimen­\nsion of the space of matrices of trace zero? Use the matrix 'units,' i.e., matrices with \nexactly one non-zero entry, to construct enough linearly independent matrices of \nthe form AB - BA.) \n107 \n3.6. The Double Dual \nOne question about dual bases which we did not answer in the last \nsection was whether every basis for V* is the dual of some basis for V. One \nway to answer that question is to consider V**, the dual space of V*. \nIf a is a vector in V, then a induces a linear functional La on V* \ndefined by \nLa (f) \nf(a), \nf in \nV*. \nThe fact that La is linear is just a reformulation of the definition of linear \noperations in V* : \nLa(cf + g) = (cf + g)(a) \n= (cf) (a) + g(a) \n= cf(a) \ng(a) \n= cL,,(f) + L,,(g). \nIf V is finite-dimensional and a :;r: 0, then La :;r: 0; in other words, there \nexists a linear functional f such that f(a) :;r: 0. The proof is very simple \nand was given in Section 3.5: Choose an ordered basis ffi \n{all . . . , an} \nfor V such that al = a and let f be the linear functional which assigns to \neach vector in V its first coordinate in the ordered basis <3. \nTheorem 17. Let V be a finite-dimensional vector space over the field F. \nFor each vector a in V define \nL,,(f) = f(a), \nf in V*. \nThe mapping a -t L\" is then an isomorphism of V onto V**. \nProof. We showed that for each a the function La is linear.",
    "Theorem 17. Let V be a finite-dimensional vector space over the field F. \nFor each vector a in V define \nL,,(f) = f(a), \nf in V*. \nThe mapping a -t L\" is then an isomorphism of V onto V**. \nProof. We showed that for each a the function La is linear. \nSuppose a and fJ are in V and c is in F, and let l' = Ca + fJ. Then for eachf \nin V* \nL-y(f) = f('Y) \nf(ca + (3) \n= cf(a) + f(fJ) \n= cLa(f) \nLfJ(f) \nand so \n108 \nLinear Transformation8 \nChap. 3 \nThis shows that the mapping a -t L\" is a linear transformation from V \ninto V**. This transformation is non-singular; for, according to the \nremarks above L\" = 0 if and only if a = O. Now a -t La is a non-singular \nlinear transformation from V into V**, and since \ndim V** = dim V* = dim V \nTheorem 9 tells us that this transformation is invertible, and is therefore \nan isomorphism of V onto V**. I \nCorollary. Let V be a finite-dimensional vector space over the field F. \nIf L is a linear functional on the dual space V* of V, then there i8 a unique \nvector a in V such that \nL(f) \n= f(a) \nfor every f in V*. \nCorollary. Let V be a finite-dimensional vector space over the field F. \nEach basis for V* is the dual of some basis for V. \nProof. Let m* \n{/I, . . .  , fn} be a basis for V*. By Theorem 15, \nthere is a basis {Ll' . . .  , Ln} for V** such that \nLi(!;) \n= Oij. \nUsing the corollary above, for each i there is a vector ai in V such that \nLM) \n= f(ai) \nfor every f in V*, i.e., such that Li = L\",. It follows immediately that \n{aI, . . .  , an} is a basis for V and that m* is the dual of this basis. I \nIn view of Theorem 17, we usually identify a with L\" and say that V \n'is' the dual space of V* or that the spaces V, V* are naturally in duality \nwith one another. Each is the dual space of the other. In the last corollary \nwe have an illustration of how that can be useful. Here is a further illustra­\ntion. \nIf E is a subset of V*, then the annihilator EO is (technically) a subset",
    "with one another. Each is the dual space of the other. In the last corollary \nwe have an illustration of how that can be useful. Here is a further illustra­\ntion. \nIf E is a subset of V*, then the annihilator EO is (technically) a subset \nof V* ... If we choose to identify V and V* * as in Theorem 17, then EO is a \nsubspace of V, namely, the set of all a in V such thatf(a) \n0 for allf in E. \nIn a corollary of Theorem 16 we noted that each subspace W is determined \nby its annihilator Woo How is it determined? The answer is that W is the \nsubspace annihilated by all f in wo, that is, the intersection of the null \nspaces of all f's in Woo In our present notation for annihilators, the answer \nmay be phrased very simply: W = (WO)o. \nTheorem 18. If 8 is any subset of a finite-dimensional vector space V, \nthen (8°)° is the subspace spanned by 8. \nSec. 3.6 \nThe Double Dual \nProof. IJet W be the subspace spanned by 8. Clearly wo = So. \nTherefore, what we are to prove is that W = Woo. We have given one \nproof. Here is another. By Theorem 16 \ndim W + dim WO \ndim V \ndim WO + dim Woo = dim V* \nand since dim V = dim V* we have \ndim W = dim Woo. \nSince W is a subspace of Woo, we see that W = Woo. \nI \nThe results of this section hold for arbitrary vector spaces; however, \nthe proofs require the use of the so-called Axiom of Choice. We want to \navoid becoming embroiled in a lengthy discussion of that axiom, so we shall \nnot tackle annihilators for general vector spaces. But, there are two results \nabout linear functionals on arbitrary vector spaces which are so fundamen­\ntal that we should include them. \nLet V be a vector space. We want to define hyperspaces in V. Unless \nV is finite-dimensional, we cannot do that with the dimension of the \nhyperspace. But, we can express the idea that a space N falls just one \ndimension short of filling out V, in the following way : \n1. N is a proper subspace of V; \n2. if W is a subspace of V which contains N, then either W = N or \nW \nV.",
    "hyperspace. But, we can express the idea that a space N falls just one \ndimension short of filling out V, in the following way : \n1. N is a proper subspace of V; \n2. if W is a subspace of V which contains N, then either W = N or \nW \nV. \nConditions (1) and (2) together say that N is a proper subspace and there \nis no larger proper subspace, in short, N is a maximal proper subspace. \nDefinition. If V is a vector space, a hyperspace in V is a maximal \nproper subspace of V. \nTheorem 19. If f is a non-zero linear functional on the vector space V, \nthen the null space of f is a hyperspace in V. Conversely, every hyperspace in V \nis the null space of a (not unique) non-zero linear functional on V. \nProof. Let f be a non-zero linear functional on V and NI its null \nspace. Let a be a vector in V which is not in Nj, Le., a vector such that \nf(a) rf O. We shall show that every vector in V is in the subspace spanned \nby Nt and a. That subspace consists of all vectors \n'Y + ca, \nLet f3 be in V. Define \nc \n'Y in Nj, c in F. \nf(f3) \nf(a) \n109 \n110 \nLinear Transformations \nChap. 3 \nwhich makes sense because f(a) ;;t!- O. Then the vector ')I = fJ -\nca is in Nj \nsince \nf(')I) = f(fJ - ca) \n= f(f3) - cf(a) \nO. \nSo fJ is in the subspace spanned by Nj and a. \nNow let N be a hyperspace in V. Fix some vector a which is not in N. \nSince N is a maximal proper subspace, the subspace spanned by N and a \nis the entire space V. Therefore each vector fJ in V has the form \nfJ = ')I + ca, \n'Y in N, c in F. \nThe vector ')I and the scalar c are uniquely determined by fJ. If we have also \nfJ \n')I' + c'a, \n')I' in N, c' in F. \nthen \n(c' - c)a = ')I - ')I'. \nIf c' - c ;;t!- 0, then a would be in N ;  hence, c' = c and ')I' = ')I. Another \nway to phrase our conclusion is this: If fJ is in V, there is a unique scalar c \nsuch that fJ \nca is in N. Call that scalar g(f3). It is easy to see that g is a \nlinear functional on V and that N is the null space of g. \nI",
    "way to phrase our conclusion is this: If fJ is in V, there is a unique scalar c \nsuch that fJ \nca is in N. Call that scalar g(f3). It is easy to see that g is a \nlinear functional on V and that N is the null space of g. \nI \nLemma. If f and II; are linear functionals on a vector space V, then g \nis a scalar multiple of f if and only if the null space of g contains the null space \nof f, that is, if and only if f(a) = 0 implies g(a) = O. \nProof. If f = 0 then g = 0 as well and g is trivially a scalar \nmultiple off. Supposef ;;t!- 0 so that the null space Nj is a hyperspace in V. \nChoose some vector a in V with f(a) ;;t!- 0 and let \ng(a) \nc = f(a)' \nThe linear functional h = g - cf is 0 on Nj, since both f and g are 0 there, \nand h(a) = g(a) - cf(a) = O. Thus h is 0 on the subspace spanned by Nf \nand a-and that subspace is V. We conclude that h \n0, i.e., that g \ncf. \nI \nTheorem 20. Let g, fl' . . . , fr be linear functionals on a vector space V \nv;ith respective null spaces N, Nl, . . .  , Nr• Then g is a linear combination of \nf1' . . .  , fr if and only if N contains the intersection Nl n . . .  n Nr. \nProof. If g = cdl + . . .  + crfr and fiCa) = 0 for each i, then \nclearly g(a) \nO. Therefore, N contains Nl n ' \" n Nr• \nWe shall prove the converse (the 'if' half of the theorem) by induction \non the number r. The preceding lemma handles the case r = 1. Suppose we \nknow the result for r = k - 1, and let fll . .\n. , fk be linear functionals with \nnull spaces NI, •\n.\n.\n , Nk such that Nl n . . .  n Nk is contained in N, the \nSec. 3.7 \nThe Transpose of a Linear Transformation \nnull space of g. Let g', ii, . . .  , i£-1 be the restrictions of g, ii, . . .  , ik-l to \nthe subspace Nk. Then g', ii, . . .  , i£ - 1  are linear functionals on the vector \nspace Nk• Furthermore, if a is a vector in Nk and i;(a) = 0, i = 1, . .\n. , \nk - 1, then a is in Nt n . . .  n Nk and so g'(a) \n= O. By the induction \nhypothesis (the case r \nk - 1), there are scalars Ci such that \nNow let",
    "space Nk• Furthermore, if a is a vector in Nk and i;(a) = 0, i = 1, . .\n. , \nk - 1, then a is in Nt n . . .  n Nk and so g'(a) \n= O. By the induction \nhypothesis (the case r \nk - 1), there are scalars Ci such that \nNow let \n(3-16) \n. . . + ck-d£-I. \nk-l \nh = 9 \n7 Cdi. \ni=1 \nThen h is a linear functional OIl V and (3-16) tells us that h(a) = 0 for \nevery a in Nk. By the preceding lemma, h is a scalar multiple of jk. If h \nCk/k, then \nk \ng = \n8 c;f;. I \n; = 1  \nExercises \n1. Let n be a positive integer and F a field. Let W be the set of all vectors \n(Xl, •\n•\n•\n , Xn) in Fn such that Xl + . . .  + Xn \n= O. \n(a) Prove that WO consists of all linear functionals J of the form \nn \nJ(Xl, •\n•\n•\n , xn) = C 8 Xi' \nj = 1 \n(b) Show that the dual space W* of W can be 'naturally' identified with the \nlinear functionals \nJ(X), •\n•\n•\n , Xn) \nCIXI + . . . + CnXn \non Fn which satisfy CI + . . . + Cn \nO. \n2. Use Theorem 20 to prove the following. If W is a subspace of a finite-dimen­\nsional vector space V and if {gl, . . .  , gT} is any basis for wo, then \nr \nW = Ii Ng,. \n; = 1  \n3. Let S be a set, F a field, and YeS; F) the space of all functions from S into F: \n(f + g)(x) = J(x) + g(x) \n(ef)(x) = cf(x). \nLet W be any n-dimensional subspace of YeS; F). Show that there exist points \nXl, •\n•\n•\n , Xn in S and functions h, . . . , fn in W such that !;(Xi) = Oi;. \n111 \n3.7. The Transpose of a Linear \nTransformation \nSuppose that we have two vector spaces over the field F, V, and W, \nand a linear transformation T from V into W. Then T induces a linear \n112 \nLinear Transformations \nChap. 3 \ntransformation from W* into V*, as follows. Suppose g is a linear functional \non W, and let \n(3-17) \nf(a) = g(Ta) \nfor each a in V. Then (3-17) defines a function f from V into F, namely, \nthe composition of T, a function from V into W, with g, a function from \nW into F. Since both T and g are linear, Theorem 6 tells us that f is also",
    "on W, and let \n(3-17) \nf(a) = g(Ta) \nfor each a in V. Then (3-17) defines a function f from V into F, namely, \nthe composition of T, a function from V into W, with g, a function from \nW into F. Since both T and g are linear, Theorem 6 tells us that f is also \nlinear, i.e., f is a linear functional on V. Thus T provides us with a rule Tt \nwhich associates with each linear functional g on W a linear functional \nf = Ttg on V, defined by (3-17). Note also that Tt is actually a linear \ntransformation from W* into V*; for, if gl and g2 are in W* and c is a scalar \n[Tt(Cgl + g2)] (a) = (Cgl + g2)(Ta) \n= cgl('l'a) + g2(Ta) \n= c(Ttg1)(a) + (Ttg2)(a) \nso that Tt(Cgl + g2) \nCTtgl + Ttg2. Let us summarize. \nTheorem 21. Let V and W be vector spaces over the field F. For each \nlinear transformation T from V into W, there is a unique linear transformation \nTt from W* into V* such that \n(Ttg)(a) == g(Ta) \nfor every g in W* and a in V. \nWe shall call Tt the transpose of T. This transformation Tt is often \ncalled the adjoint of T; however, we shall not use this terminology. \nTheorem 22. Let V and W be vector spaces over the field F, and let T \nbe a linear transformation from V into W. The null space of Tt is the annihi­\nlator of the range of T. If V and W are finite-dimensional, then \n(i) rank (Tt) \nrank (T) \n(ii) the range of Tt is the annihilator of the null space of T. \nProof. If g is in W*, then by definition \n(Ttg)(a) = g(Ta) \nfor each a in V. The statement that g is in the null space of Tt means that \ng(Ta) = 0 for every a in V. Thus the null space of Tt is precisely the \nannihilator of the range of 'J'. \nSuppose that V and W are finite-dimensional, say dim V = n and \ndim W = m. For (i) : Let r be the rank of T, i.e., the dimension of the range \nof T. By Theorem 16, the annihilator of the range of T then has dimension \n(m \nr). By the first statement of this theorem, the nullity of Tt must be \n(m - r). But then since Tt is a linear transformation on an m-dimensional",
    "of T. By Theorem 16, the annihilator of the range of T then has dimension \n(m \nr). By the first statement of this theorem, the nullity of Tt must be \n(m - r). But then since Tt is a linear transformation on an m-dimensional \nspace, the rank of Tt is m - (m - r) \n= r, and so T and Tt have the same \nrank. For (ii) : Let N be the null space of T. Every functional in the range \nSec. 3.7 \nThe Transpose of a Linear Transformation \nof Tt is in the annihilator of N; for, suppose f \nTtg for some g in W*; then, \nif a is in N \nf(a) = (Ttg)(a) = g(Ta) = g(O) = O. \nN ow the range of Tt is a subspace of the space N°, and \ndim N° = n - dim N = rank: (T) = rank: (Tt) \nso that the range of Tt must be exactly N°. I \nTheorem 23. Let V and W be finite-dimensional vector spaces over the \nfield F. Let (£ be an ordered basis for V with dual basis CB*, and let CB' be an \nordered basis for W with dual basis (£'*. Let T be a linear transformation \nfrom V into W; let A be the matrix of T relative to eE, ffi' and let B be the matrix \nof Tt relative to (£'*, (£*. Then Bij = Aji. \nProof. Let \nBy definition, \n(£ = {all . . .  , an}, \n(£* = {il, . . .  , fn}, \nm \nTaj = 2': Aij{3i, \ni - I  \nn \nTIgj = 2': Bijh \n,-I \n(£' = {{31, •\n•\n•\n , 13m}, \nm'* \n= {g \ng } \nil \nb .\n•\n•\n , m .  \nj = 1, . . .  , n \nj \n= 1, . . . , m. \nOn the other hand, \nFor any linear functional f on V \nm \n= 2': Akiojk \n1.:-1 \n= Aji. \nm \nf \n2': f(ai)/i. \ni = 1  \nIf we apply this formula to the functional f = TIgj and use the fact that \n(Ttgj)(ai) = Aj;, we have \nn \nTIg; \n2': Aid. \ni = 1  \nfrom which it immediately follows that Bij = Aji. I \n113 \n114 \nLinear Transformations \nChap. 3 \nDefinition. If A is an m X n matrix over the field F, the transpose of \nA is the n X m matrix At defined by Afj = Aji. \nTheorem 23 thus states that if T is a linear transformation from V \ninto W, the matrix of which in some pair of bases is A, then the transpose \ntransformation Tt is represented in the dual pair of bases by the transpose \nmatrix A t.",
    "A is the n X m matrix At defined by Afj = Aji. \nTheorem 23 thus states that if T is a linear transformation from V \ninto W, the matrix of which in some pair of bases is A, then the transpose \ntransformation Tt is represented in the dual pair of bases by the transpose \nmatrix A t. \nl'heorem 24. Let A be any m X n matrix over the field F. Then the \nrow rank of A is equal to the column rank of A. \nProof. Let CB be the standard ordered basis for Fn and CB' the \nstandard ordered basis for Fm. Let T be the linear transformation from Fn \ninto Fm such that the matrix of T relative to the pair CB, CB' is A, i.e., \nwhere \nT(Xl' . . .  , xn) = (Yl, . . .  , Ym) \nn \nYi = 7 AijXj. \n;= 1 \nThe column rank of A is the rank of the transformation T, because the \nrange of T consists of all m-tuples which are linear combinations of the \ncolumn vectors of A .  \nRelative to the dual bases CB'* and CB*, the transpose mapping Tt is \nrepresented by the matrix A t. Since the columns of A t are the rows of A, \nwe see by the same reasoning that the row rank of A (the column rank of A t) \nis equal to the rank of T'. By Theorem 22, T and Tt have the same rank, \nand hence the row rank of A is equal to the column rank of A .  I \nNow we see that if A is an m X n matrix over F and T is the linear \ntransformation from Fn into Fm defined above, then \nrank (T) = row rank (A) = column rank (A) \nand we shall call this number simply the rank of A. \nEXAMPLE 25. This example will be of a general nature--more dis­\ncussion than example. Let V be an n-dimensional vector space over the \nfield F, and let T be a linear operator on V. Suppose CB = {all '  . . , an} \nis an ordered basis for V. The matrix of T in the ordered basis CB is defined \nto be the n X n matrix A such that \nn \nTaj = 7 A;jai \n;= 1 \nin other words, A ij is the ith coordinate of the vector Taj in the ordered \nbasis CB. If {il, . . .  , fn} is the dual basis of CB, this can be stated simply \nAi; = fi(TaJ. \nSec. 3.7",
    "to be the n X n matrix A such that \nn \nTaj = 7 A;jai \n;= 1 \nin other words, A ij is the ith coordinate of the vector Taj in the ordered \nbasis CB. If {il, . . .  , fn} is the dual basis of CB, this can be stated simply \nAi; = fi(TaJ. \nSec. 3.7 \nThe Transpose of a Linear Transformation \nLet us see what happens when we change basis. Suppose \n(B' = {ai, . . .  , a} \nis another ordered basis for V, with dual basis {fi, . . .  , j} . If B is the \nmatrix of T in the ordered basis (B', then \nBij = j;(Ta;). \nLet U be the invertible linear operator such that U aj = a;. Then the \ntranspose of U is given by Utj; \nj;. It is easy to verify that since U is \ninvertible, so is Ut and (Ut)-l = (U-I)t. Thusj; = (U-I) tji, i = 1, . . .  , n. \nTherefore, \nBij \n[(U-I) tjiJ (Ta;) \n= ji(U-ITa;) \n= fi(U-ITUaj). \nNow what does this say? Well, ji(U-I'1'Uaj) is the i, j entry of the matrix \nof U-ITU in the ordered basis (B. Our computation above shows that this \nscalar is also the i, j entry of the matrix of T in the ordered basis (B'. In \nother words \n[T]lB' = [U-ITU]rn \n= [U-IJrn[TJrn[U]<l\\ \n= [U]CB l[TJrn[UJrn \nand this is precisely the change-of-basis formula which we derived earlier. \nExercises \n1. Let F be a field and let f be the linear functional on {l'2 defined by f(XI, X2) \naXI + bX2. For each of the following linear operators T, let (I = Ttf, and find \n(I(XI' X2). \n(a) T(xlJ xz) = (Xl, 0) ; \n(b) T(xI, X2) = (-X2, Xl) ; \n(c) T(Xl' X2) = (Xl - X2, Xl + X2). \n2. Let V be the vector space of all polynomial functions over the field of real \nnumbers. Let a and b be fixed real numbers and let f be the linear functional on V \ndefined by \nf(p) f p(x) dx. \nIf D is the differentiation operator on V, what is Dtf? \n3. Let V be the space of all n X n matrices over a field F and let B be a fixed \nn X n matrix. If T is the linear operator on V defined by T(A) \nAB \nBA, \nand if f is the trace function, what is Ttf? \n4. Let V be a finite-dimensional vector space over the field F and let T be a",
    "3. Let V be the space of all n X n matrices over a field F and let B be a fixed \nn X n matrix. If T is the linear operator on V defined by T(A) \nAB \nBA, \nand if f is the trace function, what is Ttf? \n4. Let V be a finite-dimensional vector space over the field F and let T be a \nlinear operator on V. Let c be a scalar and suppose there is a non-zero vector a \nin V such that Ta \nca. Prove that there is a non-zero linear functional f on V \nsuch that T'f = cf. \n115 \n116 \nLinear TranBformation8 \nChap. 3 \n5. Let A be an m X n matrix with real entries. Prove that A \n0 if and only \nif trace (A tA) = O. \n6. Let n be a positive integer and let V be the space of all polynomial functions \nover the field of real numbers which have degree at most n, i.e., functions of the \nform \nI(x) \nCo + CIX + . . .  + c\"x\". \nLet D be the differentiation operator on V. Find a basis for the null space of the \ntranspose operator Dt. \n7. Let V be a finite-dimensional vector space over the field F. Show that T -+ Tt \nis an isomorphism of L(V, V) onto L(V*, V*). \n8. Let V be the vector space of n X n matrices over the field F. \n(a) If B is a fixed n X n matrix, define a function IB on V by IB(A) = trace \n(BtA). Show that lB is a linear functional on V. \n(b) Show that every linear functional on V is of the above form, i.e., is IB \nfor some B. \n(c) Show that B -+ IB is an isomorphism of V onto V*. \n4. Polynomials \n4.1 . Algebras \nThe purpose of this chapter is to establish a few of the basic prop­\nerties of the algebra of polynomials over a field. The discussion will be \nfacilitated if we first introduce the concept of a linear algebra over a field. \nDefinition. Let F be a field. A linear algebra over the field F is a \nvector space (:t over F' with an additional operation called multiplication of \nvectors which associates with each pair of vectors a, {J in a a vector a{J in \na called the product of a and (J in such a way that \n(a) multiplication is associative, \na(fJ'Y) = (afJh",
    "vector space (:t over F' with an additional operation called multiplication of \nvectors which associates with each pair of vectors a, {J in a a vector a{J in \na called the product of a and (J in such a way that \n(a) multiplication is associative, \na(fJ'Y) = (afJh \n(b) multiplication is distributive with respect to addition, \na(fJ + 'Y) = a(3 + a'Y and \n(a + (3h = a'Y + (3'Y \n(c) for each scalar c in F, \nc(a{3) = (ca){3 = a(c{3). \nIf there is an element 1 in a such that la = al = a for each a in a, \nwe call a a linear algebra with identity over F', and call 1 the identity \nof a. The algebra a is called eommutative if a{3 \n{3afor all a and fJ in a. \nEXAMPLE 1. The set of n X n matrices over a field, with the usual \noperations, is a linear algebra with identity; in particular the field itself \nis an algebra with identity. This algebra is not commutative if n s 2. \nThe field itself is (of course) commutative. \n117 \n118 \nPolynomials \nChap. 4 \nEXAMPLE 2. The space of all linear operators on a vector space, with \ncomposition as the product, is a linear algebra with identity. It is com­\nmutative if and only if the space is one-dimensional. \nThe reader may have had some experience with the dot product and \ncross product of vectors in R3. If so, he should observe that neither of \nthese products is of the type described in the definition of a linear algebra. \nThe dot product is a 'scalar product,' that is, it associates with a pair of \nvectors a scalar, and thus it is certainly not the type of product we are \npresently discussing. The cross product does associate a vector with each \npair of vectors in R3; however, this is not an associative multiplication. \nThe rest of this section will be devoted to the construction of an \nalgebra which is significantly different from the algebras in either of the \npreceding examples. Let F be a field and S the set of non-negative in­\ntegers. By Example 3 of Chapter 2, the set of all functions from S into",
    "The rest of this section will be devoted to the construction of an \nalgebra which is significantly different from the algebras in either of the \npreceding examples. Let F be a field and S the set of non-negative in­\ntegers. By Example 3 of Chapter 2, the set of all functions from S into \nF is a vector space over F. We shall denote this vector space by F\"'. The \nvectors in F'\" are therefore infinite sequences f = (fo, h, f2' .\n•\n•\n ) of scalars \nfi in F. If 9 = (go, gI, g2, . . .  ), gi in F, and a, b are scalars in F, af + bg is \nthe infinite sequence given by \n(4-1) \naf + bg \n(afo + bgo, afl + bgl, af2 + bg2, . . . ) , \nWe define a product in F'\" by associating with each pair of vectors f and \n9 in F'\" the vector fg which is given by \n(4-2) \nThus \nand as \nn = 0, 1, 2, . . . .  \nfor n \n0, 1, 2, . . .  , it follows that multiplication is commutative, fg \ngf. \nIf h also belongs to F\"', then \nn \n[(fg)h]n \n7 (fg)ih,,-i \ni = O  \nn \ni \n= 7 7 figi-ihn-i \n.-0 ;-0 \nn \nn-1 \n= l; !; 7 g;hn-H \nj - O  \ni = O  \nn \n= l; !;(gh)\"-i = [f(gh)]n \nj -O \nSec. 4.2 \nThe Algebra of Polynomials \nfor n = 0, 1, 2, . . .  , so that \n(4-3) \n(fg)h = f(gh). \nWe leave it to the reader to verify that the multiplication defined by (4-2) \nsatisfies (b) and (c) in the definition of a linear algebra, and that the \nvector 1 = (1, 0, 0, . . .  ) serves as an identity for Foo. Then Foo, with the \noperations defined above, is a commutative linear algebra with identity \nover the field F. \nThe vector (0, 1, 0, . . .  , 0, . . .  ) plays a distinguished role in what \nfollows and we shall consistently denote it by x. Throughout this chapter \nx will never be used to denote an element of the field F. The product of x \nwith itself n times will be denoted by xn and we shall put XO \n1. Then \nx2 = (0, 0, 1, 0, . . .  ), \nx3 = (0, 0, 0, 1, 0, . . . ) \nand in general for each integer k ;;:: 0, (Xk)k \n1 and (Xk)n = ° for all non­\nnegative integers n o;t. le. In concluding this section we observe that the",
    "with itself n times will be denoted by xn and we shall put XO \n1. Then \nx2 = (0, 0, 1, 0, . . .  ), \nx3 = (0, 0, 0, 1, 0, . . . ) \nand in general for each integer k ;;:: 0, (Xk)k \n1 and (Xk)n = ° for all non­\nnegative integers n o;t. le. In concluding this section we observe that the \nset consisting of 1, x, X2, •\n•\n•\n is both independent and infinite. Thus the \nalgebra F'\" is not finite-dimensional. \nThe algebra F\"\" is sometimes called the algebra of forIllal power \nseries over F. The element f = (fo, h f2' . . . ) is frequently written \n(4-4) \nThis notation is very convenient for dealing with the algebraic operations. \nWhen used, it must be remembered that it is purely formal. There are no \n(infinite sums' in algebra, and the power series notation (4-4) is not in­\ntended to suggest anything about convergence, if the reader knows what \nthat is. By using sequences, we were able to define carefully an algebra \nin which the operations behave like addition and multiplication of formal \npower series, without running the risk of confusion over such things as \ninfinite sums. \n119 \n4.2. The Algebra of Polynomials \nWe are now in a position to define a polynomial over the field F. \nDefinition. Let F[x] be the subspace of F'\" spanned by the vectors \n1, x, x2, •\n•\n•\n•\n An element of F [x] is called a polynoIllial over F. \nSince F[x] consists of all (finite) linear combinations of x and its \npowers, a non-zero vector I in F'\" is a polynomial if and only if there is \nan integer n ;;:: ° such that In o;t. ° and such that fk = ° for all integers \nk > n; this integer (when it exists) is obviously unique and is called the \ndegree of f. We denote the degree of a polynomial I by deg f, and do \n120 \nPolynomials \nChap. 4 \nnot assign a degree to the O-polynomial. If f is a non-zero polynomial of \ndegree n it follows that \n(4-5) \nf = foxo + fIX + f2X2 + . . .  + fnxn, \nfn 7f!E O. \nThe scalars fo, fI' . . .  , fn are sometimes called the coefficients of f, and",
    "120 \nPolynomials \nChap. 4 \nnot assign a degree to the O-polynomial. If f is a non-zero polynomial of \ndegree n it follows that \n(4-5) \nf = foxo + fIX + f2X2 + . . .  + fnxn, \nfn 7f!E O. \nThe scalars fo, fI' . . .  , fn are sometimes called the coefficients of f, and \nwe may say that f is a polynomial with coefficients in F. We shall call \npolynomials of the form cxo scalar polynomials, and frequently write c \nfor cxo. A non-zero polynomial f of degree n such that fn = 1 is said to \nbe a monic polynomial. \nThe reader should note that polynomials are not the same sort of \nobjects as the polynomial functions on F which we have discussed on \nseveral occasions. If F contains an infinite number of elements, there is a \nnatural isomorphism between F[x] and the algebra of polynomial func­\ntions on F. We shall discuss that in the next section. Let us verify that \nF[x] is an algebra. \nTheorem 1. Let f and g be non-zero polynomials over F. Then \n(i) fg is a non-zero polynomial; \n(ii) deg (fg) = deg f + deg g; \n(iii) fg is a monic polynomial if both f and g are monic polynomials; \n(iv) fg is a scalar polynomial if and only if both f and g are scalar \npolynomials; \n(v) if f + g 7f!E 0, \ndeg (f + g) ::; max (deg f, deg g). \nProof. Suppose f has degree m and that g has degree n. If k is a \nnon-negative integer, \nm+n+k \n(jg)m+n+k \nY \nfigm+n+k-i. \ni = O  \nIn order that figm+n+k-i 7f!E 0, it is necessary that i ::; m and m + n + \nk - i ::; n. Hence it is necessary that m + k ::; i ::; m, which implies \nk \n0 and i \nm. Thus \n(4-6) \nand \n(4-7) \nk > o. \nThe statements (i), (ii), (iii) follow immediately from (4-6) and (4-7), \nwhile (iv) is a consequence of (i) and (ii). We leave the verification of (v) \nto the reader. I \nCorollary 1. The set of all polynomials over a given field F equipped \nwith the operations (4-1) and (4-2) is a commutative linear algebra with \nidentity over F. \nSec. 4.2 \nThe Algebra oj Polynomials \nProof. Since the operations (4-1) and (4-2) are those defined in",
    "to the reader. I \nCorollary 1. The set of all polynomials over a given field F equipped \nwith the operations (4-1) and (4-2) is a commutative linear algebra with \nidentity over F. \nSec. 4.2 \nThe Algebra oj Polynomials \nProof. Since the operations (4-1) and (4-2) are those defined in \nthe algebra Foo and since F[x] is a subspace of j?cJ, it suffices to prove that \nthe product of two polynomials is again a polynomial. This is trivial when \none of the factors is 0 and otherwise follows from (i). \nI \nCorollary 2. Suppose f, g, and h are polynomials over the field F such \nthat f  0 and fg \nfh. Then g \nh. \nProof. Since fg \nfh, f(g \nh) \n0, and as f  0 it follows at \nonce from (i) that g - h = O. \nI \nCertain additional facts follow rather easily from the proof of Theorem \n1, and we shall mention some of these. \nSuppose \n11 \nn \nf = 7 fiXi and g = 7 gjXi. \ni=O \nj-O \nThen from (4-7) we obtain, \n(4-8) \nThe reader should verify, in the special case f = cxm, g = dxn with c, d in \nF, that (4-8) reduces to \n(4-9) \nNow from (4-9) and the distributive laws in F[x], it follows that the \nproduct in (4-8) is also given by \n(4-10) \n7 figjxi+i \ni,j \nwhere the sum is extended over all integer pairs i, j such that 0 ::; i ::; m, \nand 0 ::; j ::; n. \nDefinition. Let (t be a linear algebra with identity over the field F. We \nshall denote the identity of (t by 1 and make the convention that aO = 1 for \nn \neach a in (t. Then to each polynom'ial f = 7 f;Xi over F and a in (t we asso­\ni::;O \nciate an element f(a) in (t by the rule \nand \nEXAMPLE 3, Let C be the field of complex numbers and letf = x2 + 2. \n(a) If (t = C and z belongs to C, fez) = Z2 + 2, in particular f(2) = 6 \nf (1 + i.) \n1 \n_ 3 \n1. \n121 \n122 \nPolynomials \nChap, 4 \n(b) If a is the algebra of all 2 X 2 matrices over C and if \nB = [ ȣ ȤJ \nthen \nfeB) \ntJ + [-ȥ ȒT [ 3 OJ. \n-3 6 \n(c) If a is the algebra of all linear operators on Ca and T is the ele­\nment of a given by \nT(c), C2, C3) = (iV2 Ch CZ, iV2 C3)",
    "f (1 + i.) \n1 \n_ 3 \n1. \n121 \n122 \nPolynomials \nChap, 4 \n(b) If a is the algebra of all 2 X 2 matrices over C and if \nB = [ ȣ ȤJ \nthen \nfeB) \ntJ + [-ȥ ȒT [ 3 OJ. \n-3 6 \n(c) If a is the algebra of all linear operators on Ca and T is the ele­\nment of a given by \nT(c), C2, C3) = (iV2 Ch CZ, iV2 C3) \nthen f(T) is the linear operator on C3 defined by \nf(T)(cI, C2, C3) = (0, 3C2, 0). \n(d) If a is the algebra of all polynomials over C and g = X4 + ܰh, \nthen f(g) is the polynomial in a given by \nf(g) \n= -7 + 6ix4 + x8, \nThe observant reader may notice in connection with this last example \nthat if f is a polynomial over any field and x is the polynomial (0, 1, 0, .\n. ,) \nthen f = f(x), but he is advised to forget this fact. \nTheorem 2. Let F be a fi,eld and a be a linear algebra with identity \nover F. Suppose f and g are polynomials over F, that a is an element of a, \nand that c belongs to F. Then \n(i) (cf + g) (a) \n= cf(a) + g(a) ; \n(ii) (fg)(a) \nf(a)g(a). \nProof. As (i) is quite easy to establish, we shall only prove (ii). \nSuppose \nf \nBy (4-10), \nand henee by (i), \nExercises \nm \n{ fixi and g \ni=O \nfg \n{ figjXHi \ni,j \n(fg)(a) = Ӥ figjai+i \nid \n= f(a)g(a). \nI \nn \n{ gjXi. \nj=O \n1. Let F be a subfield of the complex numbers and let A be the following 2 X 2 \nmatrix over F \nSec. 4.2 \nThe Algebra of Polynomials \nFor each of the following polynomials J over P, compute J(A). \n(a) J \nx2 - X + 2; \n(b) J \nx3 \n1; \n(0) J = x2 - 5x + 7. \n2. Let T be the linear operator on R3 defined by \nT(XI' X2, X3) \n(Xl, xa, -2X2 - Xa). \nLet J be the polynomial over R defined by J \n-x3 + 2. Find J(T). \n3. Let A be an n X n diagonal matrix over the field P, i.e., a matrix satisfying \nAi; = 0 for i ;;c j. Let f be the polynomial over F defined by \nJ = (x - Au) . . . (x - An,,)' \nWhat is the matrix f(A)? \n4. If f and g are independent polynomials over a field P and h is a nOll-zero \npolynomial over P, show that fh and gh are independent.",
    "Ai; = 0 for i ;;c j. Let f be the polynomial over F defined by \nJ = (x - Au) . . . (x - An,,)' \nWhat is the matrix f(A)? \n4. If f and g are independent polynomials over a field P and h is a nOll-zero \npolynomial over P, show that fh and gh are independent. \n5. If F is a field, show that the product of two non-zero elements of poo is non-zero. \n6. Let S be a set of non-zero polynomials over a field F. If no two elements of S \nhave the same degree, show that S is an independent set in P[x]. \n7. If a and b are elements of a field P and a ;;c  0, show that the polynomials 1, \nax + b, (ax + b)2, (ax + b)3, . . .  form a basis of F[:);]. \n8. If F is a field and h is a polynomial over P of degree 2: 1, show that the map­\nping f -t J(h) is a one-one linear transformation of Ji'[xJ into P[xJ. Show that this \ntransformation is an isomorphism of F[xJ onto F[x] if and only if deg h \n1. \n9. Let P be a subfield of the complex numbers and let T, D be the transformations \non F[x] defined by \nand \n( n \n) \nn \nc .  \nT \n2: CiX' \n= 2: -\n'-. Xi+l \ni=O \ni؎o l + ƣ \n(a) Show that T is a non-singular linear operator on P[xJ. Show also that T \nis not invertible. \n(b) Show that D is a linear operator on P[xJ and find its null space. \n(c) Show that DT \nI, and TD ;;c I. \n(d) Show that T[(Tf)g] = (Tf) (Tg) - T[f(Tg)] for all f, g in F[x]. \n(e) State and prove a rule for D similar to the one given for T in (d). \n(f) Suppose V is a nOll-zero subspace of F[x] such that TJ belongs to Y for \neach J in V. Show that V is not finite-dimensional. \n(g) Suppose V is a finite-dimensional subspace of F[x]. Prove there is an \ninteger m 2: 0 such that DmJ \n0 for eaeh f in Y. \n123 \n124 \nPolynomials \nChap. 4 \n4.3. Lagrange Interpolation \nThroughout this section we shall assume F is a fixed field and that \nto, tl, . . .  , tn are n + 1 distinct elements of F. Let V be the subspace of \nF[x] consisting of all polynomials of degree less than or equal to n (to­",
    "123 \n124 \nPolynomials \nChap. 4 \n4.3. Lagrange Interpolation \nThroughout this section we shall assume F is a fixed field and that \nto, tl, . . .  , tn are n + 1 distinct elements of F. Let V be the subspace of \nF[x] consisting of all polynomials of degree less than or equal to n (to­\ngether with the O-polynomial), and let L; be the function from V into F \ndefined for I in V by \nL;(f) = I(t,), \n0 ::; i ::; n. \nBy part (i) of Theorem 2, each Li is a linear functional on V, and one of \nthe things we intend to show is that the set consisting of Lo, LI, . . .  , Ln \nis a basis for V*, the dual space of V. \nOf course in order that this be so, it is sufficient (cf. Theorem 15 of \nChapter 3) that {Lo, LI, . . .  , Ln} be the dual of a basis {Po, PI, . . .  , Pn} \nof V. There is at most one such basis, and if it exists it is characterized by \n(4-11) \nLj(P;) = Pi(tj) \nOij. \nThe polynomials \n(x - to) . . . (x - ti-I) (x - tHI) ' \"  (x - tn) \n(4-12) \nPi = (J:-৛ to) . . .  (ti \nti-I) (ti - [HI) . . . (ti - tn) \n- II \n--) \n(X - t) \nj>=i ti \nt; \nare of degree n, hence belong to V, and by Theorem 2, they satisfy (4-11). \nIf I = { CiP,., then for each j \n; \n(4-13) \nSince the O-polynomial has the property that OCt) = 0 for each t in F, it \nfollows from (4-13) that the polynomials Po, PI, . . .  , Pn are linearly in­\ndependent. The polynomials 1, X, •\n.\n.\n , xn form a basis of V and hence the \ndimension of V is (n + 1). So, the independent set {Po, PI, . . .  , Pn} \nmust also be a basis for V. Thus for each I in V \n(4-14) \nThe expression (4-14) is called Lagrange's interpolation formula. Set­\nting I \nxi in (4-14) we obtain \nn \nXi \n{ (ti)iP;. \n; = 0  \nNow from Theorem 7 of Chapter 2 it follows that the matrix \n(4-15) \ntড়] \nt7 \ntঢ় \nSec. 4.3 \nLagrange Interpolation \nis invertible. The matrix in (4-15) is called a Vandermonde matrix; it \nis an interesting exercise to show directly that such a matrix is invertible, \nwhen to, tl, . . .  , tn are n + 1 distinct elements of F.",
    "(4-15) \ntড়] \nt7 \ntঢ় \nSec. 4.3 \nLagrange Interpolation \nis invertible. The matrix in (4-15) is called a Vandermonde matrix; it \nis an interesting exercise to show directly that such a matrix is invertible, \nwhen to, tl, . . .  , tn are n + 1 distinct elements of F. \nIf f is any polynomial over F we shall, in our present discussion, de­\nnote by f- the polynomial function from F into F taking each t in F into \nf(t). By definition (cf. Example 4, Chapter 2) every polynomial function \narises in this way; however, it may happen that r = g- for two poly­\nnomials f and g such that f =;tf:. g. Fortunately, as we shall see, this un­\npleasant situation only occurs in the case where F is a field having only \na finite number of distinct elements. In order to describe in a precise way \nthe relation between polynomials and polynomial functions, we need to \ndefine the product of two polynomial functions. If f, g are polynomials \nover F, the product of r and g- is the function r g- from F into F given by \n(4-16) \nt in F. \nBy part (ii) of Theorem 2, (fg)(t) = f(t)g(t), and hence \n(fg)-(t) = r(t)g-(t) \nfor each t in F. Thus rg-\n= (fg)-, and is a polynomial function. At this \npoint it is a straightforward matter, which we leave to the reader, to verify \nthat the vector space of polynomial functions over F becomes a linear \nalgebra with identity over F if multiplication is defined by (4-16). \nDefinition. Let F be a field and let a, and a,- be linear algebras over F. \nThe algebras a, and a,- are said to be isomorphic if there is a one-to-one map­\nping a -+ a- of a, onto a,- such that \n(a) \n(ca + d,B)- = Ca- + d,B-\n(b) \n(a,B)- = a-,B-\nfor all a, {3 in a, and all scalars c, d in F. The mapping a -+ a- is called an \nisomorphism of a onto a,-. An isomorphism of a onto a,- is thus a vector­\nspace isomorphism of a onto a- which has the additional property (b) of \n'preserving' products. \nEXAMPLE 4. Let V be an n-dimensional vector space over the field F.",
    "isomorphism of a onto a,-. An isomorphism of a onto a,- is thus a vector­\nspace isomorphism of a onto a- which has the additional property (b) of \n'preserving' products. \nEXAMPLE 4. Let V be an n-dimensional vector space over the field F. \nBy Theorem 13 of Chapter 3 and subsequent remarks, each ordered basis \nCB of V determines an isomorphism T -+ [T](\\ of the algebra of linear \noperators on V onto the algebra of n X n matrices over F. Suppose now \nthat U is a fixed linear operator on V and that we are given a polynomial \nwith coefficients Ci in F. Then \nn \nf = { c;xi \ni\"' O \n125 \n126 \nPolynomials \nand since T -+ [TJ(l is a linear mapping \nn \n[f(U)](l = { Ci[Ui](l. \ni= O \nNow from the additional fact that \n[T\\T2Jm = [T\\Jm[T2J(l \nfor all T}, T2 in L(V, V) it follows that \n2 ::; i ::; n. \nAs this relation is also valid for i = 0, 1 we obtain the result that \n( 4-17) \n[j(U)](l = f([U](l). \nChap. 4 \nIn words, if U is a linear operator on V, the matrix of a polynomial in U, \nin a given basis, is the same polynomial in the matrix of U. \nTheorem 3. If F is a field containing an infinite number of distinct \nelements, the mapping f ܯ f- is an isomorphism of the algebra of polynomials \nover F onto the algebra of polynomial functions over F. \nProof. By definition, the mapping is onto, and if f, 9 belong to \nF[x] it is evident that \n(cf + dg)- = dr + dg-\nfor all scalars c and d. Since we have already shown that (fg)- = rg-, we \nneed only show that the mapping is one-to-one. To do this it suffices by \nlinearity to show that r = 0 implies f = 0. Suppose then that f is a poly­\nnomial of degree n or less such that l' = O. Let to, t}, . . .  , tn be any n + 1 \ndistinct elements of F. Since r = 0, f(ti) = 0 for i = 0, 1, . . .  , n, and it \nis an immediate consequence of (4-14) that f = O. I \nFrom the results of the next section we shall obtain an altogether \ndifferent proof of this theorem. \nExercises \n1. Use the Lagrange interpolation formula to find a polynomial f with real co­",
    "is an immediate consequence of (4-14) that f = O. I \nFrom the results of the next section we shall obtain an altogether \ndifferent proof of this theorem. \nExercises \n1. Use the Lagrange interpolation formula to find a polynomial f with real co­\nefficients such that f has degree ¨ 3 and f( -1) = -6, f(O) = 2, f(l) = -2, \nf(2) = 6. \n2. Let a, (3, \"I, 0 be real numbers. We ask when it is possible to find a polynomial f \nover R, of degree not more than 2, such that f( - 1) = a, f(l) = (3, f(3) = \"I and \nf(O) = o. Prove that this is possible if and only if \n3a + 6{3 - \"I - 80 = O. \n3. Let F be the field of real numbers, \nSec. 4.4 \nA =[T S R Q] \n0\n0\n0\n 1 \nP = (x - 2) (x - 3) (x - 1). \n(a) Show that peA) = o. \nPolynomial Ideals \n(b) Let PI, P2, Pa be the Lagrange polynomials for tl \n2, t2 \n3, ta \n1. \nCompute Ei = PiCA), i = 1, 2, 3. \n(c) Show that EI + Ez + Ea = I, EiE; = 0 if i \"\" j, E'f = Ei. \n(d) Show that A \n2EI + 3E2 + Ea. \n4. Let p \n(x \n2)(x \n3) (x \n1) and let T be any linear operator on R4 such \nthat peT) = O. Let PI, P2, P3 be the Lagrange polynomials of Exercise 3, and let \nEi = P;(T), i \n1, 2, 3. Prove that \nEiE; = 0 if i \"\" j, \nEf \nE;, and \nT \n2E, + 3Ez + Ea. \nS. Let n be a positive integer and F a field. Suppose A is an n X n matrix over F \nand P is an invertible n X n matrix over F. If J is any polynomial over F, prove \nthat \nJ(P-IAP) \nP-lj(A)P. \n6. Let F be a field. We have considered certain special linear functionals on F[x] \nobtained via 'evaluation at t' : \nL(f) = J(t). \nSuch functionals are not only linear but also have the property that L(fg) \nL(f)L(g). Prove that if L is any linear functional on F[x] such that \nL(fg) \n= L(f)L(g) \nfor all J and g, then either L = 0 or there is a t in F such that L(f) \n= J(t) for all J. \n127 \n4.4. Polynomial Ideals \nIn this section we are concerned with results which depend primarily \non the multiplicative structure of the algebra of polynomials over a field.",
    "L(fg) \n= L(f)L(g) \nfor all J and g, then either L = 0 or there is a t in F such that L(f) \n= J(t) for all J. \n127 \n4.4. Polynomial Ideals \nIn this section we are concerned with results which depend primarily \non the multiplicative structure of the algebra of polynomials over a field. \nLemma. ,suppose f and d are non-zero polynomials over a .field F such \nthat deg d :s: deg f. Then there exists a polynomial g in F [x] such that either \nand that \nf - dg = 0 or deg (f - dg) < deg f. \nProof. Suppose \nm-l \nf \namxm + E aixi, \ni = O  \nn-l \nd = bnx\" + F bixi, \ni =O \nbtl 3 O. \n128 \nPolynomials \nChap. 4 \nThen m r n, and \nf (y:)xm-nd \n0 or deg [f (ȴ:)xm-nd ] < degf. \nThus we may take 9 = (y:) xm-n. \nI \nUsing this lemma we can show that the familiar process of 'long \ndivision' of polynomials with real or complex coefficients is possible over \nany field. \nTheorem 4. If f, d are polynomials over a field F and d is different \nfrom 0 then there exist polynomials q, r in F[x] such that \n(i) f \ndq + r. \n(ii) either r = 0 or deg r < deg d. \nThe polynomials q, r satisfying (i) and (ii) are unique. \nProof. If f is 0 or deg f < deg d we may take q = 0 and r = f. In \ncase f Z 0 and deg f r deg d, the preceding lemma shows we may choose \na polynomial 9 such that f - dg \n0 or deg (f - dg) < deg f. If f -\ndg Z 0 and deg (f - dg) r deg d we choose a polynomial h such that \n(f \ndg) \ndh = 0 or \ndeg [f \nd(g + h)] < deg (f - dg). \nContinuing this process as long as necessary, we ultimately obtain poly­\nnomials q, r such that r \n0 or deg r < deg d, and f = dq + r. Now sup­\npose we also have f = dq, + r, where rl = 0 or deg r, < deg d. Then \ndq + r \ndql + rl, and d(q \nq,) \n= r, \nr. If q - ql Z 0 then d(q - ql) Z \no and \ndeg d + deg (q - ql) = deg (rl - r). \nBut as the degree of rl - r is less than the degree of d, this is impossible \nand q - ql = O. Hence also 1'1 - r = O. \nI \nDefinition. Let d be a non-zero polynomial over the field F. If f is in",
    "q,) \n= r, \nr. If q - ql Z 0 then d(q - ql) Z \no and \ndeg d + deg (q - ql) = deg (rl - r). \nBut as the degree of rl - r is less than the degree of d, this is impossible \nand q - ql = O. Hence also 1'1 - r = O. \nI \nDefinition. Let d be a non-zero polynomial over the field F. If f is in \nF[x], the preceding theorem shows there is at most one polynomial q in F [x] \nsuch that f \ndq. If such a q exists we say that d divides f, that f is divisible \nby d, that f is a multiple of d, and call q the quotient of f and d. We \nalso write q \n= f/d. \nCorollary .1. Let f be a polynomial over the field F, and let c be an ele­\nment of F. Then f is divisible by x - c if and only 1j £(c) \n= O. \nProof. By the theorem, f = (x - c)q + r where r is a scalar \npolynomial. By Theorem 2, \nf(c) = Oq(c) + r(c) = r(c). \nSee. 4.4 \nHence r = 0 if and only if fCc) = o. \nI \nPolynomial Ideals \nDefinition. Let F be a field. An element c in F is said to be a root or \na zero of a given polynomial f over F if f(c) = O. \nCorollary 2. A polynomial f of degree n over a field F has at most n roots \nin F. \nProof. The result is obviously true for polynomials of degree 0 \nand degree 1. We assume it to be true for polynomials of degree n \n1. If \na is a root of f, f = (x - a)q where q has degree n - 1. Since feb) = 0 if \nand only if a \nb or q(b) = 0, it follows by our inductive assumption that \nf has at most n roots. I \nThe reader should observe that the main step in the proof of Theorem \n3 follows immediately from this corollary. \nThe formal derivatives of a polynomial are useful in discussing mul­\ntiple roots. The derivative of the polynomial \nf = Co + CIX + . . .  + cnX,. \nis the polynomial \nl' \nCl + 2czx + . . .  + nc\"xn-1• \nWe also use the notation Df = 1'. Differentiation is linear, that is, D is a \nlinear operator on F[x]. We have the higher order formal derivatives \nf\" \nD2f, j<3) = D3f, and so on. \nTheorem 5 (Taylor's Formula). Let F be a field of characteristic",
    "l' \nCl + 2czx + . . .  + nc\"xn-1• \nWe also use the notation Df = 1'. Differentiation is linear, that is, D is a \nlinear operator on F[x]. We have the higher order formal derivatives \nf\" \nD2f, j<3) = D3f, and so on. \nTheorem 5 (Taylor's Formula). Let F be a field of characteristic \nzeta, c an element of F, and n a positive integer. If f is a polynomial over f \nwith deg f :s; n, then \nn (Dkf) \nf = k.O k! (c)(x - C)k. \nProof. Taylor's formula is a consequence of the binomial theorem \nand the linearity of the operators D, D2, . . .  , Dn. The binomial theorem \nis easily proved by induction and asserts that \nwhere \n(a + b)m = - (m) am-k bk \nk = O  k \n(m) \nm! \nm(m \nk \n= k!(m \nk) ! = \n1) .\n.\n.\n (m \nk + 1) \n1 · 2  \n. . · k \nis the familiar binomial coefficient giving the number of combinations of \nm objects taken k at a time. By the binomial theorem \nx'\" = [c + (x - c)Jm \n= / (m) cm-k(x _ C)k \nk=O k \ncm + mcm-1(x \nc) + . . .  + (x - c)m \n129 \n130 \nPolynomials \nChap. 4 \nand this is the statement of Taylor's formula for the case f = xm. If \nthen \nand \nDkf(e) = { am (Dkxm) (e) \n£ Dkf(c) (x \nk = O  \nk! \nm \n= f· \nI \nIt should be noted that because the polynomials 1, (x - c), .\n.\n.\n , \n(x - c)n are linearly independent (cf. Exercise 6, Section 4.2) Taylor's \nformula provides the unique method for writing f as a linear combination \nof the polynomials (x - C)k (0 ::; k ::; n). \nAlthough we shall not give any details, it is perhaps worth mentioning \nat this point that with the proper interpretation Taylor's formula is also \nvalid for polynomials over fields of finite characteristic. If the field F has \nfinite characteristic (the sum of some finite number of l's in F is 0) then \nwe may have k! = 0 in F, in which case the division of (Dkf) (e) by k! is \nmeaningless. Nevertheless, sense can be made out of the division of Dkf \nby k!, because every coefficient of Dkf is an element of F multiplied by an \ninteger divisible by k !  If all of this seems confusing, we advise the reader",
    "meaningless. Nevertheless, sense can be made out of the division of Dkf \nby k!, because every coefficient of Dkf is an element of F multiplied by an \ninteger divisible by k !  If all of this seems confusing, we advise the reader \nto restrict his attention to fields of characteristic 0 or to sub fields of the \ncomplex numbers. \nIf c is a root of the polynomial f, the multiplicity of c as a root of \nf is the largest positive integer r such that (x - e)r divides f. \nThe multiplicity of a root is clearly less than or equal to the degree \nof f. For polynomials over fields of characteristic zero, the multiplicity \nof c as a root of f is related to the number of derivatives of f that are 0 at c. \nTheorem 6. Let F be a field of characteristic zero and f a polynomial \nover F with deg f ::; n. Then the scalar c is a root of f of multiplicity r if and \nonly if \n(Dkf) (c) \n0, \n0 ::; k ::; r \n1 \n(Dr!) (c) :r6 O. \nProof. Suppose that r is the multiplicity of e as a root of f. Then \nthere is a polynomial g such that f = (x \nc)rg and gee) 7'\" O. For other-\nSec. 4.4 \nPolY1Wmial Ideals \nwise f would be divisible by (x - c)r+l, by Corollary 1 of Theorem 4. By \nTaylor's formula applied to g \nf = (x - c)r [ni:,T (Dmg) (e) (x - e)m] \nm=O m! \nn - r  (Dmg) \n= { _. - (x - e)r+m \nm=O m! \nSince there is only one way to write f as a linear combination of the powers \n(x \nC)k (0 ::; k ::; n) it follows that \n(Dkf) (e) {o if 0 ::; k ::; r \n1 \nї-\nDk-rg(c) \n(k _ r) ! if r ::; k ::; n. \nTherefore, Dkf(c) = 0 for 0 ::; k ::; r - 1, and Drf(c) = g(c) ;;t. O. Con­\nversely, if these conditions are satisfied, it follows at once from Taylor's \nformula that there is a polynomial g such that f \n(x \ne)rg and gee) ;;t. o. \nNow suppose that r is not the largest positive integer such that (x - e)r \ndivides f. Then there is a polynomial h such that f = (x - e)'+lh. But \nthis implies g \n(x \nc)h, by Corollary 2 of Theorem 1 ;  hence gee) = 0, \na contradiction. I",
    "(x \ne)rg and gee) ;;t. o. \nNow suppose that r is not the largest positive integer such that (x - e)r \ndivides f. Then there is a polynomial h such that f = (x - e)'+lh. But \nthis implies g \n(x \nc)h, by Corollary 2 of Theorem 1 ;  hence gee) = 0, \na contradiction. I \nDefinition. Let F be a field. An ideal in F[x] is a subspace M of \nF[x] such that fg belongs to M whenever f is in F [x] and g is in M. \nEXAMPLE 5. If F is a field and d is a polynomial over F, the set \niV[ = dF[x], of all multiples df of d by arbitrary f in F[x], is an ideal. For \nM is non-empty, M in fact contains d. If f, g belong to F[x] and c is a \nscalar, then \nc(df) - dg = d(cf - g) \nbelongs to M, so that M is a subspace. Finally M contains (df)g = d(fg) \nas well. The ideal M is called the principal ideal generated by d. \nEXAMPLE 6. Let dl, • • •  , dn be a finite number of polynomials over F. \nThen the sum M of the subspaces diF[x} is a subspace and is also an ideal. \nFor suppose p belongs to M. Then there exist polynomials h . . .  , fn in \nF[x] such that p \n= ddl + . . . + dn!\". If g is an arbitrary polynomial \nover F, then \nso that pg also belongs to M. Thus M is an ideal, and we say that M is the \nideal generated by the polynomials, dJ, . . .  , dn• \nEXAMPLE 7. Let F be a sub field of the complex numbers, and con­\nsider the ideal \nM \n(x + 2)F[x] + (x2 + 8x + 16)F[x]. \n131 \n132 \nPolynomials \nWe assert that M = F[x]. For M contains \nx2 + 8x + 16 \nx(x + 2) = 6x + 16 \nChap. 4 \nand hence M contains 6x + 16 \n6(x + 2) \n4. Thus the scalar poly-\nnomial 1 belongs to M as well as all its multiples. \nTheorem 7. If F is a field, and M is any non-zero ideal in F[x], there \nis a unique monic polynomial d in F [x] such that M is the principal ideal \ngenerated by d. \nProof. By assumption, M contains a non-zero polynomial; among \nall non-zero polynomials in M there is a polynomial d of minimal degree. \nWe may assume d is monic, for otherwise we can multiply d by a scalar to",
    "generated by d. \nProof. By assumption, M contains a non-zero polynomial; among \nall non-zero polynomials in M there is a polynomial d of minimal degree. \nWe may assume d is monic, for otherwise we can multiply d by a scalar to \nmake it monic. Now if f belongs to M, Theorem 4 shows that f = dq + r \nwhere r = ° or deg r < deg d. Since d is in M, dq and f \ndq \nr also \nbelong to M. Because d is an element of M of minimal degree we cannot \nhave deg r < deg d, so r = 0. Thus M = dF[x]. If g is another monic \npolynomial such that M \ngF[x], then there exist non-zero polynomials \np, q such that d = gp and g = dq. Thus d = dpq and \ndeg d = deg d + deg p + deg q. \nHence deg p = deg q = 0, and as d, g are monic, p \nq \nd = g. I \n1. Thus \nIt is worth observing that in the proof just given we have used a \nspecial case of a more general and rather useful fact; namely, if p is a non­\nzero polynomial in an ideal M and if f is a polynomial in M which is not \ndivisible by p, then f \npq + r where the 'remainder' r belongs to M, is \ndifferent from 0, and has smaller degree than p. We have already made \nuse of this fact in Example 7 to show that the scalar polynomial 1 is the \nmonic generator of the ideal considered there. In principle it is always \npossible to find the monic polynomial generating a given non-zero ideal. \nFor one can ultimately obtain a polynomial in the ideal of minimal degree \nby a finite number of successive divisions. \nCorollary. If PI, . . .  , pn are polynomials over a field F, not all of \nwhich are 0, there is a unique monic polynomial d in F [x] such that \n(a) d is in the ideal generated by PI, . . . , pn; \n(b) d divides each of the polynomials Pi. \nAny polynomial satisfying (a) and (b) necessarily satisfies \n(c) d is divisible by every polynomial which divides each of the poly­\nnomials PI, . . . , pn. \nProof. Let d be the monic generator of the ideal \nP1F[x] + . .  , + p,.F[x]. \nSec. 4.4 \nPolynomial Ideas",
    "Any polynomial satisfying (a) and (b) necessarily satisfies \n(c) d is divisible by every polynomial which divides each of the poly­\nnomials PI, . . . , pn. \nProof. Let d be the monic generator of the ideal \nP1F[x] + . .  , + p,.F[x]. \nSec. 4.4 \nPolynomial Ideas \nEvery member of this ideal is divisible by d; thus each of the polynomials \nPi is divisible by d. Now suppose f is a polynomial which divides each of \nthe polynomials PI, . . .  , Pn' Then there exist polynomials gl, . .\n. , gn \nsuch that Pi = f% 1 s:; i s:; n. Also, since d is in the ideal \npIF[x] + . . .  + PnF[x], \nthere exist polynomials ql, . . .  , qn in F[x] such that \nd = plql + . . . + pnqn' \nThus \nd \nf[glql + . . . + g\"qn]' \nWe have shown that d is a monic polynomial satisfying (a), (b), and (c). \nIf d' is any polynomial satisfying (a) and (b) it follows, from (a) and the \ndefinition of d, that d' is a scalar multiple of d and satisfies (c) as well. \nFinally, in case d' is a monic polynomial, we have d' = d. I \nDefinition. If Ph . . .  , Pn are polynomials over a field F, not all of \nwhich are 0, the monic generator d of the ideal \nP1F[x] \n. . . + PnF[x] \nis called the greatest common divisor (g.c.d.) of PI, . . .  , pn. 'Phis \nterminology is justified by the preceding corollary. We say that the poly­\nnomials PI, . . .  , pn are relatively prime if their greatest common divisor \nis 1, or equivalently if the ideal they generate is all of F [x]. \nEXAMPLE 8. Let C be the field of complex numbers. Then \n(a) g.e.d. (x + 2, X2 + 8x + 16) = 1 (see Example 7) ; \n(b) g.c.d. « x \n2)2(X + i), (x - 2)(x2 + 1» \n= (x \n2)(x + i). For, \nthe ideal \ncontains \n(x - 2)2(X + i)F[x] + (x \n2) (X2 + l)F[x] \n(x - 2)2(X + i) - (x \n2) (x2 + 1) = (x - 2) (x + i) (i - 2). \nHence it contains (x \n2)(x + i), which is monic and divides both \n(x - 2)2(X + i) and (x - 2)(x2 + 1). \nEXAMPLE 9. Let F be the field of rational numbers and in F[x] let \nM be the ideal generated by \n(x - l)(x + 2)2, \n(x + 2)2(X - 3), \nand \n(x - 3).",
    "2) (x2 + 1) = (x - 2) (x + i) (i - 2). \nHence it contains (x \n2)(x + i), which is monic and divides both \n(x - 2)2(X + i) and (x - 2)(x2 + 1). \nEXAMPLE 9. Let F be the field of rational numbers and in F[x] let \nM be the ideal generated by \n(x - l)(x + 2)2, \n(x + 2)2(X - 3), \nand \n(x - 3). \nThen M contains \n!ex + 2)2[(X - 1) - (x - 3)J = (x + 2)2 \nand since \n(x + 2)2 = (x - 3)(x + 7) \n17 \n133 \n134 \nPolynomials \nChap. 4 \nM contains thc scalar polynomial l. Thus M = F [x] and the polynomials \n(x \nl)(x + 2)2, \n(x + 2)2(X \n3), \nand \n(x - 3) \nare relatively prime. \nExercises \n1. Let Q be the field of rational numbers. Determine which of the following subsets \nof Q[x] are ideals. When the set is an ideal, find its monic generator. \n(a) all f of even degree; \n(b) all f of degree 2': 5; \n(c) all f such that f(O) = 0; \n(d) all f such that f(2) = f(4) = 0; \n(e) all f in the range of the linear operator T defined by \nT( ± CiXi) = ± · Xi+l. \ni=O \ni=o t + l \n2. Find the g.c.d. of each of the following pairs of polynomials \n(a) 2x5 - X3 - 3x2 \n6x + 4, X4 + x3 - X2 - 2x - 2; \n(b) 3x4 + 8x2 - 3, x3 + 2X2 + 3x + 6; \n(c) X4 - 2x3 - 2X2 - 2x - 3, x3 + 6x2 + 7x + 1 . \n3. Let A be an n X n matrix over a field F. Show that the set of all polynomials \nf in F[x] such that f(A) \n0 is an ideal. \n4. Let F be a subfield of the complex numbers, and let \nA = [ȑ -;} \nFind the monic generator of the ideal of all polynomials f in F[x] such that \nf(A) = O. \n5. Let F be a field. Show that the intersection of any number of ideals in F[x] \nis an ideal. \n6. Let F be a field. Show that the ideal generated by a finite number of poly­\nnomials fl' . . .  , fn in F[xJ is the intersection of all ideals containing fl' . . . , fn. \n7. Let K be a sub field of a field F, and suppose f, g are polynomials in K[xJ. \nLet MK be the ideal generated by f and g in K[x] and lvlF be the ideal they generate \nin F[xJ. Show that llh and ME' have the same monic generator. \n4.5. The Prime Factorization \nof a Polynomial",
    "7. Let K be a sub field of a field F, and suppose f, g are polynomials in K[xJ. \nLet MK be the ideal generated by f and g in K[x] and lvlF be the ideal they generate \nin F[xJ. Show that llh and ME' have the same monic generator. \n4.5. The Prime Factorization \nof a Polynomial \nIn this section we shall prove that each polynomial over the field F \ncan be written as a product of 'prime' polynomials. This factorization \nprovides us with an effective tool for finding the greatest common divisor \nSec. 4.5 \nThe Prime Factorization of a Polynomial \nof a finite number of polynomials, and in particular, provides an effective \nmeans for deciding when the polynomials are relatively prime. \nDefinition. Let F be a field. A polynomial f in F[x] is said to be \nreducible oyer F 1f there eX'ist polynomials g, h in F [x] oj' degree ?:: 1 such \nthat f = gh, and if not, f is said to be irreducible oyer F. A non-scalar \nirreducible polynomial over F is called a prhne polynOInial oyer F, and we \nsometimes say it is a prhne in F[xJ . \nEXAMPLE 10. The polynomial X2 + 1 is reducible over the field C of \ncomplex numbers. For \nX2 + 1 \n= (x + i) (x - i) \nand the polynomials x + i, x \ni belong to C [xJ . On the other hand, \nX2 + 1 is irreducible over the field R of real numbers. For if \nx2 + 1 \n= (ax + b) (a'x + b') \nwith a, a', b, b' in R, then \naa' = 1, \nab' + ba' \n= 0, \nbb' \n= 1. \nThese relations imply a2 + b2 = 0, which is impossible with real numbers \na and b, unless a = b = O. \nTheorem 8. Let p, f, and g be polynomials over the field F. Suppose \nthat p is a prime polynomial and that p divides the product fg. Then either p \ndivides f or p divides g. \nProof. It is no loss of generality to assume that p is a monic prime \npolynomial. The fact that p is prime then simply says that the only monic \ndivisors of p are 1 and p. Let d be the g.c.d. of f and p. Then either \nd = 1 or d = p, sillce d is a monic polynomial which divides p. If d = p,",
    "Proof. It is no loss of generality to assume that p is a monic prime \npolynomial. The fact that p is prime then simply says that the only monic \ndivisors of p are 1 and p. Let d be the g.c.d. of f and p. Then either \nd = 1 or d = p, sillce d is a monic polynomial which divides p. If d = p, \nthen p divides f and we are done. So suppose d = 1 ,  i.e., suppose f and p \nare relatively prime. We shall prove that p divides g. Since (j, p) \n= 1, \nthere are polynomials fo and Po such that 1 \n= fof + pop. Multiplying by g, \nwe obtain \n9 \n= fofg + popg \n= (fg)fo + p(Pog). \nSince p divides fg i t  divides (fg)fo, and certainly p divides p(Pog). Thus \np divides g. I \nCorollary. If P is a prime and divides a product £1 . . . fn, then p divides \none of the polynomials fl' . . .  , fn• \nProof. The proof is by induction. When n = 2, the result is simply \nthe statement of Theorem 6. Suppose we have proved the corollary for \nn = k, and that p divides the product fl . . . fk+l of some (k + 1) poly-\n136 \n136 \nPolynomials \nChap. 4 \nnomials. Since P divides (f1 . . .  fk)fk+1, either P divides fk+l or P divides \nf1 . . . /k. By the induction hypothesis, if p divides h . . . fk' then p divides \nfj for some j, 1 5 j 5 k. So we see that in any case p must divide some Ii, \n1 5 j 5 k + 1. \nI \nTheorem 9. If F is a field, a non-scalar monic polynomial in F[x] can \nbe factored as a product of monic primes in F [x] in one and, except for order, \nonly one way. \nProof. Suppose f is a non-scalar monic polynomial over F. As \npolynomials of degree one are irreducible, there is nothing to prove if \ndeg f = 1. Suppose f has degree n > 1. By induction we may assume the \ntheorem is true for all non-scalar monic polynomials of degree less than n. \nIf f is irreducible, it is already factored as a product of monic primes, and \notherwise f = gh where g and h are non-scalar monic polynomials of \ndegree less than n. Thus g and h can be factored as products of monic",
    "theorem is true for all non-scalar monic polynomials of degree less than n. \nIf f is irreducible, it is already factored as a product of monic primes, and \notherwise f = gh where g and h are non-scalar monic polynomials of \ndegree less than n. Thus g and h can be factored as products of monic \nprimes in F[x] and hence so can f. Now suppose \nf = PI . . .  pm = ql . . . qn \nwhere Ph '  . . , Pm and ql, . . . , qn are monic primes in F[xJ. Then pm \ndivides the product qi . . . qn. By the above corollary, pm must divide \nsome qi. Since qi and pm are both monic primes, this means that \n(4-16) \nqi = pm. \nFrom (4-16) we see that m = n = 1 if either m = 1 or n = 1. For \nm \nn \ndegf = E deg Pi = { deg qj. \ni = 1  \nj = 1  \nI n  this case there is nothing more to prove, so we may assume m > 1 and \nn > 1. By rearranging the q's we can then assume pm = qn, and that \nPI . . .  pm-Ipm = qi . . .  qn-Ipm. \nNow by Corollary 2 of Theorem 1 it follows that \nPI . . .  pm-I = qi . . .  qn-I. \nAs the polynomial PI . . . pm-I has degree less than n, our inductive \nassumption applies and shows that the sequence ql, . .\n. , qn-l is at most \na rearrangement of the sequence Ph .\n. . , pm-I. This together with (4-16) \nshows that the factorization of f as a product of monic primes is unique \nup to the order of the factors. \nI \nIn the above factorization of a given non-scalar monic polynomial f, \nsome of the monic prime factors may be repeated. If Pl, P2, . . . , pr are \nthe distinct monic primes occurring in this factorization of f, then \n(4-17) \nf \np1'p৖\" . . pৗ', \nthe exponent ni being the number of times the prime Pi occurs in the \nSec. 4.5 \nThe Prime Factorization of a Polynomial \nfactorization. This decomposition is also clearly unique, and is called \nthe primary decomposition of f. It is easily verified that every monic \ndivisor of f has the form \n(4-18) \nFrom (4-18) it follows that the g.c.d. of a finite number of non-scalar",
    "The Prime Factorization of a Polynomial \nfactorization. This decomposition is also clearly unique, and is called \nthe primary decomposition of f. It is easily verified that every monic \ndivisor of f has the form \n(4-18) \nFrom (4-18) it follows that the g.c.d. of a finite number of non-scalar \nmonic polynomials fl' . . .  , fs is obtained by combining all those monic' \nprimes which occur simultaneously in the factorizations of /1, . . . , fs. \nThe exponent to which each prime is to be taken is the largest for which \nthe corresponding prime power is a factor of each f;. If no (non-trivial) \nprime power is a factor of each h the polynomials are relatively prime. \nEXAMPLE 11. Suppose F is a field, and let a, b, c be distinct elements \nof F. Then the polynomials x - a, x - b, x -\nc are distinct monic primes \nin F[x]. If m, n, and s are positive integers, (x - C)8 is the g.c.d. of the \npolynomials. \n(x - b)n(x - C)8 and (x - a)m(x - C)8 \nwhereas the three polynomials \n(x - b)n(x - C)8, \nare relatively prime. \nTheorem 10. Let f be a non-scalar monic polynomial over the field F \nand let \nf = prl . . . p؍k \nbe the prime factorization of f. For each j, 1 ::; j ::; k, let \nfj = f/p!; = II pf'. \ni r'j \nThen fl' . . .  , fk are relatively prime. \nProof. We leave the (easy) proof of this to the reader. We have \nstated this theorem largely because we wish to refer to it later. \nI \nTheorem 11. Let f be a polynomial over the field F with derivative f'. \nThen f is a product of distinct irreducible polynomials over F if and only if \nf and f' are relatively prime. \nProof. Suppose in the prime factorization of f over the field F \nthat some (non-scalar) prime polynomial p is repeated. Then f = p2h for \nsome h in F[x]. Then \nf' = pW + 2pp'h \nand p is also a divisor of f'. Hence f and f' are not relatively prime. \nNow suppose f = PI . . .  Pk, where PI, . . . , Pk are distinct non-scalar \nirreducible polynomials over F. Let Ji = flpj. Then \nf' = PUl + PU2 + . . . + PHik-\n137 \n138",
    "some h in F[x]. Then \nf' = pW + 2pp'h \nand p is also a divisor of f'. Hence f and f' are not relatively prime. \nNow suppose f = PI . . .  Pk, where PI, . . . , Pk are distinct non-scalar \nirreducible polynomials over F. Let Ji = flpj. Then \nf' = PUl + PU2 + . . . + PHik-\n137 \n138 \nPolynomials \nChap. 4 \nLet P be a prime polynomial which divides both f and f'. Then P = Pi for \nsome i. Now Pi divides fj for j ;t. i, and since Pi also divides \nk \nif \nk pjIi \nj ܮl \nwe see that Pi must divide P;ji. Therefore Pi divides either fi or Pt. But Pi \ndoes not divide f; since pt, . . .  , Pk are distinct. So Pi divides p:. This is \nnot possible, since P; has degree one less than the degree of Pi. We con­\nclude that no prime divides both f and f', or that, f and l' are relatively \nprime. \nI \nDefinition. The field F is called algebraically closed if every prime \npolynomial over F has degree 1 .  \nTo say that F is algebraically closed means every non-scalar irreduc­\nible monic polynomial over F is of the form (x - e). We have already \nobserved that each such polynomial is irreducible for any F. Accordingly, \nan equivalent definition of an algebraically closed field is a field F such \nthat each non-scalar polynomial f in F [x] can be expressed in the form \nf = c(x \ne1)m . . .  (x \nCk)n. \nwhere c is a scalar, Cl, .\n•\n.\n , Ck are distinct elements of F, and nI, . . .  , nk \nare positive integers. Still another formulation is that if f is a non-scalar \npolynomial over F, then there is an element c in F such that f(c) = O. \nThe field R of real numbers is not algebraically closed, since the poly­\nnomial (x2 + 1) is irreducible over R but not of degree 1, or, because \nthere is no real number c such that c2 + 1 \n= O. The so-called Funda­\nmental Theorem of Algebra states that the field C of complex numbers is \nalgebraically closed. We shall not prove this theorem, although we shall \nuse it somewhat later in this book. The proof is omitted partly because",
    "there is no real number c such that c2 + 1 \n= O. The so-called Funda­\nmental Theorem of Algebra states that the field C of complex numbers is \nalgebraically closed. We shall not prove this theorem, although we shall \nuse it somewhat later in this book. The proof is omitted partly because \nof the limitations of time and partly because the proof depends upon a \n'non-algebraic' property of the system of real numbers. For one possible \nproof the interested reader may consult the book by Schreier and Sperner \nin the Bibliography. \nThe Fundamental Theorem of Algebra also makes it clear what the \npossibilities are for the prime fl'tctorization of a polynomial with real \ncoefficients. If f is a polynomial with real coefficients and c is a complex \nroot of f, then the complex conjugate c is also a root of f. Therefore, those \ncomplex roots which are not real must occur in conjugate pairs, and the \nentire set of roots has the form {tl' . . .  , tk, el, CI, . . .  , er, c,.} where tt, . . .  , tk \nare real and CI, •\n•\n•\n , Cr are non-real complex numbers. Thus f factors \nf = c(x - i1) • • •  (x - tk)PI . . .  P, \nwhere Pi is the quadratic polynomial \nPi \n(x - Ci)(X \nOi). \nSec. 4.5 \nThe Prime Factorization of a Polynomial \nThese polynomials Pi have real coefficients. We conclude that every \nirreducible polynomial over the real number field has degree 1 \nor 2. Each \npolynomial over R is the product of certain linear factors, obtained from \nthe real roots of j, and certain irreducible quadratic polynomials. \nExercises \n1. Let p be a monic polynomial over the field F, and let f and g be relatively \nprime polynomials over F. Prove that the g.c.d. of pf and pg is p. \n2. Assuming the Fundamental Theorem of Algebra, prove the following, If f and \ng are polynomials over the field of complex numbers, then g.c.d. (f, g) = 1 if and \nonly if f and g have no common root. \n3. Let D be the differentiation operator on the space of polynomials over the",
    "2. Assuming the Fundamental Theorem of Algebra, prove the following, If f and \ng are polynomials over the field of complex numbers, then g.c.d. (f, g) = 1 if and \nonly if f and g have no common root. \n3. Let D be the differentiation operator on the space of polynomials over the \nfield of complex numbers. Let f be a monic polynomial over the field of complex \nnumbers. Prove that \nf \n(x - C1) • • • (x - Ck) \nwhere CI, \n•\n•\n•\n , Ck are distinct complex numbers if and only if f and Df are relatively \nprime. In other words, f has no repeated root if and only if f and Df have no com­\nmon root. (Assume the Fundamental Theorem of Algebra.) \n4. Prove the following generalization of Taylor's formula. Let f, g, and h be \npolynomials over a subfield of the complex numbers, with deg f S; n. Then \nn \n1 \nf(g) = \nk;o kiJ(k)(h)(g - h)k. \n(Here f(g) denotes 'f of g.') \nFor the remaining exercises, we shall need the following definition. If f, g, \nand p are polynomials over the field P with p ,= 0, we say that f is congruent to g \nmodulo p if (f - g) is divisible by p. If f is congruent to g modulo p, we write \nf == g mod p. \n5. Prove, for any non-zero polynomial p, that congruence modulo p is an equiva-\nlence relation. \n(a) It is reflexive: f \nf mod p. \n(b) It is symmetric: if f == g mod p, then g == f mod p. \n(c) It is transitive: if f == g mod p and g \nh mod p, then f == h mod p. \n6. Suppose f \ng mod p and fl \ngl mod p. \n(a) Prove that f + fl == g + gl mod p. \n(b) Prove that fir == ggl mod p. \n7. Use Exercise 7 to prove the following. Iff, g, h, and p are polynomials over the \nfield F and p ,= 0, and if f \ng mod p, then hU) \nh(g) mod p. \n8. If p is an irreducible polynomial and fg == 0 mod p, prove that either \nf == 0 mod p or g == 0 mod p. Give an example which shows that. this is false if p \nis not irreducible. \n139 \n5. Determinants \n5.1 . Commutative Rings \nIn this chapter we shall prove the essential facts about determinants",
    "8. If p is an irreducible polynomial and fg == 0 mod p, prove that either \nf == 0 mod p or g == 0 mod p. Give an example which shows that. this is false if p \nis not irreducible. \n139 \n5. Determinants \n5.1 . Commutative Rings \nIn this chapter we shall prove the essential facts about determinants \nof square matrices. We shall do this not only for matrices over a field, but \nalso for matrices with entries which are 'scalars' of a more general type. \nThere are two reasons for this generality. First, at certain points in the \nnext chapter, we shall find it necessary to deal with determinants of \nmatrices with polynomial entries. Second, in the treatment of determi­\nnants which we present, one of the axioms for a field plays no role, namely, \nthe axiom which guarantees a multiplicative inverse for each non-zero \nelement. For these reasons, it is appropriate to develop the theory of \ndeterminant8 for matrices, the entries of which are elements from a com­\nmutative ring with identity. \nDefinition. A ring is a set K, together with two operations (x, y) -+ \nx + y and (x, y) -+ xy satisfying \n(a) K is a commutative group under the operation (x, y) -+ x + y (K \nis a commutative group under addition) ; \n(b) (xy)z \nx(yz) (multiplication is associative) ; \n(c) x(y + z) \nxy + xz; (y + z)x = yx + zx (the two distributive \nlaws hold). \nIfxy \nyxfor all x and y in K, we say that the ring K is commutative. \nIf there is an element I in K such that Ix \nxl = x for each x, K is said \nto be a ring with identity, and I is called the identity for K. \n140 \nSec. 5.2 \nDeterminant Functions \nWe are interested here in commutative rings with identity. Such a \nring can be described briefly as a set K, together with two operations \nwhich satisfy all the axioms for a field given in Chapter 1, except possibly \nfor axiom (8) and the condition 1 rf O. Thus, a field is a commutative \nring with non-zero identity such that to each non-zero x there corresponds",
    "ring can be described briefly as a set K, together with two operations \nwhich satisfy all the axioms for a field given in Chapter 1, except possibly \nfor axiom (8) and the condition 1 rf O. Thus, a field is a commutative \nring with non-zero identity such that to each non-zero x there corresponds \nan element X-I with xx-I = 1. The set of integers, with the usual opera­\ntions, is a commutative ring with identity which is not a field. Another \ncommutative ring with identity is the set of all polynomials over a field, \ntogether with the addition and multiplication which we have defined for \npolynomials. \nIf K is a commutative ring with identity, we define an m X n matrix \nover K to be a function A from the set of pairs (i, j) of integers, 1 ყ i ყ m, \n1 ყ j ყ n, into K. As usual we represent such a matrix by a rectangular \narray having m rows and n columns. The sum and product of matrices \nover K are defined as for matrices over a field \n(A + B)ij = Aij + Bij \n(AB)ij = F AikBkj \nk \nthe sum being defined when A and B have the same number of rows and \nthe same number of columns, the product being defined when the number \nof columns of A is equal to the number of rows of B. The basic algebraic \nproperties of these operations are again valid. For example, \nA (B + C) = AB + AC, \n(AB)C = A (BC), \netc. \nAs in the case of fields, we shall refer to the elements of K as scalars. \nWe may then define linear combinations of the rows or columns of a \nmatrix as we did earlier. Roughly speaking, all that we previously did for \nmatrices over a field is valid for matrices over K, excluding those results \nwhich depended upon the ability to idivide' in K. \n141 \n5.2. Determinant Functions \nLet K be a commutative ring with identity. We wish to assign to \neach n X n (square) matrix over K a scalar (element of K) to be known \nas the determinant of the matrix. It is possible to define the determinant \nof a square matrix A by simply writing down a formula for this determi­",
    "Let K be a commutative ring with identity. We wish to assign to \neach n X n (square) matrix over K a scalar (element of K) to be known \nas the determinant of the matrix. It is possible to define the determinant \nof a square matrix A by simply writing down a formula for this determi­\nnant in terms of the entries of A. One can then deduce the various prop­\nerties of determinants from this formula. However, such a formula is \nrather complicated, and to gain some technical advantage we shall proceed \nas follows. We shall define a ideterminant function' on KnXn as a function \nwhich assigns to each n X n matrix over K a scalar, the function having \nthese special properties. It is linear as a function of each of the rows of the \n142 \nDeterminants \nChap. 5 \nmatrix: its value is 0 on any matrix having two equal rows; and its value \non the n X n identity matrix is 1. We shall prove that such a function \nexists, and then that it is unique, i.e., that there is precisely one such \nfunction. As we prove the uniqueness, an explicit formula for the determi­\nnant will be obtained, along with many of its useful properties. \nThis section will be devoted to the definition of 'determinant function' \nand to the proof that at least one such function exists. \nDefinition. Let K be a commutative ring with identity, n a positive \ninteger, and let D be a function which assigns to each n X n matrix A over K \na scalar D (A) in K We say that D is n-linear if for each i, 1 S i S  n, \nD is a linear function of the ith row when the other (n - 1) rows are held fixed. \nThis definition requires some clarification. If D is a function from \nKnXn into K, and if aI, . . . , an are the rows of the matrix A, let us also \nwrite \nD(A) = D(al' . . .  , a৘) \nthat is, let us also think of D as the function of the rows of A. The state­\nment that D is n-linear then means \n(5-1) \nDCal, . . . , Cai + a;, . . .  , an) \n= cD(al, . .\n. , ai, . . .  , an) \nD(al, . . .  , a;, . . .  , a,,).",
    "write \nD(A) = D(al' . . .  , a৘) \nthat is, let us also think of D as the function of the rows of A. The state­\nment that D is n-linear then means \n(5-1) \nDCal, . . . , Cai + a;, . . .  , an) \n= cD(al, . .\n. , ai, . . .  , an) \nD(al, . . .  , a;, . . .  , a,,). \nIf we fix all rows except row i and regard D as a function of the ith row, \nit is often convenient to write D(a;) for D(A). Thus, we may abbreviate \n(5-1) to \nD(cai \naD \n= cD(ai) + D(a;) \nso long as it is clear what the meaning is. \nEXAMPLE 1. Let k1, •\n•\n•\n , k\" be positive integers, 1 S ki S n, and \nlet a be an element of K. For each n X n matrix A over K, define \n(5-2) \nThen the function D defined by (5-2) is n-linear. For, if we regard D as a \nfunction of the ith row of A, the others being fixed, we may write \nDCai) \nA (i, ki)b \nwhere b is some fixed element of K. Let a& \n(Afl, . . .  , Ai,,). Then we \nhave \nD(cai + ai৙) = [cA (i, ki) + A'(i, ki)]b \n= cD(ai) \nD(ai). \nThus D is a linear function of each of the rows of A . \nA particular n-linear function of this type is \nD(A) = AuA22 .\n.\n.\n An,,' \nSec. 5.2 \nDeterminant Functions \nIn other words, the Iproduct of the diagonal entries' is an n-linear function \non KnXn. \nEXAMPLE 2. Let us find all 2-linear functions on 2 X 2 matrices over \nK. Lct D be such a function. If we denote the rows of the 2 X 2 identity \nmatrix by El, E2, we have \nD(A) = D(AllEl + A12E2, A21El + A22E2). \nUsing the fact that D is 2-1inear, (.5-1), we have \nD(A) = AllD(El, A21El + A22E2) + A12D(E2' A21El + A22E2) \nAnA21D(El, EI) + AuA22D(El, E2) \n+ A12A21D(E2, Ej) + AI2A22D(E2, (2). \nThus D is completely determined by the four scalars \nD(EI, El), \nD(El' (2), \nD(E2, El), \nand \nD(€2, E2)' \nThe reader should find it easy to verify the following. If a, b, c, d are any \nfour scalars in K and if we define \nD(A) \nAllA21a + AllA22b + A12A21C + AIZA2Zd \nthen D is a 2-linear function on 2 X 2 matrices over K and \nD(El, El) \na, \nD(El, E2) \nb \nD(E2, El) = c, \nD(E2, E2) = d.",
    "and \nD(€2, E2)' \nThe reader should find it easy to verify the following. If a, b, c, d are any \nfour scalars in K and if we define \nD(A) \nAllA21a + AllA22b + A12A21C + AIZA2Zd \nthen D is a 2-linear function on 2 X 2 matrices over K and \nD(El, El) \na, \nD(El, E2) \nb \nD(E2, El) = c, \nD(E2, E2) = d. \nLemma. A linear combination of n-linear functions is n-linear. \nProof. It suffices to prove that a linear combination of two \nn-linear functions is n-linear. Let D and E be n-linear functions. If a and b \nbelong to K, the linear combination aD + bE is of course defined by \n(aD + bB)(A) \naD(A) + bE(A). \nHence, if we fix all rows except row i \n(aD + bE)(CCXi + a;) = aD(ca, + a;) + bE(ca, + a;) \n= acD(a.;) + aD(a৚) + bcE(ai) + bE(a;) \n= c(aD + bE)(ai) + (aD + bE)(a;). I \nIf K is a field and V is the set of n X n matrices over K, the above \nlemma says the following. The set of n-linear functions on V is a subspace \nof the space of all functions from V into K. \nEXAMPLE 3. Let D be the function defined on 2 X 2 matrices over \nK by \n(5-3) \nNow D is the sum of two functions of the type described in Example 1 :  \nD = Dl + D2 \nD1(A) = AllA22 \nD2(A) \n-A12A21. \n144 \nDeterminants \nChap. 5 \nBy the above lemma, D is a 2-linear function. The reader who has had \nany experience with determinants will not find this surprising, since he \nwill recognize (5-3) as the usual definition of the determinant of a 2 X 2 \nmatrix. Of course the function D we have just defined is not a typical \n2··linear function. It has many special properties. Let us note some of these \nproperties. First, if I is the 2 X 2 identity matrix, then D(I) = 1, i.e., \nD(€l, €2) \n1. Second, if the two rows of A are equal, then \nD(A) = AnA12 - A12All \n0. \nThird, if A' is the matrix obtained from a 2 X 2 matrix A by interchang­\ning its rows, then D(A') = -D(A) ; for \nD(A') = AilA2 - AbAh \n= A21A12 - A22Au \n= -D(A). \nDefinition. Let D be an n-linear function. We say D is alternating",
    "D(A) = AnA12 - A12All \n0. \nThird, if A' is the matrix obtained from a 2 X 2 matrix A by interchang­\ning its rows, then D(A') = -D(A) ; for \nD(A') = AilA2 - AbAh \n= A21A12 - A22Au \n= -D(A). \nDefinition. Let D be an n-linear function. We say D is alternating \n(or alternate) if the following two conditions are satisfied: \n(a) D(A) = ° whenever two rows of A are equal. \n(b) If A' is a matrix obtained from A by interchanging two rows of A, \nthen D(A') = -D(A). \nWe shall prove below that any n-linear function D which satisfies (a) \nautomatically satisfies (b). We have put both properties in the definition \nof alternating n-linear function as a matter of convenience. The reader \nwill probably also note that if D satisfies (b) and A is a matrix with two \nequal rows, then D(A) = -D(A). It is tempting to conclude that D \nsatisfies condition (a) as well. This is true, for example, if K is a field in \nwhich 1 + 1 ;r: 0, but in general (a) is not a consequence of (b). \nDefinition. Let K be a commutative ring with identity, and let n be a \npositive integer. Suppose D is a function from n X n matrices over K into \nK. We say that D is a determinant function if D is n-linear, alternating, \nand D(I) = 1 .  \nAs we stated earlier, we shall ultimately show that there is exactly \none determinant function on n X n matrices over K. This is easily seen \nfor 1 X 1 matrices A \n[a] over K. The function D given by D(A) = a \nis a determinant function, and clearly this is the only determinant func­\ntion on 1 X 1 matrices. We are also in a position to dispose of the case \nn = 2. The function \nD(A) = AnA22 - A12A21 \nwas shown in Example 3 to be a determinant function. Furthermore, the \nformula exhibited in Example 2 shows that D is the only determinant \nSec. 5.2 \nDeterminant Functions \nfunction on 2 X 2 matrices. For we showed that for any 2-1inear function D \nD(A) = A llA21D(`1, `1) + AllA22D(€1, €2) \n+ A12Az1D(€2, €l) + Al2Az2D(€2, €2)' \nIf D is alternating, then \nand \nD(€2, fl) \n= -D(El' €2)",
    "Sec. 5.2 \nDeterminant Functions \nfunction on 2 X 2 matrices. For we showed that for any 2-1inear function D \nD(A) = A llA21D(`1, `1) + AllA22D(€1, €2) \n+ A12Az1D(€2, €l) + Al2Az2D(€2, €2)' \nIf D is alternating, then \nand \nD(€2, fl) \n= -D(El' €2) \n= - D(l). \nIf D also satisfies D(l) \n1, then \nD(A) \n= AuA22 - A12A21• \nEXAMPLE 4. Let F be a field and let D be any alternating 3-linear \nfunction on 3 X 3 matrices over the polynomial ring F[x}. \nLet \nA [\u0003 \u0004 _\u0003\n2J' \n1 \n0 \nx3 \nIf we denote the rows of the 3 X 3 identity matrix by €l, €2, `3, then \nD(A) \nD(X€l \nX2€3, €2, €l + X3€3)' \nSince D is linear as a function of each row, \nD(A) = XD(€l, €2, `l + X3€3) - x2D(`3, `2, €l + X3`3) \nXD(`l, €2, €1) + x4D(`l, `2, `3) - x2D(€3, `2, €l) \nx5D(€a, `2, fa). \nBecause D is alternating it follows that \nD(A) = (x4 + x2)DCEl' `2, Ea) . \nLemma. Let D be a 2-linear function with the property that DCA) \n= 0 \nfor all 2 X 2 matrices A over K having equal rows. 'Phen D is altemating. \nProof. What we must show is that if A is a 2 X 2 matrix and A I \nis obtained by interchanging the rows of A, then D(A ') \n= -DCA). If the \nrows of A are 0: and (3, this means we must show that D«(3, 0:) \n-D(o:, f3). \nSince D is 2-1inear, \nD(o: + (3, 0: + (3) = D(o:, 0:) + D (o:, (3) + D«(3, 0:) + D«(3, (3) . \nBy our hypothesis D Co: + (3, 0: + (3) \nDCo:, o:) \nD«(3, (3) \nO. So \no = DCo:, (3) + D«(3, 0:). \nI \nLemma. Let D be an n-linear function on n X n matrices over K. \nSuppose D has the property that DCA) \n= 0 whenever two adjacent rows of \nA are equal. Then D is alternating. \nProof. We must show that DCA) \n= 0 when any two rows of A \nare equal, and that D(A') \n= -D(A) if A' is obtained by interchanging \nDeterminants \nChap. S \nsome two rows of A .  First, let us suppose that A' is obtained by inter­\nchanging two adjacent rows of A .  The reader should see that the argument \nused in the proof of the preceding lemma extends to the present case and \ngives us D(A') \n= -DCA).",
    "Determinants \nChap. S \nsome two rows of A .  First, let us suppose that A' is obtained by inter­\nchanging two adjacent rows of A .  The reader should see that the argument \nused in the proof of the preceding lemma extends to the present case and \ngives us D(A') \n= -DCA). \nNow let B b e  obtained by interchanging rows i and j of A, where \ni < j. We can obtain B from A by a succession of interchanges of pairs of \nadjacent rows. We begin by interchanging row i with row (i + 1) and \ncontinue until the rows are in the order \nThis requires Ie \n= j \ni interchanges of adjacent rows. We now move Cij \nto the ith position using (k - 1) interchanges of adjacent rows. We have \nthus obtained B from A by Ie + (k - 1) \n= 2k - 1 interchanges of adja­\ncent rows. Thus \nD(B) = \n1)2k-lD(A) = -DCA). \nSuppose A is any n X n matrix with two equal rows, say Ci.i \n= Ci.j \nwith i < j. If j = i + 1 ,  then A has two equal and adjacent rows and \nD(A) = O. If j > i + 1, we interchange Ci.i+l and Ci.j and the resulting \nmatrix B has two equal and adjacent rows, so D(B) = O. On the other \nhand, DCB) \n-DCA), hence DCA) \nO. \nI \nDefinition. If n > 1 and A is an n X n matrix over K, we let AOID \ndenote the (n \n1) X (n \n1) matrix obtained by deleting the ith row and \njth column of A. If D is an (n \n- 1)-linear function and A is a1\u0015 n X n \nmatrix, we put Dij(A) \nD[AOID]' \nTheorem 1. Let n > 1 and let D be an alternating (n \nI)-linear \nfunction on (n \n1) X (n \n1) matricea over K. For each j, 1 S; j S; n ,  \nthe function E) defined by \nn \n(5-4) \nEj(A) \n=  ( - l)i+iAuDij(A) \ni=l \nis an alternating n-linear function on n X n matrices A. If D is a determi­\nnant function, so is each Ej• \nProof. If A. is an n X n matrix, Dij(A) is independent of the ith \nrow of A .  Since D is (n - I)-linear, it is clear that Dij is linear as a func­\ntion of any row except row i. Therefore AijDij(A) is an n-linear function \nof A. A linear combination of n-linear functions is n-linear; hence, Ej is",
    "Proof. If A. is an n X n matrix, Dij(A) is independent of the ith \nrow of A .  Since D is (n - I)-linear, it is clear that Dij is linear as a func­\ntion of any row except row i. Therefore AijDij(A) is an n-linear function \nof A. A linear combination of n-linear functions is n-linear; hence, Ej is \nn-linear. To prove that Ej is alternating, it will suffice to show tha.t \nEj(A) \n= 0 whenever A has two equal and adjacent rows. Suppose ak \n= \nCi.k+l. If i :;6- k and i :;6- k + 1 ,  the matrix A(ilj) has two equal rows, and \nthus Dij(A) \n= O. Therefore \nEj(A) \n( -l)k+iAkjDkj(A) + ( - 1)k+HiA(.I;+l)jDtk+1)j(A). \nSec. 5.2 \nDeterminant Functions \nSince ak \n= ak+l, \nAkj = A(k+l)j and A (kli) = A (k + Iii). \nClearly then Ej(A) = O. \nNow suppose D is a determinant function. If [(n) is the n X n identity \nmatrix, then [(n)(jlj) is the (n \n1) X en \n1) identity matrix [(n-O. \nSince [17) = Oij, it follows from (5-4) that \n(5-5) \nEj(I(n») = D(I(n-1l). \nNow D([(n-[)) = 1, so that Ej(J<n») = 1 and Ej is a determinant func­\ntion. \nI \nCorollary. Let K be a commutative ring with identity and let n be a \npositive integer. There eXl:sts at least one determinant function on J{nXn. \nProof. We have shown the existence of a determinant function \non 1 X 1 matrices over K, and even on 2 X 2 matrices over K. Theorem 1 \ntells us explicitly how to construct a determinant function on n X n \nmatrices, given such a function on (n \n1) X (n - 1) matrices. The \ncorollary follows by induction. \nI \nEXAMPLE 5. If B is a 2 X 2 matrix over K, we let \nIBI = BnB22 - B12B21• \nThen IBI = D(B), where D is the determinant function on 2 X 2 matrices. \nWe showed that this function on K2X2 is unique. Let \nA [\u0011Ŗ: \u0011:: \u0011::J \nA 31 A32 \nAss \nbe a 3 X 3 matrix over K. If we define El, E2, Ea as in (5-4), then \n(5-6) \nEl(A) = \nA I\nA22 \nA231 \nA I\nA12 \nAl31 + A3lIA12 \nA131 \nn An Au \nu An Au \nA \nAu \n(5-7) \nE2(A) = -A121\u0011: Ȏ::I + A221\u0011: pȏ:I - Aa21\u0011:: pȐ:I \n(5-8) \nE3(A) \nA131A21 A221 A231All A121 + AS31An \nA 121· \nA3l An",
    "A 31 A32 \nAss \nbe a 3 X 3 matrix over K. If we define El, E2, Ea as in (5-4), then \n(5-6) \nEl(A) = \nA I\nA22 \nA231 \nA I\nA12 \nAl31 + A3lIA12 \nA131 \nn An Au \nu An Au \nA \nAu \n(5-7) \nE2(A) = -A121\u0011: Ȏ::I + A221\u0011: pȏ:I - Aa21\u0011:: pȐ:I \n(5-8) \nE3(A) \nA131A21 A221 A231All A121 + AS31An \nA 121· \nA3l An \nA3l An \nAu A \nIt follows from Theorem 1 that El, E2, and Ea are determinant functions. \nActually, as we shall show later, El \nE2 \nEa, but this is not yet appar­\nent even in this simple case. It could, however, be verified directly, by \nexpanding each of the above expressions. Instead of doing this we give \nsome specific examples. \n(a) Let K \nR [x] and \nA{P\nl 'ŗ2 \nx3 ] \n1 \n. \nx - 3 \n147 \n148 \nDeterminants \nThen \nand \nIx 2 \nE1(A) = (x - 1) 0 \n1 I = (x - l)(x - 2)(x - 3) \nx - 3  \nE2(A) -x213 x  31 + (x - 2)\\X  1 \n= (x - l)(x - 2)(x - 3) \nx3 I \nx - 3 \nOhap. 5 \nEa(A) = x313 x  21 Ix  1 521 (x _ 3)lx - 1 x2 I \no x - 2 \n= (x - l)(x - 2)(x - 3). \n(b) Let K = R and \nThen \nExercises \n[0 1 \nA = 0 0 \n1 0 \nE1(A) = I9 \u001fI \n= 1 \nE2(A) \nI6 6/ \n1 \nEa(A) = -Io 9/ = 1. \n1. Each of the following expressions defines a function D on the set of 3 X 3 \nmatrices over the field of real numbers. In which of these cases is D a 3-linear \nfunction? \n(a) D(A) = Au + A22 + A3a; \n(b) D(A) \n(AU)2 + 3AllA22; \n(c) D(A) = AuA12A33; \n(d) D(A) = A13A22A32 + 5A12A22A32; \n(e) D(A) \n0; \n(f) D(A) = 1. \n2. Verify directly that the three functions El, Ez, E3 defined by (5-6), (5-7), and \n(5-8) are identical. \n3. Let K be a commutative ring with identity. If A is a 2 X 2 matrix over K, \nthe classical adjoint of A is the 2 X 2 matrix adj A defined by \nadj A = [ A22 \n-Al2]. \n-A2l \nAu \nIf det denotes the unique determinant function on 2 X 2 matrices over K, show \nthat \nSec. 5.2 \n(a) (adj A)A = A (adj A) = (det A)I; \n(b) det (adj A) = det (A) ; \n(c) adj (At) = (adj A)t. \n(At denotes the transpose of A.) \nDeterminant Functions \n4. Let A be a 2 X 2 matrix over a field F. Show that A is invertible if and only",
    "that \nSec. 5.2 \n(a) (adj A)A = A (adj A) = (det A)I; \n(b) det (adj A) = det (A) ; \n(c) adj (At) = (adj A)t. \n(At denotes the transpose of A.) \nDeterminant Functions \n4. Let A be a 2 X 2 matrix over a field F. Show that A is invertible if and only \nif det A ¥- O. When A is invertible, give a formula for A-I. \n5. Let A be a 2 X 2 matrix over a field F, and suppose that A 2 = O. Show for \neach scalar C that det (cl - A) \nc2• \n6. Let K be a subfield of the complex numbers and n a positive integer. Let \njl, . . .  , jn and kl' . . .  , kn be positive integers not exceeding n. For an n X n \nmatrix A over K define \nD(A) = A UI, kl)AU2, k2) \n•\n•\n•\n A Un, kn) .  \nProve that D is n-linear if and only if the integers jt, . . .  , jn are distinct. \n7. Let K be a commutative ring with identity. Show that the determinant func­\ntion on 2 X 2 matrices A over K is alternating and 2-linear as a function of the \ncolumns of A. \n8. Let K be a commutative ring with identity. Define a function D on 3 X 3 \nmatrices over K by the rule \nDCA) = All det [A22 A23] - Au det [A21 \nA23] + Al3 det [A21 \nA22]. \nAn AC \nAu AC \nAu \nAn \nShow that D is alternating and 3-linear as a function of the columns of A. \n9. Let K be a commutative ring with identity and D an alternating n-linear \nfunction on n X n matrices over K. Show that \n(a) D(A) = 0, if one of the rows of A is O. \n(b) D(B) = D(A), if B is obtained from A by adding a scalar multiple of \none row of A to another. \n10. Let F be a field, A a 2 X 3 matrix over F, and (CI, C2, Ca) the vector in Fa \ndefined by \nShow that \n(a) rank (A) = 2 if and only if (CI, C2, Ca) ¥- 0; \n(b) if A has rank 2, then (CI, C2, ca) is a basis for the solution space of the \nsystem of equations AX = O. \n11. Let K be a commutative ring with identity, and let D be an alternating 2-linear \nfunction on 2 X 2 matrices over K. Show that D(A) = (det A)D(l) for all A. \nNow use this result (no computations with the entries allowed) to show that \ndet (AB)",
    "system of equations AX = O. \n11. Let K be a commutative ring with identity, and let D be an alternating 2-linear \nfunction on 2 X 2 matrices over K. Show that D(A) = (det A)D(l) for all A. \nNow use this result (no computations with the entries allowed) to show that \ndet (AB) \n(det A)(det B) for any 2 X 2 matrices A and B over K. \n12. Let F be a field and D a function on n X n matrices over F (with values in F) . \nSuppose D(AB) \n= D(A)D(B) for all A, B. Show that either D(A) = 0 for all A, \nor D(I) \n= 1. In the latter case show that DCA) ¥- 0 whenever A is invertible. \n13. Let R be the field of real numbers, and let D be a function on 2 X 2 matrices \n149 \n150 \nDeterminants \nChap. 5 \nover R, with values in R, such that D(AB) = D(A )D(B) for all A ,  B. Suppose \nalso that \nProve the following. \n(a) D(O) \n= 0 ;  \n(b) D(A) = 0 i f  A 2  = 0 ;  \n(c) D(B) \n= - D(A ) i f  B i s  obtained b y  interchanging the rows (or columns) \nof A ;  \n(d) D(A )  = 0 if one row (or one column) of A is OJ \n(e) D(A) = 0 whenever A is singular. \n14. Let A be a 2 X 2 matrix over a field P. Then the set of all matrices of the \nform I(A), where I is a polynomial over P, is a commutative ring K with identity. \nIf B is a 2 X 2 matrix over K, the determinant of B is then a 2 X 2 matrix over F, \nof the form I(A). Suppose I is the 2 X 2 identity matrix over P and that B is the \n2 X 2 matrix over K \nB = [A - AnI \n- A 12I J. \n- A2tf \nA - A22I \nShow that det B = I(A), where 1 =  x2 - (An + Azz)x + det A, and also that \nI(A) = O. \n5.3. Permutations and the Uniqueness \nof Determinants \nIn this section we prove the uniqueness of the determinant function \non n X n matrices over K. The proof will lead us quite naturally to con­\nsider permutations and some of their basic properties. \nSuppose D is an alternating n-linear function on n X n matrices over \nK. Let A be an n X n matrix over K with rows a!, a2, . . .  , an. If we de­\nnote the rows of the n X n identity matrix over K by tl, t2, . . .  , tn) then",
    "sider permutations and some of their basic properties. \nSuppose D is an alternating n-linear function on n X n matrices over \nK. Let A be an n X n matrix over K with rows a!, a2, . . .  , an. If we de­\nnote the rows of the n X n identity matrix over K by tl, t2, . . .  , tn) then \n(5-9) \nHence \nn \nai =  A (i, J)tj, \nj = l \n1 ::; i ::; n. \nD(A) = D (;,; A (l, j)tj, a2, . . .  , an) \n=  A (1, j)D(7i> a2, . . .  , an). \nj \nIf we now replace a2 by  A (2, kh, we see that \nk \nThus \nD(A) =  A (1, j)A (2, k)D(Ej, Ek, •\n•\n• , an). \ni.k \nSec. 5.3 \nPermutations and the Uniqueness of Determinants \nIn D(fj, fk, .\n.\n.\n , an) we next replace aa by .2\": A (3, l)€l and so on. We finally \nobtain a complicated but theoretically important expression for D (A ) ,  \nnamely \n(,5-10) \nD (A )  \nIn (5-10) the sum is extended over all sequences (kI, k2' . . .  , len) of positive \nintegers not exceeding n. This shows that D is a finite sum of functions of \nthe type described by (5-2). It should be noted that (5-10) is a consequence \njust of assumption that D is n-linear, and that a special case of (5-10) was \nobtained in Example 2. Since D is alternating, \nD(Ekl) Ek\" . . .  , Ek.) = 0 \nwhenever two of the indices ki are equal. A sequence (kI' le2, •\n•\n•\n , kn) \nof positive integers not exceeding n, with the property that no two of \nthe ki are equal, is called a perllutation of degree n. In (5-10) we need \ntherefore sum only over those sequences which are permutations of \ndegree n. \nSince a finite sequence, or n-tuple, is a function defined on the first n \npositive integers, a permutation of degree n may be defined as a one-one \nfunction from the set {I, 2, . . .  , n} onto itself. Such a function u corre-\nsponds to the n-tuple (uI, u2, . . .  , un) and is thus simply a rule for order-\ning 1, 2, . . .  , n in some well-defined way. \nIf D is an alternating n-linear function and A is an n X n matrix \nover K, we then have \n(5-11) \nD(A )  = .2\": A (l, u1) . . . A (n, (Tn)D (Eal' . . .  , Eun)",
    "sponds to the n-tuple (uI, u2, . . .  , un) and is thus simply a rule for order-\ning 1, 2, . . .  , n in some well-defined way. \nIf D is an alternating n-linear function and A is an n X n matrix \nover K, we then have \n(5-11) \nD(A )  = .2\": A (l, u1) . . . A (n, (Tn)D (Eal' . . .  , Eun) \n0' \nwhere the sum is extended over the distinct permutations u of degree n. \nN ext we shall show that \n(5-12) \nD (Eab . . .  , Ean) = ±D (El, . . .  , En) \nwhere the sign ± depends only on the permutation (T. The reason for this \nis as follows. The sequence (uI, u2, .\n. . , un) can be obtained from the \nsequence (1, 2, . . .  , n) by a finite number of interchanges of pairs of \nelements. For example, if u1  1, we can transpose 1 and (T1, obtaining \n(u1, . . .  , 1, . . . ). Proceeding in this way we shall arrive at the sequence \n(0\"1, . . .  , un) after n or less such interchanges of pairs. Since D is alter-\nnating, the sign of its value changes each time that we interchange two \nof the rows Ei and €j. Thus, if we pass from (1, 2, . .\n. , n) to (u1, u2, . .\n. , ern) \nby means of m interchanges of pairs (i, j), we shall have \nD (Eal, . . .  , Eun) = ( - l)mD(Eb . . .  , En). \nIn particular, if D is a determinant function \n(5-13) \n151 \n152 \nDeterminants \nChap. 5 \nwhere m depends only upon u, not upon D. Thus all determinant func­\ntions assign the same value to the matrix with rows ul, •\n•\n•\n , un, and this \nvalue is either 1 or \n1. \nNow a basic fact about permutations is the following. If u is a per­\nmutation of degree n, one can pass from the sequence (1, 2, . . . , n) to \nthe sequence (ul, u2, . . .  , un) by a succession of interchanges of pairs, \nand this can be done in a variety of ways; however, no matter how it is \ndone, the number of interchanges used is either always even or always \nodd. The permutation is then called even or odd, respectively. One \ndefines the sign of a permutation by \n_ { 1, if u is even \nsgn u -\n1 \n'f \n. \ndd \n-\n,\n 1 \nU 1S 0",
    "done, the number of interchanges used is either always even or always \nodd. The permutation is then called even or odd, respectively. One \ndefines the sign of a permutation by \n_ { 1, if u is even \nsgn u -\n1 \n'f \n. \ndd \n-\n,\n 1 \nU 1S 0 \nthe symbol '1' denoting here the integer 1. \nWe shall show below that this basic property of permutations can be \ndeduced from what we already know about determinant functions. Let \nus assume this for the time being. Then the integer m occurring in (5-13) \nis always even if u is an even permutation, and is always odd if u is an odd \npermutation. For any alternating n-linear function D we then have \nD(Ul' . . .  , un) = (sgn u)D(lI .\n.\n.\n , En) \nand using (5-11) \n(5-14) \nD(A) = [ȉ (sgn u)A (I, ul) . . . A (n, un) ] D(I). \nOf course I denotes the n X n identity matrix. \nFrom (5-14) we see that there is precisely one determinant function \non n X n matrices over K. If we denote this function by det, it is given by \n(5-15) \ndet (A) = 2: (sgn u)A (I, ul) . . . A (n, un) \n\" \nthe sum being extended over the distinct permutations u of degree n. We \ncan formally summarize as follows. \nTheorem 2. Let K be a commutative ring with identity and let n be a \npositive integer. There is precisely one determinant function on the set of \nn X n matrices over K, and it is the function det defined by (5-15). If D is \nany alternating n-linear function on Knxn, then for each n X n matrix A \nD(A) \n(det A)D(I). \nThis is the theorem we have been seeking, but we have left a gap in \nthe proof. That gap is the proof that for a given permutation u, when we \npass from (1, 2, . .\n. , n) to (ul, u2, . . .  , un) by interchanging pairs, the \nnumber of interchanges is always even or always odd. This basic com­\nbinatorial fact can be proved without any reference to determinants; \nSec. 5.3 \nPermutations and the Uniqueness of Determinants \nhowever, we should like to point out how it follows from the existence of \na determinant function on n X n matrices.",
    "binatorial fact can be proved without any reference to determinants; \nSec. 5.3 \nPermutations and the Uniqueness of Determinants \nhowever, we should like to point out how it follows from the existence of \na determinant function on n X n matrices. \nLet us take K to be the ring of integers. Let D be a determinant \nfunction on n X n matrices over K. Let u be a permutation of degree n, . \nand suppose we pass from (1, 2, . . .  , n) to (a-l, u2, . . .  , un) by m inter­\nchanges of pairs (i, j), i \"t j. As we showed in (5-13) \n(- l)m = D(ful, . . .  ,' fun) \nthat is, the number (- l)m must be the value of D on the matrix with \nrows fuI, . . .  , fun. If \nthen m must be even. If \nthen m must be odd. \nSince we have an explicit formula for the determinant of an n X n \nmatrix and this formula involves the permutations of degree n, let us \nconclude this section by making a few more observations about permu­\ntations. First, let us note that there are precisely n! = 1 . 2 . . .  n permu­\ntations of degree n. For, if u is such a permutation, there are n possible \nchoices for u1 ; when this choice has been made, there are (n - 1) choices \nfor u2, then (n - 2) choices for u3, and so on. So there are \nn(n - I)(n - 2) . . .  2 . 1 = n! \npermutations u. The formula (5-15) for det (A) thus gives det (A) as a \nsum of n! terms, one for each permutation of degree n. A given term is a \nproduct \nA (I, (1) . . .  A (n, un) \nof n entries of A, one entry from each row and one from each column, \nand is prefixed by a i +' or i -' sign according as u is an even or odd \npermutation. \nWhen permutations are regarded as one-one functions from the set \n{I, 2, . . .  , n} onto itself, one can define a product of permutations. The \nproduct of u and r will simply be the composed function ur defined by \n(ur) (i) = u(r(i» . \nIf e denotes the identity permutation, f(i) = i, then each u has an inverse \nu-1 such that \nOne can summarize these observations by saying that, under the opera­",
    "product of u and r will simply be the composed function ur defined by \n(ur) (i) = u(r(i» . \nIf e denotes the identity permutation, f(i) = i, then each u has an inverse \nu-1 such that \nOne can summarize these observations by saying that, under the opera­\ntion of composition, the set of permutations of degree n is a group. This \ngroup is usually called the sYIlIletric group of degree n .  \nFrom the point of view of products of permutations, the basic prop­\n, erty of the sign of a permutation is that \n(5-16) \nsgn (ur) = (sgn u)(sgn r). \n153 \n154 \nDeterminants \nChap. 5 \nIn other words, UT is an even permutation if U and T are either both even \nor both odd, while UT is odd if one of the two permutations is odd and the \nother is even. One can see this from the definition of the sign in terms of \nsuccessive interchanges of pairs (i, j). It may also be instructive if we \npoint out how sgn (UT) = (sgn u)(sgn T) follows from a fundamental \nproperty of determinants. \nLet K be the ring of integers and let U and T be permutations of \ndegree n. Let E!, •\n•\n•\n , €n be the rows of the n X n identity matrix over K, \nlet A be the matrix with rows Erll \n•\n•\n•\n , Ern, and let B be the matrix with \nrows Eul, •\n•\n•\n , €\"\" The ith row of A contains exactly one non-zero entry, \nnamely the 1 in column Ti. From this it is easy to see that €ur; is the ith \nrow of the product matrix AB. Now \ndet (A) \nsgn T, \ndet (B) \nsgn u, \nand \ndet (AB) \n= sgn (UT) . \nSo we shall have sgn (UT) \nfollowing. \n(sgn u)(sgn T) as soon as we prove the \nTheorem 3. Let K be a commutative ring with identity, and let A and \nB be n X n matrices over K. Then \ndet (AB) = (det A) (det B). \nProof. Let B be a fixed n X n matrix over K, and for each n X n \nmatrix A define D(A) == detCAB). If we denote the rows of A by al, . . . , \nan) then \nD(al, . . . , an) = det (alB, . . .  , anB). \nHere ajB denotes the 1 X n matrix which is the product of the 1 X n \nmatrix aj and the n X n matrix B. Since \n(ca; + aDB = CaiB + a;B",
    "matrix A define D(A) == detCAB). If we denote the rows of A by al, . . . , \nan) then \nD(al, . . . , an) = det (alB, . . .  , anB). \nHere ajB denotes the 1 X n matrix which is the product of the 1 X n \nmatrix aj and the n X n matrix B. Since \n(ca; + aDB = CaiB + a;B \nand det is n-linear, it is easy to see that D is n-linear. If ai = ah then \naiB = ajB, and since det is alternating, \nD(al' . . . ) an) = O. \nHence, D is alternating. Now D is an alternating n-linear function, and \nby Theorem 2 \nD(A) = (det A)D(l). \nBut D(I) = det (IB) = det B, so \ndet (AB) = D(A) = (det A)(det B). \nI \nThe fact that sgn (UT) = (sgn 0') (sgn T) is only one of many corollarics \nto Theorem 3. We shall consider some of these corollaries in the next \nsection. \nSec. 5.3 \nPermutations and the Uniqueness of Determinants \nExercises \n1. If K is a commutative ring with identity and A is the matrix over K given by \nA [-^  ŔJ \n-b \n-c \n0 \nshow that det A = O. \n2. Prove that the determinant of the Vandermonde matrix \n[1 a a2] \n1 b b2 \n1 \nC c2 \nis (b - a)(c - a)(c - b). \n3. List explicitly the six permutations of degree 3, state which are odd and which \nare even, and use this to give the complete formula (5-15) for the determinant of a \n3 X 3 matrix. \n4. Let rr and 7 be the permutations of degree 4 defined by rrl \n2, rr2 \n3, \nrr3 = 4, rr4 = 1, 71 = 3, 72 = 1, 73 = 2, 74 = 4. \n(a) Is rr odd or even? Is 7 odd or even? \n(b) Find 0\"7 and 70\". \n5. If A is an invertible n X n matrix over a field, show that det A 3 O. \n6. Let A be a 2 X 2 matrix over a field. Prove that det (/ + A) \n1 + det A \nif and only if traee (A) \n= O. \n7. An n X n matrix A is called triangular if Ai; \n0 whenever i > j or if \nAi; \n0 whenever i < j. Prove that the determinant of a triangular matrix is the \nproduct AllA22 . . .  Ann of its diagonal entries. \n8. Let A be a 3 X 3 matrix over the field of complex numbers. We form the \nmatrix xl - A with polynomial entries, the i, j entry of this matrix being the \npolynomial Oi;X - Ai;. If f",
    "product AllA22 . . .  Ann of its diagonal entries. \n8. Let A be a 3 X 3 matrix over the field of complex numbers. We form the \nmatrix xl - A with polynomial entries, the i, j entry of this matrix being the \npolynomial Oi;X - Ai;. If f \ndet (xl - A), show that f is a monic polynomial \nof degree 3. If we write \nf = (x - CI)(X - C2) (X - ca) \nwith complex numbers Cl, C2, and Ca, prove that \nCl + C2 + Ca \ntrace (A) and CIC2Ga \ndet A. \n9. Let n be a positive integer and F a field. If 0\" is a permutation of degree n, \nprove that the function \nT(xl, . . .  , x,,) \n= (X.l, . . . , xon) \nis an invertible linear operator on Fr.. \n10. Let F be a field, n a positive integer, and S the set of n X n matrices over F. \nLet V be the vector space of all functions from S into P. Let TV be the set of alter­\nnating n-linear functions on S. Prove that TV is a subspace of V. What is the dimen­\nsion of TV? \n155 \n156 \nDeterminants \nII. Let T be a linear operator on Fn. Define \nDT(al, .\n.\n•\n , an) = det (Tat, . . .  , Tan). \n(a) Show that DT is an alternating n-linear function. \n(b) If \nc \n= det (TEl, . . .  , TEn) \nshow that for any n vectors aI, . . .  , a\" we have \ndet (Tal, . . . , Tan) = c det (aI, . . .  , an). \nChap. 5 \n(c) If CB is any ordered basis for Fn and A is the matrix of T in the ordered \nbasis CB, show that det A = c. \n(d) What do you think is a reasonable name for the scalar c? \n12. If IJ' is a permutation of degree n and A is an n X n matrix over the field F \nwith row vectors aI, • • •  , an, let u(A) denote the n X n matrix with row vectors \nad, . to . , 0:0'\",\" \n(a) Prove that u(AB) \nIJ'(A)B, and in particular that IJ'(A) \nu(I)A. \n(b) If T is the linear operator of Exercise 9, prove that the matrix of T in \nthe standard ordered basis is IJ'(I). \n(c) Is u-I(I) the inverse matrix of rJ(I)? \n(d) Is it true that u(A) is similar to A? \n13. Prove that the sign function on permutations is unique in the following sense.",
    "u(I)A. \n(b) If T is the linear operator of Exercise 9, prove that the matrix of T in \nthe standard ordered basis is IJ'(I). \n(c) Is u-I(I) the inverse matrix of rJ(I)? \n(d) Is it true that u(A) is similar to A? \n13. Prove that the sign function on permutations is unique in the following sense. \nH f is any function which assigns to each permutation of degree n an integer, and \nif f(rJr) = f(Q)f(r), then f is identically 0, or f is identically 1, or f is the sign \nfunction. \n5.4. Additional Properties of Determinants \nIn this section we shall relate some of the useful properties of the \ndeterminant function on n X n matrices. Perhaps the first thing we should \npoint out is the following. In our discussion of det A, the rows of A have \nplayed a privileged role. Since there is no fundamental difference between \nrows and columns, one might very well expect that det A is an alternating \nn-linear function of the columns of A. This is the case, and to prove it, \nit suffices to show that \n(5-17) \ndet (A t) = det (A) \nwhere A t denotes the transpose of A. \nIf rJ is a permutation of degree n, \nA I(i, IJ'i) \n= A (lJ'i, i). \nFrom the expression (5-15) one then has \ndet (At) =  (sgn rJ)A(u1, 1) ' \"  A (un, n). \n\" \nWhen i = IJ'-Ij, A (lJ'i, i) \n= A (j, IJ'-Ij). Thus \nA (u1, 1) . . .  A (un, n) \nA (l, u-Il) . . , A (n, u-In). \nSec. 5.4 \nAdditional Properties of Determinants \nSince uu-1 is the identity permutation, \n(sgn u) (sgn u-1) \n= 1 \nor sgn (u-1) \n= sgn (u). \nFurthermore, as (1 varies over all permutations of degree n, so does (1-1. \nTherefore \ndet (A t) \n=  (sgn u-1)A (1, u-Il) . . . A (n, (i-in) \n11 \n= det A \nproving (5-17) . \nOn certain occasions one needs to compute specific determinants. \nWhen this is necessary, it is frequently useful to take advantage of the \nfollowing fact. If B is obtained from A by adding a multiple of one row of A \nto another (or a m18ltiple of one column to another), then \n(5-18) \ndet B = det A .",
    "When this is necessary, it is frequently useful to take advantage of the \nfollowing fact. If B is obtained from A by adding a multiple of one row of A \nto another (or a m18ltiple of one column to another), then \n(5-18) \ndet B = det A .  \nWe shall prove the statement about rows. Let B be obtained from A by \nadding caj to ai, where i < j. Since det is linear as a function of the ith row \ndet B = det A + c det (aI, . . . , ail . . .  , aj, . . . , an) \n= det A. \nAnother useful fact is the following. Suppose we have an n X n matrix \nof the block form \nwhere A is an r X r matrix, C is an s X s matrix, B is r X s, and ° denotes \nthe s X r zero matrix. Then \n(5-19) \ndet [g \u0007J = (det A) (det C). \n'1'0 prove this, define \nD(A, B, C) \ndet [g \u0007J \nIf we fix A and B, then D is alternating and s-linear as a function of the \nrows of C. Thus, by Theorem 2 \nD(A, B, C) = (det C)D(A, B, 1) \nwhere I is the s X 8 identity matrix. By subtracting multiples of the rows \nof I from the rows of B and using the statement above (5-18), we obtain \nDCA, B, 1) \n= D(A, 0, 1) . \nNow D(A, 0, I) is clearly alternating and r-linear as a function of the rows \nof A. Thus \nDCA, 0, J) = (det A)D(I, 0, I). \n157 \n158 \nDeterminants \nBut D(I, 0, I) = 1, so \nD(A, B, C) = (det C)D(A, B, I) \n= (det C)D(A, 0, I) \n(det C) (det A). \nBy the same sort of argument, or by taking transposes \n(5-20) \ndet [Ȇ ȇJ = (det A)(det C). \nChap. 5 \nEXAMPLE 6. Suppose K is the field of rational numbers and we wish \nto compute the determinant of the 4 X 4 matrix \nA [! -œ Œ _]. \n1 \n2\n3\n0\n \nBy subtracting suitable multiples of row 1 from rows 2, 3, and 4, we \nobtain the matrix \n[ -! -Ȉ -!] \no \n5 \n-9 \n13 \no \n3 \n1 \n-3 \nwhich we know by (5-18) will have the same determinant as A. If we \nsubtract i of row 2 from row 3 and then subtract t of row 2 from row 4, \nwe obtain \n1 \n2 \n3] \n4 \n-4 -4 \no \n-4 -8 \n0\n4\n0\n \nand again det B = det A. The block form of B tells us that \n\\1 \n- 11\\ -4 -81 \ndvt A \n= det B = 0 \n4 \n4 \n0 \n= 4(32) = 128 .",
    "subtract i of row 2 from row 3 and then subtract t of row 2 from row 4, \nwe obtain \n1 \n2 \n3] \n4 \n-4 -4 \no \n-4 -8 \n0\n4\n0\n \nand again det B = det A. The block form of B tells us that \n\\1 \n- 11\\ -4 -81 \ndvt A \n= det B = 0 \n4 \n4 \n0 \n= 4(32) = 128 . \nNow let n > 1 and let A be an n X n matrix over K. In Theorem 1, \nwe showed how to construct a determinant function on n X n matrices, \ngiven one on (n - 1) X en - 1) matrices. Now that we have proved the \nuniqueness of the determinant function, the formula (5-4) tells us the \nfollowing. If we fix any column index j, \nn \ndet A \n=  ( -I)i+iAij det A (ilj). \ni = l  \nThe scalar ( - 1) i+i det A (ilj) is usually called the i, j cofactor of A or \nthe cofactor of the i, j entry of A. The above formula for det A is then \nSec. 5.4 \nAdditional Properties of Determinants \ncalled the expansion of det A by cofactors of the jth column (or sometimes \nthe expansion by minors of the jth column). If we set \nGij = ( - I)i+i det A (ilj) \nthen the above formula says that for each j \nn \ndet A \n= { AiiGij \ni = l  \nwhere the cofactor Gii is ( 1)i+i times the determinant of the (n - 1) X \n(n - 1) matrix obtained by deleting the ith row and jth column of A .  \nIf j x k, then \nn \n{ AikGii = O. \n; = 1  \nFor, replace the jth column of A by its kth column, and call the resulting \nmatrix B. Then B has two equal columns and so det B = O. Since BCilj) = \nA (ilj), we have \no = det B \nn \n= { \n1)i+iBii det BCilj) \n; = 1  \nn \n= { ( - I)i+iAik det A (ilj) \n; = 1  \nn \n= { AikG;j. \ni ܭ1  \nThese properties of the cofactors can be summarized by \n(5-21) \nn \n{ AikGlj = 5jk det A. \n; = 1  \nThe n X n matrix adj A, which is the transpose of the matrix of co­\nfactors of A, is called the classi<)al adjoint of A .  Thus \n(5-22) \n(adj A)ij = Gii = ( - I)i+i det A (jli). \nThe formulas (5-21) can be summarized in the matrix equation \n(5-23) \n(adj A )A = (det A)I. \nWe wish to see that A (adj A) = (det A)I also. Since A t(iJj) = A (jli)t, \nwe have \n1)i+i det A t(ilj)",
    "(5-22) \n(adj A)ij = Gii = ( - I)i+i det A (jli). \nThe formulas (5-21) can be summarized in the matrix equation \n(5-23) \n(adj A )A = (det A)I. \nWe wish to see that A (adj A) = (det A)I also. Since A t(iJj) = A (jli)t, \nwe have \n1)i+i det A t(ilj) \n(- l)i+i det A (jli) \nwhich simply says that the i, j cofactor of A t is the j, i cofactor of A. Thus \n(5-24) \nadj (AI) \n(adj A)t \nBy applying (5-23) to A I, we obtain \nand transposing \n(adj A t)A I = (det A 1)1 = (det A)l \nA (adj A t) t \n(det A)I. \n159 \n160 \nDeterminants \nUsing (5-24), we have what we want: \n(5-25) \nA (adj A) = (det A)I. \nChap. 5 \nAs for matrices over a field, an n X n matrix A over K is called \ninvertible over K if there is an n X n matrix A-I with entries in K \nsuch that AA-l \nA-IA \nI. If such an inverse matrix exists it is unique; \nfor the same argument used in Chapter 1 shows that when BA \nAC \nI \nwe have B = C. The formulas (5-23) and (5-25) tell us the following about \ninvertibility of matrices over K. If the element det A has a multiplicative \ninverse in K, then A is invertible and A-I = (det A)-l adj A is the unique \ninverse of A .  Conversely, it is easy to see that if A is invertible over K, \nthe element det A is invertible in K. For, if BA = I we have \n1 \n= det I = det (AB) = (det A) (det B). \nWhat we have proved is the following. \nTheorem 4. Let A be an n X n matrix over K. Then A is invertible \nover K if and only if det A is invertible in K. When A is invertible, the unique \ninverse for A is \nA-I \n= (det A)-l adj A. \nIn particular, an n X n matrix over a field is invertible if and only if its \ndeterminant is different from zero. \nWe should point out that this determinant criterion for invertibility \nproves that an n X n matrix with either a left or right inverse is invertible. \nThis proof is completely independent of the proof which we gave in Chap­\nter 1 for matrices over a field. We should also like to point out what in­",
    "We should point out that this determinant criterion for invertibility \nproves that an n X n matrix with either a left or right inverse is invertible. \nThis proof is completely independent of the proof which we gave in Chap­\nter 1 for matrices over a field. We should also like to point out what in­\nvertibility means for matrices with polynomial entries. If K is the poly­\nnomial ring F[x], the only elements of K which are invertible are the \nnon-zero scalar polynomials. For if f and g are polynomials and fg = 1, \nwe have deg f + deg g \n° so that deg f = deg g = 0, Le., f and g are \nscalar polynomials. So an n X n matrix over the polynomial ring F[x] is \ninvertible 0\\ er F[x] if and only if its determinant is a non-zero scalar \npolynomial. \nEXAMPLE 7. Let K = R [x], the ring of polynomials over the field of \nreal numbers. Let \nB - [ \nX2 \n1 \nx + 2J\n. \n-\nx2 \n- 2x + 3 \nx \nThen, by a short computation, det A \nx + 1 and det B \n-6. Thus A \nis not invertible over K, whereas B is invertible over K. Note that \n-x IJ \nx2 + X \n' \n-x \n2J \nX2 \n- 1 \nSec. 5.4 \nAdditional Properties of Determinants \nand (adj A)A = (x + 1)1, (adj B)B = \n-61 . Of course, \nB-1 _\n_\n ! [ \nx \n-x - 2J\n. \n-\n6 -x2 + 2x - 3 \n1 - x2 \nEXAMPLE 8. Let K be the ring of integers and \nA = G !l \nThen det A \n-2 and \n[ 4 -21J. \nadj A = \n-3 \nThus A is not invertible as a matrix over the ring of integers; however, \nwe can also regard A as a matrix over the field of rational numbers. If we \ndo, then A is invertible and \nIn connection with invertible matrices, we should like to mention one \nfurther elementary fact. Similar matrices have the same determinant, \nthat is, if P is invertible over K and B = P-1AP, then det B = det A. \nThis is clear since \ndet (P-1AP) = (det P-1) (det A )(det P) = det A .  \nThis simple observation makes it possible to define the determinant of \na linear operator on a finite dimensional vector space. If T is a linear \noperator on V, we define the determinant of T to be the determinant of",
    "This is clear since \ndet (P-1AP) = (det P-1) (det A )(det P) = det A .  \nThis simple observation makes it possible to define the determinant of \na linear operator on a finite dimensional vector space. If T is a linear \noperator on V, we define the determinant of T to be the determinant of \nany n X n matrix which represents T in an ordered basis for V. Since all \nsuch matrices are similar, they have the same determinant and our defini­\ntion makes sense. In this connection, see Exercise 1 1  of section 5.3. \nWe should like now to discuss CranlCr's rule for solving systems of \nlinear equations. Suppose A is an n X n matrix over the field F and we \nwish to solve the system of linear equations AX = Y for some given \nn-tuple (Y1, . . .  , Yn). If AX = Y, then \n(adj A)AX = (adj A) Y \nand so \n(det A)X = (adj A ) Y. \nThus \nn \n(det A)xj = { (adj A)jiYi \n. = 1  \nn \n= { ( _ l)HiYi det A (ilj). \n; = 1  \nThis last expression is the determinant of the n X n matrix obtained by \nreplacing the jth column of A by Y. If det A = 0, all this tells us nothing; \nhowever, if det A ,e 0, we have what is known as Cramer's rule. Let A \n161 \n162 \nDeterminants \nChap. 5 \nbe an n X n matrix over the field F such that det A Y O. If Y1, . . .  , Yn \nare any scalars in F, the unique solution X \nA -1 Y of the system of \nequations AX = Y is given by \ndet Bj \nXj \ndet A ' \nj \n1, . . .  , n \nwhere B j is the n X n matrix obtained from A by replacing the jth column \nof A by Y. \nIn concluding this chapter, we should like to make some comments \nwhich serve to place determinants in what we believe to be the proper \nperspective. From time to time it is necessary to compute specific deter­\nminants, and this section has been partially devoted to techniques which \nwill facilitate such work. However, the principal role of determinants in \nthis book is theoretical. There is no disputing the beauty of facts such as \nCramer's rule. But Cramer's rule is an inefficient tool for solving systems",
    "will facilitate such work. However, the principal role of determinants in \nthis book is theoretical. There is no disputing the beauty of facts such as \nCramer's rule. But Cramer's rule is an inefficient tool for solving systems \nof linear equations, chiefly because it involves too many computations. \nSo one should concentrate on what Cramer's rule says, rather than on \nhow to compute with it. Indeed, while reflecting on this entire chapter, \nwe hope that the reader will place more emphasis on understanding what \nthe determinant function is and how it behaves than on how to compute \ndeterminants of specific matrices. \nExercises \n1. Use the classical adjoint formula to compute the inverses of each of the fol-\nlowing 3 X 3 real matrices. \n• \n[-2 3 \n2] \n6 0 \n3 , \n4 1 \n- 1  [eos 0 0 \n-sin 0] \no \n1 \n0 \nsin 0 0 \ncos 0 \n2. Use Cramer's rule to solve each of the following systems of linear equations \nover the field of rational numbers. \n(a) x + y + \nz \n1 1  \n2x \n6y \nz = 0 \n3x + 4y + 2z = \nO. \n(b) 3x - 2y = \n7 \n3y - 2z = \n6 \n3z - 2x = -1. \n3. An n X n matrix A over a field F is skew-sYIlInetric if At = -A. If A is a \nskew-symmetric n X n matrix with complex entries and n is odd, prove that \ndet A = O. \n4. An n X n matrix A over a field F is called orthogonal if AA t = I. If A is \northogonal, show that det A = ± 1 .  Give an example of an orthogonal matrix \nfor which det A \n-1. \nSec. 5.4 \nAdditional Properties of Determinants \n5. An n X n matrix A over the field of complex numbers is said to be unitary \nif AA * \nI (A * denotes the conjugate transpose of A). If A is unitary, show \nthat [det A [  = 1. \n6. Let T and U be linear operators on the finite dimensional vector space V. Prove \n(a) det (TU) \n(det T)(det U) ; \n(b) T is invertible if and only if det T 7'\" O. \n7. Let A be an n X n matrix over K, a commutative ring with identity. Suppose \nA has the block form \nwhere Ai is an rj X rj matrix. Prove \ndet A \n(det AJ)(det A2) . . .  (det Ak).",
    "(a) det (TU) \n(det T)(det U) ; \n(b) T is invertible if and only if det T 7'\" O. \n7. Let A be an n X n matrix over K, a commutative ring with identity. Suppose \nA has the block form \nwhere Ai is an rj X rj matrix. Prove \ndet A \n(det AJ)(det A2) . . .  (det Ak). \n8. Let V be the vector space of n X n matrices over the field F. Let B be a fixed \nelement of V and let TB be the linear operator on V defined by TB(A) = AB - BA. \nShow that det TB = O. \n9. Let A be an n X n matrix over a field, A 7'\" O. If r is any positive integer \nbetween 1 and n, an r X r sub matrix of A is any r X r matrix obtained by deleting \n(n - r) rows and (n - r) columns of A. The determinant rank of A is the \nlargest positive integer r such that some r X r sub matrix of A has a non-zero \ndeterminant. Prove that the determinant rank of A is equal to the row rank of \nA ( = column rank A). \n10. Let A be an n X n matrix over the field F. Prove that there are at most n \ndistinct scalars c in F such that det (el - A) \nO. \nn. Let A and B be n X n matrices over the field F. Show that if A is invertible \nthere are at most n scalars c in F for which the matrix cA + B is not invertible. \n12. If V is the vector space of n X n matrices over F and B is a fixed n X n matrix \nover F, let LB and RB be the linear operators on V defined by LB(A) = BA and \nRB(A) = AB. Show that \n(a) det LB = (det B)\"; \n(b) det RB = (det B)n. \n13. Let V be the vector space of all n X n matrices over the field of complex \nnumbers, and let B be a fixed n X n matrix over C. Define a linear operator MB \non V by MB(A) \nBAB*, where B* \nBt. Show that \ndet M B = [det B[2n. \nNow let H be the set of all Hermitian matrices in V, A being Hermitian if \nA = A *. Then H is a vector space over the field of real numbers. Show that the \nfunction TB defined by TB(A) \nBAB* is a linear operator on the real vector \nspace H, and then show that det TB \n[det B[2n. (Hint: In computing det TB,",
    "A = A *. Then H is a vector space over the field of real numbers. Show that the \nfunction TB defined by TB(A) \nBAB* is a linear operator on the real vector \nspace H, and then show that det TB \n[det B[2n. (Hint: In computing det TB, \nshow that V has a basis consisting of Hermitian matrices and then show that \ndet TB = det MB.) \n163 \n164 \nDeterminants \nOhap. 5 \n14. Let A ,  B, C, D be commuting n X n matrices over the field F. Show that the \ndeterminant of the 2n X 2n matrix \nis det (AD - BC). \n5.5. Modules \nIf K is a commutative ring \\vith identity, a module over K is an alge­\nbraic system which behaves like a vector space, with K playing the role \nof the scalar field. To be precise, we say that V is a nlOdule over K (or a \nK-module) if \n1. there is an addition (a, (3) -t a + (3 on V, under which V is a \ncommutative group; \n2. there is a muhiplication (c, a) -t ca of elements a in V and c in K \nsuch that \n(Cl + cz)a = CIa + cza \nc(al + (2) = Cal + caz \n(clcZ)a = cI(cza) \nla = a. \nFor us, the most important K-modules will be the n-tuple modules Kn. \nThe matrix modules KmXn will also be important. If V is any module, we \nspeak of linear combinations, linear dependence and linear independence, \njust as we do in a vector space. We must be careful not to apply to V any \nvector space results which depend upon division by non-zero scalars, the \none field operation which may be lacking in the ring K. For example, if \naI, . . .  , ak are linearly dependent, we cannot conclude that some ai is a \nlinear combination of the others. This makes it more difficult to find bases \nin modules. \nA basis for the module V is a linearly independent subset which \nspans (or generates) the module. This is the same definition which we gave \nfor vector spaces; and, the important property of a basis ffi is that each \nelement of V can be expressed uniquely as a linear combination of (some \nfinite number of) elements of ffi. If one admits into mathematics the Axiom",
    "for vector spaces; and, the important property of a basis ffi is that each \nelement of V can be expressed uniquely as a linear combination of (some \nfinite number of) elements of ffi. If one admits into mathematics the Axiom \nof Choice (see Appendix), it can be shown that every vector space has a \nbasis. The reader is well aware that a basis exists in any vector space \nwhich is spanned by a finite number of vectors. But this is not the case \nfor modules. Therefore we need special names for modules which have \nbases and for modules which are spanned by finite numbers of elements. \nDefinition. The K-module V is called a free module if it has a basis. \n[fV has afinite basis containing n elements, then V is called a free K-module \nwith n generators. \nSec. 5.5 \nModules \nDefinition. The module V is finitely generated ŧf it contains a finite \nsubset which spans V. The rank of a finitely generated module is the smallest \ninteger k such that some k elements span V. \nWe repeat that a module may be finitely generated without having \na finite basis. If V is a free K-module with n generators, then V is isomor­\nphic to the module Kn. If {i3), . . .  , i3n} is a basis for V, there is an iso­\nmorphism which sends the vector C)i3l + \n'\n\"\n + cnf3n onto the n-tuple \n(c), . . .  , cn) in Kn. It is not immediately apparent that the same module V \ncould not also be a free module on k generators, with k r!' n. In other \nwords, it is not obvious that any two bases for V must contain the same \nnumber of elements. The proof of that fact is an interesting application \nof determinants. \nTheorem 5. Let K be a commutative ring with identity. If V is a free \nK-module with n generators, then the rank of V is n. \nProof. We are to prove that V cannot be spanned by less than \nn of its elements. Since V is isomorphic to Kn, we must show that, if \nm < n, the module Kn is not spanned by n-tuples aI, . . .  , am. Let A be \nthe matrix with rows aI, . . .  , am. Suppose that each of the standard basis",
    "Proof. We are to prove that V cannot be spanned by less than \nn of its elements. Since V is isomorphic to Kn, we must show that, if \nm < n, the module Kn is not spanned by n-tuples aI, . . .  , am. Let A be \nthe matrix with rows aI, . . .  , am. Suppose that each of the standard basis \nvectors EI, •\n•\n•\n , En is a linear combination of aI, . . .  , am. Then there exists \na matrix P in KnXm such that \nPA = I \nwhere I is the n X n identity matrix. Let A be the n X n matrix obtained \nby adjoining n \nm rows of O's to the bottom of A, and let F be any n X n \nmatrix which has the columns of P as its first n columns. Then \nFA = I. \nTherefore det A r!' O. But, since m < n, at least one row of A has all 0 \nentries. This contradiction shows that aI, . . .  , am do not span Kn. \nI \nIt is interesting to note that Theorem 5 establishes the uniqueness \nof the dimension of a (finite-dimensional) vector space. The proof, based \nupon the existence of the determinant function, is quite different from the \nproof we gave in Chapter 2. From Theorem 5 we know that Ifree module \nof rank n' is the same as 'free module with n generators.' \nIf V is a module over K, the dual module V* consists of all linear \nfunctions f from V into K. If V is a free module of rank n, then V* is also \na free module of rank n. The proof is just the same as for vector spaces. \nIf {i3l, . . .  , f3n} is an ordered basis for V, there is an associated dual basis \n{iI, . . .  , fn} for the module V*. The function fi assigns to each a in V its \nith coordinate relative to {i3l, . . .  , f3n} : \na = fl(a)f3l + \n. . . + fn(a)f3n. \nIf f is a linear function on V, then \n! \nfCi3l)h + . . . + !Ci3n)fn. \n165 \n166 \nDeterminants \nChap. 5 \n5.6. Multilinear Functions \nThe purpose of this section is to place our discussion of determinants \nin what we believe to be the proper perspective. We shall treat alternating \nmultilinear forms on modules. These forms are the natural generalization",
    "165 \n166 \nDeterminants \nChap. 5 \n5.6. Multilinear Functions \nThe purpose of this section is to place our discussion of determinants \nin what we believe to be the proper perspective. We shall treat alternating \nmultilinear forms on modules. These forms are the natural generalization \nof determinants as we presented them. The reader who has not read (or \ndoes not wish to read) the brief account of modules in Section 5.5 can still \nstudy this section profitably by consistently reading 'vector space over F \nof dimension n' for 'free module over K of rank n.' \nLet K be a commutative ring with identity and let V be a module \nover K. If r is a positive integer, a function L from yr = V X V X . . , X V \ninto K is called multilinear if L(al' . . . , aT) is linear as a function of \neach ai when the other a/s are held fixed, that is, if for each i \nL(al' . . . , Cai + i3i, . . .  , a,) \ncL(al, . . . , ai, . . .  , ar + \nL(al, . . . , i3i, . . . , ar). \nA multilinear function on Vr will also be called an r-linear form on V \nor a multilinear form of degree r on V. Such functions are sometimes \ncalled r-tensors on V. The collection of all multilinear functions on \nyr will be denoted by MT(V). If L and M are in MT(V), then the sum \nL + M: \n(L + M) (al' . . .  , aT) = L(al' . . .  , aT) + M(al' . . . , ar) \nis also multilinear; and, if c is an element of K, the product cL : \n(cL) (al' . . .  , ar) \n= cL(al, . . .  , ar) \nis multilinear. Therefore Mr(V) is a K-module-a submodule of the \nmodule of all functions from yr into K. \nIf r = 1 we have 1I,[I(V) \n= V*, the dual module of linear functions \non V. Linear functions can also be used to construct examples of multi­\nlinear forms of higher order. If iI, . . .  , ir are linear functions on V, define \nL(al, . . .  , aT) \n= h(al)Ma2) . . .  ir(aT). \nClearly L is an r-Iinear form on V. \nEXAMPLE 9. If V is a module, a 2-linear form on V is usually called a \nbilinear form on V. Let A be an n X n matrix with entries in K. Then \nL(X, Y)",
    "L(al, . . .  , aT) \n= h(al)Ma2) . . .  ir(aT). \nClearly L is an r-Iinear form on V. \nEXAMPLE 9. If V is a module, a 2-linear form on V is usually called a \nbilinear form on V. Let A be an n X n matrix with entries in K. Then \nL(X, Y) \ndefines a bilinear form L on the module KnXl. Similarly, \nM(a, (3) \naA{3t \ndefines a bilinear form M on Kn. \nEXAMPLE 10. The determinant function associates with each n X n \nSec. 5.6 \nMultilinear Functions \nmatrix A an element det A in J(. If det A is considered as a function of \nthe rows of A : \ndet A = D(al, . . .  , an) \nthen D is an n-linear form on J(n. \nEXAMPLE 11. It is easy to obtain an algebraic expression for the \ngeneral r-linear form on the module J(n. If aI, . . . , aT are vectors in V \nand A is the r X n matrix with rows aI, . . . , aT, then for any function L \nin Mr(J(n), \nn \n:z; AlJL(€j, a2, .\n•\n•\n , aT) \n}=1 \n= , AliL(€j, , A2k€k, . . .  , aT) \nj=1 \nj= l \nn \nn \n= :z; \n:z; AljA2kL(Ej, Ek, a3, . . .  , aT) \nj=1 k=1 \nn \n:z; A1JA2kL(€j, €k, as, . . .  , aT) ' \nj.k=l \nIf we replace aa, . . .  , a, in turn by their expressions as linear combinations \nof the standard basis vectors, and if we write A (i, j) for A ij, we obtain the \nfollowing: \n(5-26) \nn \nL(al, . . . , aT) \n:z; \nA (l, jl) . . , A (1', jT)L(€jll ' . . fj,). \niI\" . .  , ir= l  \nI n  (5-26), there is one term for each r-tuple J = (j1, .\n. . , jT) of positive \nintegers between 1 and n. There are nr such 1'-tuples. Thus L is completely \ndetermined by (5-26) and the particular values: \nCJ = L(Ej!) . . .  , fj,) \nassigned to the n' elements (Ejl) •\n•\n•\n , Ej,). It is also easy to see that if for \neach 1'-tuple J we choose an element CJ of J( then \n(5-27) \nL(al, . . .  , a,) = :z; A (1, jl) . . .  A (1', j,)cJ \nJ \ndefines an 1'-linear form on Kn. \nSuppose that L is a multilinear function on V, and M is a multilinear \nfunction on V,. We define a function L ® M on Vr+8 by \n(5-28) \nIf we think of V* as V' X P, then for a in Vr and {3 in V· \n(L ® M) (a, (3) = L(a)M({3).",
    "J \ndefines an 1'-linear form on Kn. \nSuppose that L is a multilinear function on V, and M is a multilinear \nfunction on V,. We define a function L ® M on Vr+8 by \n(5-28) \nIf we think of V* as V' X P, then for a in Vr and {3 in V· \n(L ® M) (a, (3) = L(a)M({3). \n167 \n168 \nDeterminants \nChap. 5 \nIt is clear that L OO M  is multilinear on Vr+8. The function L OO M is \ncalled the tensor product of L and M. The tensor product is not com­\nmutative. In fact, M 00 L ,r= L OO M unless L = 0 or M = 0; however, \nthe tensor product does relate nicely to the module operations in Mr \nand Ma. \nLemma. Let L, Ll be r-linear forms on V, let M, Ml be s-linear forms \non V and let c be an element of K. \n(a) (cL + Ll) 00 M \nc(L 00 M) + Ll 00 M; \n(b) L 00 (cM + Ml) = c(L 00 M) + L 00 Ml. \nProof. Exercise. \nTensoring is associative, i.e., if L, M and N are (respectively) r-, s­\nand t-linear forms on V, then \n(L OO M) OO N  = L OO (M OO N). \nThis is immediate from the fact that the multiplication in K is associative. \nTherefore, if Ll, L2, •\n•\n•\n , Lk are multilinear functions on Vn, . . .  , VTl, \nthen the tensor product \nL = Ll OO · · · OO Lk \nis unambiguously defined as a multilinear function on P, where r \nrl + . . .  + rk. We mentioned a particular case of this earlier. If h, . . .  , fT \nare linear functions on V, then the tensor product \nL \nh 00 . . · OO i, \nis given by \nTheQrem 6. Let K be a commutative ring with identity. If V is a free \nK-module of rank n then Mr(V) is a free K-module of rank nr; in fact, if \n{fl' . . . , fn} is a basis for the dtwl module V*, the nr tensor products \nfll 00 . . .  00 fJ\" \n1 :S h :S n, . . .  , 1 :S jr :S n \nform a basis for Mr(V). \nProof. Let {h, . . .  , in} be an ordered basis for V* which is dual \nto the basis {1' . . . , In} for V. For each vector a in V we have \na = h(a)l + . . . + in(a)n. \nWe now make the calculation carried out in Example 11. If L is an r-linear \nform on V and al, . . .  , a, are elements of V, then by (5-26)",
    "to the basis {1' . . . , In} for V. For each vector a in V we have \na = h(a)l + . . . + in(a)n. \nWe now make the calculation carried out in Example 11. If L is an r-linear \nform on V and al, . . .  , a, are elements of V, then by (5-26) \nL(al, . . .  , aT) \n \n!i.(al) · . .  fi,(ar)L(j\" . . .  , j,). \nIn other words, \n(5-29) \niI, . . .  , it \nL = \n{ \nL(j\" . . .  , j,)!i. 00 . . .  00 fi,. \njl, . . .  , jr \nSec. 5.6 \nThis shows that the nr tensor products \n(5-30) \nEJ = !it ® . . . ® k \nMultilinear Functions \ngiven by the r-tuples J = UI, ' . .  , jr) span the module Mr(V). We see \nthat the various r-forms EJ are independent, as follows. Suppose that for \neach J we have an element CJ in K and we form the multilinear function \n(5-31) \nL = { cJEJ. \nJ \nNotice that if I \n(iI, . . . , ir), then \nEJ«(Ji\" . . .  , (Ji.) = {6; 7 :: 8. \nTherefore we see from (5-31) that \n(5-32) \nCr = L(i3iu . . .  , i3iJ. \nIn particular, if L = 0 then Cr = 0 for each r-tuple I. I \nDefinition. Let L be an r-linear form on a K-module V. We say that L \nis alternating if L(al' . . . , ar) = 0 whenever ai = aj with i r! j. \nIf L is an alternating multilinear function on Vr, then \nIn other words, if we transpose two of the vectors (with different indices) \nin the r-tuple (aI, . . .  , ar) the associated value of L changes sign. Since \nevery permutation u is a product of transpositions, we see that L(auI, . . .  , \nau,) \n(sgn a) L(al, . . . , ar). \nWe denote by N(V) the collection of all alternating r-linear forms \non V. It should be clear that Ar(V) is a submodule of Mr(V). \nEXAMPLE 12. Earlier in this chapter, we showed that on the module \nKn there is precisely one alternating n-linear form D with the property \nthat D(EI' . . .  , En) \n1. We also showed in Theorem 2 that if L is any \nform in An(Kn) then \nL = L(E!, . . .  , €n)D. \nIn other words, An(Kn) is a free K-module of rank 1. We also developed \nan explicit formula (5-15) for D. In terms of the notation we are now",
    "that D(EI' . . .  , En) \n1. We also showed in Theorem 2 that if L is any \nform in An(Kn) then \nL = L(E!, . . .  , €n)D. \nIn other words, An(Kn) is a free K-module of rank 1. We also developed \nan explicit formula (5-15) for D. In terms of the notation we are now \nusing, that formula may be written \n(5-33) \nD = E (sgn u) ful ® . . . ® fun. \nO' \nwhere fl' . . .  , in are the standard coordinate functions on Kn and the sum \nis extended over the n! different permutations u of the set {I, . . .  , n} .  \nIf we write the determinant of a matrix A as \ndet A = 7 (sgn u) A (u1, 1) ' \" A (un, n) \nO' \n169 \n170 \nDeterminants \nChap. 5 \nthen we obtain a different expression for D :  \n(5-34) \nD(al, . . .  , an) \n=  (sgn u) fl(a\"l) ' \"  fn(a\"n) \n\" \n= { (sgn u) L(a\"l' . . . , a\"n) \n\" \nwhere L = f1 ® . . . ® fn. \nThere is a general method for associating an alternating form with \na multilinear form. If L is an r-linear form on a module V and if u is a \npermutation of {I, . . .  , r}, we obtain another r-linear function L\" by \ndefining \nLiat, . . .  , ar) \n= L(aal' . . . , aaT)' \nIf L happens to be alternating, then La = (sgn u)L. Now, for each L in \nMr(V) we define a function 7rrL in MT(V) by \n(5-35) \n7rrL =  (sgn u)L\" \n\" \nthat is, \n(5-36) \n(7rTL)(al, . . .  , ar) \n (sgn u) L(a\"l, . . .  , a\"T)' \n\" \nLemma. 7rr is a linear transformation from Mr(V) into Ar(V). If L \nis in Ar(V) then 7rrL = r!L. \nProof. Let T be any permutation of {I, . . . , r} .  Then \n(7rTL) (aTl' . . . , arr) = { (sgn u) L(ar\"l' . . . , aTa,) \n\" \n= (sgn T) { (sgn TO') L(aral, . . .  , aTar) . \n\" \nAs 0' runs (once) over all permutations of {I, . .\n. , r} , so does TO'. Therefore, \n(7rrL)(aTb . . . , aTT) \n= (sgn T) (7rrL) (aI, . .\n. , aT)' \nThus 7rrL is an alternating form. \nIf L is in Ar(V), then L(a,,!, . . .  , aUT) \neach 0'; hence 7r rL = r !L. \nI \n(sgn u) L(a!, . . .  , aT) for \nIn (5-33) we showed that the determinant function D in An(Kn) is \nD \n7rn(ft ® . . .  ® fn) \nwhere fl' .\n.\n.",
    "= (sgn T) (7rrL) (aI, . .\n. , aT)' \nThus 7rrL is an alternating form. \nIf L is in Ar(V), then L(a,,!, . . .  , aUT) \neach 0'; hence 7r rL = r !L. \nI \n(sgn u) L(a!, . . .  , aT) for \nIn (5-33) we showed that the determinant function D in An(Kn) is \nD \n7rn(ft ® . . .  ® fn) \nwhere fl' .\n.\n.\n , fn are the standard coordinate functions on K\". There is \nan important remark we should make in connection with the last lemma. \nIf K is a field of characteristic zero, such that r! is invertible in K, then \n7r maps Mr(V) onto A'(V). In fact, in that case it is more natural from one \npoint of view to use the map 7rl = (l/r!)7r rather than 7r, because 71'1 is a \nprojection of M-r(V) onto Ar(V), i.e., a linear map of Mr(V) onto N(V) \nsuch that 71'1(L) = L if and only if L is in Ar(V). \nSec. 5.6 \nMultilinear Functions \nTheorem 7. Let K be a commutative ring with identity and let V be \na free K-module of rank n. If r > n, then Ar(V) \n{O} .  If 1 ::; r ::; n, then \nAr(V) is a free K-module of rank (ȅ)-\nProof. Let {i31, . . .  , i3n} be an ordered basis for V with dual \nbasis {fl, . . .  , fn} .  If L is in Mr(V), we have \n(5-37) \nL \n, L({3ju •\n.\n.\n , {3j,) fit ® . . . ® h, \nJ \nwhere the sum extends over all r-tuples J = UI' . . .  , jT) of integers be­\ntween 1 and n. If L is alternating, then \nL(i3iu . . .  , i3i,) = 0 \nwhenever two of the subscripts j, are the same. If r > n, then in each \nr-tuple J some integer must be repeated. Thus Ar(V) = {O} if r > n. \nNow suppose 1 ::; r ::; n. If L is in AT(V), the Sum in (5-37) need be \nextended only over the r-tuples J for which ii, . . .  , jr are distinct, because \nall other terms are O. Each r-tuple of distinct integers between 1 and n is \na permutation of an r-tuple J = Ul, . . . , jT) such that jl < ' \" \n< jT' \nThis special type of r-tuple is called an r-shuffie of {1, . . . , n} . There are \n(n) \nn! \nr \n= r!(n - r) ! \nsuch shufHes. \nSuppose we fix an r-shufHe J. Let LJ be the sum of all the terms in",
    "a permutation of an r-tuple J = Ul, . . . , jT) such that jl < ' \" \n< jT' \nThis special type of r-tuple is called an r-shuffie of {1, . . . , n} . There are \n(n) \nn! \nr \n= r!(n - r) ! \nsuch shufHes. \nSuppose we fix an r-shufHe J. Let LJ be the sum of all the terms in \n(5-37) corresponding to permutations of the shuffle J. If u is a permutation \nof {I, .\n. . , r}, then \nThus \n(5-38) \nwhere \n(5-39) \nL(i3j,1) .\n.\n.\n , (3)',,) = (sgn u) L(i3j1) . . .  , (3j,). \nDJ \n, (sgn u) k, ® . . . ® k, \n= 7rT(h, ® . . .  ® k)· \nWe see from (5-39) that each DJ is alternating and that \n(5-40) \nL = \n+ \nL({3j1) \"\n\" (3i,)DJ \nehufles J \nfor every L in Ar(V). The assertion is that the (Ȅ) forms DJ constitute a \nbasis for A'(V). We have seen that they span Ar(V). It is easy to see that \nthey are independent, as follows. If I = (ii, . . .  , i,) and J = (jl, . . .  , ir) \nare shufHes, then \n{1 \n1 =  J \n(5-41) \nDJ(i3i\" .\n. . , (3;,) = 0: I ¢ i \n171 \n172 \nDeterminants \nSuppose we have a scalar CJ for each shuffle and we define \nL = , cJDJ. \nJ \nFrom (5-40) and (5-41) we obtain \nCr = L({3ill .\n.\n.\n , (3i,)' \nIn particular, if L = 0 then Cr \n= 0 for each shuffle I. \nI \nChap. 5 \nCorollary. If V is a free K-module of rank n, then An(V) is a free \nK-module of rank 1. If T is a linear operator on V, there is a unique element \nc in K such that \nL(Tal, .\n.\n. , Tan) \ncL(al, . . .  , an) \nfor every alternating n-linear form L on V. \nProof. If L is in An(V), then clearly \nLT(al, . . .  , an) = L(Tal, . . . , Ta,,) \ndefines an alternating n-linear form LT. Let M be a generator for the rank \n1 module An(V). Each L in A\" (V) is uniquely expressible as L = aM for \nsome a in K. In particular, M T = cM for a certain c. For L = aM we have \nLT = (aM)T \n= aMT \n= a(cM) \n= c(aM) \ncL. I \nOf course, the element c in the last corollary is called the determinant \nof T. From (5-39) for the case r = n (when there is only one shuffle \nJ = (1, .\n. . , n» we see that the determinant of T is the determinant of",
    "LT = (aM)T \n= aMT \n= a(cM) \n= c(aM) \ncL. I \nOf course, the element c in the last corollary is called the determinant \nof T. From (5-39) for the case r = n (when there is only one shuffle \nJ = (1, .\n. . , n» we see that the determinant of T is the determinant of \nthe matrix which represents T in any ordered basis {(3l, •\n.\n.\n , {3n} . Let us \nsee why. The representing matrix has i, j entry \nAij = !;(T{3i) \nso that \nDJ(T{31, . . .  , T{3,,) = , (sgn u) A (I, ul) . . .  A (n, un) \n\" \n= det A .  \nO n  the other hand, \nDJ(T{3l, . . .  , T{3n) \n= (det T) DJ({3l, . . .  , (3,,) \ndet T. \nThe point of these remarks is that via Theorem 7 and its corollary we \nobtain a definition of the determinant of a linear operator which does not \npresume knowledge of determinants of matrices. Determinants of matrices \ncan be defined in terms of determinants of operators instead of the other \nway around. \nSec. 5.7 \nThe Grassman Ring \nWe want to say a bit more about the special altcrnating r-linear \nforms DJ, which we associated with a basis {h . . .  , fn} for V* in (5-39). \nIt is important to understand that DJ(a1' . . .  , aT) is the determinant of \na certain r X r matrix. If \n1 \u001a i \u001a r, 1 \u001a j \u001a n, \nthat is, if \nai = A \n;1(31 + ' \"  + A \nin(3n, \nand J is the Nhuffie (jJ, . . . , jT)' then \nl \u001a i \u001a r  \n(5-42) \nDJ(al, . . .  , aT) = + (sgn (7) A (1, jul) . . . A (n, jun) \nq \n= det [A (ȃ' j1) . . . A (1;, jT)]\n. \nA \n(r, jl) . . . A (r, jT) \nThus DJ(aJ, . . . , aT) is the determinant of the r X r matrix formed from \ncolumns jl, . . .  , j, of the r X n matrix which has (the coordinate n-tuples \nof) al, . . . , aT as its rows. Another notation which is sometimes used for \nthis determinant is \n(5-43) \na(al, . . .  , aT) \na «(3;\" . . . , (3jJ \nIn this notation, the proof of Theorem 7 shows that every alternating \nr-linear form L can be expressed relative to a basis {{31, •\n•\n•\n , (3n} by the \nequation \n(5-44) \nL( \n) \na(al, . . . , exT) L( \nR ) \nal, •\n.\n•\n , aT \n= . .  \n{ \n. !l(R . \nR . )",
    "(5-43) \na(al, . . .  , aT) \na «(3;\" . . . , (3jJ \nIn this notation, the proof of Theorem 7 shows that every alternating \nr-linear form L can be expressed relative to a basis {{31, •\n•\n•\n , (3n} by the \nequation \n(5-44) \nL( \n) \na(al, . . . , exT) L( \nR ) \nal, •\n.\n•\n , aT \n= . .  \n{ \n. !l(R . \nR . ) \n(3;\" .\n.\n•\n , fJj, • \n31 < . . .  <3, v fJ)lJ • • \n• , 1-'), \n173 \n5.7. The Grassman Ring \nMany of the important properties of determinants and alternating \nmultilinear forms are best described in terms of a multiplication operation \non forms, called the exterior product. If L and M are, respectively, alter­\nnating r and s-linear forms on the module V, we have an associated product \nof L and M, the tensor product L ® M. This is not an alternating form \nunless L = 0 or M = 0; however, we have a natural way of projecting it \ninto Ar+8(V). It appears that \n(5-45) \nL . M = 'lfr+8(L ® M) \nshould be the 'natural' multiplication of alternating forms. But, is it? \nLet us take a specific example. Suppose that V is the module Kn and \nfl' . . . , in are the standard coordinate functions on K\". If i :;t!: j, then \nii . !; \n'lf2(fi ® !;) \n174 \nDete:rminants \nChap. 5 \nis the (determinant) function \nDii = Ji OO Ji - iJ OO Ji \ngiven by (5-39). Now suppose k is an index different from i and j. Then \nDij . Jk = 7r3[(f, OOiJ - iJ 00 1;) OO JkJ \n= 7r3(fi 00 J1' 00 Jk) - 7r3(fj 00 Ji 00 Jk). \nThe proof of the lemma following equation (5-36) shows that for any \nr-linear form L and any permutation 0\" of {I, . . . , r} \n7r,(L,,) \nsgn 0\" 7r,(L) \nHence, Dij . Jk = 27r3(1. 00 iJ 00 Jk). By a similar computation, Ji . Djk = \n27r3(f. 00 iJ 00 Jk)' Thus we have \n(fi . Ji) \n. Ik = Ii . (h . Ik) \nand all of this looks very promising. But there is a catch. Despite the \ncomputation that we have just completed, the putative multiplication in \n(5-45) is not associative. In fact, if l is an index different from i, j, k, then \none can calculate that \nDij . DkZ = 47r4(fi 00 Jj 00 Jk 00 Jz) \nand that",
    "computation that we have just completed, the putative multiplication in \n(5-45) is not associative. In fact, if l is an index different from i, j, k, then \none can calculate that \nDij . DkZ = 47r4(fi 00 Jj 00 Jk 00 Jz) \nand that \nThus, in general \nand we see that our first attempt to find a multiplication has produced a \nnon-associative operation. \nThe reader should not be surprised if he finds it rather tedious to give \na direct verification of the two equations showing non-associativity. This \nis typical of the subject, and it is also typical that there is a general fact \nwhich considerably simplifies the work. \nSuppose L is an r-linear form and that M is an s-linear form on the \nmodule V. Then \n7rr+.« 7rrL) 00 (7r.M» \n7rr+.(2; (sgn 0\") (sgn 7\")L\" 00 Mr) \n\".r \n2; (sgn 0\") (sgn r)7rr+.(Lu 00 Mr) \nwhere 0\" varies over the symmetric group, Sr, of all permutations of \n{I, . . . , r} , and 7\" varies over S •. Each pair 0\", 7\" defines an element (0\", 7\") \nof Sr+< which permutes the first r elements of {I, . .\n. , r + s} according \nto 0\" and the last s elements according to 7\". It is clear that \nsgn (0\", 7\") = (sgn 0\") (sgn r) \nand that \nSec. 5.7 \nThe Grassman Ring \nTherefore \n<r,T \nNow we have already observed that \nsgn (u, r)7l'r+s[(L ® M)(<r.rl] = 7l'r+s(L ® M). \nThus, it follows that \n(5-46) \n7l',+s[(7l'rL) ® (7l'sM)] = rlsl 7l'r+s(L ® M). \nThis formula simplifies a number of computations. For example, suppose \nwe have an r-shuffie I \n(il' . . . 1 ir) and s-shuffie .J \n(jl, . . .  , js)' To \nmake things simple, assume, in addition, that \nil < . . .  < ir < jl < . . . < j,. \nThen we have the associated determinant functions \nDr = 7l'r(Er) \nDJ = 7l'8(EJ) \nwhere Er and EJ are given by (5-30). Using (5-46), we see immediately that \nDr . DJ \n7l'r+s[7l'r(Er) ® 7l',(EJ)] \nrls!7l'r+s(Er ® EJ). \nSince Er ® EJ = EI UJ, it follows that \nDr . DJ = r!s! DIUJ' \nThis suggests that the lack of associativity for the multiplication (5-45)",
    "DJ = 7l'8(EJ) \nwhere Er and EJ are given by (5-30). Using (5-46), we see immediately that \nDr . DJ \n7l'r+s[7l'r(Er) ® 7l',(EJ)] \nrls!7l'r+s(Er ® EJ). \nSince Er ® EJ = EI UJ, it follows that \nDr . DJ = r!s! DIUJ' \nThis suggests that the lack of associativity for the multiplication (5-45) \nresults from the fact that Dr . DJ Y DI UJ. After all, the product of Dr \nand DJ ought to be DI UJ. To repair the situation, we should define a new \nproduct, the exterior product (01' wedge product) of an alternating \nr-linear form L and an alternating s-linear form M by \n(5-47) \nWe then have \nDr 1\\ DJ \nDI UJ \nfor the determinant functions on Kn, and, if there is any justice at all, we \nmust have found the proper multiplication of alternating multilinear \nforms. Unfortunately, (5-47) fails to make sense for the most general case \nunder consideration, since we may not be able to divide by r!s! in the \nring K. If K is a field of characteristic zero, then (5-47) is meaningful, and \none can proceed quite rapidly to show that the wedge product is associative. \nTheorem 8. Let K be a field of characteristic zero and V a vector space \nover K. Then the exterior product is an associative operation on the alternating \nmultilinear forms on V. In other words, if L, M, and N are alternating \nmultilinear forms on V of degrees 1', s, and t, r'espectively, then \n175 \n176 \nDeterminants \nChap. 5 \n(L 1\\ M) 1\\ N = L 1\\ (M 1\\ N). \nProof. It follows from (.5-47) that cd(L 1\\ M) = cL 1\\ dM for \nany scalars c and d. Hence \nr!8!t![(L 1\\ M) 1\\ N] = r!8!(L 1\\ M) 1\\ tIN \nand since 'lft(N) \ntIN, it results that \nr!s!t![(L 1\\ M) 1\\ NJ = 'lfr+.(L CD M) 1\\ 'lft(N) \nI \nI \n(r + 8) ! tj 'lfr+8+t['lfr+s(L CD M) CD 'lft(N)]. \nFrom (5-46) we now see that \nr!8!t[(L 1\\ M) 1\\ N] = 'lfr+s+t(L CD M CD N). \nBy a similar computation \nr!8!t![L 1\\ (M 1\\ N)] = 'lfr+8+t(L CD M ® N) \nand therefore, (L 1\\ M) 1\\ N \nL 1\\ (M 1\\ N). \nI \nN ow we re1;urn to the general case, in which it is only assumed that K",
    "From (5-46) we now see that \nr!8!t[(L 1\\ M) 1\\ N] = 'lfr+s+t(L CD M CD N). \nBy a similar computation \nr!8!t![L 1\\ (M 1\\ N)] = 'lfr+8+t(L CD M ® N) \nand therefore, (L 1\\ M) 1\\ N \nL 1\\ (M 1\\ N). \nI \nN ow we re1;urn to the general case, in which it is only assumed that K \nis a commutative ring with identity. Our first problem is to replace (5-47) \nby an equivalent definition which works in general. If L and M are alter­\nnating multilinear forms of degrees r and 8 respectively, we shall constrnct \na canonical alternating multilinear form L 1\\ M of degree r + s such that \nr!s!(L 1\\ M) = 'lfr+.(L ® M). \nLet us recall how we define 'lfr+,(L ® M). With each permutation 0' \nof {I, . . . , r + s} we associate the multilinear function \n(5-48) \n(sgn O')(L ® M)q \nwhere \n(L ® M)q(al' . . . , ar+8) = (L ® M)(aql) . . .  , aq(r+s) \nand we sum the functions (5-48) over all permutations 0'. There are (r + s) ! \npermutations; however, since L and M are alternating, many of the func­\ntions (5-48) are the same. In fact there are at most \n(r + s) ! \nr!s! \ndistinct functions (5-48). Let us see why. Let Sr+8 be the set of permuta­\ntions of {I, . . . , r  + s}, i.e., let Sr+8 be the symmetric group of degree \nr + s. As in the proof of (5-46), we distinguish the subset G that consists \nof the permutations 0' which permute the sets {I, . .\n. , r} and {r + 1, .\n.\n.\n , \nr + s} within themselves. In other words, 0' is in G if 1 ::; O'i ::; r for each \ni between 1 and r. (It necessarily follows that r + 1 ::; O'j ::; r + 8 for \neach j between r \n1 and r + s.) Now G is a subgroup of Sr+\" that is, if \n0' and T are in G' then O'T-1 is in G. Evidently G has r!s! members. \nSec. 5.7 \nWe have a map \ndefined by \nI/;(u) \n(sgn u)(L ® M)u. \nSince L and M are alternating, \nI/;('Y) = L ® M  \nThe Grassman Ring \nfor every 'Y in G. Therefore, since (NU)T = NTU for any (r + s)-linear form \nN on V, we have \nT in Sr+s, 'Y in G. \nThis says that the map I/; is constant on each (left) coset TG of the sub­",
    "defined by \nI/;(u) \n(sgn u)(L ® M)u. \nSince L and M are alternating, \nI/;('Y) = L ® M  \nThe Grassman Ring \nfor every 'Y in G. Therefore, since (NU)T = NTU for any (r + s)-linear form \nN on V, we have \nT in Sr+s, 'Y in G. \nThis says that the map I/; is constant on each (left) coset TG of the sub­\ngroup G. If T1 and T2 are in Sr+s, the cosets T1G and T2G are either identical \nor disjoint, according as Til T1 is in G or is not in G. Each coset contains \nr!s! elements; hence, there are \n(r + s) ! \nr!s! \ndistinct cosets. If Sr+s/G denotes the collection of cosets then I/; defines \na function on Sr+./G, i.e., by what we have shown, there is a function if; \non that set so that \nI/;(T) \nif; (TG) \nfor every T in Sr+s' If H is a left coset of G, then if;(H) \nI/;(T) for every \nT in H. \nWe now define the exterior product of the alternating multilinear \nforms L and M of degrees r and 8 by setting \n(5-49) \nL /\\ M \n+ if; (H) \nH \nwhere H varies over Sr+s/G. Another way to phrase the definition of \nL /\\ M is the following. Let S be any set of permutations of {I, .\n. . , r + s} \nwhich contains exactly one element from each left coset of G. Then \nL /\\ M = + (sgn u) (L ® M)u \nu \nwhere u varies over S. Clearly \nr!s! L /\\ M \n7r,+s(L ® M) \nso that the new definition is equivalent to (5-47) when K is a field of \ncharacteristic zero. \nTheorem 9. Let K be a commutative ring with identity and let V be \na module over K. Then the exterior product is an associative operation on the \nalternating multilinear forms on V. In other words, if L, M, and N are \nalternating multilinear forms on V of degrees r, s, and t, respectively, then \n(L /\\ M) /\\ N = L /\\ (M /\\ N). \n177 \n178 \nDeterminants \nChap. 5 \nProof. Although the proof of Theorem 8 does not apply here, \nit does suggest how to handle the general case. Let G(r, s, t) be the sub­\ngroup of 8r+'+1 that consists of the permutations which permute the sets \n{I, . . .  , r}, {r + 1, . . . , r  + s}, {r + s + 1, . . .  , r  + 8 \nt}",
    "Chap. 5 \nProof. Although the proof of Theorem 8 does not apply here, \nit does suggest how to handle the general case. Let G(r, s, t) be the sub­\ngroup of 8r+'+1 that consists of the permutations which permute the sets \n{I, . . .  , r}, {r + 1, . . . , r  + s}, {r + s + 1, . . .  , r  + 8 \nt} \nwithin themselves. Then (sgn ,.,.,) (L ® M ® N)I' is the same multilinear \nfunction for aU ,.,., in a given left coset of G(r, 8, t). Choose one element \nfrom each left coset of G(r, 8, t), and let E be the sum of the corresponding \nterms (sgn ,.,.,)(L ® M ® N)I\" Then E is independent of the way in which \nthe representatives ,.,., are chosen, and \nr!s!t! E = 7rr+s+t(L ® M ® N). \nWe shall show that (L /\\ M) /\\ N and L /\\ (M /\\ N) are both equal to E. \nLet G(r + 8, t) be the subgroup of 8'+8+t that permutes the sets \n{l, . . .  , r + 8}, {r + s + 1, . . . , r + s + t} \nwithin themselves. Let T be any set of permutations of {I, . . . , r + s + t} \nwhich contains exactly one element from each left coset of G(r + 8, t). \nBy (5-50) \n(L /\\ M) /\\ N = + (sgn 7) [(L /\\ M) ® N]. \n, \nwhere the sum is extended over the permutations T in T. Now let G(r, 8) \nbe the subgroup of 8r+8 that permutes the sets \n{l, . . .  , r}, {r + 1, . . .  , r  + s} \nwithin themselves. Let 8 be any set of permutations of {I, . . .  , r + s} \nwhich contains exactly one element from each left coset of G(r, 8) . From \n(5-50) and what we have shown above, it follows that \n(L 1\\ M) /\\ N = + (sgn 0\") (sgn T) [(L ® M)q ® N]r \nwhere the sum is extended over all pairs 0\", 7 in 8 X T. If we agree to \nidentify each rr in 8r+-, with the element of 8r+s+t which agrees with 0\" on \n{I, . . .  , r  + s} and is the identity On {r + s + 1, . . . , r  + 8 + t}, then \nwe may write \nBut, \nTherefore \n(L 1\\ M) 1\\ N = + sgn (0\" T) [(L ® M ® N)q]T' \n(L /\\ M) /\\ N \n+ sgn (7 O\") (L ® M ® N)Tq. \ntr,T \nN ow suppose we have \nTI0\"1 = T20\"2 'Y \nwith 0\"; in S, Ti in T, and 'Y in G(r, s, t). Then Ti l T1 \n0\"2'Y0\"1 1, and since \nSec. 5.7",
    "we may write \nBut, \nTherefore \n(L 1\\ M) 1\\ N = + sgn (0\" T) [(L ® M ® N)q]T' \n(L /\\ M) /\\ N \n+ sgn (7 O\") (L ® M ® N)Tq. \ntr,T \nN ow suppose we have \nTI0\"1 = T20\"2 'Y \nwith 0\"; in S, Ti in T, and 'Y in G(r, s, t). Then Ti l T1 \n0\"2'Y0\"1 1, and since \nSec. 5.7 \nThe Grassman Ring \nCr2'YlTil lies in G(r + s, t), it follows that 7'1 and 7'2 are in the same left coset \nof G(r + s, 0. Therefore, 7'1 \n7'2, and ITl \n1T2'Y. But this implies that 1T1 \nand 1T2 (regarded as elements of 8r+.) lie in the same coset of G(r, s) ; hence \n1T1 = 1T2. Therefore, the products 7'1T corresponding to the \n(r + s + t) ! (r + s) ! \n(r + s) It! \nr!s! \npairs (7', IT) in T X 8 are all distinct and lie in distinct cosets of G(r, s, t). \nSince there are exactly \n(r + s + t) ! \nr!slt! \nleft cosets of G(r, s, t) in 8r+s+t, it follows that (L 1\\ M) 1\\ N = E. By \nan analogous argument, L 1\\ (M 1\\ N) \nE as well. \nI \nEXAMPLE 13. The exterior product is closely related to certain for­\nmulas for evaluating determinants known as the Laplace expansions. \nLet K be a commutative ring with identity and n a positive integer. Sup­\npose that 1 ȍ r < n, and let L be the alternating r-linear form on Kn \ndefined by \n[All \nL(al, . . .  , ar) = det \n: \nAr1 \nȂlrJ\n' \nATT \nIf s = n - r and M is the alternating s-linear form \n[Al(r+l) \nA:.lnJ \nM(al) . . .  , as) = det \n: \nA8(T+1) \nAsn \nthen L 1\\ M = D, the determinant function on Kn. This is immediate \nfrom the fact that L 1\\ M is an alternating n-linear form and (as can be \nseen) \n(L 1\\ M) (€l' . . .  , En) = 1. \nIf we now describe L 1\\ M in the correct way, we obtain one Laplace \nexpansion for the determinant of an n X n matrix over K. \nIn the permutation group 8n, let G be the subgroup which permutes \nthe sets {I, . . .  , r} and {r + 1, . . .  , n} within themselves. Each left \ncoset of G contains precisely one permutation IT such that IT 1 < 1T2 < . . . < \nITr and IT(r + 1) < . . .  < ITn. The sign of this permutation is given by \nsgn IT \n(",
    "the sets {I, . . .  , r} and {r + 1, . . .  , n} within themselves. Each left \ncoset of G contains precisely one permutation IT such that IT 1 < 1T2 < . . . < \nITr and IT(r + 1) < . . .  < ITn. The sign of this permutation is given by \nsgn IT \n( \nl)u!+ \" '+ur+(r(T-l)/2). \nThe wedge product L 1\\ M is given by \n(L 1\\ M) (a1, . . . , an) \n, (sgn IT)L(alTh . . .  , aur)M(au(r+lh . . .  , au..) \nwhere the sum is taken over a collection of IT'S, one from each coset of G. \nTherefore, \n179 \n180 \nDeterminants \nChap. 5 \nwhere \neJ \n= (_ I)it+ \" ' +i,+(r(r-l)/2) \nki \nO'(r + i). \nIn other words, \nAj\" l \nAj.,r A\"\" r+l \nAkl,n \ndet A \n= \n+ \ne.T \nh < · · · <j, \nAj\" l \nAj\"r Ak\"r+l \nAk\"n \nThis is one Laplace expansion. Others may be obtained by replacing the \nsets {I, . . .  , r} and {r + 1, . . .  , n} by two different complementary \nsets of indices. \nIf V is a K-module, we may put the various form modules Ar(V) \ntogether and use the exterior product to define a ring. For simplicity, we \nshall do this only for the case of a free K-module of rank n. The modules \nAr(V) are then trivial for r > n. We define \nA(V) \n= AO(V) EB Al(V) EB · · ·  EB An(V). \nThis is an external direct sum-something which we have not discussed \npreviously. The elements of A(V) are the (n + I) ·tuples (Lo, . . .  , Ln) \nwith Lr in N(V). Addition and multiplication by elements of K are defined \nas one would expect for (n + I)-tuples. Incidentally, AO(V) = K. If we \nidentify Ar(K) with the (n + 1)-tuples (0, . . .  , 0, L, 0, . . .  , 0) where L \nis in Ar(K), then Ar(K) is a submodule of A(V) and the direct sum \ndecomposition \nA(V) \n= AO(V) EB . . .  EB A\"(V) \nholds in the usual sense. Since Ar(V) is a free K-module of rank (ȁ} we \nsee that A(Vl is a free K-module and \nrank A(V) \n= , (n) \nr=O r \nThe exterior product defines a multiplication in A(V) : Use the exterior \nproduct on forms and extend it linearly to A(V) . It distributes over the",
    "holds in the usual sense. Since Ar(V) is a free K-module of rank (ȁ} we \nsee that A(Vl is a free K-module and \nrank A(V) \n= , (n) \nr=O r \nThe exterior product defines a multiplication in A(V) : Use the exterior \nproduct on forms and extend it linearly to A(V) . It distributes over the \naddition of A(V) and gives A(V) the structure of a ring. This ring is the \nGrassllan ring over V*. It is not a commutative ring, e.g., if L, M are \nrespectively in Ar and AS, then \nL /\\ M = (- I)T8M /\\ L. \nBut, the Grassman ring is important in several parts of mathematics. \n6. Elementary \nCanonical Forms \n6.1 . Introduction \nWe have mentioned earlier that our principal aim is to study linear \ntransformations on finite-dimensional vector spaces. By this time, we have \nseen many specific examples of linear transformations, and we have proved \na few theorems about the general linear transformation. In the finite­\ndimensional case we have utilized ordered bases to represent such trans­\nformations by matrices, and this representation adds to our insight into \ntheir behavior. We have explored the vector space L(V, W), consisting of \nthe linear transformations from one space into another, and we have \nexplored the linear algebra L(V, V), consisting of the linear transformations \nof a space into itself. \nIn the next two chapters, we shall be preoccupied with linear operators. \nOur program is to select a single linear operator T on a finite-dimensional \nvector space V and to 'take it apart to see what makes it tick.' At this \nearly stage, it is easiest to express our goal in matrix language: Given the \nlinear operator T, find an ordered basis for V in which the matrix of T \nassumes an especially simple form. \nHere is an illustration of what we have in mind. Perhaps the simplest \nmatrices to work with, beyond the scalar multiples of the identity, are the \ndiagonal matrices: \n[= \n0 \n0 \n>} \nD \ne ! \nC2 0 \n(6-1) \n0 \nCa \n0 \n0 \nCn \n181 \n182 \nElementary Canonical Forms \nChap. 6",
    "Here is an illustration of what we have in mind. Perhaps the simplest \nmatrices to work with, beyond the scalar multiples of the identity, are the \ndiagonal matrices: \n[= \n0 \n0 \n>} \nD \ne ! \nC2 0 \n(6-1) \n0 \nCa \n0 \n0 \nCn \n181 \n182 \nElementary Canonical Forms \nChap. 6 \nLet T be a linear operator on an n-dimensional space V. If we could find \nan ordered basis CB \n{aI, . . .  , an} for V in which T were represented by \na diagonal matrix D (6-1), we would gain considerable information about T. \nFor instance, simple numbers associated with T, such as the rank of T or \nthe determinant of T, could be determined with little more than a glance \nat the matrix D. We could describe explicitly the range and the null space \nof T. Since [T1B = D if and only if \n(6-2) \nk = 1, . . .  , n \nthe range would bc the subspace spanned by those ak's for which Ck rf= 0 \nand the null space would be spanned by the remaining ak's. Indeed, it \nseems fair to say that, if we knew a basis CB and a diagonal matrix D such \nthat [TJm \nD, we could answer readily any question about T which \nmight arise. \nCan each linear operator T be represented by a diagonal matrix in \nsome ordercd basis? If not, for which operators T does such a basis exist? \nHow can we find such a basis if there is one? If no such basis exists, what \nis the simplest type of matrix by which we can represent T? These are some \nof the questions which we shall attack in this (and the next) chapter. The \nform of our questions will become more sophisticated as we learn what \nsome of the difficulties are. \n6.2. Characteristic Values \nThe introductory remarks of the previous section provide us with a \nstarting point for our attempt to analyze the general linear operator T. \nWe take our cue from (6-2), which suggests that we should study vectors \nwhich are sent by T into scalar multiples of themselves. \nDefinition. Let V be a vector space over the field F and let T be a linear",
    "starting point for our attempt to analyze the general linear operator T. \nWe take our cue from (6-2), which suggests that we should study vectors \nwhich are sent by T into scalar multiples of themselves. \nDefinition. Let V be a vector space over the field F and let T be a linear \noperator on V . .A characteristic value of T is a scalar c in F such that \nthere is a non-zero vector a in V with Ta \nCa. If c is a characteristic value of \nT, then \n(a) any a such that Ta = Ca is called a characteristic vector of T \nassociated with Ihe characteristic value c ;  \n(b) the collection of all a such that Ta \nCa is called the characteristic \nspace associated with c. \nCharacteristic values are often called characteristic roots, latent roots, \neigenvalues, proper values, or spectral values. In this book we shall use \nonly the name 'characteristic values.' \nIf T is any linear operator and c is any scalar, the set of vectors a such \nthat Ta = Ca is a subspace of V. It is the null space of the linear trans-\nSec. 6.2 \nCharacteristic Values \nformation (T - cl). We call c a characteristic value of T if this subspace \nis different from the zero subspace, i.e., if (T \ncl) fails to be 1 :  1. If the \nunderlying space V is finite-dimensional, (T - c1) fails to be 1 :  1 precisely \nwhen its determinant is different from 0. Let us summarize. \nTheorem 1. Let T be a linear operator on a finite-dimensional space V \nand let c be a scalar. The following are equivalent. \n(i) c is a characteristic value of T. \n(ii) The operator (T \ncI) is singular (not invertible). \n(iii) det (T \ncI) = 0. \nThe determinant criterion (iii) is very important because it tells us \nwhere to look for the characteristic values of T. Since det (T \ncl) is a \npolynomial of degree n in the variable c, we will find the characteristic \nvalues as the roots of that polynomial. Let us explain carefully. \nIf CB is any ordered basis for V and A \n[T]I1\\, then (T - c1) is in-\nvertible if and only if the matrix (A",
    "cl) is a \npolynomial of degree n in the variable c, we will find the characteristic \nvalues as the roots of that polynomial. Let us explain carefully. \nIf CB is any ordered basis for V and A \n[T]I1\\, then (T - c1) is in-\nvertible if and only if the matrix (A \ncI) is invertible. Accordingly, we \nmake the following definition. \nDefinition. If A is an n X n matrix over the field F, a characteristic \nvalue of A in F is a scalar c in F such that the matrix (A - cI) is singular \n(not invertible) . \nSince c is a characteristic value of A if and only if det (A - cl) = 0, \nor equivalently if and only if det (cl \nA) \n0, we form the matrix \n(xl - A) with polynomial entries, and consider the polynomial f \ndet (xl - A). Clearly the characteristic values of A in F are just the \nscalars c in F such that fCc) \n= 0. For this reason f is called the charac­\nteristic polynomial of A. It is important to note that f is a monic poly­\nnomial which has degree exactly n. This is easily seen from the formula \nfor the determinant of a matrix in terms of its entries. \nLemma. Similar matrices have the same characteristic polynomial. \nProof. If B = P-IAP, then \ndet (xl - B) = det (xl - P-IAP) \n= det (P-l(xI - A)P) \ndet p-l . det (xl \nA) · det P \n= det (xl - A). I \nThis lemma enables us to define sensibly the characteristic polynomial \nof the operator T as the characteristic polynomial of any n X n matrix \nwhich represents T in some ordered basis for V. Just as for matrices, the \ncharacteristic values of T will be the roots of the characteristic polynomial \nfor T. In particular, this shows us that T cannot have more than n distinct \n183 \n184 \nElementary Canonical Forms \nChap. 6 \ncharacteristic values. It is important to point out that T may not have any \ncharacteristic values. \nEXAMPLE 1. Let T be the linear operator on R2 which is represented \nin the standard ordered basis by the matrix \nA = [\u001f -[l \nThe characteristic polynomial for T (or for A) is \ndet (xl - A) = I_Ȁ !I = x2 + 1.",
    "characteristic values. \nEXAMPLE 1. Let T be the linear operator on R2 which is represented \nin the standard ordered basis by the matrix \nA = [\u001f -[l \nThe characteristic polynomial for T (or for A) is \ndet (xl - A) = I_Ȁ !I = x2 + 1. \nSince this polynomial has no real roots, T has no characteristic values. \nIf U is the linear operator on C2 which is represented by A in the standard \nordered basis, then U has two characteristic values, i and -i. Here we \nsee a subtle point. In discussing the characteristic values of a matrix \nA ,  we must be careful to stipulate the field involved. The matrix A above \nhas no characteristic values in R, but has the two characteristic values \ni and -i in C. \nEXAMPLE 2. Let A be the (real) 3 X 3 matrix \n[3 1 2 2 2 2 -lJ \n-9 . \nThen the characteristic polynomial for A is \nx-3 \n-2 2 -1 1 \nx 2 1 = x3 \n2 x 5x2 + 8x 4 = (x \nThus the characteristic values of A are 1 and 2. l)(x -2)2. \nSuppose that T is the linear operator on R3 which is represented by A \nin the standard basis. Let us find the characteristic vectors of T associated \nwith the characteristic values, 1 and 2. Now \nA - I = [; 6 = 6J' \n2 2 -1 \nIt is obvious at a glance that A \n- I has rank equal to 2 (and hence T - I \nhas nullity equal to 1). So the space of characteristic vectors associated \nwith the characteristic value 1 is one-dimensional. The vector al = (1, 0, 2) \nspans the null space of T - I. Thus Ta = a if and only if a is a scalar \nmultiple of al. Now consider \nA \n-21 \n= [; [ -6J' \n2 2 -2 \nSec. 6.2 \nCharacteristic Values \nEvidently A \n21 also has rank 2, so that the space of characteristic \nvectors associated with the characteristic value 2 has dimension 1. Evi­\ndently Ta = 2a if and only if a is a scalar multiple of a2 = (1, 1, 2). \nDefinition. Let T be a linear operator on the finite-dimensional space . \nV. We say that T is diagonalizahle if there is a basis for V each vector \nof which is a characteristic vector of T.",
    "dently Ta = 2a if and only if a is a scalar multiple of a2 = (1, 1, 2). \nDefinition. Let T be a linear operator on the finite-dimensional space . \nV. We say that T is diagonalizahle if there is a basis for V each vector \nof which is a characteristic vector of T. \nThe reason for the name should be apparent; for, if there is an ordered \nbasis CB = {aI, . . . , an} for V in which each ai is a characteristic vector of \nT, then the matrix of T in the ordered basis CB is diagonal. If Tai \nCiai, \nthen \nWe certainly do not require that the scalars Cl, •\n•\n. , Cn be distinct; indeed, \nthey may all be the same scalar (when T is a scalar multiple of the identity \noperator). \nOne could also define T to be diagonalizable when the characteristic \nvectors of T span V. This is only superficially different from our definition, \nsince we can select a basis out of any spanning set of vectors. \nFor Examples 1 and :2 we purposely chose linear operators T on Rn \nwhich are not diagonalizable. In Example 1, we have a linear operator on \nR2 which is not diagonalizable, because it has no characteristic values. \nIn Example 2, the operator l' has characteristic values; in fact, the charac­\nteristic polynomial for T factors completely over the real number field: \nf = (x - l)(x - 2)2. Nevertheless T fails to he diagonalizable. There is \nonly a one-dimensional space of characteristic vectors associated with each \nof the two characteristic values of T. Hence, we cannot possibly form a \nbasis for R8 which consists of characteristic vectors of T. \nSuppose that T is a diagonalizable linear operator. Let Cl, •\n•\n•\n , Ck be \nthe distinct characteristic values of T. Then there is an ordered basis CB in \nwhich T is represented by a diagonal matrix which has for its diagonal \nentries the scalars Ci, each repeated a certain number of times. If Ci is \nrepeated di times, then (we may arrange that) the matrix has the block \nform \n(6-3)",
    "which T is represented by a diagonal matrix which has for its diagonal \nentries the scalars Ci, each repeated a certain number of times. If Ci is \nrepeated di times, then (we may arrange that) the matrix has the block \nform \n(6-3) \nwhere Ij is the dj X elj identity matrix. From that matrix we see two things. \nFirst, the characteristic polynomial for T is the product of (possibly \nrepeated) linear factors: \n185 \n186 \nElementary Canonical Forms \nChap. 6 \nf = (x - Cl)dl •\n•\n•\n (x - Ck)d •• \nIf the scalar field F is algebraically closed, e.g., the field of complex num­\nbers, every polynomial over F can be so factored (see Section 4.5) ; however, \nif F is not algebraically closed, we are citing a special property of T when \nwe say that its characteristic polynomial has such a factorization. The \nsecond thing we see from (6-3) is that di, the number of times which Ci is \nrepeated as root of f, is equal to the dimension of the space of characteristic \nvectors associated with the characteristic value Ci. That is because the \nnullity of a diagonal matrix is equal to the number of zeros which it has on \nits main diagonal, and the matrix rT - c;!]il\\ has di zeros on its main \ndiagonal. This relation between the dimension of the characteristic space \nand the multiplicity of the characteristic value as a root of f does not seem \nexciting at first; however, it will provide us with a simpler way of deter­\nmining whether a given operator is diagonalizable. \nLemma. Suppose that Ta = Ca. If f is any polynomial, then f(T)a = \nf(c)a. \nProof. Exercise. \nLemma. Let T be a linear operator on the finite-dimensional space V. \nLet Cl, •\n•\n•\n , Ck be the distinct characteristic values of T and let Wi be the space \nof characteristic vectors associated with the characteristic value Ci. If W = \nWI + . . .  + W k, then \ndim W = dim WI + . . .  + dim W ko \nIn fact, if <Bi is an ordered basis for Wi, then <B = (<B1' .\n.\n•\n , <Bk) is an ordered \nbasis for W. \nProof. The space W",
    "of characteristic vectors associated with the characteristic value Ci. If W = \nWI + . . .  + W k, then \ndim W = dim WI + . . .  + dim W ko \nIn fact, if <Bi is an ordered basis for Wi, then <B = (<B1' .\n.\n•\n , <Bk) is an ordered \nbasis for W. \nProof. The space W \nWI + . . .  + Wk is the subspace spanned \nby all of the characteristic vectors of T. Usually when one forms the sum \nW of subspaces Wi, one expects that dim W < dim WI + . . .  + dim Wk \nbecause of linear relations which may exist between vectors in the various \nspaces. This lemma states that the characteristic spaces associated with \ndifferent characteristic values are independent of one another. \nSuppose that (for cach i) we have a vector {3i in Wi, and assume that \n(:31 + . . .  + (:3k \nO. We shall show that {3i \n0 for each i. Let f be any \npolynomiaL Since T(:3i = Ci(:3i, the preceding lemma tells us that \no = f(T)O = f(T)(:31 + . . . + f(T)(:3k \n= f(Cl){31 + . . . + f(Ck)(:3k. \nChoose polynomials Jr, . . . , fk such that \n{I, i \nj \n0, i ჰj. \nSec. 6.2 \nThen \no = fi(T)O = + 5ij{jj \nj \n= {h \nCharacteristic Values \nNow, let CBi be an ordered basis for Wi, and let CB be the sequence \nCB = (CBI, •\n•\n•\n , CBk). Then CB spans the subspace W = WI + \n'\n\"\n + Wk. \nAlso, (B is a linearly independent sequence of vectors, for the following \nreason. Any linear relation between the vectors in CB will have the form \n{jl + . . .  + {jk = 0, where {ji is some linear combination of the vectors in \nCBi• From what we just did, we know that {ji = 0 for each i. Since each CBi \nis linearly independent, we see that we have only the trivial linear relation \nbetween the vectors in (B. I \nTheorem 2. Let T be a linear operator on a finite-dimensional space V. \nLet Cl, •\n•\n•\n , Ck be the distinct characteristic values of T and let Wi be the null \nspace of (T - cil). The following are equivalent. \n(i) T is diagonalizable. \n(ii) The characteristic polynomial for T is \nf = (x - Cl)dl •\n•\n•\n (x - Ck)dk \nand dim Wi = db i = 1, . . . , k.",
    "Let Cl, •\n•\n•\n , Ck be the distinct characteristic values of T and let Wi be the null \nspace of (T - cil). The following are equivalent. \n(i) T is diagonalizable. \n(ii) The characteristic polynomial for T is \nf = (x - Cl)dl •\n•\n•\n (x - Ck)dk \nand dim Wi = db i = 1, . . . , k. \n(iii) dim WI + \n'\n\"\n + dim Wk = dim V. \nProof. We have observed that (i) implies (ii). If the characteristic \npolynomial f is the product of linear factors, as in (ii) , then dl + . . .  + \ndk = dim V. For, the sum of the d;'s is the degree of the characteristic \npolynomial, and that degree is dim V. Therefore (ii) implies (iii). Suppose \n(iii) holds. By the lemma, we must have V = WI + . . .  + Wk, i.e., the \ncharacteristic vectors of T span V. I \nThe matrix analogue of Theorem 2 may be formulated as follows. Let \nA be an n X n matrix with entries in a field F, and let CI, . . .  , Ck be the \ndistinct characteristic values of A in F. For each i, let Wi be the space of \ncolumn matrices X (with entries in F) such that \n(A - c;l)X = 0, \nand let CBi be an ordered basis for Wi. The bases CBI, •\n•\n•\n , CBk collectively \nstring together to form the sequence of columns of a matrix P :  \nP = [PI, P2, •\n•\n•\n J = (CBI, •\n•\n•\n , CBk). \nThe matrix A is similar over F to a diagonal matrix if and only if P is a \nsquare matrix. When P is square, P is invertible and P-IAP is diagonal. \nEXAMPLE 3. Let T be the linear operator on R3 which is represented in \nthe standard ordered basis by the matrix \n187 \n188 \nElementary Canonical Forms \n- 6  -6J \n4 \n2 ·  \n- 6  -4 \nChap. 6 \nLet us indicate how one might compute the characteristic polynomial, \nusing various row and column operations: \nx - 5  \n1 \n-3 \n6 \nx - 4  \n6 \n6 \nx - 5  \n0 \n6 \n-2 \n1 \nx - 2 \n-2 \nx + 4 \n-3 \n2 - x x + 4 \nx - 5 \n0 \n6 \n= (x - 2) \n1 \n1\n-2 \n-3 \n-1 x + 4 \nx - 5 0 \n6 \n= (x - 2) \n1 \n1\n-2 \n-2 \n0 x + 2 \n= (x - 2) Iǿ; 5 \nx ! 21 \n= (x - 2)(X2 - 3x + 2) \n= (x - 2)2(X - 1). \nWhat are the dimensions of the spaces of characteristic vectors associated",
    "6 \n6 \nx - 5  \n0 \n6 \n-2 \n1 \nx - 2 \n-2 \nx + 4 \n-3 \n2 - x x + 4 \nx - 5 \n0 \n6 \n= (x - 2) \n1 \n1\n-2 \n-3 \n-1 x + 4 \nx - 5 0 \n6 \n= (x - 2) \n1 \n1\n-2 \n-2 \n0 x + 2 \n= (x - 2) Iǿ; 5 \nx ! 21 \n= (x - 2)(X2 - 3x + 2) \n= (x - 2)2(X - 1). \nWhat are the dimensions of the spaces of characteristic vectors associated \nwith the two characteristic values? We have \nA - I ჼH \n-6 -ő] \n3 \n-6 -5 \nA - 2I ჽH \n-6 -6J \n2 \n2 . \n-6 -6 \nWe know that A - J is singular and obviously rank (A - J) r 2. There­\nfore, rank (A - J) = 2. It is evident that rank (A - 21) = 1. \nLet Wl, W2 be the spaces of characteristic vectors associated with the \ncharacteristic values 1, 2. We know that dim WI = 1 and dim W2 = 2. By \nTheorem 2, T is diagonalizable. It is easy to exhibit a basis for Ra in which \nT is represented by a diagonal matrix. The null space of (T - I) is spanned \nby the vector al = (3, - 1, 3) and so {al} is a basis for WI. The null space \nof T - 21 (i.e., the space W2) consists of the vectors (Xl, X2, Xa) with Xl = \n2X2 + 2xa. Thus, one example of a basis for W2 is \na2 = (2, 1, 0) \naa = (2, 0, 1). \nIf CB = {aI, a2, aa} , then [TJm is the diagonal matrix \nSec. 6.2 \nCharacteristic Values \nD = [ō ǽ ŎJ' \n0\n0\n2\n \nThe fact that T is diagonalizable means that the original matrix A is \nsimilar (over R) to the diagonal matrix D. The matrix P which enables us \nto change coordinates from the basis Cl3 to the standard basis is (of course) \nthe matrix which has the transposes of aI, a2, a3 as its column vectors: \nP = [-Ő q ŏJ' \n3\n0\n1\n \nFurthermore, AP = PD, so that \nP-1AP = D. \nExercises \n1. In each of the following cases, let T be the linear operator on R2 which is \nrepresented by the matrix A in the standard ordered basis for R2, and let U be \nthe linear operator on C2 represented by A in the standard ordered basis. Find the \ncharacteristic polynomial for T and that for U, find the characteristic values of \neach operator, and for each such characteristic value c find a basis for the cor­",
    "the linear operator on C2 represented by A in the standard ordered basis. Find the \ncharacteristic polynomial for T and that for U, find the characteristic values of \neach operator, and for each such characteristic value c find a basis for the cor­\nresponding space of eharacteristic vectors. \nA \n= [; w} \nA \n= [  iT \nA \n= G xl \n2. Let V be an n-dimensional vector space over F. What is the characteristic \npolynomial of the identity operator on V? What is the characteristic polynomial \nfor the zero operator? \n3. Let A be an n X n triangular matrix over the field F. Prove that the charac­\nteristic values of A are the diagonal entries of A, i.e., the scalars Au. \n4. Let T be the linear operator on R3 which is represented in the standard ordered \nbasis by the matrix \n[ =Ǿ : !]. \n- 16 8 7 \nProve that T is diagonalizable by exhibiting a basis for R3, each veetor of which \nis a characteristic vector of T. \n5. Let \nA = [1 =O =;]. \n10 \n-5 -3 \nIs A similar over the field R to a diagonal matrix? Is A similar over the field C to a \ndiagonal matrix'! \n189 \n190 \nElementary Canonical Forms \nChap. 6 \n6. Let T be the linear operator on R4 which is represented in the standard ordered \nbasis by the matrix \nUnder what conditions on a, b, and c is T diagonalizable? \n7. Let T be a linear operator on the n-dimensional vector space V, and suppose \nthat T has n distinct characteristic values. Prove that T is diagonalizable. \n8. Let A and B be n X n matrices over the field F. Prove that if (I - AB) is \ninvertible, then I - BA is invertible and \n(l - BA)-l = I + B(l - AB)-IA. \n9 .  Use the result of Exercise 8 to prove that, if A and B are n X n matrices \nover the field F, then AB and BA have precisely the same characteristic values in F. \n10. Suppose that A is a 2 X 2 matrix with real entries which is symmetric (A e \nA). \nProve that A is similar over R to a diagonal matrix. \nII. Let N be a 2 X 2 complex matrix such that N2 = O. Prove that either N = 0 \nor N is similar over C to",
    "10. Suppose that A is a 2 X 2 matrix with real entries which is symmetric (A e \nA). \nProve that A is similar over R to a diagonal matrix. \nII. Let N be a 2 X 2 complex matrix such that N2 = O. Prove that either N = 0 \nor N is similar over C to \n12. Use the result of Exercise 11 to prove the following: If A is a 2 X 2 matrix \nwith complex entries, then A is similar over C to a matrix of one of the two types \n13. Let V be the vector space of all functions from R into R which are continuous, \ni.e., the space of continuous real-valued functions on the real line. Let T be the \nlinear operator on V defined by \n(Tf)(x) = loX f(t) dt. \nProve that T has no characteristic values. \n14. Let A be an n X n diagonal matrix with characteristic polynomial \n(x - Cl)d, •\n.\n.\n (x - Ck)d., \nwhere Cl, •\n•\n•\n , Ck are distinct. Let V be the space of n X n matrices B such that \nAB \nBA. Prove that the dimension of V is di + . . , + dі. \n15. Let V be the space of n X n matrices over F. Let A be a fixed n X n matrix \nover F. Let T be the linear operator 'left multiplication by A' on V. Is it true that \nA and T have the same characteristic values? \n6.3. Annihilating Polynomials \nIn attempting to analyze a linear operator T, one of the most useful \nthings to know is the class of polynomials which annihilate T. Specifically, \nSec. 6.3 \nAnnihilating Polynomials \nsuppose T is a linear operator on V, a vector space over the field F. If p is a \npolynomial over F, then p(T) is again a linear operator on V. If q is another \npolynomial over F, then \n(p + q) (T) \n= peT) + q(T) \n(pq) (T) \n= p(T)q(T). \nTherefore, the collection of polynomials p which annihilate T, in the sense \nthat \np(T) \n= 0, \nis an ideal in the polynomial algebra F[x]. It may be the zero ideal, i.e., it \nmay be that T is not annihilated by any non-zero polynomial. But, that \ncannot happen if the space V is finite-dimensional. \nSuppose T is a linear operator on the n-dimensional space V. Look at \nthe first (n2 + 1) powers of T :",
    "may be that T is not annihilated by any non-zero polynomial. But, that \ncannot happen if the space V is finite-dimensional. \nSuppose T is a linear operator on the n-dimensional space V. Look at \nthe first (n2 + 1) powers of T :  \nI, T, T2, . . .  , Tn'. \nThis is a sequence of n2 + 1 operators in L(V, V), the space of linear \noperators on V. The space L(V, V) has dimension n2• Therefore, that \nsequence of n2 + 1 operators must be linearly dependent, i.e., we have \nCol + c1T + . .\n. + cn.Tn' = 0 \nfor some scalars Ci, not all zero. So, the ideal of polynomials which annihilate \nT contains a non-zero polynomial of degree n2 or less. \nAccording to Theorem 5 of Chapter 4, every polynomial ideal consists \nof all multiples of some fixed monic polynomial, the generator of the ideal. \nThus, there corresponds to the operator T a monic polynomial p with this \nproperty : If f is a polynomial over F, then f(T) = 0 if and only if f \npg, \nwhere 9 is some polynomial over F. \nDefinition. Let T be a linear operator on a finite-dimensional vector \nspace V over the field F. The minimal polynomial for T is the (unique) \nmonic generator of the ideal of polynomials over F which annihilate T. \nThe name 'minimal polynomial' stems from the fact that the generator \nof a polynomial ideal is characterized by being the monic polynomial of \nminimum degree in the ideal. That means that the minimal polynomial p \nfor the linear operator T is uniquely determined by these three properties : \n( 1) p is a monic polynomial over the scalar field F. \n(2) p(T) \nO. \n(3) No polynomial over F which annihilates T has smaller degree than \np has. \nIf A is an n X n matrix over F, vve define the minimal polynomial \nfor A in an analogous way, as the unique monic generator of the ideal of all \npolynomials over F which annihilate A .  If the operator T is represented in \n191 \n192 \nElementary Canonical Forms \nChap. 6 \nsome ordered basis by the matrix A, then T and A have the same minimal",
    "for A in an analogous way, as the unique monic generator of the ideal of all \npolynomials over F which annihilate A .  If the operator T is represented in \n191 \n192 \nElementary Canonical Forms \nChap. 6 \nsome ordered basis by the matrix A, then T and A have the same minimal \npolynomial. That is because f(T) is represented in the basis by the matrix \nf(A), so that f(T) = 0 if and only if f(A) = o. \nFrom the last remark about operators and matrices it follows that \nsimilar matrices have the same minimal polynomial. That fact is also clear \nfrom the definitions because \nf(P-IAP) = P-If(A)P \nfor every polynomial j. \nThere is another basic remark which we should make about minimal \npolynomials of matrices. Suppose that A is an n X n matrix with entries \nin the field F. Suppose that FI is a field which contains F as a subfield. (For \nexample, A might be a matrix with rational entries, while FI is the field of \nreal numbers. Or, A might be a matrix with real entries, while FI is the \nfield of complex numbers.) We may regard A either as an n X n matrix \nover F or as an n X n matrix over Fl. On the surface, it might appear that \nwe obtain two different minimal polynomials for A. Fortunately that is \nnot the case; and we must see why. What is the definition of the minimal \npolynomial for A, regarded as an n X n matrix over the field F? We \nconsider all monic polynomials with coefficients in F which annihilate A, \nand we choose the one of least degree. If f is a monic polynomial over F: \n(6-4) \nk-l \nf = Xk + + ajxj \nj=O \nthen f(A) = 0 merely says that we have a linear relation between the \npowers of A :  \n(6-5) \nThe degree of the minimal polynomial is the least positive integer k such \nthat there is a linear relation of the form (6-5) between the powers I, \nA ,  . . .  , A k. Furthermore, by the uniqueness of the minimal polynomial, \nthere is for that k one and only one relation of the form (6-5) ; i.e., once the",
    "that there is a linear relation of the form (6-5) between the powers I, \nA ,  . . .  , A k. Furthermore, by the uniqueness of the minimal polynomial, \nthere is for that k one and only one relation of the form (6-5) ; i.e., once the \nminimal k is determined, there are unique scalars ao, . . .  , ak_l in F such \nthat (6-5) holds. They are the coefficients of the minimal polynomial. \nNow (for each k) we have in (6-5) a system of n2 linear equations for \nthe 'unknowns' ao, . . .  , ak-l. Since the entries of A lie in F, the coefficientll \nof the system of equations (6-5) are in F. Therefore, if the system has :L \nsolution with ao, . . .  , ak-l in FI it has a solution with ao, . . .  , ak_l in F. \n(See the end of Section 1.4.) It should now be clear that the two minima.l \npolynomials are the same. \nWhat do we know thus far about the minimal polynomial for a linear \noperator on an n-dimensional space? Only that its degree does not exceed \nn2• That turns out to be a rather poor estimate, since the degree cannot \nexceed n. We shall prove shortly that the operator is annihilated by its \ncharacteristic polynomial. First, let us observe a more elementary fact. \nSec. 6.3 \nAnnihilating Polynomials \nTheorem 3. Let T be a linear operator on an n-dimensional vector \nspace V [or, let A be an n X n matrix]. The characteristic and minimal \npolynomials for T [for A] have the same roots, except for multiplicities. \nProof. Let p be the minimal polynomial for T. Let c be a scalar. \nWhat we want to show is that p(c) = 0 if and only if c is a characteristic \nvalue of T. \nFirst, suppose p(c) = O. Then \np = (x - c)q \nwhere q is a polynomial. Since deg q < deg p, the definition of the minimal \npolynomial p tells us that q(T) ˻ O. Choose a vector {3 such that q(T){3 ˻ O. \nLet a = q(T){3. Then \no = p(T){3 \n(T \ncI)q(T){3 \n= (T - cI)a \nand thus, c is a characteristic value of T. \nNow, suppose that c is a characteristic value of T, say, Ta = ca with \na ˻ O. As we noted in a previous lemma,",
    "polynomial p tells us that q(T) ˻ O. Choose a vector {3 such that q(T){3 ˻ O. \nLet a = q(T){3. Then \no = p(T){3 \n(T \ncI)q(T){3 \n= (T - cI)a \nand thus, c is a characteristic value of T. \nNow, suppose that c is a characteristic value of T, say, Ta = ca with \na ˻ O. As we noted in a previous lemma, \np(']')a = p(c)a. \nSince p(T) = 0 and a :r! 0, we have p(c) = o. \nI \nIJet T be a diagonalizable linear operator and let CI, .\n•\n•\n , Ck be the \ndistinct characteristic values of T. Then it is easy to see that the minimal \npolynomial for T is the polynomial \np \n(x - CI) • • •  (x - Ck). \nIf a is a characteristic vector, then one of the operators T \ncd, . . .  , \nT - ckI sends a into O. Therefore \n(T \ncd) ' \" (T - cd)a = 0 \nfor every characteristic vector a. There is a basis for the underlying space \nwhich consists of characteristic vectors of T; hence \npeT) = (T - cd) . . .  (T - ckI) = O. \nWhat we have concluded is this. If T is a diagonalizable linear operator, \nthen the minimal polynomial for T is a product of distinct linear factors. \nAs we shall soon see, that property characterizes diagonalizable operators. \nEXAMPLE 4. Let's try to find the minimal polynomials for the operators \nin Examples 1, 2, and 3. We shall discuss them in reverse order. The oper­\nator in Example 3 was found to be diagonalizable with characteristic \npolynomial \nf \n(x - l)(x - 2)2. \n193 \n194 \nElementary Canonical Forms \nChap. 6 \nFrom the preceding paragraph, we know that the minimal polynomial for \nT is \np = (x - 1)(x - 2). \nThe reader might find it reassuring to verify directly that \n(A - I) (A - 21) = o. \nIn Example 2, the operator T also had the characteristic polynomial \nf = (x \n1)(x - 2)2. But, this T is not diagonalizable, so we don't know \nthat the minimal polynomial is (x \n1) (x - 2). What do we know about \nthe minimal polynomial in this case'? From Theorem 3 we know that its \nroots are 1 and 2, with some multiplicities allowed. Thus we search for p \namong polynomials of the form (x \nl)k(x",
    "that the minimal polynomial is (x \n1) (x - 2). What do we know about \nthe minimal polynomial in this case'? From Theorem 3 we know that its \nroots are 1 and 2, with some multiplicities allowed. Thus we search for p \namong polynomials of the form (x \nl)k(x \n2)1, k 2: 1, l 2: 1. Try (x - 1) \n(x - 2) : \n[Ō \n1 \n(A - I)(A - 21) \n1 \n2 \n-[! \n0 \n0 \n0 \n- IJ[1 \n- 1  \n2 \n- 1  \n2 \n- IJ \n- 1  . \n- 2  \nThus, the minimal polynomial has degree at least 3 .  So, next we should try \neither (x - l)lњ(x \n2) or (x - l)(x - 2)2. The second, being the charac­\nteristic polynomial, would seem a less random choice. One can readily \ncompute that (A - J) (A \n21) 2 \nO. Thus the minimal polynomial for T \nis its characteristic polynomial. \nIn Example 1 we discussed the linear operator T on R2 which is \nrepresented in the standard basis by the matrix \nThe characteristic polynomial is x2 + 1, which has no real roots. To \ndetermine the minimal polynomial, forget about T and concentrate on A. \nAs a complex 2 X 2 matrix, A has the characteristic values i and -i. \nBoth roots must appear in the minimal polynomial. Thus the minimal \npolynomial is divisible by x2 + 1. It is trivial to verify that A 2 + I \nO. \nSo the minimal polynomial is X2 + 1. \nTheorem 4 (Cayley-llamilton). Let T be a linear operator on a \nfinite dimensional vector space V. If f is the characteristic polynomial for T, \nthen f(T) = 0; in other words, the minimal polynomial divides the charac­\nteristic polynomial for T. \nProof. Later on we shall give two proofs of this result independent \nof the one to be given here. The present proof, although short, may be \ndifficult to understand. Aside from brevity, it has the virtue of providing \nSec. 6.3 \nAnnihilating Polynomials \nan illuminating and far from trivial application of the general theory of \ndeterminants developed in Chapter 5. \nLet K be the commutative ring with identity consisting of all poly­\nnomials in T. Of course, K is actually a commutative algebra with identity",
    "Sec. 6.3 \nAnnihilating Polynomials \nan illuminating and far from trivial application of the general theory of \ndeterminants developed in Chapter 5. \nLet K be the commutative ring with identity consisting of all poly­\nnomials in T. Of course, K is actually a commutative algebra with identity \nover the scalar field. Choose an ordered basis {aI, . . . , an} for V, and let A \nbe the matrix which represents T in the given basis. Then \nn \nTai = + A;ia,., \njܳ1  \n1 ::; i ::; n. \nThese equations may be written in the equivalent form \nn \n+ (5i;T \nAjil)aj = 0, \n; = 1  \n1 ::; i ::; n. \nLet B denote the element of KnXn with entries \nBij = 5ijT \nAiJ. \nWhen n = 2 \nand \ndet B = (T - AnI)(T - A2d) - A12A211 \n= T2 - (An \nA22)T + (AnA22 - A12A21)I \nJ(T) \nwhere J is the characteristic polynomial: \nJ = x2 \n(trace A)x + det A .  \nFor the case n > 2, it is also clear that \ndet B = J(T) \nsince J is the determinant ,of the matrix xl \nA whose entries are the \npolynomials \n(xl - A)ij = OijX - Aj;. \nWe wish to show thatJ(T) = 0. In order thatJ(T) be the zero operator, \nit is necessary and sufficient that (det B)ak = ° for k = 1, . . , , n. By the \ndefinition of B, the vectors aI, . . .  , an satisfy the equations \n(6-6) \nn \n+ B ijaj = 0, \ni = 1  \n1 ::; i ::; n. \nWhen n = 2, it is suggestive to write (6-6) in the form \n[T \nA nI \n-A2l1 J [alJ [OJ \n-AI2I \nT - A2d \na2 = 0 ' \nIn this case, the classical adjoint, adj B is the matrix \nB [T \nA2d A2d \nJ \nA 121 \nT - AnI \n195 \n196 \nElementary Canonical Forms \nand \nHence, we have \nBB [det B 0 J. \no \ndet B \n(det B) [\nall = (BB) [al] \na2..J \na2 \n= B (B[::J) \n= [5J \nIn the general case, let B =' adj B. Then by (6-6) \nn \n_ \n2; BkiBijaj = 0 \ni=l \nfor each pair k, i, and summing on i, we have \nn \nn \no \n= 2; 2; BkiBijaj \n;=1 i- I  \nn (\nn \n_ \n) \n2; \n2; BkiBij a;. \n;= 1 i=' 1 \nNow BB = (det B)/, so that \nTherefore \nn o \n= 2; (hj(det B)aj \ni-I \n1 ::; k ::; n. \nI \nChap. 6 \nThe Cayley-Hamilton theorem is useful to us at this point primarily",
    "i=l \nfor each pair k, i, and summing on i, we have \nn \nn \no \n= 2; 2; BkiBijaj \n;=1 i- I  \nn (\nn \n_ \n) \n2; \n2; BkiBij a;. \n;= 1 i=' 1 \nNow BB = (det B)/, so that \nTherefore \nn o \n= 2; (hj(det B)aj \ni-I \n1 ::; k ::; n. \nI \nChap. 6 \nThe Cayley-Hamilton theorem is useful to us at this point primarily \nbecause it narrows down the search for the minimal polynomials of various \noperators. If we know the matrix A which represents T in some ordered \nbasis, then we can compute the characteristic polynomial f. We know that \nthe minimal polynomial p divides f and that the two polynomials have the \nsame roots. There is no method for computing precisely the roots of a \npolynomial (unless its degree is small) ; however, if f factors \n(6-7) \nf = (x - CI)dl . . .  (x - Ck)dk, \nCI, •\n•\n•\n , Ck distinct, d; ?: 1 \nthen \n(6-8) \np = (x - CIt' . . .  (x - Ck)Tk, \n1 :::; rj ::; dj• \nThat is all we can say in general. If f is the polynomial (6-7) and has \ndegree n, then for every polynomial p as in (6-8) we can find an n X n \nmatrix which has f as its characteristic polynomial and p as its minimal \nSec. 6.3 \nAnnihilating Polynomials \npolynomial. We shall not prove this now. But, we want to emphasize the \nfact that the knowledge that the characteristic polynomial has the form \n(6-7) tells us that the minimal polynomial has the form (6-8), and it tells us \nnothing else about p. \nEXAMPLE 5. Let A be the 4 X 4 (rational) matrix \n[0 1 0 \nn \nA \n= \n1 \n0 1 \no 1 0 \n1 0 1 \nThe powers of A are easy to compute: \nA' ɰ [ 0 2 \nJ \n2 0 0 2 \n2 0 \nA '  ɱ [ \n4 0 \nn \n0 4 \n4 0 0 4 \nThus A 3 = 4A, i.e., if p = x3 - 4x = x(x + 2)(x - 2), then peA) = o. \nThe minimal polynomial for A must divide p. That minimal polynomial is \nobviously not of degree 1, since that would mean that A was a scalar \nmultiple of the identity. Hence, the candidates for the minimal polynomial \nare: p, x(x + 2), x(x - 2), x2 - 4. The three quadratic polynomials can be",
    "The minimal polynomial for A must divide p. That minimal polynomial is \nobviously not of degree 1, since that would mean that A was a scalar \nmultiple of the identity. Hence, the candidates for the minimal polynomial \nare: p, x(x + 2), x(x - 2), x2 - 4. The three quadratic polynomials can be \neliminated because it is obvious at a glance that A2  -2A, A 2 Y 2A, \nA 2 ǭ 41. Therefore p is the minimal polynomial for A .  In particular 0, 2, \nand -2 are the characteristic values of A. One of the factors x, x - 2, \nx + 2 must be repeated twice in the characteristic polynomial. Evidently, \nrank (A) = 2. Consequently there is a two-dimensional space of charac­\nteristic vectors associated with the characteristic value O. From Theorem \n2, it should now be clear that the characteristic polynomial is X2(X2 - 4) \nand that A is similar over the field of rational numbers to the matrix \nExercises \n1. Let V be a finite-dimensional vector space. What is the minimal polynomial \nfor the identity operator on V? What is the minimal polynomial for the zero \noperator? \n197 \n198 \nElementary Canonical Forms \nChap. 6 \n2. Let a, b, and c be elements of a field F, and let A be the following 3 X 3 matrix \nover F: \n[0 0 C] \nA = \nl\nO\nb · \no 1 a \nProve that the characteristic polynomial for A is x3 - ax2 - bx - C and that this \nis also the minimal polynomial for A. \n3. Let A be the 4 X 4 real matrix [- - \nA = \n-2 -2 \n1 1 o 0] o 0 2 1' \n-1 0 \nShow that the characteristic polynomial for A is x2(x - 1)2 and that it is also \nthe minimal polynomial. \n4. Is the matrix A of Exercise 3 similar over the field of complex numbers to a \ndiagonal matrix? \n5. Let V be an n-dimensional vector space and let T be a linear operator on V. \nSuppose that there exists some positive integer k so that Tk = O. Prove that \nTn = O. \n6. Find a 3 X 3 matrix for which the minimal polynomial is x2• \n7. Let n be a positive integer, and let V be the space of polynomials over R",
    "Suppose that there exists some positive integer k so that Tk = O. Prove that \nTn = O. \n6. Find a 3 X 3 matrix for which the minimal polynomial is x2• \n7. Let n be a positive integer, and let V be the space of polynomials over R \nwhich have degree at most n (throw in the O-polynomial). Let D be the differentia­\ntion operator on V. What is the minimal polynomial for D? \n8. Let P be the operator on R2 which projects each vector onto the x-axis, parallel \nto the y-axis: P(x, y) = (x, 0). \nShow that P is linear. What is the minimal poly­\nnomial for P? \n9. Let A be an n X n matrix with characteristic polynomial \nf = (x - CI)dl •\n.\n•\n (x - Ck)d •• \nShow that \ncldl + . . .  + Ckdk = trace (A). \n10. Let V be the vector space of n X n matrices over the field F. Let A be a fixed \nn X n matrix. Let T be thl:) linear operator on V defined by \nT(B) \nAB. \nShow that the minimal polynomial for T is the minimal polynomial for A .  \nII. Let A and B be n X n matrices over the field F. According to Exercise 9 of \nSection 6.1, the matrices AB and BA have the same characteristic values. Do \nthey have the same characteristic polynomial? Do they have the same minimal \npolynomial? \n6.4. Invariant Subspaces \nIn this section, we shall introduce a few concepts which are useful in \nattempting to analyze a linear operator. We shall use these ideas to obtain \nSec. 6.4 \nInvariant Subspaces \ncharacterizations of diagonalizable (and triangulable) operators in terms \nof their minimal polynomials. \nDefinition. Let V be a vector space and T a linear operator on V. If \nW is a subspace of V, we say that W is invariant under T if for each vector \n0; in W the vector To; is in W, i.e., if T(W) is contained in W. \nEXAMPLE 6. If T is any linear operator on V, then V is invariant \nunder T, as is the zero subspace. The range of T and the null space of T \nare also invariant under T. \nEXAMPLE 7. Let F be a field and let D be the differentiation operator",
    "EXAMPLE 6. If T is any linear operator on V, then V is invariant \nunder T, as is the zero subspace. The range of T and the null space of T \nare also invariant under T. \nEXAMPLE 7. Let F be a field and let D be the differentiation operator \non the space F[x] of polynomials over F. Let n be a positive integer and \nlet W be the subspace of polynomials of degree not greater than n. Then W \nis invariant under D. This is just another way of saying that D is 'degree \ndecreasing. ' \nEXAMPLE 8. Here is a very useful generalization of Example 6. Let T \nbe a linear operator on V. Let U be any linear operator on V which com­\nmutes with T, i.e., TU \nUT. Let W be the range of U and let N be the \nnull space of U. Both W and N are invariant under T. If 0; is in the range \nof U, say 0; = U{3, then To; = T(U(3) = U(T(:3) so that Tet is in the range \nof U. If 0; is in N, then U(To;) = T(Uet) \nT(O) \n0; hence, To; is in N. \nA particular type of operator which commutes with T is an operator \nU = geT), where g is a polynomial. For instance, we might have U = \nT - cI, where c is a characteristic value of T. The null space of U is \nfamiliar to us. We see that this example includes the (obvious) fact that \nthe space of characteristic vectors of T associated with the characteristic \nvalue c is invariant under T. \nEXAMPLE 9. Let T be the linear operator on R2 which is represented \nin the standard ordered basis by the matrix \nThen the only subspaces of R2 which are invariant under T are RZ and the \nzero subspace. Any other invariant subspace would necessarily have \ndimension 1. But, if W is the subspace spanned by some non-zero vector 0;, \nthe fact that W is invariant under T means that 0; is a characteristic \nvector, but A has no real characteristic values. \nWhElIl the subspace W is invariant under the operator T, then T \ninduces a linear operator Tw on the space W. The linear operator Tw is \ndefined by Tw(o;) = '1'(0;), for 0; in W, but Tw is quite a different object",
    "vector, but A has no real characteristic values. \nWhElIl the subspace W is invariant under the operator T, then T \ninduces a linear operator Tw on the space W. The linear operator Tw is \ndefined by Tw(o;) = '1'(0;), for 0; in W, but Tw is quite a different object \nfrom '1' since its domain is W not V. \nWhen V is finite-dimensional, the invariance of W under T has a \n199 \n'£00 \nElementary Canonical Forms \nChap. 6 \nsimple matrix interpretation, and perhaps we should mention it at this \npoint. Suppose we choose an ordered basis il \n{all ' . .  , an} for V such \nthat il' \n= {al' . . . , ar} is an ordered basis for TV (r \ndim TV). Let A \n[T](l so that \nn \nTaj =  Aijai. \n; = 1  \nSince TV is invariant under T, the vector Taj belongs to TV for j :S r. This \nmeans that \n(6-9) \nr \nTaj =  A'jai, \ni \u0014 l  \nj 5, r. \nIn other words, A ij \n0 if j :S r and i > r. \nSchematieally, A has the block form \n(6-10) \nA [\f \rJ \nwhere B is an r X r matrix, C is an r X (n - r) matrix, and D is an \n(n - r) X (n \nr) matrix. The reader should note that according to \n(6-9) the matrix B is precisely the matrix of the induced operator Tw in \nthe ordered basis il' . \n.Most often, we shall carry out arguments about T and Tw without \nmaking use of the block form of the matrix A in (6-10). But we should note \nhow certain relations between Tw and T are apparent from that block form. \nLemma. Let W be an invariant subspace for T. The characteristic \npolynomial for the restriction operator Tw divides the characteristic polynomial \nfor T. The minimal polynomial for Tw divides the minimal polynomial for T. \nProof. We have \nA [\n \u000bJ \nwhere A \n[T](B and B \n[Tw](B\" Because of the block form of the matrix \ndet (xl - A) \n= det (xl - B) det (xl - D). \nThat proves the statement about characteristic polynomials. Notice that \nwe used I to represent identity matrices of three different sizes. \nThe kth power of the matrix A has the block form \nAk = [\fk ;:]",
    "[Tw](B\" Because of the block form of the matrix \ndet (xl - A) \n= det (xl - B) det (xl - D). \nThat proves the statement about characteristic polynomials. Notice that \nwe used I to represent identity matrices of three different sizes. \nThe kth power of the matrix A has the block form \nAk = [\fk ;:] \nwhere Ck is some r X (n - r) matrix. Therefore, any polynomial which \nannihilates A also annihilates B (and D too). So, the minimal polynomial \nfor B divides the minimal polynomial for A. I \nEXAMPLE 10. I¦et T be any linear operator on a finite-dimensional \nspace V. Let TV be the subspace spanned by all of the characteristic vectors \nSec. 6.4 \nInvariant Subspaces \nof T. Let CI, •\n.\n.\n , Ck be the distinct characteristic values of T. For each i, \nlet Wi be the space of characteristic vectors associated with the charac­\nteristic value Ci, and let mi be an ordered basis for Wi. The lemma before \nTheorem 2 tells us that m' \n(m), . . .  , mk) is an ordered basis for W. In \nparticular, \ndim W \ndim WI + ' \"  + dim Wk. \nLet m' = {aI, . . .  , aT} SO that the first few a's form the basis ml, the next \nfew m2, and so on. Then \nwhere (tl) \"\n\" tr) \ndim Wi times. \nTai = tiai, \ni \n1, . . . , r \nNow W is invariant under T, since for each a in W we have \na\nXIal + \n.\n.\n.\n + xrar \nTa = tlxla1 + . . .  + trXrar. \nChoose any other vectors ar+l, . . .  , an in V such that m = {a!, . . .  , an} \nis a basis for V. The matrix of T relative to m has the block form (6-10), and \nthe matrix of the restriction operator Tw relative to the basis m' is \nB \nThe characteristic polynomial of B (i.e., of Tw) is \n9 = (x - CI)e1 • • •  (x - Ck)ek \nwhere ei = dim Wi. Furthermore, 9 divides f, the characteristic polynomial \nfor T. Therefore, the multiplicity of Ci as a root of f is at least dim Wi. \nAll of this should make Theorem 2 transparent. It merely says that T \nis diagonalizable if and only if r \nn, if and only if €1 + . . , + €k \nn. It",
    "for T. Therefore, the multiplicity of Ci as a root of f is at least dim Wi. \nAll of this should make Theorem 2 transparent. It merely says that T \nis diagonalizable if and only if r \nn, if and only if €1 + . . , + €k \nn. It \ndoes not help us too much with the non-diagonalizable ease, since we don't \nknow the matrices C and D of (6-10). \nDefinition. Let W be an invariant subspace fo)' T and let a be a v€ctor \nin V. The T-conductor of a into W is the set ST(a; W), which consists of \nall polynomials g (over the scalar field) such that g(T)a is in W. \nSince the operator T will be fixed throughout most discussions, we \nshall usually drop the subscript T and write S (a ; W) . The authors usually \ncall that collection of polynomials the 'stuffer' (das einstopfencle I deal). \n'Conductor' is the more standard term, preferred by those who envision \na less aggressive operator geT), gently leading the vector a into W. In the \nspecial case W \n{O} the conductor is called the T-annihilator of a. \n201 \n202 \nElementary Canonical Forms \nCbap. 6 \n(..,emma. If W is an invariant subspace for T, then W is invariant \nunder every polynomial in T. Thus, for each a in V, the conductor Sea; W) is \nan ideal in the polynomial algebra F[x]. \nProof. If (3 is in W, then T{3 is in W. Consequently, T(T{3) = T2{3 \nis in W. By induction, Tk{3 is in W for each k. Take linear combinations to \nsee that f(T){3 is in W for every polynomial f. \nThe definition of Sea ;  W) makes sense if W is any subset of V. If W is \na subspace, then Sea; W) is a subspace of F[x], because \n(cf + g)(T) \n= cf(T) + geT). \nIf W is also invariant under T, let 9 be a polynomial in Sea; W), i.e., let \ng( T)a be in W. If f is any polynomial, then f( T) [g( T)aJ will be in W. Since \n(fg)(T) \n= f(T)g(T) \nfg is in Sea; W). Thus the conductor absorbs multiplication by any poly­\nnomial. I \nThc unique monic generator of the ideal Sea; W) is also called the \nT-conductor of a into W (the T-annihilator in case W \n{O} ). The",
    "(fg)(T) \n= f(T)g(T) \nfg is in Sea; W). Thus the conductor absorbs multiplication by any poly­\nnomial. I \nThc unique monic generator of the ideal Sea; W) is also called the \nT-conductor of a into W (the T-annihilator in case W \n{O} ). The \nT-conductor of a into W is the monic polynomial 9 of least degree such that \ng(T)a is in W. A polynomialf is in Sea; W) if and only if 9 divides f. Note \nthat the conductor Sea; W) always contains the minimal polynomial for T; \nhence, every T-conductor divides the minimal polynomial for T. \nAs the first illustration of how to use the conductor Sea; W), we shall \ncharacterize triangulable operators. The linear operator T is called tri­\nangulable if there is an ordered basis in which T is represented by a \ntriangular matrix. \nLemma. Let V be a finite-dimensional vector space over the field F. \nLet T be a linear operator on V such that the minimal polynomial for T is a \nproduct of linear factors \nCi in F. \nLet W be a proper (W .,:= V) subspace of V which is invariant under T. There \nexists a vector a in V 81lch that \n(a) a is not in W; \n(b) (T \ncI)a is in W, for some characteristic value c of the operator T. \nProof. What (a) and (b) say is that the T-conductor of a into W \nis a linear polynomial. Let (3 be any vector in V which is not in W. Let (j be \nthe T-conductor of (3 into W. Then (j divides p, the minimal polynomial \nfor T. Since (3 is not in W, the polynomial 9 is not constant. Therefore, \n9 = (x - C1)\" . . . (x - Ck)\" \nSec. 6.4 \nJ nvariant Subspaces \nwhere at least one of the integers ei is positive. Choose j so that ej > O. \nThen (x - Cj) divides g: \n9 \n(x \ncj)h. \nBy the definition of g, the vector a \nh(T){3 cannot be in W. But \n(T - c) h(T)(3 \n(T \ncjJ)a \ng(T)(3 \nis in W. \nI \nTheorem 5. Let V be a finite-dimensional vector space over the field F \nand let T be a linear operator on V. Then T is triangulable if and only if the \nminimal polynomial for T is a product of linear polynomials over F.",
    "h(T){3 cannot be in W. But \n(T - c) h(T)(3 \n(T \ncjJ)a \ng(T)(3 \nis in W. \nI \nTheorem 5. Let V be a finite-dimensional vector space over the field F \nand let T be a linear operator on V. Then T is triangulable if and only if the \nminimal polynomial for T is a product of linear polynomials over F. \nProof. Suppose that the minimal polynomial factors \np \n(x \nCI)rt • . • (x \nCk)r •• \nBy repeated application of the lemma above, we shall arrive at an ordered \nbasis <B = {aIt . . . , an} in which the matrix representing T is upper­\ntriangular: \n[f' \nal2 al3 \nalnl \na22 a23 \na2n \n(6-1 1) \n[T](Il \n0 a33 \nN3nJ \n0 \n0 \nann \nNow (6-11) merely says that \n(6-12) \nTaj = aljal + . . .  + ajjaiJ \nl S; j S; n \nthat is, Taj is in the subspace spanned by aI, . . .  , aj. To find aI, . . .  , an, \nwe start by applying the lemma to the subspace W \n{O} , to obtain the \nvector al. Then apply the lemma to WI, the space spanned by aI, and we \nobtain Q!2. Next apply the lemma to TV 2, the space spanned by al and a2. \nContinue in that way. One point deserves comment. After aI, . . . , ai have \nbeen found, it is the triangular-type relations (6-12) for j = 1, . . .  , i \nwhich ensure that the subspace spanned by aI, . . . , ai is invariant under \nT. \nIf T is triangulable, it is evident that the characteristic polynomial for \nT has the form \nCi in F. \nJust look at the triangular matrix (6-11). The diagonal entries au, .\n•\n.\n , aln \nare the characteristic values, with Ci repeated di times. But, if f can be so \nfactored, so can the minimal polynomial p, because it divides f. \nI \nCorollary. Let F be an algebraically closed field, e.g., the complex num­\nber field. Every u X n matrix over F is similar over F to a triangular matrix. \n203 \n204 \nElementary Canonical Forms \nChap. 6 \nTheorem 6. Let V be a finite-dimensional vector space over the field F \nand let T be a linear operator on V. Then T is diagonalizable if and only if the \nminimal polynomial for T has the form \np = (x - C1) . • • (x - Ck)",
    "203 \n204 \nElementary Canonical Forms \nChap. 6 \nTheorem 6. Let V be a finite-dimensional vector space over the field F \nand let T be a linear operator on V. Then T is diagonalizable if and only if the \nminimal polynomial for T has the form \np = (x - C1) . • • (x - Ck) \nwhere C1, . . .  , Ck are distinct elements of F. \nProof. We have noted earlier that, if T is diagonalizable, its \nminimal polynomial is a product of distinct linear factors (see the discussion \nprior to Example 4). To prove the converse, let W be the subspace spanned \nby all of the characteristic vectors of T, and suppose W ,.r, V. By the lemma \nused in the proof of Theorem 5, there is a vector a not in W and a charac­\nteristic value Cj of T such that the vector \n(3 = (T - cjI)a \nlies in W. Since (3 is in W, \n(3 = (31 + . . . + (310 \nwhere T(3i = Ci(3i, 1 ȍ i ৕ k, and therefore the vector \nh(T)(3 = h(C1)(31 + . . , + h(Ck)(3k \nis in W, for every polynomial h. \nNow p = (x - Cj)q, for some polynomial q. Also \nq - q(Cj) = (x - cj)h. \nWe have \nq(T)a - q(cj)a = h(T)(T - ciI)a = h(T)B \nBut h(T)(3 is in W and, since \no = p(T)a = (T - cjI)q(T)a \nthe vector q(T)a is in W. Therefore, q(cj)a is in W. Since a is not in W, we \nhave q(Cj) = O. That contradicts the fact that p has distinct roots. I \nAt the end of Section 6.7, we shall give a different proof of Theorem 6. \nIn addition to being an elegant result, Theorem 6 is useful in a computa­\ntional way. Suppose we have a linear operator T, represented by the matrix \nA in some ordered basis, and we wish to know if T is diagonalizable. We \ncompute the characteristic polynomial f. If we can factor f: \nf = (x - Cl)dl •\n•\n•\n (x - Ck)do \nwe have two different methods for determining whether or not T is diago­\nnalizable. One method is to see whether (for each i) we can find di inde­\npendent characteristic vectors associated with the characteristic value Ci. \nThe other method is to check whether or not (T - cd) . .\n. (T - ckI) is \nthe zero operator.",
    "nalizable. One method is to see whether (for each i) we can find di inde­\npendent characteristic vectors associated with the characteristic value Ci. \nThe other method is to check whether or not (T - cd) . .\n. (T - ckI) is \nthe zero operator. \nTheorem 5 provides a different proof of the Cayley-Hamilton theorem. \nThat theorem is easy for a triangular matrix. Hence, via Theorem 5, we \nSec. 6.4 \nInvariant Subspaces \nobtain the result for any matrix over an algebraically closed field. Any \nfield is a subfield of an algebraically closed field. If one knows that result, \none obtains a proof of the Cayley-Hamilton theorem for matrices over any \nfield. If we at least admit into our discussion the Fundamental Theorem of \nAlgebra (the complex number field is algebraically closed), then Theorem 5 \nprovides a proof of the Cayley-Hamilton theorem for complex matrices, \nand that proof is independent of the one which we gave earlier. \nExercises \n1. Let T be the linear operator on R2, the matrix of which in the standard ordered \nbasis is \nA \n= [ȯ -no \n(a) Prove that the only subspaces of R2 invariant under T are R2 and the \nzero subspace. \n(b) If U is the linear operator on C2, the matrix of which in the standard \nordered basis is A, show that U has I-dimensional invariant subspaces. \n2. Let W be an invariant subspace for T. Prove that the minimal polynomial \nfor the restriction operator Tw divides the minimal polynomial for T, without \nreferring to matrices. \n3. Let c be a characteristic value of T and let W be the space of characteristic \nvectors associated with the characteristic value C. What is the restriction opera­\ntor Tw? \n4. Let \n[0 \n1 0J \nA = \n2 \n-2 2 . \n2 \n-3 2 \nIs A similar over the field of real numbers to a triangular matrix? If so, find such a \ntriangular matrix. \n5. Every matrix A such that A2 \n= A is similar to a diagonal matrix. \n6. Let T be a diagonalizable linear operator on the n-dimensional vector space V,",
    "[0 \n1 0J \nA = \n2 \n-2 2 . \n2 \n-3 2 \nIs A similar over the field of real numbers to a triangular matrix? If so, find such a \ntriangular matrix. \n5. Every matrix A such that A2 \n= A is similar to a diagonal matrix. \n6. Let T be a diagonalizable linear operator on the n-dimensional vector space V, \nand let W be a subspace which is invariant under T. Prove that the restriction \noperator Tw is diagonalizable. \n7. Let T be a linear operator on a finite-dimensional vector space over the field \nof complex numbers. Prove that T is diagonalizable if and only if T is annihilated \nby some polynomial over C which has distinct roots. \n8. Let T be a linear operator on V. If every subspace of V is invariant under T, \nthen T is a scalar multiple of the identity operator. \n9. Let T be the indefinite integral operator \n(Tf)(x) = !ox J(t) dt \n205 \n206 \nElementary Canonical Forms \nChap. 6 \non the space of continuous functions on the interval [0, 1J. Is the space of poly­\nnomial functions invariant under T? The space of differentiable functions? The \nspace of functions which vauish at x \n!? \n10. Let A be a 3 X 3 matrix with real entries. Prove that, if A is not similar over R \nto a triangular matrix, then A is similar over C to a diagonal matrix. \n11. True or false? If the triangular matrix A is similar to a diagonal matrix, then \nA is already diagonal. \n12. Let T be a linear operator on a finite-dimensional vector space over an alge­\nbraically closed field F. Let J be a jlolynomial over F. Prove that c is a character­\nistic value of J(T) if and ouly if c \nJ(t), where t is a characteristic value of T. \n13. Let V be the space of n X n matrices over F. Let A be a fixed n X n matrix \nover F. Let T and U be the linear operators on V defined by \nT(B) = .1B \nU(B) = A B  - BA . \n(a) True or false'? If A is diagonalizable (over F), then T is diagonalizable. \n(b) True or false? If A is diagoualizablc, then U i\" diagonalizable. \n6.5. Simultaneous Triangulation; \nSimultaneous Diagonalization",
    "T(B) = .1B \nU(B) = A B  - BA . \n(a) True or false'? If A is diagonalizable (over F), then T is diagonalizable. \n(b) True or false? If A is diagoualizablc, then U i\" diagonalizable. \n6.5. Simultaneous Triangulation; \nSimultaneous Diagonalization \nLet V be a finite-dimensional space and let ;:v be a family of linear \noperators on V. We ask when we can simultaneously triangulate or diago­\nnalize the operators in ;:V, i.e., find one basis 03 such that all of the matrices \n[T]([l, T in ;:V, are triangular (or diagonal). In the case of diagonalization, it \nis necessary that ;:v be a commuting family of operators : UT = TU for all \nT, U in ;:v. That follows from the fact that all diagonal matrices commute. \nOf course, it is also necessary that each operator in ;:v be a diagonalizable \noperator. In order to simultaneously triangulate, each operator in ;:v must \nbe triangulable. It is not necessary that ;:v be a commuting family; however, \nthat condition is sufficient for simultaneous triangulatioll (if each T can be \nindividually triangulated). These results follow from minor variations of \nthe proofs of Theorems 5 and G. \nThe subspace W is invariant under (the family of operators) ;:v if \nW is invariant under each operator in ;:V. \nLemma. Let ;:v be a commuting family of triangulable linear operators \non V. Let W be a proper subspace of V 'which is invariant under ;:v. There \nexists a vector 0: in V such that \n(a) 0: is not in W; \n(b) for each T in ;:V, the vector To: is in the subspace spanned by 0: and W. \nProof. It is no loss of generality to assume that ;:v contains only a \nfinite number of operators, because of this observation. Let {TIl ' . .  , Tr} \nSec. 6.5 \nSimultaneous Triangulation; Simultaneous Diagonalization \nbe a maximal linearly independent subset of ff, i.e., a basis for the subspace \nspanned by ff. If a is a vector such that (b) holds for each Ti, then (b) will \nhold for every operator which is a linear combination of T1, • • •  , Tr•",
    "Simultaneous Triangulation; Simultaneous Diagonalization \nbe a maximal linearly independent subset of ff, i.e., a basis for the subspace \nspanned by ff. If a is a vector such that (b) holds for each Ti, then (b) will \nhold for every operator which is a linear combination of T1, • • •  , Tr• \nBy the lemma before Theorem 5 (this lemma for a single operator), we \ncan find a vector fh (not in TV) and a scalar Cl such that (TI - crI)flr is in TV. \nLet VI be the collection of all vectors (J in V such that (TI - crI)(J is in W. \nThen VI is a subspace of V which is properly larger than W. Furthermore, \nVI is invariant under ff, for this reaSOll. If T commutes with TI, then \n(TI - crI)(T(3) \nT(TI - crI)(3. \nIf {3 is in VI, then (TI - cll){3 is in W. Since W is invariant under each T in \nff, we have T(TI - crI)(J ill W, i.e., T(3 in VI, for all (J in VI and all T in ff. \nNow TV is a proper subspace of VI. Let U2 be the linear operator on VI \nobtained by restricting Tz to the subspace V!. The minimal polynomial for \nUz divides the minimal polynomial for T2• Therefore, we may apply the \nlemma before Theorem 5 to that operator and the invariant subspace W. \nWe obtain a vector (32 in VI (not in W) and a scalar Cz such that (T2 \nc21){32 \nis in W. Note that \n(a) {32 is not in W; \n(b) (TI - crI)(32 is in W; \n(c) (T2 - Cd){32 is in W. \nLet V2 be the set of all vcctors {3 in VI such that (T2 \ncd)(J is in W. \nThen Vz is invariant under ff. Apply the lemma before Theorem 5 to Us, \nthe restriction of Ta to Vz• If we continue in this way, we shall reach a \nvector a = {3r (not in W) such that (Tj - cjI)a is in W, j = 1, . .\n. , r. \nI \nTheorem 7. Let V be a finite-dimensional vector space over the field F. \nLet ff be a commuting family of triangulable linear operators on V. There exists \nan ordered basis for V such that every operator in ff is represented by a triangu­\nlar matrix in that basis. \nProof. Given the lemma which we just proved, this theorem has",
    "Let ff be a commuting family of triangulable linear operators on V. There exists \nan ordered basis for V such that every operator in ff is represented by a triangu­\nlar matrix in that basis. \nProof. Given the lemma which we just proved, this theorem has \nthe same proof as does Theorem 5, if one replaces l' by ff, \nI \nCorollary. Let ff be a: commuting family of n X n matrices over an \nalgebraically closed field F. There exists a non-singular n X n matrix P with \nentries in F such that P-IAP is upper-triangular, for every matrix A in ff. \nTheorem 8. Let ff be a commuting family of diagonalizable linear \noperators on the finite-dimensional vector space V. There exists an ordered basis \nfor V such that every operator in ff is represented in that basis by a diagonal \nmatrix. \nProof. We could prove this theorem by adapting the lemma \nbefore Theorem 7 to the diagonalizable case, just as we auapted the lemma \n207 \n;e08 \nElementary Canonical Forms \nChap. 6 \nbefore Theorem .5 to the diagonalizable case in order to prove Theorem 6. \nHowever, at this point it is easier to proceed by induction on the dimension \nof V. \nIf dim V = 1, there is nothing to prove. Assume the theorem for \nvector spaces of dimension less than n, and let V be an n-dimensional space. \nChoose any T in rr- which is not a scalar multiple of the identity. Let \nCl) . . .  , Ck be the distinct characteristic values of T, and (for each i) let Wi \nbe the null space of T - cJ. Fix an index i. Then Wi is invariant under \nevery operator which commutes with T. Let rr-i be the family of linear \noperators on Wi obtained by restricting the operators in rr- to the (invariant) \nsubspace Wi. Each operator in rr-i is diagonalizable, because its minimal \npolynomial divides the minimal polynomial for the corresponding operator \nin rr-. Since dim Wi < dim V, the operators in rr-i can be simultaneously \ndiagonalized. In other words, Wi has a basis (J3i which consists of vectors",
    "polynomial divides the minimal polynomial for the corresponding operator \nin rr-. Since dim Wi < dim V, the operators in rr-i can be simultaneously \ndiagonalized. In other words, Wi has a basis (J3i which consists of vectors \nwhich are simultaneously characteristic vectors for every operator in rr-i. \nSince T is diagonalizable, the lemma before Theorem 2 tells us that \n(J3 = «(J3I, •\n•\n•\n , (J3k) is a basis for V. That is the basis we seek. \nI \nExercises \n1. Find an invertible real matrix P such that P-IAP and P-IBP are both diago­\nnal, where A and B are the real matrices \n(a) \n(b) \nA = [; Ǹ} \nA = [ȧ ǹ} \nB = [Ȩ =ǼJ \nB = [! Ƿl \n2. Let rr- be a commuting family of 3 X 3 complex matrices. How many linearly \nindependent matrices can rr- contain? What about the n X n case? \n3. Let T be a linear operator on an n-dimensional space, and suppose that T \nhas n distinct characteristic values. Prove that any linear operator which commutes \nwith T is a polynomial in T. \n4. Let A, B, C, and D be n X n complex matrices which commute. Let E be the \n2n X 2n matrix \nE = [Ǻ ǻJ \nProve that det E = det (AD - BC). \n5. Let F be a field, n a positive integer, and let V be the space of n X n matrices \nover F. If A is a fixed n X n matrix over F, let T A be the linear operator on V \ndefined by T A(B) = A B - BA. Consider the family of linear operators T A ob­\ntained by letting A vary over all diagonal matrices. Prove that the operators in \nthat family are simultaneously diagom ًizable. \nSec. 6.6 \nDirect-Sum Decompositions \n209 \n6.6. Direct-Sum Decompositions \nAs we continue with our analysis of a single linear operator, we shall \nformulate our ideas in a slightly more sophisticated way-less in terms of \nmatrices and more in terms of subspaces. When we began this chapter, we \ndescribed our goal this way: To find an ordered basis in which the matrix \nof T assumes an especially simple form. Now, we shall describe our goal",
    "formulate our ideas in a slightly more sophisticated way-less in terms of \nmatrices and more in terms of subspaces. When we began this chapter, we \ndescribed our goal this way: To find an ordered basis in which the matrix \nof T assumes an especially simple form. Now, we shall describe our goal \nas follows: To decompose the underlying space V into a sum of invariant \nsubspaces for 7' such that the restriction operators on those subspaces are \nsimple. \nDefinition. Let WI, . . . , Wk be subspaces of the vector space V. We \nsay that WI, . . . , Wk are independent if \nal + . . . + ak = 0, \nai in Wi \nimplies that each ai is O. \nFor k \n2, the meaning of independence is {o} intersection, i.e., WI \nand W2 are independent if and only if WI n W2 = {O} . If k > 2, the \nindependence of WI, . . .  , Wk says much more than Wl n . . .  n Wk = \n{O} . It says that each Wj intersects the sum of the other subspaces Wi \nonly in the zero vector. \nThe significance of independence is this. Let W = WI + . . .  + Wk \nbe the subspace spanned by Wi, . . . , Wk. Each vector a in W can be \nexpressed as a sum \na = 0'1 + ' \"  + ak, \nai in W;. \nIf WI, . . .  , Wk are independent, then that expression for a is unique; for if \na = /31 + . . .  + 13k, \n13i in Wi \nthen ° = (0'1 - (31) + . . . + (ak \n(3k), hence ai - 13i = 0, i = 1, .\n. . , k. \nThus, when WI, . . . , Wk are independent, we can operate with the vectors \nin W as k-tuples (ai, . . .  , ak), ai in Wi, in the same way as we operate with \nvectors in Rk as k-tuples of numbers. \nLemma. Let V be a finite-dimensional vector space. Let WI, . . .  , Wk \nbe subspaces of V and let W \nWI + .\n. . + W ko The following are equivalent. \n(a) WI, . . .  , W k are independent. \n(b) For each j, 2 ¢ j ¢ k, we have \nWj n (WI + . . . + Wi_I) = {O} .  \n(c) If ffii is an ordered basis for Wi, 1 ¢ i ȍ k, then the sequence ffi = \n(ffil, .\n. . , ffik) is an ordered basis for W. \n210 \nElementary Canonical Forms \nChap. 6",
    "(a) WI, . . .  , W k are independent. \n(b) For each j, 2 ¢ j ¢ k, we have \nWj n (WI + . . . + Wi_I) = {O} .  \n(c) If ffii is an ordered basis for Wi, 1 ¢ i ȍ k, then the sequence ffi = \n(ffil, .\n. . , ffik) is an ordered basis for W. \n210 \nElementary Canonical Forms \nChap. 6 \nProof. Assume (a). Let a be a vector in the intersection Wj n \n(Wl + . . , + Wj-l). Then there are vectors aI, . . .  , aj-l with ai in Wi \nsuch that a \nal + . . . + aj-l. Since \nal + . . . \naj-l + ( -a) + 0 + . . .  \n0 = 0  \nand since Wl, . . .  , Wk are independent, it must be that al = az \naj-l = a = O. \nNow, let us observe that (b) implies (a). Suppose \no = al + . . . + ak, \nai in Wi. \nLet j be the largest integer i such that ai ;;z!' O. Then \naj ;;z!' O. \nThus aj \n-al \naj_l is a non-zero vector in Wi n (Wl + . . .  + \nWj..-l)' \nNow that we know (a) and (b) are the same, let us see why (a) is \nequivalent to (c). Assume (a). Let ffii be a basis for Wi, 1 ::; i ::; k, and let \nffi = (ffir, . . .  , ffik). Any linear relation between the vectors in ffi will have \nthe form \n{3l + . . .  \nwhere {3i is some linear combination of the vectors in ffii. Since WI, . . .  , Wk \nare independent, each fJi is O. Since each ffii is independent, the relation we \nhave between the vectors in ffi is the trivial relation. \nWe relegate the proof that (c) implies (a) to the exercises (Exercise \n2). \nI \nIf any (and hence all) of the conditions of the last lemma hold, we \nsay that the sum W = Wl + ' \"  + Wk is direct or that W is the direct \nsum of Wl, . . .  , Wk and we write \nW \nWl 8j · · ·  8j Wk. \nIn the literature, the reader may find this direct sum referred to as an \nindependent sum or the interior direct sum of WI, . . .  , Wk. \nEXAMPLE 11. Let V be a finite-dimensional vector space over the field \nF and let {al, . . . , an} be any basis for V. If Wi is the one-dimensional \nsubspace spanned by ai, then V = WI 8j . . .  8j Wn• \nEXAMPLE 12. Let n be a positive integer and F a sub field of the com­",
    "EXAMPLE 11. Let V be a finite-dimensional vector space over the field \nF and let {al, . . . , an} be any basis for V. If Wi is the one-dimensional \nsubspace spanned by ai, then V = WI 8j . . .  8j Wn• \nEXAMPLE 12. Let n be a positive integer and F a sub field of the com­\nplex numbers, and let V be the space of all n X n matrices over F. Let \nWl be the subspace of all symmetric matrices, i.e., matrices A such that \nA t = A .  Let lIV2 be the subspace of all slww-symmetric matrices, i.e., \nmatrices A such that At = -A. Then V = Wl 8j Wz. If A is any matrix \nin V, the unique expression for A as a sum of matrices, one in WI and the \nother in Wz, is \nSec. 6.6 \nA = Al + A2 \nA l  = HA + A t) \nA2 \n!(A \nA t). \nDirect-Sum Decompositions \nEXAMPLE 13. Let T be any linear operator on a finite-dimensional \nspace V. Let CI, •\n•\n•\n , Ck be the distinct characteristic values of T, and let \nWi be the space of characteristic vectors associated with the characteristic \nvalue Ci. Then Wb . . .  , Wk are independent. See the lemma before Theo-\nrem 2. In particular, if T is diagonalizable, then V \nWI ED .\n. . ED Wk. \nDefinition. If V is a vector space, a projection of V is a linear \noperator E on V such that E2 = E. \nSuppose that E is a projection. Let R be the range of E and let N be \nthe null space of E. \n1. The vector {3 is in the range R if and only if E{3 = {3. If {3 = Ea, \nthen E{3 = E2a = Ea = {3. Conversely, if {3 \nE{3, then (of course) {3 is in \nthe range of E. \n2. V = R ED N. \n3. The unique expression for a as a sum of vectors in R and N is \na \nEa + (a - Ea). \nFrom (1), (2), (3) it is easy to see the following. If R and N are sub­\nspaces of V such that V \nR ED  N, there is one and only one projection \noperator E which has range R and null space N. That operator is called the \nprojection on R along N. \nAny projection E is (trivially) diagonalizable. If {al, . . .  , ar} is a \nbasis for R and {ar+l, . . .  , an} a basis for N, then the basis CB = {aI, . . . ,",
    "operator E which has range R and null space N. That operator is called the \nprojection on R along N. \nAny projection E is (trivially) diagonalizable. If {al, . . .  , ar} is a \nbasis for R and {ar+l, . . .  , an} a basis for N, then the basis CB = {aI, . . . , \nan} diagonalizes E :  \n[EJcs = [Ƕ 4J \nwhere I is the r X r identity matrix. That should help explain some of the \nterminology connected with projections. The reader should look at various \ncases in the plane R2 (or 3-space, R3), to convince himself that the projec­\ntion on R along N sends each vector into R by projecting it parallel to N. \nProjections can be used to describe direct-sum decompositions of the \nspace V. For, suppose V = Wl ED .\n. . ED Wk. For each j we shall define \nan operator Ej on V. Let a be in V, say a = al + . \" + ak with ai in Wi. \nDefine Eja \naj. Then Ej is a well-defined rule. It is easy to see that Ej is \nlinear, that the range of Ej is Wj, and that EJ = Ej• The null space of Ej \nis the subspace \n(WI + . . . + Wj-I + Wi+! + . . , + Wk) \nfor, the statement that Eja = 0 simply means aj = 0, i.e., that a is actually \n211 \n212 \nElementary Canonical Forms \nChap. 6 \na sum of vectors from the spaces Wi with i ;:£; i In terms of the projections \nEj we have \n(6-13) \na = Ela + \n. . .  + Eka \nfor each a in V. What (6-13) says is that \nI = El + . . . + Ek• \nNote also that if i ;:£; j, then E;Ej = 0, because the range of Ej is the \nsubspace Wj which is contained in the null space of Ei. We shall now \nsummarize our findings and state and prove a converse. \nTheorem 9. If V \nWi EB . . .  EB Wk, then there exist k linear opera-\ntors El, . . .  , F;k on V such that \n(i) each Ei is a projection (Er = Ei) ; \n(ii) EiEj = 0, if i ;:£; j ;  \n(iii) I \nEl + . .\n. + Ek; \n(iv) the range of Ei is Wi. \nConversely, if El, . . .  , Ek are k linear operators on V which satisfy conditions \n(i), (ii), and (iii), and if we let Wi be the range of Ei, then V = Wi EB . . .  EB \nWk.",
    "(ii) EiEj = 0, if i ;:£; j ;  \n(iii) I \nEl + . .\n. + Ek; \n(iv) the range of Ei is Wi. \nConversely, if El, . . .  , Ek are k linear operators on V which satisfy conditions \n(i), (ii), and (iii), and if we let Wi be the range of Ei, then V = Wi EB . . .  EB \nWk. \nProof. We have only to prove the converse statement. Suppose \nEl, • • •  , Ek are linear operators on V which satisfy the first three condi­\ntions, and let Wi be the range of Ei• Then certainly \nV = Wl + . . .  + Wk; \nfor, by condition (iii) we have \na = Ela + . . . + Eka \nfor each a in V, and Eia is in Wi. This expression for a is unique, because if \nwith ai in Wi, say ai = Ei{3i, then using (i) and (ii) we have \nk \nEja \n+ Ejai \n; = 1  \nk \n+ EjEj{3; \n; = 1  \n= EJ{3j \n= Ej{3j \nThis shows that V is the direct sum of the Wi. \nI \nSec. 6.7 \nInvariant Direct Sums \nExercises \n1. Let V be a finite-dimensional vector space and let WI be any subspace of V. \nProve that there is a subspace W2 of V such that V = WI E8 W2• \n2. Let V be a finite-dimensional vector space and let WI, . . .  , W k be subspaces \nof V such that \nV = WI + '\n\"\n + Wk and dim V = dim WI + \n. . .  + dim Wk. \nProve that V = WI E8 . . . E8 Wk. \n3. Find a projection E which projects R2 onto the subspace spanned by (1, - 1) \nalong the subspace spanned by (1, 2). \n4. If EI and E2 are projections onto independent subspaces, then EI + E2 is a \nprojection. True or false? \n5. If E is a projection and f is a polynomial, then fee) = aI + bE. What are \na and b in terms of the coefficients of f? \n6. True or false? If a diagonalizable operator has only the characteristic values \no and 1, it is a projection. \n7. Prove that if E is the projection on R along N, then (I - E) is the projection \non N along R. \n8. Let EI, . . .  , Ek be linear operators on the space V such that EI + . . . + Ek = I. \n(a) Prove that if EiEj = 0 for i r£ j, then Ef = Ei for each i. \n(b) In the case k = 2, prove the converse of (a). That is, if EI + E2 = I and",
    "on N along R. \n8. Let EI, . . .  , Ek be linear operators on the space V such that EI + . . . + Ek = I. \n(a) Prove that if EiEj = 0 for i r£ j, then Ef = Ei for each i. \n(b) In the case k = 2, prove the converse of (a). That is, if EI + E2 = I and \nE'f. = EI, E'5 = E2, then EIE2 = O. \n9. Let V be a real vector space and E an idempotent linear operator on V, i.e. , \na projection. Prove that (I + E) is invertible. Find (l + E)-I. \n10. Let F be a subfield of the complex numbers (or, a field of characteristic zero). \nLet V be a finite-dimensional vector space over F. Suppose that EI, \n•\n.\n.\n , Ek \nare projections of V and that EI + '\n\"\n + Ek = I. Prove that EiEj = 0 for i r£ j \n(Hint: Use the trace function and ask yourself what the trace of a projection is.) \n11. Let V be a vector space, let WI, . . .  , Wk be subspaces of V, and let \nVi = WI + \n. . . + Wi-l + Wj+1 + .\n.\n. + Wk. \nSuppose that V = WI E8 . . . E8 Wk. Prove that the dual space V* has the direct­\nsum decomposition y* = vy E8 . .\n. E8 n. \n213 \n6.7. Invariant Direct Sums \nWe are primarily interested in direct-sum decompositions V = \nWI EB . . .  EB Wk, where each of the subspaces Wi is invariant under some \ngiven linear operator T. Given such a decomposition of V, T induces a \nlinear operator T; on each Wi by restriction. The action of T is then this. \n214 \nElementary Canonical Forms \nChap. 6 \nIf a is a vector in V, we have unique vectors ai, . . .  , ak with ai in Wi such \nthat \nand then \nTa = Tlal + . . .  + Tkak. \nWe shall describe this situation by saying that T is the direct sum of the \noperators TI, •\n•\n•\n , Tk• It must be remembered in using this terminology \nthat the Ti are not linear operators 011 the space V but on the various \nsubspaces Wi. The fact that V = WI EB . . .  EB Wk enables us to associate \nwith each a in V a  unique k-tuple (ai, . . .  , ak) of vectors ai ill Wi (by a = \nal + . . . + ak) in such a way that we can carry out the linear operations",
    "subspaces Wi. The fact that V = WI EB . . .  EB Wk enables us to associate \nwith each a in V a  unique k-tuple (ai, . . .  , ak) of vectors ai ill Wi (by a = \nal + . . . + ak) in such a way that we can carry out the linear operations \nin V by working in the individual subspaces Wi. The fact, that each Wi is \ninvariant under T enables us to view the action of T as the independent \naction of the operators 1\\ on the subspaces Wi. Our purpose is to study T \nby finding invariant direct-sum decompositions in which the Ti are opera­\ntors of an elementary nature. \nBefore looking at an example, let us note the matrix analogue of this \nsituation. Suppose we select an ordered basis CBi for each Wi, and let CB \nbe the ordered basis for V consisting of the union of the CBi arranged in \nthe order CBI, . . .  , CBk, so that CB is a basis for V. From our discussion \nconcerning the matrix analogue for a single invariant subspace, it is easy \nto see that if A \n[TJCB and Ai = [TiJ<BiJ then A has the block form \n(6-14) \nA [:' ;' u \nIn (6-14), Ai is a di X d; matrix (d; = dim Wi), and the O's are symbols \nfor rectangular blocks of scalar O's of various sizes. It also seems appro­\npriate to describe (6-14) by saying that A is the direct sum of the matrices \nAI, . . .  , Ak• \nMost often, we shall describe the subspace Wi by means of the associ­\nated projections Ei (Theorem 9). Therefore, we need to be able to phrase \nthe invariance of the subspaces Wi in terms of the Ei. \nTheorem 10. Let T be a linear operator on the space V, and let \nWI, . . .  , Wk and EI, •\n•\n•\n , Ek be as in Theorem 9. Then a necessary and \nsufficient condition that each subspace Wi be invariant under T is that T \ncommute 'With each of the projections Ei, i.e., \nTEj = EiT, \ni = 1, . . . , k. \nProof. Suppose T commutes with each Ei• Let a be in Wj. Then \nTa = T(Eja) \n= Ej(Ta) \nSee. 6.7 \nInvariant Direct Sums \nwhich shows that TO'. is in the range of Ej, i.e., that Wj is invariant under T.",
    "commute 'With each of the projections Ei, i.e., \nTEj = EiT, \ni = 1, . . . , k. \nProof. Suppose T commutes with each Ei• Let a be in Wj. Then \nTa = T(Eja) \n= Ej(Ta) \nSee. 6.7 \nInvariant Direct Sums \nwhich shows that TO'. is in the range of Ej, i.e., that Wj is invariant under T. \nAssume now that each Wi is invariant under T. We shall show that \nTEj = EjT. Let 0'. be any vector in V. Then \n0'. = BlO'. + \n'\n\"\n + EkO'. \nTO'. = TElO'. + \n'\n\"\n + TEkO'.. \nSince EiO'. is in Wi, which is mvariant under T, we must have T(EiO'.) \nEll; for some vector f3i. Then \ni r= J' \nif i = j. \nThus \nE'jTO'. \nEjTElO'. + . . . + EjTEkO'. \n= Bjf3j \n= 1'EiO'.· \nThis holds for each 0'. in V, so EiT \n= TEj. \nI \nWe shall now describe a diagonalizable operator T ill the language of \ninvariant direct sum decompositions (projections which commute with T). \nThis will be a great help to us in understanding some deeper decomposition \ntheorems later. The reader may feel that the description which we are \nabout to give is rather complicated, in comparison to the matrix formula­\ntion or to the simple statement that the characteristic vectors of T span the \nunderlying space. But, he should bear in mind that this is our first glimpse \nat a very effective method, by means of which various problems concerned \nwith subspaces, bases, matrices, and the like can be reduced to algebraic \ncalculations with linear operators. With a little experience, the efficiency \nand elegance of this method of reasoning should become apparent. \nTheorem 11. Let T be a linear operator on a finite-dimensional space V. \nIf T is diagonalizable and if Cl, .\n•\n•\n , Ck are the distinct characteristic \nvalues of T, then there exist linear operators El, . . .  , Ek on V such that \n(i) T = clEl + . . . + i}kEk; \n(ii) I \n= El + . . . + Ek; \n(iii) EiEj \n0, i r= j ;  \n(iv) Er = Ei (Ei is a projection); \n(v) the range of Ei is the characteristic space for T associated with Ci. \nConversely, if there exist k distinct scalars Cl, •\n•\n.",
    "(i) T = clEl + . . . + i}kEk; \n(ii) I \n= El + . . . + Ek; \n(iii) EiEj \n0, i r= j ;  \n(iv) Er = Ei (Ei is a projection); \n(v) the range of Ei is the characteristic space for T associated with Ci. \nConversely, if there exist k distinct scalars Cl, •\n•\n.\n , Ck and k non-zero \nlinear operators El, . . .  , Ek which satisfy conditions (i), (ii), and (iii), then \nT is diagonalizable, Cl, .\n•\n•\n , Ck are the distinct characteristic values of T, and \nconditions (iv) and (v) are satisfied also. \nProof. Suppose that T is diagonalizable, with distinct charac-\n215 \n216 \nElementary Canonical Forms \nChap. 6 \nteristic values CI, . . . , Ck .  Let Wi be the space of characteristic vectors \nassociated with the characteristic value Ci. As we have seen, \nV = W 1 EB . . .  EB Wk. \nLet EI, • • •  , Ek be the projections associated with this decomposition, as \nin Theorem 9. Then (ii), (iii), (iv) and (v) are satisfied. To verify (i), \nproceed as follows. For each a in V, \na = Ela + . . .  + Eka \nand so \nTa = TEla + . . . + TEka \n= clEla + . . . + ckEka. \nIn other words, T = clEI + . . .  + ckEk• \nNow suppose that we are given a linear operator T along with distinct \nscalars Ci and non-zero operators Ei which satisfy (i), (ii) and (iii). Since \nE;Ei = 0 when i r\"- j, we multiply both sides of I = EI + . . .  + Ek by \nEi and obtain immediately E; = Ei. Multiplying T = clEl + . . .  + ckEk \nby Ei, we then have TEi = ciEi, which shows that any vector in the range \nof Ei is in the null space of (T - cd). Since we have assumed that Ei r\"- 0, \nthis proves that there is a non-zero vector in the null space of (T - cd), \ni.e., that C i  is a characteristic value of T. Furthermore, the C i  are all of the \ncharacteristic values of T; for, if c is any scalar, then \nT - cI = (Cl - C)EI + . . . + (Ck - C)Ek \nso if (T - cl)a = 0, we must have (Ci - C)Eia = 0. If a is not the zero \nvector, then Eia r\"- 0 for some i, so that for this i we have Ci - C = 0.",
    "characteristic values of T; for, if c is any scalar, then \nT - cI = (Cl - C)EI + . . . + (Ck - C)Ek \nso if (T - cl)a = 0, we must have (Ci - C)Eia = 0. If a is not the zero \nvector, then Eia r\"- 0 for some i, so that for this i we have Ci - C = 0. \nCertainly T is diagonalizable, since we have shown that every non­\nzero vector in the range of Ei is a characteristic vector of T, and the fact \nthat I = EI + . . .  + Ek shows that these characteristic vectors span V. \nAll that remains to be demonstrated is that the null space of (T - cd) is \nexactly the range of Ei. But this is clear, because if Ta = Cia, then \nhence \nand then \nk \n+ (Cj - ci)Eja = ° \nj = 1  \n(Cj - ci)EjOi = 0 \nfor each j \nEja = 0, \nj r\"- i. \nSince a = E1a + . . .  + Eka, and Eja = ° for j ჰ i, we have a = Eia, \nwhich proves that a is in the range of Ei. \nI \nOne part of Theorem 9 says that for a diagonalizable operator T, \nthe scalars CI, . . .  , Ck and the operators EI, • • . , Ek are uniquely deter­\nmined by conditions (i), (ii) , (iii), the fact that the Ci are distinct, and \nthe fact that the Ei are non-zero. One of the pleasant features of the \nSec. 6.7 \nInvariant Direct Sums \ndecomposition T = clEl + . . . + ckEk is that if 9 is any polynomial over \nthe field F, then \ngeT) \nq(cl)Et + ' \"  + g(ck)Ek• \nWe leave the details of the proof to the reader. To see how it is proved one \nneed only compute Tr for eaeh positive integer r. For example, \nk \nk \nT2 \n+ CiEi + CjEj \n; = 1  \nj = 1  \nk \nk \n= + + C;CjEiEj \n; = l j = 1  \nk \n= + c7Em \n; = 1  \nThe reader should eompare 1;his with g(A) where A is a diagonal matrix; \nfor then g(A) is simply the diagonal matrix with diagonal entries g(All), \n. . . , g(Ann). \nWe should like in particular to note what happens when one applies \nthe Lagrange polynomials corresponding to the scalars CI, • • •  , Ck :  \nPJ = II (x \nCi) \ntr'J (Cj - c,) \nWe have Pi(Ci) \n= aii> which means that \nk \npj(T) = + aijE; \n; = 1  \n= Ej•",
    ". . . , g(Ann). \nWe should like in particular to note what happens when one applies \nthe Lagrange polynomials corresponding to the scalars CI, • • •  , Ck :  \nPJ = II (x \nCi) \ntr'J (Cj - c,) \nWe have Pi(Ci) \n= aii> which means that \nk \npj(T) = + aijE; \n; = 1  \n= Ej• \nThus the projections Ej not only commute with T but are polynomials in \nT. \nSuch calculations with polynomials in T can be used to give an \nalternative proof of Theorem 6, which characterized diagonalizable opera­\ntors in terms of their minimal polynomials. The proof is entirely inde­\npendent of our earlier proof. \nIf T is diagonalizable, ']' = clEl + . . . + ckEk, then \ngeT) = g(cl)El + \n'\n\"\n + g(ck)Ek \nfor every polynomial g. Thus geT) = 0 if and only if g(Ci) = 0 for each i. \nIn particular, the minimal polynomial for T is \np = (x - Cl) .\n.\n.\n (x - Ck). \nNow suppose T is a linear operator with minimal polynomial p = \n(x \nCl) ' •\n.\n (x \nCk), where Cl, . . • , Ck are distinct elements of the scalar \nfield. We form the Lagrange polynomials \n217 \n218 \nElementary Canonical Forms \n(x - Ci) \nPi = II \n. \ni,.<j (Cj \nCi) \nOhap. 6 \nWe recall from Chapter 4 that Pi(C;) \nOij and for any polynomial 9 of \ndegree less than or equal to (k - 1) we have \n9 = g(Cl)Pl + ' \" + g(Ck)Pk. \nTaking 9 to be the scalar polynomial 1 and then the polynomial x, we have \n1 = PI + . . .  + Pk \nX = C1Pl + . . .  + CkPk. \n(6-15) \n(The astute reader will note that the application to x may not be valid \nbecause k may be 1. But if k \n1, T is a scalar multiple of the identity and \nhence diagonalizable.) Now let Ej = Pi(T). From (6-15) we have \n1 = El + . . . + Ek \nT = clEI + . . . + ckEk• \n(6-16) \nObserve that if i ǭ j, then PiPi is divisible by the minimal polynomial p, \nbecause PiPi contains every (x \ncr) as a factor. Thus \n(6-17) \nEiEj = 0, \ni ǭ j. \nWe must note one further thing, namely, that Ei ǭ 0 for each i. This \nis because p is the minimal polynomial for T and so we cannot have",
    "Observe that if i ǭ j, then PiPi is divisible by the minimal polynomial p, \nbecause PiPi contains every (x \ncr) as a factor. Thus \n(6-17) \nEiEj = 0, \ni ǭ j. \nWe must note one further thing, namely, that Ei ǭ 0 for each i. This \nis because p is the minimal polynomial for T and so we cannot have \nPi(T) = 0 since Pi has degree less than the degree of p. This last comment, \ntogether with (6-16), (6-17), and the fact that the Ci are distinct enables us \nto apply Theorem 1 1  to conclude that T is diagonalizable. \nI \nExercises \n1. Let E be a projection of V and let T be a linear operator on V. Prove that the \nrange of E is invariant under T if and only if ETE = TE. Prove that both the \nrange and null space of E are invariant under T if and only if ET = TE. \n2. Let T be the linear operator on R2, the matrix of which in the standard ordered \nbasis is \nLet WI be the subspace of R2 spanncd by the vector €! = (1, 0). \n(a) Prove that WI is invariant under T. \n(b) Prove that there is no subspace W2 which is invariant under T and which \nis complementary to WI: \nR2 \nWI EB W2• \n(Compare with Exercise 1 of Section 6 . •  '5.) \n3. Let T be a linear operator on a finite-dimensional vector space V. Let R be \nthe range of T and let N be the null space of T. Prove that R and N are inde­\npendent if and only if V \nR EB  N. \nSec. 6.8 \nThe Primary Decomposition Theorem \n4. Let T be a linear operator on V. Suppose V = WI ED . . .  ED Wk, where each \nWi is invariant under T. Let Ti be the induced (restriction) operator on Wi. \n(a) Prove that det (T) \nclet (TI) • • • det (Tk). \n(b) Prove that the characteristic polynomial for f is the product of the charac­\nteristic polynomials for /I, . . .  , !k-\n(c) Prove that the minimal polynomial for T is the least common multiple \nof the minimal polynomials for TI, •\n•\n•\n , Tk• (Hint: Prove and then use the cor­\nresponding facts about direct sums of matrices.) \n5. Let T be the diagonalizable linear operator on R3 which we discussed in",
    "(c) Prove that the minimal polynomial for T is the least common multiple \nof the minimal polynomials for TI, •\n•\n•\n , Tk• (Hint: Prove and then use the cor­\nresponding facts about direct sums of matrices.) \n5. Let T be the diagonalizable linear operator on R3 which we discussed in \nExample 3 of Section 6.2. Use 1;he Lagrange polynomials to write the representing \nmatrix A in the form A \n= EI + 2Ez, EI + E2 = I, EIE2 = O. \n6. Let A be the 4 X 4 matrix in Example 6 of Section 6.3. Find matrices E), E2, Ea \nsuch that A \nclEI + czEz + CaEa, EI + E2 + Ea \nI, and EiEi \n0, i ;'\" j. \n7. In Exercises 5 and 6, notice that (for each i) the space of characteristic vectors \nassociated with the characteristic value Ci is spanned by the column vectors of the \nvarious matrices E; with j ;'\" i. Is that a coincidence? \n8. Let T be a linear operator on V which commutes with every projection operator \non V. What ean you say about T? \n9. Let V be the vector space of continuous real-valued functions on the interval \n1, IJ of the real line. Let We be the subspace of even functions, f( -x) \nf(x), \nand let Wo be the subspace of odd functions, f( -x) = -f(x). \n(a) Show that V = W. ED Wo. \n(b) If T is the indefinite integral operator \n(Tf)(x) J: f(t) dt \nare We and Wo invariant under T? \n219 \n6.8. The Primary Decomposition Theorem \nWe are trying to study a linear operator T on the finite-dimensional \nspace V, by decomposing T into a direct sum of operators which are in \nsome sense elementary. We can do this through the characteristic values \nand vectors of T in certain 8pecial cases, i.e., when the minimal polynomial \nfor T factors over the scalar field F into a product of distinct monic poly­\nnomials of degree 1. What can we do with the general T? If we try to study \nT using characteristic values, we are confronted with two problems. First, \nT may not have a single characteristic value; this is really a deficiency in",
    "nomials of degree 1. What can we do with the general T? If we try to study \nT using characteristic values, we are confronted with two problems. First, \nT may not have a single characteristic value; this is really a deficiency in \nthe scalar field, namely, that it is not algebraically closed. Second, even if \nthe characteristic polynomial factors completely over F into a product of \npolynomials of degree 1, th€:re may not be enough characteristic vectors for \nT to span the space V; this is clearly a deficiency in T. The second situation \n220 \nElementary Canonical Forms \nChap. 6 \nis illustrated by the operator T on Fa (F any field) represented in the \nstandard basis by \nA = [ŉ Ŋ ŋJ' \no 0 - 1 \nThe characteristic polynomial for A is (x - 2)2(X + 1) and this is plainly \nalso the minimal polynomial for A (or for T). Thus T is not diagonalizable. \nOne sees that this happens because the null space of (T - 21) has dimen­\nsion 1 only. On the other hand, the null space of (T + I) and the null space \nof (T - 21) 2 together span V, the former being the subspace spanned by \n(a and the latter the subspace spanned by (1 and ،2. \nThis will be more or less our general method for the second problem. \nIf (remember this is an assumption) the minimal polynomial for T de-\ncomposes \np = (x - C1)Tl •\n•\n•\n (x - Ck)n \nwhere C1, •\n•\n•\n , Ck are distinct elements of F, then we shall show that the \nspace V is the direct sum of the null spaces of (T - cJ)'i, i = 1, . . .  , k. \nThe hypothesis about p is equivalent to the fact that T is triangulable \n(Theorem 5) ; however, that knowledge will not help us. \nThe theorem which we prove is more general than what we have \ndescribed, since it works with the primary decomposition of the minimal \npolynomial, whether or not the primes which enter are all of first degree. \nThe reader will find it helpful to think of the special case when the primes \nare of degree 1, and even more particularly, to think of the projection-type",
    "polynomial, whether or not the primes which enter are all of first degree. \nThe reader will find it helpful to think of the special case when the primes \nare of degree 1, and even more particularly, to think of the projection-type \nproof of Theorem 6, a special case of this theorem. \nTheorem 12 (Primary Decomposition Theorem). Let T be a linear \noperator on the finite-dimensional vector space V over the field F. Let p be the \nminimal polynomial for T, \np = pi' . . .  pჸk \nwhere the Pi are distinct irreducible monic polynomials over F and the ri are \npositive integers. Let Wi be the null space of p;(T)ri, i = 1, . . .  , k. Then \nCi) V = W1 EB . . \n· EB Wk; \n(ii) each Wi is invariant under T; \n(iii) if Ti is the operator induced on Wi by T, then the minimal poly­\nnomial for Ti is pP. \nProof. The idea of the proof is this. If the direct-sum decomposi­\ntion (i) is valid, how can we get hold of the projections E1, •\n•\n•\n , Ek associ­\nated with the decomposition? The projection Ei will be the identity on Wi \nand zero on the other Wj. We shall find a polynomial hi such that hiCT) is \nthe identity on Wi and is zero on the other WJ, and so that h1(T) + . . . + \nhk(T) = I, etc. \nSec. 6.8 \nThe Primary Decomposition Theorem \nFor each i, let \nfi \nSince PI, . . .  , Pk are distinct prime polynomials, the polynomials h, .\n. . , fk \nare relatively prime (Theorem 10, Chapter 4). Thus there are polynomials \ngl, . . .  , gk such that \nn \n+ figi \n1. \ni = 1  \nNote also that if i r!' j, then fJj is divisible by the polynomial p, because \nfdi contains each p\";; as a factor. We shall show that the polynomials \nhi = figi behave in the manner described in the first paragraph of the proof. \nLet Ei = hi(T) = fi(T)qi(T). Since hl + . . , + hk = 1 and p divides \nfdi for i r!' j, we have \nEl + . . . + Ek \nI \nEiE; =\" 0, \nif i r!' j. \nThus the Ei are projections which correspond to some direct-sum de­\ncomposition of the space V. We wish to show that the range of Ei is exactly",
    "Let Ei = hi(T) = fi(T)qi(T). Since hl + . . , + hk = 1 and p divides \nfdi for i r!' j, we have \nEl + . . . + Ek \nI \nEiE; =\" 0, \nif i r!' j. \nThus the Ei are projections which correspond to some direct-sum de­\ncomposition of the space V. We wish to show that the range of Ei is exactly \nthe subspace Wi. It is clear that each vector in the range of Ei is in Wi, for \nif a is in the range of E i, then a = E ia and so \nPi(T}\" a \nPi(T)\"Eia \n= Pi(T)rji(T)gi(T)a \n= 0 \nbecause P'jigi is divisible by the minimal polynomial p. Conversely, \nsuppose that a is in the null space of pi(T)\". If j r!' i, then fjgi is divisible \nby pi' and so fi(T)gj(T)a = 0, i .e. , Eja = 0 for j r!' i. But then it is im­\nmediate that Eia = a, i.e., t.hat a is in the range of Ei. This completes the \nproof of statement (i). \nIt is certainly clear that the subspaces Wi are invariant under T. \nIf Ti is the operator induced on Wi by T, then evidently Pi(Ti)Ti = 0, \nbecause by definition Pi(T)\"' is 0 on the subspace Wi. This shows that the \nminimal polynomial for Ti divides p৔i. Conversely, let g be any polynomial \nsuch that g(Ti) = O. Then g(T)fi('P) = O. Thus gf; is divisible by the \nminimal polynomial p of T i.e., P¯ji divides gfi. It is easily seen that Pt' \ndivides g. Hence the minimal polynomial for T; is pi'. I \nCorollary. If El, . . .  , Ek are the projections associated with the primary \ndecomposition of T, then each Ei is a polynomial in T, and accordingly if a \nlinear operator U commutes 'with T then U com/mutes with each of the Ei, i.e., \neach subspace Wi is invariant under U. \nIn the notation of the proof of Theorem 12, let us take a look at the \nspecial case in which the minimal polynomial for T is a product of first-\n221 \n222 \nElementary Canonical Forms \nChap. 6 \ndegree polynomials, i.e., the case in which each Pi is of the form \nPi = X - Ci. Now the range of Ei is the null space Wi of (T - cd)\" . \nLet us put D = clEI + . . . + ckEk• By Theorem 1 1, D is a diagonal­",
    "221 \n222 \nElementary Canonical Forms \nChap. 6 \ndegree polynomials, i.e., the case in which each Pi is of the form \nPi = X - Ci. Now the range of Ei is the null space Wi of (T - cd)\" . \nLet us put D = clEI + . . . + ckEk• By Theorem 1 1, D is a diagonal­\nizable operator which we shall call the diagonalizable part of T. Let us \nlook at the operator N = T - D. Now \nT = TEl + . . .  + TEk \nD = clEI + . . .  + ckEk \nso \nN = (T - cd)El + . . .  + (T - cd)Eko \nThe reader should be familiar enough with projections by now so that he \nsees that \nN2 = (T - Cd)2EI + . . .  + (T - ckI)2Ek \nand in general that \nNr = (T - cd)rEI + . . .  + (T - cd)rEk• \nWhen r 2 ri for each i, we shall have Nr = 0, because the operator \n(T - c;l)r will then be 0 on the range of Ei. \nDefinition. Let N be a linear operator on the vector space V. We say \nthat N is nilpotent if there is some positive integer r such that Nr = O. \nTheorem 13. Let T be a linear operator on the finite-dimensional vector \nspace V over the field F. Suppose that the minimal polynomial for T de­\ncomposes over F into a product of linear polynomials. Then there is a diago­\nnalizable operator D on V and a nilpotent operator N on V such that \n(i) T = D + N, \n(ii) DN = ND. \nThe diagonalizable operator D and the nilpotent operator N are uniquely \ndetermined by (i) and (ii) and each of them is a polynomial in T. \nProof. We have just observed that we can write T = D + N \nwhere D is diagonalizable and N is nilpotent, and where D and N not only \ncommute but are polynomials in T. Now suppose that we also have T = \nD' + N' where D' is diagonalizable, N' is nilpotent, and D'N' = N'D'. \nWe shall prove that D = D' and N = N'. \nSince D' and N' commute with one another and T = D' + N', we \nsee that D' and N' commute with T. Thus D' and N' commute with any \npolynomial in T; hence they commute with D and with N. Now we have \nD + N  = D' + N' \nor \nD - D' = N' - N \nand all four of these operators commute with one another. Since D and D'",
    "see that D' and N' commute with T. Thus D' and N' commute with any \npolynomial in T; hence they commute with D and with N. Now we have \nD + N  = D' + N' \nor \nD - D' = N' - N \nand all four of these operators commute with one another. Since D and D' \nare both diagonalizable and they commute, they are simultaneously \nSec. 6.8 \nThe Primary Decomposition Theorem \ndiagonalizable, and D \nDi is diagonalizable. Since N and N' are both \nnilpotent and they commute, the operator (N' - N) is nilpotent; for, \nusing the fact that N and 1'/' commute \n(N' \n± (r.) (N')r-i( _ N)i \nj=O \nJ \nand so when r is sufficiently large every term in this expression for \n(N' - N)r will be O. (Actm,lly, a nilpotent operator on an n-dimensional \nspace must have its nth power 0; if we take r = 2n above, that will be \nlarge enough. It then follows that r \n= n is large enough, but this is not \nobvious from the above expression.) Now D \nD' is a diagonalizable \noperator which is also nilpotent. Such an operator is obviously the zero \noperator; for since it is nilpotent, the minimal polynomial for this operator \nis of the form xr for some r ::; m; but then since the operator is diagonaliza­\nble, the minimal polynomial cannot have a repeated root; hence r = 1 and \nthe minimal polynomial is s:,mply x, which says the operator is O. Thus we \nsee that D = D' and N = N'. I \nCorollary. Let V be a finite-dimensional vector space over an algebra­\nically closed field F, e.g., the field of complex numbers. Then every linear \noperator T on V can be written as the sum of a diagonalizable operator D \nand a nilpotent operator N which commute. These operators D and N are \nunique and each is a polynomial in T. \nFrom these results, one sees that the study of linear operators on \nvector spaces over an algebraically closed field is essentially reduced to \nthe study of nilpotent operators. For vector spaces over non-algebraically \nclosed fields, we still need to find some substitute for characteristic values",
    "From these results, one sees that the study of linear operators on \nvector spaces over an algebraically closed field is essentially reduced to \nthe study of nilpotent operators. For vector spaces over non-algebraically \nclosed fields, we still need to find some substitute for characteristic values \nand vectors. It is a very interesting fact that these two problems can be \nhandled simultaneously and this is what we shall do in the next chapter. \nIn concluding this section, we should like to give an example which \nillustrates some of the ideas of the primary decomposition theorem. We \nhave chosen to give it at the end of the section since it deals with differential \nequations and thus is not purely linear algebra. \nEXAMPLE 14. In the primary decomposition theorem, it is not neces­\nsary that the vector space V be finite dimensional, nor is it necessary for \nparts (i) and (ii) that p be the minimal polynomial for T. If T is a linear \noperator on an arbitrary vector space and if there is a monic polynomial \np such that peT) \n0, then parts (i) and (ii) of Theorem 12 are valid for T \nwith the proof which we gave. \nLet n be a positive integer and let V be the space of all n times con­\ntinuously differentiable fUllctions f on the real line which satisfy the \ndifferential equation \n223 \n224 \nElementary Canonical Forms \nChap. 6 \n(6-18) \nwhere ao, . . .  , an-l are some fixed constants. If en denotes the space of \nn times continuously differentiable functions, then the space V of solutions \nof this differential equation is a subspace of en. If D denotes the differentia­\ntion operator and p is the polynomial \np = x\" + a,,_lx,,-l + . . .  + alX + ao \nthen V is the null space of the operator p(D), because (6-18) simply says \np(D)f \nO. Therefore, V is invariant under D. Let us now regard D as a \nlinear operator on the subspace V. Then p(D) = O. \nIf we are discussing differentiable complex-valued functions, then en",
    "then V is the null space of the operator p(D), because (6-18) simply says \np(D)f \nO. Therefore, V is invariant under D. Let us now regard D as a \nlinear operator on the subspace V. Then p(D) = O. \nIf we are discussing differentiable complex-valued functions, then en \nand V are complex vector spaces, and ao, . . . , an-l may be any complex \nnumbers. We now write \np = (x - CIY' . . .  (x - cd' \nwhere CI, .\n•\n.\n , Ck are distinct complex numbers. If Wj is the null space of \n(D - cjl)rl, then Theorem 12 says that \nV = Wl EB . . .  EB Wk. \nIn other words, if f satisfies the differential equation (6-18), then f is \nuniquely expressible in the form \nf \nfl + \n. . .  \nfk \nwhere fi satisfies the differential equation (D \ncjl)r;f) \nO. Thus, the \nstudy of the solutions to the equation (6-18) is reduced to the study of \nthe space of solutions of a differential equation of the form \n(6-19) \n(D - cl)rf = O. \nThis reduction has been accomplished by the general methods of linear \nalgebra, i.e., by the primary decomposition theorem. \nTo describe the space of solutions to (6-19), one must know something \nabout differential equations, that is, one must know something about D \nother than the fact that it is a linear operator. However, one does not need \nto know very much. It is very easy to establish by indootion on r that if f \nis in er then \nthat is, \ndf \nd \ndt - cf(t) = eet 1ft (e-ctj), etc. \nThus (D - cI)rf = 0 if and only if Dr(e-ctj) \nO. A function g such that \nDrg = 0, i.e., drg/dtr = 0, must be a polynomial function of degree (r - 1) \nor less: \nSec. 6.8 \nThe Primary Decomposition Theorem \nThus f satisfies (6-19) if and only if f has the form \nfCt) \n= ect(bo + bit + . . .  + br_1tr-l). \nAccordingly, the 'functions' eet, teet, . . .  , tr-Iee! span the space of solutions \nof (6-19). Since 1, t, . . .  , tr-·j are linearly independent functions and the. \nexponential function has no zeros, these r functions tieet, 0 $ j $ r \n1, \nform a basis for the space of solutions.",
    "Accordingly, the 'functions' eet, teet, . . .  , tr-Iee! span the space of solutions \nof (6-19). Since 1, t, . . .  , tr-·j are linearly independent functions and the. \nexponential function has no zeros, these r functions tieet, 0 $ j $ r \n1, \nform a basis for the space of solutions. \nReturning to the differential equation (6-18), which is \np(D)f = 0 \np \n(x \nCl)rt . \" \n(x - Ck)rk \nwe see that the n functions tmccit, 0 $ m $ ri \n1, 1 $ j $ k, form a \nbasis for the space of solutions to (6-18) . In particular, the space of solutions \nis finite-dimensional and hal3 dimension equal to the degree of the poly­\nnomial p. \nExercises \n1. Let T be a linear operator on R3 which is represented in the standard ordered \nbasis by the matrix \n[\n6 \n-3 \n-2J \n4 \n- 1  -2 · \n10 \n- 5  -3 \nExpress the minimal polynomial p for T in the form P = PIPZ, where PI and pz \nare monic and irreducible over the field of real numbers. Let Wi be the null space \nof Pi(T). Find bases <Bi for the spaces WI and Wz• If Ti is the operator induced on \nWi by T, find the matrix of Ti in the basis <Bi (above). \n2. Let T be the linear operator on R3 which is represented by the matrix \n[; ; =ZJ \n2 2 \n0 \nin the standard ordered basis. Show that there is a diagonalizable operator D \non R3 and a nilpotent operator N on R3 such that T = D + N and DN = ND. \nFind the matrices of D and lIT in the standard basis. (Just repeat the proof of \nTheorem 12 for this special case.) \n3. If V is the space of all polynomials of degree less than or equal to n over a \nfield F, prove that the differentiation operator on V is nilpotent. \n4. Let T be a linear operator on the finite-dimensional space V with characteristic \npolynomial \nand minimal polynomial \nP = (x - Cl)\" . . . (x - Ck)r •• \nLet Wi be the null space of (T - cd)r\" \n225 \n226 \nElementary Canonical Forms \nChap. 6 \n(a) Prove that Wi is the set of all vectors a in V such that (T \nc;l)ma \n0 \nfor some positive integer m (which may depend upon a).",
    "polynomial \nand minimal polynomial \nP = (x - Cl)\" . . . (x - Ck)r •• \nLet Wi be the null space of (T - cd)r\" \n225 \n226 \nElementary Canonical Forms \nChap. 6 \n(a) Prove that Wi is the set of all vectors a in V such that (T \nc;l)ma \n0 \nfor some positive integer m (which may depend upon a). \n(b) Prove that the dimension of Wi is di• (Hint: If Ti is the operator induced \non Wi by T, then Ti - c;l is nilpotent; thus the characteristic polynomial for \nTi - cJ must be xe; where ei is the dimension of Wi (proof?) ; thus the charac­\nteristic polynomial of Ti is (x - Ci)e;; now use the fact that the characteristic \npolynomial for T is the product of the characteristic polynomials of the Ti to show \nthat ei = di.) \n5. Let V be a finite-dimensional vector space over the field of complex numbers. \nLet T be a linear operator on V and let D be the diagonalizable part of T. Prove \nthat if g is any polynomial with complex coefficients, then the diagonalizable part \nof geT) is g(D). \n6. Let V be a finite-dimensional vector space over the field F, and let T be a \nlinear operator on V such that rank (T) \n1. Prove that either T is diagonalizable \nor T is nilpotent, not both. \n7. Let V be a finite-dimensional vector space over P, and let T be a linear operator \non Y. Suppose that T commutes with every diagonalizable linear operator on Y. \nProve that T is a scalar multiple of the identity operator. \n8. Let Y be the space of n X n matrices over a field F, and let A be a fixed n X n \nmatrix over P. Define a linear operator T on Y by T(B) \nAB - BA. Prove \nthat if A is a nilpotent matrix, then T is a nilpotent operator. \n9. Give an example of two 4 X 4 nilpotent matrices which have the same minimal \npolynomial (they necessarily have the same characteristic polynomial) but which \nare not similar. \n10. Let T be a linear operator on the finite-dimensional space Y, let p = P'i' . . . p,/ \nbe the minimal polynomial for T, and let Y = Wi EB . . . EB Wk be the primary",
    "polynomial (they necessarily have the same characteristic polynomial) but which \nare not similar. \n10. Let T be a linear operator on the finite-dimensional space Y, let p = P'i' . . . p,/ \nbe the minimal polynomial for T, and let Y = Wi EB . . . EB Wk be the primary \ndecomposition for T, i.e., Wi is the null space of Pi(T)rj• Let W be any subspace \nof Y which is invariant under T. Prove that \nW = (W () Wi) EB (W () W2) EB . . . EB (W () Wk). \nn. What's wrong with the following proof of Theorem 13? Suppose that the \nminimal polynomial for T is a product of linear factors. Then, by Theorem 5, \nT is triangulable. Let CB be an ordered basis such that A = [TJCl is upper-triangular. \nLet D be the diagonal matrix with diagonal entries an, . . .  , an\", Then A \nD + N, \nwhere N is strictly upper-triangular. Evidently N is nilpotent. \n12. If you thought about Exercise 11, think about it again, after you observe \nwhat Theorem 7 tells you about the diagonalizable and nilpotent parts of T. \n13. Let T be a linear operator on V with minimal polynomial of the form p\", \nwhere P is irreducible over the scalar field. Show that there is a vector a in Y \nsuch that the T-annihilator of a is p\". \n14. Use the primary decomposition theorem and the result of Exercise 13 to prove \nthe following. If T is any linear operator on a finite-dimensional vector space Y, \nthen there is a vector a in V with T-annihilator equal to the minimal polynomial \nfor T. \n15. If N is a nilpotent linear operator on an n-dimensional vector space Y, then \nthe characteristic polynomial for N is xn. \n7. The Rational \nand Jordan Forms \n7.1 . Cyclic Subspaces and Annihilators \nOnce again V is a finite-dimensional vector space over the field F \nand T is a fixed (but arbitrary) linear operator on V. If a is any vector \nin V, there is a smallest sUibspace of V which is invariant under T and \ncontains a. This subspace can be defined as the intersection of all T­",
    "Once again V is a finite-dimensional vector space over the field F \nand T is a fixed (but arbitrary) linear operator on V. If a is any vector \nin V, there is a smallest sUibspace of V which is invariant under T and \ncontains a. This subspace can be defined as the intersection of all T­\ninvariant subspaces which contain a; however, it is more profitable at the \nmoment for us to look at things this way. If W is any subspace of V which \nis invariant under T and contains a, then W must also contain the vector \nTa; hence W must contain T(Ta) \nT2a, T(T2a) \nT3a, etc. In other \nwords W must contain g(T).ჹ for every polynomial g over F. The set of all \nvectors of the form g(T)a, with g in F[x], is clearly invariant under T, and \nis thus the smallest T-invariant subspace which contains a. \nDefinition. If a 1:S any vector in V, the T-cyclic subspace generated \nby a is the subspace Z(a ; T) of all vectors of the form g(T)a, g in F[x]. \nIf Z(a; T) = V, then a is called a cyclic vector for T. \nAnother way of describing the subspace Z(a; T) is that Z(a; T) is \nthe subspace spanned by the vectors Tka, k 2: 0, and thus a is a cyclic \nvector for T if and only if these vectors span V. We caution the reader \nthat the general operator T has no cyclic vectors. \nEXAMPLE 1. For any T, the T-cyclic subspace generated by the zero \nvector is the zero subspace. The space Z(a; T) is one-dimensional if and \nonly if a is a characteristic vector for T. For the identity operator, every \n227 \n228 \nThe Rational and Jordan Forms \nChap. 7 \nnon-zero vector generates a one-dimensional cyclic subspace; thus, if \ndim V > 1, the identity operator has no cyclic vector. An example of an \noperator which has a cyclic vector is the linear operator T on F2 which is \nrepresented in the standard ordered basis by the matrix \nHere the cyclic vector (a cyclic vector) is El; for, if 6 = (a, b), then with \ng = a + bx we have (3 = g(T)€I. For this same operator T, the cyclic",
    "operator which has a cyclic vector is the linear operator T on F2 which is \nrepresented in the standard ordered basis by the matrix \nHere the cyclic vector (a cyclic vector) is El; for, if 6 = (a, b), then with \ng = a + bx we have (3 = g(T)€I. For this same operator T, the cyclic \nsubspace generated by €2 is the one-dimensional space spanned by €2, \nbecause EZ is a characteristic vector of T. \nFor any T and a, we shall be interested in linear relations \ncoa + clTa + . . . + ckTka = 0 \nbetween the vectors Tia, that is, we shall be interested in the polynomials \ng = Co + C1X + . .  , + CkXk which have the property that g(T)a \nO. The \nset of all g in F[x] such that g(T)a \n0 is clearly an ideal in F[x]. It is also \na non-zero ideal, because it contains the minimal polynomial p of the \noperator T (p(T)a = 0 for every a in V). \nDefinition. If a is any vector in V, the T-annihilator of a is the ideal \nM(a; T) in F[x] consisting of all polynomials g over F such that g(T)a = O. \nThe unique monic polynomial pa which generates this ideal will also be \ncalled the T-annihilator of a. \nAs we pointed out above, the T-annihilator pc< divides the minimal \npolynomial of the operator T. The reader should also note that deg (Pa) > 0 \nunless a is the zero vector. \nTheorem 1. Let a be any non-zero vector in V and let p\" be the \nT-annihilator of a. \n(i) The degree of Pc< is equal to the dimension of the cyclic subspace \nZ(a; T). \n(ii) If the degree of pa is k, then the vectors a, Ta, Pa, . . . , Tk-la \nform a basis for Z(a; T). \n(iii) If U is the linear operator on Z(a; T) induced by T, then the minimal \npolynomial for U is p\". \nProof. Let g be any polynomial over the field F. Write \ng = pc<q + r \nwhere either r = 0 or deg (r) < deg (Pa) \n= k. The polynomial pc<q is in \nthe T-annihilator of a, and so \ng(T)a \nr(T)a. \nSince r = 0 or deg (r) < k, the vector r(T)a is a linear combination of \nthe vectors a, Ta, . . .  , Tk-la, and since g(T)a is a typical vector in \nSec. 7.1",
    "g = pc<q + r \nwhere either r = 0 or deg (r) < deg (Pa) \n= k. The polynomial pc<q is in \nthe T-annihilator of a, and so \ng(T)a \nr(T)a. \nSince r = 0 or deg (r) < k, the vector r(T)a is a linear combination of \nthe vectors a, Ta, . . .  , Tk-la, and since g(T)a is a typical vector in \nSec. 7.1 \nCyclic Subapacca and Annihilatora \nZ(a; T), this shows that these k vectors span Z(a; T). These vectors are \ncertainly linearly independent, because any non-trivial linear relation \nbetween them would give us a non-zero polynomial g such that g(T)a = 0 \nand deg (g) < deg (Pa), which is absurd. This proves (i) and (ii). \nLet U be the linear operator on Z(a; T) obtained by restricting T to \nthat subspace. If g is any polynomial over F, then \np,,(U)g(T)a = Pa(T)g(T)a \n= g(T)p,,(T)a \n= g(T)O \n= o. \nThus the operator p,,(U) sends every vector in Z(a; T) into 0 and is the \nzero operator on Z(a; T). Furthermore, if h is a polynomial of degree \nless than k, we cannot have h(U) \n= 0, for then h(U)a = h(T)a = 0, \ncontradicting the definition of p\". This shows that p\" is the minimal \npolynomial for U. I \nA particular consequenee of this theorem is the following: If a happens \nto be a cyclic vector for T, then the minimal polynomial for T must have \ndegree equal to the dimension of the space V; hence, the Cayley-Hamilton \ntheorem tells us that the minimal polynomial for T is the characteristic \npolynomial for T. We shall prove later that for any T there is a vector a in \nV whieh has the minimal polynomial of T for its annihilator. It will then \nfollow that T has a cyclic vector if and only if the minimal and charac­\nteristic polynomials for T are identical. But it will take a little work for us \nto see this. \nOur plan is to study the general T by using operators which have a \neyclic vee tor. So, let us take a look at a linear operator U on a space W \nof dimension k which has a eyelic vector a. By Theorem 1, the vectors",
    "to see this. \nOur plan is to study the general T by using operators which have a \neyclic vee tor. So, let us take a look at a linear operator U on a space W \nof dimension k which has a eyelic vector a. By Theorem 1, the vectors \na, . . .  , Uk-la form a basis for the spaee W, and the annihilator p\" of a \nis the minimal polynomial for U (and hence also the characteristic poly­\nnomial for U). If we let ai = Ui-la, i = 1, . . . , k, then the action of U \non the ordered basis CB = {aI, . . .  , ak} is \n(7-1) \nUai = ai+l, \ni = 1, . . . , k - 1 \nU ak = -COal - Cla2 - . , . - Ck-Iak \nwhere Pc< = Co + CIX + . . . + Ck_lXk-1 + Xk. The expression for Uak \nfollows from the fact that p,,(U)a = 0, i.e., \nUka + Ck_1 Uk-la + . , . + elUa + coa = O. \nThis says that the matrix of U in the ordered basis CB is \n(7-2) \n[! H ň =: ]. \no 0 0 \n1 \n-Ck-l \n229 \n230 \nThe Rational and Jordan Forms \nChap. 7 \nThe matrix (7-2) is called the companion matrix of the monic poly­\nnomial Pa. \nTheorem 2. If U is a linear operator on the finite-dimensional space \nW, then U has a cyclic vector if and only if there is some ordered basis for W \nin which U is represented by the companion matrix of the minimal polynomial \nfor U. \nProof. We have just observed that if U has a cyclic vector, then \nthere is such an ordered basis for W. Conversely, if we have some ordered \nbasis {al, . . .  , ak} for W in which U is reprcsented by the companion \nmatrix of its minimal polynomial, it is obvious that al is a cyclic vector \nfor U. I \nCorollary. If A is the companion matrix of a monic polynomial p, \nthen p is both the minimal and the characteristic polynomial of A. \nProof. One way to see this is to let U be the linear operator on \nFk which is represented by A in the standard ordered basis, and to apply \nTheorem ) together with the Cayley-Hamilton theorem. Another method \nis to use Theorem 1 to see that p is the minimal polynomial for A and to \nverify by a direct calculation that p is the characteristic polynomial for",
    "Fk which is represented by A in the standard ordered basis, and to apply \nTheorem ) together with the Cayley-Hamilton theorem. Another method \nis to use Theorem 1 to see that p is the minimal polynomial for A and to \nverify by a direct calculation that p is the characteristic polynomial for \nA. \nI \nOne last comment -if T is any linear operator on the space V and \na is any vector in V, then the operator U which T induces on the cyclic \nsubspace Z(a; T) has a cyclic vector, namely, a. Thus Z(a; T) has an \nordered basis in which U is represented by the companion matrix of Pa, \nthe T-annihilator of a. \nExercises \n1. Let T be a linear operator on F2. Prove that any non-zero vector which is not \na characteristic vector for T is a cyclic vector for T. Hence, prove that either T \nhas a cyclic vector or T is a scalar multiple of the identity operator. \n2. Let T be the linear operator on R3 which is represented in the standard ordered \nbasis by the matrix \n[Ř ǵ MJ' \no 0 \n- 1  \nProve that T has n o  cyclic vector. What is the T-cyclic subspace generated by the \nvector (1, \n1, 3)? \n3. Let T be the linear operator on C3 which is represented in the standard ordered \nbasis by the matrix \nSec. 7.2 \nCyclic Decomposition8 and the Rational Form \nFind the T-annihilator of the vector (1, 0, 0). Find the T-annihilator of (1, 0, i). \n4. Prove that if T2 has a cyclic vector, then T has a cyclic vector. Is the converse \ntrue? \n5. Let V be an n-dimensional vector space over the field F, and let N bc a nilpotent \nlinear operator on V. Suppose Nn-l \"'\" 0, and let a be any vector in V such that \nNn-Ia \"'\" O. Prove that a is a cyclic vector for N. What cxactly is the matrix of N \nin the ordered basis {a, Na, . . .  , Nn-Ia} ? \n6. Give a direct proof that if A is the companion matrix of the monic polynomial \np, then p is the characteristic polynomial for A. \n7. Let V be an n-dimensional vector space, and let T be a linear operator on V. \nSuppose that T is diagonalizable.",
    "in the ordered basis {a, Na, . . .  , Nn-Ia} ? \n6. Give a direct proof that if A is the companion matrix of the monic polynomial \np, then p is the characteristic polynomial for A. \n7. Let V be an n-dimensional vector space, and let T be a linear operator on V. \nSuppose that T is diagonalizable. \n(a) If T has a cyclic vector, show that T has n distinct characteristic values. \n(b) If T has n distinct characteristic values, and if {aI, . . .  , an} is a basis of \ncharacteristic vectors for T, show that a = al + . .  , + an is a cyclic vector for T. \n8. Let T be a linear operator on the finite-dimensional vector space V' Suppose T \nhas a cyclic vector. Prove that if U is any linear operator which commutes with T, \nthen U is a polynomial in T. \n231 \n7.2. Cyclic Decompositions and \nthe Rational Form \nThe primary purpose of this section is to prove that if T is any linear \noperator on a finite-dimensional space V, then there exist vectors aI, . . . , ar \nin V such that \nIn other words, we wish to prove that V is a direct sum of T-cyclic sub­\nspaces. This will show that T is the direct sum of a finite number of linear \noperators, each of which has a cyclic vector. The effect of this will be to \nreduce many questions about the general linear operator to similar ques­\ntions about an operator which has a cyclic vector. The theorem which we \nprove (Theorem 3) is one of the deepest results in linear algebra and has \nmany interesting corollaries. \nThe cyclic decomposition theorem is closely related to the following \nquestion. Which T-invariant subspaces W have the property that there \nexists a T-invariant subspace W' such that V \nW ED W\"? If W is any \nsubspace of a finite-dimensional space V, then there exists a subspace W' \nsuch that V = W ED W'. Usually there are many such subspaces W' and \neach of these is called complementary to W. We are asking when a T­\ninvariant subspace has a complementary subspace which is also invariant \nunder T.",
    "subspace of a finite-dimensional space V, then there exists a subspace W' \nsuch that V = W ED W'. Usually there are many such subspaces W' and \neach of these is called complementary to W. We are asking when a T­\ninvariant subspace has a complementary subspace which is also invariant \nunder T. \nLet us suppose that V = W ED W' where both W and W' are invariant \nunder T and then see what we can discover about the subspace W. Each \n232 \nThe Rational and Jordan Forms \nChap. 7 \nvector (3 in V is of the form (3 = 'Y + 'Y' where 'Y is in W and 'Y' is in W'. \nIf f is any polynomial over the scalar field, then \nf(T)(3 = f(Th + f(Th'· \nSince W and W' are invariant under T, the vector f(Th is in W andf(Th' \nis in W'. Therefore f(T)(3 is in W if and only if f(Th' \nO. What interests \nus is the seemingly innocent fact that, iff(T)(3 is in W, thenf(T)(3 = f(Th. \nDefinition. Let T be a linear operator on a vector space V and let W \nbe a subspace of V . We say that W is T -adJUissible if \n(i) W is invariant under T; \n(ii) if f(T)(3 is in W, there exists a vector 'Y in W such that f(T)(3 = f(Th. \nAs we just showed, if W is invariant and has a complementary in­\nvariant subspace, then W is admissible. One of the consequences of Theo­\nrem 3 will be the converse, so that admissibility characterizes those \ninvariant subspaces which have complementary invariant subspaces. \nLet us indicate how the admissibility property is involved in the \nattempt to obtain a decomposition \nV = Zeal; T) EB . . .  EB Z(ar; T). \nOur basic method for arriving at such a decomposition will be to inductively \nselect the vectors al, . . .  , a,. Suppose that by some process or another we \nhave selected al, . . . , aj and the subspace \nWj = Zeal; T) + .\n. . + Z(aj; T) \nis proper. We would like to find a non-zero vector aj+l such that \nWj n Z(ai+1; T) = {O} \nbecause the subspace Wj+l \nWj EB Z(aj+l; T) would then come at least \none dimension nearer to exhausting V. But, why should any such aj+l",
    "Wj = Zeal; T) + .\n. . + Z(aj; T) \nis proper. We would like to find a non-zero vector aj+l such that \nWj n Z(ai+1; T) = {O} \nbecause the subspace Wj+l \nWj EB Z(aj+l; T) would then come at least \none dimension nearer to exhausting V. But, why should any such aj+l \nexist? If al, . . . , aj have been chosen so that Wj is a T-admissible subspace, \nthen it is rather easy to see that we can find a suitable aj+l. This is what \nwill make our proof of Theorem 3 work, even if that is not how we phrase \nthe argument. \nLet W be a proper T-invariant subspace. Let us try to find a non-zero \nvector a such that \n(7-3) \nW n Z(a; T) = {O}. \nWe can choose some vector (3 which is not in W. Consider the T-conductor \n8((3; W), which consists of all polynomials g such that g(T)(3 is in W. Recall \nthat the monic polynomial f = s((3; W) which generates the ideal 8((3; W) \nis also called the T-conductor of (3 into W. The vector f(T)(3 is in W. Now, if \nW is T-admissible, there is a 'Y in W with f(T)(1 = f(Th. Let a = (3 - 'Y \nand let g be any polynomial. Since (3 - a is in W, g(T)f3 will be in W if and \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nonly if g(T)a IS \nIII W; in other words, Sea; W) = S({3 ;  W). Thus the \npolynomial f is also the T-conductor of a into W. But f(T)a = O. That \ntells us that g(T)a is in W if and only if g(T)a = 0, i.e., the subspaces \nZ(a; T) and W are independent (7-3) and f is the T-annihilator of a. \nTheorem 3 (Cyclic Decomposition Theorem). Let T be a linear \noperator on a finite-dimensional vector space V and let Wo be a proper T­\nadmissible subspace of V. There exist non-zero vectors aI, . . .  , ar in V with \nrespective T-annihilators PI, . . .  , pr such that \n(i) V = Wo EB Zeal; T) EB . . .  EB Z(ar; T) ; \n(ii) Pk divides Pk-l, k \n= 2, . . . , r. \nFurthermore, the integer r and the annihilators PI, . . .  , pr are uniquely \ndetermined by (i), (ii), and the fact that no ak is O.",
    "respective T-annihilators PI, . . .  , pr such that \n(i) V = Wo EB Zeal; T) EB . . .  EB Z(ar; T) ; \n(ii) Pk divides Pk-l, k \n= 2, . . . , r. \nFurthermore, the integer r and the annihilators PI, . . .  , pr are uniquely \ndetermined by (i), (ii), and the fact that no ak is O. \nProof. The proof is rather long; hence, we shall divide it into four \nsteps. For the first reading it may seem easier to take Wo = {O} , although \nit does not produce any substantial simplification. Throughout the proof, \nwe shall abbreviate f(T)(3 to f(3. \nStep 1 .  There exist non-zero vectors {31, . . . , (3r in V such that \n(a) V = Wo + ZeSI; T) + . , . + zeSr; T) ; \n(b) if 1 ::; k ::; r and \nWk = Wo + Z({3I; T) + . . . + Z«(3k; T) \nthen the conductor Pk \n= S«(3k; W k-l) has maximum degree among all T­\nconductors into the subspace Wk-l, i.e., for every k \ndeg Pk = max deg sea ; Wk-1) . \na in V \nThis step depends only upon the fact that Wo is an invariant subspace. \nIf W is a proper T-invariant subspace, then \no < max deg sea; W) ::; dim V \n'\" \nand we can choose a vector (3 so that deg s«(3; W) attains that maximum. \nThe subspace W + Z«(3; T) is then T-invariant and has dimension larger \nthan dim W. Apply this process to W = Wo to obtain (31. If WI = Wo + \nZ«(31; T) is still proper, then apply the process to WI to obtain (32. Continue \nin that manner. Since dim Wk > dim wk_l, we must reach Wr = V in not \nmore than dim V steps. \nStep 2. Let (31, . . .  , (3r be non-zero vectors which satisfy conditions \n(a) and (b) of Step 1 .  Fix k, 1 ::; k ::; r. Let (3 be any vector in V and let \nf = s({3; Wk_1). If \nf{3 = !3o + \n+ \ngi!3;, \n1 9 <k \n!3i in Wi \nthen f divides each polynomial gi and {3o = ho, where 'Yo is in Woo \n233 \n234 \nThe Rational and Jordan Porms \nChap. 7 \nIf k = 1, this is just the statement that Wo is T-admissible. In order \nto prove the assertion for k > 1, apply the division algorithm: \n(7-4) \ngi \njhi + Ti, \nTi = 0 or deg Ti < degj. \nWe wish to show that ri \n0 for each i. Let",
    "233 \n234 \nThe Rational and Jordan Porms \nChap. 7 \nIf k = 1, this is just the statement that Wo is T-admissible. In order \nto prove the assertion for k > 1, apply the division algorithm: \n(7-4) \ngi \njhi + Ti, \nTi = 0 or deg Ti < degj. \nWe wish to show that ri \n0 for each i. Let \n(7-5) \nk-l \n'Y = /3 -  h;/3;. \n1 \nSince 'Y - /3 is in Wk-J, \n8('Y; Wk-1) = 8(/3; Wk-l) = f· \nFurthermore \n(7-6) \nSuppose that some Ti is different from O. We shall deduce a contradiction. \nLet j be the largest index i for which Ti 7\"= O. Then \n(7-7) \nj \nh = /30 +  ri/3;, \n1 \nrj 7\"= 0 and deg Tj < degf. \nLet p = s( 'Y ;  Wi-I). Since Wk-I contains Wj-I, the conductor j = s('Y; Wk-1) \nmust divide p :  \np \nfg· \nApply geT) to both sides of (7-7) : \n(7-8) \nP'Y = gh \ngrj/3j + g/3o + \n gri/3i. \n1 :O;i <i \nBy definition, P'Y is in WH, and the last two terms on the right side of (7-8) \nare in WH. Therefore, grj/3j is in Wi_I. Now we use condition (b) of Step 1 :  \ndeg (gTj) 2 deg s(/3;; Wj-l) \ndeg pi \n2 deg s('Y ; Wi-I) \n= deg p \ndeg (fg). \nThus deg rj 2 degj, and that contradicts the choice of j. We now know \nthat f divides each gi and hence that /30 \nh. Since Wo is T-admissible, \n/30 = ho where 'Yo is in Woo We remark in passing that Step 2 is a strength­\nened form of the assertion that each of the subspaces WI, W2, •\n•\n•\n , Wr is \nT -admissible. \nStep 3. There exist non-zero vectors \nen, . . .  , ar in V which \nsatisfy conditions (i) and (ij) of Theorem 3. \nStart with vectors /31, . . .  , /3, as in Step 1. Fix k, 1 :::; k :::; r. We apply \nStep 2 to the vector /3 = /3k and the T-conductor f = Pk. We obtain \n(7-9) \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nwhere ')'0 is in Wo and hi, . . .  , hk_l are polynomials. Let \n(7-10) \nak = {3k - ')'0 -\n \nhi{3i. \n1 ::;i <k \nSince (3k - ak is in Wk-1, \n(7-11) \nand since Pkak = 0, we have \n(7-12) \nBecause each ak satisfies (7-11) and (7-12), it follows that \nWk = Wo EB Zeal; T) EB . . .  EB Z(ak; T)",
    "where ')'0 is in Wo and hi, . . .  , hk_l are polynomials. Let \n(7-10) \nak = {3k - ')'0 -\n \nhi{3i. \n1 ::;i <k \nSince (3k - ak is in Wk-1, \n(7-11) \nand since Pkak = 0, we have \n(7-12) \nBecause each ak satisfies (7-11) and (7-12), it follows that \nWk = Wo EB Zeal; T) EB . . .  EB Z(ak; T) \nand that Pk is the T-annihilator of ak. In other words, the vectors ai, . . . , aT \ndefine the same sequence of subspaces WI, W2, • \n• \n• as do the vectors \n{31, •\n•\n•\n , (3T and the T-conductors Pk \ns(ak, Wk-l) have the same max­\nimality properties (condition (b) of Step 1). The vectors ai, . . . , aT have \nthe additional property that the subspaces Wo, Z(al; T), Z(a2; T), . . .  are \nindependent. It is therefore easy to verify condition (ii) in Theorem 3. \nSince Piai \n= ° for each i, we have the trivial relation \nPkak = 0 + pial + . \" + Pk-lak-l. \nApply Step 2 with {31, •\n•\n.\n , {3k replaced by aJ, . . . , ak and with (3 = ak. \nConclusion: Pk divides each Pi with i < k. \nStep 4. The number r and the polynomials PI, . . . , pr are uniquely \ndetermined by the conditions of Theorem 3. \nSuppose that in addition to the vectors aI, . . .  , aT in Theorem 3 we \nhave non-zero vectors ')'1, •\n•\n•\n , ')'. with respective T-annihilators 01, .\n.\n. , O. \nsuch that \n(7-13) \nv = Wo EB Z(')'l; T) EB . . . EB Z(,),.; T) \nOk divides Ok-I, \nk = 2, .\n. . , 8. \nWe shall show that r \n8 and Pi \nOi for each i. \nIt is very easy to see that PI = 01. The polynomial 01 is determined \nfrom (7-13) as the T-conductor of V into Woo Let S(V; Wo) be the collection \nof polynomials f such that f{3 is in Wo for every fJ in V, i.e., polynomials f \nsuch that the range of f(T) is contained in Woo Then S(V; Wo) is a non-zero \nideal in the polynomial algebra. The polynomial 01 is the monic generator \nof that ideal, for this reason. Each (3 in V has the form \nand so \nfJ = fJo + fm + . . .  + f'')'8 \n8 \nOlfJ = OlfJO +  gJi')'i. \n1 \nSince each gi divides 01, we have gl')'i \n° for all i and OlfJ \n= glfJO is in Woo",
    "ideal in the polynomial algebra. The polynomial 01 is the monic generator \nof that ideal, for this reason. Each (3 in V has the form \nand so \nfJ = fJo + fm + . . .  + f'')'8 \n8 \nOlfJ = OlfJO +  gJi')'i. \n1 \nSince each gi divides 01, we have gl')'i \n° for all i and OlfJ \n= glfJO is in Woo \nThus gl is in S(V; Wo). Since 01 is the monic polynomial of least degree \n235 \n236 \nThe Rational and Jordan Forms \nChap. 7 \nwhich sends /'1 into Wo, we see that gl is the monic polynomial of least degree \nin the ideal S(V; Wo). By the same argument, PI is the generator of that \nideal, so PI \n91. \nIf f is a polynomial and W is a subspace of V, we shall employ the \nshorthand fW for the set of all vectors fa with a in W. We have left to the \nexercises the proofs of the following three facts. \n1. fZ(a; T) \nZ(fa; T). \n2. If V = Vl EB . . .  EB Vk, where each Vi is invariant under T, then \nfV = fV1 EB . . .  EB fVko \n3. If a and /, have the same T-annihilator, then fa and h have the \nsame T-annihilator and (therefore) \ndim Z(fa; T) \ndim Z(h; T). \nNow, we proceed by induction to show that r = 13 and Pi = gi for \ni = 2, . . .  , r. The argument consists of counting dimensions in the right \nway. We shall give the proof that if r ;:: 2 then P2 \n92, and from that the \ninduction should be clear. Suppose that r ;:: 2. Then \ndim Wo + dim Z(a1; T) < dim V. \nSince we know that PI = gl, we know that Zeal; T) and Z(/'l; T) have the \nsame dimension. Therefore, \ndim Wo + dim Z(/'l; T) < dim V \nwhich shows that s ;:: 2. Now it makes sense to ask whether or not P2 \ng2' \nFrom the two decompositions of V, we obtain two decompositions of the \nsubspace P2 V: \npz V = P2 W 0 EB Z (P2al; T) \npz V = P2 Wo EB Z(P2/'1; T) EB . . .  EB Z(P2/'s; T). \n(7-14) \nWe have made use of facts (1) and (2) above and we have used the fact \nthat P2ai \n0, i ;:: 2. Since we know that Pl \ngl, fact (3) above tells us \nthat Z(P2al; T) and Z(P2/'1; T) have the same dimension. Hence, it is \napparent from (7-14) that",
    "(7-14) \nWe have made use of facts (1) and (2) above and we have used the fact \nthat P2ai \n0, i ;:: 2. Since we know that Pl \ngl, fact (3) above tells us \nthat Z(P2al; T) and Z(P2/'1; T) have the same dimension. Hence, it is \napparent from (7-14) that \ndim Z(P2/'i; T) = 0, \ni 2: 2. \nWe conclude that P2/'2 = 0 and 92 divides P2. The argument can be reversed \nto show that P2 divides g2. Therefore P2 \n92. I \nCorollary. If T is a linear operator on a finite-dimensional vector \nspace, then every T-admissible subspace has a complementary subspace which \nis also invariant under T. \nProof. Let Wo be an admissible subspace of V. If Wo = V, the \ncomplement we seek is {O} . If Wo is proper, apply Theorem 3 and let \nW& \nZ(a1; T) EB . . .  EB Z(ar; T). \nThen W6 is invariant under T and V = Wo EB W6. \nI \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nCorollary. Let T be a linear operator on a finite-dimensional vector \nspace V. \n(a) There exists a vector a in V such that the T-annihilator of a is the \nminimal polynomial for T. \n(b) T has a cyclic vector if and only if the characteristic and minimal \npolynomials for T are identical. \nProof. If V = {O}, the results are trivially true. If V ;F- {O}, let \n(7-15) \nV = Zeal; T) EB . . . EB Z(ar; T) \nwhere the T-annihilators PI, . . .  , Pr are such that Pk+l divides Pk, 1 :::; k :::; \nr \n- 1. As we noted in the proof of Theorem 3, it follows easily that PI is the \nminimal polynomial for T, i.e., the T-conductor of V into {O} . We have \nproved (a). \nWe saw in Section 7.1 that, if T has a cyclic vector, the minimal \npolynomial for T coincides with the characteristic polynomial. The content \nof (b) is in the converse. Choose any a as in (a). If the degree of the minimal \npolynomial is dim V, then V = Z(a; T). I \nTheorem 4 (Generalized Cayley-Hamilton Theorem). Let T be \na linear operator on a finite-dimensional vector space V. Let p and f be the \nminimal and characteristic polynomials for T, respectively. \n(i) p divides f.",
    "polynomial is dim V, then V = Z(a; T). I \nTheorem 4 (Generalized Cayley-Hamilton Theorem). Let T be \na linear operator on a finite-dimensional vector space V. Let p and f be the \nminimal and characteristic polynomials for T, respectively. \n(i) p divides f. \n(ii) p and f have the same prime factors, except for multiplicities. \n(iii) If \n(7-16) \np = £'il . . .  f; \nis the prime factorization of p, then \n(7-17) \nf = fჺl . .  · f჻k \nwhere di is the nullity of L(T)fi divided by the degree of fi. \nProof. We disregard the trivial case V = {O} . To prove (i) and \n(ii), consider a cyclic decomposition (7-15) of V obtained from Theorem 3. \nAs we noted in the proof of the second corollary, PI = p. Let Vi be the \nrestriction of T to Z(ai; T). Then Vi has a cyclic vector and so Pi is both \nthe minimal polynomial and the characteristic polynomial for Vi. There­\nfore, the characteristic polynomial f is the product f = PI .\n. . pro That is \nevident from the block form (6-14) which the matrix of T assumes in a \nsuitable basis. Clearly PI \n= P dividesf, and this proves (i). Obviously any \nprime divisor of p is a prime divisor of f. Conversely, a prime divisor of \nf = PI . . .  pr must divide one of the factors pi, which in turn divides Pl. \nLet (7-16) be the prime factorization of p. We employ the primary \ndecomposition theorem (Theorem 12 of Chapter 6). It tells us that, if Vi \nis the null space of fi(T)ri, then \n1237 \n238 \nThe Rational and Jordan Forms \nChap. 7 \n(7-18) \nandf? is the minimal polynomial of the operator 1';, obtained by restricting \nl' to the (invariant) subspace Vi. Apply part (ii) of the present theorem to \nthe operator T i. Since its minimal polynomial is a power of the prime 1;, \nthe characteristic polynomial for Ti has the form ft', where d; ;:: rio Obvi­\nously \nd. _ dim Vi \n• - deg f; \nand (almost by definition) dim Vi = nullity fi(T)\". Since T is the direct \nsum of the operators TI, •\n•\n• , Tk, the characteristic polynomial f is the \nproduct",
    "the characteristic polynomial for Ti has the form ft', where d; ;:: rio Obvi­\nously \nd. _ dim Vi \n• - deg f; \nand (almost by definition) dim Vi = nullity fi(T)\". Since T is the direct \nsum of the operators TI, •\n•\n• , Tk, the characteristic polynomial f is the \nproduct \nCorollary. If T is a nilpotent linear operator on a vector space of \ndimension n, then the characteristic polynomial for T is xu. \nNow let us look at the matrix analogue of the cyclic decomposition \ntheorem. If we have the operator T and the direct-sum decomposition of \nTheorem 3, let (Bi be the 'cyclic ordered basis' \n{¢ . Ta ' \nTk'- l¢ .} \n'-4t, \n21 ·\n'\n\"\n \nʋl \nfor Z(ai; T). Here ki denotes the dimension of Z(ai; T), that is, the degree \nof the annihilator Pi. The matrix of the induced operator Ti in the ordered \nbasis (Bi is the companion matrix of the polynomial Pi. Thus, if we let (B be \nthe ordered basis for V which is the union of the (Bi arranged in the order \n(BI, •\n•\n. , (Bn then the matrix of T in the ordered basis (B will be \n(7-19) \nwhere Ai is the ki X ki companion matrix of Pi. An n X n matrix A, \nwhich is the direct sum (7-19) of companion matrices of non-scalar monic \npolynomials PI, . .\n. , 'Pr such that Pi+! divides Pi for i \n1, .\n. . , r \n1, \n'NiH be said to be in rational form. The cyclic decomposition theorem \ntells us the following concerning matrices. \nTheorem 5. Let F be a field and let B be an n X n matrix over F. \nThen B is similar over the field F to one and only one matrix which is in \nrational form. \nProof. Let l' be the linear operator on F\" which is represented by \nB in the standard ordered basis. As we have just observed, there is some \nordered basis for Fn in which T is represented by a matrix A in rational \nform. Then B is similar to this matrix A . Suppose B is similar over F to \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nanother matrix C which is in rational form. This means simply that there",
    "ordered basis for Fn in which T is represented by a matrix A in rational \nform. Then B is similar to this matrix A . Suppose B is similar over F to \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nanother matrix C which is in rational form. This means simply that there \nis some ordered basis for Fn in which the operator T is represented by the \nmatrix C. If C is the direct sum of companion matrices Ci of monic poly-\nllomials g!, . . .  , gs such that gi+l divides gi for i \n1, . . . , 8 \n- 1, then it \nis apparent that we shall have non-zero vectors i3l, . . .  , i3s in V with T-\nannihilators gl, . . .  , gs such that \nV = Z(i3l; T) EB . . .  EB Z(i3.; T). \nBut then by the uniqueness statement in the cyclic decomposition theorem, \nthe polynomials gi are identical with the polynomials Pi which define the \nmatrix A .  Thus C \nA .  I \nThe polynomials PI, .\n. . , pr are called the invariant factors for \nthe matrix B. In Section 7.4, we shall describe an algorithm for calculating \nthe invariant factors of a given matrix B. The fact that it is possible to \ncompute these polynomials by means of a finite number of rational opera­\ntions on the entries of B is whttt gives the rational form its name. \nEXAMPLE 2. Suppose that V is a two-dimensional vector space over \nthe field F and T is a linear operator on V. The possibilities for the cyclic \nsubspace decomposition for T are very limited. For, if the minimal poly­\nnomial for T has degree 2, it is equal to the characteristic polynomial for \n']' and T has a cyclic vector. Thus there is some ordered basis for V in \nwhich T is represented by the companion matrix of its characteristic \npolynomial. If, on the other hand, the minimal polynomial for T has degree \n1, then T is a scalar multiple of the identity operator. If T = eI, then for \nany two linear independent vectors al and a2 in V we have \nV \nZeal; T) EB Z(a2; T) \nPI = P2 = x - c. \nFor matrices, this analysis says that every 2 X 2 matrix over the field F",
    "1, then T is a scalar multiple of the identity operator. If T = eI, then for \nany two linear independent vectors al and a2 in V we have \nV \nZeal; T) EB Z(a2; T) \nPI = P2 = x - c. \nFor matrices, this analysis says that every 2 X 2 matrix over the field F \nis similar over F to exactly one matrix of the types \n[o \nEXAMPLE 3. Let T be the linear operator on R3 which is represented \nby the matrix \n-6 \n4 \n-6 \n-6J \n2 \n-4 \nin the standard ordered basis. We have computed earlier that the char­\nacteristic polynomial for T is f \n(x - 1) (x \n2) 2 and the minimal \npolynomial for T is p = (x \nl)(x - 2). Thus we know that in the cyclic \ndecomposition for T the first vector al will have p as its T-annihilator. \n239 \n240 \nThe Rational and Jordan Forms \nChap. 7 \nSince we are operating in a three-dimensional space, there can be only one \nfurther vector, a2. It must generate a cyclic subspace of dimension 1, Le., \nit must be a characteristic vector for T. Its T-annihilator P2 must be \n(x - 2), because we must have PP2 = f. Notice that this tells us im­\nmediately that the matrix A is similar to the matrix \nB [Ň -Ǵ 4] \no \n0 2 \nthat is, that T is represented by B in some ordered basis. How can we find \nsuitable vectors IXI and IX2? Well, we know that any vector which generates \na T-cyclic subspace of dimension 2 is a suitable IXI. SO let's just try fl. We \nhave \nTEl = (5, - 1, 3) \nwhich is not a scalar multiple of EI; hence Z(EI; T) has dimension 2. This \nspace consists of all vectors af1 + b(Ttl) : \na(I, 0, 0) + b(5, \n1, 3) \n(a + 5b, -b, 3b) \nor, all vectors (Xl, X2, Xa) satisfying X3 \n-3X2. Now what we want is \na vector a2 such that 'I'a2 = 2a2 and Z(IX2; T) is disjoint from Z(€l; T). \nSince IX2 is to be a characteristic vector for T, the space Z(IX2; T) will simply \nbe the one-dimensional space spanned by a2, and so what we require is that \na2 not be in Z(€l; T). If IX \n(x!, X2, xs), one can easily compute that \nTIX",
    "Since IX2 is to be a characteristic vector for T, the space Z(IX2; T) will simply \nbe the one-dimensional space spanned by a2, and so what we require is that \na2 not be in Z(€l; T). If IX \n(x!, X2, xs), one can easily compute that \nTIX \n= 2IX if and only if Xl = 2X2 + 2xa. Thus IX2 = (2, 1, 0) satisfies Ta2 = \n2a2 and generates a T-cyclic subspace disjoint from Z(€l; T). The reader \nshould verify directly that the matrix of T in the ordered basis \n{(I, 0, 0), (5, - 1, 3), (2, 1, O)} \nis the matrix B above. \nEXAMPLE 4. Suppose that T is a diagonalizable linear operator on V. \nIt is interesting to relate a cyclic decomposition for T to a basis which \ndiagonalizes the matrix of T. Let CI, .\n•\n•\n , Ck be the distinct characteristic \nvalues of T and let Vi be the space of characteristic vectors associated with \nthe characteristic value Ci. Then \nand if di = dim Vi then \nf = (x - CI)d1 •\n•\n•\n (x - Ck)d. \nis the characteristic polynomial for T. If a is a vector in V, it is easy to \nrelate the cyclic subspace Z(a; T) to the subspaces VI, . . .  , Vk • There are \nunique vectors (31, .\n•\n.\n , (3k such that (3i is in Vi and \n. . .  + (3k. \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nSince T{3i \nCi{3i, we have \n(7-20) \nf(T)a = f(cI){31 + . . . + f(Ck){3k \nfor every polynomial f. Given any scalars tl, •\n•\n•\n , tk there exists a poly­\nnomial ! such that f(Ci) \n= ti, 1 ؋ i  k. Therefore, Z(a; T) is just thE: \nsubspace spanned by the vectors {31, .\n•\n•\n , {3k. What is the annihilator of a? \nAccording to (7-20), we havef(T)a \n0 if and only iff(Ci){3i \n0 for each i. \nIn other words, f(T)a = 0 provided !(Ci) = 0 for each i such that {3, ´ O. \nAccordingly, the annihilator of a is the product \n(7-21) \nII (x - Ci). \n/l,r\"O \nNow, let ffii = {{3i, . . .  , {3ჱ,} be an ordered basis for Vi. Let \nr = max di• \nWe define vectors ai, . . .  , ar by \n(7-22) \na; =  (3j, \nd,?:.i \ni \n1  j S r. \nThe cyclic subspace Z(aj; T) is the subspace spanned by the vectors (3), as",
    "(7-21) \nII (x - Ci). \n/l,r\"O \nNow, let ffii = {{3i, . . .  , {3ჱ,} be an ordered basis for Vi. Let \nr = max di• \nWe define vectors ai, . . .  , ar by \n(7-22) \na; =  (3j, \nd,?:.i \ni \n1  j S r. \nThe cyclic subspace Z(aj; T) is the subspace spanned by the vectors (3), as \ni runs over those indices for which di :2: j. The T-annihilator of aj is \n(7-23) \nWe have \nPi \nII (x - c,). \nd,?:.i \nV \nZ(al ; T) EB .\n. . EB Z(ar; T) \nbecause each (3j belongs to one and only one of the subspaces Zeal; T), . . .  , \nZ(ar; T) and ffi = (ffil, •\n•\n•\n , ffik) is a basis for V. By (7-23), Pi+! divides Pj. \nExercises \n1. Let T be the linear operator on F2 which is represented in the standard ordered \nbasis by the matrix \nLet al = (0, 1). Show that F2 \"e Zeal; T) and that there is no non-zero vector a2 \nin F2 with Z(a2; T) disjoint from Zeal; T). \n2. Let T be a linear operator on the finite-dimensional space V, and let R be \nthe range of T. \n(a) Prove that R has a complementary T-invariant subspace if and only if R \nis independent of the null space N of T. \n(b) If R and N are independent, prove that N is the unique T-invariant sub­\nspace complementary to R. \n3. Let T be the linear operator on R3 which is represented in the standard ordered \nbasis by the matrix \n[\n2 \n° \n0J \n1 \n2 0 . \n0\n0\n3\n \n242 \nThe Rational and Jordan Forms \nChap. 7 \nLet W be the null space of T - 21. Prove that W has no complementary T-invariant \nsubspace. (Hint: Let (3 \n€l and observe that (T - 2I)/3 is in W. Prove there is \nno a in W with (T \n21)(3 \n(T - 2I)a.) \n4. Let T be the linear operator on F4 which is represented in the standard ordered \nbasis by the matrix \n[ ǲ V V]. \no \n1 \nc \n0 \no O\nl\ne \nLet W be the null space of T - c/. \n(a) Prove that W is the subspace spanned by €4. \n(b) Find the monic generators of the ideals S(€4; W), S(ea; W), S(€2; W), \nS(€l; W). \n5. Let T be a linear operator on the vector space V over the field F. If f is a poly­",
    "[ ǲ V V]. \no \n1 \nc \n0 \no O\nl\ne \nLet W be the null space of T - c/. \n(a) Prove that W is the subspace spanned by €4. \n(b) Find the monic generators of the ideals S(€4; W), S(ea; W), S(€2; W), \nS(€l; W). \n5. Let T be a linear operator on the vector space V over the field F. If f is a poly­\nnomial over F and a is in V, let fa = f(T)a. If VI, . . . , Vk are T-invariant sub-\nspaces and V \nVI EB . . . EB Vk, show that \nfV \nfV! EB . . . EBfVk• \n6. Let T, V, and F be as in Rxercise 5. Suppose a and /3 are vectors in V which \nhave the same T-annihilator. Prove that, for any polynomial f, the veetors fa \nand f/3 have the same T-annihilator. \n7. Find the minimal polynomials and the rational forms of each of the following \nreal matrices. [ 0 \n- 1  - IJ \n1 \n0 \n0 '  \n- 1  \n0 \n0 \n[ c O  - IJ \no c \n1\n, \n- 1  1 c \n[ COS 8 sin 8J \n-sin 8 cos () . \n8. Let T be the linear operator on R3 which is represented in the standard ordered \nbasis by \n[ 3 \n-4 \n-4J \n- 1  \n3 \n2 ·  \n2 \n-4 -3 \nFind non-zero vectors aI, . . .  , aT satisfying the conditions of Theorem 3. \n9. Let A be the real matrix \nFind an invertible 3 X 3 real matrix P such that P-IAP is in rational form. \n10. Let F be a subfield of the complex numbers and let T be the linear operator \non F4 which is represented in the standard ordered basis by the matrix \n[i ǳ V V]. \no \na \n2 0 \no 0 b \n2 \nSec. 7.2 \nCyclic Decompositions and the Rational Form \nFind the characteristic polynomial for T. Consider the cases a = b = 1 ;  a = b = 0; \na \n0, b \n1. In each of these cases, find the minimal polynomial for T and non­\nzero vectors ai, . . . , aT which satisfy the conditions of Theorem 3. \nn. Prove that if A and B are 3 X 3 matrices over the field F, a necessary and \nsufficient condition that A and B be similar over F is that they have the same \ncharacteristic polynomial and the same minimal polynomial. Give an example \nwhich shows that this is false for 4 X 4 matrices.",
    "n. Prove that if A and B are 3 X 3 matrices over the field F, a necessary and \nsufficient condition that A and B be similar over F is that they have the same \ncharacteristic polynomial and the same minimal polynomial. Give an example \nwhich shows that this is false for 4 X 4 matrices. \n12. Let F be a subfield of the field of complex numbers, and let A and B be n X n \nmatrices over F. Prove that if A and B are similar over the field of complcx num­\nbers, then they are similar over F. (Hint: Prove that the rational form of A is the \nsame whether A is viewed as a matrix over F or a matrix over C; likewise for B.) \n13. Let A be an n X n matrix with complex entries. Prove that if every character­\nistic value of A is real, then A is similar to a matrix with real entries. \n14. Let T be a linear operator on the finite-dimensional space V. Prove that there \nexists a vector a in V with this property. If f is a polynomial and f(T)a = 0, \nthen J(T) \n0, (Such a vector a is called a separating vector for the algebra of \npolynomials in T.) When T has a cyclic vector, give a direct proof that any cyclic \nvector is a separating vector for the algebra of polynomials in T. \n15. Let F be a subfield of the field of complex numbers, and let A be an n X n \nmatrix over F. Let p be the minimal polynomial for A. If we regard A as a matrix \nover C, then A has a minimal polynomial f as an n X n matrix over C. Use a \ntheorem on linear equations to prove p \nJ. Can you also see how this follows from \nthe cyclic decomposition theorem? \n16. Let A be an n X n matrix with real entries such that A2 + I = O. Prove that \nn is even, and if n = 2k, then A is similar over the field of real numbers to a matrix \nof the block form \nwhere I is the k X k identity matrix. \n17. Let T be a linear operator on a finite-dimensional vector space V. Suppose that \n(a) the minimal polynomial for T is a power of an irreducible polynomial; \n(b) the minimal polynomial is equal to the characteristic polynomial.",
    "of the block form \nwhere I is the k X k identity matrix. \n17. Let T be a linear operator on a finite-dimensional vector space V. Suppose that \n(a) the minimal polynomial for T is a power of an irreducible polynomial; \n(b) the minimal polynomial is equal to the characteristic polynomial. \nShow that no non-trivial T-invariant subspace has a complementary T-invari­\nant subspace. \n18. If T is a diagonalizable linear operator, then every T-invariant subspace has \na complementary T-invariant subspace. \n19. Let T be a linear operator on the finite-dimensional space V. Prove that T \nhas a cyclic vector if and only if the following is true: Every linear operator U \nwhich commutes with T is a polynomial in T. \n20. Let V be a finite-dimensional vector space over the field F, and let T be a \nlinear operator on V. We ask when it is true that every non-zero vector in V is a \ncyclic vector for T. Prove that this is the case if and only if the characteristic \npolynomial for T is irreducible over F. \n243 \n244 \nThe Rational and Jordan Forms \nChap. 7 \n21. Let A be an n X n matrix with real entries. Let T be the linear operator on Rn \nwhich is represented by A in the standard ordered basis, and let U be the linear \noperator on en which is represented by A in the standard ordered basis. Use the \nresult of Exercise 20 to prove the following: If the only subspaces invariant under \nT are Rn and the zero subspace, then U is diagonalizable. \n7.3. The Jordan Form \nSuppose that N is a nilpotent linear operator on the finite-dimen­\nsional space V. Let us look at the cyclic decomposition for N which we \nobtain from Theorem 3. We have a positive integer r and r non-zero vectors \nai, . . .  , ar in V with N-annihilators PI, . .\n. , p., such that \nV = Zeal; N) EB . . .  EB Z(ar; N) \nand Pi+l divides Pi for i = 1, . . .  , r \n1. Since N is nilpotent, the minimal \npolynomial is Xk for some k ::; n. Thus each Pi is of the form Pi = Xk', \nand the divisibility condition simply says that",
    ". , p., such that \nV = Zeal; N) EB . . .  EB Z(ar; N) \nand Pi+l divides Pi for i = 1, . . .  , r \n1. Since N is nilpotent, the minimal \npolynomial is Xk for some k ::; n. Thus each Pi is of the form Pi = Xk', \nand the divisibility condition simply says that \nkl ჲ k2 ჲ . . .  ჲ kr• \nOf course, kl \nk and leT ჲ 1 .  The companion matrix of Xk. is the ki X ki \nmatrix \n(7-24) \n[0 0 \nA. 7 ! I \no 0] \no 0 \n  . \n. \n. \n1 0 \nThus Theorem 3 gives us an ordered basis for V in which the matrix of N \nis the direct sum of the elementary nilpotent matrices (7-24), the sizes of \nwhich decrease as i increases. One sees from this that associated with a \nnilpotent n X n matrix is a positive integer r and r positive integers \nkl' . . . , k, such that kl + .\n. . + k,. = n and ki ჳ lei+l, and these positive \nintegers determine the rational form of the matrix, i.e., determine the \nmatrix up to similarity. \nHere is one thing we should like to point out about the nilpotent \noperator N above. The positive integer r is precisely the nullity of N; \nin fact, the null space has as a basis the r vectors \n(7-25) \nFor, let a be in the null space of N. We write a in the form \na \nflal + . . . + frar \nwhere Ji is a polynomial, the degree of which we may assume is less than \nki. Since N a = 0, for each i we have \nSec. 7.3 \no = N(fiCXi) \n= Nji(N)CXi \n= (Xji)CXi. \nThe Jordan Form \nThus Xji is divisible by Xki, and since deg (ji) > ki this means that \nwhere Ci is some scalar. But then \ncx = Cl(Xkl- 1cxl) + . . . + cr(Xk,-lcxr) \nwhich shows us that the vectors (7-25) form a basis for the null space of N. \nThe reader should note that this fact is also quite clear from the matrix \npoint of view. \nN ow what we wish to do is to combine our findings about nilpotent \noperators or matrices with the primary decomposition theorem of Chapter \n6. The situation is this: Suppose that T is a linear operator on Y and that \nthe characteristic polynomial for T factors over F as follows: \nj \n(x \nCl)d! •\n•\n•\n (x \nCk)d.",
    "operators or matrices with the primary decomposition theorem of Chapter \n6. The situation is this: Suppose that T is a linear operator on Y and that \nthe characteristic polynomial for T factors over F as follows: \nj \n(x \nCl)d! •\n•\n•\n (x \nCk)d. \nwhere Cl, • • •  , Ck are distinct elements of F and di 2: 1. Then the minimal \npolynomial for T will be \np \n(x \nCl)'\" \n. . (x - Ck)\" \nwhere 1 ::; ri ::; di• If Wi is the null space of (T \ncJ)\"', then the primary \ndecomposition theorem tells us that \nY = WI EB . . .  EB Wk \nand that the operator Ti induced on Wi by T has minimal polynomial \n(x - Ci)fi. Let Ni be the linear operator on Wi defined by Ni = Ti - cJ. \nThen Ni is nilpotent and has minimal polynomial Xfi. On Wi, T acts like \nNi plus the scalar Ci times the identity operator. Suppose we choose a \nbasis for the subspace Wi corresponding to the cyclic decomposition for \nthe nilpotent operator Ni• Then the matrix of Ti in this ordered basis will \nbe the direct sum of matrices \n(7-26) \n[< Ǳ P] \no 0 \n1 \nC \neach with c \nCi. Furthermore, the sizes of these matrices will decrease \nas one reads from left to right. A matrix of the form (7-26) is called an \nelementary Jordan matrix with characteristic value c. Now if we put \nall the bases for the Wi together, we obtain an ordered basis for Y. Let \nus describe the matrix A of T in this ordered basis. \n245 \n246 \nThe Rational and Jordan Forms \nThe matrix A is the direct sum \n(7-27) \n[AI 0 \nA =  2 \n. \n. \no \n0 1.] \nof matrices AI, . . .  , Ak• Each Ai is of the form \n[Ji$) 0 \n. _ \n0 J%j) \nA. -\n. \n. \no \n0 \nLJ \nChap. 7 \nwhere each Jjll is an elementary Jordan matrix with characteristic value \nCi. Also, within each A i, the sizes of the matrices Jji) decrease as j in­\ncreases. An n X n matrix A which satisfies all the conditions described \nso far in this paragraph (for some distinct scalars CI, •\n•\n•\n , Ck) will be said \nto be in Jordan form. \nWe have just pointed out that if T is a linear operator for which the",
    "creases. An n X n matrix A which satisfies all the conditions described \nso far in this paragraph (for some distinct scalars CI, •\n•\n•\n , Ck) will be said \nto be in Jordan form. \nWe have just pointed out that if T is a linear operator for which the \ncharacteristic polynomial factors completely over the scalar field, then \nthere is an ordered basis for V in which T is represented by a matrix which \nis in Jordan form. We should like to show now that this matrix is some­\nthing uniquely associated with T, up to the order in which the charac­\nteristic values of T are written down. In other words, if two matrices are \nin Jordan form and they are similar, then they can differ only in that the \norder of the scalars Ci is different. \nThe uniqueness we see as follows. Suppose there is some ordered basis \nfor V in which T is represented by the Jordan matrix A described in the \nprevious paragraph. If A i is a di X di matrix, then di is clearly the multi­\nplicity of Ci as a root of the characteristic polynomial for A, or for T. In \nother words, the characteristic polynomial for T is \nf = (x - CI)dl \n•\n•\n•\n (x - Ck)dk. \nThis shows that CI, •\n•\n•\n , Ck and dl, •\n•\n•\n , dk are unique, up to the order in \nwhich we write them. The fact that A is the direct sum of the matrices \nAi gives us a direct sum decomposition V = WI EB . . . EB Wk invariant \nunder T. Now note that Wi must be the null space of (T - c;l)n, where \nn = dim V; for, A i  - c;l is clearly nilpotent and Aj - c;l is non-singular \nfor j ,r; i. So we see that the subspaces Wi are unique. If Ti is the operator \ninduced on Wi by T, then the matrix Ai is uniquely determined as the \nrational form for (Ti \nc;l). \nNow we wish to make some further observations about the operator \nT and the Jordan matrix A which represents T in some ordered basis. \nWe shall list a string of observations: \n(1) Every entry of A not on or immediately below the main diagonal \nSec. 7.3 \nThe Jordan Form",
    "c;l). \nNow we wish to make some further observations about the operator \nT and the Jordan matrix A which represents T in some ordered basis. \nWe shall list a string of observations: \n(1) Every entry of A not on or immediately below the main diagonal \nSec. 7.3 \nThe Jordan Form \nis O. On the diagonal of A occur the k distinct characteristic values \nCI, •\n.\n.\n , Ck of T. Also, Ci is repeated di times, where d; is the multiplicity \nof Ci as a root of the characteristic polynomial, i.e., di = dim Wi. \n(2) For each i, the matrix Ai is the direct sum of ni elementary \nJordan matrices J]i) with characteristic value Ci. The number ni is pre­\ncisely the dimension of the space of characteristic vectors associated with \nthe characteristic value Ci. For, ni is the number of elementary nilpotent \nblocks in the rational form for (Ti - cd), and is thus equal to the dimen­\nsion of the null space of (T - cJ). In particular notice that T is diag­\nonalizable if and only if ni = di for each i. \n(3) For each i, the first block Jil) in the matrix Ai is an Ti X T, \nmatrix, where 1\"i is the multiplicity of Ci as a root of the minimal poly­\nnomial for T. This follows from the fact that the minimal polynomial for \nthe nilpotent operator (Ti \ncJ) is x\". \nOf course we have as usual the straight matrix result. If B is an \nn X n matrix over the field F and if the characteristic polynomial for B \nfactors completely over F, then B is similar over F to an n X n matrix \nA in Jordan form, and A is unique up to a rearrangement of the order \nof its characteristic values. We call A the Jordan forll of B. \nAlso, note that if F is an algebraically closed field, then the above \nremarks apply to every linear operator on a finite-dimensional space over \nF, or to every n X n matrix over F. Thus, for example, every n X n \nmatrix over the field of complex numbers is similar to an essentially unique \nmatrix in Jordan form. \nEXAMPLE 5. Suppose T is a linear operator on C2. The characteristic",
    "F, or to every n X n matrix over F. Thus, for example, every n X n \nmatrix over the field of complex numbers is similar to an essentially unique \nmatrix in Jordan form. \nEXAMPLE 5. Suppose T is a linear operator on C2. The characteristic \npolynomial for T is either (x \nCl) (x \nC2) where Cl and C2 are distinct \ncomplex numbers, or is (x \nC)2. In the former case, T is diagonalizable \nand is represented in some ordered basis by \n[Cl \nOJ\n. \no \nC2 \nIn the latter case, the minimal polynomial for T may be (x - c), in which \ncase T \ncI, or may be (x \nC)2, in which case T is represented in some \nordered basis by the matrix \nThus every 2 X 2 matrix over the field of complex numbers is similar to \na matrix of one of the two types displayed above, possibly with Cl = C2. \nEXAMPLE 6. Let A be the complex 3 X 3 matrix \n[2 0 \nA = \na 2 \nb \nc 4l' \n- 1  \n247 \nThe Rational and Jordan Forms \nChap. 7 \nThe characteristic polynomial for A is obviously (x - 2)2(X + 1). Either \nthis is the minimal polynomial, in which case A is similar to \n[ş Ņ j] \nor the minimal polynomial is (x - 2)(x + 1), in which case A is similar to \n[2 0 \no 2 \no 0 5]. \n- 1 \nNow \n(A \n2I)(A + 1) = [3ņ 5 3] \nac 0 0 \nand thus A is similar to a diagonal matrix if and only if a \nO. \nEXAMPLE 7. Let \n[2 0 0 0] \nA =\n1\n2\n0\n0 . \no 0 2 0 \no 0 a 2 \nThe characteristic polynomial for A is (x - 2)4. Since A is the direct sum \nof two 2 X 2 matrices, it is clear that the minimal polynomial for A is \n(x - 2)2. Now if a = 0 or if a = 1, then the matrix A is in Jordan form. \nNotice that the two matrices we obtain for a = 0 and a = 1 have the \nsame characteristic polynomial and the same minimal polynomial, but \nare not similar. They are not similar because for the first matrix the solu­\ntion space of (A - 21) has dimension 3, while for the second matrix it \nhas dimension 2. \nEXAMPLE 8. Linear differential equations with constant coefficients \n(Example 14, Chapter 6) provide a nice illustration of the Jordan form.",
    "tion space of (A - 21) has dimension 3, while for the second matrix it \nhas dimension 2. \nEXAMPLE 8. Linear differential equations with constant coefficients \n(Example 14, Chapter 6) provide a nice illustration of the Jordan form. \nLet ao, . . .  , an-l be complex numbers and let V be the space of all n times \ndifferentiable functions j on an interval of the real line which satisfy the \ndifferential equation \ndnj \ndn-1j \ndj \ndxn + an-l dxn-1 + . . .  + al dx + ao! \nO. \nLet D be the differentiation operator. Then V is invariant under D, because \nV is the null space of p(D), where \np \nxn + \n. . .  + alX + ao. \nWhat is the Jordan form for the differentiation operator on V? \nSec. 7.3 \nLet Cll •\n•\n•\n , Ck be the distinct complex roots of p :  \np = (x - Ct)T1 \n•\n•\n•\n (x - Ck)'·. \nThe Jordan Form \nLet Vi be the null space of (D - cil)\", that is, the set of solutions to the \ndifferential equation \n(D - cd)\"'! = O. \nThen as we noted in Example 15, Chapter 6 the primary decomposition \ntheorem tells us that \nLet Ni be the restriction of D - cd to Vi. The Jordan form for the oper­\nator D (on V) is then determined by the rational forms for the nilpotent \noperators NI, •\n•\n.\n , Nk on the spaces VI, . . .  , Vk• \nSo, what we must know (for various values of e) is the rational form \nfor the operator N = (D - eI) on the space Vo which consists of the \nsolutions of the equation \n(D - el)T! = O. \nHow many elementary nilpotent blocks will there be in the rational form \nfor N? The number will be the nullity of N, i.e., the dimension of the \ncharacteristic space associated with the characteristic value c. That \ndimension is 1, because any function which satisfies the differential \nequation \nDf = cf \nis a scalar multiple of the exponential function hex) \n= eCZ. Therefore, the \noperator N (on the space Vo) has a cyclic vector. A good choice for a \ncyclic vector is g = x..-Ih: \ng(x) \n= x,-Ie\"\". \nThis gives \nNg = (r - l)xr-2h \nNr-Ig = (r - 1) !h",
    "equation \nDf = cf \nis a scalar multiple of the exponential function hex) \n= eCZ. Therefore, the \noperator N (on the space Vo) has a cyclic vector. A good choice for a \ncyclic vector is g = x..-Ih: \ng(x) \n= x,-Ie\"\". \nThis gives \nNg = (r - l)xr-2h \nNr-Ig = (r - 1) !h \nThe preceding paragraph shows us that the Jordan form for D (on \nthe space V) is the direct sum of k elementary Jordan matrices, one for \neach root ei. \nExercises \n1. Let Nl and N2 be 3 X 3 nilpotent matrices over the field F. Prove that Nl \nand N2 are similar if and only if they have the same minimal polynomial. \n2. Use the result of Exercise 1 and the Jordan form to prove the following: Let \n250 \nThe Rational and Jordan Forms \nChap. 7 \nA and B be n X n matrices over the field F which have the same characteristic \npolynomial \nf = (x - Cl)dl •\n•\n• (x - Ck)dʖ \nand the same minimal polynomial. If no d; is greater than 3, then A and B are \nsimilar. \n3. If A is a complex 5 X 5 matrix with characteristic polynomial \nf \n(x - 2)3(X + 7)2 \nand minimal polynomial p \n(x \n2)2(x + 7), what is the Jordan form for A? \n4. How many possible ,Jordan forms are there for a 6 X 6 complex matrix with \ncharacteristic polynomial (x + 2)4(X - 1)2? \n5. The differentiation operator on the space of polynomials of degree less than \nor equal to 3 is represented in the 'natural' ordered basis by the matrix \n[   n \nWhat is the Jordan form of this matrix? (F a subfield of the complex numbers.) \n6. Let A be the complex matrix \n2 0 0 0 0 \n0 \n1 \n2 0 0 0 \n0 \n- 1  0 2 0 0 \n0 \n0 1 \n0 2 0 \n0 \n1 \n1 \n1 \n1 \n2 \n0 \n0 0 0 0 \n1 \n- 1  \nFind the Jordan form for A. \n7. If 11 is an n X n matrix over the field F with characteristic polynomial \nwhat is the trace of A? \n8. Classify up to similarity all 3 X 3 complex matrices 11 such that A 3 = I. \n9. Classify up to similarity all n X n complex matrices A such that An = 1. \n10. Let n be a positive integer, n ;:: 2, and let N be an n X n matrix over the \nfield F such that Nn",
    "what is the trace of A? \n8. Classify up to similarity all 3 X 3 complex matrices 11 such that A 3 = I. \n9. Classify up to similarity all n X n complex matrices A such that An = 1. \n10. Let n be a positive integer, n ;:: 2, and let N be an n X n matrix over the \nfield F such that Nn \n0 but Nn-J ¢ O. Prove that N has no square root, i.e., \nthat there is no n X n matrix 11 such that A.2 = N. \nll. Let Nl and N2 be 6 X 6 nilpotent matrices over the field F. Suppose that \nNJ and N2 have the same minimal polynomial and the same nullity. Prove that \nNJ and N2 are similar. Show that this is not true for 7 X 7 nilpotent matrices. \n12. Use the result of Exercise 1 1  and the Jordan form to prove the following: \nLet 11 and B be n X n matrices over the field F which have the same characteristic \npolynomial \nSec. 7.4 \nComputation of Invariant Factors \nand the same minimal polynomial. Suppose also that for each i the solution spaces \nof (A - c;l) and (B - c;l) have the same dimension. If no di is greater than 6, \nthen A and B are similar. \n13. If N is a k X k elementary nilpotent matrix, i.e., Nk = 0 but Nk-l ,t. 0, show \nthat Nt is similar to N. Now use the Jordan form to prove that every complex \nn X n matrix is similar to its transpose. \n14. vVhat's wrong with the following proof? If A is a complex n X n matrix \nsuch that AI = -A, then A is O. (Proof: Let J be the Jordan form of A. Since \nAt \n-A ,  JI \n-J. But J is triangular so that Jt \n-J implies that every \nentry of J is zero. Since J \n0 and A is similar to J, we see that A \n0.) (Give \nan example of a nOll-zero A such that AI = -A.) \n15. If N is a nilpotent 3 X 3 matrix over C, prove that A = I + !N - tN2 \nsatisfies A 2 \nI + N, i.e., A is a square root of I + N. Use the binomial series for \n(1 + t)I/2 to obtain a similar formula for a square root of I + N, where N is any \nnilpotent n X n matrix over C. \n16. Use the result of Exercise 15 to prove that if c is a non-zero complex number",
    "satisfies A 2 \nI + N, i.e., A is a square root of I + N. Use the binomial series for \n(1 + t)I/2 to obtain a similar formula for a square root of I + N, where N is any \nnilpotent n X n matrix over C. \n16. Use the result of Exercise 15 to prove that if c is a non-zero complex number \nand N is a nilpotent complex matrix, then (cI + N) has a square root. Now use \nthe Jordan form to prove that every non-singular complex n X n matrix has a \nsquare root. \n251 \n7.4. Computation of Invariant Factors \nSuppose that A is an n X n matrix with entries in the field F. We \nwish to find a method for computing the invariant factors PI, . . .  , pr \nwhich define the rational form for A. Let us begin with the very simple \ncase in which A is the companion matrix (7.2) of a monic polynomial \nP = xn + Cn_lXn-1 + . . .  + CIX + co. \nIn Section 7.1 we saw that P is both the minimal and the characteristic \npolynomial for the companion matrix A .  Now, we want to give a direct \ncalculation which shows that p is the characteristic polynomial for A .  In \nthis case, \nx \n0 0 \n0 \nCo \n- 1  \nx 0 \n0 \nCl \nxl - A \n0 \n- 1 x \n0 \nC2 \n0 \n0 0 \nX \nCn-2 \n0 \n0 0 \n- 1 x + Cn-l \nAdd x times row n to row (n \n1). This will remove the x in the (n \n1, \nn - 1) place and it will not change the determinant. Then, add x times \nthe new row (n - 1) to row (n - 2). Continue successively until all of \nthe x's on the main diagonal have been removed by that process. The \nresult is the matrix \n252 \nThe Rational and Jordan Forms \nChap. 7 \n0 \n0 0 \n0 \nxn + \n.\n.\n.\n + CIX + Co \n- 1  \n0 0 \n0 xn-l + \n.\n.\n.\n + C2X + Cl \n0 \n- 1  0 \n0 xn-2 + \n. .\n. + CaX + C2 \n0 \n0 0 \n0 \nX2 + Cn-IX + Cn-2 \n0 \n0 0 \n- 1 \nx + Cn-l \nwhich has the same determinant as xl \n- A. The upper right-hand entry \nof this matrix is the polynomial p. We clean up the last column by adding \nto it appropriate multiples of the other columns: \n0 \n0 0 \n0 P \n- 1  \n0 0 \n0 0 \n0 \n- 1  0 \n0 0 \n0 \n0 0 \n0 0 \n0 \n0 0 \n- 1  0",
    "0 \n0 0 \n- 1 \nx + Cn-l \nwhich has the same determinant as xl \n- A. The upper right-hand entry \nof this matrix is the polynomial p. We clean up the last column by adding \nto it appropriate multiples of the other columns: \n0 \n0 0 \n0 P \n- 1  \n0 0 \n0 0 \n0 \n- 1  0 \n0 0 \n0 \n0 0 \n0 0 \n0 \n0 0 \n- 1  0 \nMultiply each of the first (n - 1) columns by - 1  and then perform \n(n - 1) interchanges of adjacent columns to bring the present column n \nto the first position. The total effect of the 2n - 2 sign changes is to leave \nthe determinant unaltered. We obtain the matrix \n(7-28) \n[f : :  i} \nIt is then clear that p = \ndet (xl - A). \nWe are going to show that, for any n X n matrix A, there is a suc­\ncession of row and column operations which will transform xl \nA into \na matrix much like (7-28), in which the invariant factors of A appear \ndown the main diagonal. Let us be completely clear about the operations \nwe shall use. \nWe shall be concerned with F[xJmxn, the collection of m X n matrices \nwith entries which are polynomials over the field F. If M is such a matrix, \nan elementary row operation on M is one of the following \n1. multiplication of one row of M by a non-zero scalar in Fi \n'2. replacement of the rth row of M by row r plus f times row 8 ,  where \nf is any polynomial over F and r =;zf 8 ;  \n3. interchange of two rows of M. \nThe inverse operation of an elementary row operation is an elementary \nrow operation of the same type. Notice that we could not make such an \nassertion if we allowed non-scalar polynomials in (1). An m X m ele-\nSec. 7.4 \nComputation of Invariant Factors \nmentary matrix, that is, an elementary matrix in F[x]mXm, is one which \ncan be obtained from the m X m identity matrix by means of a single \nelementary row operation. Clearly each elementary row operation on M \ncan be effected by multiplying M on the left by a suitable m X m ele­\nmentary matrix; in fact, if e is the operation, then \ne(M) = e(I)M.",
    "can be obtained from the m X m identity matrix by means of a single \nelementary row operation. Clearly each elementary row operation on M \ncan be effected by multiplying M on the left by a suitable m X m ele­\nmentary matrix; in fact, if e is the operation, then \ne(M) = e(I)M. \nLet M, N be matrices in F[x]mxn. We say that N is row-equivalent \nto M if N can be obtained from M by a finite succession of elementary \nrow operations: \nM = Mo -+ M1 -+ · · ·  -+ Mk = N. \nEvidently N is row-equivalent to M if and only if M is row-equivalent to \nN, so that we may use the terminology 'M and N are row-equivalent.' \nIf N is row-equivalent to M, then \nN \nPM \nwhere the m X m matrix P is a product of elementary matrices: \nP = El . . .  Ek• \nIn particular, P is an invertible matrix with inverse \np-l = Ekl . . .  Ell. \nOf course, the inverse of Ej comes from the inverse elementary row \noperation. \nAll of this is just as it is in the case of matrices with entries in F. It \nparallels the elementary results in Chapter 1. Thus, the next problem \nwhich suggests itself is to introduce a row-reduced echelon form for poly­\nnomial matrices. Here, we meet a new obstacle. How do we row-reduce \na matrix? The first step is to single out the leading non-zero entry of row 1 \nand to divide every entry of row 1 by that entry. We cannot (necessarily) \ndo that when the matrix has polynomial entries. As we shall see in the \nnext theorem, we can circumvent this difficulty in certain cases; however, \nthere is not any entirely suitable row-reduced form for the general matrix \nin F[x]mXn. If we introduce column operations as well and study the type \nof equivalence which results from allowing the use of both types of oper­\nations, we can obtain a very useful standard form for each matrix. The \nbasic tool is the following. \nLemma. Let M be a matrix in F [x]mXn which has some non-zero entry \nin its first column, and let p be the greatest common divisor of the entries in",
    "ations, we can obtain a very useful standard form for each matrix. The \nbasic tool is the following. \nLemma. Let M be a matrix in F [x]mXn which has some non-zero entry \nin its first column, and let p be the greatest common divisor of the entries in \ncolumn 1 of M. Then M is row-equivalent to a matrix N which has \nas its first column. \n253 \n254. \nThe Rational and Jordan Forms \nChap. 7 \nProof. We shall prove something more than we have stated. \nWe shall show that there is an algorithm for finding N, i.e., a prescription \nwhich a machine could use to calculate N in a finite number of steps. \nFirst, we need some notation. \nLet M be any m X n matrix with entries in F[x] which has a non­\nzero first column \nDefine \n(7-29) \np(M1) \ng.c.d. (h, .\n. . , 1m). \nLet j be some index such that degiJ \nl(Ml). To be specific, let j be \nthe smallest index i for which degfi = l(M}). Attempt to divide each f, \nby !J: \n(7-30) \nJi = fjg. + Ti, \nTi = 0 or deg Ti < degfi, \nFor each i different from j, replace row i of M by row i minus gi times \nrow j. Multiply row j by the reciprocal of the leading coefficient of fj and \nthen interchange rows j and 1 .  The result of all these operations is a matrix \nM' which has for its first column \n(7-31) \nMi = \nwhereJj is the monic polynomial obtained by normalizingiJ to have leading \ncoefficient 1 .  We have given a well-defined procedure for associating with \neach M a matrix M' with these properties. \n(a) M' is row-equivalent to M. \n(b) p(Mi) = P(Ml). \n(c) Either l(MO < l(Ml) or \nM\n[ [\nprJ \nIt is easy to verify (b) and (c) from (7-30) and (7-31). Property (c) \nSec. 7.4 \nComputation of Invariant Factors \nis just another way of stating that either there is some i such that ri ;;z:! 0 \nand deg ri < deg1j or else ri \n0 for all i and Jj is (therefore) the greatest \ncommon divisor of iI, . . .  , 1m. \nThe proof of the lemma is now quite simple. We start with the matrix \nJ1 and apply the above procedure to obtain M'. Property (c) tells us that",
    "and deg ri < deg1j or else ri \n0 for all i and Jj is (therefore) the greatest \ncommon divisor of iI, . . .  , 1m. \nThe proof of the lemma is now quite simple. We start with the matrix \nJ1 and apply the above procedure to obtain M'. Property (c) tells us that \neither M' will serve as the matrix N in the lemma or l(MD < l(Ml) ' In \nthe latter case, we apply the procedure to NI' to obtain the matrix M(2) \n(M'),. If 11/[(2) is not a suitable N, we form M(3) \n= (M(Z) ', and so on. The \npoint is that the strict inequalities \nl(Ml) > l(Mi) > l(Mi2) > . . .  \ncannot continue for very long. After not more than l(Ml) iterations of our \nprocedure, we must arrive at a matrix 11:f(k) which has the properties we \nseek. I \nTheorem 6. Let P be an m X m matrix with entries in the polynomial \nalgebra F [x]. The following are equivalent. \n(i) P is invertible. \n(ii) The determinant of P is a non-zero scalar polynomial. \n(iii) P is row-equivalent to the m X m identity matrix. \n(iv) P is a product of elementary matrices. \nProof. Certainly (i) implies (ii) because the determinant func­\ntion is multiplicative and the only polynomials invertible in F[x] are the \nnon-zero scalar ones. As a matter of fact, in Chapter 5 we used the classical \nadjoint to show that (i) and (ii) are equivalent. Our argument here pro­\nvides a different proof that (i) follows from (ii). We shall complete the \nmerry-go-round \n(i) -+ (ii) \nt \n-!­\n(iv) +- (iii). \nThe only implication which is not obvious is that (iii) follows from (ii). \nAssume (ii) and consider the first column of P. It contains certain \npolynomials Pl, . . . , pm, and \ng.c.d. (PI, . . .  , Pm) = 1 \nbecause any common divisor of PI, . . .  , pm must divide (the scalar) det P. \nApply the previous lemma to P to obtain a matrix \n(7-32) \nB ] \nwhich is row-equivalent to P. An elementary row operation changes the \ndeterminant of a matrix by (at most) a non-zero scalar factor. Thus det Q \n255 \n256 \nThe Rational and Jordan Forms \nChap. 7",
    "Apply the previous lemma to P to obtain a matrix \n(7-32) \nB ] \nwhich is row-equivalent to P. An elementary row operation changes the \ndeterminant of a matrix by (at most) a non-zero scalar factor. Thus det Q \n255 \n256 \nThe Rational and Jordan Forms \nChap. 7 \nis a non-zero scalar polynomial. Evidently the (m \n1) X (m - 1) \nmatrix B in (7-32) has the same determinant as does Q. Therefore, we \nmay apply the last lemma to B. If we continue this way for m steps, we \nobtain an upper-triangular matrix \nwhich is row-equivalent to R. Obviously R is row-equivalent to the m X m \nidentity matrix. \nI \nCorollary. Let M and N be m X n matrices with entries in the poly­\nnomial algebra F[x]. Then N is row-equivalent to M if and only if \nN = PM \nwhere P is an invertible m X m matrix with entries in F [xJ. \nWe now define elementary column operations and column­\nequivalence in a manner analogous to row operations and row-equivalence. \nWe do not need a new concept of elementary matrix because the class of \nmatrices which can be obtained by performing one elementary column \noperation on the identity matrix is the same as the class obtained by \nusing a single elementary row operation. \nDefinition. The matrix N is equivalent to the matrix M if we can \npass from M to N by means of a sequence of operations \nM = Mo -t MI -t . . .  -t Mk = N \neach of which is an elementary row operation or an elementary column \noperation. \nTheorem 7. Let M and N be m X n matrices with entries in the \npolynomial algebra F[xJ. Then N is equivalent to M if and only if \nN = PMQ \nwhere P is an invertible matrix in F [x]mXm and Q is an invertible matrix in \nFfxlnXn. \nTheorem 8. Let A be an n X n matrix with entries in the field F, \nand let PI, . . .  , Pr be the invariant factors for A. The matrix xl - A is \nequivalent to the n X n diagonal matrix with diagonal entries PI, . . .  , Pr, \n1, 1, . .\n. , 1. \nProof. There exists an invertible n X n matrix P, with entries",
    "and let PI, . . .  , Pr be the invariant factors for A. The matrix xl - A is \nequivalent to the n X n diagonal matrix with diagonal entries PI, . . .  , Pr, \n1, 1, . .\n. , 1. \nProof. There exists an invertible n X n matrix P, with entries \nin F, such that P AP-l is in rational form, that is, has the block form \nSec. 7.4 \nComputation of Invariant Factors \nwhere A i  is the companion matrix of the polynomial Pi. According to \nTheorem 7, the matrix \n(7-33) \nP(xI - A)P-I \nxl \nP AP-I \nis equivalent to xl - A. Now [Xl (' \n0 \n(7-34) \nxl \nPAP-l \nxl - A2 \n0 \nxl J \nwhere the various I's we have used are identity matrices of appropriate \nsizes. At the beginning of this section, we showed that xl - Ai is equiv­\nalent to the matrix \n[:' : U \nFrom (7-33) and (7-34) it is then clear that xl - A is equivalent to a \ndiagonal matrix which has the polynomials Pi and (n - r) l's on its main \ndiagonal. By a succession of row and column interchanges, we can arrange \nthose diagonal entries in any order we choose, for example: PI, . .. \n, p\" \n1, . . .  , 1. \nI \nTheorem 8 does not give us an effective way of calculating the ele­\nmentary divisors PI, . . .  , pr because our proof depends upon the cyclic \ndecomposition theorem. We shall now give an explicit algorithm for re­\nducing a polynomial matrix to diagonal form. Theorem 8 suggests that \nwe may also arrange that successive elements on the main diagonal divide \none another. \nDefinition. Let N be a matrix in F [x]mXn. We say that N is in (Smith) \nnormal form if \n(a) every entry off the main diagonal of N is 0; \n(b) on the main diagonal of N there appear (in order) polynomials \nfl' . . .  , f I such that h divides fk+l, 1 S k S l \n1. \nIn the definition, the number l is l = min (m, n). The main diagonal \nentries are /k = Nkk, k \n1, ... \n, l. \nTheorem 9. Let M be an m X n matrix with entries in the polynomial \nalgebra F[xJ. Then M is equivalent to a matrix N which is in normal form. \n257 \n258 \nThe Rational and Jordan Forms \nChap. 7",
    "entries are /k = Nkk, k \n1, ... \n, l. \nTheorem 9. Let M be an m X n matrix with entries in the polynomial \nalgebra F[xJ. Then M is equivalent to a matrix N which is in normal form. \n257 \n258 \nThe Rational and Jordan Forms \nChap. 7 \nProof. If M = 0, there is nothing to prove. If M [ 0, we shall \ngive an algorithm for finding a matrix M' which is equivalent to M and \nwhich has the form \n(7-35) \n['1 ° \nM' = ? \n° \nR \nwhere R is an (m - 1) X (n - 1) matrix and it divides every entry of R. \nWe shall then be finished, because we can apply the same procedure to R \nand obtain f2' etc. \nLet l(M) be the minimum of the degrees of the non-zero entries of M. \nFind the first column which contains an entry with degree l(M) and \ninterchange that column with column 1. Call the resulting matrix M(O). \nWe describe a procedure for finding a matrix of the form \n(7-36) \nwhich is equivalent to M(O), We begin by applying to the matrix M(O) the \nprocedure of the lemma before Theorem 6, a procedure which we shall \ncall PL6. There results a matrix \n(7-37) \nIf the entries a, . . . , b are all 0, fine. If not, we use the analogue of PL6 \nfor the first row, a procedure which we might call PL6'. The result is a \nmatrix \n(7-38) \n[q ° \n' ' \n. \n. \nb' d' \n. . . \n0\n,] \n. . .  \ne \n. . . \nf' \nwhere q \nis the greatest common divisor of p, a, .\n. . , b. In producing M(2), \nwe may or may not have disturbed the nice form of column 1. If we did, \nwe can apply PL6 once again. Here is the point. In not more than l(M) \nsteps: \nM(O) PL6 M(l) PL6' M(2) PL6 . . .\n..... M(t) \nwe must arrive at a matrix M(t) which has the form (7-36), because at \neach successive step we have l(M(k+ll) < l(M(k» .  We name the process \nwhich we have just defined P7-36: \nM(O) - M(I). \nSec. 7.4 \nComputation of Invariant Factors \nIn (7-36), the polynomial g may or may not divide every entry of S. \nIf it does not, find the first column which has an entry not divisible by g",
    "which we have just defined P7-36: \nM(O) - M(I). \nSec. 7.4 \nComputation of Invariant Factors \nIn (7-36), the polynomial g may or may not divide every entry of S. \nIf it does not, find the first column which has an entry not divisible by g \nand add that column to column 1 .  The new first column contains both g \nand an entry gh + r where r ǭ 0 and deg r < deg g. Apply process P7-36 \nand the result will be another matrix of the form (7-36), where the degree \nof the corresponding g has decreased. \nIt should now be obvious that in a finite number of steps we will \nobtain (7-35), i.e., we will reach a matrix of the form (7-36) where the \ndegree of g cannot be further reduced. I \nWe want to show that the normal form associated with a matrix M \nis unique. Two things we have seen provide clues as to how the poly­\nnomials !l, . . .  , !J in Theorem 9 are uniquely determined by M. First, \nelementary row and column operations do not change the determinant \nof a square matrix by more than a non-zero scalar factor. Second, ele­\nmentary row and column operations do not change the greatest common \ndivisor of the entries of a matrix. \nDefinition. Let M be an m X n matrix with entries in F[x]. If \n1 ჴ k ჴ min (m, n), we define 5k(M) to be the greatest common divisor of \nthe determinants of all k X k submatrices of M. \nRecall that a k X k submatrix of M is one obtained by deleting some \nm - k rows and some n - k columns of M. In other words, we select \ncertain k-tuples \nI \n(il, . . .  , ik), \n1 \"5 il < . . . < ik ჵ m \nJ = (jl, . . .  , jk), \n1  j1 < . . . < jk  n \nand look at the matrix formed using those rows and columns of M. We \nare interested in the determinants \n(7-39) \nThe polynomial 5k(M) is the greatest common divisor of the polynomials \nDr,J(M), as I and J range over the possible k-tuples. \nTheorem 10. If M and N are equivalent m X n matrices with entries \nin F [x], then \n(7-40) \n1 ჵ k ჴ min (m, n). \nProof. It will suffice to show that a single elementary row oper­",
    "Dr,J(M), as I and J range over the possible k-tuples. \nTheorem 10. If M and N are equivalent m X n matrices with entries \nin F [x], then \n(7-40) \n1 ჵ k ჴ min (m, n). \nProof. It will suffice to show that a single elementary row oper­\nation e does not change 5k• Since the inverse of e is also an elementary row \noperation, it will suffice to show this: If a polynomial f divides every \nD1,J(M), then ! divides Dl,J(e(M)) for all k-tuples I and J. \n259 \n260 \nThe Rational and Jordan Forms \nChap. 7 \nSince we are considering a row operation, let al, . . .  , am be the rows \nof M and let us employ the notation \nDJ(O!i., . . .  , O!iѕ) = DrAM) . \nGiven I and J, what is the relation between DrAM) and DI.J(e(M»? \nConsider the three types of operations e: \n(a) multiplication of row r by a non-zero scalar c; \n(b) replacement of row r by row r plus g times row 8 ,  r \"'\" 8; \n(c) interchange of rows r and 8, r \"'\" 8. \nForget about type (c) operations for the moment, and concentrate \non types (a) and (b), which change only row r. If r is not one of the indices \nil, .\n•\n. , ik, then \nDu(e(M» \nDuell'f). \nIf r is among the indices il, . . .  , ik, then in the two cases we have \n(a) Dr,J(e(M» = DJ(ail) . . . , car, . . .  , a;.) \n= cD J( ail) . . .  , a\" . . .  , O!ik) \n= cDr,J(M) ; \n(b) DrAe(M» = DJ(ail) . . .  , aT + gas, . . .  , ai.) \nDr,J(M) + gDJ(a;l) . . .  , a., . . .  , O!i.). \nFor type (a) operations, it is clear that any f which divides DrAM) \nalso divides DrAe(M» . For the case of a type (c) operation, notice that \nDJ(O!iIl . . .  , a8, •\n•\n•\n , a;.) = 0, \nif 8 \n= ij for some j \nDAO!iu .\n.\n.\n , a., . . . , O!i.) = ±D/ AM) , \nif s \"'\" i; for all j. \nThe I' in the last equation is the k-tuple (il' . . .  , 8, . . .  , ik) arranged in \nincreasing order. It should now be apparent that, if f divides every Dr ,J(M), \nthen f divides every DrAe(M» . \nOperations of type (c) can be taken care of by roughly the same",
    "if s \"'\" i; for all j. \nThe I' in the last equation is the k-tuple (il' . . .  , 8, . . .  , ik) arranged in \nincreasing order. It should now be apparent that, if f divides every Dr ,J(M), \nthen f divides every DrAe(M» . \nOperations of type (c) can be taken care of by roughly the same \nargument or by using the fact that such an operation can be effected by \na sequence of operations of types (a) and Cb) . \nI \nCorollary. Each matrix M in F[xJrnxn is equivalent to precisely one \nmatrix N which i8 in normal form. The polynomials fl' . , . , fl which occur \non the main diagonal of N are \n9MM) \nOk_1CM)' \n1 :::; k :::; min (m, n) \nwhere, for convenience, we define ooCM) = 1. \nProof. If N is in normal form with diagonal entries fl' . . .  , Jz, \nit is quite easy to see that \nok(N) = fJ2 . . . fk. I \nSec. 7.4 \nComputation of Invariant Factors \nOf course, we call the matrix N in the last corollary the norlllal form \nof M. The polynomials fI' . . .  , lz are often called the invariant factors \nof M. \nSuppose that A is an n X n matrix with entries in F, and let PI, .\n. . , pr \nbe the invariant factors for A. We now see that the normal form of the \nmatrix xl \nA has diagonal entries 1, 1, . . .  , 1, p\" . . . , Pl. The last \ncorollary tells us what PI, . . .  , pr are, in terms of submatrices of xl - A. \nThe number n - r is the largest k such that ჶk(xI - A) = 1. The minimal \npolynomial PI is the characteristic polynomial for A divided by the greatest \ncommon divisor of the determinants of all (n - 1) X (n - 1) submatrices \nof xl - A, etc. \nExercises \n1. True or false? Every matrix in F[xJnxn is row-equivalent to an upper-triangular \nmatrix. \n2. Let T be a linear operator on a finite-dimensional vector space and let A be \nthe matrix of T in some ordered basis. Then T has a cyclic vector if and only if \nthe determinants of the (n - 1) X (n - 1) submatrices of xl - A are relatively \nprime. \n3. Let A be an n X n matrix with entries in the field F and let fl' . . .  , fn be the",
    "the matrix of T in some ordered basis. Then T has a cyclic vector if and only if \nthe determinants of the (n - 1) X (n - 1) submatrices of xl - A are relatively \nprime. \n3. Let A be an n X n matrix with entries in the field F and let fl' . . .  , fn be the \ndiagonal entries of the normal form of xl - A. For which matrices A is fl :F- I? \n4. Construct a linear operator T with minimal polynomial X2(X - 1)2 and charac­\nteristic polynomial x3(x - 1)4. Describe the primary decomposition of the vector \nspace under T and find the projections on the primary components. Find a basis \nin which the matrix of T is in Jordan form. Also find an explicit direct sum decom­\nposition of the space into T-cyclic subspaces as in Theorem 3 and give the invariant \nfactors. \n5. Let T be the linear operator on RB which is represented in the standard \nbasis by the matrix \n1 \n1 \n1 \n1 \n1 \n1 \n1 \n1 \n0 \n0 \n0 \n0 \n0 0 0 \n1 \n0 \n0 \n0 \n0 \n0 0 0 \n- 1  \nA =  0 \n1 \n1 \n0 \n0 0 0 \n1 \n0 \n0 \n0 \n1 \n1 0 0 \n0 \n0 \n1 \n1 \n1 \n1 \n1 0 \n1 \n0 - 1  - 1  - 1  - 1  0 1 \n- 1  \n0 \n0 \n0 \n0 \n0 0 0 \n0 \n(a) Find the characteristic polynomial and the invariant factors. \n(b) Find the primary decomposition of RB under T and the projections on \nthe primary components. Find cyclic decompositions of each primary component \nas in Theorem 3. \n261 \n262 \nThe Rational and Jordan Forms \nChap. 7 \n(c) Find the Jordan form of A. \n(d) Find a direct-sum decomposition of R8 into T-cyclic subspaces as in \nTheorem 3. (Hint: One way to do this is to use the results in (b) and an appropriate \ngeneralization of the ideas discussed in Example 4.) \n7.5. Summary; Semi-Simple Operators \nIn the last two chapters, we have been dealing with a single linear \noperator T on a finite-dimensional vector space V. The program has been \nto decompose T into a direct sum of linear operators of an elementary \nnature, for the purpose of gaining detailed information about how T \n'operates' on the space V. Let us review briefly where we stand.",
    "operator T on a finite-dimensional vector space V. The program has been \nto decompose T into a direct sum of linear operators of an elementary \nnature, for the purpose of gaining detailed information about how T \n'operates' on the space V. Let us review briefly where we stand. \nWe began to study T by means of characteristic values and charac­\nteristic vectors. We introduced diagonalizable operators, the operators \nwhich can be completely described in terms of characteristic values and \nvectors. We then observed that T might not have a single characteristic \nvector. Even in the case of an algebraically closed scalar field, when every \nlinear operator does have at least one characteristic vector, we noted that \nthe characteristic vectors of T need not span the space. \nWe then proved the cyclic decomposition theorem, expressing any \nlinear operator as the direct sum of operators with a cyclic vector, with \nno assumption about the scalar field. If U is a linear operator with a cyclic \nvector, there is a basis {aI, . . .  , an} with \nUaj = aj+l, \nj = 1, . . .  , n - 1 \nU an = \nCOal - Cla2 -\n•\n.\n.\n - Cn-Ian• \nThe action of U on this basis is then to shift each aj to the next vector \na;+1, except that U a\" is some prescribed linear combination of the vectors \nin the basis. Since the general linear operator T is the direct sum of a \nfinite number of such operators U, we obtained an explicit and reasonably \nelementary description of the action of T. \nWe next applied the cyclic decomposition theorem to nilpotent \noperators. For the case of an algebraically closed scalar field, we combined \nthis with the primary decomposition theorem to obtain the Jordan form. \nThe Jordan form gives a basis {aI, . . . , an} for the space V such that, \nfor each j, either Taj is a scalar multiple of a; or Ta; = caj + a;+!. Such \na basis certainly describes the action of T in an explicit and elementary \nmanner. \nThe importance of the rational form (or the Jordan form) derives",
    "for each j, either Taj is a scalar multiple of a; or Ta; = caj + a;+!. Such \na basis certainly describes the action of T in an explicit and elementary \nmanner. \nThe importance of the rational form (or the Jordan form) derives \nfrom the fact that it exists, rather than from the fact that it can be com­\nputed in specific cases. Of course, if one is given a specific linear operator \nT and can compute its cyclic or Jordan form, that is the thing to do; \nfor, having such a form, one can reel off vast amounts of information \nSec. 7 . .5 \nSummary; Semi-Simple Operators \nabout T. Two different types of difficulties arise in the computation of \nsuch standard forms. One difficulty is, of course, the length of the com­\nputations. The other difficulty is that there may not be any method for \ndoing the computations, even if one has the necessary time and patience. \nThe second difficulty arises in, say, trying to find the Jordan form of a \ncomplex matrix. There simply is no well-defined method for factoring the \ncharacteristic polynomial, and thus one is stopped at the outset. The \nrational form does not suffer from this difficulty. As we showed in Section \n7.4, there is a well-defined method for finding the rational form of a given \nn X n matrix; however, such computations are usually extremely lengthy. \nIn our summary of the results of these last two chapters, we have not \nyet mentioned one of the theorems which we proved. This is the theorem \nwhich states that if T is a linear operator on a finite-dimensional vector \nspace over an algebraically closed field, then T is uniquely expressible as \nthe sum of a diagonalizable operator and a nilpotent operator which \ncommute. This was proved from the primary decomposition theorem and \ncertain information about diagonalizable operators. It is not as deep a \ntheorem as the cyclic decomposition theorem or the existence of the \nJordan form, but it does have important and useful applications in certain",
    "commute. This was proved from the primary decomposition theorem and \ncertain information about diagonalizable operators. It is not as deep a \ntheorem as the cyclic decomposition theorem or the existence of the \nJordan form, but it does have important and useful applications in certain \nparts of mathematics. In concluding this chapter, we shall prove an \nanalogous theorem, without assuming that the scalar field is algebraically \nclosed. We begin by defining the operators which will play the role of the \ndiagonalizable operators. \nDefinition. Let V be a finite-dimensional vector space over the field F, \nand let T be a linear operator on V. We say that T is semi-simple if every \nT -invariant subspace has a complementary T-invariant subspace. \nWhat we are about to prove is that, with some restriction on the \nfield F, every linear operator T is uniquely expressible in the form T \nS + N, where S is semi-simple, N is nilpotent, and SN = NS. First, \nwe are going to characterize semi-simple operators by means of their \nminimal polynomials, and this characterization will show us that, when F \nis algebraically closed, an operator is semi-simple if and only if it is \ndiagonalizable. \nLemma. Let T be a linear operator on the finite-dimensional vector \nspace V, and let V = WI EB . . .  EB Wk be the primary decomposition for T. \nIn other words, if p is the minimal polynomial for T and p = p'i' . . .  pჷk is \nthe prime factorization of p, then Wj is the null space of pj(T)ri• Let W be \nany subspace of V which is invariant under T. Then \nW = (W n WI) EB · · ·  EB (W n Wk) \nProof. For the proof we need to recall a corollary to our proof \nof the primary decomposition theorem in Section 6.8. If El, •\n•\n.\n , Ek are \n263 \n264 \nThe Rational and Jordan Forms \nChap. 7 \nthe projections associated with the decomposition V = W1 ED . . .  ED Wk, \nthen each Ej is a polynomial in T. That is, there are polynomials hl' . . . , hk \nsuch that Ej = hj(T).",
    "•\n.\n , Ek are \n263 \n264 \nThe Rational and Jordan Forms \nChap. 7 \nthe projections associated with the decomposition V = W1 ED . . .  ED Wk, \nthen each Ej is a polynomial in T. That is, there are polynomials hl' . . . , hk \nsuch that Ej = hj(T). \nNow let W be a subspace which is invariant under T. If a is any \nvector in W, then a = al + ' \"  + ak, where aj is in Wj. Now aj = Eja = \nhj(7')a, and since W is invariant under T, each aj is also in W. Thus each \nvector a in W is of the form a = al + . . .  + ak, where aj is in the inter­\nsection W n Wj. This expression is unique, since V = W1 ED . . .  ED Wk. \nTherefore \nLemma. Let T be a linear operator on V, and suppose that the minimal \npolynomial for T is irreducible over the scalar field F. Then T is semi-simple. \nProof. Let W be a subspace of V which is invariant under T. \nWe must prove that W has a complementary T-invariant subspace. \nAccording to a corollary of Theorem 3, it will suffice to prove that if f is \na polynomial and (3 is a vector in V such that f(T)(3 is in W, then there is \na vector a in W with f( T)(3 = f( T)a. So suppose (3 is in V and f is a poly­\nnomial such that f(T)(3 is in W. If f(T)fj = 0, we let a = ° and then a is a \nvector in W with f(T)fj \nf(T)a. If f(T)(3 ¢ 0, the polynomial f is not \ndivisible by the minimal polynomial p of the operator T. Since p is prime, \nthis means that f and p are relatively prime, and there exist polynomials \ng and h such that fg + ph \n1. Because peT) \n0, we then have \nf(T)g(T) \nI. From this it follows that the vector fj must itself be in the \nsubspace W j for \n(3 = g(T)f(T)fj \n= g(T) (f(T){3) \nwhile f(T)fj is in W and W is invariant under T. Take a \nfj. \nI \nTheorem 11. Let T be a linear operator on the finite-dimensional vector \nspace V. A necessary and sufficient condition that T be semi-simple is that \nthe minimal polynomial pfor T be ojtheform p = Pl ' \"  pk, where pl, . . .  , Pk \nare distinct irreducible polynomials over the scalar field F.",
    "I \nTheorem 11. Let T be a linear operator on the finite-dimensional vector \nspace V. A necessary and sufficient condition that T be semi-simple is that \nthe minimal polynomial pfor T be ojtheform p = Pl ' \"  pk, where pl, . . .  , Pk \nare distinct irreducible polynomials over the scalar field F. \nProof. Suppose T is semi-simple. We shall show that no irre­\nducible polynomial is repeated in the prime factorization of the minimal \npolynomial p. Suppose the contrary. Then there is some non-scalar monic \npolynomial g such that g2 divides p. Let W be the null space of the oper­\nator g(7'). Then W is invariant under T. Now p = g2h for some poly­\nnomial h. Since g is not a scalar polynomial, the operator g(T)h(T) is not \nthe zero operator, and there is some vector (3 in V such that g(T)h(T){3 ¢ 0, \ni.e., (gh){3 ¢ 0. Now (gh)fj is in the subspace W, since g(gh{3) = g2h{3 = \np(3 \n0. But there is no vector a in W such that ghfj \nghaj for, if a is in W \n(gh)a \n(hg) a \nh(ga) \nh(O) \nO. \nSec. 7.5 \nSummary; Semi-Simple Operators \nThus, W cannot have a complementary T-invariant subspace, contra­\ndicting the hypothesis that T is semi-simple. \nN ow suppose the prime factorization of p is p \nPl ' . . Pk, where \nPI, . . .  , Pk are distinct irreducible (non-scalar) monic polynomials. Let \nW be a subspace of V which is invariant under T. We shall prove that W· \nhas a complementary T-invariant subspace. Let V \nWI EB . . .  EB Wk \nbe the primary decomposition for T, i.e., let Wj be the null space of p/T). \nLet Ti be the linear operator induced on Wj by T, so that the minimal \npolynomial for 'Pj is the prime Pj. Now W n Wj is a subspace of Wj which \nis invariant under Tj (or under T). By the last lemma, there is a subspace \nVi of Wj such that Wi = (W n Wj) EB Vj and Vj is invariant under Tj \n(and hence under T). Then we have \nV = WI EB . . .  EB Wk \n(W n WI) EB VI EB . . .  EB (W n Wk) EB Vk \n= (W n WI) + . . . + (W n Wk) EB VI EB . . . EB Vk•",
    "Vi of Wj such that Wi = (W n Wj) EB Vj and Vj is invariant under Tj \n(and hence under T). Then we have \nV = WI EB . . .  EB Wk \n(W n WI) EB VI EB . . .  EB (W n Wk) EB Vk \n= (W n WI) + . . . + (W n Wk) EB VI EB . . . EB Vk• \nBy the first lemma above, W = (W n WI) EB . . .  EB (W n Wk), so that \nif W' = VI EB . . .  EB Vk, then V = W EB  W' and W' is invariant under \nT. I \nCorollary. If T is a linear operator on a finite-dimensional vector space \nover an algebraically closed field, then T is semi-simple if and only if T is \ndiagonalizable. \nProof. If the scalar field F is algebraically closed, the monic \nprimes over F are the polynomials x - c. In this case, T is semi-simple \nif and only if the minimal polynomial for T is P \n(x - CI) . . . (x - Ck), \nwhere CI, •\n•\n•\n , Ck are distinct elements of F. This is precisely the criterion \nfor T to be diagonalizable, which we established in Chapter 6. I \nWe should point out that T is semi-simple if and only if there is some \npolynomial j, which is a product of distinct primes, such that f(T) = O. \nThis is only superficially different from the condition that the minimal \npolynomial be a product of distinct primes. \nWe turn now to expressing a linear operator as the sum of a semi­\nsimple operator and a nilpotent operator which commute. In this, we \nshall restrict the scalar field to a subfield of the complex numbers. The \ninformed reader will see that what is important is that the field F be a \nfield of characteristic zero, that is, that for each positive integer n the \nsum 1 + . . .  + 1 (n times) in F should not be O. For a polynomial j over \nF, we denote by j(k) the kth formal derivative of f. In other words, \nJ<k) = Dkj, where D is the differentiation operator on the space of poly­\nnomials. If g is another polynomial, f(g) denotes the result of substituting \ng in j, i.e., the polynomial obtained by applying f to the element g in the \nlinear algebra F[x]. \n265 \n266 \nThe Rational and Jordan Forms \nOhap. 7",
    "nomials. If g is another polynomial, f(g) denotes the result of substituting \ng in j, i.e., the polynomial obtained by applying f to the element g in the \nlinear algebra F[x]. \n265 \n266 \nThe Rational and Jordan Forms \nOhap. 7 \nLemma (Taylor's Formula). Let F be a field of characteristic zero \nand let g and h be polynomials over F. If f is any polynomial over F with \ndeg f :::; n, then \nf(g) = f(h) + f(l)(h)(g - h) + f(2)(h) (g \n_ h)2 + . . .  + f(n)(h) (g _ h)n. \n2 !  \nn !  \nProof. What we are proving is a generalized Taylor formula. The \nreader is probably used to seeing the special case in which h = c, a scalar \npolynomial, and g = x. Then the formula says \nf = f(x) = f(c) + 1(1) (c) (x - c) \nJC2)(C) \n1(n) (c) \n+ -\n2 '  - (x - C)2 + . . . + -,- (x - c)n. \n. \nn. \nThe proof of the general formula is just an application of the binomial \ntheorem \n(a + b)k = ak + kak-1b + k(k \n2 1) \nak-2b2 + . . . + bk• \nFor the reader should see that, since substitution and differentiation are \nlinear processes, one need only prove the formula when 1 = Xk. The for-\nn \nmula for f =  CkXk follows by a linear combination. In the case f = Xk \nk=O \nwith k :::; n, the formula says \ngk = hk + khk-I(g \nh) + k(k \n2 1) \nhk-2(g - h)2 + . . .  + (g \nh)k \nwhich is just the binomial expansion of \ngk \n[h + (g \nh)]k. I \nLemma. Let F be a sub field of the complex numbers, let f be a poly-\nnomial over F, and let fl be the derivative of f. The following are equivalent: \n(a) f is the product of distinct polynomials irreducible over F. \n(b) f and f' are relatively prime. \n(c) As a polynomial with complex coefficients, f has no repeated root. \nProof. Let us first prove that (a) and (b) are equivalent state­\nments about f. Suppose in the prime factorization of f over the field F that \nsome (non-scalar) prime polynomial p is repeated. Then f = p2h for some \nh in F[x]. Then \nf' = pW + 2pp'h \nand p is also a divisor of f'. Hence f and l' are not relatively prime. We \nconclude that (b) implies (a).",
    "ments about f. Suppose in the prime factorization of f over the field F that \nsome (non-scalar) prime polynomial p is repeated. Then f = p2h for some \nh in F[x]. Then \nf' = pW + 2pp'h \nand p is also a divisor of f'. Hence f and l' are not relatively prime. We \nconclude that (b) implies (a). \nNow suppose 1 = PI . . . Pk, where PI, •\n.\n. , Pk are distinct non-scalar \nirreducible polynomials over F. Let fj \n= f /Pi' Then \nf' = p;/I + Pƫf2 + . . .  + p£k \nSec. 7.5 \nSummary; Semi-Simple Operators \nLet p be a prime polynomial which divides both f and f'. Then p = Pi for \nsome i. Now Pi divides Ii for j r'\" i, and since Pi also divides \nk \nf' = 2: pili \nj = l  \nwe see that Pi must divide PUi. Therefore Pi divides either fi or p;. But Pi \ndoes not divide fi since PI, . . .  , Pk are distinct. So Pi divides p;. This is \nnot possible, since P; has degree one less than the degree of Pi. We con­\nelude that no prime divides both f and f', or that (f, f') \n1. \nTo see that statement (c) is equivalent to (a) and (b), we need only \nobserve the following: Suppose f and g are polynomials over F, a subfield \nof the complex numbers. We may also regard f and g as polynomials with \ncomplex coefficients. The statement that f and g are relatively prime as \npolynomials ovcr F is equivalent to the statement that f and g are rela­\ntively prime as polynomials over the field of complex numbers. We leave \nthe proof of this as an exercise. We use this fact with g \nf'. Note that \n(c) is just (a) when f is regarded as a polynomial over the field of complex \nnumbers. Thus (b) and (c) are equivalent, by the same argument that \nwe used above. I \nWe can now prove a theorem which makes the relation between semi­\nsimple operators and diagonalizable operators even more apparent. \nTheorem 12. Let F be a subfield of the field of complex numbers, let V \nbe a finite-dimensional vector space over F, and let T be a linear operator on",
    "We can now prove a theorem which makes the relation between semi­\nsimple operators and diagonalizable operators even more apparent. \nTheorem 12. Let F be a subfield of the field of complex numbers, let V \nbe a finite-dimensional vector space over F, and let T be a linear operator on \nV. Let il be an ordered basis for V and let A be the matrix of T in the ordered \nbasis il. Then T is semi-simple if and only if the matrix A is similar over the \nfield of complex numbers to a diagonal matrix. \nProof. Let p be the minimal polynomial for T. According to \nTheorem 1 1, T is semi-simple if and only if p = PI . . . Pk where Pl, . . . , Pk \nare distinct irreducible polynomials over F. By the last lemma, we see \nthat T is semi-simple if and only if p has no repeated complex root. \nNow p is also the minimal polynomial for the matrix A. We know \nthat A is similar over the field of complex numbers to a diagonal matrix \nif and only if its minimal polynomial has no repeated complex root. This \nproves the theorem. I \nTheorem 13. Let F be a subfield of the field of complex numbers, let V \nbe a finite-dimensional vector space over F, and let T be a linear operator on V. \nThere is a semi-simple operator S on V and a nilpotent operator N on V such \nthat \n(i) T = S + N i  \n(ii) S N  \nNS. \n267 \n268 \nThe Rational and Jordan Forms \nChap. 7 \nFurthermore, the semi-simple S and nilpotent N satisfying (i) and (ii) are \nunique, and each is a polynomial in T. \nProof. Let pi' . . .  pit be the prime factorization of the minimal \npolynomial for T, and letf \nPl ' . . Pko Let r be the greatest of the positive \nintegers rl, . . .  , rk. Then the polynomial f is a product of distinct primes, \nl' is divisible by the minimal polynomial for T, and so \nf(T)' = O. \nWe are going to construct a sequence of polynomials: go, gI, g2, . . . \nsuch that \nis divisible by r+!, n = 0, 1, 2, . . . .  We take go = 0 and thenf(x - gof) = \nf(x) = f is divisible by f. Suppose we have chosen go, . . .  , gn-I. Let \nn-l",
    "f(T)' = O. \nWe are going to construct a sequence of polynomials: go, gI, g2, . . . \nsuch that \nis divisible by r+!, n = 0, 1, 2, . . . .  We take go = 0 and thenf(x - gof) = \nf(x) = f is divisible by f. Suppose we have chosen go, . . .  , gn-I. Let \nn-l \nh = x -  gjfi \nj=O \nso that, by assumption, f(h) is divisible by fn. We want to choose gn so that \nf(h - gnin) \nis divisible by fn+!. We apply the general Taylor formula and obtain \nwhere b is some polynomial. By assumption f(h) \nqfn. Thus, we see that \nto have f(h \ngnin) divisible by r+! we need only choose gn in such a way \nthat (q - gn!') is divisible by f. This can be done, because f has no re­\npeated prime factors and so f and f' are relatively prime. If a and e are \npolynomials such that af + ef' \n1, and if we let gn \neq, then q - gnf' \nis divisible by f. \nNow we have a sequence go, gl, . . .  such that fn+! divides \nf (x - ,i; gdi). Let us take n = r - 1 and then since f(T)' = 0 \n3 = 0 \nLet \nn \nf (T :ǰ: gj(T)f(T)i) \nO. \nr-l \nr-1 \nN =  gj(T)f(T)i =  gj(T)f(T)i. \nj = l  \nj = O  \nSince  gjfi is divisible by f, we see that Nr = 0 and N is nilpotent. Let \nj = l  \nS = T \nN. Then f(S) \nf(T - N) = O. Since f has distinct prime \nfactors, S is semi-simple. \nNow we have T \nS + N where S is semi-simple, N is nilpotent, \nand each is a polynomial in T. To prove the uniqueness statement, we \nSec. 7.5 \nSummary; Semi-Simple Operators \nshall pass from the scalar field F to the field of complex numbers. Let il \nbe some ordered basis for the space V. Then we have \n[T](\\ = [S](\\ + [N](\\ \nwhile [8J(\\ is diagonalizable over the complex numbers and [NJ(\\ is nil­\npotent. This diagonalizable matrix and nilpotent matrix which commute \nare uniquely determined, as we have shown in Chapter 6. \nI \nExercises \n1. If N is a nilpotent linear operator on V, show that for any polynomial f the \nsemi-simple part of f(N) is a scalar multiple of the identity operator (F a subfield \nof C).",
    "are uniquely determined, as we have shown in Chapter 6. \nI \nExercises \n1. If N is a nilpotent linear operator on V, show that for any polynomial f the \nsemi-simple part of f(N) is a scalar multiple of the identity operator (F a subfield \nof C). \n2. Let F be a subfield of the complex numbers, V a finite-dimensional vector \nspace over F, and T a semi-simple linear operator on V. If f is any polynomial \nover F, prove that f(T) is semi-simple. \n3. Let T be a linear operator on a finite-dimensional space over a subfield of C. \nProve that T is semi-simple if and only if the following is true: If f is a polynomial \nand f(T) is nilpotent, then f(T) = O. \n269 \n8. Inner Product \nSpaces \nB.l . Inner Products \nThroughout this chapter we consider only real or complex vector \nspaces, that is, vector spaces over the field of real numbers or the field of \ncomplex numbers. Our main object is to study vector spaces in which it \nmakes sense to speak of the 'length' of a vector and of the 'angle' between \ntwo vectors. We shall do this by studying a certain type of scalar-valued \nfunction on pairs of vectors, known as an inner product. One example of \nan inner product is the scalar or dot product of vectors in R3. The scalar \nproduct of the vectors \na = (Xl, X2, Xa) and {3 = (Yl, Y2, Ys) \nin R3 is the real number \n(al{3) = XIYI \nX2Y2 \nXaY3. \nGeometrically, this dot product is the product of the length of a, the \nlength of (3, and the cosine of the angle between a and (3. It is therefore \npossible to define the geometric concepts of 'length' and 'angle' in R3 by \nmeans of the algebraically defined scalar product. \nAn inner product on a vector space is a function with properties \nsimilar to the dot product in R3, and in terms of such an inner product \none can also define 'length' and 'angle.' Our comments about the general \nnotion of angle will be restricted to the concept of perpendicularity (or \northogonality) of vectors. In this first section we shall say what an inner",
    "similar to the dot product in R3, and in terms of such an inner product \none can also define 'length' and 'angle.' Our comments about the general \nnotion of angle will be restricted to the concept of perpendicularity (or \northogonality) of vectors. In this first section we shall say what an inner \nproduct is, consider some particular examples, and establish a few basic \n1370 \nSec. 8.1 \nInner Products \nproperties of inner products. Then we turn to the task of discussing length \nand orthogonality. \nDefinition. Let F be the field of real numbers or the field of complex \nnumbers, and V a vector space over F. An inner product on V is a function \nwhich assigns to each ordered pair of vectors a, (3 in V a scalar (al(3) in F in \nsuch a way that for all a, (3, \"I in V and all scalars c \n(a) (a 1- (31\"1) \n(al\"l) 1- «(31\"1) ;  \n(b) (cal(3) = c(al(3) ; \n(c) ({3la) = (৓), the bar denoting complex conjugation; \n(d) (ala) > 0 if a ˻ O. \nIt should be observed that conditions (a), (b), and (c) imply that \n(e) \n(alc(3 1- \"I) = c(al{3) 1- (al,,)· \nOne other point should be made. When F is the field R of real numbers, \nthe complex conjugates appearing in (c) and (e) are superfluous; however, \nin the complex case they are necessary for the consistency of the condi­\ntions. Without these complex conjugates, we would have the contradiction: \n(ala) > 0 and (ialia) = - l (ala) > o. \nIn the examples that follow and throughout the chapter, F is either \nthe field of real numbers or the field of complex numbers. \nEXAMPLE 1. On Fn there is an inner product which we call the \nstandard inner product. It is defined on a = (Xl, . . .  , xn) and {3 = \n(YI, . . . , Yn) by \n(8-1) \n(al{3) =  x/fh· \ni \nWhen F = R, this may also be written \nIn the real case, the standard inner product is often called the dot or \nscalar product and denoted by a . (3. \nEXAMPLE 2. For a = (Xl, X2) and {3 = (YI, Y2) in R2, let \n(aim = XIYI - X2YI \nXIY2 + 4X2Y2' \nSince (ala) = (Xl - X2)2",
    "(8-1) \n(al{3) =  x/fh· \ni \nWhen F = R, this may also be written \nIn the real case, the standard inner product is often called the dot or \nscalar product and denoted by a . (3. \nEXAMPLE 2. For a = (Xl, X2) and {3 = (YI, Y2) in R2, let \n(aim = XIYI - X2YI \nXIY2 + 4X2Y2' \nSince (ala) = (Xl - X2)2 \n3xӦ, it follows that (ala) > 0 if a ˼ O. Condi-\ntions (a), (b), and (c) of the definition are easily verified. \nEXAMPLE 3. Let V be Fnxn, the space of all n X n matrices over F. \nThen V is isomorphic to Fn' in a natural way. It therefore follows from \nExample 1 that the equation \n(AlB) =  Aj.l::Bjk \nj,k \n271 \n272 \nInner Product Spaces \nChap. 8 \ndefines an inner product on V. Furthermore, if we introduce the conjugate \ntranspose matrix B*, where Btj \nBik' we may express this inner product \non FnXn in terms of the trace function :  \nFor \n(A IB) \ntr (AB*) \ntr (B* A). \ntr (AB*) =  (AB*)jj \ni \nEXAMPLE 4. Let FnXI be the space of n X 1 (column) matrices over \nF, and let Q be an n X n invertible matrix over F. For X, Y in FnXI set \n(XI Y) \n= Y*Q*QX. \nWe are identifying the 1 X 1 matrix on the right with its single entry. \nWhen Q is the identity matrix, this inner product is essentially the same \nas that in Example 1 ;  we call it the standard inner product on FnXI. \nThe reader should note that the terminology 'standard inner product' is \nused in two special contexts. For a general finite-dimensional vector space \nover F, there is no obvious inner product that one may call standard. \nEXAMPLE 5. Let V be the vector space of all continuous complex­\nvalued functions on the unit interval, 0 ::; t ::; 1. Let \n(fIg) 101 f(t)g(t) dt. \nThe reader is probably more familiar with the space of real-valued con­\ntinuous functions on the unit interval, and for this space the complex \nconjugate on g may be omitted. \nEXAMPLE 6. This is really a whole class of examples. One may con­\nstruct new inner products from a given one by the following method.",
    "tinuous functions on the unit interval, and for this space the complex \nconjugate on g may be omitted. \nEXAMPLE 6. This is really a whole class of examples. One may con­\nstruct new inner products from a given one by the following method. \nLet V and W be vector spaces over F and suppose ( I ) is an inner product \non W. If T is a non-singular linear transformation from V into W, then \nthe equation \npT(a, (3) \n(TaI T{3) \ndefines an inner product PT on V. The inner product in Example 4 is a \nspecial case of this situation. The following are also special cases. \n(a) Let V be a finite-dimensional vector space, and let \nil = {ai, . . .  , a,,} \nSec. 8.1 \nInner Products \nbe an ordered basis for V. Let fl, •\n•\n•\n , En be the standard basis vectors in \nFn, and let T be the linear transformation from V into Fn such that Taj = \nE;, j = 1, .\n. . , n. In other words, let T be the 'natural' isomorphism of V \nonto Fn that is determined by ffi. If we take the standard inner product \non Fn, then \nn \nPT('J:, Xjaj, J:, Ykak) = J:, x/fh \nj \nk \nj = 1  \nThus, for any basis for V there is an inner product on V with the property \n(ajlak) = Ojk; in fact, it is easy to show that there is exactly one such \ninner product. Later we shall show that every inner product on V is \ndetermined by some basis ffi in the above manner. \n(b) We look again at Example 5 and take V = W, the space of \ncontinuous functions on the unit interval. Let T be the linear operator \n'multiplication by t,' that is, (TJ) (t) = tJ(t) , 0 0 t 0 l. It is easy to see \nthat T is linear. Also T is non-singular; for suppose TJ = O. Then tJ(t) = 0 \nfor 0  t ყ 1 ;  hence J(t) = 0 for t > O. Since J is continuous, we have \nJ(O) = 0 as well, or f = O. Now using the inner product of Example 5, \nwe construct a new inner product on V by setting \nPT(f, g) = 101 (TJ) (t) (Tg) (t) dt \n= 101 J(t)g(tW dt. \nWe turn now to some general observations about inner products.",
    "J(O) = 0 as well, or f = O. Now using the inner product of Example 5, \nwe construct a new inner product on V by setting \nPT(f, g) = 101 (TJ) (t) (Tg) (t) dt \n= 101 J(t)g(tW dt. \nWe turn now to some general observations about inner products. \nSuppose V is a complex vector space with an inner product. Then for all \na, f3 in V \n(alf3) = Re (alf3) + i 1m (alf3) \nwhere Re (alf3) and 1m (alf3) are the real and imaginary parts of the \ncomplex number (aim. If z is a complex number, then 1m (z) = Re (-iz). \nIt follows that \n1m (aim = Re [-i(alf3)] \n= Re (alif3). \nThus the inner product is completely determined by its 'real part' In \naccordance with \n(8-2) \n(alf3) \nRe (alf3) + i Re (alif3). \nOccasionally it is very useful to know that an inner product on a real \nor complex vector space is determined by another funetion, the so-called \nquadratic form determined by the inner product. To define it, we first \ndenote the positive square root of (ala) by Iiall ;  I iall is called the norll \nof a with respect to the inner product. By looking at the standard inner \nproducts in Rl, Cl, R2, and R3, the reader should be able to convince him­\nself that it is appropriate to think of the norm of a as the 'length' or \n'magnitude' of a. The quadratic forll determined by the inner product \n273 \n274 \nInner Product Spaces \nChap. 8 \nis the function that assigns to each vector a the scalar Ilal/2• It follows \nfrom the properties of the inner product that \nI/a ± fJI/2 = Iial /2 ± 2 Re (alfJ) + I IfJW \nfor all vectors a and fJ. Thus in the real case \n(8-3) \n1 \n1 \n(aim = 4 \"a + 131/2 - 4 \"a - fJ1 I2. \nIn the complex case we use (8-2) to obtain the more complicated expression \n(8-4) \n(aim = ! l Ia + 131 12 - ! I/a - 13112 + i lIa + ifJII2 - i l Ia - ii3W \n4 \n4 \n4 \n4 \n. \nEquations (8-3) and (8-4) are called the polarization identities. Note \nthat (8-4) may also be written as follows: \n(ali3) = ! i i\" lIa + ini3l12• \n4 n = 1  \nThe properties obtained above hold for any inner product on a real",
    "4 \n4 \n4 \n4 \n. \nEquations (8-3) and (8-4) are called the polarization identities. Note \nthat (8-4) may also be written as follows: \n(ali3) = ! i i\" lIa + ini3l12• \n4 n = 1  \nThe properties obtained above hold for any inner product on a real \nor complex vector space V, regardless of its dimension. We turn now to \nthe case in which V is finite-dimensional. As one might guess, an inner \nproduct on a finite-dimensional space may always be described in terms \nof an ordered basis by means of a matrix. \nSuppose that V is finite-dimensional, that \nil = {aI, . . . , an} \nis an ordered basis for V, and that we are given a particular inner product \non V; we shall show that the inner product is completely determined by \nthe values \n(8-5) \nit assumes on pairs of vectors in il. If a = .2; Xkak and 13 = .2; Yjaj, then \nk \nj \n= Y*GX \nwhere X, Y are the coordinate matrices of a, 13 in the ordered basis il, \nand G is the matrix with entries Gjk = (aklaj). We call G the matrix \nof the inner product in the ordered basis il. It follows from (8-5) \nSec. 8.1 \nInner Products \nthat a is hermitian, i.e., that a = a* ; however, G is a rather special kind \nof hermitian matrix. For a must satisfy the additional condition \n(8-6) \nx*ax > 0, \nx á o. \nIn particular, a must be invertible. For otherwise there exists an X á 0 \nsuch that ax \n0, and for any such X, (8-6) is impossible. More explicitly, \n(8-6) says that for any scalars Xl, . . .  , X .. not all of which are 0 \n(8-7) \nFrom this we see immediately that each diagonal entry of a must be \npositive; however, this condition on the diagonal entries is by no means \nsufficient to insure the validity of (8-6). Sufficient conditions for the \nvalidity of (8-6) wil be given later. \nThe above process is reversible; that is, if a is any n X n matrix over \nF which satisfies (8-6) and the condition a = a*, then a is the matrix in \nthe ordered basis CB of an inner product on V. This inner product is given \nby the equation \n(al.8) = y*ax",
    "validity of (8-6) wil be given later. \nThe above process is reversible; that is, if a is any n X n matrix over \nF which satisfies (8-6) and the condition a = a*, then a is the matrix in \nthe ordered basis CB of an inner product on V. This inner product is given \nby the equation \n(al.8) = y*ax \nwhere X and Y are the coordinate matrices of a and {3 in the ordered \nbasis CB. \n1. Let V be a vector space and ( \\ ) an inner product on V. \n(a) Show that (O\\{3) = 0 for all {3 in V. \n(b) Show that if (a\\{3) \n0 for all {3 in V, then a = O. \nExercises \n2. Let V be a vector space over F. Show that the sum of two inner products \non V is an inner product on V. Is the difference of two inner products an inner \nproduct? Show that a positive multiple of an inner product is an inner product, \n3. Describe explicitly all inner products on RI and on 01• \n4. Verify that the standard inner product on Fn is an inner product. \n5. Let ( \\ ) be the standard inner product on R2, \n(a) Let a = (1, 2), (3 = ( - 1, 1). If \"I is a vector such that (a\\\"I) = - 1  and \n({3\\\"I) = 3, find \"I, \n(b) Show that for any a in R2 we have a = (a\\EI)EI + (a\\E2)E2' \n6. Let ( I \n) be the standard inner product on R2, and let T be the linear operator \nT(xJ, X2) = (-X2, XI)' Now T is 'rotation through 90°' and has the property \nthat (a\\Ta) \n0 for all a in R2, Find all inner products [ \\ ] on R2 such that \n[alTa] = 0 for each a. \n7. Let ( \\ ) be the standard inner product on 02• Prove that there is no non­\nzero linear operator on 02 such that (a\\Ta) = 0 for every a in 02, Generalize. \n275 \n276 \nI nne?' Product Spaces \n8. Let A be a 2 X 2 matrix with real entries. For X, Y in RzxI let \nIA(X, Y) = PAX. \nChap. 8 \nShow that 1 A is an inner product on RZXl if and only if A = A I, All > 0, A22 > 0, \nand det A >  0. \n9. Let V be a real or complex vector space with an inner product. Show that the \nquadratic form determined by the inner product satisfies the parallelogram law",
    "IA(X, Y) = PAX. \nChap. 8 \nShow that 1 A is an inner product on RZXl if and only if A = A I, All > 0, A22 > 0, \nand det A >  0. \n9. Let V be a real or complex vector space with an inner product. Show that the \nquadratic form determined by the inner product satisfies the parallelogram law \n10. Let ( I ) be the inner product on R2 defined in Example 2, and let (B be \nthe standard ordered basis for R2. Find the matrix of this inner product relative \nto (B. \nU. Show that the formula \n(° . il° b k) \n_ ° \naibk \nj a,X k kX \n- j,lej + k + 1 \ndefines an inner product on the space R[x] of polynomials over the field R. Let W \nbe the subspace of polynomials of degree less than or equal to n. Restrict the above \ninner product to W, and find the matrix of this inner product on W, relative to the \nordered basis {I, x, x2, •\n•\n•\n , xn} . (Hint: To show that the formula defines an inner \nproduct, observe that \nand work with the integral.) \n(fl g) = 101 I(t)g(t) dt \n12. Let V be a finite-dimensional vector space and let (B = {ah . . .  , an} be a \nbasis for V. Let ( I ) be an inner product on V. If CIJ •\n•\n•\n , en are any n scalars, \nshow that there is exactly one vector a in V such that (alai) = Cj, j \n1, . . .  , n. \n13. Let V be a complex vector space. A function J from V into V is called a \nconjugation if J(a + (3) = J(a) + J({3), J(oo) = cJ(a), and J(J(a» = a, for \nall scalars c and all a, {3 in V. If J is a conjugation show that: \n(a) The set W of all a in V such that Ja = a is a vector space over R with \nrespect to the operations defined in V. \n(b) For each a in V there exist unique vectors (3, 'Y in W such that a \n(3 + i'Y. \n14. Let V be a complex vector space and W a subset of V with the following \nproperties: \n(a) W is a real vector space with respect to the operations defined in V. \n(b) For each a in V there exist unique vectors (3, 'Y in W such that a \n(3 + i'Y. \nShow that the equation Ja = {3 \ni'Y defines a conjugation on V such that Ja \na",
    "properties: \n(a) W is a real vector space with respect to the operations defined in V. \n(b) For each a in V there exist unique vectors (3, 'Y in W such that a \n(3 + i'Y. \nShow that the equation Ja = {3 \ni'Y defines a conjugation on V such that Ja \na \nif and only if a belongs to W, and show also that J is the only conjugation on V \nwith this property. \n15. Find all conjugations on CI and C2. \n16. Let W be a finite-dimensional real subspace of a complex vector space V. \nShow that W satisfies condition (b) of Exercise 14 if and only if every basis of W \nis also a basis of V. \nSec. 8.2 \nInner Product Spaces \n17. Let V be a complex vector space, J a conjugation on V, W the set of a in V \nsuch that Ja = a, and I an inner product on W. Show that: \n(a) There is a unique inner product g on V such that g(a, (3) \n= I(a, (3) for \nall a, (3 in W, \n(b) g(Ja, J(3) \n= g({3, a) for all a, {3 in V. \nWhat does part (a) say about the relation between the standard inner products \non Rl and Cl, or on Rn and Cn? \n277 \n8.2. Inner Product Spaces \nNow that we have some idea of what an inner product is, we shall \nturn our attention to what can be said about the combination of a vector \nspace and some particular inner product on it. Specifically, we shan \nestablish the basic properties of the concepts of 'length' and 'orthogo­\nnality' which are imposed on the space by the inner product. \nDefinition. An inner product space is a real or complex vector space, \ntogether with a specified inner product on that space. \nA finite-dimensional real inner product space is often called a Euclid­\nean space. A complex inner product space is often referred to as a unitary \nspace. \nTheorem 1 .  If V is an inner product space, then for any vectors a, {3 \nin V and any scalar e \n(i) Ileall \nlei liall ; \n(ii) Iiall > 0 for a H 0; \n(iii) I (al{3) I Y lIali lliJl I ;  \n(iv) !la + (311 Y lIall + 1I{311· \nProof. Statements (i) and (ii) follow almost immediately from",
    "space. \nTheorem 1 .  If V is an inner product space, then for any vectors a, {3 \nin V and any scalar e \n(i) Ileall \nlei liall ; \n(ii) Iiall > 0 for a H 0; \n(iii) I (al{3) I Y lIali lliJl I ;  \n(iv) !la + (311 Y lIall + 1I{311· \nProof. Statements (i) and (ii) follow almost immediately from \nthe various definitions involved. The inequality in (iii) is clearly valid \nwhen a = O. If a H 0, put \nThen h·la) \n= 0 and \no ÿ Ibl12 = ({3 - 1fl\u001a a I iJ - ifl\t a) \n== ({31{3) - (iJla)(alf3) \nlIal/2 \n= 1113112 \n_ I (a/f3) 12. \n//a//2 \n278 \nInner Product Spaces \nHence l (alml2 ȍ IIal 121 1i31 12, Now using (c) we find that \nIia + i3W = IIal 12 + (ali3) + (i3la) + 1 1131 12 \n= IlaW + 2 Re (ali3) + l Ii3W \n I Ial12 + 2 1 1all lli31 1 + 1 1131 12 \n= (liall + 1 1131 1)2, \nThus, I ia + 1311 ȍ Iiall + 111311· I \nChap. 8 \nThe inequality in (iii) is called the Cauchy-Schwarz inequality. \nIt has a wide variety of applications. The proof shows that if (for example) \na is non-zero, then l (ali3) 1 < Ilall lli3l 1 unless \n(i3la) \n13 = lIall2 a. \nThus, equality occurs in (iii) if and only if a and 13 are linearly dependent. \nEXAMPLE 7. If we apply the Cauchy-Schwarz inequality to the \ninner products given in Examples 1, 2, 3, and 5, we obtain the following; \n(a) \nI  XkYkl  (  IXkI2) 1/2(  IYk1Z) 1/2 \n(b) \nIXlYl \nXZYI - XIY2 \n4X2Y21 \n ((Xl - X2)2 + 3X)1/2((Yl - Y2)2 + 3y) 1/2 \n(c) \nItr (AB*) I  (tr (AA *» 1I2(tr (BB*»1/2 \n(d) \n1101 f(x)g(x) dxl ȍ (101 If(x) /2 dx r/2 (101 Ig(x)12 dx r/2• \nDefinitions. Let a and 13 be vectors in an inner product space V. Then a \nis orthogonal to 13 ij (aim \n0; since this implies 13 is orthogonal to a, \nwe ojten simply say that a and 13 are orthogonal. Ij S is a set oj vectors in V, \nS is called an orthogonal set provided all pairs oj distinct vectors in S are \northogonal. An orthonormal set is an orthogonal set S with the additional \nproperty that lIall = 1 for every a in S. \nThe zero vector is orthogonal to every vector in V and is the only",
    "S is called an orthogonal set provided all pairs oj distinct vectors in S are \northogonal. An orthonormal set is an orthogonal set S with the additional \nproperty that lIall = 1 for every a in S. \nThe zero vector is orthogonal to every vector in V and is the only \nvector with this property. It is appropriate to think of an orthonormal \nset as a set of mutually perpendicular vectors, each having length 1. \nEXAMPLE 8. The standard basis of either R\" or en is an orthonormal \nset with respect to the standard inner product. \nEXAMPLE 9. The vector (x, y) in R2 is orthogonal to (-y, x) with \nrespect to the standard inner product, for \n((x, y)l (  - y, x» \n-xy + yx = O. \nHowever, if R2 is equipped with the inner product of Example 2, then \n(x, y) and (-y, x) are orthogonal if and only if \nSec. 8.2 \nInner Product Spaces \ny = ! ( -3 ± V13)x. \nEXAMPLE 10. Let V be Cnxn, the space of complex n X n matrices, \nand let EM be the matrix whose only non-zero entry is a 1 in row p and \ncolumn q. Then the set of all such matrices EplJ is orthonormal with respect \nto the inner product given in Example 3. For \nEXAMPLE 11. Let V be the space of continuous complex-valued (or \nreal-valued) functions on the interval 0 S x S 1 with the inner product \n(fIg) = f f(x)g(x) dx. \nSuppose fn(x) = V2 cos 211'nx and that g,,(x) = V2 sin 211'nx. Then \n{I, fl' gl, f2' g2, . . . } is an infinite orthonormal set. In the complex case, \nwe may also form the linear combinations \nn = 1, 2, . . . . \nIn this way we get a new orthonormal set S which consists of all functions \nof the form \nn = ±1, ±2, . . . . \nThe set S' obtained from S by adjoining the constant function 1 is also \northonormal. We assume here that the reader is familiar with the calcula­\ntion of the integrals in question. \nThe orthonormal sets given in the examples above are all linearly \nindependent. We show now that this is necessarily the case. \nTheorem 2. An orthogonal set of non-zero vectors is linearly inde­\npendent.",
    "tion of the integrals in question. \nThe orthonormal sets given in the examples above are all linearly \nindependent. We show now that this is necessarily the case. \nTheorem 2. An orthogonal set of non-zero vectors is linearly inde­\npendent. \nProof. Let S be a finite or infinite orthogonal set of non-zero \nvectors in a given inner product space. Suppose aI, a2, . . .  , am are distinct \nvectors in S and that \nThen \n(tJlak) = ( Cjajlak) \nj \n=   c;(ajlak) \nj \n279 \n280 \nInner Product Spaces \n1 H k H m. \nThus when {3 = 0, each Ck = 0; so S is an independent set. \nI \nCha.p. 8 \nCorollary. If a vector {3 is a linear combination of an orthogonal \nsequence of non-zero vectors al, . . .  , am, then {J is the particular linear \ncombination \n(8-8) \nThis corollary follows from the proof of the theorem. There is another \ncorollary which although obvious, should be mentioned. If {aI, . . .  , am} \nis an orthogonal set of non-zero vectors in a finite-dimensional inner \nproduct space V, then m H dim V. This says that the number of mutually \northogonal directions in V cannot exceed the algebraically defined dimen­\nsion of V. The maximum number of mutually orthogonal directions in V \nis what one would intuitively regard as the geometric dimension of V, \nand we have just seen that this is not greater than the algebraic dimension. \nThe fact that these two dimensions are equal is a particular corollary of \nthe next result. \nTheorem 3. Let V be an inner product space and let {Jl, \n•\n•\n•\n , {In be \nany independent vectors in V. Then one may construct orthogonal vectors \naI, . . .  , an in V such that for each k = 1, 2, . .\n. , n the set \n{all . . .  , ak} \nis a basis for the subspace spanned by {JI, .\n.\n•\n , {3k. \nProof. The vectors ai, . . . , an will be obtained by means of a \nconstruction known as the Grall-Schllidt orthogonalization process. \nFirst let a1 = {3l. The other vectors are then given inductively as follows:",
    "{all . . .  , ak} \nis a basis for the subspace spanned by {JI, .\n.\n•\n , {3k. \nProof. The vectors ai, . . . , an will be obtained by means of a \nconstruction known as the Grall-Schllidt orthogonalization process. \nFirst let a1 = {3l. The other vectors are then given inductively as follows: \nSuppose ai, . . .  , am (1 H m < n) have been chosen so that for every k \n{aI, . . . , ak}, \n1 H k H m \nis an orthogonal basis for the subspace of V that is spanned by {JI, . . .  , {Jk. \nTo construct the next vector am+l, let \n(8-9) \nThen am+1 rE O. For otherwise {Jm+l is a linear combination of ai, . . . , am \nand hence a linear combination of {JI, •\n.\n•\n , {3m. Furthermore, if 1 H j H m, \nthen \n({3m+llaj) - ({3m+llaj) \n= O. \nSec. 8.2 \nInner Product Spaces \nTherefore {aI, ' . .  , am+l} is an orthogonal set consisting of m + 1 non­\nzero vectors in the subspace spanned by (31, . . .  , (3m+l. By Theorem 2, \nit is a basis for this subspace. Thus the vectors aI, . . .  , an may be con­\nstructed one after the other in accordance with (8-9). In particular, when \nn = 4, we have \n(8-10) \naI = (31 \n((32Ial) \na2 = {32 - Q a1 \n({3slal) \n({3sl a2) \nas = {33 - Q al - + a2 \n(RlaJ \n(Rla0 \n(RlaJ \na4 = {34 - * a1 - IIa2W a2 - IlasW as. I \nCorollary. Every finite-dimensional inner product space has an ortho­\nnormal basis. \nProof. Let V be a finite-dimensional inner product space and \n{{31, . .\n. , {3n} a basis for V. Apply the Gram-Schmidt process to construct \nan orthogonal basis {aI, . . .  , an} . Then to obtain an orthonormal basis, \nsimply replace each vector ak by adllakll· I \nOne of the main advantages which orthonormal bases have over \narbitrary bases is that computations involving coordinates are simpler. \nTo indicate in general terms why this is true, suppose that V is a finite­\ndimensional inner product space. Then, as in the last section, we may use \nEquation (8-5) to associate a matrix G with every ordered basis CB = \n{aI, . . .  , an} of V. Using this matrix \nGjk = (aklaj),",
    "To indicate in general terms why this is true, suppose that V is a finite­\ndimensional inner product space. Then, as in the last section, we may use \nEquation (8-5) to associate a matrix G with every ordered basis CB = \n{aI, . . .  , an} of V. Using this matrix \nGjk = (aklaj), \nwe may compute inner products in terms of coordinates. If CB is an ortho­\nnormal basis, then G is the identity matrix, and for any scalars Xj and Yk \n(S xpjlS Ykak) \n= S x/[h-\nj \nk \nj \nThus in terms of an orthonormal basis, the inner product in V looks like \nthe standard inner product in Fn. \nAlthough it is of limited practical use for computations, it is inter­\nesting to note that the Gram-Schmidt process may also be used to test \nfor linear dependence. For suppose (31, •\n.\n.\n , (3n are linearly dependent \nvectors in an inner product space V. To exclude a trivial case, assume \nthat {31 ˼ O. Let m be the largest integer for which {31, .\n. . , (3m are inde­\npendent. Then 1 ყ m < n. Let aI, . . .  , am be the vectors obtained by \napplying the orthogonalization process to {31, . . .  , 13m. Then the vector \nam+l given by (8-9) is necessarily O. For am+l is in the subspace spanned \n281 \n282 \nInner Product Spaces \nChap. 8 \nby aI, . . .  , am and orthogonal to each of these vectors; hence it is ° by \n(8-8). Conversely, if aI, . . .  , am are different from ° and a\",+l \n= 0, then \n/31, . . .  , /3\",+1 are linearly dependent. \nEXAMPLE 12. Consider the vectors \n/31 = (3, 0, 4) \n(32 = (-1, 0, 7) \n/33 \n= (2, 9, 1 1) \nin R3 equipped with the standard inner product. Applying the Gram­\nSchmidt process to /31, /32, /33, we obtain the following vectors. \na1 \n(3, 0, 4) \na2 \n( 1, 0, 7) - -'-'--'--'-::-':'-'-'-'--'--'-'- (3, 0, 4) \n= (- 1, 0, 7) - (3, 0, 4) \n( \n0, 3) \na3 \n= (2, 9, 11) - ((2, 9, Iȵr3, 0, 4) (3, 0, 4) \n_ ((2, 9, 11) 1 (  -4, 0, 3» (-4 ° 3) \n25 \n' \n, \n= (2, 9, 11) - 2(3, 0, 4) - -4, 0, 3) \n(0, 9, 0). \nThese vectors are evidently non-zero and mutually orthogonal. Hence",
    "= (- 1, 0, 7) - (3, 0, 4) \n( \n0, 3) \na3 \n= (2, 9, 11) - ((2, 9, Iȵr3, 0, 4) (3, 0, 4) \n_ ((2, 9, 11) 1 (  -4, 0, 3» (-4 ° 3) \n25 \n' \n, \n= (2, 9, 11) - 2(3, 0, 4) - -4, 0, 3) \n(0, 9, 0). \nThese vectors are evidently non-zero and mutually orthogonal. Hence \n{aI, az, a3} is an orthogonal basis for R3. To express an arbitrary vector \n(Xl, X2, Xa) in R3 as a linear combination of all a2, as it is not neeessary to \nsolve any linear equations. Por it suffices to usc (8-8). Thus \n3XI + 4xa \n-4Xl + 3X3 \nX2 \n(Xl, X2, Xa) \n= \n25 \nal + \n25 \na2 + \"9 as \nas is readily verified. In particular, \n(1, 2, 3) \n! (3, 0, 4) + i (-4, 0, 3) + i (0, 9, 0). \nTo put this point in another way, what we have shown is the following: \nThe basis {fl, 12, fa} of (RS)* which is dual to the basis {aI, a2, aa} is defined \nexplicitly by the equations \n3Xl + 4xa \n25 \nSec. 8.2 \nInner Product Spaces \nand these equations may be written more generally in the form \nj\"( \n) \n_ ((Xl, XŤ.!3) laj). \nJ Xl, X2, Xa -\nI Iajl12 \nFinally, note that from al, a2, aa we get the orthonormal basis \ni (3, 0, 4), i (-4, 0, 3), \n(0, 1, 0). \nEXAMPLE 13. Let A = [8 mJ where a, b, c, and d are complex num­\nbers. Set (31 = (a, b), {32 = (e, d), and suppose that (31 ჰ O. If we apply \nthe orthogonalization process to (31, {32, using the standard inner product \nin C2, we obtain the following vectors: \nal \n(a, b) \na2 = (c, d) \n((c, d)l(a, b» ( b) \nlal2 + IW a, \n(c, d) \n(ca + db) \nlal2 + Ibl2 (a, b) \n(ebb - dba daa \ncab) \n= \nlal2 + IW' '-lalC:-2 -:-;-;lb:O:12 \ndet A \n-\n_ \nlal2 + IW (-b, a). \nNow the general theory tells us that a2 ჰ 0 if and only if (31, {32 are linearly \nindependent. On the other hand, the formula for a2 shows that this is the \ncase if and only if det A ˼ O. \nIn essence, the Gram-Schmidt process consists of repeated applica­\ntions of a basic geometric operation called orthogonal projection, and it \nis best understood from this point of view. The method of orthogonal",
    "case if and only if det A ˼ O. \nIn essence, the Gram-Schmidt process consists of repeated applica­\ntions of a basic geometric operation called orthogonal projection, and it \nis best understood from this point of view. The method of orthogonal \nprojection also arises naturally in the solution of an important approxima­\ntion problem. \nSuppose W is a subspace of an inner product space V, and let (3 be \nan arbitrary vector in V. The problem is to find a best possible approxima­\ntion to {3 by vectors in W. This means we want to find a vector a for which \n1 1{3 \nal l  is as small as possible subject to the restriction that a should \nbelong to W. Let us make our language precise. \nA best approximation to (3 by vectors in W is a vector a in W such that \n11{3 - all ::; 11{3 - 'I'll \nfor every vector 'I' in W. \nBy looking at this problem in R2 or in Ra, one sees intuitively that a \nbest approximation to (3 by vectors in W ought to be a vector a in W such \nthat {3 \na is perpendicular (orthogonal) to W and that there ought to \n283 \n284 \nInner Product Spaces \nChap. 8 \nbe exactly one such a. These intuitive ideas are correct for finite-dimen­\nsional subspaces and for some, but not all, infinite-dimensional subspaces. \nSince the precise situation is too complicated to treat here, we shall prove \nonly the following result. \nTheorem 4. Let W be a subspace of an inner product space V and \nlet (:J be a vector in V. \n(i) The vector a in W is a best approximation to (:J by vectors in W if \nand only if (3 - a is orthogonal to every vector in W. \n(ii) If a best approximation to (3 by vectors in W exists, it is unique. \n(iii) If W is finite-dimensional and {aI, . . . , an} is any orthonormal \nbasis for W, then the vector \n((3lak) \na = f IIakl 12 ak \nis the (unique) best approximation to (:J by vectors in W. \nProof. First note that if \"I is any vector in V, then (3 - \"I = \n((3 - a) + (a - \"I), and \n11(3 \n'YW \n11(3 - al 12 + 2 Re ({3 \nala \n\"I) + Iia \n\"11 12• \nNow suppose {3",
    "basis for W, then the vector \n((3lak) \na = f IIakl 12 ak \nis the (unique) best approximation to (:J by vectors in W. \nProof. First note that if \"I is any vector in V, then (3 - \"I = \n((3 - a) + (a - \"I), and \n11(3 \n'YW \n11(3 - al 12 + 2 Re ({3 \nala \n\"I) + Iia \n\"11 12• \nNow suppose {3 \na is orthogonal to every vector in W, that \"I is in W \nand that \"I ჰ a. Then, since a \n\"I is in W, it follows that \n1 1{3 - \"1112 = 1 1{3 - all2 + l Ia - 'YW \n> 1 1(3 - a1 l2• \nConversely, suppose that 1 1(3 - \"III 2 11{3 - al l  for every \"I In W. \nThen from the first equation above it follows that \n2 Re ((3 \nala \n\"I) + lIa \n\"1112 2 0 \nfor all \"I in W. Since every vector in W may be expressed in the form \na - \"I with \"I in W, we see that \n2 Re ((3 - alT) + IITII2 2 0 \nfor every T in W. In particular, if \"I is in W and \"I ჰ a, we may take \nT \n({3 - ala - \"I) \nlIa - \"11 12 \n(a \n\"I). \nThen the inequality reduces to the statement \n_2 1((3 - ala - \"1) 12 + 1 ((3 - ala - 'Y)i2 > o. \nlIa \n\"1112 \nlIa - \"1112 \n-\nThis holds if and only if ({3 - ala - \"I) \nO. Therefore, (3 - a is orthog-\nonal to every vector in W. This completes the proof of the equivalence \nof the two conditions on a given in (i). The orthogonality condition is \nevidently satisfied by at most one vector in W, which proves (ii). \nSec. 8.2 \nI nner Product Spaces \nNow suppose that W is a finite-dimensional subspace of V. Then we \nknow, as a corollary of Theorem 3, that W has an orthogonal basis. Let \n{aI, . . .  , an} be any orthogonal basis for W and define a by (8-11). Then, \nby the computation in the proof of Theorem 3, {3 -\na is orthogonal to \neach of the vectors ak ({3 - a is the vector obtained at the last stage when \nthe orthogonalization process is applied to aI, . . .  , an, (3). Thus (3 - a is \northogonal to every linear combination of aI, . . .  , an, i.e., to every vector \nin W. If 'Y is in W and 'Y ˼ a, it follows that 11(3 - 'YII > 11(3 - all. There­\nfore, a is the best approximation to {3 that lies in W. I",
    "orthogonal to every linear combination of aI, . . .  , an, i.e., to every vector \nin W. If 'Y is in W and 'Y ˼ a, it follows that 11(3 - 'YII > 11(3 - all. There­\nfore, a is the best approximation to {3 that lies in W. I \nDefinition. Let V be an inner product space and S any set of vectors \nin V. The orthogonal com.plem.ent of S is the set S.L of all vectors in V \nwhich are orthogonal to every vector in S. \nThe orthogonal complement of V is the zero subspace, and conversely \n{OP = V. If S is any subset of V, its orthogonal complement S.L (S perp) \nis always a subspace of V. For S is non-empty, since it contains 0; and \nwhenever a and (3 are in S1. and c is any scalar, \n(ca + .a1'Y) = c(a\\'Y) + (.aI'Y) \n= cO + 0 \n= 0  \nfor every 'Y in S, thus Ca + {3 also lies in S. In Theorem 4 the character­\nistic property of the vector a is that it is the only vector in W such that \n(3 - a belongs to W.L. \nDefinition. Whenever the vector a in Theorem 4 exists it is called the \northogonal projection of (3 on W. If every vector in V has an orthogonal \nprojection on W, the mapping that assigns to each vector in V its orthogonal \nprojection on W is called the orthogonal projection of V on W. \nBy Theorem 4, the orthogonal projection of an inner product space \non a finite-dimensional subspace always exists. But Theorem 4 also implies \nthe following result. \nCorollary. Let V be an inner product space, W a finite-dimensional \nsubspace, and E the orthogonal projection of V on W. Then the mapping \nis the orthogonal projection of V on W.L. \nProof. Let (3 be an arbitrary vector in V. Then f3 - E.a is in W.L, \nand for any 'Y in W.L, (3 - 'Y = E{3 + ({3 - E{3 - 'Y). Since E(3 is in W \nand (3 - E.a - 'Y is in W.L, it follows that \n285 \n286 \nInner Produ,ct Spaces \nChap. 8 \n11(1 - 1'112 \nI/Ei31 /2 + 1113 - Ei3 - 1'1/2 \nჲ 1/13 - (13 - Ef3)112 \nwith strict inequality when l' rf- 13 - Ei3. Therefore, 13 - Ei3 is the best \napproximation to 13 by vectors in W.L. I",
    "and (3 - E.a - 'Y is in W.L, it follows that \n285 \n286 \nInner Produ,ct Spaces \nChap. 8 \n11(1 - 1'112 \nI/Ei31 /2 + 1113 - Ei3 - 1'1/2 \nჲ 1/13 - (13 - Ef3)112 \nwith strict inequality when l' rf- 13 - Ei3. Therefore, 13 - Ei3 is the best \napproximation to 13 by vectors in W.L. I \nEXAMPLE 14 . Give R3 the standard inner product. Then the orthog­\nonal projection of ( 10, 2, 8) on the subspace W that is spanned by \n(3, 12, \n1) is the vector \n= {L!Q! )! 8) 1(3, 12, - 1» (3 12 - 1) \na \n9 + 144 + 1 \n\" \n- 14 \n= \n(3, 12, \n1). \nThe orthogonal projection of RS on W is the linear transformation E \ndefined by \nXS) (3, 12, - 1). \nThe rank of E is clearly 1; hence its nullity is 2. On the other hand, \nE(x!, X2, xs) = (0, 0, 0) \nif and only if 3x! + 12x2 - Xa = O. This is the case if and only if (Xl, X2, xs) \nis in W.L. Therefore, W.L is the null space of E, and dim (W.L) = 2. \nComputing \nxa) (3, 12, - 1) \nwe see that the orthogonal projection of R3 on W.L is the linear transforma­\ntion I - E that maps the vector (Xl, X2, Xa) onto the vector \nThe observations made in Example 14 generalize in the following \nfashion. \nTheorem 5. Let W be a finite-dimensional 8ub8pace of an inner product \n8pace V and let E be the orthogonal projection of V on W. Then E is an idem­\npotent linear tran8formation of V onto W, W.L is the null space of E, and \nV = W Ei:) W.L. \nProof. Let (3 be an arbitrary vector in V. Then Ei3 is the best \napproximation to 13 that lies in W. In particular, Ei3 = 13 when 13 is in W. \nTherefore, E(Ef3) \nE/3 for every 13 in V; that is, E is idempotent: E2 \nE. \nTo prove that E is a linear transformation, let a and {3 be any vectors in \nSec. 8.2 \nInner Product Spaces \nV and c an arbitrary scalar. Then, by Theorem 4, a \nEa and (3 \nE(3 \nare each orthogonal to every vector in W. Hence the vector \nc(a - Ea) + «(3 - E(3) = (ca + (3) - (cEa + E(3) \nalso belongs to W.1.. Since cEa + E(3 is a vector in W, it follows from \nTheorem 4 that \nE(ca + (3) \ncEa + E(3.",
    "V and c an arbitrary scalar. Then, by Theorem 4, a \nEa and (3 \nE(3 \nare each orthogonal to every vector in W. Hence the vector \nc(a - Ea) + «(3 - E(3) = (ca + (3) - (cEa + E(3) \nalso belongs to W.1.. Since cEa + E(3 is a vector in W, it follows from \nTheorem 4 that \nE(ca + (3) \ncEa + E(3. \nOf course, one may also prove the linearity of E by using (8-11). Again \nlet (3 be any vector in V. Then E(3 is the unique vector in W such that \n(:J \nE(3 is in W.1.. Thus E(:J = 0 when (:J is in TP. Conversely, (:J is in W.L \nwhen E(3 = O. Thus W.1. is the null space of E. The equation \n(:J \nE(:J + (3 \nE(3 \nshows that V = W + TP ; moreover, W n TP \n{O} . For if a is a \nvector in W n w  \\ then (ala) = O. Therefore, a = 0, and V is the direct \nsum of W and W.1.. \nI \nCorollary. Under the conditions of the theorem, I - E is the orthogonal \nprojection of V on W.L. It is an idempotent linear transformation of V onto \nW.L with null space W. \nProof. We have already seen that the mapping {3 -t {3 \nE{3 is \nthe orthogonal projection of V on W.L. Since E is a linear transformation, \nthis projection on W.1. is the linear transformation I - E. From its geo­\nmetric properties one sees that I \nE is an idempotent transformation \nof V onto W. This also follows from the computation \n(I - E) (I - E) = I - E \nE + E2 \n= I - E. \nMoreover, (I - E)(3 = 0 if and only if {3 = E{3, and this is the case if and \nonly if (3 is in W. Therefore W is the null space of I \nE. \nI \nThe Gram-Schmidt process may now be described geometrically in \nthe following way. Given an inner product space V and vectors {3l, .\n.\n•\n , (:In \nin V, let Pk (k > 1) be the orthogonal projection of V on the orthogonal \ncomplement of the subspace spanned by {3l, . . .  , (3k-l, and set Pl = I. \nThen the vectors .one obtains by applying the orthogonalization process \nto {h, .\n.\n.\n , (:In are defined by the equations \n(8-12) \nak \nPk{3k, \n1 :s; k :s; n. \nTheorem 5 implies another result known as Bessel's inequality.",
    "complement of the subspace spanned by {3l, . . .  , (3k-l, and set Pl = I. \nThen the vectors .one obtains by applying the orthogonalization process \nto {h, .\n.\n.\n , (:In are defined by the equations \n(8-12) \nak \nPk{3k, \n1 :s; k :s; n. \nTheorem 5 implies another result known as Bessel's inequality. \nCorollary. Let {al, . . .  , an} be an orthogonal set of non-zero vectors \nin an inner product space V. If {3 is any vector in V, then \n؉ I ({3 I ak)i2 \n:s; I I(3W \nk I lakW \n287 \n288 \nInner Product Spaces \nChap. 8 \nand equality holds if and only if \n(i3lak) \n13 t IIakl12 ako \nProof· Let 'Y =   [(i3lak)/llakI12] ak. Then 13 = 'Y + µ where \nk \n('YIħ) = O. Hence \n1113112 = 11'Y112 + 11t112. \nIt now suffices to prove that \nIhW =   l(i3lak)12. \nk IlakW \nThis is straightforward computation in which one uses the fact that \n(ajlak) = 0 for j rf k. \nI \nIn the special case in which {al, ' . . , an} is an orthonormal set, \nBessel's inequality says that \n  1 (13 1 ak)12 ::; Ili3W· \nk \nThe corollary also tells us in this case that 13 is in the subspace spanned by \naI, . . .  , an if and only if \nor if and only if Bessel's inequality is actually an equality. Of course, in \nthe event that V is finite dimensional and {aI, . . .  , an} is an orthogonal \nbasis for V, the above formula holds for every vector 13 in V. In other \nwords, if {all ' . .  , an} is an orthonormal basis for V, the kth coordinate \nof (3 in the ordered basis {all ' . .  , a,,} is ((3lak)' \nEXAMPLE 1.5. We shall apply the last corollary to the orthogonal \nsets described in Example 11. We find that \n(a) \n(b) \n(c) \nExercises \nkJ-n 1101 !(t)e-27rikt dtr ::; 101 \nI!(t) 12 dt \n11 I n \n\\2 \nn \n  Cke2 .. ikt dt =   ICkl2 \no k= -n \nk= -n \n101 (V2 cos 27rt + V2 sin 47rt)2 dt = 1 + 1 = 2. \n1. Consider R4 with the standard inner product. Let W be the subspace of \nR4 consisting of all vectors which are orthogonal to both a = (1, 0, -1, 1) and \n(3 = (2, 3, -1, 2). Find a basis for W. \nSec. 8.2 \nInner Product Spaces",
    "o k= -n \nk= -n \n101 (V2 cos 27rt + V2 sin 47rt)2 dt = 1 + 1 = 2. \n1. Consider R4 with the standard inner product. Let W be the subspace of \nR4 consisting of all vectors which are orthogonal to both a = (1, 0, -1, 1) and \n(3 = (2, 3, -1, 2). Find a basis for W. \nSec. 8.2 \nInner Product Spaces \n2. Apply the Gram-Schmidt process to the vectors (31 = (1, 0, 1), (32 \n= (1, 0, - 1), \n(33 \n(0, 3, 4), to obtain an orthonormal basis for R3 with the standard inner \nproduct. \n3. Consider C3, with the standard inner product. Find an orthonormal basis for \nthe subspace spanned by (31 \n(1, 0, i) and (32 \n(2, 1, 1 + i). \n4. Let V be an inner product space. The distance between two vectors a and (3 \nin V is defined by \nShow that \n(a) dCa, (3) :?: 0; \ndCa, (3) = I ia \n(311. \n(b) dCa, (3) = 0 if and only if a \n(3; \n(c) dCa, (3) \n= d((3, a) ; \n(d) dCa, (3) ::; dCa, ')') + de')', (3). \n5. Let V be an inner product space, and let a, (3 be vectors in V. Show that \na = (3 if and only if (al')') \n((311') for every ,), in V. \n6. Let W be the subspace of R2 spanned by the vector (3, 4). Using the standard \ninner product, let E be the orthogonal projection of R2 onto W. Find \n(a) a formula for E(Xl, xz) ; \n(b) the matrix of E in the standard ordered basis; \n(c) W.L; \n(d) an orthonormal basis in which E is represented by the matrix \n7. Let V be the inner product space consisting of R2 and the inner product \nwhose quadratic form is defined by \nI I(xl, x2112 = (Xl - X2)2 + 3xG. \nLet E be the orthogonal projection of V onto the subspace W spanned by the \nvector (3, 4). Now answer the four questions of Exercise 6. \n8. Find an inner product on R2 such that (El, €2) \n2. \n9. Let V be the subspace of Rfx] of polynomials of degree at most 3. Equip V \nwith the inner product \n(fIg) 101 f(t)g(t) dt. \n(a) Find the orthogonal complement of the subspace of scalar polynomials. \n(b) Apply the Gram-Schmidt process to the basis {I, X, X2, X3} .",
    "2. \n9. Let V be the subspace of Rfx] of polynomials of degree at most 3. Equip V \nwith the inner product \n(fIg) 101 f(t)g(t) dt. \n(a) Find the orthogonal complement of the subspace of scalar polynomials. \n(b) Apply the Gram-Schmidt process to the basis {I, X, X2, X3} . \n10. Let V be the vector space of all n X n matrices over C, with the inner product \n(A IB) = tr (AB*). Find the orthogonal complement of the subspace of diagonal \nmatrices. \nll. Let V be a finite-dimensional inner product space, and let {al, . . .  , an} be \nan orthonormal basis for V. Show that for any vectors a, (3 in V \n289 \n290 \nInner Product Spaces \nChap. 8 \n12. Let W be a finite-dimensional subspace of an inner product space V, and let E \nbe the orthogonal projection of V on W. Prove that (Eal(3) = (aiEl') for all a, (3 \nin V. \n13. Let S be a subset of an inner product space V. Show that (S.L).L contains the \nsubspace spanned by S. When V is finite-dimensional, show that (Sly. is the sub­\nspace spanned by S. \n14. Let V be a finite-dimensional inner product space, and let (B = {aI, . . .  , an} \nbe an orthonormal basis for V. Let T be a linear operator on V and A the matrix \nof T in the ordered basis (B. Prove that \nAij = (Tajlai). \n15. Suppose V = WI E8 W2 and that 11 and /2 are inner products on WI and W2, \nrespectively. Show that there is a unique inner product I on V such that \n(a) W2 \nwt ; \n(b) I(a, 1') \n= fk(a, (3), when a, (3 are in Wk, k = 1, 2. \n16. Let V be an inner product space and W a finite-dimensional subspace of V. \nThere are (in general) many projections which have W as their range. One of \nthese, the orthogonal projection on W, has the property that I IEal1 :::; I ial l for \nevery a in V. Prove that if E is a projection with range W, such that I IEal 1  :::; Iiall \nfor all a in V, then E is the orthogonal projection on W. \n17. Let V be the real inner product space consisting of the space of real-valued",
    "every a in V. Prove that if E is a projection with range W, such that I IEal 1  :::; Iiall \nfor all a in V, then E is the orthogonal projection on W. \n17. Let V be the real inner product space consisting of the space of real-valued \ncontinuous functions on the interval, - 1  :::; t :::; 1, with the inner product \n(fIg) fl I(t)g(t) dt. \nLet W be the subspace of odd functions, i.e., functions satisfying f( -t) = -I(t). \nFind the orthogonal complement of W. \nB.3. Linear Functionals and Adjoints \nThe first portion of this section treats linear functionals on an inner \nproduct space and their relation to the inner product. The basic result is \nthat any linear functional f on a finite-dimensional inner product space \nis 'inner product with a fixed vector in the space,' i.e., that such an f has \nthe form f(a) \n(al(3) for some fixed (3 in V. We use this result to prove \nthe existence of the 'adjoint' of a linear operator T on V, this being a linear \noperator T* such that (Tal(3) = (al T*{j) for all a and I' in V. Through the \nuse of an orthonormal basis, this adjoint operation on linear operators \n(passing from T to T*) is identified with the operation of forming the \nconjugate transpose of a matrix. We explore slightly the analogy between \nthe adjoint operation and conjugation on complex numbers. \nLet V be any inner product space, and let I' be some fixed vector in V. \nWe define a function ff3 from V into the scalar field by \nSec. 8.3 \nLinear Functionals and Adjoints \nffl(a) \n(aim· \nThis function fll is a linear functional on V, because, by its very definition, \n(aim is linear as a function of a. If V is finite-dimensional, every linear \nfunctional on V arises in this way from some {3. \nTheorem 6. Let V be a finite-dimensional inner product space, and f a \nlinear functional on V. Then there exists a unique vector (3 in V such that \nf(a) = (aim for all a in V. \nProof. Let {a!, a2, . . .  , an} be an orthonormal basis for V. Put \n(8-13)",
    "Theorem 6. Let V be a finite-dimensional inner product space, and f a \nlinear functional on V. Then there exists a unique vector (3 in V such that \nf(a) = (aim for all a in V. \nProof. Let {a!, a2, . . .  , an} be an orthonormal basis for V. Put \n(8-13) \nand let fll be the linear functional defined by \nf(j(a) = (al{3). \nThen \nSince this is true for each ak, it follows that f = fli' Now suppose 'Y is a \nvector in V such that (aim \n(ai'Y) for all a. Then ({3 \n'Y1{3 \n'Y) \n0 \nand {3 = 'Y. Thus there is exactly one vector {3 determining the linear func­\ntional f in the stated manner. \nI \nThe proof of this theorem can be reworded slightly, in terms of the \nrepresentation of linear functionals in a basis. If we choose an ortho­\nnormal basis {aI, . . .  , an} for V, the inner product of a\nxlal + . . . \nXnan and {3 = Ylal + \n'\n\"\n + Ynan will be \n(aim \nxrih \n' \"  + xnYn' \nIf f is any linear functional on V, then f has the form \nf(a) \nCIXI + . . . + CnXn \nfor some fixed scalars CI, . . .  , Cn determined by the basis. Of course \nCj = f(aj). If we wish to find a vector (3 in V such that (al{3) = f(a) for all a, \nthen clearly the coordinates Yi of (3 must satisfy Yi = Cj or Yi = f(aj). \nAccordingly, \nis the desired vector. \nSome further comments are in order. The proof of Theorem 6 that \nwe have given is admirably brief, but it fails to emphasize the essential \ngeometric fact that {3 lies in the orthogonal complement of the null space \nof f. Let W be the null space of f. Then V \nW + W 1., and f is completely \ndetermined by its values on W1.. In fact, if P is the orthogonal projection \nof V on Wl., then \n291 \n292 \nInner Product Spaces \nChap. 8 \nf(a) = f(Pa) \nfor all a in V. Suppose f ჰ O. Then f is of rank 1 and dim (W.i) = 1. If 'Y \nis any non-zero vector in W.i, it follows that \nfor all a in V. Thus \nf(a) = (al'Y) \n. ȶlr2 \nfor all a, and (3 \n[f('Y)/I I'YWJ 'Y. \nEXAMPLE 16. We should give one example showing that Theorem 6",
    "Chap. 8 \nf(a) = f(Pa) \nfor all a in V. Suppose f ჰ O. Then f is of rank 1 and dim (W.i) = 1. If 'Y \nis any non-zero vector in W.i, it follows that \nfor all a in V. Thus \nf(a) = (al'Y) \n. ȶlr2 \nfor all a, and (3 \n[f('Y)/I I'YWJ 'Y. \nEXAMPLE 16. We should give one example showing that Theorem 6 \nis not true without the assumption that V is finite dimensional. Let V be \nthe vector space of polynomials over the field of complex numbers, with \nthe inner product \n(fIg) = 101 f(t)g(t) dt. \nThis inner product can also be defined algebraically. If f \n  akxk and \ng =   bkxk, then \n1 \n-\n(fIg) =   . + k + 1 ajbk• \nJ.k) \nLet z be a fixed complex number, and let L be the linear functional \n'evaluation at z' : \nL(f) = fez). \nIs there a polynomial g such that (fig) = L(f) for every f? The answer is \nno; for suppose we have \nfez) = 101 f(t)g(t) dt \nfor every f. Let h x \nz, so that for any f we have (hf)(z) \nO. Then \no 101 h(t)f(t)g(t) dt \nfor all j. In particular this holds when f = hg so that \n101 Ih(t)12Ig(t)j2 dt = 0 \nand so hg \nO. Since h ჰ 0, it must be that g \nO. But L is not the zero \nfunctional; hence, no such g exists. \nOne can generalize the example somewhat, to the case where L is a \nlinear combination of point evaluations. Suppose we select fixed complex \nnumbers Zl, • • • , Zn and scalars Cl, • \n• \n• , Cn and let \nSec. 8.3 \nLinear Functionals and A djoints \nThen L is a linear functional on V, but there is no g with L(f) \n(fIg), \nunless CI = C2 = . . . \n= Cn \n= O. Just repeat the above argument with \nh = (x - Zl) . . .  (x - Zn). \nWe turn now to the concept of the adjoint of a linear operator. \nTheorem 7. For any linear operator T on a finite-dimensional inner \nproduct space V, there exists a unique Unear operator T* on V such that \n(8-14) \n(Tal{1) = (aIT*{1) \nfor all a, {1 in V. \nProof. Let {1 be any vector in V. Then a -+ (Tal{1) is a linear \nfunctional on V. By Theorem 6 there is a unique vector {1' in V such that",
    "product space V, there exists a unique Unear operator T* on V such that \n(8-14) \n(Tal{1) = (aIT*{1) \nfor all a, {1 in V. \nProof. Let {1 be any vector in V. Then a -+ (Tal{1) is a linear \nfunctional on V. By Theorem 6 there is a unique vector {1' in V such that \n(Tal{1) = (al{1') for every a in V. Let T* denote the mapping (3 -+ {1': \n(3' = T*(3. \nWe have (8-14), but we must verify that T* is a linear operator. Let (3, 'Y \nbe in V and let C be a scalar. Then for any a, \n(aIT*(c(3 + 'Y» \n(Talc{1 + 'Y) \n(Talc(3) + (Tal'Y) \n= c(Tal{1) + (Ta/'Y) \n= c(aIT*,6) + (aI T*'Y) \n= (alcT*(3) + (aI T*'Y) \n= (alcT*{1 + T*'Y). \nThus T*(c{1 + 'Y) = cT*(3 + T*'Y and T* is linear. \nThe uniqueness of T* is clear. For any (3 in V, the vector T*{1 is \nuniquely determined as the vector (3' such that (Tal{1) \n(alln for \nevery a. I \nTheorem 8. Let V be a finite-dimensional inner product space and let \nil = {aI, . . .  , an} be an (ordered) orthonormal basis for V. Let T be a \nlinear operator on V and let A be the matrix of T in the ordered basis il. Then \nAkj = (Tajlak). \nProof. Since il is an orthonormal basis, we have \nn \na = { (a/ak)ak. \nk = l  \nThe matrix A is defined by \nand since \nn \nTaj = { AkjUk \nk= l  \nn \nTaj = { (Tajlak)ak \nk= l  \n293 \n294 \nInner Product Spaces \nChap. 8 \nCorollary. Let V be a finite-dimensional inner product space, and let \nT be a linear operator on V. In any orthonormal ba8i8for V, the matrix of T* \nis the conj1gate tran8pose of the matrix of T. \nProof. Let (B \n{aI, . . . , an} be an orthonormal basis for V, let \nA \n= [T](B and B = [T*](B. According to Theorem 8, \nAkj = (Tajlak) \nBkj = (T*ajlak)' \nBy the definition of T* we then have \nBkj = (T*ajlak) \n= (akIT*aj) \n= (Taklaj) \n= .4\";. \nI \nEXAMPLE 17. Let V be a finite-dimensional inner product space and \nE the orthogonal projection of V on a subspace W. Then for any vectors \na and {3 in V. \n(Eal!3) = (EaIE{3 + (1 - E){3) \n= (EaIE{3) \n(Ea \n(1 \nE)aIE{3) \n= (aIE{3).",
    "Bkj = (T*ajlak) \n= (akIT*aj) \n= (Taklaj) \n= .4\";. \nI \nEXAMPLE 17. Let V be a finite-dimensional inner product space and \nE the orthogonal projection of V on a subspace W. Then for any vectors \na and {3 in V. \n(Eal!3) = (EaIE{3 + (1 - E){3) \n= (EaIE{3) \n(Ea \n(1 \nE)aIE{3) \n= (aIE{3). \nFrom the uniqueness of the operator E* it follows that E* = E. Now \nconsider the projection E described in Example 14. Then \nA = _1_ [ 3: \n1! -Ńń] \n154 \n- 3  \n- 12 \n1 \nis the matrix of E in the standard orthonormal basis. Since E \nE*, A is \nalso the matrix of E*, and because A = A *, this does not contradict the \npreceding corollary. On the other hand, suppose \nal = (154, 0, 0) \na2 = (145, - 36, 3) \na3 = ( - 36, 10, 12). \nThen {aI, a2, a3} is a basis, and \nEal \n(9, 36, - 3) \nEa2 = (0, 0, 0) \nEa3 = (0, 0, 0) . \nSince (9, 36, - 3) = \n- (154, 0, 0) - (145, - 36, 3), the matrix B of E in \nthe basis {aJ, a2, as} is defined by the equation \nSec. 8.3 \n[- 1  0 \nB = \n- 1  0 \no 0 \nLinear Functionals and Adjoints \nIn this case B r\" B*, and B* is not the matrix of E* = E in the basis \n{ar, a2, a3} ' Applying the corollary, we conclude that {aI, a2, a3} is not \nan orthonormal basis. Of course this is quite obvious anyway. \nDefinition. Let T be a linear operator on an inner product space V. \nThen we say that T has an adjoint on V if there exists a linear operator T* \non V such that (Tal(3) = (aIT*i1) for all a and i1 in V. \nBy Theorem 7 every linear operator on a finite-dimensional inner \nproduct space V has an adjoint on V. In the infinite-dimensional case this \nis not always true. But in any ease there is at most one such operator T*; \nwhen it exists, we call it the adjoint of T. \nTwo comments should be made about the finite-dimensional case. \n1. The adjoint of l' depends not only on T but on the inner product \nas well. \n2. As shown by Example 17, in an arbitrary ordered basis il, the \nrelation between [TJm and [T*Jm is more complicated than that given in \nthe corollary above.",
    "1. The adjoint of l' depends not only on T but on the inner product \nas well. \n2. As shown by Example 17, in an arbitrary ordered basis il, the \nrelation between [TJm and [T*Jm is more complicated than that given in \nthe corollary above. \nEXAMPLE 18. Let V be CnXl, the space of complex n X 1 matrices, \nwith inner product (XI Y) = y* X. If A is an n X n matrix with complex \nentries, the adjoint of the linear operator X --t AX is the operator \nX --t A *X. For \n(AXI Y) = Y*AX = (A*Y)*X = (XIA*Y). \nThe reader should convince himself that this is really a special case of the \nlast corollary. \nEXAMPLE 19. This is similar to Example 18. Let V be Cnxn with the \ninner product (A IB) = tr (B* A). Let M be a fixed n X n matrix over C. \nThe adjoint of left multiplication by M is left multiplication by M*. Of \ncourse, 'left multiplication by M' is the linear operator LM defined by \nLM(A) = MA. \n(LM(A) IB) \ntr (B*(MA» \n= tr (MAB*) \n= tr (AB*M) \n= tr (A (M*B)*) \n= (AjLM*(B» . \n295 \n296 \nInner Product Spaces \nChap. 8 \nThus (1.,].1)* = 1.,].1*. In the computation above, we twice used the char­\nacteristic property of the trace function: tr (AB) = tr (BA). \nEXAMPLE 20. Let V be the space of polynomials over the field of \ncomplex numbers, with the inner product \n(fJg) \n= 101 f(t)g(t) dt. \nIf f is a polynomial, f = 2; akxk, we let 1 = 2; akxk. That is, 1 is the poly­\nnomial whose associated polynomial function is the complex conjugate \nof that for f: \n1(t) \nf(t), \nt real \nConsider the operator 'multiplication by f,' that is, the linear operator \nM, defined by M,(g) = fg. Then this operator has an adjoint, namely, \nmultiplication by ]. For \n(M,(g) Jh) = (fgJh) \n= 101 f(t)g(t)h(t) dt \n= 101 g(t) [f(t)h(t)] dt \n= (gJ1h) \n= (gJM1(h» \nEXAMPLE 21. In Example 20, we saw that some linear operators on \nan infinite-dimensional inner product space do have an adjoint. As we \ncommented earlier, some do not. Let V be the inner product space of",
    "(M,(g) Jh) = (fgJh) \n= 101 f(t)g(t)h(t) dt \n= 101 g(t) [f(t)h(t)] dt \n= (gJ1h) \n= (gJM1(h» \nEXAMPLE 21. In Example 20, we saw that some linear operators on \nan infinite-dimensional inner product space do have an adjoint. As we \ncommented earlier, some do not. Let V be the inner product space of \nExample 21, and let D be the differentiation operator on C[x]. Integra­\ntion by parts shows that \n(DfJg) = f(l)g(l) - f(O)g(O) - (fJDg). \nLet us fix g and inquire when there is a polynomial D*g such that \n(DfJg) \n(fJD*g) for all f. If such a D*g exists, we shall have \n(fJD*g) \nf(l)g(l) - f(O)g(O) \n(fJDg) \nor \n(fiD*g + Dg) = f(l)g(l) - f(O)g(O). \nWith g fixed, L(f) \n= f(1)g(1) - f(O)g(O) is a linear functional of the type \nconsidered in Example 16 and cannot be of the form LU) = UJh) unless \nL = O. If D*g exists, then with h = D*g + Dg we do have L(f) = (fJh), \nand so g(O) = g(l) \n= O. The existence of a suitable polynomial D*g implies \ng(O) = g(l) \n= O. Conversely, if g(O) = g(l) \n0, the polynomial D*g = \n-Dg satisfies (DfJg) \n(fJD*g) for all f. If we choose any g for which \ng(O) crf= 0 or g(l) crf= 0, we cannot suitably define D*g, and so we conclude \nthat D has no adjoint. \nSec. 8.3 \nLinear Functionals and Adjoints \nWe hope that these examples enhance the reader's understanding of \nthe adjoint of a linear operator. We see that the adjoint operation, passing \nfrom T to T*, behaves somewhat like conjugation on complex numbers. \nThe following theorem strengthens the analogy. \nTheorem 9. Let V be a finite-dimensional inner product space. If T \nand U are linear operators on V and c is a scalar, \n(i) (T + U)* = T* + U*; \n(ii) (cT)* = cT*; \n(iii) (TU)* = U*T*; \n(iv) (T*)* = T. \nThen \nProof. To prove (i), let a and (3 be any vectors in V. \n« T + U)a\\(3) \n= (Ta + Ua\\(3) \n= (Ta\\(3) + (Ua\\(3) \n= (a\\ T*(3) + (a\\ U*(3) \n= (a\\ T*(3 + U*(3) \n= (a\\ CT* + U*)(3). \nFrom the uniqueness of the adjoint we have (T + U)* = T* + U*. We",
    "(iii) (TU)* = U*T*; \n(iv) (T*)* = T. \nThen \nProof. To prove (i), let a and (3 be any vectors in V. \n« T + U)a\\(3) \n= (Ta + Ua\\(3) \n= (Ta\\(3) + (Ua\\(3) \n= (a\\ T*(3) + (a\\ U*(3) \n= (a\\ T*(3 + U*(3) \n= (a\\ CT* + U*)(3). \nFrom the uniqueness of the adjoint we have (T + U)* = T* + U*. We \nleave the proof of Cii) to the reader. We obtain (iii) and (iv) from the \nrelations \nCTUa\\,B) = (UajT*,B) \n= (aj U*T*(3) \n(T*a\\(3) = «(3IT*a) = (T,Bla) = (a\\T,B). \nI \nTheorem 9 is often phrased as follows: The mapping T -+ T* is a \nconjugate-linear anti-isomorphism of period 2. The analogy with complex \nconjugation which we mentioned above is, of course, based upon the \nobservation that complex conjugation has the properties (ZI + Z2) \n= \nZI + Z2, (ZIZ2) = ZIZ2, Z = z. One must be careful to observe the reversal \nof order in a product, which the adjoint operation imposes: (UT)* = \nT*U*. We shall mention extensions of this analogy as we continue our \nstudy of linear operators on an inner product space. We might mention \nsomething along these lines now. A complex number Z is real if and only \nif Z = Z. One might expect that the linear operators T such that T = T* \nbehave in some way like the real numbers. This is in fact the case. For \nexample, if T is a linear operator on a finite-dimensional complex inner \nproduct space, then \n(8-15) \nwhere UI = Ui and U2 = Ut Thus, in some sense, T has a 'real part' and \nan 'imaginary part.' The operators U1 and U2 satisfying Ul = Ui, and \nU2 = U9, and (8-15) are unique, and are given by \n297 \n298 \nInner Product Spaces \n! (T + T*) \n2 \n1 \nU2 = 2i (T - T*). \nChap. 8 \nA linear operator T such that T \nT* is called self-adjoint (or \nHermitian). If CB is an orthonormal basis for V, then \n[T*Jm \n[TJ& \nand so T is self-adjoint if and only if its matrix in every orthonormal basis \nis a self-adjoint matrix. Self-adjoint operators are important, not simply \nbecause they provide us with some sort of real and imaginary part for the",
    "Hermitian). If CB is an orthonormal basis for V, then \n[T*Jm \n[TJ& \nand so T is self-adjoint if and only if its matrix in every orthonormal basis \nis a self-adjoint matrix. Self-adjoint operators are important, not simply \nbecause they provide us with some sort of real and imaginary part for the \ngeneral linear operator, but for the following reasons: (1) Self-adjoint \noperators have many special properties. For example, for such an operator \nthere is an orthonormal basis of characteristic vectors. (2) Many operators \nwhich arise in practice are self-adjoint. We shall consider the special \nproperties of self-adjoint operators later. \nExercises \n1. Let Y be the space C2, with the standard inner product. Let T be the linear \noperator defined by TEl = (I, -2), TEz \n(i, - 1). If a \n(Xl, Xz), find T*a. \n2. Let T be the linear operator on C2 defined by TEl = (1 + i, 2), TEz \n(i, i). \nUsing the standard inner product, find the matrix of T* in the standard ordered \nbasis. Does T commute with T*? \n3. Let Y be C3 with the standard inner product. Let T be the linear operator on \nV whose matrix in the standard ordered basis is defined by \nAik \nii+k, \n(i2 \n- 1). \nFind a basis for the null space of T*. \n4. Let Y be a finite-dimensional inner product space and T a linear operator on Y. \nShow that the range of T* is the orthogonal complement of the null space of T. \n5. Let Y be a finite-dimensional inner product space and T a linear operator on Y. \nIf T is invertible, show that T* is invertible and (T*)-l \n(T-l)*. \n6. Let 11 be an inner product space and (3, 'Y fixed vectors in Y. Show that \nTa = (al(3)\"( defineى a linear operator on Y. Show that T has an adjoint, and \ndescribe T* explicitly. \nNow suppose 11 is Cn with the standard inner product, (3 \n(YI, . . . , Yn) , and \n'Y = (Xl, . . . , Xn). What is the j, k entry of the matrix of T in the standard ordered \nbasis? What is the rank of this matrix?",
    "describe T* explicitly. \nNow suppose 11 is Cn with the standard inner product, (3 \n(YI, . . . , Yn) , and \n'Y = (Xl, . . . , Xn). What is the j, k entry of the matrix of T in the standard ordered \nbasis? What is the rank of this matrix? \n7. Show that the product of two self-adjoint operators is self-adjoint if and only \nif the two operators commute. \nSec. 8.4 \nUnitary Operators \n8. Let V be the vector space of the polynomials over R of degree less than or \nequal to 3, with the inner product \n(fIg) = 101 f(t)g(t) dt. \nIf t is a real number, find the polynomial gt in V such that (flgt) \nJ(t) for allf in V. \n9. Let V be the inner product space of Exercise 8, and let D be the differentiation \noperator on V. Find D*. \n10. Let V be the space of n X n matrices over the complex numbers, with the \ninner product (A, B) \ntr (AB*). Let P be a fixed invertible matrix in V, and \nlet Tp be the linear operator on V defined by Tp(A) = P-IAP. Find the adjoint \nof Tp• \n11. Let V bE' a finite-dimensional inner product space, and let E be an idempotent \nlineal' operator on V, i.e., E2 \nE. Prove that E is self-adjoint if and only if \nEE* = E*E. \n12. Let V bB a finite-dimensional complex inner product spaCB, and let T be a \nlinear operator on V. Prove that T is sBlf-adjoint if and only if (Tala) is real for \nevery a in V. \n299 \n8.4. Unitary Operators \nIn this section, we consider the concept of an isomorphism between \ntwo inner product spaces. If V and W are vector spaces, an isomorphism \nof V onto W is a one-one linear transformation from V onto W, i.e., a \none-one correspondence between the elements of V and those of W, which \n'preserves' the vector space operations. Now an inner product space con­\nsists of a vector space and a specified inner product on that space. Thus, \nwhen V and W are inner product spaces, we shall require an isomorphism \nfrom V onto W not only to preserve the linear operations, but also to \npreserve inner products. An isomorphism of an inner product space onto",
    "sists of a vector space and a specified inner product on that space. Thus, \nwhen V and W are inner product spaces, we shall require an isomorphism \nfrom V onto W not only to preserve the linear operations, but also to \npreserve inner products. An isomorphism of an inner product space onto \nitself is called a 'unitary operator' on that space. We shall consider various \nexamples of unitary operators and establish their basic properties. \nDefinition. Let V and W be inner product spaces over the same field, \nand let T be a linear transformation from V into W. We say that T pre­\nserves inner products if (TaITJ3) = (aim jor all a, {3 in V. An iso­\nmorphism of V onto W is a vector space isomorphism T oj V onto W which \nalso preserves inner products. \nIf ']' preserves inner products, then IITal 1 \nI ial l and so T is neces-\nsarily non-singular. Thus an isomorphism from V onto W can also be \ndefined as a linear transformation from V onto W which preserves inner \nproducts. If T is an isomorphism of V onto W, then T-l is an isomorphism \n300 \nInner Product Spaces \nChap. 8 \nof W onto V; hence, when such a T exists, we shall simply say V and W \nare iSOllorphic. Of course, isomorphism of inner product spaces is an \nequivalence relation. \nTheorem 10. Let V and W be finite-dimensional inner product spaces \nover the same field, having the same dimension. If T is a linear transformation \nfrom V into W, the following are equivalent. \n(i) T preserves inner products. \n(ii) T is an (inner product space) isomorphism. \n(iii) T carries every orthonormal basis for V onto an orthonormal basis \nfor W. \n(iv) T carries some orthonormal basis for V onto an orthonormal basis \nfor W. \nProof. (i) -+ (ii) If T preserves inner products, then II Tall = I ial l \nfor all a in V. Thus T is non-singular, and since dim V \ndim W, we know \nthat T is a vector space isomorphism. \n(ii) -+ (iii) Suppose T is an isomorphism. Let {al, . . .  , an} be an",
    "for W. \nProof. (i) -+ (ii) If T preserves inner products, then II Tall = I ial l \nfor all a in V. Thus T is non-singular, and since dim V \ndim W, we know \nthat T is a vector space isomorphism. \n(ii) -+ (iii) Suppose T is an isomorphism. Let {al, . . .  , an} be an \northonormal basis for V. Since T is a vector space isomorphism and \ndim W \ndim V, it follows that {Tal, . . .  , Tan} is a basis for W. Since T \nalso preserves inner products, (TajITak) = (ajlak) = µjk. \n(iii) -+ (iv) This requires no comment. \n(iv) -+ (i) Let {aI, . . .  , an} be an orthonormal basis for V such that \n{Tal, . . .  , Tan} is an orthonormal basis for W. Then \n(TajI Tak) = (ajlak) = 8jk• \nFor any a = Xlal + . . . + X\"an and {3 = Ylal + . . . + y\"an in V, we have \n(TaIT(3) = (}) xjTajl }) ySak) \nj \nk \n= }) })  xjiMTajl Tak) \nj k \nn \n= }) x/fh \nj= 1  \nand so T preserves inner products. I \nCorollary. Let V and W be finite-dimensional inner product spaces \nover the same field. Then V and W are isomorphic if and only if they have \nthe same dimension. \nProof. If {al, . . .  , an} is an orthonormal basis for V and \n{,BJ, . . . , ,Bn} is an orthonormal basis for W, let T be the linear transfor­\nmation from V into TV defined by Taj = {3j. Then T is an isomorphism of \nV onto W. \nI \nSec. 8.4 \nUnitary Operators \nEXAMPLE 22. If V is an n-dimensional inner product space, then each \nordered orthonormal basis ffi \n{ai, . . .  , an} determines an isomorphism \nof V onto Fn with the standard inner product. The isomorphism is simply \nT(xlal + . . . + Xnan) = (Xl, .\n•\n•\n , X .. ) . \nThere is the superficially different isomorphism which ffi determines of V \nonto the space FnXl with (XI Y) = y* X as inner product. The isomor­\nphism is \na -+ [aJm \ni.e., the transformation sending a into its coordinate matrix in the ordered \nbasis ffi. For any ordered basis ffi, this is a vector space isomorphism; \nhowever, it is an isomorphism of the two inner product spaces if and only \nif ffi is orthonormal.",
    "phism is \na -+ [aJm \ni.e., the transformation sending a into its coordinate matrix in the ordered \nbasis ffi. For any ordered basis ffi, this is a vector space isomorphism; \nhowever, it is an isomorphism of the two inner product spaces if and only \nif ffi is orthonormal. \nEXAMPLE 23. Here is a slightly less superficial isomorphism. Let W \nbe the space of all 3 X 3 matrices A over R which are skew-symmetric, \ni.e., A t  = -A. We equip W with the inner product (A IB) = ! tr (ABt), \nthe ! being put in as a matter of convenience. Let V be the space Ra with \nthe standard inner product. Let T be the linear transformation from V \ninto W defined by \n7p, \n-Xa .,] \nT(XI, X2, Xa) \n0 -l . \n-X2 \nXl \nThen T maps V onto W, and putting \nA \nwe have \n[ 0 -Xa \"J [ 0 -ya \nXa 0 -l , \nB \nya 0 \n-X2 \nXl \n-Y2 \nYI \ntr (ABt) \nXaYa + X2Y2 + XaYa + X2YZ + XIYl \n= 2(XIYl + X2Y2 + XaY3). \ny,] \n-\nťl \nThus (al(J) \n(Tal T(3) and T is a vector space isomorphism. Note that T \ncarries the standard basis {El' €z, €a} onto the orthonormal basis consisting \nof the three matrices \n[0 0 \no 0 \no 1 \n-1 OJ \n0 0 · \no 0 \n-u [ 0 0 1J \no 0 0 ,  \n-1 0 0 \nEXAMPLE 24. It is not always particularly convenient to describe an \nisomorphism in terms of orthonormal bases. For example, suppose G = P*P \nwhere P is an invertible n X n matrix with complex entries. Let V be the \nspace of complex n X 1 matrices, with the inner product [XI YJ \nY*GX. \n301 \n802 \nInner Product Spaces \nChap. 8 \nLet W be the same vector space, with the standard inner product (XI Y) = \ny* X. We know that V and W are isomorphic inner product spaces. It \nwould seem that the most convenient way to describe an isomorphism \nbetween V and W is the following: Let T be the linear transformation \nfrom V into W defined by T(X) \nPX. Then \n(TXI TY) \n(PXIPY) \nHence T is an isomorphism. \n= (PY)*(PX) \n= Y*P*PX \nY*GX \n= [XI Y]. \nEXAMPLE 25. Let V be the space of all continuous real-valued func­\ntions on the unit interval, 0 ::; t ::; 1, with the inner product",
    "from V into W defined by T(X) \nPX. Then \n(TXI TY) \n(PXIPY) \nHence T is an isomorphism. \n= (PY)*(PX) \n= Y*P*PX \nY*GX \n= [XI Y]. \nEXAMPLE 25. Let V be the space of all continuous real-valued func­\ntions on the unit interval, 0 ::; t ::; 1, with the inner product \n[flgJ = 101 f(t)g(tW dt. \nLet W be the same vector space with the inner product \n(jIg) = 101 f(t)g(t) dt. \nLet T be the linear transformation from V into W given by \n(Tf)(t) = tf(t). \nThen (TfI Tg) = [flgJ, and so T preserves inner products; however, T is \nnot an isomorphism of V onto W, because the range of T is not all of W. \nOf course, this happens because the underlying vector space is not finite­\ndimensional. \nTheorem 11. Let V and W be inner product spaces over the same field, \nand let T be a linear transformation from V into W. Then T preserves inner \nproducts if and only if I ITal 1 = I ial l for every a in V. \nProof. If T preserves inner products, T 'preserves norms.' Sup­\npose II Tal l  = I ial l for every a in V. Then II Tall 2 = II all 2. Now using the \nappropriate polarization identity, (8-3) or (8-4), and the fact that ']' is \nlinear, one easily obtains (aim = (Tal T,,) for all a, \" in V. I \nDefinition. A unitary operator on an inner product space is an iso­\nmorphism of the space (mto itself. \nThe product of two unitary operators is unitary. For, if UI and U2 \nare unitary, then U2U1 is invertible and II U2U1all \nI I  Ulall \nlIall for \neach a. Also, the inverse of a unitary operator is unitary, since I I  U all \n/ lall says I I U-I\"II = 1 1\"11, where \" = Ua. Since the identity operator is \nSec. 8.4 \nUnitary Operators \nclearly unitary, we see that the set of all unitary operators on an inner \nproduct space is a group, under the operation of composition. \nIf V is a finite-dimensional inner produet spaee and U is a linear \noperator on V, Theorem 10 tells us that U is unitary if and only if \n( U  al U(3) = (al{3) for each a, {3 in V ;  or, if and only if for some (every)",
    "product space is a group, under the operation of composition. \nIf V is a finite-dimensional inner produet spaee and U is a linear \noperator on V, Theorem 10 tells us that U is unitary if and only if \n( U  al U(3) = (al{3) for each a, {3 in V ;  or, if and only if for some (every) \northonormal basis {al, . . .  , an} it is true that {Ual, . . .  , Uan} is an \northonormal basis. \nTheorem 12. Let U be a linear operator on an inner prod 'ro( space V. \nThen U is unitary if and only if the adjoint U* of U eX'ists and IT* = \nU*U = I. \nProof. Suppose U is unitary. Then U is invertible and \n(UalfJ) = ( UaI UU-I{3) = (aI U-l{3) \nfor all a, fJ. Hence U-I is the adjoint of U. \nConversely, suppose U* exists and UU* = U*U = I. Then U is \ninvertible, with U-I \nU*. So, we need only show that U preserves inner \nproducts. We have \nfor all a, {3. I \n(UaI U{3) = (aI U*U{3) \n= (aII{3) \n(aim \nEXAMPLE 26. Consider CnXl with the inner product (XI Y) \ny* X. \nLet A be an n X n matrix over C, and let U be the linear operator defined \nby U(X) \n= AX. Then \n(UXI UY) = (AXIA Y) = Y*A*AX \nfor all X, Y. Hence, U is unitary if and only if A * A = I. \nDefinition. A complex n X n matrix A 1'8 called unitary, if A * A \n1. \nTheorem 13. Let V be a finite-dimensional inner product space and \nlet U be a linear operator on V. Then U is unitary if and only if the matrix \nof U in some (or every) ordered orthonormal basis is a unitary matrix. \nProof. At this point, this is not much of a theorem, and we state \nit largely for emphasis. If CB = {al, . . .  , an} is an ordered orthonormal \nbasis for V and A is the matrix of U relative to CB, then A * A \n= I if and \nonly if U*U \nI. The result now follows from Theorem 12. I \nLet A be an n X n matrix. The statement that A is unitary simply \nmeans \n(A * A)jk = {jjk \n303 \n304 \nI nner Product Spaces \nor \nn \n´ ArjArk = Ojk. \nr=l \nChap, 8 \nIn other words, it means that the columns of A form an orthonormal set",
    "only if U*U \nI. The result now follows from Theorem 12. I \nLet A be an n X n matrix. The statement that A is unitary simply \nmeans \n(A * A)jk = {jjk \n303 \n304 \nI nner Product Spaces \nor \nn \n´ ArjArk = Ojk. \nr=l \nChap, 8 \nIn other words, it means that the columns of A form an orthonormal set \nof column matrices, with respect to the standard inner product (Xl Y) = \ny*X. Since A * A = I if and only if AA * = I, we see that A is unitary \nexactly when the rows of A comprise an orthonormal set of n-tuples in en \n(with the standard inner product). So, using standard inner products, \nA is unitary if and only if the rows and columns of A are orthonormal sets. \nOne sees here an example of the power of the theorem which states that a \none-sided inverse for a matrix is a two-sided inverse. Applying this theorem \nas we did above, say to real matrices, we have the following: Suppose we \nhave a square array of real numbers such that the sum of the squares of \nt.he entries in each row is 1 and distinct rows are orthogonal. Then the \nsum of the squares of the entries in each column is 1 and distinct columns \nare orthogonal. Write down the proof of this for a 3 X 3 array, without \nusing any knowledge of matrices, and you should be reasonably impressed. \nDefinition. A real or complex n X n matrix A is said to be orthogo­\nnal, if AtA \n1. \nA real orthogonal matrix is unitary; and, a unitary matrix IS \northogonal if and only if each of its entries is real. \nEXAMPLE 27. We give some examples of unitary and orthogonal \nmatrices. \n(a) A 1 X 1 matrix [c] is orthogonal if and only if c \n± 1, and \nunitary if and only if cc = 1. The latter condition means (of course) that \nlei = 1, or e = ei9, where 8 is real. \n(b) Let \nA [8 ml \nThen A is orthogonal if and only if \nA t \nA-1 = \nThe determinant of any orthogonal matrix is easily seen to be ± 1. Thus \nA is orthogonal if and only if \nor \nA = [  !J \nA = l-\na \nbJ \nb \n-a \nSec. 8.4 \nUnitary Operators",
    "lei = 1, or e = ei9, where 8 is real. \n(b) Let \nA [8 ml \nThen A is orthogonal if and only if \nA t \nA-1 = \nThe determinant of any orthogonal matrix is easily seen to be ± 1. Thus \nA is orthogonal if and only if \nor \nA = [  !J \nA = l-\na \nbJ \nb \n-a \nSec. 8.4 \nUnitary Operators \nwhere a2 + b2 = 1. The two cases are distinguished by the value of det A .  \n(c) The well-known relations between the trigonometric functions \nshow that the matrix \n-sin OJ \ncos 0 \nis orthogonal. If 0 is a real number, then Ae is the matrix in the standard \nordered basis for R2 of the linear operator Ue, rotation through the angle o. \nThe statement that Ae is a real orthogonal matrix (hence unitary) simply \nmeans that Ue is a unitary operator, i.e., preserves dot products. \n(d) Let \nA = [8 ǧJ \nThen A is unitary if and only if \n[0: c] \n1 [ d  -b] \nb \nd \n= ad - be -e\na · \nThe determinant of a unitary matrix has absolute value 1, and is thus a \ncomplex number of the form eie, () real. Thus A is unitary if and only if \nwhere 0 is a real number, and a, b are complex numbers such that \nlal2 + IW = 1. \nAs noted earlier, the unitary operators on an inner product space \nform a group. From this and Theorem 13 it follows that the set U(n) of \nall n X n unitary matrices is also a group. Thus the inverse of a unitary \nmatrix and the product of two unitary matrices are again unitary. Of \ncourse this is easy to see directly. An n X n matrix A with complex entries \nis unitary if and only if A -1 = A *. Thus, if A is unitary, we have (A -1)-1 = \nA = (A *)-1 = (A-1)*. If A and B are n X n unitary matrices, then \n(AB)-l = B-IA-l = B*A * = (AB)*. \nThe Gram-Schmidt process in Cn has an interesting corollary for \nmatrices that involves the group U(n). \nTheorem 14. For every invertible complex n X n matrix B there exists \na unique lower-triangular matrix M with positive entries on the main diagonal \nsuch that MB is unitary. \nProof. The rows (31, . . .  , (3n of B form a basis for Cn. Let aI, . . .  , an",
    "matrices that involves the group U(n). \nTheorem 14. For every invertible complex n X n matrix B there exists \na unique lower-triangular matrix M with positive entries on the main diagonal \nsuch that MB is unitary. \nProof. The rows (31, . . .  , (3n of B form a basis for Cn. Let aI, . . .  , an \nbe the vectors obtained from {h, . . .  , (3n by the Gram-Schmidt process. \nThen, for 1 ::; k ::; n, {all . . .  , ak} is an orthogonal basis for the subspace \nspanned by {(31) .\n•\n.\n , (3k} ,  and \n305 \n306 \nInner Product Spaces \nHence, for each k there exist unique scalars Ckj such that \nCik = (:h - ´ Ckj{3j. \nj<k \nLet U be the unitary matrix with rows \nand M the matrix defined by \nCil \nCin \nIICiIII' . . .  , IlCinll \n1 \nC \nif J' < k  \n-IICikll ' kj, \n1 \n'f '  k \nII Cikll , \n1 J \n= \n0, if j > k. \nChap. 8 \nThen M is lower-triangular, in the sense that its entries above the main \ndiagonal are 0, The entries Mkk of M on the main diagonal are all > 0, and \n1 :;; k :;; n. \nN ow these equations simply say that \nU = ME. \nTo prove the uniqueness of M, let T+(n) denote the set of all complex \nn X n lower-triangular matrices with positive entries on the main diagonal. \nSuppose MI and M2 are elements of T+(n) such that M;B is in U(n) for \ni = 1, 2, Then because U(n) is a group \n(MIB)(M2B)-1 = MIMil \nlies in U(n). On the other hand, although it is not entirely obvious, T+(n) \nis also a group under matrix multiplication. One way to see this is to con­\nsider the geometric properties of the linear transformations \nX -+ MX, \n(M in T+(n» \non the space of column matrices, Thus Mit, MIMit, and (MIMi 1)_1 are \nall in T+(n) , But, since MIMil is in U(n), (MlMil)-l = (MlMil)*. The \ntranspose or conjugate transpose of any lower-triangular matrix is an \nupper-triangular matrix, Therefore, MIMi! is simultaneously upper­\nand lower-triangular, i.e., diagonal. A diagonal matrix is unitary if and \nonly if each of its entries on the main diagonal has absolute value 1 ;  if the",
    "transpose or conjugate transpose of any lower-triangular matrix is an \nupper-triangular matrix, Therefore, MIMi! is simultaneously upper­\nand lower-triangular, i.e., diagonal. A diagonal matrix is unitary if and \nonly if each of its entries on the main diagonal has absolute value 1 ;  if the \ndiagonal entries are all positive, they must equal 1. Hence MIMi I = I \nand Ml \n= M2• \nI \nSec. 8.4 \nUnitary Operators \n. Let GL(n) denote the set of all invertible complex n X n matrices. \nThen GL(n) is also a group under matrix multiplication. This group is \ncalled the general linear group. Theorem 14 is equivalent to the fol­\nlowing result. \nCorollary. For each B in GL(n) there exist unique matrices N and U \nsuch that N is in T+(n), U is in U(n), and \nB \nN ·  U. \nProof. By the theorem there is a unique matrix M in T+(n) such \nthat MB is in U(n). Let MB = U and N = M-I. Then N is in T+(n) and \nB = N . U. On the other hand, if we are given any elements N and U \nsuch that N is in T+(n), U is in U(n), and B \nN ·  U, then N-IB is in \nU(n) and N-l is the unique matrix M which is characterized by the \ntheorem; furthermore U is necessarily N-IB. I \nEXAMPLE 28. Let Xl and X2 be real numbers such that xi + xৌ = 1 \nand Xl :;C O. Let \nB = [ŕ1 2 ŀJ' \n0\n0\n1\n \nApplying the Gram-Schmidt process to the rows of B, we obtain the \nvectors \nal \n(Xl, X2, 0) \na2 = (0, 1, 0) - X2(XI, X2, 0) \n= XI( -X2, Xl, 0) \na3 = (0, 0, 1). \nLet U be the matrix with rows ai, (adxI), as. Then U is unitary, and \nNow multiplying by the inverse of \n[-; \n0 :J \nM \n1 \nXl Xl \n0 \n0 \nwe find that \n[Ł' \nX2 łJ \n­ [Ŀ \n0 \n0J[ X, X2 n \n1 \nXl Ǧ \n-2 Xl \n0 \n0 \n0 \n307 \n308 \nInner Product Spaces \nChap. 8 \nLet us now consider briefly change of coordinates in an inner product \nspace. Suppose V is a finite-dimensional inner product space and that \nCB = {aI, . . .  , an} and CB' = {ai, . . .  , aŲ} are two ordered orthonormal \nbases for V. There is a unique (necessarily invertible) n X n matrix P \nsuch that",
    "space. Suppose V is a finite-dimensional inner product space and that \nCB = {aI, . . .  , an} and CB' = {ai, . . .  , aŲ} are two ordered orthonormal \nbases for V. There is a unique (necessarily invertible) n X n matrix P \nsuch that \nfor every a in V. If U is the unique linear operator on V defined by \nUaj = aj, then P is the matrix of U in the ordered basis CB :  \nn \nak = ´ Pjkaj. \nj = l \nSince CB and CB' are orthonormal bases, U is a unitary operator and P is \na unitary matrix. If T is any linear operator on V, then \n[Tlil' = P-l[ThP = P*[T]mP. \nDefinition. Let A and B be complex n X n matrices. We say that B \nis unitarily equivalent to A if there is an n X n unitary matrix P such \nthat B = P-IAP. We say that B is orthogonally equivalent to A if there \nis an n X n orthogonal matrix P such that B = P-IAP. \nWith this definition, what we observed above may be stated as \nfollows: If CB and CB' are two ordered orthonormal bases for V, then, for \neach linear operator T on V, the matrix [TJm' is unitarily equivalent to \nthe matrix [TJm. In case V is a real inner product space, these matrices \nare orthogonally equivalent, via a real orthogonal matrix. \nExercises \n1. Find a unitary matrix which is not orthogonal, and find an orthogonal matrix \nwhich is not unitary. \n2. Let V be the space of complex n X n matrices with inner product (AlB) \n= \ntr (AB*). For each M in V, let TM be the linear operator defined by TM(A) = MA. \nShow that TM is unitary if and only if M is a unitary matrix. \n3. Let V be the set of complex numbers, regarded as a real vector space. \n(a) Show that (aliJ) = Re (as) defines an inner product on V. \n(b) Exhibit an (inner product space) isomorphism of V onto R2 with the \nstandard inner product. \n(c) For each l' in V, let M'Y be the linear operator on V defined by M'Y(a) = 'Ya. \nShow that (M'Y)* = M;y. \n(d) For which complex numbers l' is M'Y self-adjoint? \n(e) For which l' is M'Y unitary? \nSec. 8.4 \n(f) For which \"I is M'Y positive?",
    "standard inner product. \n(c) For each l' in V, let M'Y be the linear operator on V defined by M'Y(a) = 'Ya. \nShow that (M'Y)* = M;y. \n(d) For which complex numbers l' is M'Y self-adjoint? \n(e) For which l' is M'Y unitary? \nSec. 8.4 \n(f) For which \"I is M'Y positive? \n(g) What is det (M,,!)? \n(h) Find the matrix of M'Y in the basis {I, i} . \nUnitary Operators \n(i) If T is a linear operator on V, find necessary and sufficient conditions \nfor T to be an M'Y' \n(j) Find a unitary operator on V which is not an M 'Y' \n4. Let V be R2, with the standard inner product. If U is a unitary operator on V, \nshow that the matrix of U in the standard ordered basis is either \n[COS 8 \n-sin 8J \n[COS 8 \nsin 8 \ncos 0 \nor \nsin 0 \nsin 8J \n-cos () \nfor some real 8, 0 .::; 8 < 27r. Let Ua be the linear operator corresponding to the \nfirst matrix, i.e., Uo is rotation through the angle O. Now convince yourself that \nevery unitary operator on V is either a rotation, or reflection about the ͏l-axis \nfollowed by a rotation. \n(a) What is UoU<I>? \n(b) Show that U;; = U_o. \n(c) Let 4> be a fixed real number, and let <B = {ai, a2} be the orthonormal \nbasis obtained by rotating {EI, ͏2} through the angle 4>, i.e., ai = U\",Ei' If 8 is \nanother real number, what is the matrix of Uo in the ordered basis <B? \n5. Let V be Ra, with the standard inner product. Let W be the plane spanned \nby a = (1, 1, 1) and {3 = (1, 1, -2) .  Let U be the linear operator defined, geo­\nmetrically, as follows: U is rotation through the angle 0, about the straight line \nthrough the origin which is orthogonal to W. There are actually two such rotations \n-choose one. Find the matrix of U in the standard ordered basis. (Here is one \nway you might proceed. Find al and a2 which form an orthonormal basis for W. \nLet a3 be a vector of norm 1 which is orthogonal to W. Find the matrix of U in \nthe basis {aI, a2, aa} . Perform a change of basis.) \n6. Let V be a finite-dimensional inner product space, and let W be a subspace",
    "way you might proceed. Find al and a2 which form an orthonormal basis for W. \nLet a3 be a vector of norm 1 which is orthogonal to W. Find the matrix of U in \nthe basis {aI, a2, aa} . Perform a change of basis.) \n6. Let V be a finite-dimensional inner product space, and let W be a subspace \nof V. Then V = W EB  W\\ that is, each a in V is uniquely expressible in the form \na = {3 + \"I, with {3 in W and \"I in Wl.. Define a linear operator U by Ua = (3 - \"I. \n(a) Prove that U is both self-adjoint and unitary. \n(b) If V is R3 with the standard inner product and W is the subspace spanned \nby (1, 0, 1), find the matrix of U in the standard ordered basis. \n7. Let V be a complex inner product space and T a self-adjoint linear operator \non V. Show that \n(a) \\\\a + iTa\\\\ = \\\\a - iTa\\ \\ for every a in V. \n(b) a + iTa = {3 + iT(3 if and only if a = {3. \n(c) I + iT is non-singular. \n(d) I - iT is non-singular. \n(e) Now suppose V is finite-dimensional, and prove that \nU = (I - iT)(I + iT)-1 \nis a unitary operator; U is called the Cayley transform of T. In a certain sense, \nU = jeT), where j(x) = (1 \n- ix)/(l + ix). \n309 \n310 \nI nner Product Spaces \nChap. 8 \n8. If 0 is a real number, prove that the following matrices are unitarily equivalent \n[COS 0 -sin OJ \nsin 0 \ncos 0 ' [ei8 \n0 J \no \ne-i8 \n9. Let V be a finite-dimensional inner product space and T a positive linear \noperator on V. Let PT be the inner product on V defined by PT(a, (3) \n(Tal(3). \nLet U be a linear operator on V and U* its adjoint with respect to ( I ). Prove \nthat U is unitary with respect to the inner product PT if and only if T = U*TU. \n10. Let V be a finite-dimensional inner product space. For each a, (3 in V, let \nT\",{l be the linear operator on V defined by Ta ,Il('Y) \n('YI(3)a. Show that \n(a) T;,{l = Til,,,. \n(b) trace (T\",{l) = (al(3). \n(c) T a,!i'l\\,. \nT\",(fll'Y).' \n(d) Under what conditions is T a,f3 self-adjoint? \nII. Let V be an n-dimensional inner product space over the field P, and let L(V, V)",
    "T\",{l be the linear operator on V defined by Ta ,Il('Y) \n('YI(3)a. Show that \n(a) T;,{l = Til,,,. \n(b) trace (T\",{l) = (al(3). \n(c) T a,!i'l\\,. \nT\",(fll'Y).' \n(d) Under what conditions is T a,f3 self-adjoint? \nII. Let V be an n-dimensional inner product space over the field P, and let L(V, V) \nbe the space of linear operators on V. Show that there is a unique inner product \non L(V, V) with the property that IIT\",/l1 12 \nIla1121 1(3W for all a, (3 in V. (T\",f3 \nis the operator defined in Exercise 10.) Find an isomorphism between L(V, V) \nwith this inner product and the space of n X n matrices over P, with the inner \nproduct (A IB) = tr (AB*). \n12. Let V be a finite-dimensional inner product space. In Exercise 6, we showed \nhow to construct some linear operators on V which are both self-adjoint and \nunitary. Now prove that there are no others, i.e., that every self-adjoint unitary \noperator arises from some subspace W as we described in Exercise 6. \n1 3. Let V and W be finite-dimensional inner product spaces having the same \ndimension. Let U be an isomorphism of V onto W. Show that: \n(a) The mapping T -+ UTU-l is an isomorphism of the vector space L(V, V) \nonto the vector spaee L(W, W). \n(b) trace (UTU-l) \n= traee (T) for each T in L(V, V). \n(c) UT\"./lU-I = Tua,u/l (T\",C defined in Exercise 10). \n(d) ( UTU-l)* = UT*U-I. \n(e) If we equip L(V, V) with inner product (T1IT2) = trace (Tin), and \nsimilarly for L(W, W), then T -+ UTU-l is an inner product space isomorphism. \n14. If V is an inner product space, a rigid motion is any function T from V \ninto V (not necessarily linear) such that II Ta - Ti311 = Iia - (311 for all a, i3 in V. \nOne example of a rigid motion is a linear unitary operator. Another example is \ntranslation by a fixed vector 'Y :  \n(a) Let V be R2 with the standard inner product. Suppose T is a rigid motion \nof V and that T(O) \nO. Prove that T is linear and a unitary operator.",
    "One example of a rigid motion is a linear unitary operator. Another example is \ntranslation by a fixed vector 'Y :  \n(a) Let V be R2 with the standard inner product. Suppose T is a rigid motion \nof V and that T(O) \nO. Prove that T is linear and a unitary operator. \n(b) Use the result of part (a) to prove that every rigid motion of R2 is com­\nposed of a translation, followed by a unitary operator. \n(c) Now show that a rigid motion of R2 is either a translation followed by a \nrotation, or a translation followed by a reflection followed by a rotation. \nSec. 8.5 \nNormal Operators \n15. A unitary operator on R4 (with the standard inner product) is simply a linear \noperator which preserves the quadratic form \nII(x, y, Z, t)112 = x2 + y2 + Z2 + t2 \nthat is, a linear operator V such that II [JaW = IIal12 for all a in R4. In a certain \npart of the theory of relativity, it is of interest to find the linear operators T which \npreserve the form \nII(x, y, z, t)lll = t2 - x2 - y2 - Z2. \nNow I I  Iii does not come from an inner product, but from something called \nthe 'Lorentz metric' (which we shall not go into). For that reason, a linear operator \nT on R4 such that IITalli = Iialli, for every a in R4, is called a Lorentz \ntransformation. \n(a) Show that the function V defined by \nV(x,y,z,t) = \n. \n[t + x \n11 - LZ \n11 + iZ] \nt - x  \nis an isomorphism of R4 onto the real vector space II of all self-adjoint 2 X 2 \ncomplex matrices. \n(b) Show that Iialli = det (Va). \n(c) Suppose T is a (real) linear operator on the space II of 2 X 2 self-adjoint \nmatrices. Show that L = V-lTV is a linear operator on R4. \n(d) Let M be any 2 X 2 complex matrix. Show that 7'M(A) \nM* AM defines \na linear operator TM on II. (Be sure you check that TM maps II into H.) \n(e) If M is a 2 X 2 matrix such that Idet MI \n1, show that LM V-lTMV \nis a Lorentz transformation on R4. \n(f) Find a Lorentz transformation which is not an LM• \n311 \nB.S. Normal Operators",
    "M* AM defines \na linear operator TM on II. (Be sure you check that TM maps II into H.) \n(e) If M is a 2 X 2 matrix such that Idet MI \n1, show that LM V-lTMV \nis a Lorentz transformation on R4. \n(f) Find a Lorentz transformation which is not an LM• \n311 \nB.S. Normal Operators \nThe principal objective in this section is the solution of the following \nproblem. If T is a linear operator on a finite-dimensional inner product \nspace V, under what conditions does V have an orthonormal basis con­\nsisting of characteristic vectors for T? J n other words, when is there an \northonormal basis (B for V. such that the matrix of T in the basis (B is \ndiagonal? \nWe shall begin by deriving some necessary conditions on T, which \nwe shall subsequently show are sufficient. Suppose (B = {aI' . . .  , an} is \nan orthonormal basis for V with the property \n(8-16) \nj \n1, .\n.\n.\n , n. \nThis simply says that the matrix of T in the ordered basis (B is the diagonal \nmatrix with diagonal entries CI, .\n•\n•\n , Cn. The adjoint operator T* is repre­\nsented in this same ordered basis by the conjugate transpose matrix, i.e., \nthe diagonal matrix with diagonal entries el, . . . , en' If V is a real inner \n312 \nInner Product Spaces \nChap. 8 \nproduct space, the scalars CI, . . . , Cn are (of course) real, and so it must \nbe that T \nT* .In other words, if V is a finite-dimensional real inner \npro but space and T is a linear operator for which there is an orthonormal \nbasis of characteristic vectos, then T must be self-adjoint. If V is a com­\nplex inner product space, the scalars CI, •\n•\n•\n , Cn need not be real, i.e., \nT need not be self-adjoint. But notice that T must satisfy \n(8-17) \nTT* \nT*T. \nFor, any two diagonal matrices commute, and since T and T* are both \nrepresented by diagonal matrices in the ordered basis CB, we have (8-17). \nIt is a rather remarkable fact that in the complex case this condition is \nalso sufficient to imply the existence of an orthonormal basis of charac­\nteristic vectors.",
    "represented by diagonal matrices in the ordered basis CB, we have (8-17). \nIt is a rather remarkable fact that in the complex case this condition is \nalso sufficient to imply the existence of an orthonormal basis of charac­\nteristic vectors. \nDefinition. Let V be a finite-dimensional inner product space and T a \nlinear operator on V. We say that T is normal if it commutes with its adjoint \ni.e., TT* = T*T. \nAny self-adjoint operator is normal, as is any unitary operator. Any \nscalar multiple of a normal operator is normal; however, sums and prod­\nucts of normal operators are not generally normal. Although it is by no \nmeans necessary, we shall begin our study of normal operators by con­\nsidering self-adjoint operators. \nTheorem 15. Let V be an inner product space and T a self-adjoint \nlinear operator on V. Then each characteristic value of T is real, and char­\nacteristic vectors of T associated with distinct characteristic values are \northogonal. \nProof. Suppose c is a characteristic value of T, i.e., that Ta = Ca \nfor some non-zero vector a. Then \nc(ala) = (cala) \n= (Tala) \n= (al Ta) \n= (alca) \n= c(ala). \nSince (ala) :;C 0, we must have c \nc. Suppose we also have T{3 \nd{3 with \n(3 :;C O. Then \nIf c :;C d, then (al,6) \nO. I \nc(al{3) = (Tal{3) \n(al T(3) \n= (ald(3) \n= d(al{3) \nd(al{3)· \nSec. 8.5 \nNormal Operators \nIt should be pointed out that Theorem 15 says nothing about the \nexistence of characteristic values or characteristic vectors. \nTheorem 16. On a finite-dimensional inner product space of positive \ndimension, every self-adjoint operator has a (non-zero) characteristic vector. \nProof. Let V be an inner product space of dimension n, where \nn > 0, and let T be a self-adjoint operator on V. Choose an orthonormal \nbasis CB for V and let A = [T]m. Since T = T*, we have A = A *. Now \nlet W be the space of n X 1 matrices over C, with inner product (XI Y) \n= \ny*x. Then U(X) \n= AX defines a self-adjoint linear operator U on W.",
    "n > 0, and let T be a self-adjoint operator on V. Choose an orthonormal \nbasis CB for V and let A = [T]m. Since T = T*, we have A = A *. Now \nlet W be the space of n X 1 matrices over C, with inner product (XI Y) \n= \ny*x. Then U(X) \n= AX defines a self-adjoint linear operator U on W. \nThe characteristic polynomial, det (xl - A), is a polynomial of degree n \nover the complex numbers ; every polynomial over C of positive degree \nhas a root. Thus, there is a complex number c such that det (cl - A) = O. \nThis means that A - cl is singular, or that there exists a non-zero X \nsuch that A X  = cX. Since the operator U (multiplication by A) is self­\nadjoint, it follows from Theorem 15 that c is real. If V is a real vector \nspace, we may choose X to have real entries. For then A and A - cl have \nreal entries, and since A - cl is singular, the system (A - cI)X = 0 has \na non-zero real solution X. It follows that there is a non-zero vector 0: in \nV such that To: = ca. I \nThere are several comments we should make about the proof. \n(1) The proof of the existence of a non-zero X such that A X  = eX \nhad nothing to do with the fact that A was Hermitian (self-adjoint). It \nshows that any linear operator on a finite-dimensional complex vector \nspace has a characteristic vector. In the case of a real inner product space, \nthe self-adjointness of A is used very heavily, to tell us that each charac­\nteristic value of A is real and hence that we can find a suitable X with \nreal entries. \n(2) The argument shows that the characteristic polynomial of a self­\nadjoint matrix has real coefficients, in spite of the fact that the matrix \nmay not have real entries. \n(3) The assumption that V is finite-dimensional is necessary for the \ntheorem; a self-adjoint operator on an infinite-dimensional inner product \nspace need not have a characteristic value. \nEXAMPLE 29. Let V be the vector space of continuous complex­\nvalued (or real-valued) continuous functions on the unit interval,",
    "theorem; a self-adjoint operator on an infinite-dimensional inner product \nspace need not have a characteristic value. \nEXAMPLE 29. Let V be the vector space of continuous complex­\nvalued (or real-valued) continuous functions on the unit interval, \no ã t ã 1, with the inner product \n(fIg) = 101 f(t)g(t) dt. \nThe operator 'multiplication by t,' (Tf)(t) = tj(t) , is self-adjoint. Let us \nsuppose that Tf = cf. Then \n313 \n314 \nInner Product Spaces \nChap. 8 \n(t \nc)f(t) \n0, \n0 ::; t ::; 1 \nand so f(t) \n0 for t :;t: c. Since f is continuous, f \nO. Hence T has no \ncharacteristic values (vectors). \nTheorem 17. Let V be a finite-dimensional inner product space, and \nlet T be any linear operator on V. 81্ppose W is a subspace of V 'Which is \ninvariant under T. Then the orthogonal complement of W is invariant \nunder '1'*. \nProof. We recall that the fact that W is invariant under T does \nnot mean that each vector in W is left fixed by T; it means that if a is in \nW then Ta is in W. Let {3 be in W.1. We must show that T*{3 is in W\\ \nthat is, that (al T*(3) \n0 for every a in W. If a is in W, then Ta is in W, \nso (Taj{3) \nO. But (Tal{3) \n(aI T*(3). I \nTheorem 18. Let V be a finite-dimensional inner product space, and \nlet T be a self-adjoint linear operator on V. Then there is an orthonormal basis \nfor V, each vector of 'Which is a characteristic vector for T. \nProof. We are assuming dim V > O. By Theorem 16, T has a \ncharacteristic vector a. Let 0.1 = ali ial l so that al is also a characteristic \nvector for T and lIalli \n1. If dim V \n1, we are done. Now we proceed \nby induction on the dimension of V. Suppose the theorem is true for inner \nproduct spaces of dimension less than dim V. Let W be the one-dimensional \nsubspace spanned by the vector al. The statement that al is a characteristic \nvector for T simply means that W is invariant under T. By Theorem 17, \nthe orthogonal complement W.1 is invariant under '1'* = T. Now W.1,",
    "product spaces of dimension less than dim V. Let W be the one-dimensional \nsubspace spanned by the vector al. The statement that al is a characteristic \nvector for T simply means that W is invariant under T. By Theorem 17, \nthe orthogonal complement W.1 is invariant under '1'* = T. Now W.1, \nwith the inner product from V, is an inner product space of dimension \none less than the dimension of V. Let U be the linear operator induced \non W.1 by T, that is, the restriction of T to W.1. Then U is self-adjoint, \nand by the induction hypothesis, W.1 has an orthonormal basis {0.2' •\n•\n•\n , an} \nconsisting of characteristic vectors for U. Now each of these vectors is \nalso a characteristic vector for T, and since V = W EB W.1, we conclude \nthat {ai, . . .  , an} is the desired basis for V. I \nCorollary. Let A be an n X n Hermitian (self-adjoint) matrix. Then \nthere is a unitary matrix P such that P-IAP is diagonal (A is unitarily \nequivalent to a diagonal matrix). If A is a real symmetric matrix, there is a \nreal orthogonal matrix P such that P-IAP is diagonal. \nProof. Let V be CnXl, with the standard inner product, and let T \nbe the linear operator on V which is represented by A in the standard \nordered basis. Since A = A *, we have T = T*. Let il \n{ai, . . .  , a,.} \nbe an ordered orthonormal basis for V, such that Taj \nCjaj, j \n1, . . .  , n. \nIf D = [TJ<l, then D is the diagonal matrix with diagonal entries CI, .\n•\n.\n , cn• \nLet P be the matrix with column vectors ai, . . .  , an' Then D = P-IAP. \nSec. 8.5 \nNormal Operators \nIn case each entry of A is real, we can take V to be Rn, with the \nstandard inner product, and repeat the argument. In this case, P will be \na unitary matrix with real entries, i.e., a real orthogonal matrix. I \nCombining Theorem 18 with our comments at the beginning of this \nsection, we have the following: If V is a finite-dimensional real inner \nproduct space and '1' is a linear operator on V, then V has an orthonormal",
    "a unitary matrix with real entries, i.e., a real orthogonal matrix. I \nCombining Theorem 18 with our comments at the beginning of this \nsection, we have the following: If V is a finite-dimensional real inner \nproduct space and '1' is a linear operator on V, then V has an orthonormal \nbasis of characteristic vectors for T if and only jf '1' is self-adjoint. Equiv­\nalently, if A is an n X n matrix with real entries, there is a real orthogonal \nmatrix P such that ptAP is diagonal if and only if A \nA t. There is no \nsuch result for complex symmetric matrices. Tn other words, for complex \nmatrices there is a significant difference between the conditions A = A t  \nand A \nA*. \nHaving disposed of the self-adjoint case, we now return to the study \nof normal operators in general. We shall prove the analogue of Theorem 18 \nfor normal operators, in the complex case. There is a reason for this restric­\ntion. A normal operator on a real inner product space may not have any \nnon-zero characteristic vectors. This is true, for example, of all but two \nrotations in R2. \nTheorem 19. Let V be a finite-dimensional inner product space and \nT a normal operator on V. Suppose a is a vector in V. Then a is a charac­\nteristic vector for T with characteristic value c 'if and only if a is a charac­\nteristic vector for T* with characteristic val'ue c. \nProof. Suppose U is any normal operator on V. Then II U al l \nI I U*all. For using the condition UU* = U*U one sees that \nI I UaW = ( UaI Ua) = (aI U*Ua) \n(al UU*a) \n(U*al U*a) \nI I U*aI 12• \nIf c is any scalar, the operator U \n'1' \ncI is normal. For ('1' \ncI)* \n'1'* - el, and it is easy to check that UU* = U*U. Thus \n1 1 ('1' - cI)al l  = 1 1 ('1'* - cI)al l  \nso that ('1' - cI)a = 0 if and only if ('1'* - cI)a = O. I \nDefinition. A complex n X II matrix A is called normal if AA * = \nA*A. \nIt is not so easy to understand what normality of matrices or oper­\nators really means; however, in trying to develop some feeling for the",
    "so that ('1' - cI)a = 0 if and only if ('1'* - cI)a = O. I \nDefinition. A complex n X II matrix A is called normal if AA * = \nA*A. \nIt is not so easy to understand what normality of matrices or oper­\nators really means; however, in trying to develop some feeling for the \nconcept, the reader might find it helpful to know that a triangular matrix \nis normal if and only if it is diagonal. \nTheorem 20. Let V be a finite-dimensional inner product space, T a \nlinear operator on V, and il an orthonormal basis for V. Suppose that the \n315 \n316 \nInner Product Spaces \nChap. 8 \nmatrix A of T in the basis CB is upper triangular. Then T is normal if and \nonly if A is a diagonal matrix. \nProof. Since CB is an orthonormal basis, A * is the matrix of T* \nin CB. If A is diagonal, then AA * \nA * A, and this implies TT* \nT*T. \nConversely, suppose T is normal, and let ili \n{aI, . . . , an} . Then, since \nA is upper-triangular, Tal = Anal. By Theorem 19 this implies, T*al = \nAnal. On the other hand, \nT*al = ´ (A *)jlaj \nj \n= ´ Aljaj. \nj \nTherefore, Ali \n0 for every j > 1. In particular, Al2 \n0, and since A \nis upper-triangular, it follows that \nTa2 = A22a2. \nThus T*a2 = A22a2 and A2i = 0 for all j ჭ 2. Continuing in this fashion, \nwe find that A is diagonal. I \nTheorem 21. Let V be a finite-dimensional complex inner product \nspace and let T be any linear operator on V. Then there is an orthonormal \nbasis for V in which the matrix of T is upper triang1tiar. \nProof. Let n be the dimension of V. The theorem is true when \nn = 1, and we proceed by induction on n, assuming the result is true for \nlinear operators on complex inner product spaces of dimension n - 1. \nSince V is a finite-dimensional complex inner product space, there is a \nunit vector a in V and a scalar c such that \nT*a \nCa. \nLet W be the orthogonal complement of the subspace spanned by a and \nlet S be the restriction of T to W. By Theorem 17, W is invariant under T.",
    "Since V is a finite-dimensional complex inner product space, there is a \nunit vector a in V and a scalar c such that \nT*a \nCa. \nLet W be the orthogonal complement of the subspace spanned by a and \nlet S be the restriction of T to W. By Theorem 17, W is invariant under T. \nThus S is a linear operator on W. Since W has dimension n - 1, our \ninductive assumption implies the existence of an orthonormal basis \n{aI, . . .  , an-I} for W in which the matrix of S is upper-triangular; let \nan = a. Then {ai, . . .  , an} is an orthonormal basis for V in which the \nmatrix of T is upper-triangular. I \nThis theorem implies the following result for matrices. \nCorollary. For every complex n X n matrix A there is a unitary matrix \nU such that U-IAU is upper-triangular. \nN ow combining Theorem 21 and Theorem 20, we immediately obtain \nthe following analogue of Theorem 18 for normal operators. \nSec. 8.5 \nNormal Operators \nTheorem 22. Let V be a finite-dimensional complex inner product \nspace and T a normal operator on V. Then V has an orthonormal basis con­\nsisting of characteristic vectors for T. \nAgain there is a matrix interpretation. \nCorollary. For every normal matrix A there is a unitary matrix P \nsuch that P-IAP is a diagonal matrix. \nExercises \n1. For each of the following real symmetric matrices A, find a real orthogonal \nmatrix P such that PtAP is diagonal. \n[COS 0 \nsin 0 \nsin OJ \n-cos () \n2. Is a complex symmetric matrix self-adjoint? Is it normal? \n3. For \n[1 \n2 3J \nA = \n2 3 4 \n3\n4\n5\n \nthere is a real orthogonal matrix P such that PtAP = D is diagonal. Find such a \ndiagonal matrix D. \n4. Let V be Cz, with the standard inner product. Let T be the linear operator on \nV which is represented in the standard ordered basis by the matrix \nShow that T is normal, and find an orthonormal basis for V, consisting of charac­\nteristic vectors for T. \n5. Givp an example of a 2 X 2 matrix A such that A 2 is normal, but A is not \nnormal.",
    "V which is represented in the standard ordered basis by the matrix \nShow that T is normal, and find an orthonormal basis for V, consisting of charac­\nteristic vectors for T. \n5. Givp an example of a 2 X 2 matrix A such that A 2 is normal, but A is not \nnormal. \n6. Let T be a normal operator on a finite-dimensional complex inner product \nspace. Prove that T is self-adjoint, positive, or unitary according as every charac­\nteristic value of T is real, positive, or of absolute value 1. (Use Theorem 22 to \nreduce to a similar question about diagonal matrices.) \n7. Let T be a linear operator on the finite-dimensional inner product space V, \nand suppose T is both positive and unitary. Prove T = I. \n8. Prove T is norm2,l if amI only if T = Tl + iT2, where Tl and Tz are self­\nadjoint operators which commute. \n9. Prove that a real symmetric matrix has a real symmetric cube root; i.e., if A \nis real symmetric, there is a real symmetric B such that B3 = A. \n10. Prove that every positive matrix is the square of a positive matrix. \n317 \n318 \nInner Product Spaces \nChap. 8 \nn. Prove that a normal and nilpotent operator is the zero operator. \n12. If T is a normal operator, prove that characteristic vectors for T which are \nassociated with distinct characteristic values are orthogonal. \n13. Let T be a normal operator on a finite-dimensional complex inner product \nspace. Prove that there is a polynomial !, with complex coefficients, such that \nT* \n!(T). (Represent 7' by a diagonal matrix, and see what ! must be.) \n14. If two normal operators commute, prove that their product is normal. \n9. Operators on \nInner Product Spaces \n9.1 . Introduction \nWe regard most of the topics treated in Chapter 8 as fundamental, \nthe material that everyone should know. The present chapter is for the \nmore advanced student or for the reader who is eager to expand his knowl­\nedge concerning operators on inner product spaces. With the exception of",
    "We regard most of the topics treated in Chapter 8 as fundamental, \nthe material that everyone should know. The present chapter is for the \nmore advanced student or for the reader who is eager to expand his knowl­\nedge concerning operators on inner product spaces. With the exception of \nthe Principal Axis theorem, which is essentially just another formulation of \nTheorem 18 on the orthogonal diagonalization of self adjoint operators, and \nthe other results on forms in Section 9.2, the material presented here is \nmore sophisticated and generally more involved technically. We also make \nmore demands of the reader, just as we did in the later parts of Chapters \n5 and 7. The arguments and proofs are written in a more condensed style, \nand there are almost no examples to smooth the way; however, we have \nseen to it that the reader is well supplied with generous sets of exercises. \nThe first three sections are devoted to results concerning forms on \ninner product spaces and the relation between forms and linear operators. \nThe next section deals with spectral theory, i.e., with the implications of \nTheorems 18 and 22 of Chapter 8 concerning the diagonalization of self­\nadjoint and nonnal operators. In the final section, we pursue the study of \nnonnal operators treating, in particular, the real case, and in so doing we \nexamine what the primary decomposition theorem of Chapter 6 says about \nnonnal operators. \n319 \n3130 \nOperators on Inner Product Spaces \nChap. 9 \n9.2. Forms on Inner Product Spaces \nIf ']' is a linear operator on a finite-dimensional inner product space V \nthe function f defined on V X V by \nf(a, (3) \n(Tal(3) \nmay be regarded as a kind of substitute for T. Many questions about T are \nequivalent to questions concerning f. In fact, it is easy to see that f deter­\nmines T. For if il = {aI, . . .  , an} is an orthonormal basis for V, then the \nentries of the matrix of T in il are given by \nAjk \nf(ak, aj).",
    "may be regarded as a kind of substitute for T. Many questions about T are \nequivalent to questions concerning f. In fact, it is easy to see that f deter­\nmines T. For if il = {aI, . . .  , an} is an orthonormal basis for V, then the \nentries of the matrix of T in il are given by \nAjk \nf(ak, aj). \nIt is important to understand why f determines T from a more abstract \npoint of view. The crucial properties of f are described in the following \ndefinition. \nDefinition. A (sesqui-linear) forIll on a real or complex vector space \nV is a function f on V X V with values in the field of scalars such that \n(a) \nf(ca + (3, 'Y) \ncf(a, 'Y) + f((3, 'Y) \n(b) \nf(a, c(3 + 'Y) \ncf(a, (3) + f(a, 'Y) \njor all a, (3, 'Y in V and all scalars c. \nThus, a sesqui-linear form is a function on V X V such that f(a, m \nis a linear function of a for fixed (3 and a conjugate-linear function of (3 \nfor fixed a. In the real case, f(a, (3) is linear as a function of each argument; \nin other words, f is a bilinear forIll. In the complex case, the sesqui­\nlinear form f is not bilinear unless f \nO. In the remainder of this chapter, \nwe shall omit the adjective 'sesqui-linear' unless it seems important to \ninclude it. \nIf f and g are forms on V and c is a scalar, it is easy to check that \ncf + g is also a form. From this it follows that any linear combination of \nforms on V is again a form. Thus the set of all forms on V is a subspace of \nthe vector space of all scalar-valued functions on V X V. \nTheorem 1. Let V be a finite-dimensional inner product space and f a \nform on V. Then there is a unique linear operator T on V such that \nf(a, (3) \nfor all a, (3 in V, and the map f -+ T is an isomorphism of the space of forms \nonto L(V, V). \nProof. Fix a vector (3 in V. Then a -+ f(a, (3) is a linear function \non V. By Theorem 6 there is a unique vector {3' in V such that lea, (3) = \n(al{3') for every a. We define a function U from V into V by setting U{3 = \n{3'. Then \nSec. 9.2 \nForms on Inner Product Spaces",
    "onto L(V, V). \nProof. Fix a vector (3 in V. Then a -+ f(a, (3) is a linear function \non V. By Theorem 6 there is a unique vector {3' in V such that lea, (3) = \n(al{3') for every a. We define a function U from V into V by setting U{3 = \n{3'. Then \nSec. 9.2 \nForms on Inner Product Spaces \nf(alei3 + 1') \n(al U(ei3 + 1'» \n= cf(a, 13) + f(a, 1') \n= c(al Vi3) + (al VI') \n(aicV,B + VI') \nfor all a, 13, I' in V and all scalars e. Thus V is a linear operator on V and \nT \nV* is an operator such thatj(a, (3) \n(Tali3) for all a and 13. If we also \nhave f(a, 13) = (T' aim, then \n(Ta - T'al,B) \n= 0 \nfor all a and /3; so Ta = T'a for all a. Thus for each formf there is a unique \nlinear operator Tj such that \nfor all a, f3 in V. If j and g are forms and e a scalar, then \n(ef + g)(a, (3) = (Tcf+oal(3) \nfor all a and (3 in V. Therefore, \nef(a, (3) + g(a, (3) \n= c(Tjali3) + (Tyalf3) \n= ((cTj + To)alm \nTcj+o = cTj + To \nso f -+ Tj is a linear map. For each T in L( V, V) the eauation \nf(a, (3) \n= (Tal(3) \ndefines a form such that Tj = T, and Tj = 0 if and only if f = O. Thus \nf -+ Tj is an isomorphism. \nI \nCorollary. The equation \n(fig) = tr (T/r:;) \ndefines an inner product on the space of forms with the property that \n(fig) \n= F f(ak, aj)g(ak' aj) \ni,k \nfor every orthonormal basis {al' . . .  , an} of V. \nProof. It follows easily from Example 3 of Chapter 8 that \n(T, U) -+ tr (TV*) is an inner product on L(V, V), Since f -+ Tf is an \nisomorphism, Example 6 of Chapter 8 shows that \n(fIg) = tr (TjT';) \nis an inner product. Now suppose that A and B are the matrices of Tj and \nTo in the orthonormal basis <B = {al, . . .  , an} . Then \nAjk = (Tjaklaj) = f(ak, aj) \n321 \nOperators on Inner Product Spaces \nChap. 9 \nand Bjk = (Toaklaj) = g(ak, aj). Since AB* is the matrix of T!Tৎ in the \nbasis CB, it follows that \n(fIg) = tr (AB*) = A AjkBj/c. I \ni,k \nDefinition. If f is a form and CB \n{al, . . .  , an} an arbitrary ordered \nbasis of V, the matrix A with entries \nAjk = f(ak, aj)",
    "Chap. 9 \nand Bjk = (Toaklaj) = g(ak, aj). Since AB* is the matrix of T!Tৎ in the \nbasis CB, it follows that \n(fIg) = tr (AB*) = A AjkBj/c. I \ni,k \nDefinition. If f is a form and CB \n{al, . . .  , an} an arbitrary ordered \nbasis of V, the matrix A with entries \nAjk = f(ak, aj) \nis called the matrix of f in the ordered basis CB. \nWhen CB is an orthonormal basis, the matrix of f in CB is also the matrix \nof the linear transformation Tj, but in general this is not the case. \nIf A is the matrix of f in the ordered basis CB = {al, . . .  , an}, it follows \nthat \n(9-1) \nf(A x.a., A Ytar) = A 1JrArsX• \n3 \nT \nT,3 \nfor all scalars x. and Yr (1 ::s; r, s ::s; n). In other words, the matrix A has \nthe property that \nf(a, (3) \nY*AX \nwhere X and Y are the respective coordinate matrices of a and {3 in the \nordered basis CB. \nThe matrix of f in another basis \nn \naj = A Pijai, \ni = l  \n(1 ::s; j ::s; n) \nis given by the equation \n(9-2) \nA' \nP*AP. \nFor \nAJk = f(af, ai) \n= f(A P.ka., A Priar) \nr,' \n• \nr \n= (p* AP)jk. \nSince p* = pMl for unitary matrices, it follows from (9-2) that results \nconcerning unitary equivalence may be applied to the study of forms. \nTheorem 2. Let f be a form on a finite-dimensional complex inner \nproduct space V. Then there is an orthonormal basis for V in which the matrix \nof f is upper-triangular. \nProof. Let T be the linear operator on V such that f(a, (3) = \n(Tal{3) for all a and {3. By Theorem 21, there is an orthonormal basis \nSec. 9.2 \nForms on Inner Prod1©ct Spaces \n{ai, . . . , an} in which the matrix of T is upper-triangular. Hence, \nfCak, aj) = CTak!aj) = 0 \nwhen j > k. I \nDefinition. A form f on a real or complex vector space V is called \nHerllitian if \nf(a, (3) \nf((3, a) \nfor all a and (3 in V. \nIf T is a linear operator on a finite-dimensional inner product space V \nand f is the form \nfCa, (3) \nCTa!(3) \nthen f((3, a) = (a!T(3) \n(T*a!(3) ;  so f is Hermitian if and only if T is self-\nadjoint.",
    "Herllitian if \nf(a, (3) \nf((3, a) \nfor all a and (3 in V. \nIf T is a linear operator on a finite-dimensional inner product space V \nand f is the form \nfCa, (3) \nCTa!(3) \nthen f((3, a) = (a!T(3) \n(T*a!(3) ;  so f is Hermitian if and only if T is self-\nadjoint. \nWhenf is Hermitianf(a, a) is real for every a, and on complex spaces \nthis property characterizes Hermitian forms. \nTheorem 3. Let V be a complex vector space and f a form on V such \nthat f(a, a) is real for every a. Then f is Hermitian. \nProof. Let a and (3 be vectors in V. We must show that fCa, (3) = \nf((3, a). Now \nfCa + (3, a + (3) = fCa, (3) + fCa, (3) + f((3, a) + fC(3, (3). \nSince fCa + (3, a + (3), f(a, a), and f((3, (3) are real, the number f(a, (3) + \nf((3, a) is real. Looking at the same argument with a + i(3 instead of a + (3, \nwe see that -if(a, (3) + if((3, a) is real. Having concluded that two num­\nbers are real, we set them equal to their complex conjugates and obtain \nfCa, (3) + fC(3, a) = f(a, (3) + fC(3, a) \n-if(a, (3) + ifC(3, a) = if(a, (3) - if«(3, a) \nIf we multiply the second equation by i and add the result to the first \nequation, we obtain \n2f(a, (3) \n2fC(3, a). \nI \nCorollary. Let T be a linear operator on a complex finite-dimensional \ninner product space V. Then T is self-adjoint if and only if (Ta!a) is real for \nevery a in V. \nTheorem 4 (Principal Axis Theorem). For every Hermitian form f \non a finite-dimensional inner product space V, there is an orthononnal basis of \nV in which f is represented by a diagonal matrix with real entries. \n323 \n324 \nOperators on Inner Product Spaces \nChap. 9 \nProof. Let T be the linear operator such thatf(a, (3) \n(Talf3) for \nall a and (3 in V. Then, since f(a, (3) = f({3, a) and (T{3\\a) = (aIT{3), it \nfollows that \n(Talf3) = f({3, a) = (aIT{3) \nfor all a and {3; hence T \nT*. By Theorem 18 of Chapter 8, there is an \northonormal basis of V which consists of characteristic vectors for T. \nSuppose {ai, . . .  , an} is an orthonormal basis and that",
    "follows that \n(Talf3) = f({3, a) = (aIT{3) \nfor all a and {3; hence T \nT*. By Theorem 18 of Chapter 8, there is an \northonormal basis of V which consists of characteristic vectors for T. \nSuppose {ai, . . .  , an} is an orthonormal basis and that \nTaj = Cja; \nfor 1 S j S n. Then \nand by Theorem 15 of Chapter 8 each Ck is real. I \nCorollary. Under the above conditions \nExercises \nf(ძ Xjaj, ძ Ykak) = ძ CjxlYj. \nj \nk \ni \n1. vVhich of the following functions I, defined on vectors a = (XI, X2) and {3 = \n(YI, Y2) in C2, are (sesqui-linear) forms on C2? \n(a) I(a, (3) \n= 1. \n(b) I(a, (3) \n(XI - 'fh)2 + X21h. \n(c) I(a, (3) = (Xl + fh) 2 - (Xl - fh) 2. \n(d) I(a, {3) = xlJh - X2YI. \n2. Let I be the form on R2 defined by \n1« Xl, YI), (X2, Y2» \nXlYI + X2Y2. \nFind the matrix of I in each of the following bases: \n{(I, 0), (0, I)} , {(I, -1), (1, I)} , {(I, 2), (3, 4)} . \n3. Let \nA = [_ǥ ;] \nand let g be the form (on the space of 2 X 1 complex matrices) defined by g(X, Y) \ny* AX. Is g an inner product? \n4. Let V be a complex vector space and let I be a (sesqui-linear) form on V which \nis symmetric: f(a, (3) \nf({3, a). What is f? \n5. Let f be the form on R2 given by \nI« XI, X2), (YI, Y2» \nXIYI + 4X2Y2 + 2XIY2 + 2X2YI. \nFind an ordered basis in which f is represented by a diagonal matrix. \n6. Call the form f (left) non-degenerate if 0 is the only vector a such that \nI(a, f3) \n0 for all {3. Let f be a form on an inner product space V. Prove that f is \nSec. 9.3 \nPositive Forms \nnon-degenerate if and only if the associated linear operator T, (Theorem 1) is \nnon-singular. \n7. Let f be a form on a finite-dimensional vector space V. Look at the definition \nof left non-degeneracy given in Exercise 6. Define right non-degeneracy and prove \nthat the form f is left non-degenerate if and only if f is right non-degenerate. \nS. Let f be a non-degenerate form (Exercises 6 and 7) on a finite-dimensional \nspace V. Let L be a linear functional on V. Show that there exists one and only one",
    "that the form f is left non-degenerate if and only if f is right non-degenerate. \nS. Let f be a non-degenerate form (Exercises 6 and 7) on a finite-dimensional \nspace V. Let L be a linear functional on V. Show that there exists one and only one \nvector f3 in V such that L(a) = f(a, (3) for all a. \n9. Let f be a non-degenerate form on a finite-dimensional space V. Show that \neach linear operator S has an tadioint relative to f,' i.e., an operator 8' such that \nf(8a, (3) = f(a, S'(3) for all a, (3. \n325 \n9.3. Positive Forms \nIn this section, we shall discuss non-negative (sesqui-linear) forms \nand their relation to a given inner product on the underlying vector space. \nDefinitions. A form f an a real or complex vector space V is non­\nnegative if it is Hermitian and f(a, a) 2. 0 for every a in V. The form f is \npositive if f is Hermitian and f(a, a) > 0 for all a ;t- O. \nA positive form on V is simply an inner product on V. A non-negative \nform satisfies all of the properties of an inner product except that some non­\nzero vectors may be 'orthogonal' to themselves. \nLetfbe a form on the finite-dimensional space V. Let CB = {al, . . . , an} \nbe an ordered basis for V, and let A be the matrix of f in the basis CB, that is, \nAjk = fCak, aj). If a = Xlal + \n.\n,\n.\n + Xnan, then \nfCa, a) = fC{ Xjaj, { XkCXk) \ni \nk \n= { { xixk!(cxj, CXk) \ni Ie \n= { { Akjxj?Ck. \nj \nIe \nSo, we see that j is non-negative if and only if \nand \n(9-3) \nA = A* \n{ { AkjxjXk 2. 0 for all scalars Xl, . . .  , Xn. \nj \nIe \nIn order that j should be positive, the inequality in C9-3) must be strict for \nall (Xl, . . .  , xn) ;t- O. The conditions we have derived state that f is a \npositive form on V if and only if the function \ng(X, Y) = Y*AX \n326 \nOperators on Inner Product Spaces \nChap. 9 \nis a positive form on the space of n X 1 column matrices over the scalar \nfield. \nTheorem 5. Let F be the field of real numbers or the field of complex",
    "positive form on V if and only if the function \ng(X, Y) = Y*AX \n326 \nOperators on Inner Product Spaces \nChap. 9 \nis a positive form on the space of n X 1 column matrices over the scalar \nfield. \nTheorem 5. Let F be the field of real numbers or the field of complex \nnumbers. Let A be an n X n matrix over F. The function g defined by \n(9-4) \ng(X, Y) = y* AX \nis a positive form, on the space FnXI if and only if there exists an invertible \nn X n matrix P with entries in F such that A \nP*P. \nProof. For any n X n matrix A, the function g in (9-4) is a form \non the space of column matrices. We are trying to prove that g is positive \nif and only if A = P*P. First, suppose A = P*P. Then g is Hermitian and \ng(X, X) = X*P*PX \n= (PX)*PX \ns 0. \nIf P is invertible and X ¥ 0, then (PX) *PX > O. \nNow, suppose that g is a positive form on the space of column matrices. \nThen it is an inner product and hence there exist column matrices Qh . . .  , \nQ,. such that \nOjk \ng(Qi' Qk) \n= Q:AQj. \nBut this just says that, if Q is the matrix with columns QI, . . .  , Qn, then \nQ*AQ = I. Since {QI, . . .  , Qn} is a basis, Q is invertible. Let P = Q-l and \nwe have A = P*P. \nI \nIn practice, it is not easy to verify that a given matrix A satisfies the \ncriteria for positivity which we have given thus far. One consequence of \nthe last theorem is that if g is positive then det A > 0, because det A \ndet (P*P) = det P* det P = Idet P12. The fact that det A > ° is by no \nmeans sufficient to guarantee that g is positive; however, there are n \ndeterminants associated with A which have this property: If A \nA * and \nif each of those determinants is positive, then g is a positive form. \nDefinition. Let A be an n X n matrix over the field F. The principal \nminors of A are the scalars Ak(A) defined by \n[All \nAk(A) = det \n: \nAkl \nlkJ \n. \n, \nAkk 1 :::; k :::; n. \nLemma. Let A be an invertible n X n matrix with entries in a field F. \nThe following two statements are equivalent. \nSec. 9.3 \nPositive Forms",
    "minors of A are the scalars Ak(A) defined by \n[All \nAk(A) = det \n: \nAkl \nlkJ \n. \n, \nAkk 1 :::; k :::; n. \nLemma. Let A be an invertible n X n matrix with entries in a field F. \nThe following two statements are equivalent. \nSec. 9.3 \nPositive Forms \n(a) There is an 1lpper-triangular matrix P with Pkk = 1 (1 ::; k ::; n) \nsuch that the matrix B = AP is lower-triangular. \n(b) The principal minors of A are all different from 0. \nProof. Let P be any n X n matrix and set B AP. Then \nBjk = B AjrPrk• \nr \nIf P is upper-triangular and Pkk \n1 for every k, then \nk-l \nB AjrPrk = Bjk - Au, \nk > l. \nr=1 \nNow B is lower-triangular provided Bik = ° for j < k. Thus B will be \nlower-triangular if and only if \nk-l \n(9-5) \nB AirPrk = -Akk, \nr=1 \nl ::; j ::; k - l  \n2 ::; k :::; n. \nSo, we see that statement (a) in the lemma is equivalent to the statement \nthat there exist scalars Prk, 1 ::; r ::; k, 1 ::; k ::; n, which satisfy (9-5) and \nPkk \n1, 1 ::; k ::; n. \nIn (9-5), for each k > 1 we have a system of k - 1 linear equations \nfor the unknowns Plk, P2k, . . .  , Pk-l.k. The coefficient matrix of that \nsystem is \n[All \n°k-l \nA1.k-l ] \nA±-I'k-1 \nand its determinant is the principal minor Llk_l(A). If each Llk-l(A) :;:.t: 0, \nthe systems (9-5) have unique solutions. We have shown that statement \n(b) implies statement (a) and that the matrix P is unique. \nNow suppose that (a) holds. Then, as we shall see, \nLlk(A) = Llk(B) \n(9-6) \nk = 1, . . .  , n. \nTo verify (9-6), let AI, . . .  , An and BI, \n•\n•\n•\n , Bn be the columns of A and \nB, respectively. Then \n(9-7) \nBl Al r-l \nBr = B PirAj + Ar, \nj=1 \nr >  1. \nFix k, 1 ::; k ::; n. From (9-7) we see that the rth column of the matrix \n[Bll \n²kl \n³kk] \nBkk \nis obtained by adding to the rth column of \n327 \n328 \nOperators on Inner Product Spaces [ľ11 \nAt] \nAkl \nAkk \nChap. 9 \na linear combination of its other columns. Such operations do not change \ndeterminants. That proves (9-6), except for the trivial observation that",
    "[Bll \n²kl \n³kk] \nBkk \nis obtained by adding to the rth column of \n327 \n328 \nOperators on Inner Product Spaces [ľ11 \nAt] \nAkl \nAkk \nChap. 9 \na linear combination of its other columns. Such operations do not change \ndeterminants. That proves (9-6), except for the trivial observation that \nbecaui:le B is triangular !:,.k(B) = Bll . • •  Bkk• Since A and P are invertible, \nB is invertible. Therefore, \n!:\"(B) = Bll . . .  Bnn :;e. ° \nand so !:\"k(A) :;e. O, k  \n1, . . .  , n. \nI \nTheorem 6. Let f be a form on a finite-dimensional vector space V \nand let A be the matrix of f in an ordered basis CB. Then f is a positive form if \nand only if A \nA * and the principal minors of A are all positive. \nProof. Let's do the interesting half of the theorem first. Suppose \nthat A \nA * and !:\"k(A) > 0, 1 s:; k s:; n. By the lemma, there exists an \n(unique) upper-triangular matrix P with Pkk = 1 such that B = AP is \nlower-triangular. The matrix P* is lower-triangular, so that P* B = P* AP \nis also lower-triangular. Since A is self-adjoint, the matrix D \nP* AP is \nself-adjoint. A self-adjoint triangular matrix is necessarily a diagonal \nmatrix. By the same reasoning which led to (9-6), \n!:,.k(D) = !:,.k(P*B) \n!:,.k(B) \n= !:\"k(A). \nSince D is diagonal, its principal minors are \n!:,.k(D) \nDll • • •  Du. \nFrom !:,.k(D) > 0, 1 s:; k s:; n, we obtain Dkk > ° for each k. \nIf A is the matrix of the formf in the ordered basis CB = {al' . . .  , an}, \nthen D = P*AP is the matrix of f in the basis {a(, . . .  , aw} defined by \nn \na; \n2; Pijai. \niܬ 1 \nSee (9-2). Since D is diagonal with positive entries OIl its diagonal, it is \nobvious that \nX*DX > 0, \nx :;e. O  \nfrom which it follows that f is a positive form. \nNow, suppose we start with a positive formf. We know that A \nA *. \nHow do we show that !:\"k(A) > 0, 1 s:; k s:; n7 Let Vk be the subspace \nspanned by aI, . . .  , ak and let fk be the restriction of f to Vk X Vk• Evi-\nSec. 9.3 \nPositive Forms",
    "from which it follows that f is a positive form. \nNow, suppose we start with a positive formf. We know that A \nA *. \nHow do we show that !:\"k(A) > 0, 1 s:; k s:; n7 Let Vk be the subspace \nspanned by aI, . . .  , ak and let fk be the restriction of f to Vk X Vk• Evi-\nSec. 9.3 \nPositive Forms \ndently fk is a positive form on Vk and, in the basis {al' . . .  , ak} it is \nrepresented by the matrix \nAs a consequence of Theorem 5, we noted that the positivity of a form \nimplies that the determinant of any representing matrix is positive. \nI \nThere are some comments we should make, in order to complete our \ndiscussion of the relation between positive forms and matrices. What is it \nthat characterizes the matrices which represent positive forms? If f is a \nform on a complex vector space and A is the matrix of f in some ordered \nbasis, then f will be positive if and only if A = A * and \n(9-8) \nX*AX > 0, \nfor all complex X ჭ O. \nIt follows from Theorem 3 that the condition A \n= A * is redundant, i.e., \nthat (9-8) implies A = A *. On the other hand, if we are dealing with a real \nvector space the form f will be positive if and only if A = A t and \n(9-9) \nXtAX > 0, \nfor all real X ჭ O. \nWe want to emphasize that if a real matrix A satisfies (9-9), it does not \nfollow that A = A t. One thing which is true is that, if A \nA t and (9-9) \nholds, then (9-8) holds as well. That is because \n(X + iY)*A (X + iY) = (Xt - iyt)A (X + iY) \nXtAX + YtA Y + i[XtA Y - ytAX] \nand if A \n= A t  then YtAX = XtA Y. \nIf A is an n X n matrix with complex entries and if A satisfies (9-9), \n\\ye shall call A a positive matrix. The comments which we have just \nmade may be summarized by saying this: In either the real or complex \ncase, a form f is positive if and only if its matrix in some (in fact, every) \nordered basis is a positive matrix. \nN ow suppose that V is a finite-dimensional inner product space. Let f \nbe a non-negative form on V. There is a unique self-adjoint linear operator",
    "case, a form f is positive if and only if its matrix in some (in fact, every) \nordered basis is a positive matrix. \nN ow suppose that V is a finite-dimensional inner product space. Let f \nbe a non-negative form on V. There is a unique self-adjoint linear operator \nT on V such that \n(9-10) \nf(a, {3) \n(Talf3)· \nand T has the additional property that (Fala) 2 O. \nDefinition. A linear operator T on a finite-dimensional inner product \nspace V is non-negative if T = T* and (Tala) 2 0 for all a in V. A \npositive linear operator is one such that T = T* and (Tala) > 0 for' all \nCI ჭ O. \n329 \n330 \nOperators on Inner Product Spaces \nChap. 9 \nIf V is a finite-dimensional (real or complex) vector space and if ( · 1\n· ) is \nan inner product on V, there is an associated class of positive linear oper­\nators on V. Via (9-10) there is a one-one correspondence between that class \nof positive operators and the collection of all positive forms on V. We shall \nuse the exercises for this section to emphasize the relationships between \npositive operators, positive forms, and positive matrices. The following \nsummary may be helpful. \nIf A is an n X n matrix over the field of complex numbers, the follow­\ning are equivalent. \n(1) A is positive, i.e.,     AkjxjXk > 0 whenever Xl, •\n•\n•\n , X\" are \ni k \ncomplex numbers, not all O. \n(2) (XI Y) = y* AX is an inner product on the space of n X 1 complex \nmatrices. \n(3) Relative to the standard inner product (XI Y) \ny* X on n X 1 \nmatrices, the linear operator X --+ AX is positive. \n(4) A = P*P for some invertible n X n matrix P over C. \n(5) A \nA *, and the principal minors of A are positive. \nIf each entry of A is real, these are equivalent to: \n(6) A = A t, and A A AkjxjXk > 0 whenever Xl, .\n.\n•\n , Xn are real \nj k \nnumbers not all O. \n(7) (XI Y) = PAX is an inner product on the space of n X 1 real \nmatrices. \n(8) Relative to the standard inner product (XI Y) = ytX on n X 1 \nreal matrices, the linear operator X --+ AX is positive.",
    "(6) A = A t, and A A AkjxjXk > 0 whenever Xl, .\n.\n•\n , Xn are real \nj k \nnumbers not all O. \n(7) (XI Y) = PAX is an inner product on the space of n X 1 real \nmatrices. \n(8) Relative to the standard inner product (XI Y) = ytX on n X 1 \nreal matrices, the linear operator X --+ AX is positive. \n(9) There is an invertible n X n matrix P, with real entries, such \nthat A \nptp. \nExercises \n1. Let 11 be C2, with the standard inner product. For which vectors a in 11 is \nthere a positive linear operator T such that a = Ttl? \n2. Let V be R2, with the standard inner product. If fJ is a real number, let T \nbe the linear operator 'rotation through fJ,' \nTe(Xl, X2) = (Xl cos fJ - X2 sin fJ, Xl sin fJ + X2 cos fJ). \nFor which values of fJ is Te a positive operator? \n3. Let V be the space of n X 1 matrices over C, with the inner product (XI Y) = \nY*GX (where G is an n X n matrix such that this is an inner product). Let A be \nan n X n matrix and T the linear operator T(X) = AX. Find T*. If Y is a fixed \nelement of V, find the element Z of V which determines the linear functional \nX --+ Y*X. In other words, find Z such that Y*X = (XIZ) for all X in 11. \nSec. 9.3 \nPositive Forms \n4. Let V be a finite-dimensional inner product space. If T and U are positive \nlinear operator8 on V, prove' that (T + U) is positive. Give an example which \nshowF that TU need not be positive. \n5. Let \nA = [ǣ tJ \n(a) Show that A is positive. \n(b) Let V be the space of 2 X 1 real matrices, with the inner product \n(XI Y) \n= yt A X. Find an orthonormal basis for V, by applying the Gram-Schmidt \nprocess to the basis {Xl, X2} defined by \nX2 = [Ǥl \n(c) Find an invertible 2 X 2 real matrix P such that A \n= ptP. \n6. Which of the following matrices are positive? \n[ 1 1 + iJ \n1 - i \n3 \n' [L -1 -1 -1 \n[1 ! iJ \n! \n1 t . \n.1 \n-l \n1 \n3 \n4 \n7> \n7. Give an example of an n X n matrix which has all its principal minors positive, \nbut which is not a positive matrix. \n8. Does « Xl, X2) I (Yl, Y2»",
    "= ptP. \n6. Which of the following matrices are positive? \n[ 1 1 + iJ \n1 - i \n3 \n' [L -1 -1 -1 \n[1 ! iJ \n! \n1 t . \n.1 \n-l \n1 \n3 \n4 \n7> \n7. Give an example of an n X n matrix which has all its principal minors positive, \nbut which is not a positive matrix. \n8. Does « Xl, X2) I (Yl, Y2» \n= Xlllt + 2X2'!lt + 2xllh + X2'[}2 define an inner product \non C2? \n9. Prove that every entry on the main diagonal of a positive matrix is positive. \n10. Let V be a finite-dimensional inner product space. If T and U are linear \noperators on V, we write T < U if U - T is a positive operator. Prove the fol­\nlowing: \nCa) T < U and U < T is impossible. \n(b) If T < U and U < S, then T < S. \n(c) If T < U and 0 < S, it need not be that ST < SU. \nll. Let V be a finite-dimensional inner product space and E the orthogonal \nprojection of V onto some subspace. \n(a) Prove that, for any positivE' number c, the operator cI + E is positive. \n(b) Express in terms of E a self-adjoint linear operator T such that T2 = I + E. \n12. Let n be a positive intpger and L1 the n X n matrix \n1 1 1 \n1 \n2 \n3 \nn \n1 1 \n1 \nA =  2 \n3 \n4 \nn + l  \n1 1 1 \n1 \nn n + l  n + 2  \n2n -1 \nProve that A is positive. \n331 \n332 \nOperators on Inner Product Spaces \nChap. {) \n13. Let A be a self-adjoint n X n matrix. Prove that there is a real number c \nsuch that the matrix cI + A is positive. \n14. Prove that the product of two positive linear operators is positive if and \nonly if they commute. \n15. Let S and T be positive operators. Prove that every characteristic value of \nST is positive. \n9.4. More on Forms \nThis section contains two results which give more detailed information \nabout (sesqui-linear) forms. \nTheorem 7. Let f be a form on a real or complex vector space V and \n{al, . . . , ar} a basis for the finite-dimensional subspace W of V. Let M be the \nr X r matrix with entries \nMik \nf(ak, aj) \nand Wi the set of all vectors {3 in V such that f(a, (3) \n0 for all a in W. Then",
    "Theorem 7. Let f be a form on a real or complex vector space V and \n{al, . . . , ar} a basis for the finite-dimensional subspace W of V. Let M be the \nr X r matrix with entries \nMik \nf(ak, aj) \nand Wi the set of all vectors {3 in V such that f(a, (3) \n0 for all a in W. Then \nW' is a subspace of V, and W n W' = {O} if and only if M is invertible. \nWhen this is the case, V = W + W'. \nProof. If {3 and \"{ are vectors in W' and c is a scalar, then for \nevery a in W \nf(a, c{3 + \"() = cj(a, (3) + f(a, \"() \n= o. \nHence, W' is a subspace of V. \nN ow suppose IX \nr \nr \nA XkIXk and that (3 \nk=1 \nA YiIXi' Then \nj = 1 \n= t (1 YiM;k )Xk' \nIt follows from this that W n W' ¥ {O} if and only if the homogeneous \nsystem \nr \nA yjM;k = 0, \nj=1 \nhas a non-trivial solution (Yll .\n.\n.\n , Yr). Hence W n W' = {O} if and only \nif M* is invertible. But the invertibility of M* is equivalent to the inverti­\nbility of M. \nSuppose that M is invertible and let \nA \"\" (M*)-l = (M-l)*. \nSec. 9.4 \nDefine gj on V by the equation \nThen \ngj(c(3 + 'Y) \nA \nk \n= C ძ AjkJCak, (3) + A AjkJ(ak, 'Y) \nk \nk \n= cgj«(3) + gj('Y). \nMore on Forms \nHence, each gj is a linear function on V. Thus we may define a linear \noperator E on V by setting \nSince \ngj(an) = A AjkJ(ak, an) \nk \n= A Ajk(M*)kn \nk \nit follows that E(an) = an for 1 :s; n :s; r. This implies Ea \na for every \na in W. Therefore, E maps V onto W and E2 \nE. If (3 is an arbitrary \nvector in V, then \nJ(an, E(3) = J (an' 7 gj«(3)aj) \nA gj((3)J(an, aj) \nj \n= 1 (t \nAjk/(ak, (3») J(an, a,.). \nSince A * = M-l, it follows that \nJ(an, E(3) \n( (1 (M-1h,.Mjn) J(ak, (3) \nA OknJ(ak, (3) \nk \n= J(an, (J). \nThis implies J(a, E(3) = J(a, (3) for every a in W. Hence \nJ(a, (3 - E(3) = 0 \nfor all a in W and f3 in V. Thus 1 - E maps V into W'. The equation \nf3 = Ef3 + (1 \nE)(3 \nshows that V = W + W'. One final point should be mentioned. Since \nW n W' = {O}, every vector in V is uniquely the sum of a vector in W \n333 \n334 \nOperators on Inner Product Spaces \nChap. 9",
    "for all a in W and f3 in V. Thus 1 - E maps V into W'. The equation \nf3 = Ef3 + (1 \nE)(3 \nshows that V = W + W'. One final point should be mentioned. Since \nW n W' = {O}, every vector in V is uniquely the sum of a vector in W \n333 \n334 \nOperators on Inner Product Spaces \nChap. 9 \nand a vector in W'. If {3 is in W', it follows that E{3 \nO. Hence I \nE \nmaps V onto W'. \nI \nThe projection E constructed in the proof may he eharacterir,ed as \nfollows: E{3 \na if and only if a is in W and {3 \na belongs to W'. Thus E \nis independent of the basis of W that was used in its construction. Hence \nwe may refer to E as the projection of V on W that is determined by \nthe direct sum decomposition \nV = W EB W'. \nNote that E is an orthogonal projection if and only if W' \nW.L. \nTheorem 8. Let f be a form on a real or complex vector space V and A \nthe matrix of f in the ordered basis {aI, . . .  , an} of V. Suppose the principal \nminors of A are all different from o. Then there is a unique upper-triangular \nmatrix P with Pkk = 1 (1 :::; k :::; n) such that \nP*AP \nis upper-triangular. \nProof. Since Llk(A *) \nLlk(A) (1 :::; k :::; n), the principal minors \nof A * are all different from o. Hence, by the lemma used in the proof of \nTheorem 6, there exists an upper-triangular matrix P with Pkk = 1 such \nthat A * P is lower-triangular. Therefore, P* A = (A * P) * is upper-tri­\nangular. Since the product of two upper-triangular matrices is again upper­\ntriangular, it follows that P* AP is upper-triangular. This shows the \nexistence but not the uniqueness of P. However, there is another more \ngeometric argument which may be used to prove both the existence and \nuniqueness of P. \nLet Wk be the subspace spanned by aI, . . .  , ak and Wხ the set of all \n{3 in V such that f(a, {3) = 0 for every a in Wk. Since Llk(A) ჭ 0, the \nk X k matrix M with entries \nMii = f(ah ai) = Aii \n(1 :::; i, j :::; k) is invertible. By Theorem 7 \nV = Wk EB Wf,",
    "uniqueness of P. \nLet Wk be the subspace spanned by aI, . . .  , ak and Wხ the set of all \n{3 in V such that f(a, {3) = 0 for every a in Wk. Since Llk(A) ჭ 0, the \nk X k matrix M with entries \nMii = f(ah ai) = Aii \n(1 :::; i, j :::; k) is invertible. By Theorem 7 \nV = Wk EB Wf, \nLet Ek be the projection of V on Wk which is determined by this decom-\nposition, and set Eo \nO. Let \n(1 S; k :::; n). \nThen {31 \naI, and Ek-1ak belongs to Wk-l for k > 1. Thus when k > 1, \nthere exist unique scalars Pjk such that \nk- 1 \n- B Pjkaj. \n; = 1 \nSec. 9.5 \nSpectral Theory \nSetting Pkk = 1 and Pjk = 0 for j > k, we then have an n X n upper­\ntriangular matrix P with Ph = 1 and k \n(3k = ძ Pjkaj \nj = l \nfor k = 1, . .. , n. Suppose 1 :::; i < k. Then (3i is in Wi and Wi C Wk-1• \nSince fA belongs to Wµ - 1, it follows that f((3i, (3k) \n= O. Let B denote the \nmatrix of f in the ordered basis {{31, . . . , (3n} . Then \nBki = f({3i, (3k) \nso Bki = 0 when k > i. Thus B is upper-triangular. On the other hand, \nB = P*AP. \nConversely, suppose P is an upper-triangular matrix with Pkk = 1 \nsuch that P* AP is upper-triangular. Set \n(1 :::; k :::; n). \nThen {(31, . . .  , {3k} is evidently a basis for Wk. Suppose k > 1. Then \n{(31, . . .  , (3k-1} is a basis for Wk-1, and since f((3;, (3k) \n= 0 when i < k, we \nsee that (3k is a vector in W£- l. The equation defining (3k implies \n(k-l ) \nak = -\n.´ Pjkaj + (3k. \nJ = l \nk - 1  \nNow ´ Pjkaj belongs to Wk-1 and (3k is in W¶-l. Therefore, P1k, . . .  , Pk-1k \n; = 1 \nare the unique scalars such that \nk-1 \nEk-1ak = - ´ Pjkaj \n;=1 \nso that P is the matrix constructed earlier. \nI \n335 \n9.5. Spectral Theory \nIn this section, we pursue the implications of Theorems 18 and 22 \nof Chapter 8 concerning the diagonalization of self-adjoint and nonnal \noperators. \nTheorem 9 (Spectral Theorem). Let T be a normal operator on a \nfinite-dimensional complex inner product space V or a self-adjoint operator on",
    "In this section, we pursue the implications of Theorems 18 and 22 \nof Chapter 8 concerning the diagonalization of self-adjoint and nonnal \noperators. \nTheorem 9 (Spectral Theorem). Let T be a normal operator on a \nfinite-dimensional complex inner product space V or a self-adjoint operator on \na finite-dimensional real inner product space V. Let CII • • • I Ck be the distinct \ncharacteristic values of T. Let Wi be the characteristic space associated with Cj \nand Ej the orthogonal projection of V on Wj• Then Wi is orthogonal to Wi \nwhen i r£ j, V is the direct sum of WI, . . .  , W k, and \n(9-11) \n336 \nOperators on Inner Product Spaces \nCha.p. 9 \nProof. Let a be a vector in Wh f3 a vector in Wi, and suppose \ni ;;t'i j. Then cj(a\\f3) \n(Ta\\f3) \n(a\\T*f3) \n(a\\c,f3). Hence (Cj - c,)(a\\f3) \n0, and since Cj - Ci ;;t'i 0, it follows that (al,l3) = 0. Thus Wj is orthogonal \nto Wi when i ;;t'i j. From the fact that V has an orthonormal basis consisting \nof characteristic vectors (cf. Theorems 18 and 22 of Chapter 8), it fol­\nlows that V = WI + .. . + Wk. If aj belongs to Vj (1 ::; j ::; k) and \nal + ... + ak = 0, then \no = (ail\u001e a;) = \u001e (a.laj) \nj \nj \n= Ilai\\12 \nfor every i, so that V is the direct sum of WI, . . .  , Wk. Therefore EI + \n'\n\"\n \n+ Ek = I and \nT \nTEl + . . . + TEk \n= clEI + . . .  + ckEk. I \nThe decomposition (9-11) is called the spectral resolution of T. \nThis terminology arose in part from physical applications which caused \nthe spectrum of a linear operator on a finite-dimensional vector space \nto be defined as the set of characteristic values for the operator. It is \nimportant to note that the orthogonal projections EI, •\n•\n•\n , Ek are canoni­\ncally associated with T; in fact, they are polynomials in T. \nCorollary. If ej = ij (ǡ \n_ Ǣ:} then Ej \n= ej(T) for 1 ::; j s k. \nProof. Since EiEj = ° when i ;;t'i j, it follows that \nT2 = dEl + . . .  + dEk \nand by an easy induction argument that \nTn = c1EI + . . .  + CĨEk",
    "•\n•\n , Ek are canoni­\ncally associated with T; in fact, they are polynomials in T. \nCorollary. If ej = ij (ǡ \n_ Ǣ:} then Ej \n= ej(T) for 1 ::; j s k. \nProof. Since EiEj = ° when i ;;t'i j, it follows that \nT2 = dEl + . . .  + dEk \nand by an easy induction argument that \nTn = c1EI + . . .  + CĨEk \nfor every integer n ჲ O. For an arbitrary polynomial \nwe have \nr \nf =  \u001e a\"xn \nn = O  \nr \nJ(T) = \u001e aSn \nnǬO \nr \nk \n= \u001e an \u001e cJE; \n,,ǭO j= 1 \njǮl CǯO anC}) Ej \nk \n= \u001e J(Cj)Ej• \n; = 1 \nSince ej(cm) = Ojm, it follows that e/T) \nEj• \nI \nSec. 9.5 \nSpectral Theory \nBecause El, •\n.\n•\n , Ek are canonically associated with T and \n1 =  El + . .  , + Ek \nthe family of projections {Eb . . .  , Ek} is called the resolution of the \nidentity defined by T. \nThere is a comment that should be made about the proof of the spectral \ntheorem. We derived the theorem using Theorems 18 and 22 of Chapter 8 \non the diagonalization of self-adjoint and normal operators. There is an­\nother, more algebraic, proof in which it must first be shown that the mini­\nmal polynomial of a normal operator is a product of distinct prime factors. \nThen one proceeds as in the proof of the primary decomposition theorem \n(Theorem 12, Chapter 6). We shall give such a proof in the next section. \nIn various applications it is necessary to know whether one may \ncompute certain functions of operators or matrices, e.g., square roots. \nThis may be done rather simply for diagonalizable normal operators. \nDefinition. Let T be a diagonalizable normal operator on a finite­\ndimensional inner product space and \nk \nT =  cjEj \nj= l \nits spectral resolution. Suppose f is a function whose domain includes the \nspectrum of T that has values in the field of scalars. Then the linear operator \nf(T) is defined by the equation \n(9-12) \nk \nf(T) \n f(cj)Ej. \nj = l \nTheorem 10. Let T be a diagonalizable normal operator with spectrum S \non a finite-dimensional inner product space V. Suppose f is a function whose",
    "spectrum of T that has values in the field of scalars. Then the linear operator \nf(T) is defined by the equation \n(9-12) \nk \nf(T) \n f(cj)Ej. \nj = l \nTheorem 10. Let T be a diagonalizable normal operator with spectrum S \non a finite-dimensional inner product space V. Suppose f is a function whose \ndomain contains S that has values in the field of scalars. Then f(T) is a \ndiagonalizable normal operator with spectrum f(S). If U is a unitary map of \nV onto V' and T' = UTU-l, then S is the spectrum of T' and \nf(T') = Uf(T)U-l. \nProof. The normality of J(T) follows by a simple computation \nfrom (9-12) and the fact that \nf(T) * =  f(c,)Ej. \ni \nMoreover, it is clear that for every a in EjCV) \nf(T)a = f(cj)a. \nThus, the setf(S) of allf(c) with c in S is contained in the spectrum off(T) . \nConversely, suppose a  0 and that \nf(T)a = ba. \n337 \n838 \nOperators on Inner Product Spaces \nThen a \nA Ep and \nj \nHence, \nf(T)a \nA f(']')Eja \nj \nII\u001e (f(cJ - b)EjaW = A If(cj) - bl21 1EjaW \nj \ni \n= O. \nOhap. 9 \nTherefore, fCc;) = b or Eja = O. By assumption, a rf; 0, so there exists an \nindex i such that Eia rf; O. It follows that f(ci) = b and hence that f(8) is \nthe spectrum of f(T). Suppose, in fact, that \nf(8) = {b1, •\n•\n•\n , br} \nwhere bm rf; bn when m rf; n. Let Xm be the set of indices i such that \n1 ::; i ::; k and fCc;) \nbm. Let Pm \nA E;, the sum being extended over \ni \nthe indices i in Xm• Then Pm is the orthogonal projection of V on the \nsubspace of characteristic vectors belonging to the characteristic value bm \nof f(']') , and \nr \nf(T) = A bmPm \nm=l \nis the spectral resolution of f(T). \nN ow suppose U is a unitary transformation of V onto V' and that \nT' = UTU-l, Then the equation \nholds if and only if \nTa = Ca \nT'Ua \ncUa, \nThus 8 is the spectrum of T', and U maps each characteristic subspace for \nT onto the corresponding subspace for T', In fact, using (9-12), we see that \nT' = A cjEj, \ni \nis the spectral resolution of ']\". Hence \nf(T') = \u001e f(cj)Ej \ni \n= A fCc;) UEjU-l \ni",
    "holds if and only if \nTa = Ca \nT'Ua \ncUa, \nThus 8 is the spectrum of T', and U maps each characteristic subspace for \nT onto the corresponding subspace for T', In fact, using (9-12), we see that \nT' = A cjEj, \ni \nis the spectral resolution of ']\". Hence \nf(T') = \u001e f(cj)Ej \ni \n= A fCc;) UEjU-l \ni \nU (\u001e f(cj)Ej) U-l \ni \n= Uf(T) U-l, \nI \nSec. 9.5 \nSpectral Theory \nIn thinking about the preceding discussion, it is important for one to \nkeep in mind that the spectrum of the normal operator T is the set \n8 = {CI, . . .  , Ck} \nof distinct characteristic values. When T is represented by a diagonal \nmatrix in a basis of characteristic vectors, it is necessary to repeat each \nvalue Cj as many times as the dimension of the corresponding space of \ncharacteristic vectors. This is the reason for the change of notation in the \nfollowing result. \nCorollary . With the assumptions of Theorem 10, suppose that T is \nrepresented in the ordered basis CB = {alJ . . . , an} by the diagonal matrix D \nwith entries dl, •\n•\n•\n , dn• Then, in the basis CB, f(T) is represented by the \ndiagonal matrix feD) with entries f(dl), . . .  , f(dn). If CB' = {a(, . . .  , aD} \nis any other ordered basis and P the matrix such that \naj = 2: Pijai \ni \nthen P-lf(D)P is the matrix of f(T) in the basis CB'. \nProof. For each index i, there is a unique j such that 1 ყ j ყ le, \nai belongs to Ej(V), and d; = Cj. Hence f(T)ai = f(di)ai for every i, and \nf(T)ai = '1:, Pi;!(T)ai \ni \n= 2: d;Pijai \ni \n= '1:, (DP)ijai \ni \nIt follows from this result that one may form certain functions of a \nnormal matrix. For suppose A is a normal matrix. Then there is an inverti­\nble matrix P, in fact a uuitary P, such that P AP-l is a diagonal matrix, say \nD with entries dl, . . . , dn• Let f be a complex-valued function which can \nbe applied to dl\n, . . .  , dn, and let feD) be the diagonal matrix with entries \nf(dl), .\n. . , f(dn). Then P-lf(D)P is independent of D and just a function of",
    "D with entries dl, . . . , dn• Let f be a complex-valued function which can \nbe applied to dl\n, . . .  , dn, and let feD) be the diagonal matrix with entries \nf(dl), .\n. . , f(dn). Then P-lf(D)P is independent of D and just a function of \nA in the following sense. If Q is another invertible matrix such that QAQ-l \nis a diagonal matrix D', then f may be applied to the diagonal entries of D' \nand \nP-lf(D)P = Q-lf(D')Q. \nDefinition. Under the above conditions, f(A) is defined as P-lf(D)P. \nThe matrix f(A) may also be characterized in a different way. In \ndoing this, we state without proof some of the results on normal matrices \n339 \n340 \nOperators on Inner Product Spaces \nChap. 9 \nthat one obtains by formulating the matrix analogues of the preceding \ntheorems. \nTheorem 11. Let A be a normal matrix and CI, . . .  , Ck the distinct \ncomplex roots of det (xl - A). Let \ne· = II (؈) \n1 \nir'i Ci - Cj \nand Ei = ei(A) (1 s:; i s:; k). Then E;Ej = 0 when i ჰ j, Eწ = Ei, EI' = Ei, \nand \nI \nEI + .\n, . + Ek. \nIf f is a complex-valued function whose domain includes CI, . . .  , Ck, then \nf(A) = f(CI)EI + . . . + f(ck)Ek; \nin particular, A \nclEI + . . .  + ckEk• \nWe recall that an operator on an inner product space V is non-negative \nif T is self-adjoint and (Tala) ;:: 0 for every a in V. \nTheorem 12. Let T be a diagonalizable normal operator on a finite­\ndimensional inner product space V. Then T is self-adjoint, non-negative, or \nunitary according as each characteristic value of T is real, non-negative, or of \nabsolute value 1. \nProof. Suppose T has the spectral resolution T = clEl + . . . + \nckEk, then T* \nclEI + . . .  + ckEk• To say T is self-adjoint is to say \nT = T*, or \n(Cl - cl)El + . . . + (Ck - ck)Ek = O. \nUsing the fact that EiEJ = 0 for i ჭ j, and the fact that no EJ is the zero \noperator, we see that T is self-adjoint if and only if CJ = Cil j = 1, . .\n. , k. \nTo distinguish the normal operators which are non-negative, let us look at \n(Tala) \n(JlcjEJali>l Eia)",
    "Using the fact that EiEJ = 0 for i ჭ j, and the fact that no EJ is the zero \noperator, we see that T is self-adjoint if and only if CJ = Cil j = 1, . .\n. , k. \nTo distinguish the normal operators which are non-negative, let us look at \n(Tala) \n(JlcjEJali>l Eia) \n= A A e;(EJaIEia) \ni j \nA cjl lEjaW· \nj \nWe have used the fact that (EJaIEia) \n0 for i ჰ j. From this it is clear \nthat the condition (Tala) ;:: 0 is satisfied if and only if Cj ;:: 0 for each j. \nTo distinguish the unitary operators, observe that \nTT* = ClclE1 + \n'\n\"\n + CkCkEk \n= ICl12El + . . . \nICkI2Ek• \nIf TT* = I, then I \nICl12El \n+ [ckI2Ek, and operating with Ej \nE; \nICiI2Ej. \nSec. 9.5 \nSpectral Theory \nSince Ej 7'\" 0, we have \\Cj\\2 = 1 or lej\\ \neach j, it is clear that TT* = I. I \n1. Conversely, if \\ Cj\\2 = 1 for \nIt is important to note that this is a theorem about normal operators. \nIf T is a general linear operator on V which has real characteristic values, \nit does not follow that T is self-adjoint. The theorem states that if T has \nreal characteristic values, and if T is diagonalizable and normal, then T is \nself-adjoint. A theorem of this type serves to strengthen the analogy be­\ntween the adjoint operation and the process of forming the conjugate of a \ncomplex number. A complex number z is real or of absolute value 1 accord­\ning as z = Z, or zz = 1. An operator T is self-adjoint or unitary according \nas T = T* or T*T = I. \nWe are going to prove two theorems now, which are the analogues of \nthese two statements; \n(1) Every non-negative number has a unique non-negative square \nroot. \n(2) Every complex number is expressible in the form ru, where r is \nnon-negative and lui = 1. This is the polar decomposition z = rei8 for \ncomplex numbers. \nTheorem 13. Let V be a finite-dimensional inner product space and \nT a non-negative operator on V. Then T has a unique non-negative square root, \nthat is, there is one and only one non-negative operator N on V such that \nN2 = T. \nProof. Let T = clEI + .\n.\n.",
    "complex numbers. \nTheorem 13. Let V be a finite-dimensional inner product space and \nT a non-negative operator on V. Then T has a unique non-negative square root, \nthat is, there is one and only one non-negative operator N on V such that \nN2 = T. \nProof. Let T = clEI + .\n.\n.\n + ckEk be the spectral resolution of \nT. By Theorem 12, each Cj ჲ O. If c is any non-negative real number, let \nVc denote the non-negative square root of c. Then according to Theorem \n1 1  and (9-12) N = vr is a well-defined diagonalizable normal operator \non V. It is non-negative by Theorem 12, and, by an obvious computation, \nN2 = T. \nNow let P be a non-negative operator on V such that p2 = T. We \nshall prove that P = N. Let \nP = dlFl + . . .  + drFr \nbe the spectral resolution of P. Then dj ჲ 0 for each J', since P is non­\nnegative. From p2 = T we have \nT = d৏Fl + . . .  + d;Fr. \nNow FI, . . .  , Fr satisfy the conditions I = FI + . . .  + F\" F;Fj = 0 \nfor i 7'\" j, and no Fj is O. The numbers dt . . .  , d; are distinct, because \ndistinct non-negative numbers have distinct squares. By the uniqueness \nof the spectral resolution of T, we must have r = k, and (perhaps reorder­\ning) Fj = E,., d; = Cj. Thus P = N. \nI \nOperators on Inner Product Spaces \nChap. 9 \nTheorem 14. Let V be a finite-dimensional inner product space and \nlet T be any linear operator on V. Then there exist a unitary operator U on V \nand a non-negative operator N on V such that T \nUN. The non-negative \noperator N is unique. If T is invertible, the operator U is also unique. \nProof. Suppose we have T \nUN, where U is unitary and N \nis non-negative. Then T* = (UN)* = N*U* = NU*. Thus T*T = \nNU*UN = N2. This shows that N is uniquely determined as the non­\nnegative square root of the non-negative operator T*T. \nSo, to begin the proof of the existence of U and N, we use Theorem \n13 to define N as the unique non-negative square root of T*T. If T is \ninvertible, then so is N because \n(NaINa) = (N2ala) \n= (T*Tala) = (TaITa).",
    "negative square root of the non-negative operator T*T. \nSo, to begin the proof of the existence of U and N, we use Theorem \n13 to define N as the unique non-negative square root of T*T. If T is \ninvertible, then so is N because \n(NaINa) = (N2ala) \n= (T*Tala) = (TaITa). \nIn this case, we define U = TN-l and prove that U is unitary. Now \nU* \n(TN-I) * \n(N-l) *T* \n(N*)-lT* \nN-IT*. Thus \nand U is unitary. \nUU* \nTN-IN-IT* \nT(N-l)2T* \n= T(N2)-lT* \n= T(T*T)-lT* \nTT-l(T*)-lT* \n= 1 \nIf T is not invertible, we shall have to do a bit more work to define U. \nWe first define U on the range of N. Let a be a vector in the range of \nN, say a = N(3. We define Ua = T(3, motivated by the fact that we \nwant UN(3 = T(3. We must verify that U is well-defined on the range \nof N; in other words, if N(3' \nN(3 then T(3' \nT(3. We verified above \nthat I INI'I 12 = I ITI'I1 2 for every l' in V. Thus, with I' \n= (3 - (3', we see \nthat N«(3 - (3') = 0 if and only if T«(3 - (3') = o. So U is well-defined on \nthe range of N and is clearly linear where defined. Now if W is the range \nof N, we are going to define U on W.1. To do this, we need the following \nobservation. Since T and N have the same null space, their ranges have \nthe same dimension. Thus W.1 has the same dimension as the orthogonal \ncomplement of the range of T. Therefore, there exists an (inner product \nspace) isomorphism Uo of W.1 onto T(V).1. Now we have defined U on W, \nand we define U 011 W.1 to be Uo• \nLet us repeat the definition of U. Since V = W EEl W.1, each a in V \nis uniquely expressible in the form a \nN(3 + 1', where N(3 is in the range \nW of N, and I' is in W.L. We define \nUa = T(3 + Uol'. \nThis U is clearly linear, and we verified above that it is well-defined. Also \nSec. 9.5 \nSpectral Theory \n(U al U a) \n(T{3 + Uo1'1 T{3 + Uo1') \n= (T{31 T(3) + ( U01'1 Uo1') \n= (N(3IN{3) \n(1'11') \n= (ala) \nand so U is unitary. We also have UN(3 = '1'(3 for each (3. I \nWe call T = UN a polar deeomposition for T. We certainly cannot",
    "Sec. 9.5 \nSpectral Theory \n(U al U a) \n(T{3 + Uo1'1 T{3 + Uo1') \n= (T{31 T(3) + ( U01'1 Uo1') \n= (N(3IN{3) \n(1'11') \n= (ala) \nand so U is unitary. We also have UN(3 = '1'(3 for each (3. I \nWe call T = UN a polar deeomposition for T. We certainly cannot \ncall it the polar decomposition, since U is not unique. Even when T is \ninvertible, so that U is unique, we have the difficulty that U and N may \nnot commute. Indeed, they commute if and only if T is normal. For \nexample, if l' \nUN \nNU, with N non-negative and U unitary, then \nTT* \n(NU)(NU)* = NUU*N = N2 \nT*T. \nThe general operator l' will also have a decomposition T = N1U1, with \nNl non-negative and U1 unitary. Here, Nl will be the non-negative square \nroot of 1'1'*. We can obtain this result by applying the theorem just \nproved to the operator T*, and then taking adjoints. \nWe turn now to the problem of what can be said about the simultane­\nous diagonalization of commuting families of normal operators. For this \npurpose the following terminology is appropriate. \nDefinitions. Let g: be a family of operators on an inner product space \nV. A function r on g: with values in the field F of scalars will be called a root \nof g: if there is a non-zero a in V such that \nTa = r(T)a \nfor all T in g:. For any function r from g: to F, let VCr) be the set of all a in V \nsuch that Ta = r(T)a for every T in g:. \nThen VCr) is a subspace of V, and r is a root of g: if and only if VCr) ;;e \n{O} . Each non-zero a in V (r) is simultaneously a characteristic vector for \nevery T in g:. \nTheorem 15. Let g: be a commuting family of diagonalizable normal \noperators on a finite-dimensional inner product space V. Then g: has only a \nfinite number of roots. If rl, . . .  , rk are the distinct roots of g:, then \n(i) Veri) is orthogonal to V(rj) when i ;;e j, and \n(ii) V = V(rlH B . . . EEl V(rk). \nProof. Suppose r and s are distinct roots of F. Then there is an \noperator T in g: such that reT) ;;e s(1'). Since characteristic vectors",
    "(i) Veri) is orthogonal to V(rj) when i ;;e j, and \n(ii) V = V(rlH B . . . EEl V(rk). \nProof. Suppose r and s are distinct roots of F. Then there is an \noperator T in g: such that reT) ;;e s(1'). Since characteristic vectors \nbelonging to distinct characteristic values of T are necessarily orthogonal, \nit follows that VCr) is orthogonal to V(s). Because V is finite-dimensional, \nthis implies g: has at most a finite number of roots. Let rl, . . .  , rk be the \nOperators on Inner Product Spaces \nChap. 9 \nroots of F. Suppose {TI, •\n•\n•\n , T m} is a maximal linearly independent subset \nof 5', and let \n{EiI, Ei2' . . .  } \nbe the resolution of the identity defined by T; (1 S i S  m). Then the \nprojections Eij form a commutative family. For each Eij is a polynomial \nin Ti and TI, •\n•\n• , Tm commute with one another. Since \nI = (' Eli,) (' E2J,) . . . ({ EmiJ \nit \nh \nim \neach vector a in V may be written in the form \n(9-13) \nSuppose }I, . . .  , }m are indices for which (3 \nEIilE2j, . . . Emjma Y O. Let \n(3; \n( II  Enj.) a. \nnr'i \nThen (3 = Ei)i(3i; hence there is a scalar Ci such that \nTi{3 \nCifJ, 1 S i S  m. \nFor each T in 5', there exist unique scalars bi such that \nThus \nm \nT = ' b/l\\. \n; = 1  \nTfJ = ' b;T;fJ \n= C' bici) fJ· \ni \nThe function T -+ ´ bici is evidently one of the roots, say rt of 5', and (3 lies \n, \nin VCrt). Therefore, each nOll-zero term in (9-13) belongs to one of the \nspaces V(rl), . . .  , V(rk). It follows that V is the orthogonal direct sum of \nVCrl), . . .  , V(rk). I \nCorollary. Under the assumptions of the theorem, let Pj be the orthogonal \npro}ection of V on V(rj), (1 S j S k). Then PiPj = 0 when i ჰ j, \nI = PI + . . . + Pk, \nand every T in 5' may be written in the form \n(9-14) \nDefinitions. The family of orthogonal projections {I\\, . . .  , Pk} is \ncalled the resolution of the identity determined by 5', and (9-14) is the \nspectral resolution of T in terms of this family.",
    "I = PI + . . . + Pk, \nand every T in 5' may be written in the form \n(9-14) \nDefinitions. The family of orthogonal projections {I\\, . . .  , Pk} is \ncalled the resolution of the identity determined by 5', and (9-14) is the \nspectral resolution of T in terms of this family. \nAlthough the projections PI, . . .  , Pk in the preceding corollary are \ncanonically associated with the family 5', they are generally not in 5' nor \nSec. 9.5 \nSpectral Theory \neven linear combinations of operators in 5' ;  however, we shall show that \nthey may be obtained by forming certain products of polynomials in \nelements of 5'. \nIn the study of any family of linear operators on an inner product \nspace, it is usually profitable to consider the self-adjoint algebra generated \nby the family. \nDefinition. A self-adjoint algebra of operators on an inner \nproduct space V is a linear subalgebra of L(V, V) which contains the adjoint \nof each of its members. \nAn example of a self-adjoint algebra is L(V, V) itself. Since the \nintersection of any collection of self-adjoint algebras is again a self-adjoint \nalgebra, the following terminology is meaningful. \nDefinition. If 5' is a, family of linear operators on a finite-dimensional \ninner product space, the self-adjoint algebra generated by 5' is the smallest \nself-adjoint algebra which contains 5'. \nTheorem 16. Let ::Ji be a commuting family of diagonalizable normal \noperators on a finite-dimC'.'1,sional inner product space V, and let a be the self­\nadjoint algebra, generated by 5' and the identity operator. Let {PI, . . .  , Pk} be \nthe resolution of the identity defined by 5'. Then a is the set of all opera,tors on \nV of the form \n(9-15) \nk \nT = 2: CjPj \nj= 1 \nwhere Cl, •\n.\n• , Ck are arb'l৒trary scalars. \nProof. Let e denote the set of all operators on V of the form \n(9-15). Then e contains the identity operator and the adjoint \nT* = 2: ejPj \nj \nof each of its members. If T = 2: CjPj and U = 2: djP}, then for every \nscalar a \nand \nj \nj",
    "j= 1 \nwhere Cl, •\n.\n• , Ck are arb'l৒trary scalars. \nProof. Let e denote the set of all operators on V of the form \n(9-15). Then e contains the identity operator and the adjoint \nT* = 2: ejPj \nj \nof each of its members. If T = 2: CjPj and U = 2: djP}, then for every \nscalar a \nand \nj \nj \naT + U = 2: (ac + dj)Pj \nj \nTU = 2: cidjPiPj \ni,i \n= UT. \nThus e is a self-adjoint commutative algebra containing 5' and the identity \noperator. Therefore e contains a. \n346 \nOperators on Inner Product Spaces \nChap. 9 \nN ow let rl, . . .  , rk be all the roots of 5'. Then for each pair of indices \n(i, n) with i ჰ n, there is an operator Tin in 5' such that ri(Tin) ჰ rn(Tin). \nLet ain = ri(Tin) \nrn(Tin) and bin = rn(Tin). Then the linear operator \nQi \nII aiI(Tin \nbinI) \nn T\"i \nis an element of the algebra ct. We will show that Qi = Pi (1 ::; i ::; k). For \nthis, suppose j ჰ i and that a is an arbitrary vector in V(rj). Then \nTija = rj('l';j)a \n= bija \nso that (Tij - bi) a = O. Since the factors in Qi all commute, it follows \nthat Qia O. Hence Qi agrees with Pi on V(rj) whenever j ჭ i. Now \nsuppose a is a vector in Veri). Then Tina \nri(Tin)a, and \nai-;, I (Tin \nbinI) a \nai;;l[ri(Tin) - r,,(Tin)]a \na. \nThus Qia \na and Qi agrees with Pi on V(ri) ; therefore, Qi \nPi for \ni = 1, . . .  , k. From this it follows that ct = <3. \nI \nThe theorem shows that the algebra a is commutative and that each \nelement of ct is a diagonalizable normal operator. We show next that ct has \na single generator. \nCorollary . Under the assumptions of the theorem, there is an operator \nT in ct such that every member of ct is a polynomial in T. \nk \nProof. Let T = ' ljPj where tl, •\n•\n• , tk are distinct scalars. Then \ni = 1 \nfor n = 1, 2, . . . . If \nit follows that \nk \nTn \n= ' t7P; \n; = 1  \n8 \nf = ' anx\" \nn=1 \n8 \n• \nk \nf(T) = ' anTn = ' ' a\"t7Pj \nGiven an arbitrary \nn = 1  \nn = 1  ; = 1  \njǠ1 Cǟ1 ant']) Pj \nk \n' f(t;)Pj• \nj=1 \nk \nU = ' ejP,. \nj=1",
    "•\n• , tk are distinct scalars. Then \ni = 1 \nfor n = 1, 2, . . . . If \nit follows that \nk \nTn \n= ' t7P; \n; = 1  \n8 \nf = ' anx\" \nn=1 \n8 \n• \nk \nf(T) = ' anTn = ' ' a\"t7Pj \nGiven an arbitrary \nn = 1  \nn = 1  ; = 1  \njǠ1 Cǟ1 ant']) Pj \nk \n' f(t;)Pj• \nj=1 \nk \nU = ' ejP,. \nj=1 \nin ct, there is a polynomial f such that f(tj) = Cj (1 ::; j ::; k), and for any \nsuch f, U \nf(T). \nI \nSec. 9.5 \nSpectral Theory \nExercises \n1. Give a reasonable definition of a non-negative n X n matrix, and then prove \nthat such a matrix has a unique non-negative square root. \n2. Let A be an n X n matrix with complex entries such that A * \nand let \nB = eA. Show that \n(a) det B = etr A ; \n(b) B* = e-A ; \n(c) B is unitary. \n3. If U and T are normal operators which commute, prove that U + T and UT \nare normal. \n4. Let T be a linlيar operator on the finite-dimensional complex inner product \nspace V. Prove that the following ten statements about T are equivalent. \n(a) T is normal. \n(b) \\\\Ta\\\\ \n\\\\'1'*a\\\\ for every a in V. \n(c) T = TJ + iT2, where TJ and T2 are self-adjoint and TJT2 = T2T1• \n(d) If a is a vector and c a scalar such that Ta \nca, then T*a \nca. \n(e) There is an orthonormal basis for V consisting of characteristic vectors \nfor T. \n(f) There is an orthonormal basis C!3 such that [TJm is diagonal. \n(g) There is a polynomial g with complex coefficients such that T* = geT). \n(h) Every subspace which is invariant under T is also invariant under T*. \n(i) T \nN U, where N is non-negative, U is unitary, and N commutes with U. \nCD T = clEI + . . .  + ckEk, where I = EI + . . . + Ek, E;E; = 0 for i y6. j, \nand E; = E i = E1. \n5. Use Exercise 3 to show that any commuting family of normal operators (not \nnecessarily diagonalizable ones) on a finite-dimensional inner product space gen­\nerates a commutative self-adjoint algebra of normal operators. \n6. Let V be a finite-dimensional complex inner product space and U a unitary \noperator on V such that Ua \na implies a = O. Let \nand show that",
    "necessarily diagonalizable ones) on a finite-dimensional inner product space gen­\nerates a commutative self-adjoint algebra of normal operators. \n6. Let V be a finite-dimensional complex inner product space and U a unitary \noperator on V such that Ua \na implies a = O. Let \nand show that \nf( ) - . (1 + z) \nz - ƣ (1 - z)' \n(a) feU) = i(I + U)(I - U)-I; \n(b) feU) is self-adjoint; \nz ¢ 1 \n(c) for every self-adjoint operator T on V, the operator \nU \n(T - iI)(T + iI)-1 \nis unitary and such that T = feU). \n7. Let V be the space of complex n X n matrices equipped with the inner product \n(AlB) = tr (AB*). \nOperators on Inner Product Spaces \nChap. 9 \nIf B is an element of V, let LB, RB, and TB denote the linear operators on V de­\nfined by \n(a) LB(A) = BA. \n(b) RB(A) \nAB. \n(c) TB(A) \nBA - AB. \nConsider the three families of operators obtained by letting B vary over all diagonal \nmatrices. Show that each of these families is a commutative self-adjoint algebra \nand find their spectral resolutions. \n8. If B is an arbitrary member of the inner product space in Exercise 7, show that \nLB is unitarily equivalent to RBI. \n9. Let V be the inner product space in Exercise 7 and G the group of unitary \nmatrices in V. If B is in G, let CB denote the linear operator on V defined by \nCB(A) \nBAB-l. \nShow that \n(a) CB is a unitary operator on V; \n(b) CBIB, = CBlCB,; \n(c) there is no unitary transformation U on V such that \nULBU-l \nCB \nfor all B in G. \n10. Let £I' be any family of linear operators on a finite-dimensional inner product \nspace V and a the self-adjoint algebra generated by £1'. Show that \n(a) each root of a defines a root of £1'; \n(b) each root r of a is a multiplicative linear function on A, i.e., \nr(TU) \nr(T)r(U) \nr(eT + U) = cr(T) + r(U) \nfor all T and U in a and all scalars c. \nn. Let £I' be a commuting family of diagonalizable normal operators on a finite­\ndimensional iflner product space V; and let a be the self-adjoint algebra generated",
    "r(TU) \nr(T)r(U) \nr(eT + U) = cr(T) + r(U) \nfor all T and U in a and all scalars c. \nn. Let £I' be a commuting family of diagonalizable normal operators on a finite­\ndimensional iflner product space V; and let a be the self-adjoint algebra generated \nby £I' and the identity operator I. Show that each root of a is d.fferent from 0, \nand that for each root r of £I' there is a unique root 8 of a such that seT) = reT) \nfor all T in £1'. \n12. Let £I' be a commuting family of diagonalizable normal operators on a finite­\ndimensional inner product space V and Ao the self-adjoint algebra generated by ff'. \nLet a be the self-adjoint algebra generated by £I' and the identity operator I. \nShow that \n(a) a is the set of all operators on V of the form eI + T where c is a scalar \nand T an operator in ao \n(b) There is at most one root r of a such that reT) = 0 for all T in <:to. \n(c) If one of the roots of a is 0 on ao, the projections PI, . . . , Pk in the resolu­\ntion of the identity defined by £I' may be indexed in such a way that ao consists \nof all operatorll on V of the form \nSec. 9.6 \nFurther Properties of Normal Operators \nk \nT = ɣ c;P; \n;=2 \nwhere C2, •\n•\n•\n , Ck are arbitrary scalars. \n(d) a = aD if and only if for each root r of a there exists an operator T in ao \nsuch that reT) ؏ O. \n9.6. Further Properties of Normal \nOperators \nIn Section 8.5 we developed the basic properties of self-adjoint and \nnormal operators, using the simplest and most direct methods possible. \nIn Section 9.5 we considered various aspects of spectral theory. Here we \nprove some results of a more technical nature which are mainly about \nnormal operators on real spaces. \nWe shall begin by proving a sharper version of the primary decompo­\nsition theorem of Chapter 6 for normal operators. It applies to both the \nreal and complex cases. \nTheorem 17. Let T be a normal operator on a finite-dimensional inner \nproduct space V. Let p be the minimal polynomial for T and PI, \"\n' ,  Pk",
    "sition theorem of Chapter 6 for normal operators. It applies to both the \nreal and complex cases. \nTheorem 17. Let T be a normal operator on a finite-dimensional inner \nproduct space V. Let p be the minimal polynomial for T and PI, \"\n' ,  Pk \nits distinct monic prime factors. Then each pj occurs with multiplicity 1 in \nthe factorization of p and has degree 1 or 2. Suppose Wj is the null space of \npj (T). Then \n(i) Wi is orthogonal to Wi when i á j ;  \n(ii) V = WI EB · · ·  EB Wk; \n(iii) Wj is invariant under T, and Pi is the minimal polynomial for the \nrestriction of T to W j ; \n(iv) for every j, there is a polynomial ej with coefficients in the scalar \nfield such that ej(T) is the orthogonal proJ'ection of V on Wj. \nIn the proof we use certain basic facts which we state as lemmas. \nLemma 1. Let N be a normal operator on an inner product space W. \nThen the null space of N is the orthogonal complement of its range. \nProof. Suppose (aINiJ) = 0 for all 13 in W. Then (N*aliJ) = 0 \nfor all 13; hence N*a = O. By Theorem 19 of Chapter 8, this implies N a = O. \nConversely, if Na = 0, then N*a = 0, and \nfor all 13 in W. I \n(N*aliJ) \n= (aIN!3) = 0 \nLemma 2. If N is a normal operator and a is a vector such that \nN2a = 0, then Na = O. \n350 \nOperators on Inner Product Spaces \nChap. 9 \nProof. Suppose N is normal and that N2o. = O. Then No. lies in \nthe range of N and also lies in the null space of N. By Lemma 1, this \nimplies No. \nO. I \nLemma 3. Let T be a normal operator and f any polynomial with \ncoefficients in the scalar field. Then f(T) is also normal. \nand \nProof. Suppose f \nao + alX + . \" \n+ anxn. Then \nf(T) \naoZ + alT + . . , + aSn \nf(T)* = ao! + aIT* + . . . + an(T*)n. \nSince T*T = TT*, it follows that f(T) commutes with f(T)*. I \nLemma 4. Let T be a normal operator and f, g relatively prime poly­\nnomials with coefficients in the scalar field. Suppose a and (3 are vectors such \nthat f(T)a \n0 and g(T)(3 \nO. Then (0.1(3) \no.",
    "f(T)* = ao! + aIT* + . . . + an(T*)n. \nSince T*T = TT*, it follows that f(T) commutes with f(T)*. I \nLemma 4. Let T be a normal operator and f, g relatively prime poly­\nnomials with coefficients in the scalar field. Suppose a and (3 are vectors such \nthat f(T)a \n0 and g(T)(3 \nO. Then (0.1(3) \no. \nProof. There are polynomials a and b with coefficients in the \nscalar field such that af + bg = 1. Thus \na(T)f(T) + b(T)g(T) = I \nand a = g(T)b(T)a. It follows that \n(0.1(3) = (g(T)b(T)a/(3) = (b(T)a/g(T)*(3). \nBy assumption g(T)(3 = O. By Lemma 3, geT) is normal. Therefore, by \nTheorem 19 of Chapter 8, g(T)*(3 = 0; hence (0./(3) = O. I \nProof of Theorem 17. Recall that the minimal polynomial for T \nis the monic polynomial of least degree among all polynomials f such that \nf(T) \nO. The existence of such polynomials follows from the assumption \nthat V is finite-dimensional. Suppose some prime factor Pi of p is repeated. \nThen p = pig for some polynomial g. Since peT) \n0, it follows that \n(pj(T» 2g(T)a = 0 \nfor every a in V. By Lemma 3, pj(T) is normal. Thus Lemma 2 implies \npi(T)g(T)a \n0 \nfor every a in V. But this contradicts the assumption that p has least \ndegree among all f such that f(T) = O. Therefore, p = PI . . .  Pk. If V is \na complex inner product space each Pi is necessarily of the form \nPi = x - Cj \nwith Cj real or complex. On the other hand, if V is a real inner product \nspace, then pj = Xj \nCi with Cj in R or \nPi \n(x \nC) (x \nc) \nwhere c is a non-real complex number. \nSec. 9.6 \nFurther Properties of Normal Operators \nNow let Ii = pip;. Then, since fl' . . .  , fk are relatively prime, there \nexist polynomials gj with coefficients in the scalar field such that \n(9-16) \nWe briefly indicate how such gj may be constructed. If Pi = X - Ch \nthen fj(cj) ჭ 0, and for gj we take the scalar polynomial Ilficj). When \nevery pj is of this form, the fjgj are the familiar Lagrange polynomials \nassociated with el, . . .  , Ck, and (9-16) is clearly valid. Suppose some",
    "(9-16) \nWe briefly indicate how such gj may be constructed. If Pi = X - Ch \nthen fj(cj) ჭ 0, and for gj we take the scalar polynomial Ilficj). When \nevery pj is of this form, the fjgj are the familiar Lagrange polynomials \nassociated with el, . . .  , Ck, and (9-16) is clearly valid. Suppose some \npj = (x - e)(x - c) with e a non-real complex number. Then V is a real \ninner product space, and we take \ng . = ৐+ ৑ \nJ \nS \nS \nwhere 8 \n(C - c)fj(e). Then \nso that gj is a polynomial with real coefficients. If p has degree n, then \n1 \nJ:, fjgj \nj \nis a polynomial with real coefficients of degree at most n - 1 ;  moreover, \nit vanishes at each of the n (complex) roots of p, and hence is identically 0. \nNow let ex be an arbitrary vector in V. Then by (9-16) \nex = J:, fj(T)gj(T)ex \nj \nand since pj(T)fj(T) \n0, it follows that fj(T)gj(T)ex is in Wj for every i \nBy Lemma 4, Wj is orthogonal to Wi whenever i ჭ j. Therefore, V is the \northogonal direct sum of Wl, •\n•\n•\n , Wk. If (3 is any vector in Wi> then \npj(T)T{3 = Tpj(T){3 = 0; \nthus Wj is invariant under T. Let Tj be the restriction of T to Wj. Then \npj(Tj) = 0, so that pj is divisible by the minimal polynomial for Tj• Since \nPi is irreducible over the scalar field, it follows that pj is the minimal poly­\nnomial for Tj• \nNext, let ej = fjgj and Ej = ej(T). Then for every vector ex in V, \nE\\ex is in W;, and \nex \nJ:, Ejex. \nj \nThus ex - Eiex = J:, Ejex; since Wj is orthogonal to Wi when j ჰ i, this \ni,\",i \nimplies that ex \nEi(X is in Wi..L. It now follows from Theorem 4 of Chapter \n8 that E; is the orthogonal projection of V on Wi. I \nDefinition. We call the subspaces Wj (1 ::; j ::; k) the primary com­\nponents of V under T. \n351 \n352 \nOperators on Inner Product Spaces \nChap. 9 \nCorollary. Let T be a normal operator on a finite-dimensional inner \nproduct space V and Wl, . . .  , W k the primary components of V under T. \nSuppose W is a subspace of V which is invariant under T. Then \nW \n{ W n Wj. \nj",
    "351 \n352 \nOperators on Inner Product Spaces \nChap. 9 \nCorollary. Let T be a normal operator on a finite-dimensional inner \nproduct space V and Wl, . . .  , W k the primary components of V under T. \nSuppose W is a subspace of V which is invariant under T. Then \nW \n{ W n Wj. \nj \nProof. Clearly W contains { W n Wi' On the other hand, W, being \ni \ninvariant under T, is invariant under every polynomial in T. In particular, \nW is invariant under the orthogonal projection Ej of V on Wi' If a is in W, \nit follows that Ep is in W n Wil and, at the same time, a = { Eja. \nTherefore W is contained in { W n Wi' I \nj \nj \nTheorem 17 shows that every normal operator T on a finite­\ndimensional inner product space is canonically specified by a finite number \nof normal operators Til defined on the primary components Wj of V under \nT, each of whose minimal polynomials is irreducible over the field of \nscalars. To complete our understanding of normal operators it is necessary \nto study normal operators of this special type. \nA normal operator whose minimal polynomial is of degree 1 is clearly \njust a scalar multiple of the identity. On the other hand, when the minimal \npolynomial is irreducible and of degree 2 the situation is more complicated. \nEXAMPLE 1. Suppose r > ° and that (J is a real number which is not \nan integral multiple of 11\". Let T be the linear operator on R2 whose matrix \nin the standard orthonormal basis is \nA \n[COS (J \nr \n. \nsm 0 \n-sin OJ. \ncos 0 \nThen T is a scalar multiple of an orthogonal transformation and hence \nnormal. Let p be the characteristic polynomial of T. Then \np = det (xl \nA) \n(x \nr cos 0)2 + r2 sin2 0 \n= x - 2r cos Ox + r2. \nLet a = r cos (J, b = r sin (J, and c = a + ib. Then b ჯ 0, c = rei8 \nand p = (x - c)(x - c). Hence p is irreducible over R. Since p is divisible \nby the minimal polynomial for T, it follows that p is the minimal poly­\nnomial. \nThis example suggests the following converse. \nSec. 9.6 \nFurther Properties of Normal Operators",
    "and p = (x - c)(x - c). Hence p is irreducible over R. Since p is divisible \nby the minimal polynomial for T, it follows that p is the minimal poly­\nnomial. \nThis example suggests the following converse. \nSec. 9.6 \nFurther Properties of Normal Operators \nTheorem 18. Let T be a normal operator on a finite-dimensional real \ninner product space V and p its minimal polynomial. Suppose \np = (x - a)2 + b2 \nwhere a and b are real and b r-= O. Then there is an integer s > 0 such that \np. is the characteristic polynomial for T, and there exist subspaces Vl, . .\n. , V. \nof V such that \n(i) Vj is orthogonal to Vi when i r-= j ;  \n(ii) V = VI EB . . .  EB V.; \n(iii) each Vj has an orthonormal basis {aj, /Jj} with the property that \nTaj = at:Xj + b/Jj \nT/Jj = \n-baj + a/Jj. \nIn other words, if r = va2 + b2 and (J is chosen so that a = r cos (J \nand b = r sin (J, then V is an orthogonal direct sum of two-dimensional \nsubspaces Vj on each of which T acts as ir times rotation through the \nangle 0' . \nThe proof of Theorem 18 will be based on the following result. \nLemma. Let V be a real inner product space and S a normal operator \non V such that S2 + I \n= O. Let a be any vector in V and /J = Sa. Then \nS*a = -/J \n(9-17) \nS*/J = c. \n(allJ) = 0, and Iiall = I I/JII · \nProof. We have Sa = /J and S(3 = S2a = -a. Therefore \no = l iSa - /JW + IIS(3 + al 12 = IISal12 - 2(SallJ) + I IJ3W \n+ I ISJ3112 + 2(S(3la) + liali2• \nSince S is normal, it follows that \no = I IS*aW - 2(S*/Jla) + 11(3112 + I IS*/JW + 2(S*aj(3) + Iladj2 \n= IIS*a + /J112 + I IS*/J - aW· \nThis implies (9-17) ; hence \nand (aim \n= O. Similarly \n(aj/J) \n= (S*/Jjm = (/JjSm \n= ((3I -a) \n= - (ai/J) \nI lal J2 = (S*J3la) = ((3ISa) = 11(3J12· I \nProof of Theorem 18. Let VI, . . .  , Vs be a maximal collection \nof two-dimensional subspaces satisfying (i) and (ii), and the additional \nconditions \n353 \n354 \nOperators on Inner Product Spaces \nT*aj \naaj - Mil \n(9-18) \n1  j  s. \nT*{3j = baj + a{3j \nOhap. 9",
    "Proof of Theorem 18. Let VI, . . .  , Vs be a maximal collection \nof two-dimensional subspaces satisfying (i) and (ii), and the additional \nconditions \n353 \n354 \nOperators on Inner Product Spaces \nT*aj \naaj - Mil \n(9-18) \n1  j  s. \nT*{3j = baj + a{3j \nOhap. 9 \nLet TV = VI + . . . + V.. Then TV is the orthogonal direct sum of \nVI, . . .  , V •. We shall show that W = V. Suppose that this is not the case. \nThen W 1. Y {O} . Moreover, since (iii) and (9-18) imply that W is invariant \nunder T and T*, it follows that W1. is invariant under T* and T \nT**. \nLet S = b-I(T - aI). Then S* = b-1(T* - aI), S*S = SS*, and W1. is \ninvariant under S and S*. Since (T - aI)2 + b21 = 0, it follows that \nS2 + I \n0. Let a be any vector of norm 1 in W 1. and set {3 \nSa. Then \n{3 is in W.L and S{3 = -a. Since T = aI + bS, this implies \nTa = aa + b{3 \nT{3 = -ba + a{3. \nBy the lemma, S*a = -{3, S*{3 = a, (aim = 0, and i i{3i i  = 1 .  Because \nT* = aI + bS*, it follows that \nT*a = aa - b{3 \nT*{3 = ba + a{3. \nBut this contradicts the fact that VI, . . .  , V. is a maximal collection of \nsubspaces satisfying (i), (iii), and (9-18). Therefore, W = V, and since \ndet [\nx \n-b \na \nx  aJ = (x - a)2 \nit follows from (i), (ii) and (iii) that \ndet (xl - T) = [(x - a)2 + b2]8. I \nCorollary. Under the conditions of the theorem, T is invertible, and \nT* \n(a2 + b2)T-I. \nProof. Since \n[Ǟ \nit follows from (iii) and (9-18) that TT* = (a2 + b2)J. Hence T is invertible \nand T* = (a2 + b2) T-1. \nTheorem 19. Let T be a normal operator on a finite-dimensional inner \nproduct space V. Then any linear operator that commutes with T also com­\nmutes with T*. Moreover, every subspace invariant under T is also invariant \nunder T*. \nProof. Suppose U is a linear operator on V that commutes with \nT. Let Ej be the orthogonal projection of V on the primary component \nSec. 9.6 \nFurther Properties of Normal Operators \nWj (1 ::; j ::; k) of V under T. Then Ej is a polynomial in '1' and hence \ncommutes with U. Thus",
    "under T*. \nProof. Suppose U is a linear operator on V that commutes with \nT. Let Ej be the orthogonal projection of V on the primary component \nSec. 9.6 \nFurther Properties of Normal Operators \nWj (1 ::; j ::; k) of V under T. Then Ej is a polynomial in '1' and hence \ncommutes with U. Thus \nEjUEj = UEJ = UEj. \nThus U(Wj) is a subset of Wj. Let Tj and Uj denote the restrictions of T and \nU to Wj. Suppose Ij is the identity operator on Wj. Then Uj commutes \nwith Tj, and if Tj \ncjh it is clear that Ui also commutes with Tj = cih \nOn the other hand, if Tj is not a scalar multiple of h then Tj is invertible \nand there exist real numbers aj and bj such that \n'1'; = (aJ + bJ) Tj-1• \nSince UjTj = TjU;, it follows that Tj-1 Uj = UjTj-1• Therefore Ui com­\nmutes with Tj in both cases. Now '1'* also commutes with E;, and hence \nWj is invariant under '1'*. Moreover for every a and fJ in Wj \n(TalfJ) \nSince T*(Wj) is contained in Wj, this implies '1'; is the restriction of '1'* \nto Wj. Thus \nUT*aj = T*Uaj \nfor every aj in Wi' Since V is the sum of WI, . . .  , Wk, it follows that \nUT*a = T*Ua \nfor every a in V and hence that U commutes with T*. \nNow suppose W is a subspace of V that is invariant under '1', and let \nZj \nW n Wj. By the corollary to Theorem 17, W \n{ Zj. Thus it suffices \nj \nto show that each Zj is invariant under Tj. This is clear if Tj = c;I. When \nthis is not the case, Tj is invertible and maps Zj into and hence onto Zj. \nThus Tj-1(Zj) = Zj, and since \nT; = (aJ + bJ)Tj-1 \nit follows that T*(Zj) is contained in Zh for every j. I \nSuppose '1' is a normal operator on a finite-dimensional inner product \nspace V. Let W be a subspace invariant under T. Then the preceding \ncorollary shows that W is invariant under T*. From this it follows that \nWJ. is invariant under T** = '1' (and hence under T* as well). Using this \nfact one can easily prove the following strengthened version of the cyclic \ndecomposition theorem given in Chapter 7.",
    "corollary shows that W is invariant under T*. From this it follows that \nWJ. is invariant under T** = '1' (and hence under T* as well). Using this \nfact one can easily prove the following strengthened version of the cyclic \ndecomposition theorem given in Chapter 7. \nTheorem 20. Let T be a normal linear operator on a finite-dimensional \ninner product space V (dim V :?: 1). Then there exist r non-zero vectors \naI, . . .  , aT in V with respective T-annihilators el, . . .  , eT such that \n(i) V \nZ(al; T) EB . . .  EB Z(ar; T) ; \n(ii) if 1 ყ k ყ r - 1, then ek+l divides ek; \n355 \n356 \nOperators on Inner Product Spaces \nChap. 9 \n(iii) Z(aj ; T) is orthogonal to Z(ak; T) when j ;t. k. Furthermore, the \ninteger r and the annihilators el, . . .  , er are uniquely determined by condi­\ntions (i) and (ii) and the fact that no ak is O. \nCorollary. If A is a normal matrix with real (complex) entries, then \nthere is a real orthogonal (unitary) matrix P such that P-IAP is in rational \ncanonical form. \nIt follows that two normal matrices A and B are unitarily equivalent \nif and only if they have the same rational form; A and B are orthogonally \nequivalent if they have real entries and the same rational form. \nOn the other hand, there is a simpler criterion for the unitary equiv­\nalence of normal matrices and normal operators. \nDefinitions. Let V and V' be inner product spaces over the same field. \nA linear transformation \nU: V -+ V' \nis called a unitary transformation if it maps V onto V' and preserves \ninner products. If T is a linear operator on V and T' a linear operator on V', \nthen T is unitarily equivalent to T' if there exists a unitary transformation \nU of V onto V' such that \nUTU-I = 'fl. \nLemma. Let V and V' be finite-dimensional inner product spaces over \nthe same field. Suppose T is a linear operator on V and that T' is a linear \noperator on V'. Then T is unitarily equivalent to T' if and only if there is an",
    "U of V onto V' such that \nUTU-I = 'fl. \nLemma. Let V and V' be finite-dimensional inner product spaces over \nthe same field. Suppose T is a linear operator on V and that T' is a linear \noperator on V'. Then T is unitarily equivalent to T' if and only if there is an \northonormal basis (B of V and an orthonormal basis (B' of V' such that \n[T]LI = [T']LI\" \nProof. Suppose there is a unitary transformation U of V onto \nV' such that UTU-I = T'. Let (B = {ai, . . .  , an} be any (ordered) \northonormal basis for V. Let aj = Uaj (1 ::; j ::; n). Then (B' = {ai, .\n.\n.\n , \na} is an orthonormal basis for V' and setting \nwe see that \nHence [71<Il = A \n[T']<Il/ \nn \nTaj = { Akjak \nk= l  \nT'aj = UTaj \n= { AkjUak \nk \n= { Akjak \nk \nSec. 9.6 \nFurther Properties of Normal Operators \nConversely, suppose there is an orthonormal basis CB of V and an \northonormal basis CB' of V' such that \n[TJ{\\ = [T/J(B' \nand let A = [TJ(B. Suppose CB = {aI, . . .  , an} and that CB' = {aL . . .  , aჩ} . \nLet U be the linear transformation of V into V' such that U aj = a; \n(1 ყ j ყ n). Then U is a unitary transformation of V onto V', and \nUTU-1a; = UTaj \n= U Ӥ Akjak \nk \n= { Akja£. \nk \nTherefore, UTU--laӥ = T'aӥ (1 ყ j ყ n), and this implies UTU-l = \nT'. I \nIt follows immediately from the lemma that unitarily equivalent \noperators on finite-dimensional spaces have the same characteristic poly­\nnomial. For normal operators the converse is valid. \nTheorem 21. Let V and V' be finite-dimensional inner product spaces \nover the same field. Suppose T is a normal operator on V and that T' is a \nnormal operator on V'. Then T is unitarily equivalent to T' if and only if T \nand T' have the same characteristic polynomial. \nProof. Suppose T and T' have the same characteristic poly­\nnomial f. Let Wj (1 ყ j ყ k) be the primary components of V under T \nand Tj the restriction of T to Wj. Suppose Ij is the identity operator on \nWj. Then \nk \nf = II det (xIj - Tj). \nj = l",
    "and T' have the same characteristic polynomial. \nProof. Suppose T and T' have the same characteristic poly­\nnomial f. Let Wj (1 ყ j ყ k) be the primary components of V under T \nand Tj the restriction of T to Wj. Suppose Ij is the identity operator on \nWj. Then \nk \nf = II det (xIj - Tj). \nj = l  \nLet pj be the minimal polynomial for Tj• If Pi = X - Cj it is clear that \ndet (xIj - Tj) = (x - Cj)\"j \nwhere Sj is the dimension of Wj. On the other hand, if Pi = (x - aj)2 + bJ \nwith aj, bj real and bj ;rf 0, then it follows from Theorem 18 that \ndet (xIj - Tj) = pji \nwhere in this case 2sj is the dimension of Wj. Therefore f = II p? Now \nj \nwe can also compute f by the same method using the primary components \nof V' under T'. Since PI, . . .  , Pk are distinct primes, it follows from the \nuniqueness of the prime factorization of f that there are exactly k primary \ncomponents W; (1 ყ j ყ k) of V' under T' and that these may be indexed \nin such a way that pj is the minimal polynomial for the restriction Tj of \nT' to Wi. If pj = x - Ch then Tj = cjIj and Ti = cjIi where Ii is the \n357 \n358 \nOperators on Inner Product Spaces \nChap. 9 \nidentity operator on Wj. In this case it is evident that 1'; is unitarily \nequivalent to Tj. If Pi = (x - aY + bJ, as above, then using the lemma \nand Theorem 20, we again see that Tj is unitarily equivalent to Tj. Thus \nfor each j there are orthonormal bases illj and il; of Wj and Wi, respec­\ntively, such that \n[TJ:s; = [Tall;'. \nNow let U be the linear transformation of V into V' that maps each il; \nonto ilj. Then U is a unitary transformation of V onto V' such that \nUTU-l = T'. I \n10. Bilinear \nForms \n10.1. Bilinear Forms \nIn this chapter, we treat bilinear forms on finite-dimensional vector \nspaces. The reader will probably observe a similarity between some of the \nmaterial and the discussion of determinants in Chapter 5 and of inner \nproducts and forms in Chapter 8 and in Chapter 9. The relation between",
    "In this chapter, we treat bilinear forms on finite-dimensional vector \nspaces. The reader will probably observe a similarity between some of the \nmaterial and the discussion of determinants in Chapter 5 and of inner \nproducts and forms in Chapter 8 and in Chapter 9. The relation between \nbilinear forms and inner products is particularly strong; however, this \nchapter does not presuppose any of the material in Chapter 8 or Chapter 9. \nThe reader who is not familiar with inner products would probably profit \nby reading the first part of Chapter 8 as he reads the discussion of bilinear \nforms. \nThis first section treats the space of bilinear forms on a vector space \nof dimension n. The matrix of a bilinear form in a,n ordered basis is intro­\nduced, and the isomorphism between the space of forms and the space of \nn X n matrices is established. The rank of a bilinear form is defined, and \nnon-degenerate bilinear forms are introduced. The second section discusses \nsymmetric bilinear forms and their diagonalization. The third section \ntreats skew-symmetric bilinear forms. The fourth section discusses the \ngroup preserving a non-degenerate bilinear form, with special attention \ngiven to the orthogonal groups, the pseudo-orthogonal groups, and a \nparticular pseudo-orthogonal group-the Lorentz group. \nDefinition. Let V be a vector space over the field F. A bilinear form \non V is a function f, which assigns to each ordered pair of vectors a, (3 in V a \nscalar f(a, f3) in F, and which satisfies \n359 \n360 \nBilinear Forms \n(10-1) \nf(cal + tX2, (3) = cf(al, (3) + f(a2, (3) \nf(a, C{3l + (32) \ncf(a, (31) + rea, (32). \nChap. 10 \nIf we let V X V denote the set of all ordered pairs of vectors in V, \nthis definition can be rephrased as follows: A bilinear form on V is a func­\ntion f from V X V into F which is linear as a function of either of its \narguments when the other is fixed. The zero function from V X V into F",
    "If we let V X V denote the set of all ordered pairs of vectors in V, \nthis definition can be rephrased as follows: A bilinear form on V is a func­\ntion f from V X V into F which is linear as a function of either of its \narguments when the other is fixed. The zero function from V X V into F \nis clearly a bilinear form. It is also true that any linear combination of \nbilinear forms on V is again a bilinear form. To prove this, it is sufficient \nto consider linear combinations of the type cf + g, where f and g are \nbilinear forms on V. The proof that ef + g satisfies (10-1) is similar to many \nothers we have given, and we shall thus omit it. All this may be summarized \nby saying that thc set of all bilinear forms on V is a subspace of the space \nof all functions from V X V into F (Example 3, Chapter 2). We shall \ndenote the space of bilinear forms on V by L(V, V, F). \nEXAMPLE 1. Let V be a vector space over the field F and let L1 and \nL2 be linear functions on V. Define f by \n!(ex, (3) = L1(a)L2({3). \nIf we fix (3 and regard f as a function of a, then we simply have a scalar \nmultiple of the linear functional L1• With a fixed, ! is a scalar multiple of \nL2• Thus it is clear that ! is a bilinear form on V. \nEXAMPLE 2. Let m and n be positive integers and F a field. Let V be \nthe vector space of all m X n matrices over F. Let A be a fixed m X m \nmatrix over F. Define \n!A(X, Y) \ntr (XiA Y). \nThen fA is a bilinear form on V. For, if X, Y, and Z are m X n matrices \nover F, \n!A(eX + Z, Y) \ntr [(eX + Z)tA Y] \n= tr (cXtA Y) + tr (ZtA Y) \n= efA(X, Y) + fA(Z, Y). \nOf course, we have used the fact that the transpose operation and the \ntrace function are linear. It is even easier to show that fA is linear as a \nfunction of its second argument. In the special case n = 1, the matrix \nXtA Y is 1 X 1, i.e., a scalar, and the bilinear form is simply \nfA(X, Y) = XtA Y \n= { { Ai;XiYj· \ni i \nWe shall presently show that every bilinear form on the space of m X 1",
    "function of its second argument. In the special case n = 1, the matrix \nXtA Y is 1 X 1, i.e., a scalar, and the bilinear form is simply \nfA(X, Y) = XtA Y \n= { { Ai;XiYj· \ni i \nWe shall presently show that every bilinear form on the space of m X 1 \nmatrices is of this type, i.e., is fA for some m X m matrix A. \nSec. 10.1 \nBilinear Forms \nEXAMPLE 3. Let F be a field. Let us find all bilinear forms on the \nspace F2. Suppose 1 is such a bilinear form. If a = (Xl, X2) and (3 = (Yl, Y2) \nare vectors in F2, then \nI(a, (3) = I(XIEI \nX2E2, (3) \nxd( EI, (3) \nxd( f2, (3) \n= Xd(EI' YIEI + Y2f2) + Xd(E2, YIEI + Y2E2) \n= xlyd(EI, EI) + xlyd(El, E2) + x2yd(E2, El) + x2yd(e2, E2). \nThus f is completely determined by the four scalars Aij = f(Ei, Ej) by \n1(a, (3) = AllxlYl + A12XIY2 + A21X2Yl + A22X2Y2 \n= { A ijx,Yj· \ni,f \nIf X and Y are the coordinate matrices of a and f3, and if A is the 2 X 2 \nmatrix with entries A (i, j) \nA ij \nf( Ei, Ej) ,  then \n(10-2) \n1(a, (3) = XtA Y. \nWe observed in Example 2 that if A is any 2 X 2 matrix over F, then \n(10-2) defines a bilinear form on F2. We see that the bilinear forms on F2 \nare precisely those obtained from a 2 X 2 matrix as in (10-2). \nThe discussion in Example 3 can be generalized so as to describe all \nbilinear forms on a finite-dimensional vector space. Let V be a finite­\ndimensional vector space over the field F and let CB = {aI, . . .  , an} be \nan ordered basis for V. Suppose 1 is a bilinear form on V. If \na\nXial + . . .  + Xnan and f3 \nYlal + . . . + Ynan \nare vectors in V, then \nI(a, (3) \n1 (1 Xiai, (3) \n= { xJ(a;, (3) \ni \n= 1 x;j (ai, r Yjai) \n{ { xiy;!(ai, aj). \ni \nf \nIf we let Aii = 1(a;, aj), then \n1(a, (3) \n{ { AijXiYi \n; j \n= XtA Y \nwhere X and Y are the coordinate matrices of a and (3 in the ordered \nbasis CB. Thus every bilinear form on V is of the type \n(10-3) \n!(a, (3) \n[a JiBA [(3Jm \nfor some n X n matrix A over F. Conversely, if we are given any n X n",
    "If we let Aii = 1(a;, aj), then \n1(a, (3) \n{ { AijXiYi \n; j \n= XtA Y \nwhere X and Y are the coordinate matrices of a and (3 in the ordered \nbasis CB. Thus every bilinear form on V is of the type \n(10-3) \n!(a, (3) \n[a JiBA [(3Jm \nfor some n X n matrix A over F. Conversely, if we are given any n X n \nmatrix A, it is easy to see that (10-3) defines a bilinear form ! on V, such \nthat Ai; = 1(a;, aj). \n361 \n362 \nBilinear Forms \nChap. 10 \nDefinition. Let V be a finite-dimensional vector space, and let \nil \n= {al, . . .  , an} be an ordered basis for V. If f is a bilinear form on V, \nthe matrix of f in the ordered basis il is the n X n matrix A with entries \nAii = f(ai' aJ. At times, we shall denote this matrix by [fJm. \nTheorem 1. Let V be a finite-dimensional vector space over the field F. \nFor each ordered basis il of V, the function which associates with each bilinear \nform on V its matrix 1:n the ordered basis ill is an isomorphism of the space \nL(V, V, F) onto the space of n X n matrices over the field F. \nProof. We observed above that f -t [nCB is a one-one corre­\nspondence between the set of bilinear forms on V and the set of all n X n \nmatrices over F. That this is a linear transformation is easy to see, because \n(cf + y)(ai' aj) \ncf(ai, a)) + yea\"c 0.)) \nfor each i and j. This simply says that \n[cf + gJrn \nc[nm + [yJm. I \nCorollary. If il = {al, . . .  , an} is an ordered basis for V, and \nil* \n{LI' . . .  , Ln} is the dual basis for V*, then the n2 bilinear forms \nfuCa, (3) \n= Li(a)Lj ((3), \n1 .:; i ':; n, 1 .:; j .:; n \nform a basis for the space L(V, V, F). In particular, the dimension of \nL(V, V, F) is n2• \nProof. The dual basis {LJ, . . .  , Ln} is essentially defined by the \nfact that Li(a) is the ith coordinate of a in the ordered basis il (for any \na in V). Now the functions fij defined by \nf,;(a, (3) \nLi(a)Lj((3) \nare bilinear forms of the type considered in Example 1. If \na \nXl(11 + .\n. . + xnan and (3 \nYlal + . . .  + Ynan, \nthen \nfoCa, (3) \n= XiY;·",
    "fact that Li(a) is the ith coordinate of a in the ordered basis il (for any \na in V). Now the functions fij defined by \nf,;(a, (3) \nLi(a)Lj((3) \nare bilinear forms of the type considered in Example 1. If \na \nXl(11 + .\n. . + xnan and (3 \nYlal + . . .  + Ynan, \nthen \nfoCa, (3) \n= XiY;· \nLet f be any bilinear form on V and let A be the matrix of f in the \nordered basis il. Then \nfCa, (3) \nѓ AijXiYj \n'.J \nwhich simply says that \nIt is now clear that the n2 forms fij comprise a basis for L(V, V, F). \nI \nOne can rephrase the proof of the corollary as follows. The bilinear \nform fi) has as its matrix in the ordered basis il the matrix 'unit' Ei,i, \nSec. 10.1 \nBilinear Forms \nwhose only non-zero entry is a 1 in row i and column j. Since these matrix \nunits comprise a basis for the space of n X n matrices, the forms fij com­\nprise a basis for the space of bilinear forms. \nThe concept of the matrix of a bilinear form in an ordered basis is \nsimilar to that of the matrix of a lineal' operator in an ordered basis. Just \nas for linear operators, we shall be interested in what happens to the \nmatrix representing a bilinear form, as we change from one ordered basis \nto another. So, suppose (B = {al, . . .  , an} and (B' = {ai, . . .  , aÞ} are \ntwo ordered bases for V and that f is a bilinear form on V. How are the \nmatrices [f]<Jl and [!JCII' related? Well, let P be the (invertible) n X n \nmatrix such that \n[a]<Jl = P [a]<Jl' \nfor all a in V. In other words, define P by \nn \naj = { Pijai. \ni= l \nFor any vectors a, (3 in V \nf(a, (3) \n= [aJ&! [fJc\\l [(3J<Jl \n= (P [aJ<Jl,) t [jJ<JlP [(3]<Jl' \n[aJlw(pt [jJ<JlP) [(3]<Jl', \nBy the definition and uniqueness of the matrix representing f in the \nordered basis (B', we must have \n(10-4) \n[jJm' = pt [jJmP. \nEXAMPLE 4. Let V be the vector space R2. Let f be the bilinear form \ndefined on a = (Xl, X2) and (3 \n(Yl, Y2) by \nf(a, (3) \nXIYl + XIY2 + X2Yl + X2Y2. \nNow \nf(a, (3) = [Xl, X2J [l lJ [ǝ:J",
    "ordered basis (B', we must have \n(10-4) \n[jJm' = pt [jJmP. \nEXAMPLE 4. Let V be the vector space R2. Let f be the bilinear form \ndefined on a = (Xl, X2) and (3 \n(Yl, Y2) by \nf(a, (3) \nXIYl + XIY2 + X2Yl + X2Y2. \nNow \nf(a, (3) = [Xl, X2J [l lJ [ǝ:J \nand so the matrix of f in the standard ordered basis (B = {EI' E2} is \nLet (B' = {Ei, En be the ordered basis defined by Ei = (1, -1), Eß = (1, 1). \nIn this case, the matrix P which changes coordinates from (B' to (B is \nThus \n[jJ<Jl' = pt [fJ<JlP \n= [1 -lJ [1 1J [ 1 1J \n1 1 1 1 -1 1 \n363 \n364 \nBilinear Forms \n= [2 -2J [3 4J \n= [3 ǜl \nChap. 10 \nWhat this means is that if we express the vectors a and fJ by means of \ntheir coordinates in the basis (B', say \nthen \nf(a, fJ) \n4xৈy৉. \nOne consequence of the change of basis fonnula (10-4) is the following: \nIf A and B are n X n matrices which represent the same bilinear form \non V in (possibly) different ordered bases, then A and B have the same \nrank. For, if P is an invertible n X n matrix and B = PtAP, it is evident \nthat A and B have the same rank. This makes it possible to define the \nrank of a bilinear form on V as the rank of any matrix which represents \nthe form in an ordered basis for V. \nIt is desirable to give a more intrinsic definition of the rank of a \nbilinear form. This can be done as follows: Suppose f is a bilinear form \non the vector space V. If we fix a vector a in V, then f(a, fJ) is linear as \na function of fJ. In this way, each fixed a determines a linear functional \non V; let us denote this linear functional by LrCa). To repeat, if a is a \nvector in V, then Lt(a) is the linear functional on V whose value on any \nvector fJ is f(a, fJ). This gives us a transformation a -+ Lj(a) from V into \nthe dual space V*. Since \nwe see that \nLiCeal + (2) = eLj(al) + Lj(a2) \nthat is, Lj is a linear transformation from V into V*. \nIn a similar manner, f determines a linear transformation RI from V",
    "vector fJ is f(a, fJ). This gives us a transformation a -+ Lj(a) from V into \nthe dual space V*. Since \nwe see that \nLiCeal + (2) = eLj(al) + Lj(a2) \nthat is, Lj is a linear transformation from V into V*. \nIn a similar manner, f determines a linear transformation RI from V \ninto V*. For each fixed fJ in V, f(a, fJ) is linear as a function of a. We define \nRf(fJ) to be the linear functional on V whose value on the vector a isf(a, (3). \nTheorem 2. Let f be a bilinear form on the finite-dimensional vector \nspace V. Let Lf and Rf be the linear transformations from V into V* defined \nby (Lfa)(fJ) = f(a, fJ) = (RffJ) (a). Then rank (Lf) = rank (Rf). \nProof. One can give a 'coordinate free' proof of this theorem. \nSuch a proof is similar to the proof (in Section 3.7) that the row-rank of a \nmatrix is equal to its column-rank. So, here we shall give a proof which \nproceeds by choosing a coordinate system (basis) and then using the \n'row-rank equals column-rank' theorem. \nTo prove rank (LI) \nrank CRt), it will suffice to prove that LI and \nSec. 10.1 \nBilinear Forms \nRf have the same nullity. Let <3 be an ordered basis for V, and let A = [fJ(\\. \nIf a and fJ are vectors in V, with coordinate matrices X and Y in the \nordered basis <3, then f(a, (3) \n= XtA Y. Now Rf(fJ) \n= 0 means that \nf(a, (3) = 0 for every a in V, i.e., that XtA Y = 0 for every n X 1 matrix X. \nThe latter condition simply says that A Y = O. The nullity of Rj is there-\nfore equal to the dimension of the space of solutions of A Y \nO. \nSimilarly, Lf(a) = 0 if and only if XIA Y \n0 for every n X 1 matrix \nY. Thus a is in the null space of Lf if and only if XIA \n= 0, i.e., A IX = O. \nThe nullity of Lf is therefore equal to the dimension of the space of solu­\ntions of A IX \nO. Since the matrices A and A t have the same column­\nrank, we see that \nnullity (Lf) = nullity (Rr)· \nI \nDefinition. If f is a bilinear form on the finite-dimen.sional space V, \nthe rank of f is the integer r = rank (LI) = rank (HI).",
    "tions of A IX \nO. Since the matrices A and A t have the same column­\nrank, we see that \nnullity (Lf) = nullity (Rr)· \nI \nDefinition. If f is a bilinear form on the finite-dimen.sional space V, \nthe rank of f is the integer r = rank (LI) = rank (HI). \nCorollary 1. The rank of a bilinear form is equal to the rank of the \nmatrix of the form in any ordered basis. \nCorollary 2. If f is a bilinear form on the n-dimensional vector space \nV, the following are equivalent: \n(a) rank (f) \n= n. \n(b) For each non-zero a in V, there is a (3 in V such that f(a, (3) Y o. \n(c) For each non-zero (3 in V, there is an a in V such that f(a, (3) ჭ O. \nProof. Statement (b) simply says that the null space of Lj is the \nzero subspace. Statement (c) says that the null space of Rf is the zero \nsubspace. The linear transformations Lf and Rj have nullity 0 if and only \nif they have rank n, i.e., if and only if rank (f) = n. \nI \nDefinition. A bilinear form f on a vector space V is called non­\ndegenerate (or non-singular) if it satisfies conditions (b) and (c) of \nCorollary 2. \nIf V is finite-dimensional, then f is non-degenerate provided f satisfies \nany one of the three conditions of Corollary 2. In particular, f is non­\ndegenerate (non-singular) if and only if its matrix in some (every) ordered \nbasis for V is a non-singular matrix. \nEXAMPLE 5. Let V = R\", and let f be the bilinear form defined on \na = (Xl, . . .  , Xn) and (3 = (YI, . . . , Yn) by \nf(a, (3) = XlYl + . . . \nXnYn' \n365 \n366 \nBilinear Forms \nChap. 10 \nThen f is a non-degenerate bilinear form on Rn. The matrix of f in the \nstandard ordered basis is the n X n identity matrix: \nf(X, y) = Xly. \nThis f is usually called the dot (or scalar) product. The reader is probably \nfamiliar with this bilinear form, at least in the case n \n3. Geometrically, \nthe number f(a, (3) is the product of the length of a, the length of (3, and \nthe cosine of the angle between a and (3. In particular, f(a, (3) = 0 if and",
    "familiar with this bilinear form, at least in the case n \n3. Geometrically, \nthe number f(a, (3) is the product of the length of a, the length of (3, and \nthe cosine of the angle between a and (3. In particular, f(a, (3) = 0 if and \nonly if the vectors a and (3 are orthogonal (perpendicular). \nExercises \n1. Which of the following functions I, defined on vectors a \n= (Xl, xz) and {3 \n(YI, Yz) in R2, are bilinear forms? \n(a) I(a, (3) = 1. \n(b) I(a, (3) \n(Xl - Yl)2 + X2Yz. \n(c) I(a, (3) = (Xl + YIF - (Xl - Yl)2. \n(d) I(a, (3) = XIY2 \nX2Yl. \n2. Lct I be the bilinear form on R2 defined by \nf«Xl, Yl), (X2, Y2» \n= XIYl + X2YZ· \nFind the matrix of I in each of the following bases: \n{(I, 0), (0, I)} , \n{(I, -1), (1, In , \n{(I, 2), (3, 4)} . \n3. Let V be the space of all 2 X 3 matrices over R, and let f be the bilinear form \non V defined by I(X, Y) = trace (XtA Y), where \nA [ǚ Ǜl \nFind the matrix of f in the ordered basis \n{Ell, E12, Ela, E21, E22, E23} \nwhere Eii is the matrix whose only non-zero entry is a 1 in row i and column j. \n4. Describe explicitly all bilinear forms f on R3 with the property that f(a, (3) \n= \nf({3, a) for all a, (3. \n5. Describe the bilinear forms on R3 which satisfy f(a, (:1) \n= \n-f«(3, a) for all a, (3. \n6. Let n be a positive integer, and let V be the space of all n X n matrices over \nthe field of complex numbers. Show that the equation \nI(A, B) \nn tr (AB) - tr (A) tr (B) \ndefines a bilinear form f on V. Is it true that I(A, B) \nfeB, A) for all A, B? \n7. Let I be the bilinear form defined in Exercise 6. Show that I is degenerate \n(not non-degenerate). Let VI be the subspace of V consisting of the matrices of \ntrace 0, and let fl be the restriction of f to VI. Show that fl is non-ciegenerate. \nSec. 10.2 \nSymmetric Bilinear Forms \n8. Let f be the bilinear form defined in Exercise 6, and let Y2 be the subspace \nof Y consisting of all matrices 11 such that trace (A) \n= 0 and A * = -A (11 * is",
    "trace 0, and let fl be the restriction of f to VI. Show that fl is non-ciegenerate. \nSec. 10.2 \nSymmetric Bilinear Forms \n8. Let f be the bilinear form defined in Exercise 6, and let Y2 be the subspace \nof Y consisting of all matrices 11 such that trace (A) \n= 0 and A * = -A (11 * is \nthe conjugate transpose of 11). Denote by 12 the restriction of 1 to Y2• Show that \nh is negative definite, i.e., that 12(11, A) < 0 for each non-zero A in V2• \n9. Let 1 be the bilinear form defined in Exercise 6. Let W be the set of all matrices \nA in V such that f(A) B) = 0 for all B. Show that IV is a subspace of V. Describe \nW explicitly and find its dimension. \n10. Let 1 be allY bilinear form on a finite-dimensional vector space V. Let W be the \nsubspace of all (3 such that f(a, (3) = 0 for every a. Show that \nrank f = dim Y - dim W. \nUsc this result and the result of Exercise 9 to compute the rank of the bilinear \nform defined in Exercise 6. \nll. Let f be a bilinear form on a finite-dimensional vector space V. Suppose VI \nis a subspace of V with the propcrty that the restriction of f to VI is non-degenerate. \nShow that rank f 9 dim VI. \n12. Let f, y be bilinear forms on a finite-dimensional vector space V. Suppose y \nis non-singular. Show that there exist unique linear operatonl TI, T2 on Y such that \nf(a, (3) \ny(Tla, (3) = yea, T2(3) \nfor all a, (3. \n13. Show that the result given in Exercise 12 need not be true if g is singular. \n14. Let f be a bilinear form on a finite-dimensional vector space V. Show that f can \nbe expres̘ed as a product of two linear functionals (i.e., f(a, (3) = LI(a)L2((3) for \nL1, Lz ill Y*) if and only if f has rank 1 .  \n367 \n10.2. Symmetric Bilinear Forms \nThe main purpose of this section is to answer the following question: \nIf j is a bilinear form on the finite-dimensional vector space V, when is \nthere an ordered basis CB for V in which f is represented by a diagonal \nmatrix? We prove that this is possible if and only if j is a symmetric",
    "The main purpose of this section is to answer the following question: \nIf j is a bilinear form on the finite-dimensional vector space V, when is \nthere an ordered basis CB for V in which f is represented by a diagonal \nmatrix? We prove that this is possible if and only if j is a symmetric \nbilinear form, i.e., f(a, (3) = f((3, a) . The theorem is proved only when \nthe scalar field has characteristic zero, that is, that if n is a positive integer \nthe sum 1 + . . .  + 1 (n times) in F is not O. \nDefinition. Let f be a bilinear form on the vector space V. We say \nthat f is symmetrie if f(a, (3) \nf«(3, a) jor all vectors a, j3 in V. \nIf V is a finite-dimensional, the bilinear form f is symmetric if and \nonly if its matrix A in some (or every) ordered basis is symmetric, A t  \nA .  \nTo see this, one inquires when the bilinear form \nf(X, Y) = XtA Y \n368 \nBilinear Forms \nChap. 10 \nis symmetric. This happens if and only if XtA Y \nPAX for all column \nmatrices X and Y. Since XtA Y is a 1 X 1 matrix, we have XtA Y \nytA IX. \nThus f is symmetric if and only if PA IX = ylAX for all X, Y. Clearly \nthis just means that A \n= A t. In particular, one should note that if there \nis an ordered basis for V in which f is represented by a diagonal matrix, \nthen f is symmetric, for any diagonal matrix is a symmetric matrix. \nIf f is a symmetric bilinear form, the quadratic form associated \nwith f is the function q from V into F defined by \nq(a) \nf(a, a). \nIf F is a sub field of the complex numbers, the symmetric bilinear form f \nis completely determined by its associated quadratic form, according to \nthe polarization identity \n(10-5) \nf(a, (3) \ntq(a + (3) - tq(a \nm. \nThe establishment of (10-1) is a routine computation, which we omit. If \nf is the bilinear form of Example 5, the dot product, the associated quad­\nratic form is \nq(Xl, . . .  , xn) \n= xi + \n. . .  + x;. \nIn other words, q(a) is the square of the length of a. For the bilinear form \nfA(X, Y)",
    "m. \nThe establishment of (10-1) is a routine computation, which we omit. If \nf is the bilinear form of Example 5, the dot product, the associated quad­\nratic form is \nq(Xl, . . .  , xn) \n= xi + \n. . .  + x;. \nIn other words, q(a) is the square of the length of a. For the bilinear form \nfA(X, Y) \nXIA Y, the associated quadratic form is \nqA(X) \nXIAX \n{ Aijxixj. \ni,j \nOne important class of symmetric bilinear forms consists of t.he inner \nproducts on real vector spaces, discussed in Chapter 8. If V is a real \nvector space, an inner product on V is a symmet.ric bilinear form f on \nV which satisfies \n(10-6) \nf(a, a) > 0 if a -:;t! O. \nA bilinear form satisfying (10-6) is called positive definite. Thus, an \ninner product on a real vector space is a positive definite, symmetric \nbilinear form on that. space. Note that an inner product is non-degenerate. \nTwo vectors a, (3 are called orthogonal with respect to the inner product f \nif f(a, (3) \nO. The quadratic form q(a) \nf(a, a) takes only non-negative \nvalues, and q(a) is usually thought of as the square of the length of a. Of \ncourse, these concepts of length and orthogonality stem from the most. \nimportant example of an inner product-the dot product of Example 5. \nIf f is any symmetric bilinear form on a vector space V, it is con­\nvenient to apply some of the terminology of inner products to f. It is \nespecially convenient to say that a and (3 are orthogonal with respect to \nf if f(a, (3) = O. It is not advisable to think of f(a, a) as the square of the \nlength of a; for example, if V is a complex vector space, we may have \nf(a, a) \n= v=t, or on a real vector space, f(a, a) = \n-2. \nWe turn now to the basic theorem of this section. In reading the \nSec. 10.2 \nSymmetric Bilinear Forms \nproof, the reader should find it helpful to think of the special case in \nwhich V is a real vector space and j is an inner product on V. \nTheorem 3. Let V be a finite-dimensional vector space over a field",
    "Sec. 10.2 \nSymmetric Bilinear Forms \nproof, the reader should find it helpful to think of the special case in \nwhich V is a real vector space and j is an inner product on V. \nTheorem 3. Let V be a finite-dimensional vector space over a field \nof characteristic zero, and let f be a symmetric bilinear form on V. Then there \nis an ordered basis jor Y in which f is represented by a diagonal matrix. \nProoj. What we must find is an ordered basis \nil \n= {al, . . . , all} \nsuch that j(a;, aj) \n= 0 for i .,:E j. If j = 0 or n = 1, the theorem is obvi­\nously true. Thus we may suppose j .,:E 0 and n > l. If j(a, a) \n0 for \nevery a in V, the associated quadratic form q is identically 0, and the \npolarization identity (10-5) shows that f = O. Thus there is a vector a in \nV such that j(a, a) \n= q(a) .,:E O. Let W be the one-dimensional subspace \nof V which is spanned by a, and let H'.1. be the set of all vectors f3 in V \nsuch that f(a, (3) \nO. Now we claim that V = W EB  W.1.. Certainly the \nsubspaces W and W.1. are independent. A typical vector in W is ca, where c is \na scalar. If Ca is also in W.L, thenj(ca, ca) \nc2j(a, a) \nO. Butj(a, a) 'jIfi. 0, \nthus c = O. Also, each vector in V is the sum of a vector in W and a vector \nin W.L. For, let 'Y be any vector in V, and put \nThen \nf3 \n'Y \nf( 'Y, a) \n-·¦- a \nf(a, a) . \nlea, (3) \n= lea, 'Y) -j:;: f(a, a) \nand since f is symmetric, j(a, (3) = O. Thus f3 is in the subspace W.1.. The \nexpression \n'Y \nshows us that V \nW + W.L. \nThe restriction of f to W.L is a symmetric bilinear form on W.1.. Since \nW.L has dimension (n - 1), we may assume by induction that W.L has a \nbasis {a2' . . .  , an} such that \nPutting a!1 \nfor i 'jIfi. j. \nI \nf(a;, aj) \n= 0, \ni .,:E j (i 2 2, j 2 2). \na!, we obtain a basis {ai, . . .  , an} for V such thatj(a;, aj) \no \nCorollary. Let F be a subfield of the complex numbers, and let A be a \nsymmetric n X n matrix over F. 'Then there is an invertible n X n matrix \nP aver F such that PtAP is diagonal.",
    "I \nf(a;, aj) \n= 0, \ni .,:E j (i 2 2, j 2 2). \na!, we obtain a basis {ai, . . .  , an} for V such thatj(a;, aj) \no \nCorollary. Let F be a subfield of the complex numbers, and let A be a \nsymmetric n X n matrix over F. 'Then there is an invertible n X n matrix \nP aver F such that PtAP is diagonal. \nIn case F is the field of real numbers, the invertible matrix P in this \ncorollary can be chosen to be an orthogonal matrix, i.e., pt = P-I. In \n369 \n370 \nBilinear Forms \nChap. 10 \nother words, if A is a real symmetric n X n matrix, there is a real or­\nthogonal matrix P such that ptAP is diagonal; however, this is not at all \napparent from what we did ahove (see Chapter 8). \nTheorem 4. Let V be a finite-dimensional vector space over the field of \ncomplex numbers. Let f be a symmetric bilinear form on V which has rank r. \nThen there is an ordered basis (B = {/3I, . . .  , /3n} for V such that \n(i) the matrix of f in the ordered basis (B is diagonal; \n(ii) f(/3j, /3j) = {0\n1\n, ' >\n= 1, .\n. . \n, l' \n,\nJ\n r. \nProof. By Theorem 3, there is an ordered basis {al, . . .  , an} \nfor V such that \nf(ai, aj) = ° for i Y j. \nSince f has rank 1', so does its matrix in the ordered basis {aI, .\n. . , an} .  \nThus we must have f(ah ai) Y ° for precisely l' values of i By reordering \nthe vectors aj, we may assume that \nf(ah aj) 7'\" 0, \nj = 1, \n. .\n. , r. \nNow we use the fact that the scalar field is the field of complex numbers. \nIf \naj) denotes any complex square root of f(ah aJ, and if we put \n{v 1 \naj, j = 1, .\n. . , r \n/3j \n= \nf(ah aj) \naj, j > l' \nthe basis {{:h, . . .  , /3n} satisfies conditions (i) and (ii). \nI \nOf course, Theorem 4 is valid if the scalar field is any subfield of the \ncomplex numbers in which each element has a square root. It is not valid, \nfor example, when the scalar field is the field of real numbers. Over the \nfield of real numbers, we have the following substitute for Theorem 4. \nTheorem 5. Let V be an n-dimensional vector space over the fi·eld of",
    "complex numbers in which each element has a square root. It is not valid, \nfor example, when the scalar field is the field of real numbers. Over the \nfield of real numbers, we have the following substitute for Theorem 4. \nTheorem 5. Let V be an n-dimensional vector space over the fi·eld of \nreal n1llnbers, and let f be a symmetric bilinear form on V which has rank r. \nThen there is an ordered basis {/3I, /32, . . .  , /3n} for V 1:n which the matrix of \nf is diagonal and such that \nf(/3j, /3j) = ±1, \n1, . . .  , r. \nFurthermore, the number of basis vectors {3j for which f(/3j, (3j) \n1 is inde­\npendent of the choice of basis. \nProof. There is a basis {a!, . . .  , an} for V such that \nf(ai, aj) \n= 0, \ni Y j \nf(ah aj) Y 0, \n1 ყ j ყ l' \nf(aj, aj) \n0, \nj > r. \nSec. 10.2 \nSymmetric Bilinear Forms \nLet \n(3i = IfCaj, ai) 1-1/2aj, \n{3j = ai> \nj > r. \nThen {{31, . . .  , {3n} is a basis with the stated properties. \nLet p be the number of basis vectors {3j for which fC{3j, (3j) \n= 1 ;  we \nmust show that the number p is independent of the particular basis we \nhave, satisfying the stated conditions. Let Y+ be the subspace of Y \nspanned by the basis vectors {3i for which f((3j, (3j) \n= 1, and let Y- be the \nsubspace spanned by the basis vectors (3j for which f({3j, (3j) = - 1. Now \np = dim Y+, so it is the uniqueness of the dimension of Y+ which we \nmust demonstrate. It is easy to see that if a is a non-zero vector in Y+, \nthen f(a, a) > 0; in other words, f is positive definite on the subspace Y+. \nSimilarly, if a is a non-zero vector in Y-, thenf(a, a) < 0, i.e.,! is negative \ndefinite on the subspace Y-. Now let y1- be the subspace spanned by the \nbasis vectors (3j for which fC{3h (3j) = O. If a is in Y 1-, then fCa, (3) = 0 for \nall {3 in Y. \nSince {{3l, . .\n. , {3n} is a basis for Y, we have \nY = Y+ Efj Y- Efj y1-. \nFurthermore, we claim that if W is any subspace of Y on which f is posi­\ntive definite, then the subspaces W, Y-, and y1- are independent. For,",
    "all {3 in Y. \nSince {{3l, . .\n. , {3n} is a basis for Y, we have \nY = Y+ Efj Y- Efj y1-. \nFurthermore, we claim that if W is any subspace of Y on which f is posi­\ntive definite, then the subspaces W, Y-, and y1- are independent. For, \nsuppose a is in W, {3 is in Y-, 'Y is in y1-, and a + (3 + 'Y = O. Then \no = f(a, a + {3 + 'Y) \n= fea, a) + fea, (3) + fea, 'Y) \no = f((3, a + (3 + 'Y) = fC{3, a) + fC{3, (3) + f({3, 'Y). \nSince 'Y is in Y 1- ,  f(a, 'Y) = f({3, 'Y) \n= 0; and since f is symmetric, we obtain \no = f(a, a) + f(a, {3) \no = fC{3, (3) + fCa, (3) \nhence f(a, a) = f({3, {3). Since f(a, a) 2 0 and f({3, (3) ::; 0, it follows that \nf(a, a) \n= fC{3, (3) \n= O. \nBut f is positive definite on W and negative definite on V-. We conclude \nthat a = {3 = 0, and hence that 'Y = 0 as well. \nSince \nY = Y+ Efj Y- EB yl. \nand W, V-, V1- are independent, we see that dim W ::; dim Y+. That is, \nif W is any subspace of V on which f is positive definite, the dimension \nof W cannot exceed the dimension of V+. If (£h is another ordered basis \nfor V which satisfies the conditions of the theorem, we shall have corre­\nsponding subspaces Yt, YI , and Vi; and, the argument above shows \nthat dim vt ::; dim Y+. Reversing the argument, we obtain dim Y+ ::; \ndim Vi, and consequently \ndim Y+ = dim Vi. I \n371 \n372 \nBilinear Forms \nChap. 10 \nThere are several comments we should make about the basis \n{!31, . . . , !3n} of Theorem 5 and the associated subspaces V+, V-, and Vol \nFirst, note that V.L is exactly the subspace of vectors which are 'orthogonal' \nto all of V. We noted above that V.L is contained in this subspace; but, \ndim V.L \ndim V - (dim V+ + dim V-) \ndim V - rankf \nso every vector a such that f( a, (3) = 0 for all f3 must be in V.L. Thus, the \nsubspace V.L is unique. The subspaces V+ and V- are not unique; however, \ntheir dimensions are unique. The proof of Theorem 5 shows us that dim \nV+ is the largest possible dimension of any subspace on whichf is positive",
    "subspace V.L is unique. The subspaces V+ and V- are not unique; however, \ntheir dimensions are unique. The proof of Theorem 5 shows us that dim \nV+ is the largest possible dimension of any subspace on whichf is positive \ndefinite. Similarly, dim V- is the largest dimension of any subspace on \nwhich f is negative definite. Of course \ndim V+ + dim V- = rankf. \nThe number \ndim V+ - dim V-\nis often called the signature of f. It is introduced because the dimensions \nof V+ and V- are easily determined from the rank of f and the signature \nof f. \nPerhaps we should make one final comment about the relation of \nsymmetric bilinear forms on real vector spaces to inner products. Suppose \nV is a finite-dimensional real vector space and that VI, V2, V3 are sub­\nspaces of V such that \nSuppose that f1 is an inner product on VI, and f2 is an inner product on V2. \nWe can then define a symmetric bilinear form f on V as follows: If a, (3 \nare vectors in V, then we can write \na = Ɗ + Ƌ + ƌ  ცd (3 = ƍ + Ǝ + Ə  \nwith ai and (3i in Vi' Let \nf(a, (3) \nfl(a1, (31) \nf2(a2, (32). \nThe subspace V.L for f will be Va, VI is a suitable V+ for f, and V2 is a \nsuitable V-. One part of the statement of Theorem 5 is that every sym­\nmetric bilinear form on V arises in this way. The additional content of \nthe theorem is that an inner product is represented in some ordered basis \nby the identity matrix. \nExercises \n1. The following expressions define quadratic forms q on R2. Find the symmetric \nbilinear form f corresponding to each q. \nSec. 10.2 \nSymmetric Bilinear Forms \n(a) axi. \n(b) bXIX2. \n(c) cxY. \n(e) xi + 9xY. \n(f) 3XIX2 - xZ. \n(g) 4xi + 6XIX2 - 3xY. \n(d) 2xI - lXlxz. \n2. Find the matrix, in the standard ordered basis, and the rank of each of the \nbilinear forms determined in Exercise 1 .  Indicate which forms are non-degenerate. \n3. Let q(XI, X2) = ax! + bX\\X2 + cx[ be the quadratic form associated with a \nsymmetric bilinear form f on R2. Show that f is non-degenerate if and only if",
    "bilinear forms determined in Exercise 1 .  Indicate which forms are non-degenerate. \n3. Let q(XI, X2) = ax! + bX\\X2 + cx[ be the quadratic form associated with a \nsymmetric bilinear form f on R2. Show that f is non-degenerate if and only if \nb2 - 4ac ;P O. \n4. Let V be a finite-dimensional vector space over a subfield F of the complex \nnumbers, and let S be the set of all symmetric bilinear forms on V. \n(a) Show that S is a subspace of L(V, V, F). \n(b) Find dim S. \nLet Q be the set of all quadratic forms on V. \n(c) Show that Q is a subspace of the space of all functions from V into F. \n(d) Describe explicitly an isomorphism T of Q onto S, without reference to \na basis. \n(e) Let U be a linear operator on V and q an element of Q. Show that the \nequation (Utq)(a) = q(Ua) defines a quadratic form Utq on V. \n(f) If U is a linear operator on V, show that the function ut defined in part \n(e) is a linear operator on Q. Show that ut is invertible if and only if U is invertible. \n5. Let q be the quadratic form on R2 given by \nq(Xl, xz) \n= ax\\ + 2bxlxz + cxY, \nFind an invertible linear operator U on RZ such that \na ;p  O. \n(Utq) (XI, xz) \n= ax] + (c \n-Ǚ) xZ. \n(Hint: To find U-I (and hence U), complete the square. For the definition of ut, \nsee part (e) of Exercise 4.) \n6. Let q be the quadratic form on RZ given by \nq(xJ, X2) \n= 2bxIXZ. \nFind an invertible linear operator U on RZ such that \n( utq) (xJ, xz) = 2bxi - 2bxY. \n7. Let q be the quadratic form on R3 given by \nq(xJ, Xz, xa) = XJX2 + 2xJxa + x^. \nFind an invertible linear operator U on R3 such that \n(utq) (xJ, Xz, X3) = xi - xY + x^. \n(Hint: Express U as a product of operators similar to those used in Exercises 5 \nand 6.) \n373 \n374 \nBilinear Forms \nChap. 10 \n8. Let A be a symmetric n X n matrix over R, and let q be the quadratic form \non Rn given by \n'.J \nGeneralize the method used in Exercise 7 to show that there is an invertible linear \noperator U on Rn such that \nwhere Ci is 1, - 1, or 0, i = 1, . . . , n.",
    "373 \n374 \nBilinear Forms \nChap. 10 \n8. Let A be a symmetric n X n matrix over R, and let q be the quadratic form \non Rn given by \n'.J \nGeneralize the method used in Exercise 7 to show that there is an invertible linear \noperator U on Rn such that \nwhere Ci is 1, - 1, or 0, i = 1, . . . , n. \n9. Let f be a symmetric bilinear form on Rn. Use the result of Exercise 8 to prove \nthe existence of an ordered basis CB such that [fJm is diagonal. \n10. Let V be the real vector space of all 2 X 2 (complex) Hermitian matrices, \nthat is, 2 X 2 complex matrices A which satisfy Aij = Aii• \n(a) Show that the equation q(A) = det A defines a quadratic form q on V. \n(b) Let W be the sub̙pace of V of matrices of trace O. Show that the bilinear \nform f determined by q i\" negative definite on the ̚ubspace W. \nn. Let V be a finite̛dimensional vector space and f a non̜degenerate symmetric \nbilinear form on V. Show that for each linear operator T on V there is a unique \nlinear operator T' on V such that f(Ta, {3) = f(a, T'{3) for all a, {3 in V. Also \nshow that \n(TIT2)' = T;T( \n(CI']'! + c2T2)' \ncS'; + c2T; \n(T')' \nT. \nHow much of the above is valid without the assumption that T is non-degenerate? \n12. Let F be a field and V the space of n X 1 matrices over F. Suppose A is a \nfixed n X n matrix over F and f is the bilinear form on V defined by f(X, Y) = \nX/A Y. Suppose f is symmetric and non-degenerate. Let B be an n X n matrix \nover F and T the linear operator on V sending X into BX. Find the operator T' \nof Exercise 11. \n13. Let V be a finite-dimensional vector space and f a non-degenerate symmetric \nbilinear form on V. Associated with f is a 'natural' isomorphism of V onto the \ndual Rpace V*, this isomorphism being the transformation Lj of Section 10.1. \nUsing Lh show that for each basis CB \"\" {all ' . . , an} of V there exists a unique \nbasis CB' = {ai, . . .  , a<} of V such that f(a;, a;) = O;j. Then show that for every \nvector a in V we have \na = 2; f(a, aDa; = 2; f(a;, a)a:. \ni \ni",
    "Using Lh show that for each basis CB \"\" {all ' . . , an} of V there exists a unique \nbasis CB' = {ai, . . .  , a<} of V such that f(a;, a;) = O;j. Then show that for every \nvector a in V we have \na = 2; f(a, aDa; = 2; f(a;, a)a:. \ni \ni \n14. Let V, f, CB, and CB' be as in Exercise 13. Suppose T is a linear operator on V \nand that T' is the operator which f associates with T as in Exercise 11. Show that \n(a) [T'](l\\' = [TJє. \n(b) tr (T) = tr ('1\" ) = }; f(Ta;, aD. \ni \n15. Let V, f, CB, and CB' be as in Exercise 13. Suppose [f]m = A. Show that \na: = 2: (A -l);,ai = 2; (A -l)iia,. \nj \ni \nSec. 10.3 \nSkew-Symmetric Bilinear Forms \n16. Let P be a field and V the space of n X 1 matrices over P. Suppose A is an \ninvertible, symmetric 11, X n matrix over P and that I is the bilinear form on V \ndefined by I(X, Y) = XtA Y. Let P be an invertible 11, X n matrix over P and il \nthe basis for V consisting of the columns of P. Show that the basis il' of Exercise 13 \nconsists of the columns of the matrix A -l(pt)-l. \n17. Let V be a finite-dimensional vector space over a field P and I a symmetric \nbilinear form on V. For each subspace W of V, let W.L be the set of all vectors a \nin V such that I(a, (3) \n0 for every {3 in W. Show that \n(a) W.L is a subspace. \n(b) V = {O}.L. \n(c) P = {O} if and only if I is non-degenerate. \n(d) rank I = dim V - dim y.L. \n(e) If dim Y = 11, and dim W = m, then dim W.L ::: 11, - m. (Hint: Let \n{{31, .\n.\n•\n , 13m} be a basis of W and consider the mapping \na ---t (f(a, (31), .\n•\n.\n , /(a, 13m)) \nof V into pm,) \n(f) The restriction of I to W is non-degenerate if and only if \nW n W.L \n{O} . \n(g) Y \nW \nW.L if and only if the restriction of I to W is non-degenerate. \n18. Let Y be a finite-dimensional vector space over C and I a non-degenerate \nsymmetric bilinear form on Y. Prove that there is a basis il of V such that il' = il. \n(Se.- Exercise 13 for a definition of il'.) \n375 \n10.3. Skew-Symmetric Bilinear Forms",
    "18. Let Y be a finite-dimensional vector space over C and I a non-degenerate \nsymmetric bilinear form on Y. Prove that there is a basis il of V such that il' = il. \n(Se.- Exercise 13 for a definition of il'.) \n375 \n10.3. Skew-Symmetric Bilinear Forms \nThroughout this section V will be a vector space over a subfield F \nof the field of complex numbers. A bilinear form f on V is called skew­\nsymmetric if f(a, {3) \n-f({3, a) for all vectors a, {3 in V. We shall prove \none theorem concerning the simplification of the matrix of a skew­\nsymmetric bilinear form on a finite-dimensional space V. First, let us \nmake some general observations. \nSuppose f is any bilinear form on V. If we let \ng(a, {3) \nHf(a, {3) + f({3, a)] \nh(a, {3) = Hf(a, {3) - f({3, a)] \nthen it is easy to verify that g is a symmetric bilinear form on V and h is \na skew-symmetric bilinear form on V. Also f = g + h. Furthermore, this \nexpression for V as the sum of a symmetric and a skew-symmetric form \nis unique. Thus, the space L( V, V, F) is the direct sum of the subspace \nof symmetric forms and the subspace of skew-symmetric forms. \nIf V is finite-dimensional, the bilinear form f is skew-symmetric if \nand only if its matrix A in some (or every) ordered basis is skew-symmetric, \nA t  = -A. This is proved just as one proves the corresponding fact about \n376 \nBilinear Forms \nChap. 10 \nsymmetric bilinear forms. When f is skew-symmetric, the matrix of f in \nany ordered basis will have all its diagonal entries 0. This just corresponds \nto the observation that f(a, a) \n° for every a in V, since f(a, a) = \n-f(a, a). \nLet us suppose f is a non-zero skew-symmetric bilinear form on V. \nSince f rf 0, there are vectors a, (3 in V such that fea, (3) rf 0. Multiplying \na by a suitable scalar, we may assume thatf(a, (3) \n1. Let 'Y be any vector \nin the subspace spanned by a and {3, say 'Y \nea + d{3. Then \nand so \n(10-7) \nfC'Y, a) = f(ea + d{3, a) = dfC{3, a) = -d \nf('Y, (3) = f(ea + d(3, (3) = ef(a, (3) = e",
    "a by a suitable scalar, we may assume thatf(a, (3) \n1. Let 'Y be any vector \nin the subspace spanned by a and {3, say 'Y \nea + d{3. Then \nand so \n(10-7) \nfC'Y, a) = f(ea + d{3, a) = dfC{3, a) = -d \nf('Y, (3) = f(ea + d(3, (3) = ef(a, (3) = e \n'Y = f( 'Y, (3)a - f( 'Y, a){3. \nIn particular, note that a and {3 are necessarily linearly independent; for, \nif 'Y \n0, then fC'Y, a) = fC'Y, (3) = 0. \nl .. et W be the two-dimensional subspace spanned by a and {3. Let W l. \nbe the set of all vectors 0 in V such that f(o, a) \nf(o, (3) = 0, that is, the \nset of all 0 such that f( 0, 'Y) \n° for every 'Y in the subspace W. We claim \nthat V = W EB W.l.. For, let E be any vector in V, and \n'Y = f(E, (3)a - f(E, a){3 \no = E - 'Y. \nThen 'Y is in W, and 0 is in Wl., for \nf(o, a) = feE - f(E, (3)a + f(E, a){3, a) \nfee, a) \nfCE, a)f({3, a) \n= 0  \nand similarly f( 0, (3) = O. Thus every E in V is of the form e = 'Y + 0, \nwith 'Y in W and 0 in W.l.. From (9-7) it is clear that W n W.l. = {O} , and \nso V \nW EB Wl.. \nN ow the restriction of f to W.l. is a skew-symmetric bilinear form on \nW.l.. This restriction may be the zero form. If it is not, there are vectors \na' and (3' in W.l. such that f( ai, (3') \n1. If we let W' be the two-dimensional \nsubspace spanned by a' and (3', then we shall have \nV \nW EB W' EB Wo \nwhere Wo is the set of all vectors 0 in Wl. such that f(a', 0) \nf({3', o) = 0. \nIf the restriction of f to Wo is not the zero form, we may select vectors \na\", {3\" in Wo such that f(a\", (3\") = 1, and continue. \nIn the finite-dimensional case it should be clear that we obtain a \nfinite sequence of pairs of vectors, \nCal, (31), (a2, (32), . . .  , (ak, (3k) \nwith the following properties: \nSec. 10.3 \nSkew-Symmetric Bilinear Forms \n(a) f(ah (3j) \n1, j = 1, .\n. . , k. \n(b) f(a;, aj) = f({3;, (3j) = f(a;, (3j) = 0, i ჯ j. \n(c) If Wj is the two-dimensional subspace spanned by aj and {3;, then \nV = WI EtJ . . .  EtJ Wk EtJ Wo",
    "with the following properties: \nSec. 10.3 \nSkew-Symmetric Bilinear Forms \n(a) f(ah (3j) \n1, j = 1, .\n. . , k. \n(b) f(a;, aj) = f({3;, (3j) = f(a;, (3j) = 0, i ჯ j. \n(c) If Wj is the two-dimensional subspace spanned by aj and {3;, then \nV = WI EtJ . . .  EtJ Wk EtJ Wo \nwhere every vector in Wo is 'orthogonal' to all aj and {3;, and the restric­\ntion of f to Wo is the zcro form. \nTheorem 6. Let V be an n-dimensional vector space over a subfield of \nthe complex numbers, and let f be a skew-symmetric bilinear form on V. Then \nthe rank r of f is even, and if r \n2k there is an ordered basis for V in which \nthe matrix of f is the direct sum of the (n - r) X (n - r) zero matrix and \nk copies of the 2 X 2 matrix \nProof. Let aI, {31, . . . , ak, (3k be vectors satisfying conditions (a), \n(b), and (c) above. Let hI, . . .  , 'Ys} be any ordered basis for the subspace \nWoo Then \nis an ordered basis for V. From (a), (b), and (c) it is clear that the matrix \nof f in the ordered basis il is the direct sum of the (n \n2k) X (n \n2k) \nzero matrix and k copies of the 2 X 2 matrix \n(10-8) \nFurthermore, it is clear that the rank of this matrix, and hence the rank \nof f, is 2k. \nI \nOne consequence of the above is that if f is a non-degenerate, skew­\nsymmetric bilinear form on V, then the dimension of V must be even. If \ndim V = 2k, there will be an ordered basis {aI, (31, . . .  , ak, (3k} for V such \nthat \n{O, i ჯ j \nf(a;, (3j) = \n1 \n. \n. \n, \n1, = J \nf(ai' aj) = f«(3i, (3j) = 0. \nThe matrix of f in this ordered basis is the direct sum of k copies of the \n2 X 2 skew-symmetric matrix (10-8). We obtain another standard form \nfor the matrix of a non-degenerate skew-symmetric form if, instead of the \nordered basis above, we consider the ordered basis \n{al, \"\n\"\n ak, (3k, . . . , (31} ' \n377 \n378 \nBilinear Forms \nChap. 10 \nThe reader should find it easy to verify that the matrix of f in the latter \nordered basis has the block form \nwhere .J is the k X k matrix \nExercises",
    "ordered basis above, we consider the ordered basis \n{al, \"\n\"\n ak, (3k, . . . , (31} ' \n377 \n378 \nBilinear Forms \nChap. 10 \nThe reader should find it easy to verify that the matrix of f in the latter \nordered basis has the block form \nwhere .J is the k X k matrix \nExercises \n1. Let V be a vector space over a field F. Show that the set of all skew-symmetric \nbilinear forms on V is a subspace of L(V, V, F). \n2. Find all skew-symmetric bilinear forms on R3. \n3. Find a basis for the space of all skew-symmetric bilinear forms on Rn. \n4. Let f be a symmetric bilinear form on Cn and g a skew-symmetric bilinear \nform on Cn. Suppose f + g \nO. Show that f \ng \nO. \n5. Let V be an n-dimensional vector space over a subfield F of C. Prove the \nfollowing. \n(a) The equation (Pf) (a, (3) = !I(a, (3) - !I({3, a) defines a linear operator P \non L(V, V, F). \n(b) p2 = P, i.e., P is a projection. \n( ) \nk P \nn(n \n1) \nll't P \nn(n + 1) \nc ran \n= \n2 \n; nu 1 y \n= \n2 \n. \n(d) If U is a linear operato on V, the equation (Utj) (a, (3) = f(Ua, U(3) \ndefines a linear operator ut on L(V, V, F). \n(e) For every linear operator U, the projection P eommutes with ut. \n6. Prove an analogue of Exereise 11 in Section 10.2 for non-degenerate, skew­\nsymmetric bilinear forms. \n7. Let f be a bilinear form on a vector space V. Let Lf and Rj be the mappings of \nV into V* associated with f in Section 10.1. Prove that f is skew-symmetric if and \nonly if Lf = \n-Rf• \n8. Prove an analogue of Exercise 17 in Section 10.2 for skew-symmetric forms. \n9. Let V be a finite-dimensional vector space and L1, L2 linear functionals on V. \nShow that the equation \nlea, (3) = LM)L2({3) \nL1({3)L2(a) \ndefines a skew-symmetric bilinear form on V. Show that f \n0 if and only if L1, L2 \nare linearly dependent. \n10. Let V be a finite-dimensional vector space over a subfield of the complex \nnumbers and f a skew-symmetric bilinear form on V. Show hat f has rank 2 if \nSec. lOA \nGroups Preserving Bilinear Forms",
    "0 if and only if L1, L2 \nare linearly dependent. \n10. Let V be a finite-dimensional vector space over a subfield of the complex \nnumbers and f a skew-symmetric bilinear form on V. Show hat f has rank 2 if \nSec. lOA \nGroups Preserving Bilinear Forms \nand only if there exist linearly independent linear functionals Lt, L2 on V such that \nn. Let I be any skew-symmetric bilinear form on R3. Prove that there are linear \nfunctionals Ll, L2 such that \nI(a, (3) = Ll(a)L2({3) - Lt({3)L2(a). \n12. Let V be a finite-dimensional vector space over a subfield of the complex \nnumbers, and let f, g be skew-symmetric bilinear forms on V. Show that there is \nan invertible linear operator T on V such that f(Ta, T(3) = g(a, (3) for all a, {3 \nif and only if I and g have the same rank. \n13. Show that the result of Exercise 12 is valid for symmetric bilinear forms on a \ncomplex vector space, but is not valid for symmetric bilinear forms on a real vector \nspace. \n379 \n10.4. Groups Preserving Bilinear Forms \nLet f be a bilinear form on the vector space V, and let T be a linear \noperator on V. We say that T preserves f if f(Ta, T(3) \nf(a, (3) for \nall a, {3 in V. For any T andfthe function g, defined by g(a, (3) = f(Ta, T(3), \nis easily seen to be a bilinear form on V. To say that T preservesf is simply \nto say g \nf. The identity operator preserves every bilinear form. If S \nand T are linear operators which preservef, the product ST also preserves \nf; for feSTa, ST(3) \n= f(Ta, T(3) = f(a, (3). In other words, the collection \nof linear operators which preserve a given bilinear form is closed under \nthe formation of (operator) products. In general, one cannot say much \nmore about this collection of operators; however, if f is non-degenerate, \nwe have the following. \nTheorem 7. Let f be a non-degenerate bilinear form on a finite­\ndimensional vector space V. The set of all linear operators on V which preserve \nf is a group 1).nder the operation of composition.",
    "more about this collection of operators; however, if f is non-degenerate, \nwe have the following. \nTheorem 7. Let f be a non-degenerate bilinear form on a finite­\ndimensional vector space V. The set of all linear operators on V which preserve \nf is a group 1).nder the operation of composition. \nProof. Let G be the set of linear operators preserving f. We \nobs.erved that the identity operator is in G and that whenever S and T \nare in G the composition ST is also in G. From the fact that f is non­\ndegenerate, we shall prove that any operator T in G is invertible, and \nT-t is also in G. Suppose T preservesf. Let a be a vector in the null space \nof T. Then for any (3 in V we have \nf(a, (3) \n= f(Ta, T(3) \n= f(O, T(3) \n= O. \nSince f is non-degenerate, a = O. Thus T is invertible. Clearly T-l also \npreserves f; for \nf(T-la, T-l(3) \n= f(TT-la, TT-l(3) = f(a, (3). I \n380 \nBilinear Forms \nChap. 10 \nIf f is a non-degenerate bilinear form on the finite-dimensional space \nV, then each ordered basis <B for V determines a group of matrices \n'preserving' f. The set of all matrices [T]m, where T is a linear operator \npreserving f, will be a group under matrix multiplication. There is an \nalternative description of this group of matrices, as follows. Let A \n[f](Il, \nso that if a and (3 are vectors in V with respective coordinate matrices X \nand Y relative to il, we shall have \nI(a, (3) \n= XIA Y. \nLet T be any linear operator on V and M = [T](Il. Then \nf(Ta, T(3) \n= (MX)IA (MY) \nX'(MIAM) Y. \nAccordingly, T preserves f if and only if MtAM = A. In matrix language \nthen, Theorem 7 says the following: If A is an invertible n X n matrix, \nthe set of all n X n matrices M such that MtAM = A is a group under \nmatrix multiplication. If A \n[fJ(\\, then M is in this group of matrices if \nand only if M = [T]m, where T is a linear operator which preserves f. \nBefore turning to some examples, let us make one further remark. \nSuppose f is a bilinear form which is symmetric. A linear operator T pre­",
    "matrix multiplication. If A \n[fJ(\\, then M is in this group of matrices if \nand only if M = [T]m, where T is a linear operator which preserves f. \nBefore turning to some examples, let us make one further remark. \nSuppose f is a bilinear form which is symmetric. A linear operator T pre­\nserves f if and only if T preserves the quadratic form \nq(a) \n= f(a, a) \nassociated with f. If T preserves f, we certainly have \nq(Ta) \n= I(Ta, Ta) \n= I(a, a) \n= q(a) \nfor every a in V. Conversely, since f is symmetric, the polarization identity \nf(a, (3) \n= tq(a + (3) - tq(a - (3) \nshows us that T preserves I provided that q(T\"() \n= q(\"() for each \"( in V. \n(We are assuming here that the scalar field is a subfield of the complex \nnumbers.) \nEXAMPLE 6. Let V be either the space Rn or the space en. Let f be \nthe bilinear form \nn \nI(a, (3) \n= { XjYj \nj = l  \nwhere a = (Xl, . . .  , xn) and {3 = (Yl, . . .  , Yn) . The group preserving I is \ncalled the n-dimensional (real or complex) orthogonal group. The \nname 'orthogonal group' is more commonly applied to the associated \ngroup of matrices in the standard ordered basis. Since the matrix of f \nin the standard basis is I, this group consists of the matrices M which \nsatisfy MtM = I. Such a matrix M is called an n X n (real or complex) \northogonal matrix. The two n X n orthogonal groups are usually de-\nSec. 10.4 \nGroups Preserving Bilinear Forms \nnoted O(n, R) and O(n, C) . Of course, the orthogonal group is also the \ngroup which preserves the quadratic form \nq(Xl, . . .  , Xn) = xi + ' \" + x;. \nEXAMPLE 7. Let f be the symmetric bilinear form on Rn with quad­\nratic form \nThen f is non-degenerate and has signature 2p - n. The group of ma­\ntrices preserving a form of this type is called a pseudo-orthogonal group. \nWhen p = n, we obtain the orthogonal group O(n, R) as a particular type \nof pseudo-orthogonal group. For each of the n + 1 values p = 0, 1, 2, . . .  , \nn, we obtain different bilinear forms f; however, for p = k and p = n - k",
    "When p = n, we obtain the orthogonal group O(n, R) as a particular type \nof pseudo-orthogonal group. For each of the n + 1 values p = 0, 1, 2, . . .  , \nn, we obtain different bilinear forms f; however, for p = k and p = n - k \nthe forms are negatives of one another and hence have the same associated \ngroup. Thus, when n is odd, we have (n + 1)/2 pseudo-orthogonal groups \nof n X n matrices, and when n is even, we have (n + 2)/2 such groups. \nTheorem 8. Let V be an n-dimensional vector space over the field of \ncomplex numbers, and let f be a non-degenerate symmetric bilinear form on V. \nThen the group preserving f is isomorphic to the complex orthogonal group \nO(n, C). \nProof. Of course, by an isomorphism between two groups, we \nmean a one-one correspondence between their elements which 'preserves' \nthe group operation. Let G be the group of linear operators on V which \npreserve the bilinear formf. Sincef is both symmetric and non-degenerate, \nTheorem 4 tells us that there is an ordered basis (B for V in which f is \nrepresented by the n X n identity matrix. Therefore, a linear operator T \npreserves f if and only if its matrix in the ordered basis (B is a complex \northogonal matrix. Hence \nT ৊ [Tlো \nis an isomorphism of G onto O(n, C). I \nTheorem 9. Let V be an n-dimensional vector space over the field of \nreal numbers, and let f be a non-degenerate symmetric bilinear form on V. \nThen the group preserving f is isomorphic to an n X n pseudo-orthogonal \ngroup. \nProof. Repeat the proof of Theorem 8, using Theorem 5 instead \nof Theorem 4. I \nEXAMPLE 8. Let f be the symmetric bilinear form on R4 with quad­\nratic form \nq(x, y, z, t) = t2 - X2 - y2 - Z2. \n381 \n382 \nBilinear Form8 \nChap. 10 \nA linear operator T on R4 which preserves this particular bilinear (or \nquadratic) form is called a Lorentz transformation, and the group pre­\nserving f is called the Lorentz group. We should like to give one method \nof describing some Lorentz transformations.",
    "381 \n382 \nBilinear Form8 \nChap. 10 \nA linear operator T on R4 which preserves this particular bilinear (or \nquadratic) form is called a Lorentz transformation, and the group pre­\nserving f is called the Lorentz group. We should like to give one method \nof describing some Lorentz transformations. \nLet H be the real vector space of all 2 X 2 complex matrices A which \nare Hermitian, A \n= A *. It is easy to verify that \ncI>(x, y, z, t) [ t + & \nY + iZ] \ny - ܲz \nt - x  \ndefines an isomorphism cI> of R4 onto the space H. Under this isomorphism, \nthe quadratic form q is carried onto the determinant function, that is \nor \n[ t + x \nY + iZ] \nq(x, y, z, t) = det \n. \nY - 3Z \nt - x  \nq(cc) \ndet cI>(cc). \nThis suggests that we might study Lorentz transformations on R4 by \nstudying linear operators on H which preserve determinants. \nLet M be any complex 2 X 2 matrix and for a Hermitian matrix A \ndefine \nUM(A) \n= MAM*. \nNow MAM* is also Hermitian. From this it is easy to see that UM is a \n(real) linear operator on H. Let us ask when it is true that U M 'preserves' \ndeterminants, i.e., det [UM(A)] \ndet A for eaeh A in H. Since the \ndeterminant of M* is the complex conjugate of the determinant of M, \nwe see that \ndet [UM(A)] = Idet MI2 det A .  \nThus UM preserves determinants exactly when det M has absolute value l. \nSo now let us select any 2 X 2 complex matrix M for which \nIdet MI = 1. Then UM is a linear operator on H which preserves de­\nterminants. Define \nTM \ncI>-IUMcI>. \nSince cI> is an isomorphism, TM is a linear operator on R4. Also, TM is a \nLorentz transformation; for \nq(TMCC) \n= q(cI>-lU,M<I>CC) \n= det (<I>cI>-IUMcI>CC) \ndet (UMcI>cc) \n= det (<I>cc) \n= q(cc) \nand so TM preserves the quadratic form q. \nBy using specific 2 X 2 matrices M, one can use the method above \nto compute specific Lorentz transformations. There are two comments \nwhich we might make here; they are not difficult to verify. \nSec. 10.4 \nGroups Preserving Bilinear Forms",
    "= q(cc) \nand so TM preserves the quadratic form q. \nBy using specific 2 X 2 matrices M, one can use the method above \nto compute specific Lorentz transformations. There are two comments \nwhich we might make here; they are not difficult to verify. \nSec. 10.4 \nGroups Preserving Bilinear Forms \n(1) If MI and 11-[2 are invertible 2 X 2 matrices with complex entries, \nthen U Ml = U M, if and only if M2 is a scalar multiple of MI. Thus, all of \nthe Lorentz transformations exhibited above are obtainable from uni­\nmodular matrices M, that is, from matrices M satisfying det M = 1. If \nMl and M2 are unimodular matrices such that MI Y M2 and MI ჭ -M2' \nthen TMI Y TM,. \n(2) Not every Lorentz transformation is obtainable by the above \nmethod. \nExercises \n1. Let M be a member of the complex orthogonal group, O(n, C). Show that MI, \nJI, and M* = JIt also belong to O(n, C). \n2. Suppose M belongs to O(n, C) and that M' is similar to M. Does M' also \nbelong to O(n, C)? \n3. Let \nwhere M is a member of O(n, C). Show that \n' yf = ' x;. \ni \nj \n4. Let M be an n X n matrix over C with columns MI, M2, •\n•\n•\n , Mn. Show that \nM belongs to O(n, C) if and only if \nMjMk = Ojk. \n5. Let X be an n X 1 matrix over C. Under what conditions does O(n, C) contain \na matrix M whose first column is X? \n6. Find a matrix in 0(3, C) whose first row is (2i, 2i, 3). \n7. Let V be the space of all n X 1 matrices over C and I the bilinear form on V \ngiven by I(X, Y) = Xty. Let M belong to O(n, C). What is the matrix of I in the \nbasis of V consisting of the columns MI, M2, •\n•\n•\n , Mn of M? \n8. Let X be an n X 1 matrix over C such that XtX = 1, and I; be the jth column \nof the identity matrix. Show there is a matrix M in O(n, C) such that MX = h \nIf X has real entries, show there is an M in O(n, R) with the property that MX = h \n9. Let V be the space of all n X 1 matrices over C, A an n X n matrix over C, \nand I the bilineaђ form on V given by I(X, Y) = XtA Y. Show that I is invariant",
    "If X has real entries, show there is an M in O(n, R) with the property that MX = h \n9. Let V be the space of all n X 1 matrices over C, A an n X n matrix over C, \nand I the bilineaђ form on V given by I(X, Y) = XtA Y. Show that I is invariant \nunder O(n, C), i.e., I(MX, MY) = I(X, Y) for all X, Y in V and M in O(n, C), \nif and only if A commutes with each member of O(n, C). \n10. Let 8 be any set of n X n matrices over C and 8' the set of all n X n matrices \nover C which commute with each element of 8. Show that 8' is an algebra over C. \n383 \n384 \nBilinear Forms \nChap. 10 \nn. Let F be a subfield of 0, V a finite-dimensional vector space over F, and f a \nnon-singular bilinear form on V. If T is a linear operator on V preserving f, prove \nthat det T \n±1. \n12. Let F be a subficld of 0, V the space of n X 1 matrices over F, A an invertible \nn X n matrix over F, and f the bilinear form on V given by fCX, Y) \nX'A Y. \nIf M is an n X n matrix over F, show that M preservesf if and only if A-IMtA = \nM-l. \n13. Let g be a non-singular bilinear form on a finite-dimensional vector space V. \nSuppose T is an invertible linear operator on V and that f is the bilinear form \non V given by fCa, (3) = g(a, T(3). If U is a linear operator on V, find necessary \nand sufficient conditions for U to preserve f. \n14. Let T be a linear operator on 02 which preserves the quadratic form x* - xY. \nShow that \n(a) det (T) = ±1. \n(b) If M is the matrix of T in the standard basis, then M22 \n±Mll, M21 \n±M12, MKl - Mi2 = 1. \n(c) If det M = 1, then there is a non-zero eomplex number c such that \nM [c + !  C_l] \n1 \nc \nc \n-\n. \n2\n1\n1\n \nc -\nc + -\nc \nc \n(d) If det M = - 1  then there is a complex number c such that \nM [\nc + l  \nc - !] \n1 \nc \nc \n-\n. \n2\n1\n1\n \n-c + \n-c -\nc \nc \n15. Let f be the bilinear form on 02 defined by \nShow that \nf( (Xl, X2), (YI, Y2)) \nXIY2 \nX2Yl. \n(a) if T is a linear operator on C2, then f(Ta, T(3) \n= (det T)f(a, (3) for all \na, (3 in C2. \n(b) T preserves f if and only if det T = + 1.",
    "M [\nc + l  \nc - !] \n1 \nc \nc \n-\n. \n2\n1\n1\n \n-c + \n-c -\nc \nc \n15. Let f be the bilinear form on 02 defined by \nShow that \nf( (Xl, X2), (YI, Y2)) \nXIY2 \nX2Yl. \n(a) if T is a linear operator on C2, then f(Ta, T(3) \n= (det T)f(a, (3) for all \na, (3 in C2. \n(b) T preserves f if and only if det T = + 1. \n(c) What does (b) say about the group of 2 X 2 matrices M such that \nMtAM \nA where \nA [ 0 I]? \n- 1 0 . \n16. Let n be a positive integer, I the n X n identity matrix over C, and J the \n2n X 2n matrix given by \nLet M be a 2n X 2n matrix over C of the form \nM [Ȍ ȍJ \nSec. 10.4 \nGroups Preserving Bilinear Forms \nwhere A, B, C, D are n X n matrices over C. Find necessary and sufficient con­\nditions on A, B, C, D in order that jy[tJM \nJ. \n17. Find all bilinear forms on the space of n X 1 matrices over R which are in­\nvariant under O(n, R). \n18. Find all bilinear forms on the space of n X 1 matrices over C which are in­\nvariant under O(n, C). \n385 \nAppendix \nThis Appendix separates logically into two parts. The first part, \ncomprising the first three sections, contains certain fundamental concepts \nwhich occur throughout the book (indeed, throughout mathematics). It \nis more in the nature of an introduction for the book than an appendix. \nThe second part is more genuinely an appendix to the text. \nSection 1 contains a discussion of sets, their unions and intersections. \nSection 2 discusses the concept of function, and the related ideas of range, \ndomain, inverse function, and the restriction of a function to a subset of \nits domain. Section 3 treats equivalence relations. The material in these \nthree sections, especially that in Sections 1 and 2, is presented in a rather \nconcise manner. It is treated more as an agreement upon terminology \nthan as a detailed exposition. In a strict logical sense, this material con­\nstitutes a portion of the prerequisites for reading the book; however, the \nreader should not be discouraged if he does not completely grasp the",
    "concise manner. It is treated more as an agreement upon terminology \nthan as a detailed exposition. In a strict logical sense, this material con­\nstitutes a portion of the prerequisites for reading the book; however, the \nreader should not be discouraged if he does not completely grasp the \nsignificance of the ideas on his first reading. These ideas are important, \nbut the reader who is not too familiar with them should find it easier to \nabsorb them if he reviews the discussion from time to time while reading \nthe text proper. \nSections 4 and 5 deal with equivalence relations in the context of \nlinear algebra. Section 4 contains a brief discussion of quotient spaces. \nIt can be read at any time after the first two or three chapters of the book. \nSection 5 takes a look at some of the equivalence relations which arise in \nthe book, attempting to indicate how some of the results in the book might \nbe interpreted from the point of view of equivalence relations. Section 6 \ndescribes the Axiom of choice and its implications for linear algebra. \n386 \nSec. A.I \nSets \n387 \nA.1 . Sets \nWe shall use the words 'set,' 'class,' 'collection,' and 'family' inter­\nchangeably, although we give preference to 'set.' If S is a set and x is \nan object in the set S, we shall say that x is a Ilelllber of S, that x is an \nelelllent of S, that x belongs to S, or simply that x is in S. If S has \nonly a finite number of members, Xl, . . . , Xn, we shall often describe S \nby displaying its members inside braces: \nS = {Xl, •\n•\n•\n , xn} . \nThus, the set S of positive integers from 1 through 5 would be \nS = {I, 2, 3, 4, 5} . \nIf S and T are sets, we say that S is a subset of T, or that S is con­\ntained in T, if each member of S is a member of T. Each set S is a subset \nof itself. If S is a subset of T but S and T are not identical, we call S a \nproper subset of T. In other words, S is a proper subset of T provided \nthat S is contained in T but T is not contained in S.",
    "tained in T, if each member of S is a member of T. Each set S is a subset \nof itself. If S is a subset of T but S and T are not identical, we call S a \nproper subset of T. In other words, S is a proper subset of T provided \nthat S is contained in T but T is not contained in S. \nIf S and T are sets, the union of S and T is the set S U T, consisting \nof all objects X which are members of either S or T. The intersection \nof S and T is the set S n T, consisting of all x which are members of \nboth S and T. For any two sets, S and T, the intersection S n T is a \nsubset of the union S U T. This should help to clarify the use of the word \n'or' which will prevail in this book. When we say that x is either in S or \nin T, we do not preclude the possibility that x is in both S and T. \nIn order that the intersection of S and T should always be a set, it \nis necessary that one introduce the elllpty set, i.e., the set with no mem­\nbers. Then S n T is the empty set if and only if S and T have no members \nII common. \nWe shall frequently need to discuss the union or intersection of several \nn \nsets. If Sl, . . . , Sn are sets, their union is the set U Sj consisting of all \nj = 1  \n.r which are members of at least one of the sets S1> .\n.\n.\n , Sn. Their inter-\nn \nseetion is the set n S;, consisting of all X which are members of each of \n}= 1 \nthe sets S1, . . .  , Sn' On a few occasions, we shall discuss the union or \nintersection of an infinite collection of sets. It should be clear how such \nunions and intersections are defined. The following example should clarify \nthese definitions and a notation for them. \nEXAMPLE 1. Let R denote the set of all real numbers (the real line). \nIf t is in R, we associate with t a subset St of it, defined as follows: St \nconsists of all real numbers x which are not less than t. \n388 \nAppendix \n(a) Stl U SI2 = St, where t is the smaller of tl and t2• \n(b) Stl n SI2 = St, where t is the larger of tl and t2.",
    "If t is in R, we associate with t a subset St of it, defined as follows: St \nconsists of all real numbers x which are not less than t. \n388 \nAppendix \n(a) Stl U SI2 = St, where t is the smaller of tl and t2• \n(b) Stl n SI2 = St, where t is the larger of tl and t2. \n(c) Let I be the unit interval, that is, the set of all t in R satisfying \no :5 t :5 1. Then \nA.2. Functions \nU St = So \nt in I \nA function consists of the following: \n(1) a set X, called the domain of the function; \n(2) a set Y, called the co-domain of the function; \n(3) a rule (or correspondence) j, which associates with each element \nx of X a single element j(x) of Y. \nIf (X, Y, f) is a function, we shall also say f is a function from X \ninto Y. This is a bit sloppy, since it is not f which is the function; f is \nthe rule of the function. However, this use of the same symbol for the \nfunction and its rulc provides one with a much more tractable way of \nspeaking about functions. Thus we shall say that f is a function from X \ninto Y, that X is the domain of f, and that Y is the co-domain of f-all \nthis meaning that (X, Y, f) is a function as defined above. There are \nseveral other words which are commonly used in place of the word 'func­\ntion.' Some of these are 'transformation,' 'operator,' and 'mapping.' \nThese are used in contexts where they seem more suggestive in conveying \nthe role played by a particular function. \nIf f is a function from X into Y, the range (or image) of f is the set \nof all f(x), x in X. In other words, the rarlge of f consists of all elements \ny in Y such that y = j(x) for some x in X. If the range of f is all of Y, \nwe say that f is a function from X onto Y, or simply that f is onto. The \nrange of f is often denoted f(X). \nEXAMPLE 2. (a) Let X be the set of real numbers, and let Y \nX. \nLet f be the function from X into Y defined by f(x) \nx2• The range of \nf is the set of all non-negative real numbers. Thus j is not onto. \n(h) Let X he the Euclidean plane, and Y",
    "range of f is often denoted f(X). \nEXAMPLE 2. (a) Let X be the set of real numbers, and let Y \nX. \nLet f be the function from X into Y defined by f(x) \nx2• The range of \nf is the set of all non-negative real numbers. Thus j is not onto. \n(h) Let X he the Euclidean plane, and Y \nX. Let f be defined as \nfollows: If P is a point in the plane, then f(P) is the point obtained by \nrotating P through 900 (about the origin, in the counterclockwise direc­\ntion). The range of f is all of Y, i.e., the entire plane, and so f is onto. \n(c) Again let X be the Euclidean plane. Coordinatize X as in analytic \ngeometry, using two perpendicular lines to identify the points of X with \nordered pairs of real numbers (Xl, X2). Let Y be the xl-axis, that is, all \nSec. A.2 \nFunctions \npoints (Xl, X2) with X2 = O. If P is a point of X, let f(P) be the point \nobtained by projecting P onto the xl-axis, parallel to the x2-axis. In other \nwords, f((Xl, X2)) = (Xl, 0). The range of f is all of Y, and so f is onto. \n(d) Let X be the set of real numbers, and let Y be the set of positive \nreal numbers. Define a function f from X into Y by f(x) \neX. Then f is \na function from X onto Y. \n(e) Let X be the set of positive real numbers and Y the set of all real \nnumbers. Let f be the natural logarithm function, that is, the function \ndefined by f(x) \nlog X \nIn x. Again f is onto, i.e., every real number \nis the natural logarithm of some positive number. \nSuppose that X, Y, and Z are sets, that f is a function from X into \nY, and that 9 is a function from Y into Z. There is associated withf and 9 \na function g o  f from X into Z, known as the composition of 9 and f. \nIt is defined by \n(g of) (x) \n= g(f(x)). \nFor one simple example, let X = Y = Z, the set of real numbers; let \nf, g, h bc the functions from X into X defined by \nhex) = c\" \nand then h = g o  f. The composition g o  f is often denoted simply gf; \nhowever, as the above simple example shows, there are times when this \nmay lead to confusion.",
    "For one simple example, let X = Y = Z, the set of real numbers; let \nf, g, h bc the functions from X into X defined by \nhex) = c\" \nand then h = g o  f. The composition g o  f is often denoted simply gf; \nhowever, as the above simple example shows, there are times when this \nmay lead to confusion. \nOne question of interest is the following. Suppose f is a function from \nX into Y. When is there a function 9 from Y into X such that g(f(x») = X \nfor each X in X? If we denote by I the identity function on X, that is, \nthe function from X into X defined by lex) = x, we are asking the fol­\nlowing: When is there a function 9 from Y into X such that g o  f = I? \nRoughly speaking, we want a function 9 which 'sends each element of Y \nback where it came from.' In order for such a 9 to exist, f clearly must be \n1 :1, that is, f must have the property that if Xl Y X2 then f(XI) Y f(X2)' \nIf f is 1 :1, such a 9 does exist. It is defined as follows: Let y be an element \nof Y. If y is in the range of f, then there is an element X in X such that \ny = f(x) ; and since f is 1 :1, there is exactly one such x. Define g(y) = x. \nIf y is not in the range of f, define g(y) to be any element of X. Clearly we \nthen have g o  f \nI. \nLet f be a funetion from X into Y. We say that f is invertible if \nthere is a function 9 from Y into X such that \n(1) g o  f is the identity function on X, \n(2) fog is the identity function on Y. \nWe have just seen that if there is a 9 satisfying (1), thenf is 1 :1. Similarly, \none can see that if there is a 9 satisfying (2), the range of f is all of Y, i.e., \nf is onto. Thus, if f is invertible, f is 1 :1 and onto. Conversely, if f is 1 :1 \n389 \n390 \nAppendix \nand onto, there is a function g from Y into X which satisfies (1) and (2). \nFurthermore, this g is unique. It is the function from Y into X defined by \nthis rule: if y is in Y, then g(y) is the one and only element x in X for \nwhich f(x) \ny.",
    "389 \n390 \nAppendix \nand onto, there is a function g from Y into X which satisfies (1) and (2). \nFurthermore, this g is unique. It is the function from Y into X defined by \nthis rule: if y is in Y, then g(y) is the one and only element x in X for \nwhich f(x) \ny. \nIf f is invertible (1 :1 and onto), the inverse of f is the unique function \nf-1 from Y into X satisfying \n(I') f-I(f(X» \n= x, for each x in X, \n(2') f(f-l(y» \ny, for each y in Y. \nEXAMPLE 3. Let us look at the functions in Example 2. \n(a) If X = Y, the set of real numbers, and fCx) = X2, then f is not \ninvertible. For f is neither 1 :1 nor onto. \n(b) If X \nY, the Euclidean plane, and f is 'rotation through 90°,' \nthen f is both 1 :1 and onto. The inverse function j-1 is 'rotation through \n-90°,' or 'rotation through 270°.' \n(c) If X is the plane, Y the xl-axis, and f« XI, X2» \n(Xl, 0), then f is \nnot invertible. For, although f is onto, f is not 1 :1. \nCd) If X is the set of real numbers, Y the set of positive real numbers, \nandf(x) \neX, thenf is invertible. The fUIlctionf-1 is the natural logarithm \nfunction of part (e) : log eX = x, e10g y = y. \n(e) The inverse of this natural logarithm function is the exponential \nfunction of part (d). \nLet 1 be a function from X into Y, and let 10 be a function from Xo \ninto Yo. We call fo a restriction of f (or a restriction of f to Xo) if \n(1) Xo is a subset of X, \n(2) foex) = f(x) for each x in Xo. \nOf course, when fo is a restriction of f, it follows that Yo is a subset of Y. \nThe name 'restriction' comes from the fact that 1 and fo have the same \nrule, and differ chiefly because we have restricted the domain of definition \nof the rule to the subset Xo of X. \nIf we are given the function f and any subset Xo of X, there is an \nobvious way to construct a restriction of f to Xo. We define a function \nfo from Xo into Y by fo(x) \nf(x) for each x in Xo. One might wonder why \nwe do not call this the restriction of f to Xo. The reason is that in dis­",
    "If we are given the function f and any subset Xo of X, there is an \nobvious way to construct a restriction of f to Xo. We define a function \nfo from Xo into Y by fo(x) \nf(x) for each x in Xo. One might wonder why \nwe do not call this the restriction of f to Xo. The reason is that in dis­\ncussing restrictions of f we want the freedom to change the co-domain Y, \nas well as the domain X. \nEXAMPJ,E 4. (a) Let X be the set of real numbers and f the function \nfrom X into X defined by f(x) = x2• Then f is not an invertible function, \nbut it is if we restrict its domain to the non-negative real numbers. Let \nXo be the set of non-negative real numbers, and let fo be the function \nfrom Xo into Xo defined by fo(x) = x2• Then fo is a restriction of f to Xo. \nSec. A.3 \nEquivalence Relations \nNow f is neither 1 :1 nor onto, whereas fo is both 1 :1 and onto. The latter \nstatement simply says that each non-negative number is the square of \nexactly one non-negative number. The inverse function fa 1 is the function \nfrom Xo into Xo defined by fO l(X) \nvi;. \n(b) Let X be the set of real numbers, and let f be the function from \nX into X defined by I(x) = x3 + x2 + 1. The range of f is all of X, and \nso f is onto. The function f is certainly not 1 :1, e.g., f( - 1) = f(O). But \nI is 1 :1 on Xo, the set of non-negative real numbers, because the derivative \nof I is positive for x > O. As x ranges over all non-negative numbers, f(x) \nranges over all real numbers y such that y ჲ 1. If we let Yo be the set of \nall y ჲ 1, and letfo be the function from Xo into Yo defined by fo(x) \n!(x), \nthen fo is a 1 :1 function from Xo onto Yo. Accordingly, fo has an inverse \nfunction fa 1 from Yo onto Xo. Any formula for fo ley) is rather complicated. \n(c) Again let X be the set of real numbers, and let f be the sine func­\ntion, that is, the function from X into X defined by !(x) \nsin x. The \nrange of I is the set of all y such that - 1  ყ Y ყ 1 ;  hence, I is not onto.",
    "(c) Again let X be the set of real numbers, and let f be the sine func­\ntion, that is, the function from X into X defined by !(x) \nsin x. The \nrange of I is the set of all y such that - 1  ყ Y ყ 1 ;  hence, I is not onto. \nSincef(x + 211\") = f(x), we see thatf is not 1 :1. If we let Xo be the interval \n-11\"/2 ყ x ყ 11\"/2, thenf is 1 :1 on Xo. Let Yo be the interval - 1  ყ y ყ 1, \nand let fo be the function from Xo into Yo defined by fo(x) = sin x. Then \nfo is a restriction of I to the interval Xo, and fo is both 1 :1 and onto. This \nis just another way of saying that, on the interval from \n11\"/2 to 11\"/2, \nthe sine function takes each value between - 1  and 1 exactly once. The \nfunction fo I is the inverse sine function: \nfOley) = sin-l y = arc sin y. \n(d) This is a general example of a restriction of a function. It is \nmuch more typical of the type of restriction we shall use in this book \nthan are the examples in (b) and (c) above. The example in (a) is a special \ncase of this one. Let X be a set and f a function from X into itself. Let Xo \nbe a subset of X. We say that Xo is invariant under I if for each x in Xo \nthe element f(x) is in Xo. If Xo is invariant under f, then f induces a func­\ntionfo from Xo into itself, by restricting the domain of its definition to Xo. \nThe importance of invariance is that by restricting f to Xo we can obtain \na function from Xo into itself, rather than simply a function from Xo \ninto X. \n391 \nA.3. Equivalence Relations \nAn equivalence relation is a specific type of relation between pairs \nof elements in a set. To define an equivalence relation, we must first decide \nwhat a 'relation' is. \nCertainly a formal definition of 'relation' ought to encompass such \nfamiliar relations as 'x = v,' 'x < v,' 'x is the mother of v,' and 'x is \n392 \nAppendix \nolder than y.' If X is a set, what does it take to determine a relation be­\ntween pairs of elements of X? What it takes, evidently, is a rule for deter­",
    "familiar relations as 'x = v,' 'x < v,' 'x is the mother of v,' and 'x is \n392 \nAppendix \nolder than y.' If X is a set, what does it take to determine a relation be­\ntween pairs of elements of X? What it takes, evidently, is a rule for deter­\nmining whether, for any two given elements x and y in X, x stands in \nthe given relationship to y or not. Such a rule R, we shall call a (binary) \nrelation on X. If we wish to be slightly more precise, we may proceed \nas follows. Let X X X denote the set of all ordered pairs (x, y) of elements \nof X. A binary relation on X is a function R from X X X into the set \n{O, I} . In other words, R assigns to each ordered pair (x, y) either a 1 or \na 0. The idea is that if R(x, y) \n= 1, then x stands in the given relationship \nto y, and if R(x, y) = 0, it does not. \nIf R is a binary relation on the set X, it is convenient to write xRy \nwhen R(x, y) \n1. A binary relation R is called \n(1) reflexiYe, if xRx for each x in X ;  \n(2) symmetric, if yRx whenever xRy; \n(3) transitive, if xRz whenever xRy and yRz. \nAn equivalenee relation on X is a reflexive, symmetric, and transitive \nbinary relation on X. \nEXAMPLE 5. (a) On any set, equality is an equivalence relation. In \nother words, if xRy means x \ny, then R is an equivalence relation. For, \nx = x, if x = y then y = x, if x = y and y = z then x = z. The relation \nIX 7'- y' is symmetric, but neither reflexive nor transitive. \n(b) Let X be the set of real numbers, and suppose xRy means x < y. \nThen R is not an equivalence relation. It is transitive, but it is neither \nreflexive nor symmetric. The relation IX :s; y' is reflexive and transitive, \nbut not symmetric. \n(c) Let E be the Euclidean plane, and let X be the set of all triangles \nin the plane E. Then congruence is an equivalence relation on X, that is, \n'Tl ,..., T2' (Tl is congruent to T2) is an equivalence relation on the set of \nall triangles in a plane. \n(d) Let X be the set of all integers: \n. . .  , -2, - 1, 0, 1, 2, . . . .",
    "in the plane E. Then congruence is an equivalence relation on X, that is, \n'Tl ,..., T2' (Tl is congruent to T2) is an equivalence relation on the set of \nall triangles in a plane. \n(d) Let X be the set of all integers: \n. . .  , -2, - 1, 0, 1, 2, . . . . \nLet n be a fixed positive integer. Define a relation Rn on X by: xRnY \nif and only if (x \ny) is divisible by n. The relation Rn is called con­\ngruence modulo n. Instead of xRny, one usually writes \nx \ny, mod n \n(x is congruent to y modulo n) \nwhen (x - y) is divisible by n. For each positive integer n, congruence \nmodulo n is an equivalence relation on the set of integers. \n(e) Let X and Y be sets and ! a function from X into Y. We define \na relation R on X by: x1Rx2 if and only if !(Xl) \n!(X2)' It is easy to verify \nthat R is an equivalence relation on the set X. As we shall see, this one \nexample actually encompasses all equivalence relations. \nSec. A.S \nEquivalence Relations \nSuppose R is an equivalence relation on the set X. If x is an element \nof X, we let E(x ;  R) denote the set of all elements y in X such that xRy. \nThis set E(x; R) is called the equivalence class of x (for the equivalence \nrelation R). Since R is an equivalence relation, the equivalence classes \nhave the following properties: \n(1) Each E(x; R) is non-empty; for, since xRx, the element x belongs \nto E(x; R). \n(2) Let x and y be elements of X. Since R is symmetric, y belongs to \nE(x ; R) if and only if x belongs to E(y; R) . \n(3) If x and y are elements of X, the equivalence classes E(x; R) and \nE(y; R) are either identical or they have no members in common. First, \nsuppose xRy. Let z be any element of E(x ;  R) i.e., an element of X such \nthat xRz. Since R is symmetric, we also have zRx. By assumption xRy, \nand because R is transitive, we obtain zRy or yRz. This shows that any \nmember of E(x; R) is a member of E(y; E). By the symmetry of R, we \nlikewise see that any member of E(y; R) is a member of E(x ; R) ; hence",
    "that xRz. Since R is symmetric, we also have zRx. By assumption xRy, \nand because R is transitive, we obtain zRy or yRz. This shows that any \nmember of E(x; R) is a member of E(y; E). By the symmetry of R, we \nlikewise see that any member of E(y; R) is a member of E(x ; R) ; hence \nE(x ; R) = E(y; R). Now we argue that if the relation xRy does not hold, \nthen E(x ; R) n E(y; R) is empty. For, if z is in both these equivalence \nclasses, we have xRz and yRz, thus xRz and zRy, thus xRy. \nIf we let \\1 be the family of equivalence classes for the equivalence \nrelation R, we see that (1) each set in the family \\1 is non-empty, (2) each \nelement x of X belongs to one and only one of the sets in the family \\1, \n(3) xRy if and only if :r and y belong to the same set in the family \\1. \nBriefly, the equivalence relation R subdivides X into the union of a family \nof non-overlapping (non-empty) subsets. The argument also goes in the \nother direction. Suppose \\1 is any family of subsets of X which satisfies \nconditions (1) and (2) immediately above. If we define a relation R by (3), \nthen R is an equivalence relation on X and \\1 is the family of equivalence \nclasses for R. \nEXAMPLE 6. Let us see what the equivalence classes are for the \nequivalence relations in Example 5. \n(a) If R is equality on the set X, then the equivalence class of the \nelement x is simply the set {x} , whose only member is x. \n(b) If X is the set of all triangles in a plane, and R is the congruence \nrelation, about all one can say at the outset is that the equivalence class \nof the triangle T consists of all triangles which are congruent to T. One of \nthe tasks of plane geome1jry is to give other descriptions of these equivalence \nclasses. \n(c) If X is the set of integers and Rn is the relation 'congruence \nmodulo n,' then there are precisely n equivalence classes. Each integer \nx is uniquely expressible in the form x = qn + r, where q and r are integers",
    "classes. \n(c) If X is the set of integers and Rn is the relation 'congruence \nmodulo n,' then there are precisely n equivalence classes. Each integer \nx is uniquely expressible in the form x = qn + r, where q and r are integers \nand 0 ::; r ::; n - 1. This shows that each x is congruent modulo n to \n893 \n39.4 \nAppendix \nexactly one of the n integers 0, 1, 2, . . .  , n - 1. The equivalence classes \nare \nEo \n{ . . .  , -2n, -n, 0, n, 2n, . . . } \nEl \n{ . . .  , 1 \n2n, 1 \nn, 1 + n, 1 + 2n, . . .  } \nE,,-l = { . . .  , n - 1 - 2n, n - 1 - n, n - 1, n - 1 + n, \nn \n1 \n2n, . . . } .  \n(d) Suppose X and Y are sets, ! is a function from X into Y, and R \nis the equivalence relation defined by: Xd?X2 if and only if !(Xl) \n!(X2). \nThe equivalence classes for R are just the largest subsets of X on which \nf is 'constant.' Another description of the equivalence classes is this. They \nare in 1 :1 correspondence with the members of the range of f. If y is in \nthe range of f, the set of all x in X such that fex) \n= y is an equivalence \nclass for R ;  and this defines a l :l correspondence between the members \nof the range of f and the equivalence classes of R. \nLet us make one more comment about equivalence relations. Given \nan equivalence relation R on X, let u: be the family of equivalence classes \nfor R. The association of the equivalence class E(x; R) with the element \nx, defines a function f from X into u: (indeed, onto u:) : \nf(x) \nE(x; R). \nThis shows that R is the equivalence relation associated with a function \nwhose domain is X, as in Example .5(e). What this tells us is that every \nequivalence relation on the set X is determined as follows. We have a rule \n(function) f which associates with each element x of X an object f(x), \nand xRy if and only if f(x) \nf(y). Now one should think of f(x) as some \nproperty of x, so that what the equivalence relation does (roughly) is to \nlump together all those elements of X which have this property in com­",
    "(function) f which associates with each element x of X an object f(x), \nand xRy if and only if f(x) \nf(y). Now one should think of f(x) as some \nproperty of x, so that what the equivalence relation does (roughly) is to \nlump together all those elements of X which have this property in com­\nmon. If the object f(x) is the equivalence elass of x, then all one has said \nis that the common property of the members of an equivalence class is \nthat they belong to the same equivalence class. Obviously this doesn't \nsay much. Generally, there are many different functions f which deter­\nmine the given equivalence relation as above, and one objective in the \nstudy of equivalence relations is to find such anf which gives a meaningful \nand elementary description of the equivalence relation. In Section A.5 \nwe shall see how this is accomplished for a few special equivalence rela­\ntions which arise in linear algebra. \nA.4. Quotient Spaces \nLet V be a vector space over the field Ji', and let W be a subspace of \nV. There are, in general, many subspaces W' which are complementary \nto W, i.e., subspaces with the property that V = W EB  W'. If we have \nSec. A.4 \nQuotient Spaces \nan inner product on V, and W is finite-dimensional, there is a particular \nsubspace which one would probably call the 'natural' complementary \nsubspace for W. This is the orthogonal complement of W. But, if V has \nno structure in addition to its vector space structure, there is no way of \nselecting a subspace W' which one could call the natural complementary \nsubspace for W. However, one can construct from V and W a vector space \nV /W, known as the 'quotient' of V and W, which will play the role of the \nnatural complement to W. This quotient space is not a subspace of V, \nand so it cannot actually be a subspace complementary to W; but, it is \na vector space defined only in terms of V and W, and has the property \nthat it is isomorphic to any subspace W' which is complementary to W.",
    "natural complement to W. This quotient space is not a subspace of V, \nand so it cannot actually be a subspace complementary to W; but, it is \na vector space defined only in terms of V and W, and has the property \nthat it is isomorphic to any subspace W' which is complementary to W. \nLet W be a subspace of the vector space V. If a and (3 are vectors \nin V, ,ve say that a is congruent to f3 Illodulo W, if the vector (a - (3) \nis in the subspace W. If a is congruent to (3 modulo W, we write \na == (3, \nmod W. \nNow congruence modulo W is an equivalence relation on V. \n(1) a == a, mod W, because a - a = 0 is in W. \n(2) If a == f3, mod W, then (3 == a, mod W. For, since W is a subspace \nof V, the vector (a - (3) is in W if and only if «(3 - a) is in W. \n(3) If a == (3, mod W, and (3 == \"I, mod W, then a == \"I, mod W. For, \nif (a - (3) and «(3 - \"I) are in W, then a - \"I = (a - (3) + (3 - \"I) is in W. \nThe equivalence classes for this equivalence relation are known as \nthe eosets of W. What is the equivalence class (coset) of a vector a? It \nconsists of all vectors (3 in V such that «(3 - a) is in W, that is, all vectors \n(3 of the form f3 = a + \"I, with \"I in W. For this reason, the coset of the \nvector a is denoted by \na + W. \nIt is appropriate to think of the coset of a relative to W as the set of \nvectors obtained by translating the subspace W by the vector a. To \npicture these cosets, the reader might think of the following special case. \nLet V be the space R2, and let W be a one-dimensional subspace of V. \nIf we picture V as the Euclidean plane, W is a straight line through the \norigin. If a = (Xl, X2) is a vector in V, the coset a + W is the straight line \nwhich passes through the point (Xl, X2) and is parallel to W. \nThe collection of all cosets of W will be denoted by V /W. We now \ndefine a vector addition and scalar multiplication on V /W as follows: \n(a + W) + «(3 + W) = (a + (3) + W \nc(a + W) = (ea) + W.",
    "which passes through the point (Xl, X2) and is parallel to W. \nThe collection of all cosets of W will be denoted by V /W. We now \ndefine a vector addition and scalar multiplication on V /W as follows: \n(a + W) + «(3 + W) = (a + (3) + W \nc(a + W) = (ea) + W. \nIn other words, the sum of the coset of a and the coset of (3 is the coset of \n(a + (3), and the product of the scalar e and the coset of a is the coset of \nthe vector Ca. Now many different vectors in V will have the same coset \n395 \n396 \nAppendix \nrelative to TV, and so we must verify that the sum and product above \ndepend only upon the cosets involved. What this means is that we must \nshow the following: \n(a) If a == a', mod W, and (3 == (3', mod W, then \na + (3 -+ a' + (3', \nmod TV. \n(2) If a == a', mod W, then ca == ca', mod TV. \nThese facts are easy to verify. (1) If a -- a' is in W and (3 - (3' is in \nW, then since (a + (3) - (a' \n(3') \n(a \na') + ((3 - (3'), we see that \na + (3 is congruent to a' \n(3' modulo W. (2) If a \na' is in W and c is \nany scalar, then ca - ca' \n= c(a - a') is in TV. \nIt is now easy to verify that V lTV, with 1;he vector addition and scalar \nmultiplication defined above, is a vector spaee over the field F. One must \ndirectly check each of the axioms for a vector space. Each of the properties \nof vector addition and scalar multiplication follows from the corresponding \nproperty of the operations in V. One comment should be made. The zero \nvector in V IW will be the coset of the zero vector in V. In other words, \nTV is the zero vector in V lTV. \nThe vector space V IW is called the quotient (or difference) of V \nand W. There is a natural linear transformation Q from V onto V IW. \nIt is defined by Q(a) = a + W. One should see that we have defined \nthe operations in V IW just so that this transformation Q would be linear. \nNote that the null space of Q is exactly the subspace W. We call Q the \nquotient transformation (or quotient mapping) of V onto V IW.",
    "It is defined by Q(a) = a + W. One should see that we have defined \nthe operations in V IW just so that this transformation Q would be linear. \nNote that the null space of Q is exactly the subspace W. We call Q the \nquotient transformation (or quotient mapping) of V onto V IW. \nThe relation between the quotient spa(შe V lTV and subspaces of V \nwhich are complementary to W can now be stated as follows. \nTheorem. Let W be a subspace of the vector space V, and let Q be the \nquotient mapping of V onto V IW. Suppose W' is a subspace of V. Then \nV \nW EB  w' if and only if the restriction of Q to W' is an isomorphism \nof W' onto V IW. \nProof. Suppose V = TV EB TV'. This means that each vector a in \nV is uniquely expressible in the form a = \"I + \"I', with \"I in TV and \"I' in \nW'. Then Qa \nQ\"I + Q\"I' \nQ\"I', that is a + W \n\"I' + TV. This shows \nthat Q maps W' onto VIW, i.e., that Q(lV') \n= VIW. Also Q is 1 :1 on WI; \nfor suppose \"I> and \"I? are vectors in W' and that Q\"I> = Q\"I@. Then \nQ( \"Ii - \"ID = 0 so that \"IA - \"IB is in W. This vector is also in W', which \nis disjoint from W; hence \"Ii \n\"IC \nO. The restrietion of Q to W' is \ntherefore a one-one linear transformation of W' onto V lTV. \nSuppose W' is a subspace of V such that Q is one-one on W' and \nQ(W') \nV IW. Let a be a vector in V. Then there is a vector \"I' in W' \nsuch that Q\"I' \nQa, i.e., \"I' + W = a + W. This means that a \n\"I + \"I' \nfor some vector \"I in W. Therefore V = W + W'. To see that W and W' \nSec. A.5 \nEquivalence Relations in Unear Algebra \nare disjoint, suppose 'Y is in both W and W'. Since 'Y is in W, we have \nQ'Y = O. But Q is 1 :1 on W', and so it must be that 'Y = O. Thus we have \nV = w ffi W'. I \nWhat this theorem really says is that W' is complementary to W if \nand only if W' is a subspace which contains exactly one element from each \ncoset of W. It shows that when V = W ffi W', the quotient mapping Q \n'identifies' W' with V IW. Briefly (W ffi W')IW is isomorphic to W' in \na 'natural' way.",
    "and only if W' is a subspace which contains exactly one element from each \ncoset of W. It shows that when V = W ffi W', the quotient mapping Q \n'identifies' W' with V IW. Briefly (W ffi W')IW is isomorphic to W' in \na 'natural' way. \nOne rather obvious fact should be noted. If W is a subspace of the \nfinite-dimensional vector space V, then \ndim W + dim (V IW) = dim V. \nOne can see this from the above theorem. Perhaps it is easier to observe \nthat what this dimension formula says is \nnullity (Q) + rank (Q) = dim V. \nIt is not our object here to give a detailed treatment of quotient \nspaces. But there is one fundamental result which we should prove. \nTheorem. Let V and Z be vector spaces over the field F. Suppose T is \na linear transformation of V onto Z. If W is the null space of T, then Z is \nisomorphic to V IW. \nProof. We define a transformation a from V IW into Z by \naCa + W) = Ta. We must verify that a is well defined, i.e., that if \na: + W = {3 + W then Ta = T{3. This follows from the fact that W is \nthe null space of T; for, a + W = {3 + W means a \n- (3 is in W, and this \nhappens if and only if T(a - (3) = O. This shows not only that a is well \ndefined, but also that U is one-one. \nIt is now easy to verify that a is linear and sends V IW onto Z, \nbecause T is a linear transformation of V onto Z. I \n3.97 \nA.S. Equivalence Relations \nin Linear Algebra \nWe shall consider some of the equivalence relations which arise in \nthe text of this book. This is just a sampling of such relations. \n(1) Let m and n be positive integers and F a field. Let X be the set \nof all m X n matrices over F. Then row-equivalence is an equivalence \nrelation on the set X. The statement 'A is row-equivalent to B' means \nthat A can be obtained from B by a finite succession of elementary row \noperations. If we write A \"\" B for A is row-equivalent to B, then it is not \ndifficult to check the properties (i) A '\" A ;  (ii) if A \"\" B, then B '\" A ;  \n398 \nA ppendix",
    "that A can be obtained from B by a finite succession of elementary row \noperations. If we write A \"\" B for A is row-equivalent to B, then it is not \ndifficult to check the properties (i) A '\" A ;  (ii) if A \"\" B, then B '\" A ;  \n398 \nA ppendix \n(iii) if A '\" B and B r-v C, then A rv C. What do we know about this \nequivalence relation? Actually, we know a great deal. For example, we \nknow that A rv B if and only if A \n= P B for some invertible rn X rn \nmatrix Pi or, A rv B if and only if the homogeneous systems of linear \nequations AX = 0 and BX \n0 have the same solutions. We also have \nvery explicit information about the equivalence classes for this relation. \nEach rn X n matrix A is row-equivalent to one and only one row-reduced \nechelon matrix. What this says is that each equivalence class for this rela­\ntion contains precisely one row-reduced ech()lon matrix R; the equivalence \nclass determined by R consists of all matrices A \n= PR, where P is an \ninvertible rn X rn matrix. One can also think of this description of the \nequivalence classes in the following way. Given an rn X n matrix A, we \nhave a rule (function) f which associates with A the row-reduced echelon \nmatrix f(A) which is row-equivalent to A .  Row-equivalence is completely \ndetermined by f. For, A rv B if and only if f(A) = feB), i.e., if and only \nif A and B have the same row-reduced echelon form. \n(2) Let n be a positive integer and F a field. Let X be the set of all \nn X n matrices over F. Then similarity is an equivalence relation on X; \neach n X n matrix A is similar to itself; if A is similar to B, then B is \nsimilar to A ;  if A is similar to B and B is similar to C, then A is similar to \nC. We know quite a bit about this equivalence relation too. For example, \nA is similar to B if and only if A and B represent the same linear operator \non Fn in (possibly) different ordered bases. But, we know something much \ndeeper than this. Each n X n matrix A over F is similar (over F) to one",
    "C. We know quite a bit about this equivalence relation too. For example, \nA is similar to B if and only if A and B represent the same linear operator \non Fn in (possibly) different ordered bases. But, we know something much \ndeeper than this. Each n X n matrix A over F is similar (over F) to one \nand only one matrix which is in rational form (Chapter 7). In other words, \neach equivalence class for the relation of similarity contains precisely one \nmatrix which is in rational form. A matrix in rational form is determined \nby a k-tuple (PI, . . . , Pk) of monic polynomials having the property that \nPi+1 divides Ph j = 1, . . .  , k - 1. Thus, we have a function f which \nassociates with each n X n matrix A a k-tuple f(A) \n(pI, .\n.\n.\n , Pk) \nsatisfying the divisibility condition Pi+1 divides PJ' And, A and B are \nsimilar if and only if f(A) \n= feB). \n(3) Here is a special case of Example 2 above. Let X be the set of \n3 X 3 matrices over a field F. We consider the relation of similarity on X. \nIf A and B are 3 X 3 matrices over F, then A and B are similar if and \nonly if they have the same characteristic polynomial and the same minimal \npolynomiaL Attached to each 3 X 3 matrix A, we have a pair (f, p) of \nmonic polynomials satisfying \n(a) deg f = 3, \n(b) P divides f, \nf being the characteristic polynomial for A, and P the minimal polynomial \nfor A .  Given monic polynomials f and P over F which satisfy (a) and (b), \nit is easy to exhibit a 3 X 3 matrix over F, having f and P as its charac-\nSec. A.6 \nThe Axiom of Choice \nteristic and minimal polynomials, respectively. What all this tells us is \nthe following. If we consider the relation of similarity on the set of 3 X 3 \nmatrices over F, the equivalence classes are in one-one correspondence \nwith ordered pairs (f, p) of monic polynomials over F which satisfy (a) \nand (b). \n3.99 \nA.6. The Axiom of Choice \nLoosely speaking, the Axiom of Choice is a rule (or principle) of",
    "matrices over F, the equivalence classes are in one-one correspondence \nwith ordered pairs (f, p) of monic polynomials over F which satisfy (a) \nand (b). \n3.99 \nA.6. The Axiom of Choice \nLoosely speaking, the Axiom of Choice is a rule (or principle) of \nthinking which says that, given a family of non-empty sets, we can choose \none element out of each set. To be more precise, suppose that we have \nan index set A and for each a in A we have an associated set Sa, which is \nnon-empty. To 'choose' one member of each Sa means to give a rule f \nwhich associates with each a some element f(a) in the set Sa. The axiom \nof choice says that this is possible, i.e., given the family of sets {Sa}, there \nexists a function f from A into \na \nsuch thatf(a) is in Sa for each a. This principle is accepted by most mathe­\nmaticians, although many situations arise in which it is far from clear \nhow any explicit function f can be found. \nThe Axiom of Choice has some startling consequences. Most of them \nhave little or no bearing on the subject matter of this book; however, one \nconsequence is worth mentioning: Every vector space has a basis. For \nexample, the field of real numbers has a basis, as a vector space over the \nfield of rational numbers. In other words, there is a subset S of R which \nis linearly independent over the field of rationals and has the property \nthat each real number is a rational linear combination of some finite \nnumber of elements of S. We shall not stop to derive this vector space \nresult from the Axiom of Choice. For a proof, we refer the reader to the \nbook by Kelley in the bibliography. \nBibliography \nHalrnos, P., Finite-Dimensional Vector Spaces, D. Van Nostrand Co., Princeton, \n1958. \nJacobson, N., Lectures in A bstract Algebra, II, D. Van Nostrand Co., Princeton, \n1953. \nKelley, John L., General Topology, D. Van Nostrand Co., Prineeton, 195.5. \nMacLane, S. and Birkhoff, G., Algebra, The Maernillan Co., New York, 1967.",
    "1958. \nJacobson, N., Lectures in A bstract Algebra, II, D. Van Nostrand Co., Princeton, \n1953. \nKelley, John L., General Topology, D. Van Nostrand Co., Prineeton, 195.5. \nMacLane, S. and Birkhoff, G., Algebra, The Maernillan Co., New York, 1967. \nSchreier, O. and Sperner, E., Introduction to Modern Algebra and Matrix Theory, \n2nd Ed., Chelsea Publishing Co., New York, 1955. \n% van del' Wael'den, B. L., Modern Algebra (two volumes), Rev. Ed., Frederick \nUngar Publishing Co., New York, 1 969. \n400 \nA \nAdjoint: \nclassical, 148, 159 \nof transformation, 295 \nAdmissible subspace, 232 \nAlgebra, 1 17 \nof formal power serieʗ, 1 19 \nself-adjoint, 345 \nAlgebraically closed field, 138 \nAlternating n-linear function, 144, 169 \nAnnihilator : \nof subset, 101 \nof sum and intersection, 106IEx. 1 1 )  \nof vector ( T-annihilator), 201, 202, 228 \nApproximation, 283 \nAssociativity, 1 \nof matrix multiplication, 19, 90 \nof vector addition, 28 \nAugmented matrix, 1 4  \nAxiom o f  choice, 400 \nB \nBasis, 41 \nchange of, 92 \ndual, 99, 165 \nfor module, 164 \nordered, 50 \northonormal, 281 \nstandard b!4ʘis of l!'n, 41 \nIndex \nBessel's inequality, 287 \nBilinear form, 166, 320, 359 \ndiagonalization of, 370 \ngroup preserving, 379 \nmatrix of, 362 \nnon-degenerate ( non-singular), 365 \npositive definite, 368 \nrank of, 865 \nsignature of, 872 \nskew-symmetric, 375 \nsymmetric, 367 \nc \nCauchy-Schwarz inequality, 278 \nCayley-Hamilton theorem, 194, 237 \nCayley transform, 309(Ex. 7) \nCharacteristic: \nof a field, 3 \npolynomial, 1 83 \nspace, 182 \nvalue, 182, 183 \nvector, 182 \nClassical adjoint, 148, 1 59 \nCoefficients of polynomial, 120 \nCofactor, 158 \nColumn: \nequivalence, 256 \noperations, 26, 256 \nrank, 72, 1 14 \n401 \n402 \nIndex \nCommutative: \nalgebra, 1 17 \ngroup, 83 \nring, 140 \nCompanion matrix, 230 \nComplementary Bubspace, 231 \northogonal, 286 \nComposition, 390 \nConductor, 201, 202, 232 \nCongruence, 139, 393, 396 \nConjugate, 271 \ntranspose, 272 \nConjugation, 276(Ex. 13) \nCoordinates, 50",
    "rank, 72, 1 14 \n401 \n402 \nIndex \nCommutative: \nalgebra, 1 17 \ngroup, 83 \nring, 140 \nCompanion matrix, 230 \nComplementary Bubspace, 231 \northogonal, 286 \nComposition, 390 \nConductor, 201, 202, 232 \nCongruence, 139, 393, 396 \nConjugate, 271 \ntranspose, 272 \nConjugation, 276(Ex. 13) \nCoordinates, 50 \ncoordinate matrix, 51 \nCoset, 1 77, 396 \nCramer's rule, 161 \nCyclie: \ndecomposition theorem, 233 \nsubspace, 227 \nvector, 227 \nD \nDegree: \nof multilinear form, 166 \nof polynomial, 1 19 \nDependence, linear, 40, 47 \nDerivative of polynomial, 129, 266 \nDeterminant function, 144 \nexistence of, 147 \nfor linear transformations, 172 \nuniqueness of, 152 \nDeterminant rank, 163(Ex. 9) \nDiagonalizable: \noperator, 185 \npart of linear operator, 222 \nsimultaneously, 207 \nDiagonalization, 204, 207, 216 \nof Hermitian form, 323 \nof normal matrix (operator), :n 7 \nof self-adjoint matrix (operator), 314 \nof symmetric bilinear form, 370 \nunitary, 317 \nDifferential equations, 223(Ex. 14), \n249(Ex. 8) \nDimension, 44 \nformula, 46 \nDirect sum, 210 \ninvariant, 214 \nof matrices, 214 \nof operators, 214 \nDisjoint subspaces (sce Independent : sub-\nspaces) \nDistance, 289(Ex. 4) \nDivision with remainder, 128 \nDual: \nbasiʔ, 99, 165 \nmodllle, 165 \nspace, 98 \nE \nEigenvalue (see Characteristic: value) \nElementary: \ncolumn operation, 26, 256 \nJordan matrix, 245 \nmatrix, 20, 253 \nrow operation, 6, 252 \nEmpty set, 388 \nEntrie,; of a matrix, 6 \nEquiv2Jence relation, 393 \nEquivalent systems of equations, 4 \nEuelidean space, 277 \nExterior (wedge) product, 1 75, 177 \nF \nFm x n, 29 \nFn, 29 \nFactorization of polynomial, 136 \nFactor:\" invariant, 2:39, 261 \nField, :2 \nalgebraically closed, 138 \nsubfield, 2 \nFinite-dimensional, 4 1  \nFinitely generated module, 165 \nForm: \nalternating, 1 69 \nbilinear, 166, 320, 359 \nHermitian, 323 \nmatrix of, 322 \nmnhilinear, 166 \nnon-degenerate, 324(Ex. 6) \nnon-negative, 32.5 \nnormal, 257, 261 \npositive, 325, :328 \nquadratic, 273, 368 \nr-linear, 166 \nrational, 238 \nsesq ʕi-linear, 320",
    "Finitely generated module, 165 \nForm: \nalternating, 1 69 \nbilinear, 166, 320, 359 \nHermitian, 323 \nmatrix of, 322 \nmnhilinear, 166 \nnon-degenerate, 324(Ex. 6) \nnon-negative, 32.5 \nnormal, 257, 261 \npositive, 325, :328 \nquadratic, 273, 368 \nr-linear, 166 \nrational, 238 \nsesq ʕi-linear, 320 \nFormal power series, 1 19 \nFree module, 164 \nFunction, 389 \ndeterminant, 144 \nidentity, 390 \ninverse of, 391 \ninvertible, 390 \nlinear, 67, 97, 291 \nmultilinear, 166 \nn-linear, 142 \npolynomial function, 30 \nrange of, 389 \nrestriction of, 391 \nFundamental theorem of algebra, 138 \nG \nGram-Schmidt process, 280, 287 \nGra,ʙsman ring, 180 \nGreatest common divisor, 1 3a \nGroup, 82 \ncommutative, 83 \ngeneral linear, 307 \nLorentz, 382 \northogonal, 380 \npreserving a form, a79 \npseudo-orthogonal, 381 \nsymmetric, 15a \nH \nHermitian (see Self-adjoint) \nHermitian form, 323 \nHomogeneous system of linear equations, 4 \nHyperspace, 101, 109 \nI \nIdeal, 131 \nprincipal ideal, 131 \nIdempotent transformation (see Projection) \nIdentity: \nelement, 1 17, 140 \nfunction, 390 \nmatrix, 9 \nresolution of, 337, 344 \nIndependence, linear, 40, 47 \nIndependent: \nlinearly, 40, 47 \n8ubspaces, 209 \nInner product, 271 \nmatrix of, 274 \nInner product (cont.) : \nquadratic form of, 273 \nspace, 277 \nstandard, 271, 272 \nIntegers, 2 \npositive, 2 \nInterpolation, 124 \nJ ntersection, 388 \nof subs paces, 36 \nI nvariant: \ndirect sum, 214 \nfactors of a matrix, 239, 261 \nsubset, 392 \nsubspace, 199, 206, 314 \nInverse: \nof function, 3!n \nleft, 22 \nof matrix, 22, 160 \nright, 22 \ntwo-sided, 22 \nInvertible : \nfunction, 390 \nlinear tral15formation, 79 \nmatrix, 22, 160 \nIrreducible polynomial, 135 \nIsomorphism: \nof inner product spaces, 299 \nof vector spaces, 84 \nJ \nJordan form of matrix, 247 \nK \nKronecker delta, 9 \nL \nIndex \n403 \nLagrange interpolation formula, 124 \nLaplace expansions, 1 79 \nLeft i nverse, 22 \nLinear algebra, 1 17 \nLinear combination: \nof equations, 4 \nof vectors, 31 \nLinear equations (see 8ystem of linear \nequations)",
    "J \nJordan form of matrix, 247 \nK \nKronecker delta, 9 \nL \nIndex \n403 \nLagrange interpolation formula, 124 \nLaplace expansions, 1 79 \nLeft i nverse, 22 \nLinear algebra, 1 17 \nLinear combination: \nof equations, 4 \nof vectors, 31 \nLinear equations (see 8ystem of linear \nequations) \nLinear functional, 97 \nLinearly dependent (independent), 40, 47 \n404 \nIndex \nLinear transformation (operator), 67, 76 \nadjoint of, 295 \ncyclic decomposition of, 233 \ndeterminant of, 172 \ndiagonalizable, 185 \ndiagollalizable part of, 222 \ninvertible, 79 \nmatrix in orthonormal basis, 293 \nmatrix of, 87, 88 \nminimal polynomial of, 191 \nnilpotent, 222 \nnon-negative, 329, 341 \nnOll-singular, 79 \nnormal, 312 \nIlullity of, 71 \northogonal, 303 \npolar decompoʖition of, 343 \npositive, 329 \nproduct of, 76 \nquotient, 397 \nrange of, 7 1  \nrank of, 7 1  \nself-adjoint, 298, 3 14 \nsemi-simple, 263 \ntrace of, 106(Ex. II» \ntranspose of, 1 12 \ntriangulable, 202 \nunitary, 302 \nLorentz : \ngroup, 382 \ntransformation, :31 l (Ex. 15), 382 \nM atrix, 6 \naugmented, 1 4  \nM \nof bilinear form, 362 \nclassical adjoint of, 148, 159 \ncoefficient, 6 \ncofactors, 158 \ncompanion, 230 \nconjugate transpose, 272 \ncoordinate, 51 \nelementary, 20, 253 \nelementary, Jordan, 245 \nof form, 322 \nidentity, 9 \nof inner product, 274 \ninvariant factors of, 239, 261 \ni nverse of, 22, 160 \ninvertible, 22, 160 \nJordan form of, 247, \nMatrix (cont,) : \nof linear transformation, 87, 88 \nminimal polynomial of, 191 \ntlilpotent, 244 \ntlormal, 315 \northogonal, 162(Ex. 4), 380 \npositive, 329 \nprincipal minors of, 326 \nproduct, 1 7, 90 \nrank of, 1 14 \nrational form of, 238 \nrow rank of, 56, 72, 1 14 \nrow-reduced, 9 \nrow-reduced echelon, 1 1, 56 \nself-adjoint ( Hermitian), 35, 314 \nsimilarity of, 94 \nskew-symmetric, 1 62(Ex. 3), 210 \nsymmetric, 35, 210 \ntrace of, 98 \ntranspose of, 1 14 \ntriangular, 155(Ex. 7) \nunitary, 163(Ex. 5), 303 \nupper-triangular, 27 \nVandermonde, 125 \nzero, 12 \nMinimal polynomial, 191 \nModule, 164 \nbasis for, 164 \ndual, 1 65",
    "similarity of, 94 \nskew-symmetric, 1 62(Ex. 3), 210 \nsymmetric, 35, 210 \ntrace of, 98 \ntranspose of, 1 14 \ntriangular, 155(Ex. 7) \nunitary, 163(Ex. 5), 303 \nupper-triangular, 27 \nVandermonde, 125 \nzero, 12 \nMinimal polynomial, 191 \nModule, 164 \nbasis for, 164 \ndual, 1 65 \nfinitely generated, 165 \nfree, 164 \nrank of, 165 \nMonic polynomial, 120 \nM ultilinear function (form), 166 \ndegree of, 1 66 \nMultiplicity, 130 \nN \nn-linear function, 142 \nalternating, 144, 169 \nn-tuple, 29 \nNilpotent: \nmatrix, 244 \noperator, 222 \nNon-degenerate: \nbilinear form, 365 \nform, 324(Ex. 6) \nNon-negative: \nform, 325 \noperator, 329, 341 \nNon-singular: \nform (see Non-degenerate) \nlinear transformation, 79 \nNorm, 273 \nNormal: \nform, 257, 261 \nmatrix, 315 \noperator, 312 \nNullity of linear transformation, 7 1  \nNull space, 71 \nNumbers: \ncomplex, 2 \nrational, 3 \nreal, 2 \nOnto, 389 \nOperator, linear, 76 \nOrdered basis, 50 \nOrthogonal: \no \ncomplement, 285 \nequivalence of matrices, 308 \ngroup, 380 \nlinear transformation, 304 \nmatrix, 162CEx. 4), 380 \nprojection, 285 \nset, 278 \nvectors, 278, 368 \nOrthogonalization, 280 \nOrthonormal : \nbasis, 281 \nset, 278 \np \nParallelogram law, 276(Ex. 9) \nPermutation, 151 \neven, odd, 1 52 \nproduct of, 15:3 \nsign of, 152 \nPolar decomposition, 34:3 \nPolarization identities, 274, 368 \nPolynomial, 1 1 9  \ncharacteristic, 183 \ncoefficients of, 120 \ndegree of, 1 19 \nderivative of, 129, 266 \nfunction, 30 \nirreducible (prime), 135 \nminimal, 191 \nPolynomial (cont.) : \nmonic, 120 \nIndex \n406 \nprimary decomposition of, 137 \nprime (irreducible), 135 \nprime factorization of, 1 36 \nreducible, 135 \nroot of, 129 \nscalar, 120 \nzero of, 129 \nPositive: \nform, 325, 328 \nintegers, 2 \nmatrix, 329 \noperator, 329 \nPositive definite, 368 \nPower series, 1 19 \nPrimary components, 351 \nPrimary decomposition: \nof polynomial, 137 \ntheorem, 220 \nPrime: \nfactorization of polynomial, 136 \npolynomial, 135 \nPrincipal: \naccess theorem, 323 \nideal, 131 \nminors, 326 \nProduct:",
    "integers, 2 \nmatrix, 329 \noperator, 329 \nPositive definite, 368 \nPower series, 1 19 \nPrimary components, 351 \nPrimary decomposition: \nof polynomial, 137 \ntheorem, 220 \nPrime: \nfactorization of polynomial, 136 \npolynomial, 135 \nPrincipal: \naccess theorem, 323 \nideal, 131 \nminors, 326 \nProduct: \nexterior (wedge), 175, 177 \nof linear transformations, 76 \nof matrices, .14, 90 \nof permutations, 153 \ntensor, 168 \nProjection, 2 1 1  \nProper subset, 388 \nPseudo-orthogonal group, 381 \nQ \nQuadratic form, 273, 368 \nQuotient: \nspace, 397 \ntransformation, 397 \nRange, 7 1  \nRank; \nR \nof bilinear form, 365 \ncolumn, 72, 1 14 \ndeterminant, 163(Ex. 9) \n4.06 \nIndex \nRank (cont.) :  \nof linear transformation, 7 1  \nof matrix, 1 14 \nof module, 165 \nrow, 56, 72, 1 1 4  \nRational form o f  matrix, 238 \nReducible polynomial, 135 \nRelation, 393 \nequivalence, 393 \nRelatively prime, 133 \nResolution: \nof the identity, 337, 344 \nlSpectral, 336, 344 \nRestriction: \nof function, 391 \noperator, 199 \nRight inverse, 22 \nRigid motion, 31 0(Ex. 14) \nRing, 140 \nGrassman, 180 \nRoot: \nof family of operators, 343 \nof polynomial, 129 \nRotation, 54, 309(Ex. 4) \nRow: \noperatiollH, 6, 252 \nrank, 56, 72, 1 14 \nspace, 39 \nvectors, 38 \nRow-equivalence, 7, 58, 253 \nsummary of, 55 \nRow-reduced matrix, 9 \nrow-reduced echelon matrix, 1 1, 56 \nScalar, 2 \npolynomial, 120 \nSelf-adjoint: \nalgebra, :345 \nmatrix, :35, 314 \noperator, 298, 314 \ns \nSemi-simple operator, 263 \nSeparating vector, 243(Ex. 14) \nSequence of vectors, 47 \nSesqui-linear form, 320 \nSet, 388 \nelement of (member of), 388 \nempty, 388 \nShuffle, 171 \nSignature, 372 \nSign of permutation, 152 \nSi milar matrices, 94 \nSimultaneous: \ndiagonalization, 207 \ntriangulation, 207 \nSkew-symmetric: \nbilinear form, 375 \nmatrix, 162(Ex. 3), 210 \nSolution space, 36 \nSpectral : \nresolution, 336, 344 \ntheorem, 3:3.5 \nSpectrum, 336 \nSquare root, 341 \nStandard basis of pn, 41 \nStuffer (das einstopfende Ideal), 201 \nSubfield, 2 \nSubmatrix, 163(Ex. 9) \nSubset, 388",
    "triangulation, 207 \nSkew-symmetric: \nbilinear form, 375 \nmatrix, 162(Ex. 3), 210 \nSolution space, 36 \nSpectral : \nresolution, 336, 344 \ntheorem, 3:3.5 \nSpectrum, 336 \nSquare root, 341 \nStandard basis of pn, 41 \nStuffer (das einstopfende Ideal), 201 \nSubfield, 2 \nSubmatrix, 163(Ex. 9) \nSubset, 388 \ninvariant, 392 \nproper, 388 \nSubspace, 34 \nannihilator of, 101 \ncomplementary, 231 \ncyclic, 227 \nindependent subs paces, 209 \ninvariant, 199, 206, 314 \northogonal complement of, 285 \nquotient by, 397 \nspanned by, 36 \nsum of subs paces, 37 \nT-admissible, 232 \nzero, 35 \nSum: \ndirect, 210 \nof subs paces, :37 \nSymmetric: \nbilinear form, 367 \ngroup, 153 \nmatrix, 3.5, 210 \nSystem of linear equations, 3 \nhomogeneous, 4 \nT \nT-admissible subspace, 232 \nT-annihilator, 201, 202, 228 \nT-eonductor, 201, 202, 232 \nTaylor's formula, 129, 266 \nTec\\sor, 166 \nproduct, 168 \nTrace : \nof linear transformation, 106(Ex. 15) \nof matrix, 98 \nTransformation: \ndifferentiation, 67 \nlinear, 67, 76 \nzero, 67 \nTranspose : \nconjugate, 272 \nof linear transformation, 1 12 \nof matrix, 1 14 \nTriangulable linear transformation, 202, \n316 \nTriangular matrix, 155(Ex. 7) \nTriangulation, 203, 207, 334 \nUnion, 388 \nUnitary : \ndiagonalization, 317 \nu \nequivalence of linear trarmformatiollR, \n356 \nequivalence of matrices, a08 \nmatrix, 16a(Ex. 5), 3Da \noperator, 302 \nspace, 277 \ntransformation, 3Mi \nUpper-triangular matrix, 27 \nv \nVandermonde matrix, 125 \nVector space, 28 \nbasis of, 41 \ndimension of, 44 \nfinite dimensional, 41 \nisomorphism of, 84 \nof n-tuples, 29 \nof polynomial functions, :m \nquotient of, 397 \nIndex \n407 \nof solutions to linear equations, 36 \nsubspace of, a4 \nw \nWedge (exterior) product, 1 75, 177 \nZero: \nmatrix, 12 \nof polynomial, 129 \nz"
]