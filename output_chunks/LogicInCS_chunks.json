[
    "This page intentionally left blank\nLOGIC IN COMPUTER SCIENCE\nModelling and Reasoning about Systems\nLOGIC IN COMPUTER SCIENCE\nModelling and Reasoning about Systems\nMICHAEL HUTH\nDepartment of Computing\nImperial College London, United Kingdom\nMARK RYAN\nSchool of Computer Science\nUniversity of Birmingham, United Kingdom\nCAMBRIDGE UNIVERSITY PRESS\nCambridge, New York, Melbourne, Madrid, Cape Town, Singapore, São Paulo\nCambridge University Press\nThe Edinburgh Building, Cambridge CB2 8RU, UK\nFirst published in print format\nISBN-13\n978-0-521-54310-1\nISBN-13\n978-0-511-26401-6\n© Cambridge University Press 2004\n2004\nInformation on this title: www.cambridge.org/9780521543101\nThis publication is in copyright. Subject to statutory exception and to the provision of\nrelevant collective licensing agreements, no reproduction of any part may take place\nwithout the written permission of Cambridge University Press.\nISBN-10\n0-511-26401-1\nISBN-10\n0-521-54310-X\nCambridge University Press has no responsibility for the persistence or accuracy of urls\nfor external or third-party internet websites referred to in this publication, and does not\nguarantee that any content on such websites is, or will remain, accurate or appropriate.\nPublished in the United States of America by Cambridge University Press, New York\nwww.cambridge.org\npaperback\neBook (EBL)\neBook (EBL)\npaperback\nContents\nForeword to the ﬁrst edition\npage ix\nPreface to the second edition\nxi\nAcknowledgements\nxiii\n1\nPropositional logic\n1\n1.1\nDeclarative sentences\n2\n1.2\nNatural deduction\n5\n1.2.1\nRules for natural deduction\n6\n1.2.2\nDerived rules\n23\n1.2.3\nNatural deduction in summary\n26\n1.2.4\nProvable equivalence\n29\n1.2.5\nAn aside: proof by contradiction\n29\n1.3\nPropositional logic as a formal language\n31\n1.4\nSemantics of propositional logic\n36\n1.4.1\nThe meaning of logical connectives\n36\n1.4.2\nMathematical induction\n40\n1.4.3\nSoundness of propositional logic\n45\n1.4.4\nCompleteness of propositional logic\n49\n1.5\nNormal forms\n53\n1.5.1",
    "29\n1.3\nPropositional logic as a formal language\n31\n1.4\nSemantics of propositional logic\n36\n1.4.1\nThe meaning of logical connectives\n36\n1.4.2\nMathematical induction\n40\n1.4.3\nSoundness of propositional logic\n45\n1.4.4\nCompleteness of propositional logic\n49\n1.5\nNormal forms\n53\n1.5.1\nSemantic equivalence, satisﬁability and validity\n54\n1.5.2\nConjunctive normal forms and validity\n58\n1.5.3\nHorn clauses and satisﬁability\n65\n1.6\nSAT solvers\n68\n1.6.1\nA linear solver\n69\n1.6.2\nA cubic solver\n72\n1.7\nExercises\n78\n1.8\nBibliographic notes\n91\n2\nPredicate logic\n93\n2.1\nThe need for a richer language\n93\nv\nvi\nContents\n2.2\nPredicate logic as a formal language\n98\n2.2.1\nTerms\n99\n2.2.2\nFormulas\n100\n2.2.3\nFree and bound variables\n102\n2.2.4\nSubstitution\n104\n2.3\nProof theory of predicate logic\n107\n2.3.1\nNatural deduction rules\n107\n2.3.2\nQuantiﬁer equivalences\n117\n2.4\nSemantics of predicate logic\n122\n2.4.1\nModels\n123\n2.4.2\nSemantic entailment\n129\n2.4.3\nThe semantics of equality\n130\n2.5\nUndecidability of predicate logic\n131\n2.6\nExpressiveness of predicate logic\n136\n2.6.1\nExistential second-order logic\n139\n2.6.2\nUniversal second-order logic\n140\n2.7\nMicromodels of software\n141\n2.7.1\nState machines\n142\n2.7.2\nAlma – re-visited\n146\n2.7.3\nA software micromodel\n148\n2.8\nExercises\n157\n2.9\nBibliographic notes\n170\n3\nVeriﬁcation by model checking\n172\n3.1\nMotivation for veriﬁcation\n172\n3.2\nLinear-time temporal logic\n175\n3.2.1\nSyntax of LTL\n175\n3.2.2\nSemantics of LTL\n178\n3.2.3\nPractical patterns of speciﬁcations\n183\n3.2.4\nImportant equivalences between LTL formulas\n184\n3.2.5\nAdequate sets of connectives for LTL\n186\n3.3\nModel checking: systems, tools, properties\n187\n3.3.1\nExample: mutual exclusion\n187\n3.3.2\nThe NuSMV model checker\n191\n3.3.3\nRunning NuSMV\n194\n3.3.4\nMutual exclusion revisited\n195\n3.3.5\nThe ferryman\n199\n3.3.6\nThe alternating bit protocol\n203\n3.4\nBranching-time logic\n207\n3.4.1\nSyntax of CTL\n208\nContents\nvii\n3.4.2\nSemantics of CTL\n211\n3.4.3\nPractical patterns of speciﬁcations\n215\n3.4.4",
    "The NuSMV model checker\n191\n3.3.3\nRunning NuSMV\n194\n3.3.4\nMutual exclusion revisited\n195\n3.3.5\nThe ferryman\n199\n3.3.6\nThe alternating bit protocol\n203\n3.4\nBranching-time logic\n207\n3.4.1\nSyntax of CTL\n208\nContents\nvii\n3.4.2\nSemantics of CTL\n211\n3.4.3\nPractical patterns of speciﬁcations\n215\n3.4.4\nImportant equivalences between CTL formulas\n215\n3.4.5\nAdequate sets of CTL connectives\n216\n3.5\nCTL* and the expressive powers of LTL and CTL\n217\n3.5.1\nBoolean combinations of temporal formulas in CTL\n220\n3.5.2\nPast operators in LTL\n221\n3.6\nModel-checking algorithms\n221\n3.6.1\nThe CTL model-checking algorithm\n222\n3.6.2\nCTL model checking with fairness\n230\n3.6.3\nThe LTL model-checking algorithm\n232\n3.7\nThe ﬁxed-point characterisation of CTL\n238\n3.7.1\nMonotone functions\n240\n3.7.2\nThe correctness of SATEG\n242\n3.7.3\nThe correctness of SATEU\n243\n3.8\nExercises\n245\n3.9\nBibliographic notes\n254\n4\nProgram veriﬁcation\n256\n4.1\nWhy should we specify and verify code?\n257\n4.2\nA framework for software veriﬁcation\n258\n4.2.1\nA core programming language\n259\n4.2.2\nHoare triples\n262\n4.2.3\nPartial and total correctness\n265\n4.2.4\nProgram variables and logical variables\n268\n4.3\nProof calculus for partial correctness\n269\n4.3.1\nProof rules\n269\n4.3.2\nProof tableaux\n273\n4.3.3\nA case study: minimal-sum section\n287\n4.4\nProof calculus for total correctness\n292\n4.5\nProgramming by contract\n296\n4.6\nExercises\n299\n4.7\nBibliographic notes\n304\n5\nModal logics and agents\n306\n5.1\nModes of truth\n306\n5.2\nBasic modal logic\n307\n5.2.1\nSyntax\n307\n5.2.2\nSemantics\n308\n5.3\nLogic engineering\n316\n5.3.1\nThe stock of valid formulas\n317\nviii\nContents\n5.3.2\nImportant properties of the accessibility relation\n320\n5.3.3\nCorrespondence theory\n322\n5.3.4\nSome modal logics\n326\n5.4\nNatural deduction\n328\n5.5\nReasoning about knowledge in a multi-agent system\n331\n5.5.1\nSome examples\n332\n5.5.2\nThe modal logic KT45n\n335\n5.5.3\nNatural deduction for KT45n\n339\n5.5.4\nFormalising the examples\n342\n5.6\nExercises\n350\n5.7\nBibliographic notes\n356\n6",
    "322\n5.3.4\nSome modal logics\n326\n5.4\nNatural deduction\n328\n5.5\nReasoning about knowledge in a multi-agent system\n331\n5.5.1\nSome examples\n332\n5.5.2\nThe modal logic KT45n\n335\n5.5.3\nNatural deduction for KT45n\n339\n5.5.4\nFormalising the examples\n342\n5.6\nExercises\n350\n5.7\nBibliographic notes\n356\n6\nBinary decision diagrams\n358\n6.1\nRepresenting boolean functions\n358\n6.1.1\nPropositional formulas and truth tables\n359\n6.1.2\nBinary decision diagrams\n361\n6.1.3\nOrdered BDDs\n366\n6.2\nAlgorithms for reduced OBDDs\n372\n6.2.1\nThe algorithm reduce\n372\n6.2.2\nThe algorithm apply\n373\n6.2.3\nThe algorithm restrict\n377\n6.2.4\nThe algorithm exists\n377\n6.2.5\nAssessment of OBDDs\n380\n6.3\nSymbolic model checking\n382\n6.3.1\nRepresenting subsets of the set of states\n383\n6.3.2\nRepresenting the transition relation\n385\n6.3.3\nImplementing the functions pre∃and pre∀\n387\n6.3.4\nSynthesising OBDDs\n387\n6.4\nA relational mu-calculus\n390\n6.4.1\nSyntax and semantics\n390\n6.4.2\nCoding CTL models and speciﬁcations\n393\n6.5\nExercises\n398\n6.6\nBibliographic notes\n413\nBibliography\n414\nIndex\n418\nForeword to the first edition\nby\nEdmund M. Clarke\nFORE Systems Professor of Computer Science\nCarnegie Mellon University\nPittsburgh, PA\nFormal methods have ﬁnally come of age! Speciﬁcation languages, theorem\nprovers, and model checkers are beginning to be used routinely in industry.\nMathematical logic is basic to all of these techniques. Until now textbooks\non logic for computer scientists have not kept pace with the development\nof tools for hardware and software speciﬁcation and veriﬁcation. For exam-\nple, in spite of the success of model checking in verifying sequential circuit\ndesigns and communication protocols, until now I did not know of a sin-\ngle text, suitable for undergraduate and beginning graduate students, that\nattempts to explain how this technique works. As a result, this material is\nrarely taught to computer scientists and electrical engineers who will need to",
    "gle text, suitable for undergraduate and beginning graduate students, that\nattempts to explain how this technique works. As a result, this material is\nrarely taught to computer scientists and electrical engineers who will need to\nuse it as part of their jobs in the near future. Instead, engineers avoid using\nformal methods in situations where the methods would be of genuine beneﬁt\nor complain that the concepts and notation used by the tools are compli-\ncated and unnatural. This is unfortunate since the underlying mathematics\nis generally quite simple, certainly no more diﬃcult than the concepts from\nmathematical analysis that every calculus student is expected to learn.\nLogic in Computer Science by Huth and Ryan is an exceptional book.\nI was amazed when I looked through it for the ﬁrst time. In addition to\npropositional and predicate logic, it has a particularly thorough treatment\nof temporal logic and model checking. In fact, the book is quite remarkable\nin how much of this material it is able to cover: linear and branching time\ntemporal logic, explicit state model checking, fairness, the basic ﬁxpoint\nix\nx\nForeword to the first edition\ntheorems for computation tree logic (CTL), even binary decision diagrams\nand symbolic model checking. Moreover, this material is presented at a level\nthat is accessible to undergraduate and beginning graduate students. Nu-\nmerous problems and examples are provided to help students master the\nmaterial in the book. Since both Huth and Ryan are active researchers in\nlogics of programs and program veriﬁcation, they write with considerable\nauthority.\nIn summary, the material in this book is up-to-date, practical, and ele-\ngantly presented. The book is a wonderful example of what a modern text\non logic for computer science should be like. I recommend it to the reader\nwith greatest enthusiasm and predict that the book will be an enormous\nsuccess.\n(This foreword is re-printed in the second edition with its author’s permis-\nsion.)",
    "on logic for computer science should be like. I recommend it to the reader\nwith greatest enthusiasm and predict that the book will be an enormous\nsuccess.\n(This foreword is re-printed in the second edition with its author’s permis-\nsion.)\nPreface to the second edition\nOur motivation for (re)writing this book\nOne of the leitmotifs of writing the ﬁrst edition of our book was the obser-\nvation that most logics used in the design, speciﬁcation and veriﬁcation of\ncomputer systems fundamentally deal with a satisfaction relation\nM ⊨φ\nwhere M is some sort of situation or model of a system, and φ is a speciﬁ-\ncation, a formula of that logic, expressing what should be true in situation\nM. At the heart of this set-up is that one can often specify and implement\nalgorithms for computing ⊨. We developed this theme for propositional,\nﬁrst-order, temporal, modal, and program logics. Based on the encourag-\ning feedback received from ﬁve continents we are pleased to hereby present\nthe second edition of this text which means to preserve and improve on the\noriginal intent of the ﬁrst edition.\nWhat’s new and what’s gone\nChapter 1 now discusses the design, correctness, and complexity of a SAT\nsolver (a marking algorithm similar to St˚almarck’s method [SS90]) for full\npropositional logic.\nChapter 2 now contains basic results from model theory (Compactness\nTheorem and L¨owenheim–Skolem Theorem); a section on the transitive clo-\nsure and the expressiveness of existential and universal second-order logic;\nand a section on the use of the object modelling language Alloy and its anal-\nyser for specifying and exploring under-speciﬁed ﬁrst-order logic models with\nrespect to properties written in ﬁrst-order logic with transitive closure. The\nAlloy language is executable which makes such exploration interactive and\nformal.\nxi\nxii\nPreface to the second edition\nChapter 3 has been completely restructured. It now begins with a discus-",
    "respect to properties written in ﬁrst-order logic with transitive closure. The\nAlloy language is executable which makes such exploration interactive and\nformal.\nxi\nxii\nPreface to the second edition\nChapter 3 has been completely restructured. It now begins with a discus-\nsion of linear-time temporal logic; features the open-source NuSMV model-\nchecking tool throughout; and includes a discussion on planning problems,\nmore material on the expressiveness of temporal logics, and new modelling\nexamples.\nChapter 4 contains more material on total correctness proofs and a new\nsection on the programming-by-contract paradigm of verifying program cor-\nrectness.\nChapters 5 and 6 have also been revised, with many small alterations and\ncorrections.\nThe interdependence of chapters and prerequisites\nThe book requires that students know the basics of elementary arithmetic\nand naive set theoretic concepts and notation. The core material of Chap-\nter 1 (everything except Sections 1.4.3 to 1.6.2) is essential for all of the\nchapters that follow. Other than that, only Chapter 6 depends on Chapter 3\nand a basic understanding of the static scoping rules covered in Chapter 2 –\nalthough one may easily cover Sections 6.1 and 6.2 without having done\nChapter 3 at all. Roughly, the interdependence diagram of chapters is\n1\n4\n2\n5\n3\n6\nWWW page\nThis book is supported by a Web page, which contains a list of errata;\ntext ﬁles for all the program code; ancillary technical material and links;\nall the ﬁgures; an interactive tutor based on multiple-choice questions;\nand details of how instructors can obtain the solutions to exercises in\nthis book which are marked with a ∗. The URL for the book’s page\nis www.cs.bham.ac.uk/research/lics/. See also www.cambridge.org/\n052154310x\nAcknowledgements\nMany people have, directly or indirectly, assisted us in writing this book.\nDavid Schmidt kindly provided serveral exercises for Chapter 4. Krysia\nBroda has pointed out some typographical errors and she and the other",
    "052154310x\nAcknowledgements\nMany people have, directly or indirectly, assisted us in writing this book.\nDavid Schmidt kindly provided serveral exercises for Chapter 4. Krysia\nBroda has pointed out some typographical errors and she and the other\nauthors of [BEKV94] have allowed us to use some exercises from that book.\nWe have also borrowed exercises or examples from [Hod77] and [FHMV95].\nSusan Eisenbach provided a ﬁrst description of the Package Dependency\nSystem that we model in Alloy in Chapter 2. Daniel Jackson make very\nhelpful comments on versions of that section. Zena Matilde Ariola, Josh\nHodas, Jan Komorowski, Sergey Kotov, Scott A. Smolka and Steve Vickers\nhave corresponded with us about this text; their comments are appreciated.\nMatt Dwyer and John Hatcliﬀmade useful comments on drafts of Chap-\nter 3. Kevin Lucas provided insightful comments on the content of Chapter\n6, and notiﬁed us of numerous typographical errors in several drafts of the\nbook. Achim Jung read several chapters and gave useful feedback.\nAdditionally, a number of people read and provided useful comments on\nseveral chapters, including Moti Ben-Ari, Graham Clark, Christian Haack,\nAnthony Hook, Roberto Segala, Alan Sexton and Allen Stoughton. Numer-\nous students at Kansas State University and the University of Birmingham\nhave given us feedback of various kinds, which has inﬂuenced our choice and\npresentation of the topics. We acknowledge Paul Taylor’s LATEX package for\nproof boxes. About half a dozen anonymous referees made critical, but con-\nstructive, comments which helped to improve this text in various ways. In\nspite of these contributions, there may still be errors in the book, and we\nalone must take responsibility for those.\nAdded for second edition\nMany people have helped improve this text by pointing out typos and\nmaking other useful comments after the publication date. Among them,\nxiii\nxiv\nAcknowledgements\nwe mention Wolfgang Ahrendt, Yasuhiro Ajiro, Torben Amtoft, Stephan",
    "alone must take responsibility for those.\nAdded for second edition\nMany people have helped improve this text by pointing out typos and\nmaking other useful comments after the publication date. Among them,\nxiii\nxiv\nAcknowledgements\nwe mention Wolfgang Ahrendt, Yasuhiro Ajiro, Torben Amtoft, Stephan\nAndrei, Bernhard Beckert, Jonathan Brown, James Caldwell, Ruchira Datta,\nAmy Felty, Dimitar Guelev, Hirotsugu Kakugawa, Kamran Kashef, Markus\nKr¨otzsch, Jagun Kwon, Ranko Lazic, David Makinson, Alexander Miczo,\nAart Middeldorp, Robert Morelli, Prakash Panangaden, Aileen Paraguya,\nFrank Pfenning, Shekhar Pradhan, Koichi Takahashi, Kazunori Ueda,\nHiroshi Watanabe, Fuzhi Wang and Reinhard Wilhelm.\n1\nPropositional logic\nThe aim of logic in computer science is to develop languages to model the\nsituations we encounter as computer science professionals, in such a way\nthat we can reason about them formally. Reasoning about situations means\nconstructing arguments about them; we want to do this formally, so that\nthe arguments are valid and can be defended rigorously, or executed on a\nmachine.\nConsider the following argument:\nExample 1.1 If the train arrives late and there are no taxis at the station,\nthen John is late for his meeting. John is not late for his meeting. The train\ndid arrive late. Therefore, there were taxis at the station.\nIntuitively, the argument is valid, since if we put the ﬁrst sentence and\nthe third sentence together, they tell us that if there are no taxis, then John\nwill be late. The second sentence tells us that he was not late, so it must be\nthe case that there were taxis.\nMuch of this book will be concerned with arguments that have this struc-\nture, namely, that consist of a number of sentences followed by the word\n‘therefore’ and then another sentence. The argument is valid if the sentence\nafter the ‘therefore’ logically follows from the sentences before it. Exactly\nwhat we mean by ‘follows from’ is the subject of this chapter and the next\none.",
    "‘therefore’ and then another sentence. The argument is valid if the sentence\nafter the ‘therefore’ logically follows from the sentences before it. Exactly\nwhat we mean by ‘follows from’ is the subject of this chapter and the next\none.\nConsider another example:\nExample 1.2 If it is raining and Jane does not have her umbrella with her,\nthen she will get wet. Jane is not wet. It is raining. Therefore, Jane has her\numbrella with her.\nThis is also a valid argument. Closer examination reveals that it actually\nhas the same structure as the argument of the previous example! All we have\n1\n2\n1 Propositional logic\ndone is substituted some sentence fragments for others:\nExample 1.1\nExample 1.2\nthe train is late\nit is raining\nthere are taxis at the station\nJane has her umbrella with her\nJohn is late for his meeting\nJane gets wet.\nThe argument in each example could be stated without talking about trains\nand rain, as follows:\nIf p and not q, then r. Not r. p. Therefore, q.\nIn developing logics, we are not concerned with what the sentences really\nmean, but only in their logical structure. Of course, when we apply such\nreasoning, as done above, such meaning will be of great interest.\n1.1 Declarative sentences\nIn order to make arguments rigorous, we need to develop a language in which\nwe can express sentences in such a way that brings out their logical structure.\nThe language we begin with is the language of propositional logic. It is based\non propositions, or declarative sentences which one can, in principle, argue\nas being true or false. Examples of declarative sentences are:\n(1)\nThe sum of the numbers 3 and 5 equals 8.\n(2)\nJane reacted violently to Jack’s accusations.\n(3)\nEvery even natural number >2 is the sum of two prime numbers.\n(4)\nAll Martians like pepperoni on their pizza.\n(5)\nAlbert Camus ´etait un ´ecrivain fran¸cais.\n(6)\nDie W¨urde des Menschen ist unantastbar.\nThese sentences are all declarative, because they are in principle capable of",
    "(3)\nEvery even natural number >2 is the sum of two prime numbers.\n(4)\nAll Martians like pepperoni on their pizza.\n(5)\nAlbert Camus ´etait un ´ecrivain fran¸cais.\n(6)\nDie W¨urde des Menschen ist unantastbar.\nThese sentences are all declarative, because they are in principle capable of\nbeing declared ‘true’, or ‘false’. Sentence (1) can be tested by appealing to\nbasic facts about arithmetic (and by tacitly assuming an Arabic, decimal\nrepresentation of natural numbers). Sentence (2) is a bit more problematic.\nIn order to give it a truth value, we need to know who Jane and Jack are\nand perhaps to have a reliable account from someone who witnessed the\nsituation described. In principle, e.g., if we had been at the scene, we feel\nthat we would have been able to detect Jane’s violent reaction, provided\nthat it indeed occurred in that way. Sentence (3), known as Goldbach’s\nconjecture, seems straightforward on the face of it. Clearly, a fact about\nall even numbers >2 is either true or false. But to this day nobody knows\nwhether sentence (3) expresses a truth or not. It is even not clear whether\nthis could be shown by some ﬁnite means, even if it were true. However, in\n1.1 Declarative sentences\n3\nthis text we will be content with sentences as soon as they can, in principle,\nattain some truth value regardless of whether this truth value reﬂects the\nactual state of aﬀairs suggested by the sentence in question. Sentence (4)\nseems a bit silly, although we could say that if Martians exist and eat pizza,\nthen all of them will either like pepperoni on it or not. (We have to introduce\npredicate logic in Chapter 2 to see that this sentence is also declarative if no\nMartians exist; it is then true.) Again, for the purposes of this text sentence\n(4) will do. Et alors, qu’est-ce qu’on pense des phrases (5) et (6)? Sentences\n(5) and (6) are ﬁne if you happen to read French and German a bit. Thus,\ndeclarative statements can be made in any natural, or artiﬁcial, language.",
    "(4) will do. Et alors, qu’est-ce qu’on pense des phrases (5) et (6)? Sentences\n(5) and (6) are ﬁne if you happen to read French and German a bit. Thus,\ndeclarative statements can be made in any natural, or artiﬁcial, language.\nThe kind of sentences we won’t consider here are non-declarative ones,\nlike\nr Could you please pass me the salt?\nr Ready, steady, go!\nr May fortune come your way.\nPrimarily, we are interested in precise declarative sentences, or statements\nabout the behaviour of computer systems, or programs. Not only do we\nwant to specify such statements but we also want to check whether a given\nprogram, or system, fulﬁls a speciﬁcation at hand. Thus, we need to develop\na calculus of reasoning which allows us to draw conclusions from given as-\nsumptions, like initialised variables, which are reliable in the sense that they\npreserve truth: if all our assumptions are true, then our conclusion ought to\nbe true as well. A much more diﬃcult question is whether, given any true\nproperty of a computer program, we can ﬁnd an argument in our calculus\nthat has this property as its conclusion. The declarative sentence (3) above\nmight illuminate the problematic aspect of such questions in the context of\nnumber theory.\nThe logics we intend to design are symbolic in nature. We translate a cer-\ntain suﬃciently large subset of all English declarative sentences into strings\nof symbols. This gives us a compressed but still complete encoding of declar-\native sentences and allows us to concentrate on the mere mechanics of our\nargumentation. This is important since speciﬁcations of systems or software\nare sequences of such declarative sentences. It further opens up the possibil-\nity of automatic manipulation of such speciﬁcations, a job that computers\njust love to do1. Our strategy is to consider certain declarative sentences as\n1 There is a certain, slightly bitter, circularity in such endeavours: in proving that a certain",
    "ity of automatic manipulation of such speciﬁcations, a job that computers\njust love to do1. Our strategy is to consider certain declarative sentences as\n1 There is a certain, slightly bitter, circularity in such endeavours: in proving that a certain\ncomputer program P satisﬁes a given property, we might let some other computer program Q try\nto ﬁnd a proof that P satisﬁes the property; but who guarantees us that Q satisﬁes the property\nof producing only correct proofs? We seem to run into an inﬁnite regress.\n4\n1 Propositional logic\nbeing atomic, or indecomposable, like the sentence\n‘The number 5 is even.’\nWe assign certain distinct symbols p, q, r, . . ., or sometimes p1, p2, p3, . . . to\neach of these atomic sentences and we can then code up more complex\nsentences in a compositional way. For example, given the atomic sentences\np:\n‘I won the lottery last week.’\nq:\n‘I purchased a lottery ticket.’\nr:\n‘I won last week’s sweepstakes.’\nwe can form more complex sentences according to the rules below:\n¬:\nThe negation of p is denoted by ¬p and expresses ‘I did not win the lottery\nlast week,’ or equivalently ‘It is not true that I won the lottery last week.’\n∨:\nGiven p and r we may wish to state that at least one of them is true: ‘I won the\nlottery last week, or I won last week’s sweepstakes;’ we denote this declarative\nsentence by p ∨r and call it the disjunction of p and r2.\n∧:\nDually, the formula p ∧r denotes the rather fortunate conjunction of p and r:\n‘Last week I won the lottery and the sweepstakes.’\n→:\nLast, but deﬁnitely not least, the sentence ‘If I won the lottery last week,\nthen I purchased a lottery ticket.’ expresses an implication between p and q,\nsuggesting that q is a logical consequence of p. We write p →q for that3. We\ncall p the assumption of p →q and q its conclusion.\nOf course, we are entitled to use these rules of constructing propositions\nrepeatedly. For example, we are now in a position to form the proposition\np ∧q →¬r ∨q",
    "suggesting that q is a logical consequence of p. We write p →q for that3. We\ncall p the assumption of p →q and q its conclusion.\nOf course, we are entitled to use these rules of constructing propositions\nrepeatedly. For example, we are now in a position to form the proposition\np ∧q →¬r ∨q\nwhich means that ‘if p and q then not r or q’. You might have noticed a\npotential ambiguity in this reading. One could have argued that this sentence\nhas the structure ‘p is the case and if q then . . . ’ A computer would require\nthe insertion of brackets, as in\n(p ∧q) →((¬r) ∨q)\n2 Its meaning should not be confused with the often implicit meaning of or in natural language\ndiscourse as either . . . or. In this text or always means at least one of them and should not be\nconfounded with exclusive or which states that exactly one of the two statements holds.\n3 The natural language meaning of ‘if . . . then . . . ’ often implicitly assumes a causal role of\nthe assumption somehow enabling its conclusion. The logical meaning of implication is a bit\ndiﬀerent, though, in the sense that it states the preservation of truth which might happen\nwithout any causal relationship. For example, ‘If all birds can ﬂy, then Bob Dole was never\npresident of the United States of America.’ is a true statement, but there is no known causal\nconnection between the ﬂying skills of penguins and eﬀective campaigning.\n1.2 Natural deduction\n5\nto disambiguate this assertion. However, we humans get annoyed by a pro-\nliferation of such brackets which is why we adopt certain conventions about\nthe binding priorities of these symbols.\nConvention 1.3 ¬ binds more tightly than ∨and ∧, and the latter two\nbind more tightly than →. Implication →is right-associative: expressions of\nthe form p →q →r denote p →(q →r).\n1.2 Natural deduction\nHow do we go about constructing a calculus for reasoning about proposi-\ntions, so that we can establish the validity of Examples 1.1 and 1.2? Clearly,",
    "bind more tightly than →. Implication →is right-associative: expressions of\nthe form p →q →r denote p →(q →r).\n1.2 Natural deduction\nHow do we go about constructing a calculus for reasoning about proposi-\ntions, so that we can establish the validity of Examples 1.1 and 1.2? Clearly,\nwe would like to have a set of rules each of which allows us to draw a con-\nclusion given a certain arrangement of premises.\nIn natural deduction, we have such a collection of proof rules. They al-\nlow us to infer formulas from other formulas. By applying these rules in\nsuccession, we may infer a conclusion from a set of premises.\nLet’s see how this works. Suppose we have a set of formulas4 φ1, φ2,\nφ3, . . . , φn, which we will call premises, and another formula, ψ, which we\nwill call a conclusion. By applying proof rules to the premises, we hope\nto get some more formulas, and by applying more proof rules to those, to\neventually obtain the conclusion. This intention we denote by\nφ1, φ2, . . . , φn ⊢ψ.\nThis expression is called a sequent; it is valid if a proof for it can be found.\nThe sequent for Examples 1.1 and 1.2 is p ∧¬q →r, ¬r, p ⊢q. Construct-\ning such a proof is a creative exercise, a bit like programming. It is not\nnecessarily obvious which rules to apply, and in what order, to obtain the\ndesired conclusion. Additionally, our proof rules should be carefully chosen;\notherwise, we might be able to ‘prove’ invalid patterns of argumentation. For\n4 It is traditional in logic to use Greek letters. Lower-case letters are used to stand for formulas\nand upper-case letters are used for sets of formulas. Here are some of the more commonly used\nGreek letters, together with their pronunciation:\nLower-case\nφ\nphi\nψ\npsi\nχ\nchi\nη\neta\nα\nalpha\nβ\nbeta\nγ\ngamma\nUpper-case\nΦ\nPhi\nΨ\nPsi\nΓ\nGamma\n∆\nDelta\n6\n1 Propositional logic\nexample, we expect that we won’t be able to show the sequent p, q ⊢p ∧¬q.\nFor example, if p stands for ‘Gold is a metal.’ and q for ‘Silver is a metal,’",
    "Lower-case\nφ\nphi\nψ\npsi\nχ\nchi\nη\neta\nα\nalpha\nβ\nbeta\nγ\ngamma\nUpper-case\nΦ\nPhi\nΨ\nPsi\nΓ\nGamma\n∆\nDelta\n6\n1 Propositional logic\nexample, we expect that we won’t be able to show the sequent p, q ⊢p ∧¬q.\nFor example, if p stands for ‘Gold is a metal.’ and q for ‘Silver is a metal,’\nthen knowing these two facts should not allow us to infer that ‘Gold is a\nmetal whereas silver isn’t.’\nLet’s now look at our proof rules. We present about ﬁfteen of them in\ntotal; we will go through them in turn and then summarise at the end of\nthis section.\n1.2.1 Rules for natural deduction\nThe rules for conjunction\nOur ﬁrst rule is called the rule for conjunc-\ntion (∧): and-introduction. It allows us to conclude φ ∧ψ, given that we\nhave already concluded φ and ψ separately. We write this rule as\nφ\nψ\nφ ∧ψ\n∧i.\nAbove the line are the two premises of the rule. Below the line goes the\nconclusion. (It might not yet be the ﬁnal conclusion of our argument;\nwe might have to apply more rules to get there.) To the right of the line,\nwe write the name of the rule; ∧i is read ‘and-introduction’. Notice that we\nhave introduced a ∧(in the conclusion) where there was none before (in the\npremises).\nFor each of the connectives, there is one or more rules to introduce it and\none or more rules to eliminate it. The rules for and-elimination are these\ntwo:\nφ ∧ψ\nφ\n∧e1\nφ ∧ψ\nψ\n∧e2.\n(1.1)\nThe rule ∧e1 says: if you have a proof of φ ∧ψ, then by applying this rule\nyou can get a proof of φ. The rule ∧e2 says the same thing, but allows\nyou to conclude ψ instead. Observe the dependences of these rules: in the\nﬁrst rule of (1.1), the conclusion φ has to match the ﬁrst conjunct of the\npremise, whereas the exact nature of the second conjunct ψ is irrelevant.\nIn the second rule it is just the other way around: the conclusion ψ has to\nmatch the second conjunct ψ and φ can be any formula. It is important\nto engage in this kind of pattern matching before the application of proof\nrules.",
    "In the second rule it is just the other way around: the conclusion ψ has to\nmatch the second conjunct ψ and φ can be any formula. It is important\nto engage in this kind of pattern matching before the application of proof\nrules.\nExample 1.4 Let’s use these rules to prove that p ∧q, r |−q ∧r is valid.\nWe start by writing down the premises; then we leave a gap and write the\n1.2 Natural deduction\n7\nconclusion:\np ∧q\nr\nq ∧r\nThe task of constructing the proof is to ﬁll the gap between the premises\nand the conclusion by applying a suitable sequence of proof rules. In this\ncase, we apply ∧e2 to the ﬁrst premise, giving us q. Then we apply ∧i to this\nq and to the second premise, r, giving us q ∧r. That’s it! We also usually\nnumber all the lines, and write in the justiﬁcation for each line, producing\nthis:\n1\np ∧q\npremise\n2\nr\npremise\n3\nq\n∧e2 1\n4\nq ∧r\n∧i 3, 2\nDemonstrate to yourself that you’ve understood this by trying to show on\nyour own that (p ∧q) ∧r, s ∧t |−q ∧s is valid. Notice that the φ and ψ can\nbe instantiated not just to atomic sentences, like p and q in the example we\njust gave, but also to compound sentences. Thus, from (p ∧q) ∧r we can\ndeduce p ∧q by applying ∧e1, instantiating φ to p ∧q and ψ to r.\nIf we applied these proof rules literally, then the proof above would actu-\nally be a tree with root q ∧r and leaves p ∧q and r, like this:\np ∧q\n∧e2\nq\nr\n∧i\nq ∧r\nHowever, we ﬂattened this tree into a linear presentation which necessitates\nthe use of pointers as seen in lines 3 and 4 above. These pointers allow\nus to recreate the actual proof tree. Throughout this text, we will use the\nﬂattened version of presenting proofs. That way you have to concentrate only\non ﬁnding a proof, not on how to ﬁt a growing tree onto a sheet of paper.\nIf a sequent is valid, there may be many diﬀerent ways of proving it. So if\nyou compare your solution to these exercises with those of others, they need\nnot coincide. The important thing to realise, though, is that any putative",
    "If a sequent is valid, there may be many diﬀerent ways of proving it. So if\nyou compare your solution to these exercises with those of others, they need\nnot coincide. The important thing to realise, though, is that any putative\nproof can be checked for correctness by checking each individual line, starting\nat the top, for the valid application of its proof rule.\n8\n1 Propositional logic\nThe rules of double negation\nIntuitively, there is no diﬀerence be-\ntween a formula φ and its double negation ¬¬φ, which expresses no more\nand nothing less than φ itself. The sentence\n‘It is not true that it does not rain.’\nis just a more contrived way of saying\n‘It rains.’\nConversely, knowing ‘It rains,’ we are free to state this fact in this more\ncomplicated manner if we wish. Thus, we obtain rules of elimination and\nintroduction for double negation:\n¬¬φ\nφ\n¬¬e\nφ\n¬¬φ\n¬¬i.\n(There are rules for single negation on its own, too, which we will see later.)\nExample 1.5 The proof of the sequent p, ¬¬(q ∧r) ⊢¬¬p ∧r below uses\nmost of the proof rules discussed so far:\n1\np\npremise\n2\n¬¬(q ∧r)\npremise\n3\n¬¬p\n¬¬i 1\n4\nq ∧r\n¬¬e 2\n5\nr\n∧e2 4\n6\n¬¬p ∧r\n∧i 3, 5\nExample 1.6 We now prove the sequent (p ∧q) ∧r, s ∧t |−q ∧s which\nyou were invited to prove by yourself in the last section. Please compare\nthe proof below with your solution:\n1\n(p ∧q) ∧r\npremise\n2\ns ∧t\npremise\n3\np ∧q\n∧e1 1\n4\nq\n∧e2 3\n5\ns\n∧e1 2\n6\nq ∧s\n∧i 4, 5\n1.2 Natural deduction\n9\nThe rule for eliminating implication\nThere is one rule to introduce\n→and one to eliminate it. The latter is one of the best known rules of\npropositional logic and is often referred to by its Latin name modus ponens.\nWe will usually call it by its modern name, implies-elimination (sometimes\nalso referred to as arrow-elimination). This rule states that, given φ and\nknowing that φ implies ψ, we may rightfully conclude ψ. In our calculus, we\nwrite this as\nφ\nφ →ψ\nψ\n→e.\nLet us justify this rule by spelling out instances of some declarative sen-\ntences p and q. Suppose that",
    "also referred to as arrow-elimination). This rule states that, given φ and\nknowing that φ implies ψ, we may rightfully conclude ψ. In our calculus, we\nwrite this as\nφ\nφ →ψ\nψ\n→e.\nLet us justify this rule by spelling out instances of some declarative sen-\ntences p and q. Suppose that\np : It rained.\np →q : If it rained, then the street is wet.\nso q is just ‘The street is wet.’ Now, if we know that it rained and if we\nknow that the street is wet in the case that it rained, then we may combine\nthese two pieces of information to conclude that the street is indeed wet.\nThus, the justiﬁcation of the →e rule is a mere application of common sense.\nAnother example from programming is:\np : The value of the program’s input is an integer.\np →q : If the program’s input is an integer, then the program outputs\na boolean.\nAgain, we may put all this together to conclude that our program outputs\na boolean value if supplied with an integer input. However, it is important\nto realise that the presence of p is absolutely essential for the inference\nto happen. For example, our program might well satisfy p →q, but if it\ndoesn’t satisfy p – e.g. if its input is a surname – then we will not be able to\nderive q.\nAs we saw before, the formal parameters φ and the ψ for →e can be\ninstantiated to any sentence, including compound ones:\n1\n¬p ∧q\npremise\n2\n¬p ∧q →r ∨¬p\npremise\n3\nr ∨¬p\n→e 2, 1\n10\n1 Propositional logic\nOf course, we may use any of these rules as often as we wish. For example,\ngiven p, p →q and p →(q →r), we may infer r:\n1\np →(q →r)\npremise\n2\np →q\npremise\n3\np\npremise\n4\nq →r\n→e 1, 3\n5\nq\n→e 2, 3\n6\nr\n→e 4, 5\nBefore turning to implies-introduction, let’s look at a hybrid rule which\nhas the Latin name modus tollens. It is like the →e rule in that it eliminates\nan implication. Suppose that p →q and ¬q are the case. Then, if p holds\nwe can use →e to conclude that q holds. Thus, we then have that q and ¬q\nhold, which is impossible. Therefore, we may infer that p must be false. But",
    "an implication. Suppose that p →q and ¬q are the case. Then, if p holds\nwe can use →e to conclude that q holds. Thus, we then have that q and ¬q\nhold, which is impossible. Therefore, we may infer that p must be false. But\nthis can only mean that ¬p is true. We summarise this reasoning into the\nrule modus tollens, or MT for short:5\nφ →ψ\n¬ψ\n¬φ\nMT.\nAgain, let us see an example of this rule in the natural language setting:\n‘If Abraham Lincoln was Ethiopian, then he was African. Abraham\nLincoln was not African; therefore he was not Ethiopian.’\nExample 1.7 In the following proof of\np →(q →r), p, ¬r ⊢¬q\nwe use several of the rules introduced so far:\n1\np →(q →r)\npremise\n2\np\npremise\n3\n¬r\npremise\n4\nq →r\n→e 1, 2\n5\n¬q\nMT 4, 3\n5 We will be able to derive this rule from other ones later on, but we introduce it here because it\nallows us already to do some pretty slick proofs. You may think of this rule as one on a higher\nlevel insofar as it does not mention the lower-level rules upon which it depends.\n1.2 Natural deduction\n11\nExamples 1.8 Here are two example proofs which combine the rule MT\nwith either ¬¬e or ¬¬i:\n1\n¬p →q\npremise\n2\n¬q\npremise\n3\n¬¬p\nMT 1, 2\n4\np\n¬¬e 3\nproves that the sequent ¬p →q, ¬q ⊢p is valid; and\n1\np →¬q\npremise\n2\nq\npremise\n3\n¬¬q\n¬¬i 2\n4\n¬p\nMT 1, 3\nshows the validity of the sequent p →¬q, q ⊢¬p.\nNote that the order of applying double negation rules and MT is diﬀerent\nin these examples; this order is driven by the structure of the particular\nsequent whose validity one is trying to show.\nThe rule implies introduction\nThe rule MT made it possible for us to\nshow that p →q, ¬q ⊢¬p is valid. But the validity of the sequent p →q ⊢\n¬q →¬p seems just as plausible. That sequent is, in a certain sense, saying\nthe same thing. Yet, so far we have no rule which builds implications that\ndo not already occur as premises in our proofs. The mechanics of such a rule\nare more involved than what we have seen so far. So let us proceed with",
    "¬q →¬p seems just as plausible. That sequent is, in a certain sense, saying\nthe same thing. Yet, so far we have no rule which builds implications that\ndo not already occur as premises in our proofs. The mechanics of such a rule\nare more involved than what we have seen so far. So let us proceed with\ncare. Let us suppose that p →q is the case. If we temporarily assume that\n¬q holds, we can use MT to infer ¬p. Thus, assuming p →q we can show\nthat ¬q implies ¬p; but the latter we express symbolically as ¬q →¬p. To\nsummarise, we have found an argumentation for p →q ⊢¬q →¬p:\n1\np →q\npremise\n2\n¬q\nassumption\n3\n¬p\nMT 1, 2\n4\n¬q →¬p\n→i 2−3\nThe box in this proof serves to demarcate the scope of the temporary as-\nsumption ¬q. What we are saying is: let’s make the assumption of ¬q. To\n12\n1 Propositional logic\ndo this, we open a box and put ¬q at the top. Then we continue applying\nother rules as normal, for example to obtain ¬p. But this still depends on\nthe assumption of ¬q, so it goes inside the box. Finally, we are ready to\napply →i. It allows us to conclude ¬q →¬p, but that conclusion no longer\ndepends on the assumption ¬q. Compare this with saying that ‘If you are\nFrench, then you are European.’ The truth of this sentence does not depend\non whether anybody is French or not. Therefore, we write the conclusion\n¬q →¬p outside the box.\nThis works also as one would expect if we think of p →q as a type of a\nprocedure. For example, p could say that the procedure expects an integer\nvalue x as input and q might say that the procedure returns a boolean value\ny as output. The validity of p →q amounts now to an assume-guarantee\nassertion: if the input is an integer, then the output is a boolean. This\nassertion can be true about a procedure while that same procedure could\ncompute strange things or crash in the case that the input is not an in-\nteger. Showing p →q using the rule →i is now called type checking, an\nimportant topic in the construction of compilers for typed programming",
    "assertion can be true about a procedure while that same procedure could\ncompute strange things or crash in the case that the input is not an in-\nteger. Showing p →q using the rule →i is now called type checking, an\nimportant topic in the construction of compilers for typed programming\nlanguages.\nWe thus formulate the rule →i as follows:\nφ\n...\nψ\nφ →ψ\n→i.\nIt says: in order to prove φ →ψ, make a temporary assumption of φ and then\nprove ψ. In your proof of ψ, you can use φ and any of the other formulas\nsuch as premises and provisional conclusions that you have made so far.\nProofs may nest boxes or open new boxes after old ones have been closed.\nThere are rules about which formulas can be used at which points in the\nproof. Generally, we can only use a formula φ in a proof at a given point if\nthat formula occurs prior to that point and if no box which encloses that\noccurrence of φ has been closed already.\nThe line immediately following a closed box has to match the pattern\nof the conclusion of the rule that uses the box. For implies-introduction,\nthis means that we have to continue after the box with φ →ψ, where φ\nwas the ﬁrst and ψ the last formula of that box. We will encounter two\nmore proof rules involving proof boxes and they will require similar pattern\nmatching.\n1.2 Natural deduction\n13\nExample 1.9 Here is another example of a proof using →i:\n1\n¬q →¬p\npremise\n2\np\nassumption\n3\n¬¬p\n¬¬i 2\n4\n¬¬q\nMT 1, 3\n5\np →¬¬q\n→i 2−4\nwhich veriﬁes the validity of the sequent ¬q →¬p ⊢p →¬¬q. Notice that\nwe could apply the rule MT to formulas occurring in or above the box: at\nline 4, no box has been closed that would enclose line 1 or 3.\nAt this point it is instructive to consider the one-line argument\n1\np\npremise\nwhich demonstrates p ⊢p. The rule →i (with conclusion φ →ψ) does not\nprohibit the possibility that φ and ψ coincide. They could both be instanti-\nated to p. Therefore we may extend the proof above to\n1\np\nassumption\n2\np →p\n→i 1 −1",
    "1\np\npremise\nwhich demonstrates p ⊢p. The rule →i (with conclusion φ →ψ) does not\nprohibit the possibility that φ and ψ coincide. They could both be instanti-\nated to p. Therefore we may extend the proof above to\n1\np\nassumption\n2\np →p\n→i 1 −1\nWe write ⊢p →p to express that the argumentation for p →p does not\ndepend on any premises at all.\nDeﬁnition 1.10 Logical formulas φ with valid sequent ⊢φ are theorems.\nExample 1.11 Here is an example of a theorem whose proof utilises most\nof the rules introduced so far:\n1\nq →r\nassumption\n2\n¬q →¬p\nassumption\n3\np\nassumption\n4\n¬¬p\n¬¬i 3\n5\n¬¬q\nMT 2, 4\n6\nq\n¬¬e 5\n7\nr\n→e 1, 6\n8\np →r\n→i 3−7\n9\n(¬q →¬p) →(p →r)\n→i 2−8\n10\n(q →r) →((¬q →¬p) →(p →r))\n→i 1−9\n14\n1 Propositional logic\nq →r\n→\n→\n→\n¬q →¬p\nr\np\nFigure 1.1. Part of the structure of the formula (q →r) →((¬q →¬p) →\n(p →r)) to show how it determines the proof structure.\nTherefore the sequent\n⊢(q →r) →((¬q →¬p) →(p →r)) is valid,\nshowing that (q →r) →((¬q →¬p) →(p →r)) is another theorem.\nRemark 1.12 Indeed, this example indicates that we may transform any\nproof of φ1, φ2, . . . , φn ⊢ψ in such a way into a proof of the theorem\n⊢φ1 →(φ2 →(φ3 →(· · · →(φn →ψ) . . . )))\nby ‘augmenting’ the previous proof with n lines of the rule →i applied to\nφn, φn−1,. . . , φ1 in that order.\nThe nested boxes in the proof of Example 1.11 reveal a pattern of using\nelimination rules ﬁrst, to deconstruct assumptions we have made, and then\nintroduction rules to construct our ﬁnal conclusion. More diﬃcult proofs\nmay involve several such phases.\nLet us dwell on this important topic for a while. How did we come up\nwith the proof above? Parts of it are determined by the structure of the for-\nmulas we have, while other parts require us to be creative. Consider the log-\nical structure of (q →r) →((¬q →¬p) →(p →r)) schematically depicted\nin Figure 1.1. The formula is overall an implication since →is the root of\nthe tree in Figure 1.1. But the only way to build an implication is by means\n1.2 Natural deduction\n15",
    "ical structure of (q →r) →((¬q →¬p) →(p →r)) schematically depicted\nin Figure 1.1. The formula is overall an implication since →is the root of\nthe tree in Figure 1.1. But the only way to build an implication is by means\n1.2 Natural deduction\n15\nof the rule →i. Thus, we need to state the assumption of that implication\nas such (line 1) and have to show its conclusion (line 9). If we managed\nto do that, then we know how to end the proof in line 10. In fact, as we\nalready remarked, this is the only way we could have ended it. So essentially\nlines 1, 9 and 10 are completely determined by the structure of the formula;\nfurther, we have reduced the problem to ﬁlling the gaps in between lines 1\nand 9. But again, the formula in line 9 is an implication, so we have only\none way of showing it: assuming its premise in line 2 and trying to show\nits conclusion in line 8; as before, line 9 is obtained by →i. The formula\np →r in line 8 is yet another implication. Therefore, we have to assume p in\nline 3 and hope to show r in line 7, then →i produces the desired result in\nline 8.\nThe remaining question now is this: how can we show r, using the three\nassumptions in lines 1–3? This, and only this, is the creative part of this\nproof. We see the implication q →r in line 1 and know how to get r (using\n→e) if only we had q. So how could we get q? Well, lines 2 and 3 almost look\nlike a pattern for the MT rule, which would give us ¬¬q in line 5; the latter\nis quickly changed to q in line 6 via ¬¬e. However, the pattern for MT does\nnot match right away, since it requires ¬¬p instead of p. But this is easily\naccomplished via ¬¬i in line 4.\nThe moral of this discussion is that the logical structure of the formula\nto be shown tells you a lot about the structure of a possible proof and\nit is deﬁnitely worth your while to exploit that information in trying to\nprove sequents. Before ending this section on the rules for implication,",
    "The moral of this discussion is that the logical structure of the formula\nto be shown tells you a lot about the structure of a possible proof and\nit is deﬁnitely worth your while to exploit that information in trying to\nprove sequents. Before ending this section on the rules for implication,\nlet’s look at some more examples (this time also involving the rules for\nconjunction).\nExample 1.13 Using the rule ∧i, we can prove the validity of the sequent\np ∧q →r ⊢p →(q →r):\n1\np ∧q →r\npremise\n2\np\nassumption\n3\nq\nassumption\n4\np ∧q\n∧i 2, 3\n5\nr\n→e 1, 4\n6\nq →r\n→i 3−5\n7\np →(q →r)\n→i 2−6\n16\n1 Propositional logic\nExample 1.14 Using the two elimination rules ∧e1 and ∧e2, we can show\nthat the ‘converse’ of the sequent above is valid, too:\n1\np →(q →r)\npremise\n2\np ∧q\nassumption\n3\np\n∧e1 2\n4\nq\n∧e2 2\n5\nq →r\n→e 1, 3\n6\nr\n→e 5, 4\n7\np ∧q →r\n→i 2−6\nThe validity of p →(q →r) ⊢p ∧q →r and p ∧q →r ⊢p →(q →r)\nmeans that these two formulas are equivalent in the sense that we can prove\none from the other. We denote this by\np ∧q →r ⊣⊢p →(q →r).\nSince there can be only one formula to the right of ⊢, we observe that each\ninstance of ⊣⊢can only relate two formulas to each other.\nExample 1.15 Here is an example of a proof that uses introduction and\nelimination rules for conjunction; it shows the validity of the sequent p →\nq ⊢p ∧r →q ∧r:\n1\np →q\npremise\n2\np ∧r\nassumption\n3\np\n∧e1 2\n4\nr\n∧e2 2\n5\nq\n→e 1, 3\n6\nq ∧r\n∧i 5, 4\n7\np ∧r →q ∧r\n→i 2−6\nThe rules for disjunction\nThe rules for disjunction are diﬀerent in spirit\nfrom those for conjunction. The case for conjunction was concise and clear:\nproofs of φ ∧ψ are essentially nothing but a concatenation of a proof of φ and\na proof of ψ, plus an additional line invoking ∧i. In the case of disjunctions,\nhowever, it turns out that the introduction of disjunctions is by far easier\nto grasp than their elimination. So we begin with the rules ∨i1 and ∨i2.\nFrom the premise φ we can infer that ‘φ or ψ’ holds, for we already know\n1.2 Natural deduction\n17",
    "however, it turns out that the introduction of disjunctions is by far easier\nto grasp than their elimination. So we begin with the rules ∨i1 and ∨i2.\nFrom the premise φ we can infer that ‘φ or ψ’ holds, for we already know\n1.2 Natural deduction\n17\nthat φ holds. Note that this inference is valid for any choice of ψ. By the\nsame token, we may conclude ‘φ or ψ’ if we already have ψ. Similarly, that\ninference works for any choice of φ. Thus, we arrive at the proof rules\nφ\nφ ∨ψ\n∨i1\nψ\nφ ∨ψ\n∨i2.\nSo if p stands for ‘Agassi won a gold medal in 1996.’ and q denotes the\nsentence ‘Agassi won Wimbledon in 1996.’ then p ∨q is the case because p\nis true, regardless of the fact that q is false. Naturally, the constructed dis-\njunction depends upon the assumptions needed in establishing its respective\ndisjunct p or q.\nNow let’s consider or-elimination. How can we use a formula of the form\nφ ∨ψ in a proof? Again, our guiding principle is to disassemble assumptions\ninto their basic constituents so that the latter may be used in our argumen-\ntation such that they render our desired conclusion. Let us imagine that we\nwant to show some proposition χ by assuming φ ∨ψ. Since we don’t know\nwhich of φ and ψ is true, we have to give two separate proofs which we need\nto combine into one argument:\n1.\nFirst, we assume φ is true and have to come up with a proof of χ.\n2.\nNext, we assume ψ is true and need to give a proof of χ as well.\n3.\nGiven these two proofs, we can infer χ from the truth of φ ∨ψ, since our case\nanalysis above is exhaustive.\nTherefore, we write the rule ∨e as follows:\nφ ∨ψ\nφ\n...\nχ\nψ\n...\nχ\nχ\n∨e.\nIt is saying that: if φ ∨ψ is true and – no matter whether we assume φ or\nwe assume ψ – we can get a proof of χ, then we are entitled to deduce χ\nanyway. Let’s look at a proof that p ∨q ⊢q ∨p is valid:\n1\np ∨q\npremise\n2\np\nassumption\n3\nq ∨p\n∨i2 2\n4\nq\nassumption\n5\nq ∨p\n∨i1 4\n6\nq ∨p\n∨e 1, 2−3, 4−5\n18\n1 Propositional logic\nHere are some points you need to remember about applying the ∨e rule.",
    "anyway. Let’s look at a proof that p ∨q ⊢q ∨p is valid:\n1\np ∨q\npremise\n2\np\nassumption\n3\nq ∨p\n∨i2 2\n4\nq\nassumption\n5\nq ∨p\n∨i1 4\n6\nq ∨p\n∨e 1, 2−3, 4−5\n18\n1 Propositional logic\nHere are some points you need to remember about applying the ∨e rule.\nr For it to be a sound argument we have to make sure that the conclusions in each\nof the two cases (the χ in the rule) are actually the same formula.\nr The work done by the rule ∨e is the combining of the arguments of the two cases\ninto one.\nr In each case you may not use the temporary assumption of the other case, unless\nit is something that has already been shown before those case boxes began.\nr The invocation of rule ∨e in line 6 lists three things: the line in which the\ndisjunction appears (1), and the location of the two boxes for the two cases (2–3\nand 4–5).\nIf we use φ ∨ψ in an argument where it occurs only as an assumption or\na premise, then we are missing a certain amount of information: we know\nφ, or ψ, but we don’t know which one of the two it is. Thus, we have\nto make a solid case for each of the two possibilities φ or ψ; this resem-\nbles the behaviour of a CASE or IF statement found in most programming\nlanguages.\nExample 1.16 Here is a more complex example illustrating these points.\nWe prove that the sequent q →r ⊢p ∨q →p ∨r is valid:\n1\nq →r\npremise\n2\np ∨q\nassumption\n3\np\nassumption\n4\np ∨r\n∨i1 3\n5\nq\nassumption\n6\nr\n→e 1, 5\n7\np ∨r\n∨i2 6\n8\np ∨r\n∨e 2, 3−4, 5−7\n9\np ∨q →p ∨r\n→i 2−8\nNote that the propositions in lines 4, 7 and 8 coincide, so the application of\n∨e is legitimate.\nWe give some more example proofs which use the rules ∨e, ∨i1 and ∨i2.\nExample 1.17 Proving the validity of the sequent (p ∨q) ∨r ⊢p ∨(q ∨r)\nis surprisingly long and seemingly complex. But this is to be expected, since\n1.2 Natural deduction\n19\nthe elimination rules break (p ∨q) ∨r up into its atomic constituents p, q\nand r, whereas the introduction rules then built up the formula p ∨(q ∨r).\n1\n(p ∨q) ∨r\npremise\n2\n(p ∨q)\nassumption\n3\np\nassumption\n4",
    "1.2 Natural deduction\n19\nthe elimination rules break (p ∨q) ∨r up into its atomic constituents p, q\nand r, whereas the introduction rules then built up the formula p ∨(q ∨r).\n1\n(p ∨q) ∨r\npremise\n2\n(p ∨q)\nassumption\n3\np\nassumption\n4\np ∨(q ∨r)\n∨i1 3\n5\nq\nassumption\n6\nq ∨r\n∨i1 5\n7\np ∨(q ∨r)\n∨i2 6\n8\np ∨(q ∨r)\n∨e 2, 3−4, 5−7\n9\nr\nassumption\n10\nq ∨r\n∨i2 9\n11\np ∨(q ∨r)\n∨i2 10\n12\np ∨(q ∨r)\n∨e 1, 2−8, 9−11\nExample 1.18 From boolean algebra, or circuit theory, you may know that\ndisjunctions distribute over conjunctions. We are now able to prove this in\nnatural deduction. The following proof:\n1\np ∧(q ∨r)\npremise\n2\np\n∧e1 1\n3\nq ∨r\n∧e2 1\n4\nq\nassumption\n5\np ∧q\n∧i 2, 4\n6\n(p ∧q) ∨(p ∧r)\n∨i1 5\n7\nr\nassumption\n8\np ∧r\n∧i 2, 7\n9\n(p ∧q) ∨(p ∧r)\n∨i2 8\n10\n(p ∧q) ∨(p ∧r)\n∨e 3, 4−6, 7−9\nveriﬁes the validity of the sequent p ∧(q ∨r) ⊢(p ∧q) ∨(p ∧r) and you\nare encouraged to show the validity of the ‘converse’ (p ∧q) ∨(p ∧r) ⊢p ∧\n(q ∨r) yourself.\n20\n1 Propositional logic\nA ﬁnal rule is required in order to allow us to conclude a box with a for-\nmula which has already appeared earlier in the proof. Consider the sequent\n⊢p →(q →p), whose validity may be proved as follows:\n1\np\nassumption\n2\nq\nassumption\n3\np\ncopy 1\n4\nq →p\n→i 2−3\n5\np →(q →p)\n→i 1−4\nThe rule ‘copy’ allows us to repeat something that we know already. We need\nto do this in this example, because the rule →i requires that we end the inner\nbox with p. The copy rule entitles us to copy formulas that appeared before,\nunless they depend on temporary assumptions whose box has already been\nclosed. Though a little inelegant, this additional rule is a small price to pay\nfor the freedom of being able to use premises, or any other ‘visible’ formulas,\nmore than once.\nThe rules for negation\nWe have seen the rules ¬¬i and ¬¬e, but we\nhaven’t seen any rules that introduce or eliminate single negations. These\nrules involve the notion of contradiction. This detour is to be expected since",
    "more than once.\nThe rules for negation\nWe have seen the rules ¬¬i and ¬¬e, but we\nhaven’t seen any rules that introduce or eliminate single negations. These\nrules involve the notion of contradiction. This detour is to be expected since\nour reasoning is concerned about the inference, and therefore the preserva-\ntion, of truth. Hence, there cannot be a direct way of inferring ¬φ, given\nφ.\nDeﬁnition 1.19 Contradictions are expressions of the form φ ∧¬φ or ¬φ ∧\nφ, where φ is any formula.\nExamples of such contradictions are r ∧¬r, (p →q) ∧¬(p →q) and ¬(r ∨\ns →q) ∧(r ∨s →q). Contradictions are a very important notion in logic.\nAs far as truth is concerned, they are all equivalent; that means we should\nbe able to prove the validity of\n¬(r ∨s →q) ∧(r ∨s →q) ⊣⊢(p →q) ∧¬(p →q)\n(1.2)\nsince both sides are contradictions. We’ll be able to prove this later, when\nwe have introduced the rules for negation.\nIndeed, it’s not just that contradictions can be derived from contradic-\ntions; actually, any formula can be derived from a contradiction. This can be\n1.2 Natural deduction\n21\nconfusing when you ﬁrst encounter it; why should we endorse the argument\np ∧¬p ⊢q, where\np : The moon is made of green cheese.\nq : I like pepperoni on my pizza.\nconsidering that our taste in pizza doesn’t have anything to do with the\nconstitution of the moon? On the face of it, such an endorsement may seem\nabsurd. Nevertheless, natural deduction does have this feature that any for-\nmula can be derived from a contradiction and therefore it makes this argu-\nment valid. The reason it takes this stance is that ⊢tells us all the things\nwe may infer, provided that we can assume the formulas to the left of it.\nThis process does not care whether such premises make any sense. This has\nat least the advantage that we can match ⊢to checks based on semantic\nintuitions which we formalise later by using truth tables: if all the premises\ncompute to ‘true’, then the conclusion must compute ‘true’ as well. In partic-",
    "at least the advantage that we can match ⊢to checks based on semantic\nintuitions which we formalise later by using truth tables: if all the premises\ncompute to ‘true’, then the conclusion must compute ‘true’ as well. In partic-\nular, this is not a constraint in the case that one of the premises is (always)\nfalse.\nThe fact that ⊥can prove anything is encoded in our calculus by the\nproof rule bottom-elimination:\n⊥\nφ\n⊥e.\nThe fact that ⊥itself represents a contradiction is encoded by the proof rule\nnot-elimination:\nφ\n¬φ\n⊥\n¬e.\nExample 1.20 We apply these rules to show that ¬p ∨q |−p →q is\nvalid:\n1\n¬p ∨q\n2\n¬p\npremise\n3\np\nassumption\n4\n⊥\n¬e 3, 2\n5\nq\n⊥e 4\n6\np →q\n→i 3−5\nq\npremise\np\nassumption\nq\ncopy 2\np →q\n→i 3−4\n7\np →q\n∨e 1, 2−6\n22\n1 Propositional logic\nNotice how, in this example, the proof boxes for ∨e are drawn side by side\ninstead of on top of each other. It doesn’t matter which way you do it.\nWhat about introducing negations? Well, suppose we make an assumption\nwhich gets us into a contradictory state of aﬀairs, i.e. gets us ⊥. Then our\nassumption cannot be true; so it must be false. This intuition is the basis\nfor the proof rule ¬i:\nφ\n...\n⊥\n¬φ\n¬i.\nExample 1.21 We put these rules in action, demonstrating that the se-\nquent p →q, p →¬q ⊢¬p is valid:\n1\np →q\npremise\n2\np →¬q\npremise\n3\np\nassumption\n4\nq\n→e 1, 3\n5\n¬q\n→e 2, 3\n6\n⊥\n¬e 4, 5\n7\n¬p\n¬i 3−6\nLines 3–6 contain all the work of the ¬i rule. Here is a second example,\nshowing the validity of a sequent, p →¬p ⊢¬p, with a contradictory formula\nas sole premise:\n1\np →¬p\npremise\n2\np\nassumption\n3\n¬p\n→e 1, 2\n4\n⊥\n¬e 2, 3\n5\n¬p\n¬i 2−4\nExample 1.22 We prove that the sequent p →(q →r), p, ¬r |−¬q is valid,\n1.2 Natural deduction\n23\nwithout using the MT rule:\n1\np →(q →r)\npremise\n2\np\npremise\n3\n¬r\npremise\n4\nq\nassumption\n5\nq →r\n→e 1, 2\n6\nr\n→e 5, 4\n7\n⊥\n¬e 6, 3\n8\n¬q\n¬i 4−7\nExample 1.23 Finally, we return to the argument of Examples 1.1 and 1.2,\nwhich can be coded up by the sequent p ∧¬q →r, ¬r, p |−q whose validity\nwe now prove:\n1\np ∧¬q →r",
    "1\np →(q →r)\npremise\n2\np\npremise\n3\n¬r\npremise\n4\nq\nassumption\n5\nq →r\n→e 1, 2\n6\nr\n→e 5, 4\n7\n⊥\n¬e 6, 3\n8\n¬q\n¬i 4−7\nExample 1.23 Finally, we return to the argument of Examples 1.1 and 1.2,\nwhich can be coded up by the sequent p ∧¬q →r, ¬r, p |−q whose validity\nwe now prove:\n1\np ∧¬q →r\npremise\n2\n¬r\npremise\n3\np\npremise\n4\n¬q\nassumption\n5\np ∧¬q\n∧i 3, 4\n6\nr\n→e 1, 5\n7\n⊥\n¬e 6, 2\n8\n¬¬q\n¬i 4−7\n9\nq\n¬¬e 8\n1.2.2 Derived rules\nWhen describing the proof rule modus tollens (MT), we mentioned that it\nis not a primitive rule of natural deduction, but can be derived from some\nof the other rules. Here is the derivation of\nφ →ψ\n¬ψ\n¬φ\nMT\n24\n1 Propositional logic\nfrom →e, ¬e and ¬i:\n1\nφ →ψ\npremise\n2\n¬ψ\npremise\n3\nφ\nassumption\n4\nψ\n→e 1, 3\n5\n⊥\n¬e 4, 2\n6\n¬φ\n¬i 3−5\nWe could now go back through the proofs in this chapter and replace applica-\ntions of MT by this combination of →e, ¬e and ¬i. However, it is convenient\nto think of MT as a shorthand (or a macro).\nThe same holds for the rule\nφ\n¬¬φ\n¬¬i.\nIt can be derived from the rules ¬i and ¬e, as follows:\n1\nφ\npremise\n2\n¬φ\nassumption\n3\n⊥\n¬e 1, 2\n4\n¬¬φ\n¬i 2−3\nThere are (unboundedly) many such derived rules which we could write\ndown. However, there is no point in making our calculus fat and unwieldy;\nand some purists would say that we should stick to a minimum set of rules,\nall of which are independent of each other. We don’t take such a purist view.\nIndeed, the two derived rules we now introduce are extremely useful. You will\nﬁnd that they crop up frequently when doing exercises in natural deduction,\nso it is worth giving them names as derived rules. In the case of the second\none, its derivation from the primitive proof rules is not very obvious.\nThe ﬁrst one has the Latin name reductio ad absurdum. It means ‘reduc-\ntion to absurdity’ and we will simply call it proof by contradiction (PBC\nfor short). The rule says: if from ¬φ we obtain a contradiction, then we are\nentitled to deduce φ:\n¬φ\n...\n⊥\nφ\nPBC.\n1.2 Natural deduction\n25",
    "The ﬁrst one has the Latin name reductio ad absurdum. It means ‘reduc-\ntion to absurdity’ and we will simply call it proof by contradiction (PBC\nfor short). The rule says: if from ¬φ we obtain a contradiction, then we are\nentitled to deduce φ:\n¬φ\n...\n⊥\nφ\nPBC.\n1.2 Natural deduction\n25\nThis rule looks rather similar to ¬i, except that the negation is in a diﬀerent\nplace. This is the clue to how to derive PBC from our basic proof rules.\nSuppose we have a proof of ⊥from ¬φ. By →i, we can transform this into\na proof of ¬φ →⊥and proceed as follows:\n1\n¬φ →⊥\ngiven\n2\n¬φ\nassumption\n3\n⊥\n→e 1, 2\n4\n¬¬φ\n¬i 2−3\n5\nφ\n¬¬e 4\nThis shows that PBC can be derived from →i, ¬i, →e and ¬¬e.\nThe ﬁnal derived rule we consider in this section is arguably the most\nuseful to use in proofs, because its derivation is rather long and complicated,\nso its usage often saves time and eﬀort. It also has a Latin name, tertium\nnon datur; the English name is the law of the excluded middle, or LEM for\nshort. It simply says that φ ∨¬φ is true: whatever φ is, it must be either true\nor false; in the latter case, ¬φ is true. There is no third possibility (hence\nexcluded middle): the sequent ⊢φ ∨¬φ is valid. Its validity is implicit, for\nexample, whenever you write an if-statement in a programming language:\n‘if B {C1} else {C2}’ relies on the fact that B ∨¬B is always true (and\nthat B and ¬B can never be true at the same time). Here is a proof in\nnatural deduction that derives the law of the excluded middle from basic\nproof rules:\n1\n¬(φ ∨¬φ)\nassumption\n2\nφ\nassumption\n3\nφ ∨¬φ\n∨i1 2\n4\n⊥\n¬e 3, 1\n5\n¬φ\n¬i 2−4\n6\nφ ∨¬φ\n∨i2 5\n7\n⊥\n¬e 6, 1\n8\n¬¬(φ ∨¬φ)\n¬i 1−7\n9\nφ ∨¬φ\n¬¬e 8\n26\n1 Propositional logic\nExample 1.24 Using LEM, we show that p →q ⊢¬p ∨q is valid:\n1\np →q\npremise\n2\n¬p ∨p\nLEM\n3\n¬p\nassumption\n4\n¬p ∨q\n∨i1 3\n5\np\nassumption\n6\nq\n→e 1, 5\n7\n¬p ∨q\n∨i2 6\n8\n¬p ∨q\n∨e 2, 3−4, 5−7\nIt can be diﬃcult to decide which instance of LEM would beneﬁt the progress\nof a proof. Can you re-do the example above with q ∨¬q as LEM?",
    "1\np →q\npremise\n2\n¬p ∨p\nLEM\n3\n¬p\nassumption\n4\n¬p ∨q\n∨i1 3\n5\np\nassumption\n6\nq\n→e 1, 5\n7\n¬p ∨q\n∨i2 6\n8\n¬p ∨q\n∨e 2, 3−4, 5−7\nIt can be diﬃcult to decide which instance of LEM would beneﬁt the progress\nof a proof. Can you re-do the example above with q ∨¬q as LEM?\n1.2.3 Natural deduction in summary\nThe proof rules for natural deduction are summarised in Figure 1.2. The\nexplanation of the rules we have given so far in this chapter is declarative;\nwe have presented each rule and justiﬁed it in terms of our intuition about\nthe logical connectives. However, when you try to use the rules yourself,\nyou’ll ﬁnd yourself looking for a more procedural interpretation; what does\na rule do and how do you use it? For example,\nr ∧i says: to prove φ ∧ψ, you must ﬁrst prove φ and ψ separately and then use\nthe rule ∧i.\nr ∧e1 says: to prove φ, try proving φ ∧ψ and then use the rule ∧e1. Actually,\nthis doesn’t sound like very good advice because probably proving φ ∧ψ will\nbe harder than proving φ alone. However, you might ﬁnd that you already have\nφ ∧ψ lying around, so that’s when this rule is useful. Compare this with the\nexample sequent in Example 1.15.\nr ∨i1 says: to prove φ ∨ψ, try proving φ. Again, in general it is harder to prove\nφ than it is to prove φ ∨ψ, so this will usually be useful only if you’ve already\nmanaged to prove φ. For example, if you want to prove q |−p ∨q, you certainly\nwon’t be able simply to use the rule ∨i1, but ∨i2 will work.\nr ∨e has an excellent procedural interpretation. It says: if you have φ ∨ψ, and you\nwant to prove some χ, then try to prove χ from φ and from ψ in turn. (In those\nsubproofs, of course you can use the other prevailing premises as well.)\nr Similarly, →i says, if you want to prove φ →ψ, try proving ψ from φ (and the\nother prevailing premises).\nr ¬i says: to prove ¬φ, prove ⊥from φ (and the other prevailing premises).\n1.2 Natural deduction\n27\nThe basic rules of natural deduction:\nintroduction\nelimination\n∧\nφ\nψ\nφ ∧ψ\n∧i\nφ ∧ψ\nφ\n∧e1\nφ ∧ψ\nψ\n∧e2\n∨\nφ",
    "other prevailing premises).\nr ¬i says: to prove ¬φ, prove ⊥from φ (and the other prevailing premises).\n1.2 Natural deduction\n27\nThe basic rules of natural deduction:\nintroduction\nelimination\n∧\nφ\nψ\nφ ∧ψ\n∧i\nφ ∧ψ\nφ\n∧e1\nφ ∧ψ\nψ\n∧e2\n∨\nφ\nφ ∨ψ\n∨i1\nψ\nφ ∨ψ\n∨i2\nφ ∨ψ\nφ\n...\nχ\nψ\n...\nχ\nχ\n∨e\n→\nφ\n...\nψ\nφ →ψ\n→i\nφ\nφ →ψ\nψ\n→e\n¬\nφ\n...\n⊥\n¬φ\n¬i\nφ\n¬φ\n⊥\n¬e\n⊥\n(no introduction rule for ⊥)\n⊥\nφ\n⊥e\n¬¬\n¬¬φ\nφ\n¬¬e\nSome useful derived rules:\nφ →ψ\n¬ψ\n¬φ\nMT\nφ\n¬¬φ\n¬¬i\n¬φ\n...\n⊥\nφ\nPBC\nφ ∨¬φ\nLEM\nFigure 1.2. Natural deduction rules for propositional logic.\n28\n1 Propositional logic\nAt any stage of a proof, it is permitted to introduce any formula as as-\nsumption, by choosing a proof rule that opens a box. As we saw, natural\ndeduction employs boxes to control the scope of assumptions. When an as-\nsumption is introduced, a box is opened. Discharging assumptions is achieved\nby closing a box according to the pattern of its particular proof rule. It’s\nuseful to make assumptions by opening boxes. But don’t forget you have to\nclose them in the manner prescribed by their proof rule.\nOK, but how do we actually go about constructing a proof?\nGiven a sequent, you write its premises at the top of your page and\nits conclusion at the bottom. Now, you’re trying to ﬁll in the gap,\nwhich involves working simultaneously on the premises (to bring them to-\nwards the conclusion) and on the conclusion (to massage it towards the\npremises).\nLook ﬁrst at the conclusion. If it is of the form φ →ψ, then apply6 the\nrule →i. This means drawing a box with φ at the top and ψ at the bottom.\nSo your proof, which started out like this:\n...\npremises\n...\nφ →ψ\nnow looks like this:\n...\npremises\n...\nφ\nassumption\nψ\nφ →ψ\n→i\nYou still have to ﬁnd a way of ﬁlling in the gap between the φ and the ψ.\nBut you now have an extra formula to work with and you have simpliﬁed\nthe conclusion you are trying to reach.\n6 Except in situations such as p →(q →¬r), p ⊢q →¬r where →e produces a simpler proof.\n1.2 Natural deduction\n29",
    "→i\nYou still have to ﬁnd a way of ﬁlling in the gap between the φ and the ψ.\nBut you now have an extra formula to work with and you have simpliﬁed\nthe conclusion you are trying to reach.\n6 Except in situations such as p →(q →¬r), p ⊢q →¬r where →e produces a simpler proof.\n1.2 Natural deduction\n29\nThe proof rule ¬i is very similar to →i and has the same beneﬁcial eﬀect\non your proof attempt. It gives you an extra premise to work with and\nsimpliﬁes your conclusion.\nAt any stage of a proof, several rules are likely to be applicable. Before\napplying any of them, list the applicable ones and think about which one\nis likely to improve the situation for your proof. You’ll ﬁnd that →i and ¬i\nmost often improve it, so always use them whenever you can. There is no\neasy recipe for when to use the other rules; often you have to make judicious\nchoices.\n1.2.4 Provable equivalence\nDeﬁnition 1.25 Let φ and ψ be formulas of propositional logic. We say\nthat φ and ψ are provably equivalent iﬀ(we write ‘iﬀ’ for ‘if, and only\nif’ in the sequel) the sequents φ ⊢ψ and ψ ⊢φ are valid; that is, there\nis a proof of ψ from φ and another one going the other way around.\nAs seen earlier, we denote that φ and ψ are provably equivalent by\nφ ⊣⊢ψ.\nNote that, by Remark 1.12, we could just as well have deﬁned φ ⊣⊢ψ to\nmean that the sequent ⊢(φ →ψ) ∧(ψ →φ) is valid; it deﬁnes the same\nconcept. Examples of provably equivalent formulas are\n¬(p ∧q) ⊣⊢¬q ∨¬p\n¬(p ∨q) ⊣⊢¬q ∧¬p\np →q ⊣⊢¬q →¬p\np →q ⊣⊢¬p ∨q\np ∧q →p ⊣⊢r ∨¬r\np ∧q →r ⊣⊢p →(q →r).\nThe\nreader\nshould\nprove\nall\nof\nthese\nsix\nequivalences\nin\nnatural\ndeduction.\n1.2.5 An aside: proof by contradiction\nSometimes we can’t prove something directly in the sense of taking apart\ngiven assumptions and reasoning with their constituents in a constructive\nway. Indeed, the proof system of natural deduction, summarised in Fig-\nure 1.2, speciﬁcally allows for indirect proofs that lack a constructive quality:\nfor example, the rule\n¬φ\n...\n⊥\nφ\nPBC\n30",
    "given assumptions and reasoning with their constituents in a constructive\nway. Indeed, the proof system of natural deduction, summarised in Fig-\nure 1.2, speciﬁcally allows for indirect proofs that lack a constructive quality:\nfor example, the rule\n¬φ\n...\n⊥\nφ\nPBC\n30\n1 Propositional logic\nallows us to prove φ by showing that ¬φ leads to a contradiction. Although\n‘classical logicians’ argue that this is valid, logicians of another kind, called\n‘intuitionistic logicians,’ argue that to prove φ you should do it directly,\nrather than by arguing merely that ¬φ is impossible. The two other rules\non which classical and intuitionistic logicians disagree are\nφ ∨¬φ\nLEM\n¬¬φ\nφ\n¬¬e.\nIntuitionistic logicians argue that, to show φ ∨¬φ, you have to show φ,\nor ¬φ. If neither of these can be shown, then the putative truth of the\ndisjunction has no justiﬁcation. Intuitionists reject ¬¬e since we have already\nused this rule to prove LEM and PBC from rules which the intuitionists do\naccept. In the exercises, you are asked to show why the intuitionists also\nreject PBC.\nLet\nus\nlook\nat\na\nproof\nthat\nshows\nup\nthis\ndiﬀerence,\ninvolv-\ning\nreal\nnumbers.\nReal\nnumbers\nare\nﬂoating\npoint\nnumbers\nlike\n23.54721, only some of them might actually be inﬁnitely long such as\n23.138592748500123950734 . . ., with no periodic behaviour after the deci-\nmal point.\nGiven a positive real number a and a natural (whole) number b, we can\ncalculate ab: it is just a times itself, b times, so 22 = 2 · 2 = 4, 23 = 2 · 2 · 2 =\n8 and so on. When b is a real number, we can also deﬁne ab, as follows.\nWe say that a0 def\n= 1 and, for a non-zero rational number k/n, where n ̸= 0,\nwe let ak/n def\n=\nn√\nak where\nn√x is the real number y such that yn = x. From\nreal analysis one knows that any real number b can be approximated by a\nsequence of rational numbers k0/n0, k1/n1, . . . Then we deﬁne ab to be the\nreal number approximated by the sequence ak0/n0, ak1/n1, . . . (In calculus,",
    "=\nn√\nak where\nn√x is the real number y such that yn = x. From\nreal analysis one knows that any real number b can be approximated by a\nsequence of rational numbers k0/n0, k1/n1, . . . Then we deﬁne ab to be the\nreal number approximated by the sequence ak0/n0, ak1/n1, . . . (In calculus,\none can show that this ‘limit’ ab is unique and independent of the choice of\napproximating sequence.) Also, one calls a real number irrational if it can’t\nbe written in the form k/n for some integers k and n ̸= 0. In the exercises\nyou will be asked to ﬁnd a semi-formal proof showing that\n√\n2 is irrational.\nWe now present a proof of a fact about real numbers in the informal style\nused by mathematicians (this proof can be formalised as a natural deduction\nproof in the logic presented in Chapter 2). The fact we prove is:\nTheorem 1.26 There exist irrational numbers a and b such that ab is ra-\ntional.\nProof: We choose b to be\n√\n2 and proceed by a case analysis. Either bb is\nirrational, or it is not. (Thus, our proof uses ∨e on an instance of LEM.)\n1.3 Propositional logic as a formal language\n31\n(i)\nAssume that bb is rational. Then this proof is easy since we can choose irra-\ntional numbers a and b to be\n√\n2 and see that ab is just bb which was assumed\nto be rational.\n(ii)\nAssume that bb is irrational. Then we change our strategy slightly and choose\na to be\n√\n2\n√\n2. Clearly, a is irrational by the assumption of case (ii). But we\nknow that b is irrational (this was known by the ancient Greeks; see the proof\noutline in the exercises). So a and b are both irrational numbers and\nab = (\n√\n2\n√\n2)\n√\n2\n=\n√\n2\n(\n√\n2·\n√\n2) = (\n√\n2)\n2 = 2\nis rational, where we used the law (xy)z = x(y·z).\nSince the two cases above are exhaustive (either bb is irrational, or it isn’t)\nwe have proven the theorem.\n2\nThis proof is perfectly legitimate and mathematicians use arguments like\nthat all the time. The exhaustive nature of the case analysis above rests on",
    "Since the two cases above are exhaustive (either bb is irrational, or it isn’t)\nwe have proven the theorem.\n2\nThis proof is perfectly legitimate and mathematicians use arguments like\nthat all the time. The exhaustive nature of the case analysis above rests on\nthe use of the rule LEM, which we use to prove that either b is rational or it\nis not. Yet, there is something puzzling about it. Surely, we have secured the\nfact that there are irrational numbers a and b such that ab is rational, but\nare we in a position to specify an actual pair of such numbers satisfying this\ntheorem? More precisely, which of the pairs (a, b) above fulﬁls the assertion\nof the theorem, the pair (\n√\n2,\n√\n2), or the pair (\n√\n2\n√\n2,\n√\n2)? Our proof tells\nus nothing about which of them is the right choice; it just says that at least\none of them works.\nThus, the intuitionists favour a calculus containing the introduction and\nelimination rules shown in Figure 1.2 and excluding the rule ¬¬e and the\nderived rules. Intuitionistic logic turns out to have some specialised applica-\ntions in computer science, such as modelling type-inference systems used in\ncompilers or the staged execution of program code; but in this text we stick\nto the full so-called classical logic which includes all the rules.\n1.3 Propositional logic as a formal language\nIn the previous section we learned about propositional atoms and how they\ncan be used to build more complex logical formulas. We were deliberately\ninformal about that, for our main focus was on trying to understand the\nprecise mechanics of the natural deduction rules. However, it should have\nbeen clear that the rules we stated are valid for any formulas we can form, as\nlong as they match the pattern required by the respective rule. For example,\n32\n1 Propositional logic\nthe application of the proof rule →e in\n1\np →q\npremise\n2\np\npremise\n3\nq\n→e 1, 2\nis equally valid if we substitute p with p ∨¬r and q with r →p:\n1\np ∨¬r →(r →p)\npremise\n2\np ∨¬r\npremise\n3\nr →p\n→e 1, 2",
    "long as they match the pattern required by the respective rule. For example,\n32\n1 Propositional logic\nthe application of the proof rule →e in\n1\np →q\npremise\n2\np\npremise\n3\nq\n→e 1, 2\nis equally valid if we substitute p with p ∨¬r and q with r →p:\n1\np ∨¬r →(r →p)\npremise\n2\np ∨¬r\npremise\n3\nr →p\n→e 1, 2\nThis is why we expressed such rules as schemes with Greek symbols stand-\ning for generic formulas. Yet, it is time that we make precise the notion of\n‘any formula we may form.’ Because this text concerns various logics, we will\nintroduce in (1.3) an easy formalism for specifying well-formed formulas. In\ngeneral, we need an unbounded supply of propositional atoms p, q, r, . . ., or\np1, p2, p3, . . . You should not be too worried about the need for inﬁnitely\nmany such symbols. Although we may only need ﬁnitely many of these\npropositions to describe a property of a computer program successfully, we\ncannot specify how many such atomic propositions we will need in any con-\ncrete situation, so having inﬁnitely many symbols at our disposal is a cheap\nway out. This can be compared with the potentially inﬁnite nature of En-\nglish: the number of grammatically correct English sentences is inﬁnite, but\nﬁnitely many such sentences will do in whatever situation you might be in\n(writing a book, attending a lecture, listening to the radio, having a dinner\ndate, . . . ).\nFormulas in our propositional logic should certainly be strings over the\nalphabet {p, q, r, . . . } ∪{p1, p2, p3, . . . } ∪{¬, ∧, ∨, →, (, )}. This is a trivial\nobservation and as such is not good enough for what we are trying to capture.\nFor example, the string (¬)() ∨pq →is a word over that alphabet, yet, it\ndoes not seem to make a lot of sense as far as propositional logic is concerned.\nSo what we have to deﬁne are those strings which we want to call formulas.\nWe call such formulas well-formed.\nDeﬁnition 1.27 The well-formed formulas of propositional logic are those",
    "does not seem to make a lot of sense as far as propositional logic is concerned.\nSo what we have to deﬁne are those strings which we want to call formulas.\nWe call such formulas well-formed.\nDeﬁnition 1.27 The well-formed formulas of propositional logic are those\nwhich we obtain by using the construction rules below, and only those,\nﬁnitely many times:\n1.3 Propositional logic as a formal language\n33\natom: Every propositional atom p, q, r, . . . and p1, p2, p3, . . . is a well-\nformed formula.\n¬: If φ is a well-formed formula, then so is (¬φ).\n∧: If φ and ψ are well-formed formulas, then so is (φ ∧ψ).\n∨: If φ and ψ are well-formed formulas, then so is (φ ∨ψ).\n→: If φ and ψ are well-formed formulas, then so is (φ →ψ).\nIt is most crucial to realize that this deﬁnition is the one a computer would\nexpect and that we did not make use of the binding priorities agreed upon\nin the previous section.\nConvention. In this section we act as if we are a rigorous computer and\nwe call formulas well-formed iﬀthey can be deduced to be so using the\ndeﬁnition above.\nFurther, note that the condition ‘and only those’ in the deﬁnition above\nrules out the possibility of any other means of establishing that formulas are\nwell-formed. Inductive deﬁnitions, like the one of well-formed propositional\nlogic formulas above, are so frequent that they are often given by a deﬁning\ngrammar in Backus Naur form (BNF). In that form, the above deﬁnition\nreads more compactly as\nφ ::= p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ)\n(1.3)\nwhere p stands for any atomic proposition and each occurrence of φ to the\nright of ::= stands for any already constructed formula.\nSo how can we show that a string is a well-formed formula? For example,\nhow do we answer this for φ being\n(((¬p) ∧q) →(p ∧(q ∨(¬r)))) ?\n(1.4)\nSuch reasoning is greatly facilitated by the fact that the grammar in (1.3)\nsatisﬁes the inversion principle, which means that we can invert the process",
    "So how can we show that a string is a well-formed formula? For example,\nhow do we answer this for φ being\n(((¬p) ∧q) →(p ∧(q ∨(¬r)))) ?\n(1.4)\nSuch reasoning is greatly facilitated by the fact that the grammar in (1.3)\nsatisﬁes the inversion principle, which means that we can invert the process\nof building formulas: although the grammar rules allow for ﬁve diﬀerent ways\nof constructing more complex formulas – the ﬁve clauses in (1.3) – there is\nalways a unique clause which was used last. For the formula above, this\nlast operation was an application of the ﬁfth clause, for φ is an implication\nwith the assumption ((¬p) ∧q) and conclusion (p ∧(q ∨(¬r))). By applying\nthe inversion principle to the assumption, we see that it is a conjunction of\n(¬p) and q. The former has been constructed using the second clause and\nis well-formed since p is well-formed by the ﬁrst clause in (1.3). The latter\nis well-formed for the same reason. Similarly, we can apply the inversion\n34\n1 Propositional logic\n→\n∧\n¬\np\nq\n∧\np\n∨\nq\n¬\nr\nFigure 1.3. A parse tree representing a well-formed formula.\nprinciple to the conclusion (p ∧(q ∨(¬r))), inferring that it is indeed well-\nformed. In summary, the formula in (1.4) is well-formed.\nFor us humans, dealing with brackets is a tedious task. The reason\nwe need them is that formulas really have a tree-like structure, although\nwe prefer to represent them in a linear way. In Figure 1.3 you can see the\nparse tree7 of the well-formed formula φ in (1.4). Note how brackets become\nunnecessary in this parse tree since the paths and the branching structure\nof this tree remove any possible ambiguity in interpreting φ. In representing\nφ as a linear string, the branching structure of the tree is retained by the\ninsertion of brackets as done in the deﬁnition of well-formed formulas.\nSo how would you go about showing that a string of symbols ψ is not well-\nformed? At ﬁrst sight, this is a bit trickier since we somehow have to make",
    "φ as a linear string, the branching structure of the tree is retained by the\ninsertion of brackets as done in the deﬁnition of well-formed formulas.\nSo how would you go about showing that a string of symbols ψ is not well-\nformed? At ﬁrst sight, this is a bit trickier since we somehow have to make\nsure that ψ could not have been obtained by any sequence of construction\nrules. Let us look at the formula (¬)() ∨pq →from above. We can decide\nthis matter by being very observant. The string (¬)() ∨pq →contains ¬)\nand ¬ cannot be the rightmost symbol of a well-formed formula (check all\nthe rules to verify this claim!); but the only time we can put a ‘)’ to the right\nof something is if that something is a well-formed formula (again, check all\nthe rules to see that this is so). Thus, (¬)() ∨pq →is not well-formed.\nProbably the easiest way to verify whether some formula φ is well-formed\nis by trying to draw its parse tree. In this way, you can verify that the\n7 We will use this name without explaining it any further and are conﬁdent that you will under-\nstand its meaning through the examples.\n1.3 Propositional logic as a formal language\n35\nformula in (1.4) is well-formed. In Figure 1.3 we see that its parse tree has\n→as its root, expressing that the formula is, at its top level, an implication.\nUsing the grammar clause for implication, it suﬃces to show that the left\nand right subtrees of this root node are well-formed. That is, we proceed in\na top-down fashion and, in this case, successfully. Note that the parse trees\nof well-formed formulas have either an atom as root (and then this is all\nthere is in the tree), or the root contains ¬, ∨, ∧or →. In the case of ¬\nthere is only one subtree coming out of the root. In the cases ∧, ∨or →we\nmust have two subtrees, each of which must behave as just described; this\nis another example of an inductive deﬁnition.\nThinking in terms of trees will help you understand standard notions",
    "there is only one subtree coming out of the root. In the cases ∧, ∨or →we\nmust have two subtrees, each of which must behave as just described; this\nis another example of an inductive deﬁnition.\nThinking in terms of trees will help you understand standard notions\nin logic, for example, the concept of a subformula. Given the well-formed\nformula φ above, its subformulas are just the ones that correspond to the\nsubtrees of its parse tree in Figure 1.3. So we can list all its leaves p, q\n(occurring twice), and r, then (¬p) and ((¬p) ∧q) on the left subtree of →\nand (¬r), (q ∨(¬r)) and ((p ∧(q ∨(¬p)))) on the right subtree of →. The\nwhole tree is a subtree of itself as well. So we can list all nine subformulas\nof φ as\np\nq\nr\n(¬p)\n((¬p) ∧q)\n(¬r)\n(q ∨(¬r))\n((p ∧(q ∨(¬r))))\n(((¬p) ∧q) →(p ∧(q ∨(¬r)))).\nLet us consider the tree in Figure 1.4. Why does it represent a well-formed\nformula? All its leaves are propositional atoms (p twice, q and r), all branch-\ning nodes are logical connectives (¬ twice, ∧, ∨and →) and the numbers\nof subtrees are correct in all those cases (one subtree for a ¬ node and two\nsubtrees for all other non-leaf nodes). How do we obtain the linear represen-\ntation of this formula? If we ignore brackets, then we are seeking nothing but\nthe in-order representation of this tree as a list8. The resulting well-formed\nformula is ((¬(p ∨(q →(¬p)))) ∧r).\n8 The other common ways of ﬂattening trees to lists are preordering and postordering. See any\ntext on binary trees as data structures for further details.\n36\n1 Propositional logic\n∧\nr\n¬\n∨\np\n→\nq\n¬\np\nFigure 1.4. Given: a tree; wanted: its linear representation as a logical\nformula.\nThe tree in Figure 1.21 on page 82, however, does not represent a well-\nformed formula for two reasons. First, the leaf ∧(and a similar argument\napplies to the leaf ¬), the left subtree of the node →, is not a propositional\natom. This could be ﬁxed by saying that we decided to leave the left and",
    "The tree in Figure 1.21 on page 82, however, does not represent a well-\nformed formula for two reasons. First, the leaf ∧(and a similar argument\napplies to the leaf ¬), the left subtree of the node →, is not a propositional\natom. This could be ﬁxed by saying that we decided to leave the left and\nright subtree of that node unspeciﬁed and that we are willing to provide\nthose now. However, the second reason is fatal. The p node is not a leaf\nsince it has a subtree, the node ¬. This cannot make sense if we think of\nthe entire tree as some logical formula. So this tree does not represent a\nwell-formed logical formula.\n1.4 Semantics of propositional logic\n1.4.1 The meaning of logical connectives\nIn the second section of this chapter, we developed a calculus of reasoning\nwhich could verify that sequents of the form φ1, φ2, . . . , φn ⊢ψ are valid,\nwhich means: from the premises φ1, φ2, . . . , φn, we may conclude ψ.\nIn this section we give another account of this relationship between the\npremises φ1, φ2, . . . , φn and the conclusion ψ. To contrast with the sequent\n1.4 Semantics of propositional logic\n37\nabove, we deﬁne a new relationship, written\nφ1, φ2, . . . , φn ⊨ψ.\nThis account is based on looking at the ‘truth values’ of the atomic formu-\nlas in the premises and the conclusion; and at how the logical connectives\nmanipulate these truth values. What is the truth value of a declarative sen-\ntence, like sentence (3) ‘Every even natural number > 2 is the sum of two\nprime numbers’? Well, declarative sentences express a fact about the real\nworld, the physical world we live in, or more abstract ones such as computer\nmodels, or our thoughts and feelings. Such factual statements either match\nreality (they are true), or they don’t (they are false).\nIf we combine declarative sentences p and q with a logical connective, say\n∧, then the truth value of p ∧q is determined by three things: the truth value\nof p, the truth value of q and the meaning of ∧. The meaning of ∧is captured",
    "reality (they are true), or they don’t (they are false).\nIf we combine declarative sentences p and q with a logical connective, say\n∧, then the truth value of p ∧q is determined by three things: the truth value\nof p, the truth value of q and the meaning of ∧. The meaning of ∧is captured\nby the observation that p ∧q is true iﬀp and q are both true; otherwise p ∧q\nis false. Thus, as far as ∧is concerned, it needs only to know whether p and\nq are true, it does not need to know what p and q are actually saying about\nthe world out there. This is also the case for all the other logical connectives\nand is the reason why we can compute the truth value of a formula just by\nknowing the truth values of the atomic propositions occurring in it.\nDeﬁnition 1.28 1.\nThe set of truth values contains two elements T and F, where\nT represents ‘true’ and F represents ‘false’.\n2.\nA valuation or model of a formula φ is an assignment of each propositional atom\nin φ to a truth value.\nExample 1.29 The map which assigns T to q and F to p is a valuation for\np ∨¬q. Please list the remaining three valuations for this formula.\nWe can think of the meaning of ∧as a function of two arguments; each\nargument is a truth value and the result is again such a truth value. We\nspecify this function in a table, called the truth table for conjunction, which\nyou can see in Figure 1.5. In the ﬁrst column, labelled φ, we list all possible\nφ\nψ\nφ ∧ψ\nT\nT\nT\nT\nF\nF\nF\nT\nF\nF\nF\nF\nFigure 1.5. The truth table for conjunction, the logical connective ∧.\n38\n1 Propositional logic\nφ\nψ\nφ ∧ψ\nT\nT\nT\nT\nF\nF\nF\nT\nF\nF\nF\nF\nφ\nψ\nφ ∨ψ\nT\nT\nT\nT\nF\nT\nF\nT\nT\nF\nF\nF\nφ\nψ\nφ →ψ\nT\nT\nT\nT\nF\nF\nF\nT\nT\nF\nF\nT\nφ\n¬φ\nT\nF\nF\nT\n⊤\nT\n⊥\nF\nFigure 1.6. The truth tables for all the logical connectives discussed so far.\ntruth values of φ. Actually we list them twice since we also have to deal\nwith another formula ψ, so the possible number of combinations of truth\nvalues for φ and ψ equals 2 · 2 = 4. Notice that the four pairs of φ and ψ",
    "Figure 1.6. The truth tables for all the logical connectives discussed so far.\ntruth values of φ. Actually we list them twice since we also have to deal\nwith another formula ψ, so the possible number of combinations of truth\nvalues for φ and ψ equals 2 · 2 = 4. Notice that the four pairs of φ and ψ\nvalues in the ﬁrst two columns really exhaust all those possibilities (TT, TF,\nFT and FF). In the third column, we list the result of φ ∧ψ according to the\ntruth values of φ and ψ. So in the ﬁrst line, where φ and ψ have value T,\nthe result is T again. In all other lines, the result is F since at least one of\nthe propositions φ or ψ has value F.\nIn Figure 1.6 you ﬁnd the truth tables for all logical connectives of propo-\nsitional logic. Note that ¬ turns T into F and vice versa. Disjunction is the\nmirror image of conjunction if we swap T and F, namely, a disjunction re-\nturns F iﬀboth arguments are equal to F, otherwise (= at least one of the\narguments equals T) it returns T. The behaviour of implication is not quite\nas intuitive. Think of the meaning of →as checking whether truth is being\npreserved. Clearly, this is not the case when we have T →F, since we infer\nsomething that is false from something that is true. So the second entry\nin the column φ →ψ equals F. On the other hand, T →T obviously pre-\nserves truth, but so do the cases F →T and F →F, because there is no truth\nto be preserved in the ﬁrst place as the assumption of the implication is\nfalse.\nIf you feel slightly uncomfortable with the semantics (= the meaning)\nof →, then it might be good to think of φ →ψ as an abbreviation of the\nformula ¬φ ∨ψ as far as meaning is concerned; these two formulas are very\ndiﬀerent syntactically and natural deduction treats them diﬀerently as well.\nBut using the truth tables for ¬ and ∨you can check that φ →ψ evaluates\n1.4 Semantics of propositional logic\n39\nto T iﬀ¬φ ∨ψ does so. This means that φ →ψ and ¬φ ∨ψ are semantically\nequivalent; more on that in Section 1.5.",
    "diﬀerent syntactically and natural deduction treats them diﬀerently as well.\nBut using the truth tables for ¬ and ∨you can check that φ →ψ evaluates\n1.4 Semantics of propositional logic\n39\nto T iﬀ¬φ ∨ψ does so. This means that φ →ψ and ¬φ ∨ψ are semantically\nequivalent; more on that in Section 1.5.\nGiven a formula φ which contains the propositional atoms p1, p2, . . . , pn,\nwe can construct a truth table for φ, at least in principle. The caveat is that\nthis truth table has 2n many lines, each line listing a possible combination\nof truth values for p1, p2, . . . , pn; and for large n this task is impossible to\ncomplete. Our aim is thus to compute the value of φ for each of these 2n\ncases for moderately small values of n. Let us consider the example φ in\nFigure 1.3. It involves three propositional atoms (n = 3) so we have 23 = 8\ncases to consider.\nWe illustrate how things go for one particular case, namely for the val-\nuation in which q evaluates to F; and p and r evaluate to T. What does\n¬p ∧q →p ∧(q ∨¬r) evaluate to? Well, the beauty of our semantics is that\nit is compositional. If we know the meaning of the subformulas ¬p ∧q and\np ∧(q ∨¬r), then we just have to look up the appropriate line of the →\ntruth table to ﬁnd the value of φ, for φ is an implication of these two sub-\nformulas. Therefore, we can do the calculation by traversing the parse tree\nof φ in a bottom-up fashion. We know what its leaves evaluate to since we\nstated what the atoms p, q and r evaluated to. Because the meaning of p is\nT, we see that ¬p computes to F. Now q is assumed to represent F and the\nconjunction of F and F is F. Thus, the left subtree of the node →evaluates\nto F. As for the right subtree of →, r stands for T so ¬r computes to F and q\nmeans F, so the disjunction of F and F is still F. We have to take that result,\nF, and compute its conjunction with the meaning of p which is T. Since the\nconjunction of T and F is F, we get F as the meaning of the right subtree",
    "means F, so the disjunction of F and F is still F. We have to take that result,\nF, and compute its conjunction with the meaning of p which is T. Since the\nconjunction of T and F is F, we get F as the meaning of the right subtree\nof →. Finally, to evaluate the meaning of φ, we compute F →F which is T.\nFigure 1.7 shows how the truth values propagate upwards to reach the root\nwhose associated truth value is the truth value of φ given the meanings of\np, q and r above.\nIt should now be quite clear how to build a truth table for more com-\nplex formulas. Figure 1.8 contains a truth table for the formula (p →¬q) →\n(q ∨¬p). To be more precise, the ﬁrst two columns list all possible combina-\ntions of values for p and q. The next two columns compute the corresponding\nvalues for ¬p and ¬q. Using these four columns, we may compute the column\nfor p →¬q and q ∨¬p. To do so we think of the ﬁrst and fourth columns\nas the data for the →truth table and compute the column of p →¬q ac-\ncordingly. For example, in the ﬁrst line p is T and ¬q is F so the entry for\np →¬q is T →F = F by deﬁnition of the meaning of →. In this fashion, we\ncan ﬁll out the rest of the ﬁfth column. Column 6 works similarly, only we\nnow need to look up the truth table for ∨with columns 2 and 3 as input.\n40\n1 Propositional logic\np\nq\np\nr\nT\nT\nF\nT\nF\nF\nF\nF\n¬\nF\n∧\n∨\nq\nT\nF\n∧\n¬\n→\nFigure 1.7. The evaluation of a logical formula under a given valuation.\np\nq\n¬p\n¬q\np →¬q\nq ∨¬p\n(p →¬q) →(q ∨¬p)\nT\nT\nF\nF\nF\nT\nT\nT\nF\nF\nT\nT\nF\nF\nF\nT\nT\nF\nT\nT\nT\nF\nF\nT\nT\nT\nT\nT\nFigure 1.8. An example of a truth table for a more complex logical formula.\nFinally, column 7 results from applying the truth table of →to columns 5\nand 6.\n1.4.2 Mathematical induction\nHere is a little anecdote about the German mathematician Gauss who, as a\npupil at age 8, did not pay attention in class (can you imagine?), with the\nresult that his teacher made him sum up all natural numbers from 1 to 100.\nThe story has it that Gauss came up with the correct answer 5050 within",
    "Here is a little anecdote about the German mathematician Gauss who, as a\npupil at age 8, did not pay attention in class (can you imagine?), with the\nresult that his teacher made him sum up all natural numbers from 1 to 100.\nThe story has it that Gauss came up with the correct answer 5050 within\nseconds, which infuriated his teacher. How did Gauss do it? Well, possibly\nhe knew that\n1 + 2 + 3 + 4 + · · · + n = n · (n + 1)\n2\n(1.5)\n1.4 Semantics of propositional logic\n41\nfor all natural numbers n.9 Thus, taking n = 100, Gauss could easily calcu-\nlate:\n1 + 2 + 3 + 4 + · · · + 100 = 100 · 101\n2\n= 5050.\nMathematical induction allows us to prove equations, such as the one\nin (1.5), for arbitrary n. More generally, it allows us to show that every\nnatural number satisﬁes a certain property. Suppose we have a property M\nwhich we think is true of all natural numbers. We write M(5) to say that\nthe property is true of 5, etc. Suppose that we know the following two things\nabout the property M:\n1.\nBase case: The natural number 1 has property M, i.e. we have a proof of\nM(1).\n2.\nInductive step: If n is a natural number which we assume to have property\nM(n), then we can show that n + 1 has property M(n + 1); i.e. we have a proof\nof M(n) →M(n + 1).\nDeﬁnition 1.30 The principle of mathematical induction says that, on the\ngrounds of these two pieces of information above, every natural number n\nhas property M(n). The assumption of M(n) in the inductive step is called\nthe induction hypothesis.\nWhy does this principle make sense? Well, take any natural number k.\nIf k equals 1, then k has property M(1) using the base case and so we are\ndone. Otherwise, we can use the inductive step, applied to n = 1, to infer\nthat 2 = 1 + 1 has property M(2). We can do that using →e, for we know\nthat 1 has the property in question. Now we use that same inductive step on\nn = 2 to infer that 3 has property M(3) and we repeat this until we reach",
    "done. Otherwise, we can use the inductive step, applied to n = 1, to infer\nthat 2 = 1 + 1 has property M(2). We can do that using →e, for we know\nthat 1 has the property in question. Now we use that same inductive step on\nn = 2 to infer that 3 has property M(3) and we repeat this until we reach\nn = k (see Figure 1.9). Therefore, we should have no objections about using\nthe principle of mathematical induction for natural numbers.\nReturning to Gauss’ example we claim that the sum 1 + 2 + 3 + 4 + · · · +\nn equals n · (n + 1)/2 for all natural numbers n.\nTheorem 1.31 The sum 1 + 2 + 3 + 4 + · · · + n equals n · (n + 1)/2 for all\nnatural numbers n.\n9 There is another way of ﬁnding the sum 1 + 2 + · · · + 100, which works like this: write the\nsum backwards, as 100 + 99 + · · · + 1. Now add the forwards and backwards versions, obtaining\n101 + 101 + · · · + 101 (100 times), which is 10100. Since we added the sum to itself, we now\ndivide by two to get the answer 5050. Gauss probably used this method; but the method of\nmathematical induction that we explore in this section is much more powerful and can be\napplied in a wide variety of situations.\n42\n1 Propositional logic\nWe prove M(1)\n...\n...\n1\n2\n3\n...\nn + 1\nn\nWe prove M(2) using M(1) and M(1) →M(2)\nWe prove M(3) using M(2) and M(2) →M(3)\nWe prove M(n) using M(n −1) and M(n −1) →M(n)\nWe prove M(n + 1) using M(n) and M(n) →M(n + 1)\nFigure 1.9. How the principle of mathematical induction works. By\nproving just two facts, M(1) and M(n) →M(n + 1) for a formal (and\nunconstrained) parameter n, we are able to deduce M(k) for each natural\nnumber k.\nProof: We use mathematical induction. In order to reveal the ﬁne structure\nof our proof we write LHSn for the expression 1 + 2 + 3 + 4 + · · · + n and\nRHSn for n · (n + 1)/2. Thus, we need to show LHSn = RHSn for all n ≥1.\nBase case: If n equals 1, then LHS1 is just 1 (there is only one summand),\nwhich happens to equal RHS1 = 1 · (1 + 1)/2.",
    "of our proof we write LHSn for the expression 1 + 2 + 3 + 4 + · · · + n and\nRHSn for n · (n + 1)/2. Thus, we need to show LHSn = RHSn for all n ≥1.\nBase case: If n equals 1, then LHS1 is just 1 (there is only one summand),\nwhich happens to equal RHS1 = 1 · (1 + 1)/2.\nInductive step: Let us assume that LHSn = RHSn. Recall that this as-\nsumption is called the induction hypothesis; it is the driving force of\nour argument. We need to show LHSn+1 = RHSn+1, i.e. that the longer\nsum 1 + 2 + 3 + 4 + · · · + (n + 1) equals (n + 1) · ((n + 1) + 1)/2. The key\nobservation is that the sum 1 + 2 + 3 + 4 + · · · + (n + 1) is nothing but\nthe sum (1 + 2 + 3 + 4 + · · · + n) + (n + 1) of two summands, where the\nﬁrst one is the sum of our induction hypothesis. The latter says that\n1 + 2 + 3 + 4 + · · · + n equals n · (n + 1)/2, and we are certainly entitled\nto substitute equals for equals in our reasoning. Thus, we compute\nLHSn+1\n= 1 + 2 + 3 + 4 + · · · + (n + 1)\n= LHSn + (n + 1) regrouping the sum\n1.4 Semantics of propositional logic\n43\n= RHSn + (n + 1) by our induction hypothesis\n= n·(n+1)\n2\n+ (n + 1)\n= n·(n+1)\n2\n+ 2·(n+1)\n2\narithmetic\n= (n+2)·(n+1)\n2\narithmetic\n= ((n+1)+1)·(n+1)\n2\narithmetic\n= RHSn+1.\nSince we successfully showed the base case and the inductive step, we can\nuse mathematical induction to infer that all natural numbers n have the\nproperty stated in the theorem above.\n2\nActually, there are numerous variations of this principle. For example, we\ncan think of a version in which the base case is n = 0, which would then\ncover all natural numbers including 0. Some statements hold only for all\nnatural numbers, say, greater than 3. So you would have to deal with a\nbase case 4, but keep the version of the inductive step (see the exercises for\nsuch an example). The use of mathematical induction typically suceeds on\nproperties M(n) that involve inductive deﬁnitions (e.g. the deﬁnition of kl\nwith l ≥0). Sentence (3) on page 2 suggests there may be true properties",
    "such an example). The use of mathematical induction typically suceeds on\nproperties M(n) that involve inductive deﬁnitions (e.g. the deﬁnition of kl\nwith l ≥0). Sentence (3) on page 2 suggests there may be true properties\nM(n) for which mathematical induction won’t work.\nCourse-of-values induction.\nThere is a variant of mathematical induction\nin which the induction hypothesis for proving M(n + 1) is not just M(n), but\nthe conjunction M(1) ∧M(2) ∧· · · ∧M(n). In that variant, called course-\nof-values induction, there doesn’t have to be an explicit base case at all –\neverything can be done in the inductive step.\nHow can this work without a base case? The answer is that the base\ncase is implicitly included in the inductive step. Consider the case n = 3:\nthe inductive-step instance is M(1) ∧M(2) ∧M(3) →M(4). Now consider\nn = 1: the inductive-step instance is M(1) →M(2). What about the case\nwhen n equals 0? In this case, there are zero formulas on the left of the →,\nso we have to prove M(1) from nothing at all. The inductive-step instance\nis simply the obligation to show M(1). You might ﬁnd it useful to modify\nFigure 1.9 for course-of-values induction.\nHaving said that the base case is implicit in course-of-values induction,\nit frequently turns out that it still demands special attention when you get\ninside trying to prove the inductive case. We will see precisely this in the\ntwo applications of course-of-values induction in the following pages.\n44\n1 Propositional logic\n¬\n∧\n→\n→\nq\n¬\np\n∨\nr\nq\np\nFigure 1.10. A parse tree with height 5.\nIn computer science, we often deal with ﬁnite structures of some kind, data\nstructures, programs, ﬁles etc. Often we need to show that every instance of\nsuch a structure has a certain property. For example, the well-formed for-\nmulas of Deﬁnition 1.27 have the property that the number of ‘(’ brackets\nin a particular formula equals its number of ‘)’ brackets. We can use mathe-",
    "such a structure has a certain property. For example, the well-formed for-\nmulas of Deﬁnition 1.27 have the property that the number of ‘(’ brackets\nin a particular formula equals its number of ‘)’ brackets. We can use mathe-\nmatical induction on the domain of natural numbers to prove this. In order\nto succeed, we somehow need to connect well-formed formulas to natural\nnumbers.\nDeﬁnition 1.32 Given a well-formed formula φ, we deﬁne its height to be\n1 plus the length of the longest path of its parse tree.\nFor example, consider the well-formed formulas in Figures 1.3, 1.4\nand 1.10. Their heights are 5, 6 and 5, respectively. In Figure 1.3, the\nlongest path goes from →to ∧to ∨to ¬ to r, a path of length 4, so\nthe height is 4 + 1 = 5. Note that the height of atoms is 1 + 0 = 1. Since\nevery well-formed formula has ﬁnite height, we can show statements about\nall well-formed formulas by mathematical induction on their height. This\ntrick is most often called structural induction, an important reasoning tech-\nnique in computer science. Using the notion of the height of a parse tree,\nwe realise that structural induction is just a special case of course-of-values\ninduction.\n1.4 Semantics of propositional logic\n45\nTheorem 1.33 For every well-formed propositional logic formula, the num-\nber of left brackets is equal to the number of right brackets.\nProof: We proceed by course-of-values induction on the height of well-\nformed formulas φ. Let M(n) mean ‘All formulas of height n have the same\nnumber of left and right brackets.’ We assume M(k) for each k < n and try\nto prove M(n). Take a formula φ of height n.\nr Base case: Then n = 1. This means that φ is just a propositional atom. So there\nare no left or right brackets, 0 equals 0.\nr Course-of-values inductive step: Then n > 1 and so the root of the parse tree\nof φ must be ¬, →, ∨or ∧, for φ is well-formed. We assume that it is →, the other\nthree cases are argued in a similar way. Then φ equals (φ1 →φ2) for some well-",
    "are no left or right brackets, 0 equals 0.\nr Course-of-values inductive step: Then n > 1 and so the root of the parse tree\nof φ must be ¬, →, ∨or ∧, for φ is well-formed. We assume that it is →, the other\nthree cases are argued in a similar way. Then φ equals (φ1 →φ2) for some well-\nformed formulas φ1 and φ2 (of course, they are just the left, respectively right,\nlinear representations of the root’s two subtrees). It is clear that the heights\nof φ1 and φ2 are strictly smaller than n. Using the induction hypothesis, we\ntherefore conclude that φ1 has the same number of left and right brackets and\nthat the same is true for φ2. But in (φ1 →φ2) we added just two more brackets,\none ‘(’ and one ‘)’. Thus, the number of occurrences of ‘(’ and ‘)’ in φ is the\nsame.\n2\nThe formula (p →(q ∧¬r)) illustrates why we could not prove the above\ndirectly with mathematical induction on the height of formulas. While this\nformula has height 4, its two subtrees have heights 1 and 3, respectively.\nThus, an induction hypothesis for height 3 would have worked for the right\nsubtree but failed for the left subtree.\n1.4.3 Soundness of propositional logic\nThe natural deduction rules make it possible for us to develop rigorous\nthreads of argumentation, in the course of which we arrive at a conclusion\nψ assuming certain other propositions φ1, φ2, . . . , φn. In that case, we said\nthat the sequent φ1, φ2, . . . , φn ⊢ψ is valid. Do we have any evidence that\nthese rules are all correct in the sense that valid sequents all ‘preserve truth’\ncomputed by our truth-table semantics?\nGiven a proof of φ1, φ2, . . . , φn ⊢ψ, is it conceivable that there is a valu-\nation in which ψ above is false although all propositions φ1, φ2, . . . , φn are\ntrue? Fortunately, this is not the case and in this subsection we demonstrate\nwhy this is so. Let us suppose that some proof in our natural deduction cal-\nculus has established that the sequent φ1, φ2, . . . , φn ⊢ψ is valid. We need",
    "true? Fortunately, this is not the case and in this subsection we demonstrate\nwhy this is so. Let us suppose that some proof in our natural deduction cal-\nculus has established that the sequent φ1, φ2, . . . , φn ⊢ψ is valid. We need\nto show: for all valuations in which all propositions φ1, φ2, . . . , φn evaluate\nto T, ψ evaluates to T as well.\n46\n1 Propositional logic\nDeﬁnition 1.34 If, for all valuations in which all φ1, φ2, . . . , φn evaluate to\nT, ψ evaluates to T as well, we say that\nφ1, φ2, . . . , φn ⊨ψ\nholds and call ⊨the semantic entailment relation.\nLet us look at some examples of this notion.\n1.\nDoes p ∧q ⊨p hold? Well, we have to inspect all assignments of truth values to\np and q; there are four of these. Whenever such an assignment computes T for\np ∧q we need to make sure that p is true as well. But p ∧q computes T only if\np and q are true, so p ∧q ⊨p is indeed the case.\n2.\nWhat about the relationship p ∨q ⊨p? There are three assignments for which\np ∨q computes T, so p would have to be true for all of these. However, if we\nassign T to q and F to p, then p ∨q computes T, but p is false. Thus, p ∨q ⊨p\ndoes not hold.\n3.\nWhat if we modify the above to ¬q, p ∨q ⊨p? Notice that we have to be con-\ncerned only about valuations in which ¬q and p ∨q evaluate to T. This forces q\nto be false, which in turn forces p to be true. Hence ¬q, p ∨q ⊨p is the case.\n4.\nNote that p ⊨q ∨¬q holds, despite the fact that no atomic proposition on the\nright of ⊨occurs on the left of ⊨.\nFrom the discussion above we realize that a soundness argument has to show:\nif φ1, φ2, . . . , φn ⊢ψ is valid, then φ1, φ2, . . . , φn ⊨ψ holds.\nTheorem 1.35 (Soundness) Let φ1, φ2, . . . , φn and ψ be propositional\nlogic formulas. If φ1, φ2, . . . , φn ⊢ψ is valid, then φ1, φ2, . . . , φn ⊨ψ holds.\nProof:\nSince φ1, φ2, . . . , φn ⊢ψ is valid we know there is a proof of ψ\nfrom the premises φ1, φ2, . . . , φn. We now do a pretty slick thing, namely,",
    "logic formulas. If φ1, φ2, . . . , φn ⊢ψ is valid, then φ1, φ2, . . . , φn ⊨ψ holds.\nProof:\nSince φ1, φ2, . . . , φn ⊢ψ is valid we know there is a proof of ψ\nfrom the premises φ1, φ2, . . . , φn. We now do a pretty slick thing, namely,\nwe reason by mathematical induction on the length of this proof! The length\nof a proof is just the number of lines it involves. So let us be perfectly\nclear about what it is we mean to show. We intend to show the assertion\nM(k):\n‘For all sequents φ1, φ2, . . . , φn ⊢ψ (n ≥0) which have a proof of\nlength k, it is the case that φ1, φ2, . . . , φn ⊨ψ holds.’\nby course-of-values induction on the natural number k. This idea requires\n1.4 Semantics of propositional logic\n47\nsome work, though. The sequent p ∧q →r ⊢p →(q →r) has a proof\n1\np ∧q →r\npremise\n2\np\nassumption\n3\nq\nassumption\n4\np ∧q\n∧i 2, 3\n5\nr\n→e 1, 4\n6\nq →r\n→i 3−5\n7\np →(q →r)\n→i 2−6\nbut if we remove the last line or several of the last lines, we no longer\nhave a proof as the outermost box does not get closed. We get a complete\nproof, though, by removing the last line and re-writing the assumption of\nthe outermost box as a premise:\n1\np ∧q →r\npremise\n2\np\npremise\n3\nq\nassumption\n4\np ∧q\n∧i 2, 3\n5\nr\n→e 1, 4\n6\nq →r\n→i 3−5\nThis is a proof of the sequent p ∧q →r, p ⊢p →r. The induction hypothesis\nthen ensures that p ∧q →r, p ⊨p →r holds. But then we can also reason\nthat p ∧q →r ⊨p →(q →r) holds as well – why?\nLet’s proceed with our proof by induction. We assume M(k′) for each\nk′ < k and we try to prove M(k).\nBase case: a one-line proof.\nIf the proof has length 1 (k = 1), then it must\nbe of the form\n1\nφ\npremise\nsince all other rules involve more than one line. This is the case when n = 1\nand φ1 and ψ equal φ, i.e. we are dealing with the sequent φ ⊢φ. Of course,\nsince φ evaluates to T so does φ. Thus, φ ⊨φ holds as claimed.\n48\n1 Propositional logic\nCourse-of-values inductive step:\nLet us assume that the proof of the se-",
    "and φ1 and ψ equal φ, i.e. we are dealing with the sequent φ ⊢φ. Of course,\nsince φ evaluates to T so does φ. Thus, φ ⊨φ holds as claimed.\n48\n1 Propositional logic\nCourse-of-values inductive step:\nLet us assume that the proof of the se-\nquent φ1, φ2, . . . , φn ⊢ψ has length k and that the statement we want to\nprove is true for all numbers less than k. Our proof has the following struc-\nture:\n1\nφ1 premise\n2\nφ2 premise\n...\nn\nφn premise\n...\nk\nψ justiﬁcation\nThere are two things we don’t know at this point. First, what is happening\nin between those dots? Second, what was the last rule applied, i.e. what is\nthe justiﬁcation of the last line? The ﬁrst uncertainty is of no concern; this\nis where mathematical induction demonstrates its power. The second lack\nof knowledge is where all the work sits. In this generality, there is simply no\nway of knowing which rule was applied last, so we need to consider all such\nrules in turn.\n1.\nLet us suppose that this last rule is ∧i. Then we know that ψ is of the form\nψ1 ∧ψ2 and the justiﬁcation in line k refers to two lines further up which have\nψ1, respectively ψ2, as their conclusions. Suppose that these lines are k1 and k2.\nSince k1 and k2 are smaller than k, we see that there exist proofs of the sequents\nφ1, φ2, . . . , φn ⊢ψ1 and φ1, φ2, . . . , φn ⊢ψ2 with length less than k – just take\nthe ﬁrst k1, respectively k2, lines of our original proof. Using the induction\nhypothesis, we conclude that φ1, φ2, . . . , φn ⊨ψ1 and φ1, φ2, . . . , φn ⊨ψ2 holds.\nBut these two relations imply that φ1, φ2, . . . , φn ⊨ψ1 ∧ψ2 holds as well – why?\n2.\nIf ψ has been shown using the rule ∨e, then we must have proved, as-\nsumed or given as a premise some formula η1 ∨η2 in some line k′ with\nk′ < k, which was referred to via ∨e in the justiﬁcation of line k. Thus,\nwe have a shorter proof of the sequent φ1, φ2, . . . , φn ⊢η1 ∨η2 within that\nproof, obtained by turning all assumptions of boxes that are open at",
    "sumed or given as a premise some formula η1 ∨η2 in some line k′ with\nk′ < k, which was referred to via ∨e in the justiﬁcation of line k. Thus,\nwe have a shorter proof of the sequent φ1, φ2, . . . , φn ⊢η1 ∨η2 within that\nproof, obtained by turning all assumptions of boxes that are open at\nline k′ into premises. In a similar way we obtain proofs of the sequents\nφ1, φ2, . . . , φn, η1 ⊢ψ and φ1, φ2, . . . , φn, η2 ⊢ψ from the case analysis of ∨e.\nBy our induction hypothesis, we conclude that the relations φ1, φ2, . . . , φn ⊨\nη1 ∨η2, φ1, φ2, . . . , φn, η1 ⊨ψ and φ1, φ2, . . . , φn, η2 ⊨ψ hold. But together\nthese three relations then force that φ1, φ2, . . . , φn ⊨ψ holds as well –\nwhy?\n3.\nYou can guess by now that the rest of the argument checks each possible proof\nrule in turn and ultimately boils down to verifying that our natural deduction\n1.4 Semantics of propositional logic\n49\nrules behave semantically in the same way as their corresponding truth tables\nevaluate. We leave the details as an exercise.\n2\nThe soundness of propositional logic is useful in ensuring the non-existence of\na proof for a given sequent. Let’s say you try to prove that φ1, φ2, . . . , φ2 ⊢ψ\nis valid, but that your best eﬀorts won’t succeed. How could you be sure that\nno such proof can be found? After all, it might just be that you can’t ﬁnd\na proof even though there is one. It suﬃces to ﬁnd a valuation in which φi\nevaluate to T whereas ψ evaluates to F. Then, by deﬁnition of ⊨, we don’t\nhave φ1, φ2, . . . , φ2 ⊨ψ. Using soundness, this means that φ1, φ2, . . . , φ2 ⊢ψ\ncannot be valid. Therefore, this sequent does not have a proof. You will\npractice this method in the exercises.\n1.4.4 Completeness of propositional logic\nIn this subsection, we hope to convince you that the natural deduction rules\nof propositional logic are complete: whenever φ1, φ2, . . . , φn ⊨ψ holds, then\nthere exists a natural deduction proof for the sequent φ1, φ2, . . . , φn ⊢ψ.",
    "1.4.4 Completeness of propositional logic\nIn this subsection, we hope to convince you that the natural deduction rules\nof propositional logic are complete: whenever φ1, φ2, . . . , φn ⊨ψ holds, then\nthere exists a natural deduction proof for the sequent φ1, φ2, . . . , φn ⊢ψ.\nCombined with the soundness result of the previous subsection, we then\nobtain\nφ1, φ2, . . . , φn ⊢ψ is valid iﬀφ1, φ2, . . . , φn ⊨ψ holds.\nThis gives you a certain freedom regarding which method you prefer to\nuse. Often it is much easier to show one of these two relationships (al-\nthough neither of the two is universally better, or easier, to establish).\nThe ﬁrst method involves a proof search, upon which the logic program-\nming paradigm is based. The second method typically forces you to com-\npute a truth table which is exponential in the size of occurring proposi-\ntional atoms. Both methods are intractable in general but particular in-\nstances of formulas often respond diﬀerently to treatment under these two\nmethods.\nThe remainder of this section is concerned with an argument saying that\nif φ1, φ2, . . . , φn ⊨ψ holds, then φ1, φ2, . . . , φn ⊢ψ is valid. Assuming that\nφ1, φ2, . . . , φn ⊨ψ holds, the argument proceeds in three steps:\nStep 1: We show that ⊨φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) holds.\nStep 2: We show that ⊢φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) is valid.\nStep 3: Finally, we show that φ1, φ2, . . . , φn ⊢ψ is valid.\nThe ﬁrst and third steps are quite easy; all the real work is done in the\nsecond one.\n50\n1 Propositional logic\n→\n→\n→\n→\n→\nF\nF\nF\nF\nF\nψ\nF\nT\nT\nT\nT\nT\nφn\nφn−1\nφ3\nφ2\nφ1\nFigure 1.11. The only way this parse tree can evaluate to F. We repre-\nsent parse trees for φ1, φ2, . . . , φn as triangles as their internal structure\ndoes not concern us here.\nStep 1:\nDeﬁnition 1.36 A formula of propositional logic φ is called a tautology iﬀ\nit evaluates to T under all its valuations, i.e. iﬀ⊨φ.\nSupposing that φ1, φ2, . . . , φn ⊨ψ holds, let us verify that φ1 →(φ2 →",
    "does not concern us here.\nStep 1:\nDeﬁnition 1.36 A formula of propositional logic φ is called a tautology iﬀ\nit evaluates to T under all its valuations, i.e. iﬀ⊨φ.\nSupposing that φ1, φ2, . . . , φn ⊨ψ holds, let us verify that φ1 →(φ2 →\n(φ3 →(. . . (φn →ψ) . . . ))) is indeed a tautology. Since the latter formula is\na nested implication, it can evaluate to F only if all φ1, φ2,. . .,φn evaluate to T\nand ψ evaluates to F; see its parse tree in Figure 1.11. But this contradicts the\nfact that φ1, φ2, . . . , φn ⊨ψ holds. Thus, ⊨φ1 →(φ2 →(φ3 →(. . . (φn →\nψ) . . . ))) holds.\nStep 2:\nTheorem 1.37 If ⊨η holds, then ⊢η is valid. In other words, if η is a\ntautology, then η is a theorem.\nThis step is the hard one. Assume that ⊨η holds. Given that η contains\nn distinct propositional atoms p1, p2, . . . , pn we know that η evaluates to T\nfor all 2n lines in its truth table. (Each line lists a valuation of η.) How can\nwe use this information to construct a proof for η? In some cases this can\nbe done quite easily by taking a very good look at the concrete structure of\nη. But here we somehow have to come up with a uniform way of building\nsuch a proof. The key insight is to ‘encode’ each line in the truth table of η\n1.4 Semantics of propositional logic\n51\nas a sequent. Then we construct proofs for these 2n sequents and assemble\nthem into a proof of η.\nProposition 1.38 Let φ be a formula such that p1, p2, . . . , pn are its only\npropositional atoms. Let l be any line number in φ’s truth table. For all\n1 ≤i ≤n let ˆpi be pi if the entry in line l of pi is T, otherwise ˆpi is ¬pi.\nThen we have\n1.\nˆp1, ˆp2, . . . , ˆpn ⊢φ is provable if the entry for φ in line l is T\n2.\nˆp1, ˆp2, . . . , ˆpn ⊢¬φ is provable if the entry for φ in line l is F\nProof: This proof is done by structural induction on the formula φ, that\nis, mathematical induction on the height of the parse tree of φ.\n1.\nIf φ is a propositional atom p, we need to show that p ⊢p and ¬p ⊢¬p. These\nhave one-line proofs.\n2.",
    "Proof: This proof is done by structural induction on the formula φ, that\nis, mathematical induction on the height of the parse tree of φ.\n1.\nIf φ is a propositional atom p, we need to show that p ⊢p and ¬p ⊢¬p. These\nhave one-line proofs.\n2.\nIf φ is of the form ¬φ1 we again have two cases to consider. First, assume that φ\nevaluates to T. In this case φ1 evaluates to F. Note that φ1 has the same atomic\npropositions as φ. We may use the induction hypothesis on φ1 to conclude that\nˆp1, ˆp2, . . . , ˆpn ⊢¬φ1; but ¬φ1 is just φ, so we are done.\nSecond, if φ evaluates to F, then φ1 evaluates to T and we get ˆp1, ˆp2, . . . , ˆpn ⊢φ1\nby induction. Using the rule ¬¬i, we may extend the proof of ˆp1, ˆp2, . . . , ˆpn ⊢φ1\nto one for ˆp1, ˆp2, . . . , ˆpn ⊢¬¬φ1; but ¬¬φ1 is just ¬φ, so again we are done.\nThe remaining cases all deal with two subformulas: φ equals φ1 ◦φ2, where\n◦is →, ∧or ∨. In all these cases let q1, . . . , ql be the propositional\natoms of φ1 and r1, . . . , rk be the propositional atoms of φ2. Then we cer-\ntainly have {q1, . . . , ql} ∪{r1, . . . , rk} = {p1, . . . , pn}. Therefore, whenever\nˆq1, . . . , ˆql ⊢ψ1 and ˆr1, . . . , ˆrk ⊢ψ2 are valid so is ˆp1, . . . , ˆpn ⊢ψ1 ∧ψ2 using\nthe rule ∧i. In this way, we can use our induction hypothesis and only owe\nproofs that the conjunctions we conclude allow us to prove the desired con-\nclusion for φ or ¬φ as the case may be.\n3.\nTo wit, let φ be φ1 →φ2. If φ evaluates to F, then we know that φ1 evaluates\nto T and φ2 to F. Using our induction hypothesis, we have ˆq1, . . . , ˆql ⊢φ1\nand ˆr1, . . . , ˆrk ⊢¬φ2, so\nˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 follows. We need to show\nˆp1, . . . , ˆpn ⊢¬(φ1 →φ2); but using ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2, this amounts to\nproving the sequent φ1 ∧¬φ2 ⊢¬(φ1 →φ2), which we leave as an exercise.\nIf φ evaluates to T, then we have three cases. First, if φ1 evaluates to F and\nφ2 to F, then we get, by our induction hypothesis, that ˆq1, . . . , ˆql ⊢¬φ1 and",
    "proving the sequent φ1 ∧¬φ2 ⊢¬(φ1 →φ2), which we leave as an exercise.\nIf φ evaluates to T, then we have three cases. First, if φ1 evaluates to F and\nφ2 to F, then we get, by our induction hypothesis, that ˆq1, . . . , ˆql ⊢¬φ1 and\nˆr1, . . . , ˆrk ⊢¬φ2, so ˆp1, . . . , ˆpn ⊢¬φ1 ∧¬φ2 follows. Again, we need only to\nshow the sequent ¬φ1 ∧¬φ2 ⊢φ1 →φ2, which we leave as an exercise. Second,\nif φ1 evaluates to F and φ2 to T, we use our induction hypothesis to arrive at\n52\n1 Propositional logic\nˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2 and have to prove ¬φ1 ∧φ2 ⊢φ1 →φ2, which we leave as\nan exercise. Third, if φ1 and φ2 evaluate to T, we arrive at ˆp1, . . . , ˆpn ⊢φ1 ∧φ2,\nusing our induction hypothesis, and need to prove φ1 ∧φ2 ⊢φ1 →φ2, which\nwe leave as an exercise as well.\n4.\nIf φ is of the form φ1 ∧φ2, we are again dealing with four cases in total. First, if\nφ1 and φ2 evaluate to T, we get ˆq1, . . . , ˆql ⊢φ1 and ˆr1, . . . , ˆrk ⊢φ2 by our induc-\ntion hypothesis, so ˆp1, . . . , ˆpn ⊢φ1 ∧φ2 follows. Second, if φ1 evaluates to F and\nφ2 to T, then we get ˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2 using our induction hypothesis and\nthe rule ∧i as above and we need to prove ¬φ1 ∧φ2 ⊢¬(φ1 ∧φ2), which we leave\nas an exercise. Third, if φ1 and φ2 evaluate to F, then our induction hypothesis\nand the rule ∧i let us infer that ˆp1, . . . , ˆpn ⊢¬φ1 ∧¬φ2; so we are left with prov-\ning ¬φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2), which we leave as an exercise. Fourth, if φ1 evalu-\nates to T and φ2 to F, we obtain ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 by our induction hypoth-\nesis and we have to show φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2), which we leave as an exercise.\n5.\nFinally, if φ is a disjunction φ1 ∨φ2, we again have four cases. First, if φ1 and φ2\nevaluate to F, then our induction hypothesis and the rule ∧i give us ˆp1, . . . , ˆpn ⊢\n¬φ1 ∧¬φ2 and we have to show ¬φ1 ∧¬φ2 ⊢¬(φ1 ∨φ2), which we leave as an\nexercise. Second, if φ1 and φ2 evaluate to T, then we obtain ˆp1, . . . , ˆpn ⊢φ1 ∧φ2,",
    "evaluate to F, then our induction hypothesis and the rule ∧i give us ˆp1, . . . , ˆpn ⊢\n¬φ1 ∧¬φ2 and we have to show ¬φ1 ∧¬φ2 ⊢¬(φ1 ∨φ2), which we leave as an\nexercise. Second, if φ1 and φ2 evaluate to T, then we obtain ˆp1, . . . , ˆpn ⊢φ1 ∧φ2,\nby our induction hypothesis, and we need a proof for φ1 ∧φ2 ⊢φ1 ∨φ2, which\nwe leave as an exercise. Third, if φ1 evaluates to F and φ2 to T, then we arrive\nat ˆp1, . . . , ˆpn ⊢¬φ1 ∧φ2, using our induction hypothesis, and need to establish\n¬φ1 ∧φ2 ⊢φ1 ∨φ2, which we leave as an exercise. Fourth, if φ1 evaluates to T\nand φ2 to F, then ˆp1, . . . , ˆpn ⊢φ1 ∧¬φ2 results from our induction hypothesis\nand all we need is a proof for φ1 ∧¬φ2 ⊢φ1 ∨φ2, which we leave as an\nexercise.\n2\nWe apply this technique to the formula ⊨φ1 →(φ2 →(φ3 →(. . . (φn →\nψ) . . . ))). Since it is a tautology it evaluates to T in all 2n lines of its truth\ntable; thus, the proposition above gives us 2n many proofs of ˆp1, ˆp2, . . . , ˆpn ⊢\nη, one for each of the cases that ˆpi is pi or ¬pi. Our job now is to assemble\nall these proofs into a single proof for η which does not use any premises.\nWe illustrate how to do this for an example, the tautology p ∧q →p.\nThe formula p ∧q →p has two propositional atoms p and q. By the propo-\nsition above, we are guaranteed to have a proof for each of the four sequents\np, q ⊢p ∧q →p\n¬p, q ⊢p ∧q →p\np, ¬q ⊢p ∧q →p\n¬p, ¬q ⊢p ∧q →p.\nUltimately, we want to prove p ∧q →p by appealing to the four proofs of\nthe sequents above. Thus, we somehow need to get rid of the premises on\n1.5 Normal forms\n53\nthe left-hand sides of these four sequents. This is the place where we rely on\nthe law of the excluded middle which states r ∨¬r, for any r. We use LEM\nfor all propositional atoms (here p and q) and then we separately assume all\nthe four cases, by using ∨e. That way we can invoke all four proofs of the\nsequents above and use the rule ∨e repeatedly until we have got rid of all our",
    "for all propositional atoms (here p and q) and then we separately assume all\nthe four cases, by using ∨e. That way we can invoke all four proofs of the\nsequents above and use the rule ∨e repeatedly until we have got rid of all our\npremises. We spell out the combination of these four phases schematically:\n1\np ∨¬p\nLEM\n2\np\nass\n3\nq ∨¬q\nLEM\n4\nq\nass\n5\n...\n...\n6\n7\np ∧q →p\n¬q\nass\n...\n...\np ∧q →p\n8\np ∧q →p\n∨e\n¬p\nass\nq ∨¬q\nLEM\nq\nass\n...\n...\np ∧q →p\n¬q\nass\n...\n...\np ∧q →p\np ∧q →p\n∨e\n9\np ∧q →p\n∨e\nAs soon as you understand how this particular example works, you will\nalso realise that it will work for an arbitrary tautology with n distinct atoms.\nOf course, it seems ridiculous to prove p ∧q →p using a proof that is this\nlong. But remember that this illustrates a uniform method that constructs\na proof for every tautology η, no matter how complicated it is.\nStep 3:\nFinally, we need to ﬁnd a proof for φ1, φ2, . . . , φn ⊢ψ. Take the\nproof for ⊢φ1 →(φ2 →(φ3 →(. . . (φn →ψ) . . . ))) given by step 2 and aug-\nment its proof by introducing φ1, φ2, . . . , φn as premises. Then apply →e n\ntimes on each of these premises (starting with φ1, continuing with φ2 etc.).\nThus, we arrive at the conclusion ψ which gives us a proof for the sequent\nφ1, φ2, . . . , φn ⊢ψ.\nCorollary 1.39 (Soundness and Completeness) Let\nφ1, φ2, . . . , φn, ψ\nbe formulas of propositional logic. Then φ1, φ2, . . . , φn ⊨ψ is holds iﬀthe\nsequent φ1, φ2, . . . , φn ⊢ψ is valid.\n1.5 Normal forms\nIn the last section, we showed that our proof system for propositional logic is\nsound and complete for the truth-table semantics of formulas in Figure 1.6.\n54\n1 Propositional logic\nSoundness means that whatever we prove is going to be a true fact, based on\nthe truth-table semantics. In the exercises, we apply this to show that a se-\nquent does not have a proof: simply show that φ1, φ2, . . . , φ2 does not seman-\ntically entail ψ; then soundness implies that the sequent φ1, φ2, . . . , φ2 ⊢ψ",
    "the truth-table semantics. In the exercises, we apply this to show that a se-\nquent does not have a proof: simply show that φ1, φ2, . . . , φ2 does not seman-\ntically entail ψ; then soundness implies that the sequent φ1, φ2, . . . , φ2 ⊢ψ\ndoes not have a proof. Completeness comprised a much more powerful state-\nment: no matter what (semantically) valid sequents there are, they all have\nsyntactic proofs in the proof system of natural deduction. This tight cor-\nrespondence allows us to freely switch between working with the notion of\nproofs (⊢) and that of semantic entailment (⊨).\nUsing natural deduction to decide the validity of instances of ⊢is only\none of many possibilities. In Exercise 1.2.6 we sketch a non-linear, tree-like,\nnotion of proofs for sequents. Likewise, checking an instance of ⊨by apply-\ning Deﬁnition 1.34 literally is only one of many ways of deciding whether\nφ1, φ2, . . . , φn ⊨ψ holds. We now investigate various alternatives for deciding\nφ1, φ2, . . . , φn ⊨ψ which are based on transforming these formulas syntac-\ntically into ‘equivalent’ ones upon which we can then settle the matter by\npurely syntactic or algorithmic means. This requires that we ﬁrst clarify\nwhat exactly we mean by equivalent formulas.\n1.5.1 Semantic equivalence, satisfiability and validity\nTwo formulas φ and ψ are said to be equivalent if they have the same\n‘meaning.’ This suggestion is vague and needs to be reﬁned. For example,\np →q and ¬p ∨q have the same truth table; all four combinations of T and F\nfor p and q return the same result. ’Coincidence of truth tables’ is not good\nenough for what we have in mind, for what about the formulas p ∧q →p\nand r ∨¬r? At ﬁrst glance, they have little in common, having diﬀerent\natomic formulas and diﬀerent connectives. Moreover, the truth table for\np ∧q →p is four lines long, whereas the one for r ∨¬r consists of only two\nlines. However, both formulas are always true. This suggests that we deﬁne",
    "and r ∨¬r? At ﬁrst glance, they have little in common, having diﬀerent\natomic formulas and diﬀerent connectives. Moreover, the truth table for\np ∧q →p is four lines long, whereas the one for r ∨¬r consists of only two\nlines. However, both formulas are always true. This suggests that we deﬁne\nthe equivalence of formulas φ and ψ via ⊨: if φ semantically entails ψ and\nvice versa, then these formulas should be the same as far as our truth-table\nsemantics is concerned.\nDeﬁnition 1.40 Let φ and ψ be formulas of propositional logic. We say\nthat φ and ψ are semantically equivalent iﬀφ ⊨ψ and ψ ⊨φ hold. In that\ncase we write φ ≡ψ. Further, we call φ valid if ⊨φ holds.\nNote that we could also have deﬁned φ ≡ψ to mean that ⊨(φ →ψ) ∧\n(ψ →φ) holds; it amounts to the same concept. Indeed, because of soundness\nand completeness, semantic equivalence is identical to provable equivalence\n1.5 Normal forms\n55\n(Deﬁnition 1.25). Examples of equivalent formulas are\np →q ≡¬q →¬p\np →q ≡¬p ∨q\np ∧q →p ≡r ∨¬r\np ∧q →r ≡p →(q →r).\nRecall that a formula η is called a tautology if ⊨η holds, so the tautologies\nare exactly the valid formulas. The following lemma says that any decision\nprocedure for tautologies is in fact a decision procedure for the validity of\nsequents as well.\nLemma 1.41 Given formulas φ1, φ2, . . . , φn and ψ of propositional logic,\nφ1, φ2, . . . , φn ⊨ψ holds iﬀ⊨φ1 →(φ2 →(φ3 →· · · →(φn →ψ))) holds.\nProof: First, suppose that ⊨φ1 →(φ2 →(φ3 →· · · →(φn →ψ))) holds.\nIf φ1, φ2, . . . , φn are all true under some valuation, then ψ has to be true\nas well for that same valuation. Otherwise,\n⊨φ1 →(φ2 →(φ3 →· · · →\n(φn →ψ))) would not hold (compare this with Figure 1.11). Second, if\nφ1, φ2, . . . , φn ⊨ψ holds, we have already shown that ⊨φ1 →(φ2 →(φ3 →\n· · · →(φn →ψ))) follows in step 1 of our completeness proof.\n2\nFor our current purposes, we want to transform formulas into ones which\ndon’t contain →at all and the occurrences of ∧and ∨are conﬁned to",
    "φ1, φ2, . . . , φn ⊨ψ holds, we have already shown that ⊨φ1 →(φ2 →(φ3 →\n· · · →(φn →ψ))) follows in step 1 of our completeness proof.\n2\nFor our current purposes, we want to transform formulas into ones which\ndon’t contain →at all and the occurrences of ∧and ∨are conﬁned to\nseparate layers such that validity checks are easy. This is being done by\n1.\nusing the equivalence φ →ψ ≡¬φ ∨ψ to remove all occurrences of →from a\nformula and\n2.\nby specifying an algorithm that takes a formula without any →into a normal\nform (still without →) for which checking validity is easy.\nNaturally, we have to specify which forms of formulas we think of as being\n‘normal.’ Again, there are many such notions, but in this text we study only\ntwo important ones.\nDeﬁnition 1.42 A literal L is either an atom p or the negation of an atom\n¬p. A formula C is in conjunctive normal form (CNF) if it is a conjunction\nof clauses, where each clause D is a disjunction of literals:\nL ::= p | ¬p\nD ::= L | L ∨D\n(1.6)\nC ::= D | D ∧C.\n56\n1 Propositional logic\nExamples of formulas in conjunctive normal form are\n(i)\n(¬q ∨p ∨r) ∧(¬p ∨r) ∧q\n(ii)\n(p ∨r) ∧(¬p ∨r) ∧(p ∨¬r).\nIn the ﬁrst case, there are three clauses of type D: ¬q ∨p ∨r, ¬p ∨r, and q –\nwhich is a literal promoted to a clause by the ﬁrst rule of clauses in (1.6).\nNotice how we made implicit use of the associativity laws for ∧and ∨,\nsaying that φ ∨(ψ ∨η) ≡(φ ∨ψ) ∨η and φ ∧(ψ ∧η) ≡(φ ∧ψ) ∧η, since\nwe omitted some parentheses. The formula (¬(q ∨p) ∨r) ∧(q ∨r) is not in\nCNF since q ∨p is not a literal.\nWhy do we care at all about formulas φ in CNF? One of the reasons\nfor their usefulness is that they allow easy checks of validity which other-\nwise take times exponential in the number of atoms. For example, consider\nthe formula in CNF from above: (¬q ∨p ∨r) ∧(¬p ∨r) ∧q. The semantic\nentailment ⊨(¬q ∨p ∨r) ∧(¬p ∨r) ∧q holds iﬀall three relations\n⊨¬q ∨p ∨r\n⊨¬p ∨r\n⊨q\nhold, by the semantics of ∧. But since all of these formulas are disjunctions",
    "the formula in CNF from above: (¬q ∨p ∨r) ∧(¬p ∨r) ∧q. The semantic\nentailment ⊨(¬q ∨p ∨r) ∧(¬p ∨r) ∧q holds iﬀall three relations\n⊨¬q ∨p ∨r\n⊨¬p ∨r\n⊨q\nhold, by the semantics of ∧. But since all of these formulas are disjunctions\nof literals, or literals, we can settle the matter as follows.\nLemma 1.43 A disjunction of literals L1 ∨L2 ∨· · · ∨Lm is valid iﬀthere\nare 1 ≤i, j ≤m such that Li is ¬Lj.\nProof: If Li equals ¬Lj, then L1 ∨L2 ∨· · · ∨Lm evaluates to T for all\nvaluations. For example, the disjunct p ∨q ∨r ∨¬q can never be made false.\nTo see that the converse holds as well, assume that no literal Lk has a\nmatching negation in L1 ∨L2 ∨· · · ∨Lm. Then, for each k with 1 ≤k ≤n,\nwe assign F to Lk, if Lk is an atom; or T, if Lk is the negation of an atom.\nFor example, the disjunct ¬q ∨p ∨r can be made false by assigning F to p\nand r and T to q.\n2\nHence, we have an easy and fast check for the validity of ⊨φ, provided\nthat φ is in CNF; inspect all conjuncts ψk of φ and search for atoms in ψk\nsuch that ψk also contains their negation. If such a match is found for all\nconjuncts, we have ⊨φ. Otherwise (= some conjunct contains no pair Li and\n¬Li), φ is not valid by the lemma above. Thus, the formula (¬q ∨p ∨r) ∧\n(¬p ∨r) ∧q above is not valid. Note that the matching literal has to be found\nin the same conjunct ψk. Since there is no free lunch in this universe, we can\nexpect that the computation of a formula φ′ in CNF, which is equivalent to\na given formula φ, is a costly worst-case operation.\nBefore we study how to compute equivalent conjunctive normal forms, we\nintroduce another semantic concept closely related to that of validity.\n1.5 Normal forms\n57\nDeﬁnition 1.44 Given a formula φ in propositional logic, we say that φ is\nsatisﬁable if it has a valuation in which is evaluates to T.\nFor example, the formula p ∨q →p is satisﬁable since it computes T if we\nassign T to p. Clearly, p ∨q →p is not valid. Thus, satisﬁability is a weaker",
    "57\nDeﬁnition 1.44 Given a formula φ in propositional logic, we say that φ is\nsatisﬁable if it has a valuation in which is evaluates to T.\nFor example, the formula p ∨q →p is satisﬁable since it computes T if we\nassign T to p. Clearly, p ∨q →p is not valid. Thus, satisﬁability is a weaker\nconcept since every valid formula is by deﬁnition also satisﬁable but not vice\nversa. However, these two notions are just mirror images of each other, the\nmirror being negation.\nProposition 1.45 Let φ be a formula of propositional logic. Then φ is sat-\nisﬁable iﬀ¬φ is not valid.\nProof: First, assume that φ is satisﬁable. By deﬁnition, there exists a\nvaluation of φ in which φ evaluates to T; but that means that ¬φ evaluates\nto F for that same valuation. Thus, ¬φ cannot be valid.\nSecond, assume that ¬φ is not valid. Then there must be a valuation\nof ¬φ in which ¬φ evaluates to F. Thus, φ evaluates to T and is there-\nfore satisﬁable. (Note that the valuations of φ are exactly the valuations of\n¬φ.)\n2\nThis result is extremely useful since it essentially says that we need provide\na decision procedure for only one of these concepts. For example, let’s say\nthat we have a procedure P for deciding whether any φ is valid. We obtain a\ndecision procedure for satisﬁability simply by asking P whether ¬φ is valid.\nIf it is, φ is not satisﬁable; otherwise φ is satisﬁable. Similarly, we may\ntransform any decision procedure for satisﬁability into one for validity. We\nwill encounter both kinds of procedures in this text.\nThere is one scenario in which computing an equivalent formula in CNF\nis really easy; namely, when someone else has already done the work of\nwriting down a full truth table for φ. For example, take the truth table\nof (p →¬q) →(q ∨¬p) in Figure 1.8 (page 40). For each line where (p →\n¬q) →(q ∨¬p) computes F we now construct a disjunction of literals. Since\nthere is only one such line, we have only one conjunct ψ1. That conjunct",
    "writing down a full truth table for φ. For example, take the truth table\nof (p →¬q) →(q ∨¬p) in Figure 1.8 (page 40). For each line where (p →\n¬q) →(q ∨¬p) computes F we now construct a disjunction of literals. Since\nthere is only one such line, we have only one conjunct ψ1. That conjunct\nis now obtained by a disjunction of literals, where we include literals ¬p\nand q. Note that the literals are just the syntactic opposites of the truth\nvalues in that line: here p is T and q is F. The resulting formula in CNF\nis thus ¬p ∨q which is readily seen to be in CNF and to be equivalent to\n(p →¬q) →(q ∨¬p).\nWhy does this always work for any formula φ? Well, the constructed\nformula will be false iﬀat least one of its conjuncts ψi will be false. This\nmeans that all the disjuncts in such a ψi must be F. Using the de Morgan\n58\n1 Propositional logic\nrule ¬φ1 ∨¬φ2 ∨· · · ∨¬φn ≡¬(φ1 ∧φ2 ∧· · · ∧φn), we infer that the con-\njunction of the syntactic opposites of those literals must be true. Thus, φ\nand the constructed formula have the same truth table.\nConsider another example, in which φ is given by the truth table:\np\nq\nr\nφ\nT\nT\nT\nT\nT\nT\nF\nF\nT\nF\nT\nT\nT\nF\nF\nT\nF\nT\nT\nF\nF\nT\nF\nF\nF\nF\nT\nF\nF\nF\nF\nT\nNote that this table is really just a speciﬁcation of φ; it does not tell us what\nφ looks like syntactically, but it does tells us how it ought to ‘behave.’ Since\nthis truth table has four entries which compute F, we construct four con-\njuncts ψi (1 ≤i ≤4). We read the ψi oﬀthat table by listing the disjunction\nof all atoms, where we negate those atoms which are true in those lines:\nψ1\ndef\n= ¬p ∨¬q ∨r (line 2)\nψ2\ndef\n= p ∨¬q ∨¬r (line 5)\nψ3\ndef\n= p ∨¬q ∨r\netc\nψ4\ndef\n= p ∨q ∨¬r.\nThe resulting φ in CNF is therefore\n(¬p ∨¬q ∨r) ∧(p ∨¬q ∨¬r) ∧(p ∨¬q ∨r) ∧(p ∨q ∨¬r).\nIf we don’t have a full truth table at our disposal, but do know the structure\nof φ, then we would like to compute a version of φ in CNF. It should be\nclear by now that a full truth table of φ and an equivalent formula in",
    "(¬p ∨¬q ∨r) ∧(p ∨¬q ∨¬r) ∧(p ∨¬q ∨r) ∧(p ∨q ∨¬r).\nIf we don’t have a full truth table at our disposal, but do know the structure\nof φ, then we would like to compute a version of φ in CNF. It should be\nclear by now that a full truth table of φ and an equivalent formula in\nCNF are pretty much the same thing as far as questions about validity are\nconcerned – although the formula in CNF may be much more compact.\n1.5.2 Conjunctive normal forms and validity\nWe have already seen the beneﬁts of conjunctive normal forms in that they\nallow for a fast and easy syntactic test of validity. Therefore, one wonders\nwhether any formula can be transformed into an equivalent formula in CNF.\nWe now develop an algorithm achieving just that. Note that, by Deﬁni-\ntion 1.40, a formula is valid iﬀany of its equivalent formulas is valid. We\nreduce the problem of determining whether any φ is valid to the problem\nof computing an equivalent ψ ≡φ such that ψ is in CNF and checking, via\nLemma 1.43, whether ψ is valid.\n1.5 Normal forms\n59\nBefore we sketch such a procedure, we make some general remarks about\nits possibilities and its realisability constraints. First of all, there could be\nmore or less eﬃcient ways of computing such normal forms. But even more\nso, there could be many possible correct outputs, for ψ1 ≡φ and ψ2 ≡φ\ndo not generally imply that ψ1 is the same as ψ2, even if ψ1 and ψ2 are in\nCNF. For example, take φ\ndef\n= p, ψ1\ndef\n= p and ψ2\ndef\n= p ∧(p ∨q); then convince\nyourself that φ ≡ψ2 holds. Having this ambiguity of equivalent conjunctive\nnormal forms, the computation of a CNF for φ with minimal ‘cost’ (where\n‘cost’ could for example be the number of conjuncts, or the height of φ’s\nparse tree) becomes a very important practical problem, an issue persued in\nChapter 6. Right now, we are content with stating a deterministic algorithm\nwhich always computes the same output CNF for a given input φ.\nThis algorithm, called CNF, should satisfy the following requirements:\n(1)",
    "parse tree) becomes a very important practical problem, an issue persued in\nChapter 6. Right now, we are content with stating a deterministic algorithm\nwhich always computes the same output CNF for a given input φ.\nThis algorithm, called CNF, should satisfy the following requirements:\n(1)\nCNF terminates for all formulas of propositional logic as input;\n(2)\nfor each such input, CNF outputs an equivalent formula; and\n(3)\nall output computed by CNF is in CNF.\nIf a call of CNF with a formula φ of propositional logic as input terminates,\nwhich is enforced by (1), then (2) ensures that ψ ≡φ holds for the output\nψ. Thus, (3) guarantees that ψ is an equivalent CNF of φ. So φ is valid iﬀ\nψ is valid; and checking the latter is easy relative to the length of ψ.\nWhat kind of strategy should CNF employ? It will have to function\ncorrectly for all, i.e. inﬁnitely many, formulas of propositional logic. This\nstrongly suggests to write a procedure that computes a CNF by structural\ninduction on the formula φ. For example, if φ is of the form φ1 ∧φ2, we\nmay simply compute conjunctive normal forms ηi for φi (i = 1, 2), where-\nupon η1 ∧η2 is a conjunctive normal form which is equivalent to φ provided\nthat ηi ≡φi (i = 1, 2). This strategy also suggests to use proof by structural\ninduction on φ to prove that CNF meets the requirements (1–3) stated above.\nGiven a formula φ as input, we ﬁrst do some preprocessing. Initially, we\ntranslate away all implications in φ by replacing all subformulas of the form\nψ →η by ¬ψ ∨η. This is done by a procedure called IMPL FREE. Note that\nthis procedure has to be recursive, for there might be implications in ψ or\nη as well.\nThe application of IMPL FREE might introduce double negations into the\noutput formula. More importantly, negations whose scopes are non-atomic\nformulas might still be present. For example, the formula p ∧¬(p ∧q) has\nsuch a negation with p ∧q as its scope. Essentially, the question is whether",
    "The application of IMPL FREE might introduce double negations into the\noutput formula. More importantly, negations whose scopes are non-atomic\nformulas might still be present. For example, the formula p ∧¬(p ∧q) has\nsuch a negation with p ∧q as its scope. Essentially, the question is whether\none can eﬃciently compute a CNF for ¬φ from a CNF for φ. Since nobody\nseems to know the answer, we circumvent the question by translating ¬φ\n60\n1 Propositional logic\ninto an equivalent formula that contains only negations of atoms. Formulas\nwhich only negate atoms are said to be in negation normal form (NNF). We\nspell out such a procedure, NNF, in detail later on. The key to its speciﬁcation\nfor implication-free formulas lies in the de Morgan rules. The second phase\nof the preprocessing, therefore, calls NNF with the implication-free output of\nIMPL FREE to obtain an equivalent formula in NNF.\nAfter all this preprocessing, we obtain a formula φ′ which is the result of\nthe call NNF (IMPL FREE (φ)). Note that φ′ ≡φ since both algorithms only\ntransform formulas into equivalent ones. Since φ′ contains no occurrences\nof →and since only atoms in φ′ are negated, we may program CNF by an\nanalysis of only three cases: literals, conjunctions and disjunctions.\nr If φ is a literal, it is by deﬁnition in CNF and so CNF outputs φ.\nr If φ equals φ1 ∧φ2, we call CNF recursively on each φi to get the respective output\nηi and return the CNF η1 ∧η2 as output for input φ.\nr If φ equals φ1 ∨φ2, we again call CNF recursively on each φi to get the respective\noutput ηi; but this time we must not simply return η1 ∨η2 since that formula is\ncertainly not in CNF, unless η1 and η2 happen to be literals.\nSo how can we complete the program in the last case? Well, we may resort\nto the distributivity laws, which entitle us to translate any disjunction of\nconjunctions into a conjunction of disjunctions. However, for this to result in\na CNF, we need to make certain that those disjunctions generated contain",
    "to the distributivity laws, which entitle us to translate any disjunction of\nconjunctions into a conjunction of disjunctions. However, for this to result in\na CNF, we need to make certain that those disjunctions generated contain\nonly literals. We apply a strategy for using distributivity based on matching\npatterns in φ1 ∨φ2. This results in an independent algorithm called DISTR\nwhich will do all that work for us. Thus, we simply call DISTR with the pair\n(η1, η2) as input and pass along its result.\nAssuming that we already have written code for IMPL FREE, NNF and\nDISTR, we may now write pseudo code for CNF:\nfunction CNF (φ):\n/* precondition: φ implication free and in NNF */\n/* postcondition: CNF (φ) computes an equivalent CNF for φ */\nbegin function\ncase\nφ is a literal: return φ\nφ is φ1 ∧φ2 : return CNF (φ1) ∧CNF (φ2)\nφ is φ1 ∨φ2 : return DISTR (CNF (φ1), CNF (φ2))\nend case\nend function\n1.5 Normal forms\n61\nNotice how the calling of DISTR is done with the computed conjunctive nor-\nmal forms of φ1 and φ2. The routine DISTR has η1 and η2 as input parameters\nand does a case analysis on whether these inputs are conjunctions. What\nshould DISTR do if none of its input formulas is such a conjunction? Well,\nsince we are calling DISTR for inputs η1 and η2 which are in CNF, this can\nonly mean that η1 and η2 are literals, or disjunctions of literals. Thus, η1 ∨η2\nis in CNF.\nOtherwise, at least one of the formulas η1 and η2 is a conjunction. Since\none conjunction suﬃces for simplifying the problem, we have to decide which\nconjunct we want to transform if both formulas are conjunctions. That way\nwe maintain that our algorithm CNF is deterministic. So let us suppose that\nη1 is of the form η11 ∧η12. Then the distributive law says that η1 ∨η2 ≡\n(η11 ∨η2) ∧(η12 ∨η2). Since all participating formulas η11, η12 and η2 are\nin CNF, we may call DISTR again for the pairs (η11, η2) and (η12, η2), and\nthen simply form their conjunction. This is the key insight for writing the\nfunction DISTR.",
    "(η11 ∨η2) ∧(η12 ∨η2). Since all participating formulas η11, η12 and η2 are\nin CNF, we may call DISTR again for the pairs (η11, η2) and (η12, η2), and\nthen simply form their conjunction. This is the key insight for writing the\nfunction DISTR.\nThe case when η2 is a conjunction is symmetric and the structure of\nthe recursive call of DISTR is then dictated by the equivalence η1 ∨η2 ≡\n(η1 ∨η21) ∧(η1 ∨η22), where η2 = η21 ∧η22:\nfunction DISTR (η1, η2):\n/* precondition: η1 and η2 are in CNF */\n/* postcondition: DISTR (η1, η2) computes a CNF for η1 ∨η2 */\nbegin function\ncase\nη1 is η11 ∧η12 : return DISTR (η11, η2) ∧DISTR (η12, η2)\nη2 is η21 ∧η22 : return DISTR (η1, η21) ∧DISTR (η1, η22)\notherwise (= no conjunctions): return η1 ∨η2\nend case\nend function\nNotice how the three clauses are exhausting all possibilities. Furthermore,\nthe ﬁrst and second cases overlap if η1 and η2 are both conjunctions. It\nis then our understanding that this code will inspect the clauses of a case\nstatement from the top to the bottom clause. Thus, the ﬁrst clause would\napply.\nHaving speciﬁed the routines CNF and DISTR, this leaves us with the\ntask of writing the functions IMPL FREE and NNF. We delegate the design\n62\n1 Propositional logic\nof IMPL FREE to the exercises. The function NNF has to transform any\nimplication-free formula into an equivalent one in negation normal form.\nFour examples of formulas in NNF are\np\n¬p\n¬p ∧(p ∧q)\n¬p ∧(p →q),\nalthough we won’t have to deal with a formula of the last kind since →\nwon’t occur. Examples of formulas which are not in NNF are ¬¬p and\n¬(p ∧q).\nAgain, we program NNF recursively by a case analysis over the structure of\nthe input formula φ. The last two examples already suggest a solution for two\nof these clauses. In order to compute a NNF of ¬¬φ, we simply compute\na NNF of φ. This is a sound strategy since φ and ¬¬φ are semantically\nequivalent. If φ equals ¬(φ1 ∧φ2), we use the de Morgan rule ¬(φ1 ∧φ2) ≡",
    "the input formula φ. The last two examples already suggest a solution for two\nof these clauses. In order to compute a NNF of ¬¬φ, we simply compute\na NNF of φ. This is a sound strategy since φ and ¬¬φ are semantically\nequivalent. If φ equals ¬(φ1 ∧φ2), we use the de Morgan rule ¬(φ1 ∧φ2) ≡\n¬φ1 ∨¬φ2 as a recipe for how NNF should call itself recursively in that case.\nDually, the case of φ being ¬(φ1 ∨φ2) appeals to the other de Morgan rule\n¬(φ1 ∨φ2) ≡¬φ1 ∧¬φ2 and, if φ is a conjunction or disjunction, we simply\nlet NNF pass control to those subformulas. Clearly, all literals are in NNF.\nThe resulting code for NNF is thus\nfunction NNF (φ):\n/* precondition: φ is implication free */\n/* postcondition: NNF (φ) computes a NNF for φ */\nbegin function\ncase\nφ is a literal: return φ\nφ is ¬¬φ1 : return NNF (φ1)\nφ is φ1 ∧φ2 : return NNF (φ1) ∧NNF (φ2)\nφ is φ1 ∨φ2 : return NNF (φ1) ∨NNF (φ2)\nφ is ¬(φ1 ∧φ2): return NNF (¬φ1) ∨NNF (¬φ2)\nφ is ¬(φ1 ∨φ2): return NNF (¬φ1) ∧NNF (¬φ2)\nend case\nend function\nNotice that these cases are exhaustive due to the algorithm’s precondition.\nGiven any formula φ of propositional logic, we may now convert it into an\n1.5 Normal forms\n63\nequivalent CNF by calling CNF (NNF (IMPL FREE (φ))). In the exercises, you\nare asked to show that\nr all four algorithms terminate on input meeting their preconditions,\nr the result of CNF (NNF (IMPL FREE (φ))) is in CNF and\nr that result is semantically equivalent to φ.\nWe will return to the important issue of formally proving the correctness of\nprograms in Chapter 4.\nLet us now illustrate the programs coded above on some concrete exam-\nples. We begin by computing CNF (NNF (IMPL FREE (¬p ∧q →p ∧(r →q)))).\nWe show almost all details of this computation and you should compare this\nwith how you would expect the code above to behave. First, we compute\nIMPL FREE (φ):\nIMPL FREE (φ) = ¬IMPL FREE (¬p ∧q) ∨IMPL FREE (p ∧(r →q))\n= ¬((IMPL FREE ¬p) ∧(IMPL FREE q)) ∨IMPL FREE (p ∧(r →q))",
    "We show almost all details of this computation and you should compare this\nwith how you would expect the code above to behave. First, we compute\nIMPL FREE (φ):\nIMPL FREE (φ) = ¬IMPL FREE (¬p ∧q) ∨IMPL FREE (p ∧(r →q))\n= ¬((IMPL FREE ¬p) ∧(IMPL FREE q)) ∨IMPL FREE (p ∧(r →q))\n= ¬((¬p) ∧IMPL FREE q) ∨IMPL FREE (p ∧(r →q))\n= ¬(¬p ∧q) ∨IMPL FREE (p ∧(r →q))\n= ¬(¬p ∧q) ∨((IMPL FREE p) ∧IMPL FREE (r →q))\n= ¬(¬p ∧q) ∨(p ∧IMPL FREE (r →q))\n= ¬(¬p ∧q) ∨(p ∧(¬(IMPL FREE r) ∨(IMPL FREE q)))\n= ¬(¬p ∧q) ∨(p ∧(¬r ∨(IMPL FREE q)))\n= ¬(¬p ∧q) ∨(p ∧(¬r ∨q)).\nSecond, we compute NNF (IMPL FREE φ):\nNNF (IMPL FREE φ) = NNF (¬(¬p ∧q)) ∨NNF (p ∧(¬r ∨q))\n= NNF (¬(¬p) ∨¬q) ∨NNF (p ∧(¬r ∨q))\n= (NNF (¬¬p)) ∨(NNF (¬q)) ∨NNF (p ∧(¬r ∨q))\n= (p ∨(NNF (¬q))) ∨NNF (p ∧(¬r ∨q))\n= (p ∨¬q) ∨NNF (p ∧(¬r ∨q))\n= (p ∨¬q) ∨((NNF p) ∧(NNF (¬r ∨q)))\n= (p ∨¬q) ∨(p ∧(NNF (¬r ∨q)))\n= (p ∨¬q) ∨(p ∧((NNF (¬r)) ∨(NNF q)))\n= (p ∨¬q) ∨(p ∧(¬r ∨(NNF q)))\n= (p ∨¬q) ∨(p ∧(¬r ∨q)).\n64\n1 Propositional logic\nThird, we ﬁnish it oﬀwith\nCNF (NNF (IMPL FREE φ)) = CNF ((p ∨¬q) ∨(p ∧(¬r ∨q)))\n= DISTR (CNF (p ∨¬q), CNF (p ∧(¬r ∨q)))\n= DISTR (p ∨¬q, CNF (p ∧(¬r ∨q)))\n= DISTR (p ∨¬q, p ∧(¬r ∨q))\n= DISTR (p ∨¬q, p) ∧DISTR (p ∨¬q, ¬r ∨q)\n= (p ∨¬q ∨p) ∧DISTR (p ∨¬q, ¬r ∨q)\n= (p ∨¬q ∨p) ∧(p ∨¬q ∨¬r ∨q) .\nThe formula (p ∨¬q ∨p) ∧(p ∨¬q ∨¬r ∨q) is thus the result of the call\nCNF (NNF (IMPL FREE φ)) and is in conjunctive normal form and equivalent to\nφ. Note that it is satisﬁable (choose p to be true) but not valid (choose p to be\nfalse and q to be true); it is also equivalent to the simpler conjunctive normal\nform p ∨¬q. Observe that our algorithm does not do such optimisations so\none would need a separate optimiser running on the output. Alternatively,\none might change the code of our functions to allow for such optimisations\n‘on the ﬂy,’ a computational overhead which could prove to be counter-\nproductive.\nYou should realise that we omitted several computation steps in the sub-",
    "one might change the code of our functions to allow for such optimisations\n‘on the ﬂy,’ a computational overhead which could prove to be counter-\nproductive.\nYou should realise that we omitted several computation steps in the sub-\ncalls CNF (p ∨¬q) and CNF (p ∧(¬r ∨q)). They return their input as a result\nsince the input is already in conjunctive normal form.\nAs a second example, consider φ\ndef\n= r →(s →(t ∧s →r)). We compute\nIMPL FREE (φ) = ¬(IMPL FREE r) ∨IMPL FREE (s →(t ∧s →r))\n= ¬r ∨IMPL FREE (s →(t ∧s →r))\n= ¬r ∨(¬(IMPL FREE s) ∨IMPL FREE (t ∧s →r))\n= ¬r ∨(¬s ∨IMPL FREE (t ∧s →r))\n= ¬r ∨(¬s ∨(¬(IMPL FREE (t ∧s)) ∨IMPL FREE r))\n= ¬r ∨(¬s ∨(¬((IMPL FREE t) ∧(IMPL FREE s)) ∨IMPL FREE r))\n= ¬r ∨(¬s ∨(¬(t ∧(IMPL FREE s)) ∨(IMPL FREE r)))\n= ¬r ∨(¬s ∨(¬(t ∧s)) ∨(IMPL FREE r))\n= ¬r ∨(¬s ∨(¬(t ∧s)) ∨r)\n1.5 Normal forms\n65\nNNF (IMPL FREE φ) = NNF (¬r ∨(¬s ∨¬(t ∧s) ∨r))\n= (NNF ¬r) ∨NNF (¬s ∨¬(t ∧s) ∨r)\n= ¬r ∨NNF (¬s ∨¬(t ∧s) ∨r)\n= ¬r ∨(NNF (¬s) ∨NNF (¬(t ∧s) ∨r))\n= ¬r ∨(¬s ∨NNF (¬(t ∧s) ∨r))\n= ¬r ∨(¬s ∨(NNF (¬(t ∧s)) ∨NNF r))\n= ¬r ∨(¬s ∨(NNF (¬t ∨¬s)) ∨NNF r)\n= ¬r ∨(¬s ∨((NNF (¬t) ∨NNF (¬s)) ∨NNF r))\n= ¬r ∨(¬s ∨((¬t ∨NNF (¬s)) ∨NNF r))\n= ¬r ∨(¬s ∨((¬t ∨¬s) ∨NNF r))\n= ¬r ∨(¬s ∨((¬t ∨¬s) ∨r))\nwhere the latter is already in CNF and valid as r has a matching ¬r.\n1.5.3 Horn clauses and satisfiability\nWe have already commented on the computational price we pay for trans-\nforming a propositional logic formula into an equivalent CNF. The latter\nclass of formulas has an easy syntactic check for validity, but its test for\nsatisﬁability is very hard in general. Fortunately, there are practically im-\nportant subclasses of formulas which have much more eﬃcient ways of de-\nciding their satisﬁability. One such example is the class of Horn formu-\nlas; the name ‘Horn’ is derived from the logician A. Horn’s last name.\nWe shortly deﬁne them and give an algorithm for checking their satisﬁ-\nability.\nRecall that the logical constants ⊥(‘bottom’) and ⊤(‘top’) denote an",
    "ciding their satisﬁability. One such example is the class of Horn formu-\nlas; the name ‘Horn’ is derived from the logician A. Horn’s last name.\nWe shortly deﬁne them and give an algorithm for checking their satisﬁ-\nability.\nRecall that the logical constants ⊥(‘bottom’) and ⊤(‘top’) denote an\nunsatisﬁable formula, respectively, a tautology.\nDeﬁnition 1.46 A Horn formula is a formula φ of propositional logic if it\ncan be generated as an instance of H in this grammar:\nP ::= ⊥| ⊤| p\nA ::= P | P ∧A\nC ::= A →P\nH ::= C | C ∧H.\n(1.7)\nWe call each instance of C a Horn clause.\n66\n1 Propositional logic\nHorn formulas are conjunctions of Horn clauses. A Horn clause is an impli-\ncation whose assumption A is a conjunction of propositions of type P and\nwhose conclusion is also of type P. Examples of Horn formulas are\n(p ∧q ∧s →p) ∧(q ∧r →p) ∧(p ∧s →s)\n(p ∧q ∧s →⊥) ∧(q ∧r →p) ∧(⊤→s)\n(p2 ∧p3 ∧p5 →p13) ∧(⊤→p5) ∧(p5 ∧p11 →⊥).\nExamples of formulas which are not Horn formulas are\n(p ∧q ∧s →¬p) ∧(q ∧r →p) ∧(p ∧s →s)\n(p ∧q ∧s →⊥) ∧(¬q ∧r →p) ∧(⊤→s)\n(p2 ∧p3 ∧p5 →p13 ∧p27) ∧(⊤→p5) ∧(p5 ∧p11 →⊥)\n(p2 ∧p3 ∧p5 →p13 ∧p27) ∧(⊤→p5) ∧(p5 ∧p11 ∨⊥).\nThe ﬁrst formula is not a Horn formula since ¬p, the conclusion of the\nimplication of the ﬁrst conjunct, is not of type P. The second formula does\nnot qualify since the premise of the implication of the second conjunct,\n¬q ∧r, is not a conjunction of atoms, ⊥, or ⊤. The third formula is not a\nHorn formula since the conclusion of the implication of the ﬁrst conjunct,\np13 ∧p27, is not of type P. The fourth formula clearly is not a Horn formula\nsince it is not a conjunction of implications.\nThe algorithm we propose for deciding the satisﬁability of a Horn for-\nmula φ maintains a list of all occurrences of type P in φ and proceeds like\nthis:\n1.\nIt marks ⊤if it occurs in that list.\n2.\nIf there is a conjunct P1 ∧P2 ∧· · · ∧Pki →P ′ of φ such that all Pj with 1 ≤j ≤\nki are marked, mark P ′ as well and go to 2. Otherwise (= there is no conjunct",
    "mula φ maintains a list of all occurrences of type P in φ and proceeds like\nthis:\n1.\nIt marks ⊤if it occurs in that list.\n2.\nIf there is a conjunct P1 ∧P2 ∧· · · ∧Pki →P ′ of φ such that all Pj with 1 ≤j ≤\nki are marked, mark P ′ as well and go to 2. Otherwise (= there is no conjunct\nP1 ∧P2 ∧· · · ∧Pki →P ′ such that all Pj are marked) go to 3.\n3.\nIf ⊥is marked, print out ‘The Horn formula φ is unsatisﬁable.’ and stop. Oth-\nerwise, go to 4.\n4.\nPrint out ‘The Horn formula φ is satisﬁable.’ and stop.\nIn these instructions, the markings of formulas are shared by all other oc-\ncurrences of these formulas in the Horn formula. For example, once we\nmark p2 because of one of the criteria above, then all other occurrences\nof p2 are marked as well. We use pseudo code to specify this algorithm\nformally:\n1.5 Normal forms\n67\nfunction HORN (φ):\n/* precondition: φ is a Horn formula */\n/* postcondition: HORN (φ) decides the satisﬁability for φ */\nbegin function\nmark all occurrences of ⊤in φ;\nwhile there is a conjunct P1 ∧P2 ∧· · · ∧Pki →P ′ of φ\nsuch that all Pj are marked but P ′ isn’t do\nmark P ′\nend while\nif ⊥is marked then return ‘unsatisﬁable’ else return ‘satisﬁable’\nend function\nWe need to make sure that this algorithm terminates on all Horn formulas\nφ as input and that its output (= its decision) is always correct.\nTheorem 1.47 The algorithm HORN is correct for the satisﬁability decision\nproblem of Horn formulas and has no more than n + 1 cycles in its while-\nstatement if n is the number of atoms in φ. In particular, HORN always\nterminates on correct input.\nProof: Let us ﬁrst consider the question of program termination. Notice\nthat entering the body of the while-statement has the eﬀect of marking an\nunmarked P which is not ⊤. Since this marking applies to all occurrences\nof P in φ, the while-statement can have at most one more cycle than there\nare atoms in φ.\nSince we guaranteed termination, it suﬃces to show that the answers",
    "unmarked P which is not ⊤. Since this marking applies to all occurrences\nof P in φ, the while-statement can have at most one more cycle than there\nare atoms in φ.\nSince we guaranteed termination, it suﬃces to show that the answers\ngiven by the algorithm HORN are always correct. To that end, it helps to\nreveal the functional role of those markings. Essentially, marking a P means\nthat that P has got to be true if the formula φ is ever going to be satisﬁable.\nWe use mathematical induction to show that\n‘All marked P are true for all valuations in which φ evaluates to T.’ (1.8)\nholds after any number of executions of the body of the while-statement\nabove. The base case, zero executions, is when the while-statement has not\nyet been entered but we already and only marked all occurrences of ⊤. Since\n⊤must be true in all valuations, (1.8) follows.\nIn the inductive step, we assume that (1.8) holds after k cycles of the\nwhile-statement. Then we need to show that same assertion for all marked\nP after k + 1 cycles. If we enter the (k + 1)th cycle, the condition of the\nwhile-statement is certainly true. Thus, there exists a conjunct P1 ∧P2 ∧\n· · · ∧Pki →P ′ of φ such that all Pj are marked. Let v be any valuation\n68\n1 Propositional logic\nin which φ is true. By our induction hypothesis, we know that all Pj and\ntherefore P1 ∧P2 ∧· · · ∧Pki have to be true in v as well. The conjunct P1 ∧\nP2 ∧· · · ∧Pki →P ′ of φ has be to true in v, too, from which we infer that\nP ′ has to be true in v.\nBy mathematical induction, we therefore secured that (1.8) holds no mat-\nter how many cycles that while-statement went through.\nFinally, we need to make sure that the if-statement above always renders\ncorrect replies. First, if ⊥is marked, then there has to be some conjunct\nP1 ∧P2 ∧· · · ∧Pki →⊥of φ such that all Pi are marked as well. By (1.8)\nthat conjunct of φ evaluates to T →F = F whenever φ is true. As this is\nimpossible the reply ‘unsatisﬁable’ is correct. Second, if ⊥is not marked, we",
    "correct replies. First, if ⊥is marked, then there has to be some conjunct\nP1 ∧P2 ∧· · · ∧Pki →⊥of φ such that all Pi are marked as well. By (1.8)\nthat conjunct of φ evaluates to T →F = F whenever φ is true. As this is\nimpossible the reply ‘unsatisﬁable’ is correct. Second, if ⊥is not marked, we\nsimply assign T to all marked atoms and F to all unmarked atoms and use\nproof by contradiction to show that φ has to be true with respect to that\nvaluation.\nIf φ is not true under that valuation, it must make one of its principal\nconjuncts P1 ∧P2 ∧· · · ∧Pki →P ′ false. By the semantics of implication\nthis can only mean that all Pj are true and P ′ is false. By the deﬁnition of our\nvaluation, we then infer that all Pj are marked, so P1 ∧P2 ∧· · · ∧Pki →P ′\nis a conjunct of φ that would have been dealt with in one of the cycles of\nthe while-statement and so P ′ is marked, too. Since ⊥is not marked, P ′ has\nto be ⊤or some atom q. In any event, the conjunct is then true by (1.8), a\ncontradiction\n2\nNote that the proof by contradiction employed in the last proof was not\nreally needed. It just made the argument seem more natural to us. The\nliterature is full of such examples where one uses proof by contradiction\nmore out of psychological than proof-theoretical necessity.\n1.6 SAT solvers\nThe marking algorithm for Horn formulas computes marks as constraints\non all valuations that can make a formule true. By (1.8), all marked atoms\nhave to be true for any such valuation. We can extend this idea to general\nformulas φ by computing constraints saying which subformulas of φ require\na certain truth value for all valuations that make φ true:\n‘All marked subformulas evaluate to their mark value\nfor all valuations in which φ evaluates to T.’\n(1.9)\nIn that way, marking atomic formulas generalizes to marking subformu-\nlas; and ‘true’ marks generalize into ‘true’ and ‘false’ marks. At the same\n1.6 SAT solvers\n69\ntime, (1.9) serves as a guide for designing an algorithm and as an invariant",
    "for all valuations in which φ evaluates to T.’\n(1.9)\nIn that way, marking atomic formulas generalizes to marking subformu-\nlas; and ‘true’ marks generalize into ‘true’ and ‘false’ marks. At the same\n1.6 SAT solvers\n69\ntime, (1.9) serves as a guide for designing an algorithm and as an invariant\nfor proving its correctness.\n1.6.1 A linear solver\nWe will execute this marking algorithm on the parse tree of formulas, except\nthat we will translate formulas into the adequate fragment\nφ ::= p | (¬φ) | (φ ∧φ)\n(1.10)\nand then share common subformulas of the resulting parse tree, making the\ntree into a directed, acyclic graph (DAG). The inductively deﬁned transla-\ntion\nT(p) = p\nT(¬φ) = ¬T(φ)\nT(φ1 ∧φ2) = T(φ1) ∧T(φ2)\nT(φ1 ∨φ2) = ¬(¬T(φ1) ∧¬T(φ2))\nT(φ1 →φ2) = ¬(T(φ1) ∧¬T(φ2))\ntransforms formulas generated by (1.3) into formulas generated by (1.10)\nsuch that φ and T(φ) are semantically equivalent and have the same propo-\nsitional atoms. Therefore, φ is satisﬁable iﬀT(φ) is satisﬁable; and the set\nof valuations for which φ is true equals the set of valuations for which T(φ)\nis true. The latter ensures that the diagnostics of a SAT solver, applied to\nT(φ), is meaningful for the original formula φ. In the exercises, you are asked\nto prove these claims.\nExample 1.48 For the formula φ being p ∧¬(q ∨¬p) we compute T(φ) =\np ∧¬¬(¬q ∧¬¬p). The parse tree and DAG of T(φ) are depicted in Fig-\nure 1.12.\nAny valuation that makes p ∧¬¬(¬q ∧¬¬p) true has to assign T to the\ntopmost ∧-node in its DAG of Figure 1.12. But that forces the mark T on\nthe p-node and the topmost ¬-node. In the same manner, we arrive at a\ncomplete set of constraints in Figure 1.13, where the time stamps ‘1:’ etc\nindicate the order in which we applied our intuitive reasoning about these\nconstraints; this order is generally not unique.\nThe formal set of rules for forcing new constraints from old ones is depicted\nin Figure 1.14. A small circle indicates any node (¬, ∧or atom). The force",
    "indicate the order in which we applied our intuitive reasoning about these\nconstraints; this order is generally not unique.\nThe formal set of rules for forcing new constraints from old ones is depicted\nin Figure 1.14. A small circle indicates any node (¬, ∧or atom). The force\nlaws for negation, ¬t and ¬f, indicate that a truth constraint on a ¬-node\nforces its dual value at its sub-node and vice versa. The law ∧te propagates\na T constraint on a ∧-node to its two sub-nodes; dually, ∧ti forces a T mark\non a ∧-node if both its children have that mark. The laws ∧ﬂand ∧fr force a\nF constraint on a ∧-node if any of its sub-nodes has a F value. The laws ∧ﬂl\n70\n1 Propositional logic\n∧\n¬\n¬\n¬\n¬\n¬\nq\np\n¬\n¬\n¬\nq\n∧\n¬\n¬\n∧\np\n∧\np\nFigure 1.12. Parse tree (left) and directed acyclic graph (right) of the\nformula from Example 1.48. The p-node is shared on the right.\n∧\np\n¬\n¬\n¬\n¬\n¬\nq\n∧\n1: T\n2: T\n2: T\n3: F\n4: T\n3: F\n4: T\n5: T\n6: F\nFigure 1.13. A witness to the satisfiability of the formula represented\nby this DAG.\nand ∧frr are more complex: if an ∧-node has a F constraint and one of its\nsub-nodes has a T constraint, then the other sub-node obtains a F-constraint.\nPlease check that all constraints depicted in Figure 1.13 are derivable from\nthese rules. The fact that each node in a DAG obtained a forced marking\ndoes not yet show that this is a witness to the satisﬁability of the formula\n1.6 SAT solvers\n71\nT\nF\n¬\nT\nF\nT\nT\nF\nF\nF\nF\nF\nT\nF\nF\nT\nF\nT\n∧\n∧\n∧\n∧\n¬t:\n¬f:\n∧ﬂ:\n∧fr:\n∧ﬂl:\n∧frr:\nfalse conjuncts\nforce false conjunction\nfalse conjunction and true conjunct\n¬\n∧\nforcing laws for negation\nforce false conjunction\nT\ntrue conjunctions force true conjunction\nT\n∧\nT\n∧te:\n∧ti:\ntrue conjunction forces true conjuncts\nFigure 1.14. Rules for flow of constraints in a formula’s DAG. Small\ncircles indicate arbitrary nodes (¬, ∧or atom). Note that the rules ∧ﬂl,\n∧frr and ∧ti require that the source constraints of both =⇒are present.\nrepresented by this DAG. A post-processing phase takes the marks for all",
    "Figure 1.14. Rules for flow of constraints in a formula’s DAG. Small\ncircles indicate arbitrary nodes (¬, ∧or atom). Note that the rules ∧ﬂl,\n∧frr and ∧ti require that the source constraints of both =⇒are present.\nrepresented by this DAG. A post-processing phase takes the marks for all\natoms and re-computes marks of all other nodes in a bottom-up manner, as\ndone in Section 1.4 on parse trees. Only if the resulting marks match the\nones we computed have we found a witness. Please verify that this is the\ncase in Figure 1.13.\nWe can apply SAT solvers to checking whether sequents are valid. For\nexample, the sequent p ∧q →r ⊢p →q →r is valid iﬀ(p ∧q →r) →p →\nq →r is a theorem (why?) iﬀφ = ¬((p ∧q →r) →p →q →r) is not satis-\nﬁable. The DAG of T(φ) is depicted in Figure 1.15. The annotations “1” etc\nindicate which nodes represent which sub-formulas. Notice that such DAGs\nmay be constructed by applying the translation clauses for T to sub-formulas\nin a bottom-up manner – sharing equal subgraphs were applicable.\nThe ﬁndings of our SAT solver can be seen in Figure 1.16. The solver\nconcludes that the indicated node requires the marks T and F for (1.9) to be\nmet. Such contradictory constraints therefore imply that all formulas T(φ)\nwhose DAG equals that of this ﬁgure are not satisﬁable. In particular, all\n72\n1 Propositional logic\np\nq\nr\n¬\n∧\n∧\n∧\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n¬\n= ”3” →”2”\n“5” = entire formula\n“4”\n“3” = p ∧q →r\n“2” = p →”1”\n“1” = q →r\n“2”\n“3”\n“1”\n“4”\n“5”\nFigure 1.15. The DAG for the translation of ¬((p ∧q →r) →p →q →\nr). Labels ‘‘1’’ etc indicate which nodes represent what subformulas.\nsuch φ are unsatisﬁable. This SAT solver has a linear running time in the\nsize of the DAG for T(φ). Since that size is a linear function of the length\nof φ – the translation T causes only a linear blow-up – our SAT solver has\na linear running time in the length of the formula. This linearity came with\na price: our linear solver fails for all formulas of the form ¬(φ1 ∧φ2).\n1.6.2 A cubic solver",
    "of φ – the translation T causes only a linear blow-up – our SAT solver has\na linear running time in the length of the formula. This linearity came with\na price: our linear solver fails for all formulas of the form ¬(φ1 ∧φ2).\n1.6.2 A cubic solver\nWhen we applied our linear SAT solver, we saw two possible outcomes:\nwe either detected contradictory constraints, meaning that no formula rep-\nresented by the DAG is satisﬁable (e.g. Fig. 1.16); or we managed to force\nconsistent constraints on all nodes, in which case all formulas represented by\nthis DAG are satisﬁable with those constraints as a witness (e.g. Fig. 1.13).\nUnfortunately, there is a third possibility: all forced constraints are consis-\ntent with each other, but not all nodes are constrained! We already remarked\nthat this occurs for formulas of the form ¬(φ1 ∧φ2).\n1.6 SAT solvers\n73\np\nq\nr\n¬\n∧\n∧\n∧\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n¬\n1: T\n2: F\n3: T\n4: T\n4: T\n5: F\n6: T\n5: F\n7: T\n8: F\n9: T\n11: F\n10: T\n10: T\n7: T\nits conjunction parent\n– a contradiction\nand ∧frr force F\nits children and\n∧ti force T\nFigure 1.16. The forcing rules, applied to the DAG of Figure 1.15,\ndetect contradictory constraints at the indicated node – implying that\nthe initial constraint ‘1:T’ cannot be realized. Thus, formulas represented\nby this DAG are not satisfiable.\nRecall that checking validity of formulas in CNF is very easy. We already\nhinted at the fact that checking satisﬁability of formulas in CNF is hard. To\nillustrate, consider the formula\n((p ∨(q ∨r)) ∧((p ∨¬q) ∧((q ∨¬r) ∧((r ∨¬p) ∧(¬p ∨(¬q ∨¬r))))))\n(1.11)\nin CNF – based on Example 4.2, page 77, in [Pap94]. Intuitively, this formula\nshould not be satisﬁable. The ﬁrst and last clause in (1.11) ‘say’ that at least\none of p, q, and r are false and true (respectively). The remaining three\nclauses, in their conjunction, ‘say’ that p, q, and r all have the same truth\nvalue. This cannot be satisﬁable, and a good SAT solver should discover",
    "one of p, q, and r are false and true (respectively). The remaining three\nclauses, in their conjunction, ‘say’ that p, q, and r all have the same truth\nvalue. This cannot be satisﬁable, and a good SAT solver should discover\nthis without any user intervention. Unfortunately, our linear SAT solver can\nneither detect inconsistent constraints nor compute constraints for all nodes.\nFigure 1.17 depicts the DAG for T(φ), where φ is as in (1.11); and reveals\n74\n1 Propositional logic\np\nq\nr\n¬\n¬\n¬\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n∧\n∧\n5: T\n6: F\n3: T\n4: F\n5: T\n6: F\n3: F\n4: T\n3: T\n2: T\n1: T\n2: T\n∧\n5: F\n4: T\nFigure 1.17. The DAG for the translation of the formula in (1.11). It\nhas a ∧-spine of length 4 as it is a conjunction of five clauses. Its linear\nanalysis gets stuck: all forced constraints are consistent with each other\nbut several nodes, including all atoms, are unconstrained.\nthat our SAT solver got stuck: no inconsistent constraints were found and\nnot all nodes obtained constraints; in particular, no atom received a mark!\nSo how can we improve this analysis? Well, we can mimic the role of LEM\nto improve the precision of our SAT solver. For the DAG with marks as in\nFigure 1.17, pick any node n that is not yet marked. Then test node n by\nmaking two independent computations:\n1.\ndetermine which temporary marks are forced by adding to the marks in Fig-\nure 1.17 the T mark only to n; and\n2.\ndetermine which temporary marks are forced by adding, again to the marks in\nFigure 1.17, the F mark only to n.\n1.6 SAT solvers\n75\np\nq\nr\n¬\n¬\n¬\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n∧\n∧\n5: T\n6: F\n3: T\n4: F\n5: T\n6: F\n3: F\n4: T\n3: T\n2: T\n1: T\n2: T\n∧\n5: F\n4: T\na:T\nb:F\nc:T\nf:T\ng:F\ni:F\nh:T\nb:F\nc:T\nb:F\nc:T\nc:T\nconstraints\ntemporary T mark\nat test node;\nexplore consequences\ne:F\ng:F\nd:F\nat conjunction\ncontradictory\nFigure 1.18. Marking an unmarked node with T and exploring what\nnew constraints would follow from this. The analysis shows that this",
    "∧\n5: F\n4: T\na:T\nb:F\nc:T\nf:T\ng:F\ni:F\nh:T\nb:F\nc:T\nb:F\nc:T\nc:T\nconstraints\ntemporary T mark\nat test node;\nexplore consequences\ne:F\ng:F\nd:F\nat conjunction\ncontradictory\nFigure 1.18. Marking an unmarked node with T and exploring what\nnew constraints would follow from this. The analysis shows that this\ntest marking causes contradictory constraints. We use lowercase letters\n‘a:’ etc to denote temporary marks.\nIf both runs ﬁnd contradictory constraints, the algorithm stops and re-\nports that T(φ) is unsatisﬁable. Otherwise, all nodes that received the same\nmark in both of these runs receive that very mark as a permanent one; that\nis, we update the mark state of Figure 1.17 with all such shared marks.\nWe test any further unmarked nodes in the same manner until we either\nﬁnd contradictory permanent marks, a complete witness to satisﬁability (all\nnodes have consistent marks), or we have tested all currently unmarked\nnodes in this manner without detecting any shared marks. Only in the lat-\nter case does the analysis terminate without knowing whether the formulas\nrepresented by that DAG are satisﬁable.\n76\n1 Propositional logic\nExample 1.49 We revisit our stuck analysis of Figure 1.17. We test a ¬-\nnode and explore the consequences of setting that ¬-node’s mark to T; Fig-\nure 1.18 shows the result of that analysis. Dually, Figure 1.19 tests the\nconsequences of setting that ¬-node’s mark to F. Since both runs reveal a\ncontradiction, the algorithm terminates, ruling that the formula in (1.11) is\nnot satisﬁable.\nIn the exercises, you are asked to show that the speciﬁcation of our cubic\nSAT solver is sound. Its running time is indeed cubic in the size of the\nDAG (and the length of original formula). One factor stems from the linear\nSAT solver used in each test run. A second factor is introduced since each\nunmarked node has to be tested. The third factor is needed since each new\npermanent mark causes all unmarked nodes to be tested again.\np\nq\nr\n¬\n¬\n¬\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n∧\n¬",
    "SAT solver used in each test run. A second factor is introduced since each\nunmarked node has to be tested. The third factor is needed since each new\npermanent mark causes all unmarked nodes to be tested again.\np\nq\nr\n¬\n¬\n¬\n¬\n¬\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n¬\n∧\n∧\n∧\n5: T\n6: F\n3: T\n4: F\n5: T\n6: F\n3: F\n4: T\n3: T\n2: T\n1: T\n2: T\n∧\n5: F\n4: T\ntemporary F mark\nat test node;\nexplore consequences\na: F\nc:F\nd:T\nd:T\ne:F\nf:T\ng:F\ne:F\ng:F\ne:F\ncontradictory\nconstraints\nat conjunction\nb:T\nc:F\ne:F\nf:T\nc:F\nFigure 1.19. Marking the same unmarked node with F and exploring\nwhat new constraints would follow from this. The analysis shows that\nthis test marking also causes contradictory constraints.\n1.6 SAT solvers\n77\n∧\n¬\n∧\n∧\n¬\n∧\n∧\n¬\n¬\n∧\np\nq\nr\n¬\n1: T\n2: F\ntesting this node\nwith T renders\njustifying to mark\nit with F permanently\na contradiction\nanalysis gets stuck right away\nFigure 1.20. Testing the indicated node with T causes contradictory\nconstraints, so we may mark that node with F permanently. However,\nour algorithm does not seem to be able to decide satisfiability of this\nDAG even with that optimization.\nWe deliberately under-speciﬁed our cubic SAT solver, but any implemen-\ntation or optimization decisions need to secure soundness of the analysis.\nAll replies of the form\n1.\n‘The input formula is not satisﬁable’ and\n2.\n‘The input formula is satisﬁable under the following valuation . . . ’\nhave to be correct. The third form of reply ‘Sorry, I could not ﬁgure this one\nout.’ is correct by deﬁnition. :-) We brieﬂy discuss two sound modiﬁcations\nto the algorithm that introduce some overhead, but may cause the algorithm\nto decide many more instances. Consider the state of a DAG right after we\nhave explored consequences of a temporary mark on a test node.\n1.\nIf that state – permanent plus temporary markings – contains contradictory\nconstraints, we can erase all temporary marks and mark the test node perma-\nnently with the dual mark of its test. That is, if marking node n with v resulted",
    "have explored consequences of a temporary mark on a test node.\n1.\nIf that state – permanent plus temporary markings – contains contradictory\nconstraints, we can erase all temporary marks and mark the test node perma-\nnently with the dual mark of its test. That is, if marking node n with v resulted\nin a contradiction, it will get a permanent mark v, where T = F and F = T;\notherwise\n2.\nif that state managed to mark all nodes with consistent constraints, we report\nthese markings as a witness of satisﬁability and terminate the algorithm.\nIf none of these cases apply, we proceed as speciﬁed: promote shared marks\nof the two test runs to permanent ones, if applicable.\nExample 1.50 To see how one of these optimizations may make a diﬀer-\nence, consider the DAG in Figure 1.20. If we test the indicated node with\n78\n1 Propositional logic\nT, contradictory constraints arise. Since any witness of satisﬁability has to\nassign some value to that node, we infer that it cannot be T. Thus, we may\npermanently assign mark F to that node. For this DAG, such an optimiza-\ntion does not seem to help. No test of an unmarked node detects a shared\nmark or a shared contradiction. Our cubic SAT solver fails for this DAG.\n1.7 Exercises\nExercises 1.1\n1. Use ¬, →, ∧and ∨to express the following declarative sentences in propositional\nlogic; in each case state what your respective propositional atoms p, q, etc. mean:\n(a)\n*\nIf the sun shines today, then it won’t shine tomorrow.\n(b) Robert was jealous of Yvonne, or he was not in a good mood.\n(c) If the barometer falls, then either it will rain or it will snow.\n(d)\n*\nIf a request occurs, then either it will eventually be acknowledged, or the\nrequesting process won’t ever be able to make progress.\n(e) Cancer will not be cured unless its cause is determined and a new drug for\ncancer is found.\n(f) If interest rates go up, share prices go down.\n(g) If Smith has installed central heating, then he has sold his car or he has not\npaid his mortgage.\n(h)\n*",
    "(e) Cancer will not be cured unless its cause is determined and a new drug for\ncancer is found.\n(f) If interest rates go up, share prices go down.\n(g) If Smith has installed central heating, then he has sold his car or he has not\npaid his mortgage.\n(h)\n*\nToday it will rain or shine, but not both.\n(i)\n*\nIf Dick met Jane yesterday, they had a cup of coﬀee together, or they took\na walk in the park.\n(j) No shoes, no shirt, no service.\n(k) My sister wants a black and white cat.\n2. The formulas of propositional logic below implicitly assume the binding priorities\nof the logical connectives put forward in Convention 1.3. Make sure that you fully\nunderstand those conventions by reinserting as many brackets as possible. For\nexample, given p ∧q →r, change it to (p ∧q) →r since ∧binds more tightly\nthan →.\n(a)\n*\n¬p ∧q →r\n(b) (p →q) ∧¬(r ∨p →q)\n(c)\n*\n(p →q) →(r →s ∨t)\n(d) p ∨(¬q →p ∧r)\n(e)\n*\np ∨q →¬p ∧r\n(f) p ∨p →¬q\n(g)\n*\nWhy is the expression p ∨q ∧r problematic?\nExercises 1.2\n1. Prove the validity of the following sequents:\n(a) (p ∧q) ∧r, s ∧t ⊢q ∧s\n1.7 Exercises\n79\n(b) p ∧q ⊢q ∧p\n(c)\n*\n(p ∧q) ∧r ⊢p ∧(q ∧r)\n(d) p →(p →q), p ⊢q\n(e)\n*\nq →(p →r), ¬r, q ⊢¬p\n(f)\n*\n⊢(p ∧q) →p\n(g) p ⊢q →(p ∧q)\n(h)\n*\np ⊢(p →q) →q\n(i)\n*\n(p →r) ∧(q →r) ⊢p ∧q →r\n(j)\n*\nq →r ⊢(p →q) →(p →r)\n(k) p →(q →r), p →q ⊢p →r\n(l)\n*\np →q, r →s ⊢p ∨r →q ∨s\n(m) p ∨q ⊢r →(p ∨q) ∧r\n(n)\n*\n(p ∨(q →p)) ∧q ⊢p\n(o)\n*\np →q, r →s ⊢p ∧r →q ∧s\n(p) p →q ⊢((p ∧q) →p) ∧(p →(p ∧q))\n(q) ⊢q →(p →(p →(q →p)))\n(r)\n*\np →q ∧r ⊢(p →q) ∧(p →r)\n(s) (p →q) ∧(p →r) ⊢p →q ∧r\n(t) ⊢(p →q) →((r →s) →(p ∧r →q ∧s)); here you might be able to ‘recycle’\nand augment a proof from a previous exercise.\n(u) p →q ⊢¬q →¬p\n(v)\n*\np ∨(p ∧q) ⊢p\n(w) r, p →(r →q) ⊢p →(q ∧r)\n(x)\n*\np →(q ∨r), q →s, r →s ⊢p →s\n(y)\n*\n(p ∧q) ∨(p ∧r) ⊢p ∧(q ∨r).\n2. For the sequents below, show which ones are valid and which ones aren’t:\n(a)\n*\n¬p →¬q ⊢q →p\n(b)\n*\n¬p ∨¬q ⊢¬(p ∧q)\n(c)\n*\n¬p, p ∨q ⊢q\n(d)\n*\np ∨q, ¬q ∨r ⊢p ∨r\n(e)\n*\np →(q ∨r), ¬q, ¬r ⊢¬p without using the MT rule\n(f)\n*",
    "(x)\n*\np →(q ∨r), q →s, r →s ⊢p →s\n(y)\n*\n(p ∧q) ∨(p ∧r) ⊢p ∧(q ∨r).\n2. For the sequents below, show which ones are valid and which ones aren’t:\n(a)\n*\n¬p →¬q ⊢q →p\n(b)\n*\n¬p ∨¬q ⊢¬(p ∧q)\n(c)\n*\n¬p, p ∨q ⊢q\n(d)\n*\np ∨q, ¬q ∨r ⊢p ∨r\n(e)\n*\np →(q ∨r), ¬q, ¬r ⊢¬p without using the MT rule\n(f)\n*\n¬p ∧¬q ⊢¬(p ∨q)\n(g)\n*\np ∧¬p ⊢¬(r →q) ∧(r →q)\n(h) p →q, s →t ⊢p ∨s →q ∧t\n(i)\n*\n¬(¬p ∨q) ⊢p.\n3. Prove the validity of the sequents below:\n(a) ¬p →p ⊢p\n(b) ¬p ⊢p →q\n(c) p ∨q, ¬q ⊢p\n(d)\n*\n⊢¬p →(p →(p →q))\n(e) ¬(p →q) ⊢q →p\n(f) p →q ⊢¬p ∨q\n(g) ⊢¬p ∨q →(p →q)\n80\n1 Propositional logic\n(h) p →(q ∨r), ¬q, ¬r |−¬p\n(i) (c ∧n) →t, h ∧¬s, h ∧¬(s ∨c) →p |−(n ∧¬t) →p\n(j) the two sequents implict in (1.2) on page 20\n(k) q |−(p ∧q) ∨(¬p ∧q) using LEM\n(l) ¬(p ∧q) |−¬p ∨¬q\n(m) p ∧q →r |−(p →r) ∨(q →r)\n(n)\n*\np ∧q ⊢¬(¬p ∨¬q)\n(o) ¬(¬p ∨¬q) ⊢p ∧q\n(p) p →q ⊢¬p ∨q possibly without using LEM?\n(q)\n*\n⊢(p →q) ∨(q →r) using LEM\n(r) p →q, ¬p →r, ¬q →¬r ⊢q\n(s) p →q, r →¬t, q →r ⊢p →¬t\n(t) (p →q) →r, s →¬p, t, ¬s ∧t →q ⊢r\n(u) (s →p) ∨(t →q) ⊢(s →q) ∨(t →p)\n(v) (p ∧q) →r, r →s, q ∧¬s ⊢¬p.\n4. Explain why intuitionistic logicians also reject the proof rule PBC.\n5. Prove the following theorems of propositional logic:\n(a)\n*\n((p →q) →q) →((q →p) →p)\n(b) Given a proof for the sequent of the previous item, do you now have a quick\nargument for ((q →p) →p) →((p →q) →q)?\n(c) ((p →q) ∧(q →p)) →((p ∨q) →(p ∧q))\n(d)\n*\n(p →q) →((¬p →q) →q).\n6. Natural deduction is not the only possible formal framework for proofs in propo-\nsitional logic. As an abbreviation, we write Γ to denote any ﬁnite sequence of\nformulas φ1, φ2, . . . , φn (n ≥0). Thus, any sequent may be written as Γ ⊢ψ for\nan appropriate, possibly empty, Γ. In this exercise we propose a diﬀerent notion\nof proof, which states rules for transforming valid sequents into valid sequents.\nFor example, if we have already a proof for the sequent Γ, φ ⊢ψ, then we ob-\ntain a proof of the sequent Γ ⊢φ →ψ by augmenting this very proof with one",
    "of proof, which states rules for transforming valid sequents into valid sequents.\nFor example, if we have already a proof for the sequent Γ, φ ⊢ψ, then we ob-\ntain a proof of the sequent Γ ⊢φ →ψ by augmenting this very proof with one\napplication of the rule →i. The new approach expresses this as an inference rule\nbetween sequents:\nΓ, φ ⊢ψ\nΓ ⊢φ →ψ →i.\nThe rule ‘assumption’ is written as\nφ ⊢φ assumption\ni.e. the premise is empty. Such rules are called axioms.\n(a) Express all remaining proof rules of Figure 1.2 in such a form. (Hint: some\nof your rules may have more than one premise.)\n(b) Explain why proofs of Γ ⊢ψ in this new system have a tree-like structure\nwith Γ ⊢ψ as root.\n(c) Prove p ∨(p ∧q) ⊢p in your new proof system.\n1.7 Exercises\n81\n7. Show that\n√\n2 cannot be a rational number. Proceed by proof by contradiction:\nassume that\n√\n2 is a fraction k/l with integers k and l ̸= 0. On squaring both sides\nwe get 2 = k2/l2, or equivalently 2l2 = k2. We may assume that any common 2\nfactors of k and l have been cancelled. Can you now argue that 2l2 has a diﬀerent\nnumber of 2 factors from k2? Why would that be a contradiction and to what?\n8. There is an alternative approach to treating negation. One could simply ban the\noperator ¬ from propositional logic and think of φ →⊥as ‘being’ ¬φ. Naturally,\nsuch a logic cannot rely on the natural deduction rules for negation. Which of\nthe rules ¬i, ¬e, ¬¬e and ¬¬i can you simulate with the remaining proof rules\nby letting ¬φ be φ →⊥?\n9. Let us introduce a new connective φ ↔ψ which should abbreviate (φ →ψ) ∧\n(ψ →φ). Design introduction and elimination rules for ↔and show that they\nare derived rules if φ ↔ψ is interpreted as (φ →ψ) ∧(ψ →φ).\nExercises 1.3\nIn order to facilitate reading these exercises we assume below the usual\nconventions about binding priorities agreed upon in Convention 1.3.\n1. Given the following formulas, draw their corresponding parse tree:\n(a) p\n(b)\n*\np ∧q\n(c) p ∧¬q →¬p\n(d)\n*\np ∧(¬q →¬p)\n(e) p →(¬q ∨(q →p))\n(f)",
    "Exercises 1.3\nIn order to facilitate reading these exercises we assume below the usual\nconventions about binding priorities agreed upon in Convention 1.3.\n1. Given the following formulas, draw their corresponding parse tree:\n(a) p\n(b)\n*\np ∧q\n(c) p ∧¬q →¬p\n(d)\n*\np ∧(¬q →¬p)\n(e) p →(¬q ∨(q →p))\n(f)\n*\n¬((¬q ∧(p →r)) ∧(r →q))\n(g) ¬p ∨(p →q)\n(h) (p ∧q) →(¬r ∨(q →r))\n(i) ((s ∨(¬p)) →(¬p))\n(j) (s ∨((¬p) →(¬p)))\n(k) (((s →(r ∨l)) ∨((¬q) ∧r)) →((¬(p →s)) →r))\n(l) (p →q) ∧(¬r →(q ∨(¬p ∧r))).\n2. For each formula below, list all its subformulas:\n(a)\n*\np →(¬p ∨(¬¬q →(p ∧q)))\n(b) (s →r ∨l) ∨(¬q ∧r) →(¬(p →s) →r)\n(c) (p →q) ∧(¬r →(q ∨(¬p ∧r))).\n3. Draw the parse tree of a formula φ of propositional logic which is\n(a)\n*\na negation of an implication\n(b) a disjunction whose disjuncts are both conjunctions\n(c)\n*\na conjunction of conjunctions.\n4. For each formula below, draw its parse tree and list all subformulas:\n(a)\n*\n¬(s →(¬(p →(q ∨¬s))))\n(b) ((p →¬q) ∨(p ∧r) →s) ∨¬r.\n82\n1 Propositional logic\n∧\n→\n¬\n∧\np\n¬\n¬\nFigure 1.21. A tree that represents an ill-formed formula.\n5.\n*\nFor the parse tree in Figure 1.22 ﬁnd the logical formula it represents.\n6. For the trees below, ﬁnd their linear representations and check whether they\ncorrespond to well-formed formulas:\n(a) the tree in Figure 1.10 on page 44\n(b) the tree in Figure 1.23.\n7.\n*\nDraw a parse tree that represents an ill-formed formula such that\n(a) one can extend it by adding one or several subtrees to obtain a tree that\nrepresents a well-formed formula;\n(b) it is inherently ill-formed; i.e. any extension of it could not correspond to a\nwell-formed formula.\n8. Determine, by trying to draw parse trees, which of the following formulas are\nwell-formed:\n(a) p ∧¬(p ∨¬q) →(r →s)\n(b) p ∧¬(p ∨q ∧s) →(r →s)\n(c) p ∧¬(p ∨∧s) →(r →s).\nAmong the ill-formed formulas above which ones, and in how many ways, could\nyou ‘ﬁx’ by the insertion of brackets only?\nExercises 1.4\n1.\n*",
    "well-formed:\n(a) p ∧¬(p ∨¬q) →(r →s)\n(b) p ∧¬(p ∨q ∧s) →(r →s)\n(c) p ∧¬(p ∨∧s) →(r →s).\nAmong the ill-formed formulas above which ones, and in how many ways, could\nyou ‘ﬁx’ by the insertion of brackets only?\nExercises 1.4\n1.\n*\nConstruct the truth table for ¬p ∨q and verify that it coincides with the one for\np →q. (By ‘coincide’ we mean that the respective columns of T and F values are\nthe same.)\n2. Compute the complete truth table of the formula\n(a)\n*\n((p →q) →p) →p\n(b) represented by the parse tree in Figure 1.3 on page 34\n1.7 Exercises\n83\n¬\n→\n¬\nr\n∨\np\n∧\nq\n¬\np\nFigure 1.22. A parse tree of a negated implication.\n84\n1 Propositional logic\n¬\n→\n¬\n∧\n→\nq\np\n∨\nq\nr\np\nFigure 1.23. Another parse tree of a negated implication.\n(c)\n*\np ∨(¬(q ∧(r →q)))\n(d) (p ∧q) →(p ∨q)\n(e) ((p →¬q) →¬p) →q\n(f) (p →q) ∨(p →¬q)\n(g) ((p →q) →p) →p\n(h) ((p ∨q) →r) →((p →r) ∨(q →r))\n(i) (p →q) →(¬p →¬q).\n3. Given a valuation and a parsetree of a formula, compute the truth value of the\nformula for that valuation (as done in a bottom-up fashion in Figure 1.7 on\npage 40) with the parse tree in\n(a)\n*\nFigure 1.10 on page 44 and the valuation in which q and r evaluate to T and\np to F;\n(b) Figure 1.4 on page 36 and the valuation in which q evaluates to T and p and\nr evaluate to F;\n(c) Figure 1.23 where we let p be T, q be F and r be T; and\n(d) Figure 1.23 where we let p be F, q be T and r be F.\n4. Compute the truth value on the formula’s parse tree, or specify the corresponding\nline of a truth table where\n(a)\n*\np evaluates to F, q to T and the formula is p →(¬q ∨(q →p))\n(b)\n*\nthe formula is ¬((¬q ∧(p →r)) ∧(r →q)), p evaluates to F, q to T and r\nevaluates to T.\n1.7 Exercises\n85\n5.\n*\nA formula is valid iﬀit computes T for all its valuations; it is satisﬁable iﬀit\ncomputes T for at least one of its valuations. Is the formula of the parse tree in\nFigure 1.10 on page 44 valid? Is it satisﬁable?\n6. Let ∗be a new logical connective such that p ∗q does not hold iﬀp and q are\neither both false or both true.",
    "computes T for at least one of its valuations. Is the formula of the parse tree in\nFigure 1.10 on page 44 valid? Is it satisﬁable?\n6. Let ∗be a new logical connective such that p ∗q does not hold iﬀp and q are\neither both false or both true.\n(a) Write down the truth table for p ∗q.\n(b) Write down the truth table for (p ∗p) ∗(q ∗q).\n(c) Does the table in (b) coincide with a table in Figure 1.6 (page 38)? If so,\nwhich one?\n(d) Do you know ∗already as a logic gate in circuit design? If so, what is it\ncalled?\n7. These exercises let you practice proofs using mathematical induction. Make sure\nthat you state your base case and inductive step clearly. You should also indicate\nwhere you apply the induction hypothesis.\n(a) Prove that\n(2 · 1 −1) + (2 · 2 −1) + (2 · 3 −1) + · · · + (2 · n −1) = n2\nby mathematical induction on n ≥1.\n(b) Let k and l be natural numbers. We say that k is divisible by l if there\nexists a natural number p such that k = p · l. For example, 15 is divisible by\n3 because 15 = 5 · 3. Use mathematical induction to show that 11n −4n is\ndivisible by 7 for all natural numbers n ≥1.\n(c)\n*\nUse mathematical induction to show that\n12 + 22 + 32 + · · · + n2 = n · (n + 1) · (2n + 1)\n6\nfor all natural numbers n ≥1.\n(d)\n*\nProve that 2n ≥n + 12 for all natural numbers n ≥4. Here the base case is\nn = 4. Is the statement true for any n < 4?\n(e) Suppose a post oﬃce sells only 2c| and 3c| stamps. Show that any postage of\n2c|, or over, can be paid for using only these stamps. Hint: use mathematical\ninduction on n, where nc| is the postage. In the inductive step consider two\npossibilities: ﬁrst, nc| can be paid for using only 2c| stamps. Second, paying\nnc| requires the use of at least one 3c| stamp.\n(f) Prove that for every preﬁx of a well-formed propositional logic formula the\nnumber of left brackets is greater or equal to the number of right brackets.\n8.\n*\nThe Fibonacci numbers are most useful in modelling the growth of populations.\nWe deﬁne them by F1\ndef\n= 1, F2\ndef",
    "(f) Prove that for every preﬁx of a well-formed propositional logic formula the\nnumber of left brackets is greater or equal to the number of right brackets.\n8.\n*\nThe Fibonacci numbers are most useful in modelling the growth of populations.\nWe deﬁne them by F1\ndef\n= 1, F2\ndef\n= 1 and Fn+1\ndef\n= Fn + Fn−1 for all n ≥2. So\nF3\ndef\n= F1 + F2 = 1 + 1 = 2 etc. Show the assertion ‘F3n is even.’ by mathemat-\nical induction on n ≥1. Note that this assertion is saying that the sequence\nF3, F6, F9, . . . consists of even numbers only.\n86\n1 Propositional logic\n9. Consider the function rank, deﬁned by\nrank(p)\ndef\n= 1\nrank(¬φ)\ndef\n= 1 + rank(φ)\nrank(φ ◦ψ)\ndef\n= 1 + max(rank(φ), rank(ψ))\nwhere p is any atom, ◦∈{→, ∨, ∧} and max(n, m) is n if n ≥m and m other-\nwise. Recall the concept of the height of a formula (Deﬁnition 1.32 on page 44).\nUse mathematical induction on the height of φ to show that rank(φ) is nothing\nbut the height of φ for all formulas φ of propositional logic.\n10.\n*\nHere is an example of why we need to secure the base case for mathematical\ninduction. Consider the assertion\n‘The number n2 + 5n + 1 is even for all n ≥1.’\n(a) Prove the inductive step of that assertion.\n(b) Show that the base case fails to hold.\n(c) Conclude that the assertion is false.\n(d) Use mathematical induction to show that n2 + 5n + 1 is odd for all n ≥1.\n11. For the soundness proof of Theorem 1.35 on page 46,\n(a) explain why we could not use mathematical induction but had to resort to\ncourse-of-values induction;\n(b) give justiﬁcations for all inferences that were annotated with ‘why?’ and\n(c) complete the case analysis ranging over the ﬁnal proof rule applied; inspect\nthe summary of natural deduction rules in Figure 1.2 on page 27 to see which\ncases are still missing. Do you need to include derived rules?\n12. Show that the following sequents are not valid by ﬁnding a valuation in which\nthe truth values of the formulas to the left of ⊢are T and the truth value of",
    "the summary of natural deduction rules in Figure 1.2 on page 27 to see which\ncases are still missing. Do you need to include derived rules?\n12. Show that the following sequents are not valid by ﬁnding a valuation in which\nthe truth values of the formulas to the left of ⊢are T and the truth value of\nthe formula to the right of ⊢is F.\n(a) ¬p ∨(q →p) ⊢¬p ∧q\n(b) ¬r →(p ∨q), r ∧¬q ⊢r →q\n(c)\n*\np →(q →r) ⊢p →(r →q)\n(d) ¬p, p ∨q ⊢¬q\n(e) p →(¬q ∨r), ¬r ⊢¬q →¬p.\n13. For each of the following invalid sequents, give examples of natural language\ndeclarative sentences for the atoms p, q and r such that the premises are true,\nbut the conclusion false.\n(a)\n*\np ∨q ⊢p ∧q\n(b)\n*\n¬p →¬q ⊢¬q →¬p\n(c) p →q ⊢p ∨q\n(d) p →(q ∨r) ⊢(p →q) ∧(p →r).\n14. Find a formula of propositional logic φ which contains only the atoms p, q\nand r and which is true only when p and q are false, or when ¬q ∧(p ∨r) is\ntrue.\n1.7 Exercises\n87\n15. Use mathematical induction on n to prove the theorem ((φ1 ∧(φ2 ∧(· · · ∧\nφn) . . . ) →ψ) →(φ1 →(φ2 →(. . . (φn →ψ) . . . )))).\n16. Prove the validity of the following sequents needed to secure the completeness\nresult for propositional logic:\n(a) φ1 ∧¬φ2 ⊢¬(φ1 →φ2)\n(b) ¬φ1 ∧¬φ2 ⊢φ1 →φ2\n(c) ¬φ1 ∧φ2 ⊢φ1 →φ2\n(d) φ1 ∧φ2 ⊢φ1 →φ2\n(e) ¬φ1 ∧φ2 ⊢¬(φ1 ∧φ2)\n(f) ¬φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2)\n(g) φ1 ∧¬φ2 ⊢¬(φ1 ∧φ2)\n(h) ¬φ1 ∧¬φ2 ⊢¬(φ1 ∨φ2)\n(i) φ1 ∧φ2 ⊢φ1 ∨φ2\n(j) ¬φ1 ∧φ2 ⊢φ1 ∨φ2\n(k) φ1 ∧¬φ2 ⊢φ1 ∨φ2.\n17. Does ⊨φ hold for the φ below? Please justify your answer.\n(a) (p →q) ∨(q →r)\n(b)\n*\n((q →(p ∨(q →p))) ∨¬(p →q)) →p.\nExercises 1.5\n1. Show that a formula φ is valid iﬀ⊤≡φ, where ⊤is an abbreviation for an\ninstance p ∨¬p of LEM.\n2. Which of these formulas are semantically equivalent to p →(q ∨r)?\n(a) q ∨(¬p ∨r)\n(b)\n*\nq ∧¬r →p\n(c) p ∧¬r →q\n(d)\n*\n¬q ∧¬r →¬p.\n3. An adequate set of connectives for propositional logic is a set such that for every\nformula of propositional logic there is an equivalent formula with only connectives\nfrom that set. For example, the set {¬, ∨} is adequate for propositional logic,",
    "(b)\n*\nq ∧¬r →p\n(c) p ∧¬r →q\n(d)\n*\n¬q ∧¬r →¬p.\n3. An adequate set of connectives for propositional logic is a set such that for every\nformula of propositional logic there is an equivalent formula with only connectives\nfrom that set. For example, the set {¬, ∨} is adequate for propositional logic,\nbecause any occurrence of ∧and →can be removed by using the equivalences\nφ →ψ ≡¬φ ∨ψ and φ ∧ψ ≡¬(¬φ ∨¬ψ).\n(a) Show that {¬, ∧}, {¬, →} and {→, ⊥} are adequate sets of connectives for\npropositional logic. (In the latter case, we are treating ⊥as a nullary con-\nnective.)\n(b) Show that, if C ⊆{¬, ∧, ∨, →, ⊥} is adequate for propositional logic, then\n¬ ∈C or ⊥∈C. (Hint: suppose C contains neither ¬ nor ⊥and consider\nthe truth value of a formula φ, formed by using only the connectives in C,\nfor a valuation in which every atom is assigned T.)\n(c) Is {↔, ¬} adequate? Prove your answer.\n4. Use soundness or completeness to show that a sequent φ1, φ2, . . . , φn ⊢ψ has a\nproof iﬀφ1 →φ2 →. . . φn →ψ is a tautology.\n88\n1 Propositional logic\n5. Show that the relation ≡is\n(a) reﬂexive: φ ≡φ holds for all φ\n(b) symmetric: φ ≡ψ implies ψ ≡φ and\n(c) transitive: φ ≡ψ and ψ ≡η imply φ ≡η.\n6. Show that, with respect to ≡,\n(a) ∧and ∨are idempotent:\ni. φ ∧φ ≡φ\nii. φ ∨φ ≡φ\n(b) ∧and ∨are commutative:\ni. φ ∧ψ ≡ψ ∧φ\nii. φ ∨ψ ≡ψ ∨φ\n(c) ∧and ∨are associative:\ni. φ ∧(ψ ∧η) ≡(φ ∧ψ) ∧η\nii. φ ∨(ψ ∨η) ≡(φ ∨ψ) ∨η\n(d) ∧and ∨are absorptive:\ni.\n*\nφ ∧(φ ∨η) ≡φ\nii. φ ∨(φ ∧η) ≡φ\n(e) ∧and ∨are distributive:\ni. φ ∧(ψ ∨η) ≡(φ ∧ψ) ∨(φ ∧η)\nii.\n*\nφ ∨(ψ ∧η) ≡(φ ∨ψ) ∧(φ ∨η)\n(f) ≡allows for double negation: φ ≡¬¬φ and\n(g) ∧and ∨satisﬁes the de Morgan rules:\ni. ¬(φ ∧ψ) ≡¬φ ∨¬ψ\nii.\n*\n¬(φ ∨ψ) ≡¬φ ∧¬ψ.\n7. Construct a formula in CNF based on each of the following truth tables:\n(a)\n*\np\nq\nφ1\nT\nT\nF\nF\nT\nF\nT\nF\nF\nF\nF\nT\n(b)\n*\np\nq\nr\nφ2\nT\nT\nT\nT\nT\nT\nF\nF\nT\nF\nT\nF\nF\nT\nT\nT\nT\nF\nF\nF\nF\nT\nF\nF\nF\nF\nT\nT\nF\nF\nF\nF\n1.7 Exercises\n89\n(c)\nr\ns\nq\nφ3\nT\nT\nT\nF\nT\nT\nF\nT\nT\nF\nT\nF\nF\nT\nT\nF\nT\nF\nF\nT\nF\nT\nF\nF\nF\nF\nT\nF\nF\nF\nF\nT\n8.\n*",
    "7. Construct a formula in CNF based on each of the following truth tables:\n(a)\n*\np\nq\nφ1\nT\nT\nF\nF\nT\nF\nT\nF\nF\nF\nF\nT\n(b)\n*\np\nq\nr\nφ2\nT\nT\nT\nT\nT\nT\nF\nF\nT\nF\nT\nF\nF\nT\nT\nT\nT\nF\nF\nF\nF\nT\nF\nF\nF\nF\nT\nT\nF\nF\nF\nF\n1.7 Exercises\n89\n(c)\nr\ns\nq\nφ3\nT\nT\nT\nF\nT\nT\nF\nT\nT\nF\nT\nF\nF\nT\nT\nF\nT\nF\nF\nT\nF\nT\nF\nF\nF\nF\nT\nF\nF\nF\nF\nT\n8.\n*\nWrite a recursive function IMPL FREE which requires a (parse tree of a) proposi-\ntional formula as input and produces an equivalent implication-free formula as\noutput. How many clauses does your case statement need? Recall Deﬁnition 1.27\non page 32.\n9.\n*\nCompute CNF (NNF (IMPL FREE ¬(p →(¬(q ∧(¬p →q)))))).\n10. Use structural induction on the grammar of formulas in CNF to show that the\n‘otherwise’ case in calls to DISTR applies iﬀboth η1 and η2 are of type D in (1.6)\non page 55.\n11. Use mathematical induction on the height of φ to show that the call\nCNF (NNF (IMPL FREE φ)) returns, up to associativity, φ if the latter is already\nin CNF.\n12. Why do the functions CNF and DISTR preserve NNF and why is this important?\n13. For the call CNF (NNF (IMPL FREE (φ))) on a formula φ of propositional logic,\nexplain why\n(a) its output is always a formula in CNF\n(b) its output is semantically equivalent to φ\n(c) that call always terminates.\n14. Show that all the algorithms presented in Section 1.5.2 terminate on any input\nmeeting their precondition. Can you formalise some of your arguments? Note\nthat algorithms might not call themselves again on formulas with smaller height.\nE.g. the call of CNF (φ1 ∨φ2) results in a call DISTR (CNF(φ1), CNF(φ2)), where\nCNF(φi) may have greater height than φi. Why is this not a problem?\n15. Apply algorithm HORN from page 66 to each of these Horn formulas:\n(a)\n*\n(p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(u →\ns) ∧(⊤→u)\n(b) (p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(r ∧u →\nw) ∧(u →s) ∧(⊤→u)\n(c) (p ∧q ∧s →p) ∧(q ∧r →p) ∧(p ∧s →s)\n(d) (p ∧q ∧s →⊥) ∧(q ∧r →p) ∧(⊤→s)\n(e) (p5 →p11) ∧(p2 ∧p3 ∧p5 →p13) ∧(⊤→p5) ∧(p5 ∧p11 →⊥)",
    "(a)\n*\n(p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(u →\ns) ∧(⊤→u)\n(b) (p ∧q ∧w →⊥) ∧(t →⊥) ∧(r →p) ∧(⊤→r) ∧(⊤→q) ∧(r ∧u →\nw) ∧(u →s) ∧(⊤→u)\n(c) (p ∧q ∧s →p) ∧(q ∧r →p) ∧(p ∧s →s)\n(d) (p ∧q ∧s →⊥) ∧(q ∧r →p) ∧(⊤→s)\n(e) (p5 →p11) ∧(p2 ∧p3 ∧p5 →p13) ∧(⊤→p5) ∧(p5 ∧p11 →⊥)\n(f) (⊤→q) ∧(⊤→s) ∧(w →⊥) ∧(p ∧q ∧s →⊥) ∧(v →s) ∧(⊤→\nr) ∧(r →p)\n90\n1 Propositional logic\n(g)\n*\n(⊤→q) ∧(⊤→s) ∧(w →⊥) ∧(p ∧q ∧s →v) ∧(v →s) ∧(⊤→\nr) ∧(r →p).\n16. Explain why the algorithm HORN fails to work correctly if we change the concept\nof Horn formulas by extending the clause for P on page 65 to P ::= ⊥| ⊤|\np | ¬p?\n17. What can you say about the CNF of Horn formulas. More precisely, can you\nspecify syntactic criteria for a CNF that ensure that there is an equivalent Horn\nformula? Can you describe informally programs which would translate from one\nform of representation into another?\nExercises 1.6\n1. Use mathematical induction to show that, for all φ of (1.3) on page 33,\n(a) T(φ) can be generated by (1.10) on page 69,\n(b) T(φ) has the same set of valuations as φ, and\n(c) the set of valuations in which φ is true equals the set of valuations in which\nT(φ) is true.\n2.\n*\nShow that all rules of Figure 1.14 (page 71) are sound: if all current marks\nsatisfy the invariant (1.9) from page 68, then this invariant still holds if the\nderived constraint of that rule becomes an additional mark.\n3. In Figure 1.16 on page 73 we detected a contradiction which secured the validity\nof the sequent p ∧q →r ⊢p →q →r. Use the same method with the linear SAT\nsolver to show that the sequent ⊢(p →q) ∨(r →p) is valid. (This is interest-\ning since we proved this validity in natural deduction with a judicious choice\nof the proof rule LEM; and the linear SAT solver does not employ any case\nanalysis.)\n4.\n*\nConsider the sequent p ∨q, p →r ⊢r. Determine a DAG which is not satisﬁable\niﬀthis sequent is valid. Tag the DAG’s root node with ‘1: T,’ apply the forcing",
    "of the proof rule LEM; and the linear SAT solver does not employ any case\nanalysis.)\n4.\n*\nConsider the sequent p ∨q, p →r ⊢r. Determine a DAG which is not satisﬁable\niﬀthis sequent is valid. Tag the DAG’s root node with ‘1: T,’ apply the forcing\nlaws to it, and extract a witness to the DAG’s satisﬁability. Explain in what\nsense this witness serves as an explanation for the fact that p ∨q, p →r ⊢r is\nnot valid.\n5. Explain in what sense the SAT solving technique, as presented in this chapter,\ncan be used to check whether formulas are tautologies.\n6. For φ from (1.10), can one reverse engineer φ from the DAG of T(φ)?\n7. Consider a modiﬁcation of our method which initially tags a DAG’s root node\nwith ‘1: F.’ In that case,\n(a) are the forcing laws still sound? If so, state the invariant.\n(b) what can we say about the formula(s) a DAG represents if\ni. we detect contradictory constraints?\nii. we compute consistent forced constraints for each node?\n8. Given an arbitrary Horn formula φ, compare our linear SAT solver – applied\nto T(φ) –\nto the marking algorithm – applied to φ. Discuss similarities and\ndiﬀerences of these approaches.\n1.8 Bibliographic notes\n91\n9. Consider Figure 1.20 on page 77. Verify that\n(a) its test produces contradictory constraints\n(b) its cubic analysis does not decide satisﬁability, regardless of whether the\ntwo optimizations we described are present.\n10. Verify that the DAG of Figure 1.17 (page 74) is indeed the one obtained for\nT(φ), where φ is the formula in (1.11) on page 73.\n11.\n*\nAn implementor may be concerned with the possibility that the answers to the\ncubic SAT solver may depend on a particular order in which we test unmarked\nnodes or use the rules in Figure 1.14. Give a semi-formal argument for why the\nanalysis results don’t depend on such an order.\n12. Find a formula φ such that our cubic SAT solver cannot decide the satisﬁability\nof T(φ).\n13. Advanced Project: Write a complete implementation of the cubic SAT solver",
    "nodes or use the rules in Figure 1.14. Give a semi-formal argument for why the\nanalysis results don’t depend on such an order.\n12. Find a formula φ such that our cubic SAT solver cannot decide the satisﬁability\nof T(φ).\n13. Advanced Project: Write a complete implementation of the cubic SAT solver\ndescribed in Section 1.6.2. It should read formulas from the keyboard or a ﬁle;\nshould assume right-associativity of ∨, ∧, and →(respectively); compute the\nDAG of T(φ); perform the cubic SAT solver next. Think also about including\nappropriate user output, diagnostics, and optimizations.\n14. Show that our cubic SAT solver speciﬁed in this section\n(a) terminates on all syntactically correct input;\n(b) satisﬁes the invariant (1.9) after the ﬁrst permanent marking;\n(c) preserves (1.9) for all permanent markings it makes;\n(d) computes only correct satisﬁability witnesses;\n(e) computes only correct ‘not satisﬁable’ replies; and\n(f) remains to be correct under the two modiﬁcations described on page 77 for\nhandling results of a node’s two test runs.\n1.8 Bibliographic notes\nLogic has a long history stretching back at least 2000 years, but the truth-\nvalue semantics of propositional logic presented in this and every logic text-\nbook today was invented only about 160 years ago, by G. Boole [Boo54].\nBoole used the symbols + and · for disjunction and conjunction.\nNatural deduction was invented by G. Gentzen [Gen69], and further de-\nveloped by D. Prawitz [Pra65]. Other proof systems existed before then, no-\ntably axiomatic systems which present a small number of axioms together\nwith the rule modus ponens (which we call →e). Proof systems often present\nas small a number of axioms as possible; and only for an adequate set of con-\nnectives such as →and ¬. This makes them hard to use in practice. Gentzen\nimproved the situation by inventing the idea of working with assumptions\n(used by the rules →i, ¬i and ∨e) and by treating all the connectives sepa-\nrately.\n92\n1 Propositional logic",
    "nectives such as →and ¬. This makes them hard to use in practice. Gentzen\nimproved the situation by inventing the idea of working with assumptions\n(used by the rules →i, ¬i and ∨e) and by treating all the connectives sepa-\nrately.\n92\n1 Propositional logic\nOur linear and cubic SAT solvers are variants of St˚almarck’s method\n[SS90], a SAT solver which is patented in Sweden and in the United States\nof America.\nFurther historical remarks, and also pointers to other contemporary books\nabout propositional and predicate logic, can be found in the bibliographic\nremarks at the end of Chapter 2. For an introduction to algorithms and data\nstructures see e.g. [Wei98].\n2\nPredicate logic\n2.1 The need for a richer language\nIn the ﬁrst chapter, we developed propositional logic by examining it from\nthree diﬀerent angles: its proof theory (the natural deduction calculus), its\nsyntax (the tree-like nature of formulas) and its semantics (what these for-\nmulas actually mean). From the outset, this enterprise was guided by the\nstudy of declarative sentences, statements about the world which can, for\nevery valuation or model, be given a truth value.\nWe begin this second chapter by pointing out the limitations of propo-\nsitional logic with respect to encoding declarative sentences. Propositional\nlogic dealt quite satisfactorily with sentence components like not, and, or\nand if . . . then, but the logical aspects of natural and artiﬁcial languages\nare much richer than that. What can we do with modiﬁers like there exists\n. . . , all . . . , among . . . and only . . . ? Here, propositional logic shows clear\nlimitations and the desire to express more subtle declarative sentences led\nto the design of predicate logic, which is also called ﬁrst-order logic.\nLet us consider the declarative sentence\nEvery student is younger than some instructor.\n(2.1)\nIn propositional logic, we could identify this assertion with a propositional",
    "to the design of predicate logic, which is also called ﬁrst-order logic.\nLet us consider the declarative sentence\nEvery student is younger than some instructor.\n(2.1)\nIn propositional logic, we could identify this assertion with a propositional\natom p. However, that fails to reﬂect the ﬁner logical structure of this sen-\ntence. What is this statement about? Well, it is about being a student, being\nan instructor and being younger than somebody else. These are all proper-\nties of some sort, so we would like to have a mechanism for expressing them\ntogether with their logical relationships and dependences.\nWe now use predicates for that purpose. For example, we could write\nS(andy) to denote that Andy is a student and I(paul) to say that Paul is an\ninstructor. Likewise, Y (andy, paul) could mean that Andy is younger than\n93\n94\n2 Predicate logic\nPaul. The symbols S, I and Y are called predicates. Of course, we have to\nbe clear about their meaning. The predicate Y could have meant that the\nsecond person is younger than the ﬁrst one, so we need to specify exactly\nwhat these symbols refer to.\nHaving such predicates at our disposal, we still need to formalise those\nparts of the sentence above which speak of every and some. Obviously, this\nsentence refers to the individuals that make up some academic community\n(left implicit by the sentence), like Kansas State University or the University\nof Birmingham, and it says that for each student among them there is an\ninstructor among them such that the student is younger than the instructor.\nThese predicates are not yet enough to allow us to express the sentence\nin (2.1). We don’t really want to write down all instances of S(·) where · is\nreplaced by every student’s name in turn. Similarly, when trying to codify\na sentence having to do with the execution of a program, it would be rather\nlaborious to have to write down every state of the computer. Therefore,",
    "replaced by every student’s name in turn. Similarly, when trying to codify\na sentence having to do with the execution of a program, it would be rather\nlaborious to have to write down every state of the computer. Therefore,\nwe employ the concept of a variable. Variables are written u, v, w, x, y, z, . . .\nor x1, y3, u5, . . . and can be thought of as place holders for concrete values\n(like a student, or a program state). Using variables, we can now specify the\nmeanings of S, I and Y more formally:\nS(x) :\nx is a student\nI(x) :\nx is an instructor\nY (x, y) :\nx is younger than y.\nNote that the names of the variables are not important, provided that we\nuse them consistently. We can state the intended meaning of I by writing\nI(y) :\ny is an instructor\nor, equivalently, by writing\nI(z) :\nz is an instructor.\nVariables are mere place holders for objects. The availability of variables is\nstill not suﬃcient for capturing the essence of the example sentence above.\nWe need to convey the meaning of ‘Every student x is younger than some\ninstructor y.’ This is where we need to introduce quantiﬁers ∀(read: ‘for\nall’) and ∃(read: ‘there exists’ or ‘for some’) which always come attached\nto a variable, as in ∀x (‘for all x’) or in ∃z (‘there exists z’, or ‘there is some\nz’). Now we can write the example sentence in an entirely symbolic way as\n∀x (S(x) →(∃y (I(y) ∧Y (x, y)))).\n2.1 The need for a richer language\n95\nActually, this encoding is rather a paraphrase of the original sentence. In\nour example, the re-translation results in\nFor every x, if x is a student, then there is some y which is an\ninstructor such that x is younger than y.\nDiﬀerent predicates can have a diﬀerent number of arguments. The predi-\ncates S and I have just one (they are called unary predicates), but predicate\nY requires two arguments (it is called a binary predicate). Predicates with\nany ﬁnite number of arguments are possible in predicate logic.\nAnother example is the sentence\nNot all birds can ﬂy.",
    "cates S and I have just one (they are called unary predicates), but predicate\nY requires two arguments (it is called a binary predicate). Predicates with\nany ﬁnite number of arguments are possible in predicate logic.\nAnother example is the sentence\nNot all birds can ﬂy.\nFor that we choose the predicates B and F which have one argument ex-\npressing\nB(x) :\nx is a bird\nF(x) :\nx can ﬂy.\nThe sentence ‘Not all birds can ﬂy’ can now be coded as\n¬(∀x (B(x) →F(x)))\nsaying: ‘It is not the case that all things which are birds can ﬂy.’ Alterna-\ntively, we could code this as\n∃x (B(x) ∧¬F(x))\nmeaning: ‘There is some x which is a bird and cannot ﬂy.’ Note that the\nﬁrst version is closer to the linguistic structure of the sentence above. These\ntwo formulas should evaluate to T in the world we currently live in since, for\nexample, penguins are birds which cannot ﬂy. Shortly, we address how such\nformulas can be given their meaning in general. We will also explain why\nformulas like the two above are indeed equivalent semantically.\nCoding up complex facts expressed in English sentences as logical formulas\nin predicate logic is important – e.g. in software design with UML or in\nformal speciﬁcation of safety-critical systems – and much more care must be\ntaken than in the case of propositional logic. However, once this translation\nhas been accomplished our main objective is to reason symbolically (⊢) or\nsemantically (⊨) about the information expressed in those formulas.\nIn Section 2.3, we extend our natural deduction calculus of propositional\nlogic so that it covers logical formulas of predicate logic as well. In this way\nwe are able to prove the validity of sequents φ1, φ2, . . . , φn ⊢ψ in a similar\nway to that in the ﬁrst chapter.\n96\n2 Predicate logic\nIn Section 2.4, we generalize the valuations of Chapter 1 to a proper\nnotion of models, real or artiﬁcial worlds in which formulas of predicate\nlogic can be true or false, which allows us to deﬁne semantic entailment",
    "way to that in the ﬁrst chapter.\n96\n2 Predicate logic\nIn Section 2.4, we generalize the valuations of Chapter 1 to a proper\nnotion of models, real or artiﬁcial worlds in which formulas of predicate\nlogic can be true or false, which allows us to deﬁne semantic entailment\nφ1, φ2, . . . , φn ⊨ψ.\nThe latter expresses that, given any such model in which all φ1, φ2, . . . , φn\nhold, it is the case that ψ holds in that model as well. In that case, one\nalso says that ψ is semantically entailed by φ1, φ2, . . . , φn. Although this\ndeﬁnition of semantic entailment closely matches the one for propositional\nlogic in Deﬁnition 1.34, the process of evaluating a predicate formula diﬀers\nfrom the computation of truth values for propositional logic in the treatment\nof predicates (and functions). We discuss it in detail in Section 2.4.\nIt is outside the scope of this book to show that the natural deduction\ncalculus for predicate logic is sound and complete with respect to semantic\nentailment; but it is indeed the case that\nφ1, φ2, . . . , φn ⊢ψ\niﬀ\nφ1, φ2, . . . , φn ⊨ψ\nfor formulas of the predicate calculus. The ﬁrst proof of this was done by\nthe mathematician K. G¨odel.\nWhat kind of reasoning must predicate logic be able to support? To get\na feel for that, let us consider the following argument:\nNo books are gaseous. Dictionaries are books. Therefore, no dictio-\nnary is gaseous.\nThe predicates we choose are\nB(x) :\nx is a book\nG(x) :\nx is gaseous\nD(x) :\nx is a dictionary.\nEvidently, we need to build a proof theory and semantics that allow us to\nderive the validity and semantic entailment, respectively, of\n¬∃x (B(x) ∧G(x)), ∀x (D(x) →B(x)) ⊢¬∃x (D(x) ∧G(x))\n¬∃x (B(x) ∧G(x)), ∀x (D(x) →B(x)) ⊨¬∃x (D(x) ∧G(x)).\nVerify that these sequents express the argument above in a symbolic form.\nPredicate logic extends propositional logic not only with quantiﬁers but\nwith one more concept, that of function symbols. Consider the declarative\nsentence\nEvery child is younger than its mother.",
    "Verify that these sequents express the argument above in a symbolic form.\nPredicate logic extends propositional logic not only with quantiﬁers but\nwith one more concept, that of function symbols. Consider the declarative\nsentence\nEvery child is younger than its mother.\n2.1 The need for a richer language\n97\nUsing predicates, we could express this sentence as\n∀x ∀y (C(x) ∧M(y, x) →Y (x, y))\nwhere C(x) means that x is a child, M(x, y) means that x is y’s mother\nand Y (x, y) means that x is younger than y. (Note that we actually used\nM(y, x) (y is x’s mother), not M(x, y).) As we have coded it, the sentence\nsays that, for all children x and any mother y of theirs, x is younger than y.\nIt is not very elegant to say ‘any of x’s mothers’, since we know that every\nindividual has one and only one mother1. The inelegance of coding ‘mother’\nas a predicate is even more apparent if we consider the sentence\nAndy and Paul have the same maternal grandmother.\nwhich, using ‘variables’ a and p for Andy and Paul and a binary predicate\nM for mother as before, becomes\n∀x ∀y ∀u ∀v (M(x, y) ∧M(y, a) ∧M(u, v) ∧M(v, p) →x = u).\nThis formula says that, if y and v are Andy’s and Paul’s mothers, respec-\ntively, and x and u are their mothers (i.e. Andy’s and Paul’s maternal grand-\nmothers, respectively), then x and u are the same person. Notice that we\nused a special predicate in predicate logic, equality; it is a binary predicate,\ni.e. it takes two arguments, and is written =. Unlike other predicates, it is\nusually written in between its arguments rather than before them; that is,\nwe write x = y instead of = (x, y) to say that x and y are equal.\nThe function symbols of predicate logic give us a way of avoiding this\nugly encoding, for they allow us to represent y’s mother in a more direct\nway. Instead of writing M(x, y) to mean that x is y’s mother, we simply\nwrite m(y) to mean y’s mother. The symbol m is a function symbol: it takes",
    "The function symbols of predicate logic give us a way of avoiding this\nugly encoding, for they allow us to represent y’s mother in a more direct\nway. Instead of writing M(x, y) to mean that x is y’s mother, we simply\nwrite m(y) to mean y’s mother. The symbol m is a function symbol: it takes\none argument and returns the mother of that argument. Using m, the two\nsentences above have simpler encodings than they had using M:\n∀x (C(x) →Y (x, m(x)))\nnow expresses that every child is younger than its mother. Note that we\nneed only one variable rather than two. Representing that Andy and Paul\nhave the same maternal grandmother is even simpler; it is written\nm(m(a)) = m(m(p))\nquite directly saying that Andy’s maternal grandmother is the same person\nas Paul’s maternal grandmother.\n1 We assume that we are talking about genetic mothers, not adopted mothers, step mothers etc.\n98\n2 Predicate logic\nOne can always do without function symbols, by using a predicate symbol\ninstead. However, it is usually neater to use function symbols whenever pos-\nsible, because we get more compact encodings. However, function symbols\ncan be used only in situations in which we want to denote a single object.\nAbove, we rely on the fact that every individual has a uniquely deﬁned\nmother, so that we can talk about x’s mother without risking any ambigu-\nity (for example, if x had no mother, or two mothers). For this reason, we\ncannot have a function symbol b(·) for ‘brother’. It might not make sense to\ntalk about x’s brother, for x might not have any brothers, or he might have\nseveral. ‘Brother’ must be coded as a binary predicate.\nTo exemplify this point further, if Mary has several brothers, then the\nclaim that ‘Ann likes Mary’s brother’ is ambiguous. It might be that Ann\nlikes one of Mary’s brothers, which we would write as\n∃x (B(x, m) ∧L(a, x))\nwhere B and L mean ‘is brother of’ and ‘likes,’ and a and m mean Ann and\nMary. This sentence says that there exists an x which is a brother of Mary",
    "claim that ‘Ann likes Mary’s brother’ is ambiguous. It might be that Ann\nlikes one of Mary’s brothers, which we would write as\n∃x (B(x, m) ∧L(a, x))\nwhere B and L mean ‘is brother of’ and ‘likes,’ and a and m mean Ann and\nMary. This sentence says that there exists an x which is a brother of Mary\nand is liked by Ann. Alternatively, if Ann likes all of Mary’s brothers, we\nwrite it as\n∀x (B(x, m) →L(a, x))\nsaying that any x which is a brother of Mary is liked by Ann. Predicates\nshould be used if a ‘function’ such as ‘your youngest brother’ does not always\nhave a value.\nDiﬀerent function symbols may take diﬀerent numbers of arguments.\nFunctions may take zero arguments and are then called constants: a and\np above are constants for Andy and Paul, respectively. In a domain involv-\ning students and the grades they get in diﬀerent courses, one might have\nthe binary function symbol g(·, ·) taking two arguments: g(x, y) refers to the\ngrade obtained by student x in course y.\n2.2 Predicate logic as a formal language\nThe discussion of the preceding section was intended to give an impression\nof how we code up sentences as formulas of predicate logic. In this section,\nwe will be more precise about it, giving syntactic rules for the formation\nof predicate logic formulas. Because of the power of predicate logic, the\nlanguage is much more complex than that of propositional logic.\nThe ﬁrst thing to note is that there are two sorts of things involved in\na predicate logic formula. The ﬁrst sort denotes the objects that we are\n2.2 Predicate logic as a formal language\n99\ntalking about: individuals such as a and p (referring to Andy and Paul) are\nexamples, as are variables such as x and v. Function symbols also allow us\nto refer to objects: thus, m(a) and g(x, y) are also objects. Expressions in\npredicate logic which denote objects are called terms.\nThe other sort of things in predicate logic denotes truth values; expres-",
    "examples, as are variables such as x and v. Function symbols also allow us\nto refer to objects: thus, m(a) and g(x, y) are also objects. Expressions in\npredicate logic which denote objects are called terms.\nThe other sort of things in predicate logic denotes truth values; expres-\nsions of this kind are formulas: Y (x, m(x)) is a formula, though x and m(x)\nare terms.\nA predicate vocabulary consists of three sets: a set of predicate symbols\nP, a set of function symbols F and a set of constant symbols C. Each pred-\nicate symbol and each function symbol comes with an arity, the number of\narguments it expects. In fact, constants can be thought of as functions which\ndon’t take any arguments (and we even drop the argument brackets) – there-\nfore, constants live in the set F together with the ‘true’ functions which do\ntake arguments. From now on, we will drop the set C, since it is convenient to\ndo so, and stipulate that constants are 0-arity, so-called nullary, functions.\n2.2.1 Terms\nThe terms of our language are made up of variables, constant symbols\nand functions applied to those. Functions may be nested, as in m(m(x))\nor g(m(a), c): the grade obtained by Andy’s mother in the course c.\nDeﬁnition 2.1 Terms are deﬁned as follows.\nr Any variable is a term.\nr If c ∈F is a nullary function, then c is a term.\nr If t1, t2, . . . , tn are terms and f ∈F has arity n > 0, then f(t1, t2, . . . , tn) is a\nterm.\nr Nothing else is a term.\nIn Backus Naur form we may write\nt ::= x | c | f(t, . . . , t)\nwhere x ranges over a set of variables var, c over nullary function symbols\nin F, and f over those elements of F with arity n > 0.\nIt is important to note that\nr the ﬁrst building blocks of terms are constants (nullary functions) and variables;\nr more complex terms are built from function symbols using as many previously\nbuilt terms as required by such function symbols; and\nr the notion of terms is dependent on the set F. If you change it, you change the\nset of terms.\n100",
    "r more complex terms are built from function symbols using as many previously\nbuilt terms as required by such function symbols; and\nr the notion of terms is dependent on the set F. If you change it, you change the\nset of terms.\n100\n2 Predicate logic\nExample 2.2 Suppose n, f and g are function symbols, respectively\nnullary, unary and binary. Then g(f(n), n) and f(g(n, f(n))) are terms, but\ng(n) and f(f(n), n) are not (they violate the arities). Suppose 0, 1, . . . are\nnullary, s is unary, and +, −, and ∗are binary. Then ∗(−(2, +(s(x), y)), x)\nis a term, whose parse tree is illustrated in Figure 2.14 (page 159). Usually,\nthe binary symbols are written inﬁx rather than preﬁx; thus, the term is\nusually written (2 −(s(x) + y)) ∗x.\n2.2.2 Formulas\nThe choice of sets P and F for predicate and function symbols, respectively,\nis driven by what we intend to describe. For example, if we work on a\ndatabase representing relations between our kin we might want to consider\nP = {M, F, S, D}, referring to being male, being female, being a son of . . .\nand being a daughter of . . . . Naturally, F and M are unary predicates (they\ntake one argument) whereas D and S are binary (taking two). Similarly, we\nmay deﬁne F = {mother-of, father-of}.\nWe already know what the terms over F are. Given that knowledge, we\ncan now proceed to deﬁne the formulas of predicate logic.\nDeﬁnition 2.3 We deﬁne the set of formulas over (F, P) inductively, using\nthe already deﬁned set of terms over F:\nr If P ∈P is a predicate symbol of arity n ≥1, and if t1, t2, . . . , tn are terms over\nF, then P(t1, t2, . . . , tn) is a formula.\nr If φ is a formula, then so is (¬φ).\nr If φ and ψ are formulas, then so are (φ ∧ψ), (φ ∨ψ) and (φ →ψ).\nr If φ is a formula and x is a variable, then (∀x φ) and (∃x φ) are formulas.\nr Nothing else is a formula.\nNote how the arguments given to predicates are always terms. This can also\nbe seen in the Backus Naur form (BNF) for predicate logic:",
    "r If φ is a formula and x is a variable, then (∀x φ) and (∃x φ) are formulas.\nr Nothing else is a formula.\nNote how the arguments given to predicates are always terms. This can also\nbe seen in the Backus Naur form (BNF) for predicate logic:\nφ ::= P(t1, t2, . . . , tn) | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (∀x φ) | (∃x φ)\n(2.2)\nwhere P ∈P is a predicate symbol of arity n ≥1, ti are terms over F and x\nis a variable. Recall that each occurrence of φ on the right-hand side of the\n::= stands for any formula already constructed by these rules. (What role\ncould predicate symbols of arity 0 play?)\n2.2 Predicate logic as a formal language\n101\n∀x\n∧\n→\nS\nP\nQ\nx\ny\nx\nx\nFigure 2.1. A parse tree of a predicate logic formula.\nConvention 2.4 For convenience, we retain the usual binding priorities\nagreed upon in Convention 1.3 and add that ∀y and ∃y bind like ¬. Thus,\nthe order is:\nr ¬, ∀y and ∃y bind most tightly;\nr then ∨and ∧;\nr then →, which is right-associative.\nWe also often omit brackets around quantiﬁers, provided that doing so in-\ntroduces no ambiguities.\nPredicate logic formulas can be represented by parse trees. For example,\nthe parse tree in Figure 2.1 represents the formula ∀x ((P(x) →Q(x)) ∧\nS(x, y)).\nExample 2.5 Consider translating the sentence\nEvery son of my father is my brother.\ninto predicate logic. As before, the design choice is whether we represent\n‘father’ as a predicate or as a function symbol.\n1.\nAs a predicate. We choose a constant m for ‘me’ or ‘I,’ so m is a term, and we\nchoose further {S, F, B} as the set of predicates with meanings\n102\n2 Predicate logic\nS(x, y) :\nx is a son of y\nF(x, y) :\nx is the father of y\nB(x, y) :\nx is a brother of y.\nThen the symbolic encoding of the sentence above is\n∀x ∀y (F(x, m) ∧S(y, x) →B(y, m))\n(2.3)\nsaying: ‘For all x and all y, if x is a father of m and if y is a son of x, then y is\na brother of m.’\n2.\nAs a function. We keep m, S and B as above and write f for the function which,",
    "x is a brother of y.\nThen the symbolic encoding of the sentence above is\n∀x ∀y (F(x, m) ∧S(y, x) →B(y, m))\n(2.3)\nsaying: ‘For all x and all y, if x is a father of m and if y is a son of x, then y is\na brother of m.’\n2.\nAs a function. We keep m, S and B as above and write f for the function which,\ngiven an argument, returns the corresponding father. Note that this works only\nbecause fathers are unique and always deﬁned, so f really is a function as\nopposed to a mere relation.\nThe symbolic encoding of the sentence above is now\n∀x (S(x, f(m)) →B(x, m))\n(2.4)\nmeaning: ‘For all x, if x is a son of the father of m, then x is a brother of m;’\nit is less complex because it involves only one quantiﬁer.\nFormal speciﬁcations require domain-speciﬁc knowledge. Domain-experts\noften don’t make some of this knowledge explicit, so a speciﬁer may miss\nimportant constraints for a model or implementation. For example, the spec-\niﬁcation in (2.3) and (2.4) may seem right, but what about the case when\nthe values of x and m are equal? If the domain of kinship is not common\nknowledge, then a speciﬁer may not realize that a man cannot be his own\nbrother. Thus, (2.3) and (2.4) are not completely correct!\n2.2.3 Free and bound variables\nThe introduction of variables and quantiﬁers allows us to express the notions\nof all . . . and some . . . Intuitively, to verify that ∀x Q(x) is true amounts\nto replacing x by any of its possible values and checking that Q holds for\neach one of them. There are two important and diﬀerent senses in which such\nformulas can be ‘true.’ First, if we give concrete meanings to all predicate and\nfunction symbols involved we have a model and can check whether a formula\nis true for this particular model. For example, if a formula encodes a required\nbehaviour of a hardware circuit, then we would want to know whether it is\ntrue for the model of the circuit. Second, one sometimes would like to ensure\nthat certain formulas are true for all models. Consider P(c) ∧∀y(P(y) →",
    "behaviour of a hardware circuit, then we would want to know whether it is\ntrue for the model of the circuit. Second, one sometimes would like to ensure\nthat certain formulas are true for all models. Consider P(c) ∧∀y(P(y) →\nQ(y)) →Q(c) for a constant c; clearly, this formula should be true no matter\nwhat model we are looking at. It is this second kind of truth which is the\nprimary focus of Section 2.3.\n2.2 Predicate logic as a formal language\n103\nUnfortunately, things are more complicated if we want to deﬁne formally\nwhat it means for a formula to be true in a given model. Ideally, we seek a\ndeﬁnition that we could use to write a computer program verifying that a\nformula holds in a given model. To begin with, we need to understand that\nvariables occur in diﬀerent ways. Consider the formula\n∀x ((P(x) →Q(x)) ∧S(x, y)).\nWe draw its parse tree in the same way as for propositional formulas, but\nwith two additional sorts of nodes:\nr The quantiﬁers ∀x and ∃y form nodes and have, like negation, just one subtree.\nr Predicate expressions, which are generally of the form P(t1, t2, . . . , tn), have the\nsymbol P as a node, but now P has n many subtrees, namely the parse trees of\nthe terms t1, t2, . . . , tn.\nSo in our particular case above we arrive at the parse tree in Figure 2.1.\nYou can see that variables occur at two diﬀerent sorts of places. First, they\nappear next to quantiﬁers ∀and ∃in nodes like ∀x and ∃z; such nodes always\nhave one subtree, subsuming their scope to which the respective quantiﬁer\napplies.\nThe other sort of occurrence of variables is leaf nodes containing variables.\nIf variables are leaf nodes, then they stand for values that still have to be\nmade concrete. There are two principal such occurrences:\n1.\nIn our example in Figure 2.1, we have three leaf nodes x. If we walk up the\ntree beginning at any one of these x leaves, we run into the quantiﬁer ∀x. This\nmeans that those occurrences of x are actually bound to ∀x so they represent,",
    "made concrete. There are two principal such occurrences:\n1.\nIn our example in Figure 2.1, we have three leaf nodes x. If we walk up the\ntree beginning at any one of these x leaves, we run into the quantiﬁer ∀x. This\nmeans that those occurrences of x are actually bound to ∀x so they represent,\nor stand for, any possible value of x.\n2.\nIn walking upwards, the only quantiﬁer that the leaf node y runs into is ∀x but\nthat x has nothing to do with y; x and y are diﬀerent place holders. So y is free\nin this formula. This means that its value has to be speciﬁed by some additional\ninformation, for example, the contents of a location in memory.\nDeﬁnition 2.6 Let φ be a formula in predicate logic. An occurrence of x\nin φ is free in φ if it is a leaf node in the parse tree of φ such that there\nis no path upwards from that node x to a node ∀x or ∃x. Otherwise, that\noccurrence of x is called bound. For ∀x φ, or ∃x φ, we say that φ – minus\nany of φ’s subformulas ∃x ψ, or ∀x ψ – is the scope of ∀x, respectively ∃x.\nThus, if x occurs in φ, then it is bound if, and only if, it is in the scope of\nsome ∃x or some ∀x; otherwise it is free. In terms of parse trees, the scope\nof a quantiﬁer is just its subtree, minus any subtrees which re-introduce a\n104\n2 Predicate logic\n→\n∀x\n∨\n∧\n¬\nQ\nP\nQ\nP\ny\nx\nx\nx\nfree\nfree\nbound\nbound\nFigure 2.2. A parse tree of a predicate logic formula illustrating free\nand bound occurrences of variables.\nquantiﬁer for x; e.g. the scope of ∀x in ∀x (P(x) →∃x Q(x)) is P(x). It is\nquite possible, and common, that a variable is bound and free in a formula.\nConsider the formula\n(∀x (P(x) ∧Q(x))) →(¬P(x) ∨Q(y))\nand its parse tree in Figure 2.2. The two x leaves in the subtree of ∀x are\nbound since they are in the scope of ∀x, but the leaf x in the right subtree of\n→is free since it is not in the scope of any quantiﬁer ∀x or ∃x. Note, however,\nthat a single leaf either is under the scope of a quantiﬁer, or it isn’t. Hence",
    "bound since they are in the scope of ∀x, but the leaf x in the right subtree of\n→is free since it is not in the scope of any quantiﬁer ∀x or ∃x. Note, however,\nthat a single leaf either is under the scope of a quantiﬁer, or it isn’t. Hence\nindividual occurrences of variables are either free or bound, never both at\nthe same time.\n2.2.4 Substitution\nVariables are place holders so we must have some means of replacing them\nwith more concrete information. On the syntactic side, we often need to\nreplace a leaf node x by the parse tree of an entire term t. Recall from the\ndeﬁnition of formulas that any replacement of x may only be a term; it\ncould not be a predicate expression, or a more complex formula, for x serves\nas a term to a predicate symbol one step higher up in the parse tree (see\nDeﬁnition 2.1 and the grammar in (2.2)). In substituting t for x we have to\n2.2 Predicate logic as a formal language\n105\nleave untouched the bound leaves x since they are in the scope of some ∃x\nor ∀x, i.e. they stand for some unspeciﬁed or all values respectively.\nDeﬁnition 2.7 Given a variable x, a term t and a formula φ we deﬁne φ[t/x]\nto be the formula obtained by replacing each free occurrence of variable x\nin φ with t.\nSubstitutions are easily understood by looking at some examples. Let f be a\nfunction symbol with two arguments and φ the formula with the parse tree\nin Figure 2.1. Then f(x, y) is a term and φ[f(x, y)/x] is just φ again. This\nis true because all occurrences of x are bound in φ, so none of them gets\nsubstituted.\nNow consider φ to be the formula with the parse tree in Figure 2.2. Here\nwe have one free occurrence of x in φ, so we substitute the parse tree of\nf(x, y) for that free leaf node x and obtain the parse tree in Figure 2.3.\nNote that the bound x leaves are unaﬀected by this operation. You can see\nthat the process of substitution is straightforward, but requires that it be\napplied only to the free occurrences of the variable to be substituted.",
    "f(x, y) for that free leaf node x and obtain the parse tree in Figure 2.3.\nNote that the bound x leaves are unaﬀected by this operation. You can see\nthat the process of substitution is straightforward, but requires that it be\napplied only to the free occurrences of the variable to be substituted.\nA word on notation: in writing φ[t/x], we really mean this to be the\nformula obtained by performing the operation [t/x] on φ. Strictly speaking,\nthe chain of symbols φ[t/x] is not a logical formula, but its result will be a\nformula, provided that φ was one in the ﬁrst place.\nx replaced by the term f(x, y)\nx\ny\nf\nP\n¬\n∨\nQ\ny\n→\n∀x\n∧\nP\nQ\nx\nx\nFigure 2.3. A parse tree of a formula resulting from substitution.\n106\n2 Predicate logic\nUnfortunately, substitutions can give rise to undesired side eﬀects. In\nperforming a substitution φ[t/x], the term t may contain a variable y, where\nfree occurrences of x in φ are under the scope of ∃y or ∀y in φ. By carrying\nout this substitution φ[t/x], the value y, which might have been ﬁxed by a\nconcrete context, gets caught in the scope of ∃y or ∀y. This binding capture\noverrides the context speciﬁcation of the concrete value of y, for it will now\nstand for ‘some unspeciﬁed’ or ‘all ,’ respectively. Such undesired variable\ncaptures are to be avoided at all costs.\nDeﬁnition 2.8 Given a term t, a variable x and a formula φ, we say that\nt is free for x in φ if no free x leaf in φ occurs in the scope of ∀y or ∃y for\nany variable y occurring in t.\nThis deﬁnition is maybe hard to swallow. Let us think of it in terms of\nparse trees. Given the parse tree of φ and the parse tree of t, we can perform\nthe substitution [t/x] on φ to obtain the formula φ[t/x]. The latter has a\nparse tree where all free x leaves of the parse tree of φ are replaced by the\nparse tree of t. What ‘t is free for x in φ’ means is that the variable leaves of\nthe parse tree of t won’t become bound if placed into the bigger parse tree",
    "parse tree where all free x leaves of the parse tree of φ are replaced by the\nparse tree of t. What ‘t is free for x in φ’ means is that the variable leaves of\nthe parse tree of t won’t become bound if placed into the bigger parse tree\nof φ[t/x]. For example, if we consider x, t and φ in Figure 2.3, then t is free\nfor x in φ since the new leaf variables x and y of t are not under the scope\nof any quantiﬁers involving x or y.\nExample 2.9 Consider the φ with parse tree in Figure 2.4 and let t be\nf(y, y). All two occurrences of x in φ are free. The leftmost occurrence of\nx could be substituted since it is not in the scope of any quantiﬁer, but\nsubstituting the rightmost x leaf introduces a new variable y in t which\nbecomes bound by ∀y. Therefore, f(y, y) is not free for x in φ.\nWhat if there are no free occurrences of x in φ? Inspecting the deﬁnition\nof ‘t is free for x in φ,’ we see that every term t is free for x in φ in that\ncase, since no free variable x of φ is below some quantiﬁer in the parse tree\nof φ. So the problematic situation of variable capture in performing φ[t/x]\ncannot occur. Of course, in that case φ[t/x] is just φ again.\nIt might be helpful to compare ‘t is free for x in φ’ with a precondition of\ncalling a procedure for substitution. If you are asked to compute φ[t/x] in\nyour exercises or exams, then that is what you should do; but any reasonable\nimplementation of substitution used in a theorem prover would have to check\nwhether t is free for x in φ and, if not, rename some variables with fresh\nones to avoid the undesirable capture of variables.\n2.3 Proof theory of predicate logic\n107\nthe term f(y, y) is\nnot free for x in\nthis formula\n∀y\nx\n→\nP\nx\ny\nS\nQ\n∧\nFigure 2.4. A parse tree for which a substitution has dire consequences.\n2.3 Proof theory of predicate logic\n2.3.1 Natural deduction rules\nProofs in the natural deduction calculus for predicate logic are similar to\nthose for propositional logic in Chapter 1, except that we have new proof",
    "x\ny\nS\nQ\n∧\nFigure 2.4. A parse tree for which a substitution has dire consequences.\n2.3 Proof theory of predicate logic\n2.3.1 Natural deduction rules\nProofs in the natural deduction calculus for predicate logic are similar to\nthose for propositional logic in Chapter 1, except that we have new proof\nrules for dealing with the quantiﬁers and with the equality symbol. Strictly\nspeaking, we are overloading the previously established proof rules for the\npropositional connectives ∧, ∨etc. That simply means that any proof rule\nof Chapter 1 is still valid for logical formulas of predicate logic (we origi-\nnally deﬁned those rules for logical formulas of propositional logic). As in\nthe natural deduction calculus for propositional logic, the additional rules\nfor the quantiﬁers and equality will come in two ﬂavours: introduction and\nelimination rules.\nThe proof rules for equality\nFirst, let us state the proof rules for\nequality. Here equality does not mean syntactic, or intensional, equality,\nbut equality in terms of computation results. In either of these senses, any\nterm t has to be equal to itself. This is expressed by the introduction rule\nfor equality:\nt = t\n=i\n(2.5)\nwhich is an axiom (as it does not depend on any premises). Notice that it\n108\n2 Predicate logic\nmay be invoked only if t is a term, our language doesn’t permit us to talk\nabout equality between formulas.\nThis rule is quite evidently sound, but it is not very useful on its own.\nWhat we need is a principle that allows us to substitute equals for equals\nrepeatedly. For example, suppose that y ∗(w + 2) equals y ∗w + y ∗2; then\nit certainly must be the case that z ≥y ∗(w + 2) implies z ≥y ∗w + y ∗2\nand vice versa. We may now express this substitution principle as the rule\n=e:\nt1 = t2\nφ[t1/x]\nφ[t2/x]\n=e.\nNote that t1 and t2 have to be free for x in φ, whenever we want to apply\nthe rule =e; this is an example of a side condition of a proof rule.\nConvention 2.10 Throughout this section, when we write a substitution",
    "=e:\nt1 = t2\nφ[t1/x]\nφ[t2/x]\n=e.\nNote that t1 and t2 have to be free for x in φ, whenever we want to apply\nthe rule =e; this is an example of a side condition of a proof rule.\nConvention 2.10 Throughout this section, when we write a substitution\nin the form φ[t/x], we implicitly assume that t is free for x in φ; for, as we\nsaw in the last section, a substitution doesn’t make sense otherwise.\nWe obtain proof\n1\n(x + 1) = (1 + x)\npremise\n2\n(x + 1 > 1) →(x + 1 > 0)\npremise\n3\n(1 + x > 1) →(1 + x > 0)\n=e 1, 2\nestablishing the validity of the sequent\nx + 1 = 1 + x, (x + 1 > 1) →(x + 1 > 0) ⊢(1 + x) > 1 →(1 + x) > 0.\nIn this particular proof t1 is (x + 1), t2 is (1 + x) and φ is (x > 1) →\n(x > 0). We used the name =e since it reﬂects what this rule is doing to\ndata: it eliminates the equality in t1 = t2 by replacing all t1 in φ[t1/x]\nwith t2. This is a sound substitution principle, since the assumption that\nt1 equals t2 guarantees that the logical meanings of φ[t1/x] and φ[t2/x]\nmatch.\nThe principle of substitution, in the guise of the rule =e, is quite powerful.\nTogether with the rule =i, it allows us to show the sequents\nt1 = t2 ⊢t2 = t1\n(2.6)\nt1 = t2, t2 = t3 ⊢t1 = t3.\n(2.7)\n2.3 Proof theory of predicate logic\n109\nA proof for (2.6) is:\n1\nt1 = t2\npremise\n2\nt1 = t1\n=i\n3\nt2 = t1\n=e 1, 2\nwhere φ is x = t1. A proof for (2.7) is:\n1\nt2 = t3\npremise\n2\nt1 = t2\npremise\n3\nt1 = t3\n=e 1, 2\nwhere φ is t1 = x, so in line 2 we have φ[t2/x] and in line 3 we obtain φ[t3/x],\nas given by the rule =e applied to lines 1 and 2. Notice how we applied the\nscheme =e with several diﬀerent instantiations.\nOur discussion of the rules =i and =e has shown that they force equality\nto be reﬂexive (2.5), symmetric (2.6) and transitive (2.7). These are minimal\nand necessary requirements for any sane concept of (extensional) equality.\nWe leave the topic of equality for now to move on to the proof rules for\nquantiﬁers.\nThe proof rules for universal quantification\nThe rule for eliminat-",
    "and necessary requirements for any sane concept of (extensional) equality.\nWe leave the topic of equality for now to move on to the proof rules for\nquantiﬁers.\nThe proof rules for universal quantification\nThe rule for eliminat-\ning ∀is the following:\n∀x φ\nφ[t/x]\n∀x e.\nIt says: If ∀x φ is true, then you could replace the x in φ by any term t\n(given, as usual, the side condition that t be free for x in φ) and conclude\nthat φ[t/x] is true as well. The intuitive soundness of this rule is self-evident.\nRecall that φ[t/x] is obtained by replacing all free occurrences of x in φ\nby t. You may think of the term t as a more concrete instance of x. Since φ\nis assumed to be true for all x, that should also be the case for any term t.\nExample 2.11 To see the necessity of the proviso that t be free for x in\nφ, consider the case that φ is ∃y (x < y) and the term to be substituted\nfor x is y. Let’s suppose we are reasoning about numbers with the usual\n‘smaller than’ relation. The statement ∀x φ then says that for all numbers\nn there is some bigger number m, which is indeed true of integers or real\nnumbers. However, φ[y/x] is the formula ∃y (y < y) saying that there is a\nnumber which is bigger than itself. This is wrong; and we must not allow a\nproof rule which derives semantically wrong things from semantically valid\n110\n2 Predicate logic\nones. Clearly, what went wrong was that y became bound in the process of\nsubstitution; y is not free for x in φ. Thus, in going from ∀x φ to φ[t/x],\nwe have to enforce the side condition that t be free for x in φ: use a fresh\nvariable for y to change φ to, say, ∃z (x < z) and then apply [y/x] to that\nformula, rendering ∃z (y < z).\nThe rule ∀x i is a bit more complicated. It employs a proof box similar\nto those we have already seen in natural deduction for propositional logic,\nbut this time the box is to stipulate the scope of the ‘dummy variable’ x0\nrather than the scope of an assumption. The rule ∀x i is written\nx0\n...\nφ[x0/x]\n∀x φ\n∀x i.",
    "to those we have already seen in natural deduction for propositional logic,\nbut this time the box is to stipulate the scope of the ‘dummy variable’ x0\nrather than the scope of an assumption. The rule ∀x i is written\nx0\n...\nφ[x0/x]\n∀x φ\n∀x i.\nIt says: If, starting with a ‘fresh’ variable x0, you are able to prove some\nformula φ[x0/x] with x0 in it, then (because x0 is fresh) you can derive\n∀x φ. The important point is that x0 is a new variable which doesn’t occur\nanywhere outside its box; we think of it as an arbitrary term. Since we\nassumed nothing about this x0, anything would work in its place; hence the\nconclusion ∀x φ.\nIt takes a while to understand this rule, since it seems to be going from\nthe particular case of φ to the general case ∀x φ. The side condition, that\nx0 does not occur outside the box, is what allows us to get away with\nthis.\nTo understand this, think of the following analogy. If you want to prove\nto someone that you can, say, split a tennis ball in your hand by squashing\nit, you might say ‘OK, give me a tennis ball and I’ll split it.’ So we give you\none and you do it. But how can we be sure that you could split any tennis\nball in this way? Of course, we can’t give you all of them, so how could we\nbe sure that you could split any one? Well, we assume that the one you did\nsplit was an arbitrary, or ‘random,’ one, i.e. that it wasn’t special in any\nway – like a ball which you may have ‘prepared’ beforehand; and that is\nenough to convince us that you could split any tennis ball. Our rule says\nthat if you can prove φ about an x0 that isn’t special in any way, then you\ncould prove it for any x whatsoever.\nTo put it another way, the step from φ to ∀x φ is legitimate only if we have\narrived at φ in such a way that none of its assumptions contain x as a free\nvariable. Any assumption which has a free occurrence of x puts constraints\n2.3 Proof theory of predicate logic\n111\non such an x. For example, the assumption bird(x) conﬁnes x to the realm",
    "arrived at φ in such a way that none of its assumptions contain x as a free\nvariable. Any assumption which has a free occurrence of x puts constraints\n2.3 Proof theory of predicate logic\n111\non such an x. For example, the assumption bird(x) conﬁnes x to the realm\nof birds and anything we can prove about x using this formula will have\nto be a statement restricted to birds and not about anything else we might\nhave had in mind.\nIt is time we looked at an example of these proof rules at work. Here is a\nproof of the sequent ∀x (P(x) →Q(x)), ∀x P(x) ⊢∀x Q(x):\n1\n∀x (P(x) →Q(x))\npremise\n2\n∀x P(x)\npremise\nx0\n3\nP(x0) →Q(x0)\n∀x e 1\n4\nP(x0)\n∀x e 2\n5\nQ(x0)\n→e 3, 4\n6\n∀x Q(x)\n∀x i 3−5\nThe structure of this proof is guided by the fact that the conclusion is\na ∀formula. To arrive at this, we will need an application of ∀x i, so we\nset up the box controlling the scope of x0. The rest is now mechanical:\nwe prove ∀x Q(x) by proving Q(x0); but the latter we can prove as soon as\nwe can prove P(x0) and P(x0) →Q(x0), which themselves are instances of\nthe premises (obtained by ∀e with the term x0). Note that we wrote the\nname of the dummy variable to the left of the ﬁrst proof line in its scope\nbox.\nHere is a simpler example which uses only ∀x e: we show the validity of\nthe sequent P(t), ∀x (P(x) →¬Q(x)) ⊢¬Q(t) for any term t:\n1\nP(t)\npremise\n2\n∀x (P(x) →¬Q(x))\npremise\n3\nP(t) →¬Q(t)\n∀x e 2\n4\n¬Q(t)\n→e 3, 1\nNote that we invoked ∀x e with the same instance t as in the assumption\nP(t). If we had invoked ∀x e with y, say, and obtained P(y) →¬Q(y), then\nthat would have been valid, but it would not have been helpful in the case\nthat y was diﬀerent from t. Thus, ∀x e is really a scheme of rules, one for\neach term t (free for x in φ), and we should make our choice on the basis of\nconsistent pattern matching. Further, note that we have rules ∀x i and ∀x e\nfor each variable x. In particular, there are rules ∀y i, ∀y e and so on. We\n112\n2 Predicate logic",
    "each term t (free for x in φ), and we should make our choice on the basis of\nconsistent pattern matching. Further, note that we have rules ∀x i and ∀x e\nfor each variable x. In particular, there are rules ∀y i, ∀y e and so on. We\n112\n2 Predicate logic\nwill write ∀i and ∀e when we speak about such rules without concern for the\nactual quantiﬁer variable.\nNotice also that, although the square brackets representing substitution\nappear in the rules ∀i and ∀e, they do not appear when we use those rules.\nThe reason for this is that we actually carry out the substitution that is asked\nfor. In the rules, the expression φ[t/x] means: ‘φ, but with free occurrences\nof x replaced by t.’ Thus, if φ is P(x, y) →Q(y, z) and the rule refers to\nφ[a/y], we carry out the substitution and write P(x, a) →Q(a, z) in the\nproof.\nA helpful way of understanding the universal quantiﬁer rules is to com-\npare the rules for ∀with those for ∧. The rules for ∀are in some sense\ngeneralisations of those for ∧; whereas ∧has just two conjuncts, ∀acts like\nit conjoins lots of formulas (one for each substitution instance of its vari-\nable). Thus, whereas ∧i has two premises, ∀x i has a premise φ[x0/x] for\neach possible ‘value’ of x0. Similarly, where and-elimination allows you to\ndeduce from φ ∧ψ whichever of φ and ψ you like, forall-elimination allows\nyou to deduce φ[t/x] from ∀x φ, for whichever t you (and the side condition)\nlike. To say the same thing another way: think of ∀x i as saying: to prove\n∀x φ, you have to prove φ[x0/x] for every possible value x0; while ∧i says\nthat to prove φ1 ∧φ2 you have to prove φi for every i = 1, 2.\nThe proof rules for existential quantification\nThe analogy between\n∀and ∧extends also to ∃and ∨; and you could even try to guess the rules\nfor ∃by starting from the rules for ∨and applying the same ideas as those\nthat related ∧to ∀. For example, we saw that the rules for or-introduction\nwere a sort of dual of those for and-elimination; to emphasise this point, we",
    "∀and ∧extends also to ∃and ∨; and you could even try to guess the rules\nfor ∃by starting from the rules for ∨and applying the same ideas as those\nthat related ∧to ∀. For example, we saw that the rules for or-introduction\nwere a sort of dual of those for and-elimination; to emphasise this point, we\ncould write them as\nφ1 ∧φ2\nφk\n∧ek\nφk\nφ1 ∨φ2\n∨ik\nwhere k can be chosen to be either 1 or 2. Therefore, given the form of\nforall-elimination, we can infer that exists-introduction must be simply\nφ[t/x]\n∃xφ\n∃x i.\nIndeed, this is correct: it simply says that we can deduce ∃x φ whenever we\nhave φ[t/x] for some term t (naturally, we impose the side condition that t\nbe free for x in φ).\nIn the rule ∃i, we see that the formula φ[t/x] contains, from a compu-\ntational point of view, more information than ∃x φ. The latter merely says\n2.3 Proof theory of predicate logic\n113\nthat φ holds for some, unspeciﬁed, value of x; whereas φ[t/x] has a witness\nt at its disposal. Recall that the square-bracket notation asks us actually to\ncarry out the substitution. However, the notation φ[t/x] is somewhat mis-\nleading since it suggests not only the right witness t but also the formula\nφ itself. For example, consider the situation in which t equals y such that\nφ[y/x] is y = y. Then you can check for yourself that φ could be a number\nof things, like x = x or x = y. Thus, ∃x φ will depend on which of these φ\nyou were thinking of.\nExtending the analogy between ∃and ∨, the rule ∨e leads us to the\nfollowing formulation of ∃e:\n∃x φ\nx0 φ[x0/x]\n...\nχ\nχ\n∃e.\nLike ∨e, it involves a case analysis. The reasoning goes: We know ∃x φ is\ntrue, so φ is true for at least one ‘value’ of x. So we do a case analysis over\nall those possible values, writing x0 as a generic value representing them\nall. If assuming φ[x0/x] allows us to prove some χ which doesn’t mention\nx0, then this χ must be true whichever x0 makes φ[x0/x] true. And that’s\nprecisely what the rule ∃e allows us to deduce. Of course, we impose the",
    "all those possible values, writing x0 as a generic value representing them\nall. If assuming φ[x0/x] allows us to prove some χ which doesn’t mention\nx0, then this χ must be true whichever x0 makes φ[x0/x] true. And that’s\nprecisely what the rule ∃e allows us to deduce. Of course, we impose the\nside condition that x0 can’t occur outside its box (therefore, in particular,\nit cannot occur in χ). The box is controlling two things: the scope of x0 and\nalso the scope of the assumption φ[x0/x].\nJust as ∨e says that to use φ1 ∨φ2, you have to be prepared for either of\nthe φi, so ∃e says that to use ∃x φ you have to be prepared for any possible\nφ[x0/x]. Another way of thinking about ∃e goes like this: If you know ∃x φ\nand you can derive some χ from φ[x0/x], i.e. by giving a name to the thing\nyou know exists, then you can derive χ even without giving that thing a\nname (provided that χ does not refer to the name x0).\nThe rule ∃x e is also similar to ∨e in the sense that both of them are\nelimination rules which don’t have to conclude a subformula of the formula\nthey are about to eliminate. Please verify that all other elimination rules\nintroduced so far have this subformula property.2 This property is computa-\ntionally very pleasant, for it allows us to narrow down the search space for\na proof dramatically. Unfortunately, ∃x e, like its cousin ∨e, is not of that\ncomputationally benign kind.\n2 For ∀x e we perform a substitution [t/x], but it preserves the logical structure of φ.\n114\n2 Predicate logic\nLet us practice these rules on a couple of examples. Certainly, we should\nbe able to prove the validity of the sequent ∀x φ ⊢∃x φ. The proof\n1\n∀x φ\npremise\n2\nφ[x/x]\n∀x e 1\n3\n∃x φ\n∃x i 2\ndemonstrates that, where we chose t to be x with respect to both ∀x e and\nto ∃x i (and note that x is free for x in φ and that φ[x/x] is simply φ again).\nProving the validity of the sequent ∀x (P(x) →Q(x)),\n∃x P(x)\n⊢\n∃x Q(x) is more complicated:\n1\n∀x (P(x) →Q(x))\npremise\n2\n∃x P(x)\npremise\nx0\n3\nP(x0)",
    "to ∃x i (and note that x is free for x in φ and that φ[x/x] is simply φ again).\nProving the validity of the sequent ∀x (P(x) →Q(x)),\n∃x P(x)\n⊢\n∃x Q(x) is more complicated:\n1\n∀x (P(x) →Q(x))\npremise\n2\n∃x P(x)\npremise\nx0\n3\nP(x0)\nassumption\n4\nP(x0) →Q(x0)\n∀x e 1\n5\nQ(x0)\n→e 4, 3\n6\n∃x Q(x)\n∃x i 5\n7\n∃x Q(x)\n∃x e 2, 3−6\nThe motivation for introducing the box in line 3 of this proof is the existential\nquantiﬁer in the premise ∃x P(x) which has to be eliminated. Notice that\nthe ∃in the conclusion has to be introduced within the box and observe the\nnesting of these two steps. The formula ∃x Q(x) in line 6 is the instantiation\nof χ in the rule ∃e and does not contain an occurrence of x0, so it is allowed\nto leave the box to line 7. The almost identical ‘proof’\n1\n∀x (P(x) →Q(x))\npremise\n2\n∃x P(x)\npremise\nx0\n3\nP(x0)\nassumption\n4\nP(x0) →Q(x0)\n∀x e 1\n5\nQ(x0)\n→e 4, 3\n6\nQ(x0)\n∃x e 2, 3−5\n7\n∃x Q(x)\n∃x i 6\nis illegal! Line 6 allows the fresh parameter x0 to escape the scope of the\nbox which declares it. This is not permissible and we will see on page 116 an\nexample where such illicit use of proof rules results in unsound arguments.\n2.3 Proof theory of predicate logic\n115\nA sequent with a slightly more complex proof is\n∀x (Q(x) →R(x)), ∃x (P(x) ∧Q(x)) ⊢∃x (P(x) ∧R(x))\nand could model some argument such as\nIf all quakers are reformists and if there is a protestant who is also\na quaker, then there must be a protestant who is also a reformist.\nOne possible proof strategy is to assume P(x0) ∧Q(x0), get the instance\nQ(x0) →R(x0) from ∀x (Q(x) →R(x)) and use ∧e2 to get our hands on\nQ(x0), which gives us R(x0) via →e . . . :\n1\n∀x (Q(x) →R(x))\npremise\n2\n∃x (P(x) ∧Q(x))\npremise\nx0\n3\nP(x0) ∧Q(x0)\nassumption\n4\nQ(x0) →R(x0)\n∀x e 1\n5\nQ(x0)\n∧e2 3\n6\nR(x0)\n→e 4, 5\n7\nP(x0)\n∧e1 3\n8\nP(x0) ∧R(x0)\n∧i 7, 6\n9\n∃x (P(x) ∧R(x))\n∃x i 8\n10\n∃x (P(x) ∧R(x))\n∃x e 2, 3−9\nNote the strategy of this proof: We list the two premises. The second premise",
    "premise\n2\n∃x (P(x) ∧Q(x))\npremise\nx0\n3\nP(x0) ∧Q(x0)\nassumption\n4\nQ(x0) →R(x0)\n∀x e 1\n5\nQ(x0)\n∧e2 3\n6\nR(x0)\n→e 4, 5\n7\nP(x0)\n∧e1 3\n8\nP(x0) ∧R(x0)\n∧i 7, 6\n9\n∃x (P(x) ∧R(x))\n∃x i 8\n10\n∃x (P(x) ∧R(x))\n∃x e 2, 3−9\nNote the strategy of this proof: We list the two premises. The second premise\nis of use here only if we apply ∃x e to it. This sets up the proof box in\nlines 3−9 as well as the fresh parameter name x0. Since we want to prove\n∃x (P(x) ∧R(x)), this formula has to be the last one in the box (our goal)\nand the rest involves ∀x e and ∃x i.\nThe rules ∀i and ∃e both have the side condition that the dummy variable\ncannot occur outside the box in the rule. Of course, these rules may still be\nnested, by choosing another fresh name (e.g. y0) for the dummy variable. For\nexample, consider the sequent ∃x P(x), ∀x ∀y (P(x) →Q(y)) ⊢∀y Q(y).\n(Look how strong the second premise is, by the way: given any x, y, if P(x),\nthen Q(y). This means that, if there is any object with the property P, then\nall objects shall have the property Q.) Its proof goes as follows: We take an\narbitrary y0 and prove Q(y0); this we do by observing that, since some x\n116\n2 Predicate logic\nsatisﬁes P, so by the second premise any y satisﬁes Q:\n1\n∃x P(x)\npremise\n2\n∀x∀y (P(x) →Q(y))\npremise\ny0\n3\nx0\n4\nP(x0)\nassumption\n5\n∀y (P(x0) →Q(y))\n∀x e 2\n6\nP(x0) →Q(y0)\n∀y e 5\n7\nQ(y0)\n→e 6, 4\n8\nQ(y0)\n∃x e 1, 4−7\n9\n∀y Q(y)\n∀y i 3−8\nThere is no special reason for picking x0 as a name for the dummy variable\nwe use for ∀x and ∃x and y0 as a name for ∀y and ∃y. We do this only\nbecause it makes it easier for us humans. Again, study the strategy of this\nproof. We ultimately have to show a ∀y formula which requires us to use\n∀y i, i.e. we need to open up a proof box (lines 3−8) whose subgoal is to\nprove a generic instance Q(y0). Within that box we want to make use of the\npremise ∃x P(x) which results in the proof box set-up of lines 4−7. Notice\nthat, in line 8, we may well move Q(y0) out of the box controlled by x0.",
    "∀y i, i.e. we need to open up a proof box (lines 3−8) whose subgoal is to\nprove a generic instance Q(y0). Within that box we want to make use of the\npremise ∃x P(x) which results in the proof box set-up of lines 4−7. Notice\nthat, in line 8, we may well move Q(y0) out of the box controlled by x0.\nWe have repeatedly emphasised the point that the dummy variables in\nthe rules ∃e and ∀i must not occur outside their boxes. Here is an example\nwhich shows how things would go wrong if we didn’t have this side condi-\ntion. Consider the invalid sequent ∃x P(x), ∀x (P(x) →Q(x)) ⊢∀y Q(y).\n(Compare it with the previous sequent; the second premise is now much\nweaker, allowing us to conclude Q only for those objects for which we know\nP.) Here is an alleged ‘proof’ of its validity:\n1\n∃x P(x)\npremise\n2\n∀x (P(x) →Q(x))\npremise\nx0\n3\nx0\n4\nP(x0)\nassumption\n5\nP(x0) →Q(x0)\n∀x e 2\n6\nQ(x0)\n→e 5, 4\n7\nQ(x0)\n∃x e 1, 4−6\n8\n∀y Q(y)\n∀y i 3−7\n2.3 Proof theory of predicate logic\n117\nThe last step introducing ∀y is not the bad one; that step is ﬁne. The bad\none is the second from last one, concluding Q(x0) by ∃x e and violating the\nside condition that x0 may not leave the scope of its box. You can try a few\nother ways of ‘proving’ this sequent, but none of them should work (assuming\nthat our proof system is sound with respect to semantic entailment, which\nwe deﬁne in the next section). Without this side condition, we would also\nbe able to prove that ‘all x satisfy the property P as soon as one of them\ndoes so,’ a semantic disaster of biblical proportions!\n2.3.2 Quantifier equivalences\nWe have already hinted at semantic equivalences between certain forms of\nquantiﬁcation. Now we want to provide formal proofs for some of the most\ncommonly used quantiﬁer equivalences. Quite a few of them involve several\nquantiﬁcations over more than just one variable. Thus, this topic is also\ngood practice for using the proof rules for quantiﬁers in a nested fashion.",
    "quantiﬁcation. Now we want to provide formal proofs for some of the most\ncommonly used quantiﬁer equivalences. Quite a few of them involve several\nquantiﬁcations over more than just one variable. Thus, this topic is also\ngood practice for using the proof rules for quantiﬁers in a nested fashion.\nFor example, the formula ∀x ∀y φ should be equivalent to ∀y ∀x φ since\nboth say that φ should hold for all values of x and y. What about (∀x φ) ∧\n(∀x ψ) versus ∀x (φ ∧ψ)? A moment’s thought reveals that they should have\nthe same meaning as well. But what if the second conjunct does not start\nwith ∀x? So what if we are looking at (∀x φ) ∧ψ in general and want to\ncompare it with ∀x (φ ∧ψ)? Here we need to be careful, since x might be\nfree in ψ and would then become bound in the formula ∀x (φ ∧ψ).\nExample 2.12 We may specify ‘Not all birds can ﬂy.’ as ¬∀x (B(x) →\nF(x)) or as ∃x (B(x) ∧¬F(x)). The former formal speciﬁcation is closer\nto the structure of the English speciﬁcation, but the latter is logically equiv-\nalent to the former. Quantiﬁer equivalences help us in establishing that\nspeciﬁcations that ‘look’ diﬀerent are really saying the same thing.\nHere are some quantiﬁer equivalences which you should become familiar\nwith. As in Chapter 1, we write φ1 ⊣⊢φ2 as an abbreviation for the validity\nof φ1 ⊢φ2 and φ2 ⊢φ1.\nTheorem 2.13 Let φ and ψ be formulas of predicate logic. Then we have\nthe following equivalences:\n1.\n(a) ¬∀x φ ⊣⊢∃x ¬φ\n(b) ¬∃x φ ⊣⊢∀x ¬φ.\n2.\nAssuming that x is not free in ψ:\n118\n2 Predicate logic\n(a) ∀x φ ∧ψ ⊣⊢∀x (φ ∧ψ)3\n(b) ∀x φ ∨ψ ⊣⊢∀x (φ ∨ψ)\n(c) ∃x φ ∧ψ ⊣⊢∃x (φ ∧ψ)\n(d) ∃x φ ∨ψ ⊣⊢∃x (φ ∨ψ)\n(e) ∀x (ψ →φ) ⊣⊢ψ →∀x φ\n(f) ∃x (φ →ψ) ⊣⊢∀x φ →ψ\n(g) ∀x (φ →ψ) ⊣⊢∃x φ →ψ\n(h) ∃x (ψ →φ) ⊣⊢ψ →∃x φ.\n3.\n(a) ∀x φ ∧∀x ψ ⊣⊢∀x (φ ∧ψ)\n(b) ∃x φ ∨∃x ψ ⊣⊢∃x (φ ∨ψ).\n4.\n(a) ∀x ∀y φ ⊣⊢∀y ∀x φ\n(b) ∃x ∃y φ ⊣⊢∃y ∃x φ.\nPROOF: We will prove most of these sequents; the proofs for the remaining\nones are straightforward adaptations and are left as exercises. Recall that",
    "(h) ∃x (ψ →φ) ⊣⊢ψ →∃x φ.\n3.\n(a) ∀x φ ∧∀x ψ ⊣⊢∀x (φ ∧ψ)\n(b) ∃x φ ∨∃x ψ ⊣⊢∃x (φ ∨ψ).\n4.\n(a) ∀x ∀y φ ⊣⊢∀y ∀x φ\n(b) ∃x ∃y φ ⊣⊢∃y ∃x φ.\nPROOF: We will prove most of these sequents; the proofs for the remaining\nones are straightforward adaptations and are left as exercises. Recall that\nwe sometimes write ⊥to denote any contradiction.\n1.\n(a) We will lead up to this by proving the validity of two simpler sequents\nﬁrst: ¬(p1 ∧p2) ⊢¬p1 ∨¬p2 and then ¬∀x P(x) ⊢∃x ¬P(x). The reason for\nproving the ﬁrst of these is to illustrate the close relationship between ∧and\n∨on the one hand and ∀and ∃on the other – think of a model with just two\nelements 1 and 2 such that pi (i = 1, 2) stands for P(x) evaluated at i. The\nidea is that proving this propositional sequent should give us inspiration for\nproving the second one of predicate logic. The reason for proving the latter\nsequent is that it is a special case (in which φ equals P(x)) of the one we are\nreally after, so again it should be simpler while providing some inspiration.\nSo, let’s go.\n1\n¬(p1 ∧p2)\npremise\n2\n¬(¬p1 ∨¬p2)\nassumption\n3\n¬p1\nassumption\n4\n¬p1 ∨¬p2\n∨i1 3\n5\n⊥\n¬e 4, 2\n6\np1\nPBC 3−5\n¬p2\nassumption\n¬p1 ∨¬p2\n∨i2 3\n⊥\n¬e 4, 2\np2\nPBC 3−5\n7\np1 ∧p2\n∧i 6, 6\n8\n⊥\n¬e 7, 1\n9\n¬p1 ∨¬p2\nPBC 2−8\n3 Remember that ∀x φ ∧ψ is implicitly bracketed as (∀x φ) ∧ψ, by virtue of the binding priorities.\n2.3 Proof theory of predicate logic\n119\nYou have seen this sort of proof before, in Chapter 1. It is an example of\nsomething which requires proof by contradiction, or ¬¬e, or LEM (meaning\nthat it simply cannot be proved in the reduced natural deduction system\nwhich discards these three rules) – in fact, the proof above used the rule\nPBC three times.\nNow we prove the validity of ¬∀x P(x) ⊢∃x ¬P(x) similarly, except that\nwhere the rules for ∧and ∨were used we now use those for ∀and ∃:\n1\n¬∀x P(x)\npremise\n2\n¬∃x ¬P(x)\nassumption\nx0\n3\n4\n¬P(x0)\nassumption\n5\n∃x¬P(x)\n∃x i 4\n6\n⊥\n¬e 5, 2\n7\nP(x0)\nPBC 4−6\n8\n∀x P(x)\n∀x i 3−7\n9\n⊥\n¬e 8, 1\n10\n∃x ¬P(x)\nPBC 2−9",
    "Now we prove the validity of ¬∀x P(x) ⊢∃x ¬P(x) similarly, except that\nwhere the rules for ∧and ∨were used we now use those for ∀and ∃:\n1\n¬∀x P(x)\npremise\n2\n¬∃x ¬P(x)\nassumption\nx0\n3\n4\n¬P(x0)\nassumption\n5\n∃x¬P(x)\n∃x i 4\n6\n⊥\n¬e 5, 2\n7\nP(x0)\nPBC 4−6\n8\n∀x P(x)\n∀x i 3−7\n9\n⊥\n¬e 8, 1\n10\n∃x ¬P(x)\nPBC 2−9\nYou will really beneﬁt by spending time understanding the way this proof\nmimics the one above it. This insight is very useful for constructing predicate\nlogic proofs: you ﬁrst construct a similar propositional proof and then mimic\nit.\nNext we prove that ¬∀x φ ⊢∃x ¬φ is valid:\n1\n¬∀x φ\npremise\n2\n¬∃x ¬φ\nassumption\nx0\n3\n4\n¬φ[x0/x]\nassumption\n5\n∃x¬φ\n∃x i 4\n6\n⊥\n¬e 5, 2\n7\nφ[x0/x]\nPBC 4−6\n8\n∀x φ\n∀x i 3−7\n9\n⊥\n¬e 8, 1\n10\n∃x ¬φ\nPBC 2−9\n120\n2 Predicate logic\nProving that the reverse ∃x ¬φ ⊢¬∀x φ is valid is more straightforward,\nfor it does not involve proof by contradiction, ¬¬e, or LEM. Unlike its\nconverse, it has a constructive proof which the intuitionists do accept. We\ncould again prove the corresponding propositional sequent, but we leave\nthat as an exercise.\n1\n∃x ¬φ\nassumption\n2\n∀x φ\nassumption\nx0\n3\n4\n¬φ[x0/x]\nassumption\n5\nφ[x0/x]\n∀x e 2\n6\n⊥\n¬e 5, 4\n7\n⊥\n∃x e 1, 3−6\n8\n¬∀x φ\n¬i 2−7\n2.\n(a) Validity of ∀x φ ∧ψ ⊢∀x (φ ∧ψ) can be proved thus:\n1\n(∀x φ) ∧ψ\npremise\n2\n∀x φ\n∧e1 1\n3\nψ\n∧e2 1\nx0\n4\n5\nφ[x0/x]\n∀x e 2\n6\nφ[x0/x] ∧ψ\n∧i 5, 3\n7\n(φ ∧ψ)[x0/x]\nidentical to 6, since x not free in ψ\n8\n∀x (φ ∧ψ)\n∀x i 4−7\nThe argument for the reverse validity can go like this:\n1\n∀x (φ ∧ψ)\npremise\nx0\n2\n3\n(φ ∧ψ)[x0/x]\n∀x e 1\n4\nφ[x0/x] ∧ψ\nidentical to 3, since x not free in ψ\n5\nψ\n∧e2 3\n6\nφ[x0/x]\n∧e1 3\n7\n∀x φ\n∀x i 2−6\n8\n(∀x φ) ∧ψ\n∧i 7, 5\n2.3 Proof theory of predicate logic\n121\nNotice that the use of ∧i in the last line is permissible, because ψ was obtained\nfor any instantiation of the formula in line 1; although a formal tool for proof\nsupport may complain about such practice.\n3.\n(b) The sequent (∃x φ) ∨(∃x ψ) ⊢∃x (φ ∨ψ) is proved valid using the rule",
    "121\nNotice that the use of ∧i in the last line is permissible, because ψ was obtained\nfor any instantiation of the formula in line 1; although a formal tool for proof\nsupport may complain about such practice.\n3.\n(b) The sequent (∃x φ) ∨(∃x ψ) ⊢∃x (φ ∨ψ) is proved valid using the rule\n∨e; so we have two principal cases, each of which requires the rule\n∃x i:\n1\n(∃x φ) ∨(∃x ψ)\npremise\n2\n∃x φ\nx0\n3\nφ[x0/x]\n4\nφ[x0/x]∨ψ[x0/x]\n5\n(φ ∨ψ)[x0/x]\n6\n∃x (φ ∨ψ)\n7\n∃x (φ ∨ψ)\n∃x ψ\nassumpt.\nx0\nψ[x0/x]\nassumpt.\nφ[x0/x]∨ψ[x0/x]\n∨i 3\n(φ ∨ψ)[x0/x]\nidentical\n∃x (φ ∨ψ)\n∃x i 5\n∃x (φ ∨ψ)\n∃x e 2, 3−6\n8\n∃x (φ ∨ψ)\n∨e 1, 2−7\nThe converse sequent has ∃x (φ ∨ψ) as premise, so its proof has to use ∃x e\nas its last rule; for that rule, we need φ ∨ψ as a temporary assumption and\nneed to conclude (∃x φ) ∨(∃x ψ) from those data; of course, the assumption\nφ ∨ψ requires the usual case analysis:\n1\n∃x (φ ∨ψ)\npremise\nx0\n2\n(φ ∨ψ)[x0/x]\nassumption\n3\nφ[x0/x] ∨ψ[x0/x]\nidentical\n4\nφ[x0/x]\n5\n∃x φ\n6\n∃x φ ∨∃x ψ\nψ[x0/x]\nassumption\n∃x ψ\n∃x i 4\n∃x φ ∨∃x ψ\n∨i 5\n7\n∃x φ ∨∃x ψ\n∨e 3, 4−6\n8\n∃x φ ∨∃x ψ\n∃x e 1, 2−7\n4.\n(b) Given the premise ∃x ∃y φ, we have to nest ∃x e and ∃y e to conclude ∃y ∃x φ.\nOf course, we have to obey the format of these elimination rules as done\nbelow:\n122\n2 Predicate logic\n1\n∃x ∃y φ\npremise\nx0\n2\n(∃y φ)[x0/x]\nassumption\n3\n∃y (φ[x0/x])\nidentical, since x, y diﬀerent variables\ny0\n4\nφ[x0/x][y0/y]\nassumption\n5\nφ[y0/y][x0/x]\nidentical, since x, y, x0, y0 diﬀerent variables\n6\n∃x φ[y0/y]\n∀x i 5\n7\n∃y ∃x φ\n∀y i 6\n8\n∃y ∃x φ\n∃y e3, 4−7\n9\n∃y ∃x φ\n∃x e1, 2−8\nThe validity of the converse sequent is proved in the same way by swapping\nthe roles of x and y.\n2\n2.4 Semantics of predicate logic\nHaving seen how natural deduction of propositional logic can be extended\nto predicate logic, let’s now look at how the semantics of predicate logic\nworks. Just like in the propositional case, the semantics should provide a\nseparate, but ultimately equivalent, characterisation of the logic. By ‘sepa-",
    "to predicate logic, let’s now look at how the semantics of predicate logic\nworks. Just like in the propositional case, the semantics should provide a\nseparate, but ultimately equivalent, characterisation of the logic. By ‘sepa-\nrate,’ we mean that the meaning of the connectives is deﬁned in a diﬀerent\nway; in proof theory, they were deﬁned by proof rules providing an oper-\native explanation. In semantics, we expect something like truth tables. By\n‘equivalent,’ we mean that we should be able to prove soundness and com-\npleteness, as we did for propositional logic – although a fully ﬂedged proof\nof soundness and completeness for predicate logic is beyond the scope of this\nbook.\nBefore we begin describing the semantics of predicate logic, let us look\nmore closely at the real diﬀerence between a semantic and a proof-theoretic\naccount. In proof theory, the basic object which is constructed is a proof.\nLet us write Γ as a shorthand for lists of formulas φ1, φ2, . . . , φn. Thus, to\nshow that Γ ⊢ψ is valid, we need to provide a proof of ψ from Γ. Yet,\nhow can we show that ψ is not a consequence of Γ? Intuitively, this is\nharder; how can you possibly show that there is no proof of something?\nYou would have to consider every ‘candidate’ proof and show it is not one.\nThus, proof theory gives a ‘positive’ characterisation of the logic; it pro-\nvides convincing evidence for assertions like ‘Γ ⊢ψ is valid,’ but it is not\nvery useful for establishing evidence for assertions of the form ‘Γ ⊢φ is not\nvalid.’\n2.4 Semantics of predicate logic\n123\nSemantics, on the other hand, works in the opposite way. To show that ψ\nis not a consequence of Γ is the ‘easy’ bit: ﬁnd a model in which all φi are\ntrue, but ψ isn’t. Showing that ψ is a consequence of Γ, on the other hand,\nis harder in principle. For propositional logic, you need to show that every\nvaluation (an assignment of truth values to all atoms involved) that makes",
    "true, but ψ isn’t. Showing that ψ is a consequence of Γ, on the other hand,\nis harder in principle. For propositional logic, you need to show that every\nvaluation (an assignment of truth values to all atoms involved) that makes\nall φi true also makes ψ true. If there is a small number of valuations, this\nis not so bad. However, when we look at predicate logic, we will ﬁnd that\nthere are inﬁnitely many valuations, called models from hereon, to consider.\nThus, in semantics we have a ‘negative’ characterisation of the logic. We ﬁnd\nestablishing assertions of the form ‘Γ ̸⊨ψ’ (ψ is not a semantic entailment of\nall formulas in Γ) easier than establishing ‘Γ ⊨ψ’ (ψ is a semantic entailment\nof Γ), for in the former case we need only talk about one model, whereas in\nthe latter we potentially have to talk about inﬁnitely many.\nAll this goes to show that it is important to study both proof theory and\nsemantics. For example, if you are trying to show that ψ is not a consequence\nof Γ and you have a hard time doing that, you might want to change your\nstrategy for a while by trying to prove the validity of Γ ⊢ψ. If you ﬁnd a\nproof, you know for sure that ψ is a consequence of Γ. If you can’t ﬁnd a\nproof, then your attempts at proving it often provide insights which lead\nyou to the construction of a counter example. The fact that proof theory\nand semantics for predicate logic are equivalent is amazing, but it does not\nstop them having separate roles in logic, each meriting close study.\n2.4.1 Models\nRecall how we evaluated formulas in propositional logic. For example, the\nformula (p ∨¬q) →(q →p) is evaluated by computing a truth value (T or\nF) for it, based on a given valuation (assumed truth values for p and q).\nThis activity is essentially the construction of one line in the truth table of\n(p ∨¬q) →(q →p). How can we evaluate formulas in predicate logic, e.g.\n∀x ∃y ((P(x) ∨¬Q(y)) →(Q(x) →P(y)))\nwhich ‘enriches’ the formula of propositional logic above? Could we simply",
    "This activity is essentially the construction of one line in the truth table of\n(p ∨¬q) →(q →p). How can we evaluate formulas in predicate logic, e.g.\n∀x ∃y ((P(x) ∨¬Q(y)) →(Q(x) →P(y)))\nwhich ‘enriches’ the formula of propositional logic above? Could we simply\nassume truth values for P(x), Q(y), Q(x) and P(y) and compute a truth\nvalue as before? Not quite, since we have to reﬂect the meaning of the\nquantiﬁers ∀x and ∃y, their dependences and the actual parameters of P\nand Q – a formula ∀x ∃y R(x, y) generally means something else other than\n∃y ∀x R(x, y); why? The problem is that variables are place holders for any,\nor some, unspeciﬁed concrete values. Such values can be of almost any kind:\nstudents, birds, numbers, data structures, programs and so on.\n124\n2 Predicate logic\nThus, if we encounter a formula ∃y ψ, we try to ﬁnd some instance of\ny (some concrete value) such that ψ holds for that particular instance of\ny. If this succeeds (i.e. there is such a value of y for which ψ holds), then\n∃y ψ evaluates to T; otherwise (i.e. there is no concrete value of y which\nrealises ψ) it returns F. Dually, evaluating ∀x ψ amounts to showing that\nψ evaluates to T for all possible values of x; if this is successful, we know\nthat ∀x ψ evaluates to T; otherwise (i.e. there is some value of x such that\nψ computes F) it returns F. Of course, such evaluations of formulas require\na ﬁxed universe of concrete values, the things we are, so to speak, talking\nabout. Thus, the truth value of a formula in predicate logic depends on, and\nvaries with, the actual choice of values and the meaning of the predicate and\nfunction symbols involved.\nIf variables can take on only ﬁnitely many values, we can write a program\nthat evaluates formulas in a compositional way. If the root node of φ is ∧,\n∨, →or ¬, we can compute the truth value of φ by using the truth table of\nthe respective logical connective and by computing the truth values of the",
    "If variables can take on only ﬁnitely many values, we can write a program\nthat evaluates formulas in a compositional way. If the root node of φ is ∧,\n∨, →or ¬, we can compute the truth value of φ by using the truth table of\nthe respective logical connective and by computing the truth values of the\nsubtree(s) of that root, as discussed in Chapter 1. If the root is a quantiﬁer,\nwe have sketched above how to proceed. This leaves us with the case of the\nroot node being a predicate symbol P (in propositional logic this was an\natom and we were done already). Such a predicate requires n arguments\nwhich have to be terms t1, t2, . . . , tn. Therefore, we need to be able to assign\ntruth values to formulas of the form P(t1, t2, . . . , tn).\nFor formulas P(t1, t2, . . . , tn), there is more going on than in the case of\npropositional logic. For n = 2, the predicate P could stand for something\nlike ‘the number computed by t1 is less than, or equal to, the number com-\nputed by t2.’ Therefore, we cannot just assign truth values to P directly\nwithout knowing the meaning of terms. We require a model of all function\nand predicate symbols involved. For example, terms could denote real num-\nbers and P could denote the relation ‘less than or equal to’ on the set of real\nnumbers.\nDeﬁnition 2.14 Let F be a set of function symbols and P a set of predicate\nsymbols, each symbol with a ﬁxed number of required arguments. A model\nM of the pair (F, P) consists of the following set of data:\n1.\nA non-empty set A, the universe of concrete values;\n2.\nfor each nullary function symbol f ∈F, a concrete element f M of A\n3.\nfor each f ∈F with arity n > 0, a concrete function f M : An →A from An, the\nset of n-tuples over A, to A; and\n4.\nfor each P ∈P with arity n > 0, a subset P M ⊆An of n-tuples over A.\n2.4 Semantics of predicate logic\n125\nThe distinction between f and fM and between P and P M is most im-\nportant. The symbols f and P are just that: symbols, whereas fM and",
    "set of n-tuples over A, to A; and\n4.\nfor each P ∈P with arity n > 0, a subset P M ⊆An of n-tuples over A.\n2.4 Semantics of predicate logic\n125\nThe distinction between f and fM and between P and P M is most im-\nportant. The symbols f and P are just that: symbols, whereas fM and\nP M denote a concrete function (or element) and relation in a model M,\nrespectively.\nExample 2.15 Let F\ndef\n= {i} and P\ndef\n= {R, F}; where i is a constant, F a\npredicate symbol with one argument and R a predicate symbol with two\narguments. A model M contains a set of concrete elements A – which may be\na set of states of a computer program. The interpretations iM, RM, and F M\nmay then be a designated initial state, a state transition relation, and a set\nof ﬁnal (accepting) states, respectively. For example, let A\ndef\n= {a, b, c}, iM def\n=\na, RM def\n= {(a, a), (a, b), (a, c), (b, c), (c, c)}, and F M def\n= {b, c}. We informally\ncheck some formulas of predicate logic for this model:\n1.\nThe formula\n∃y R(i, y)\nsays that there is a transition from the initial state to some state; this is true\nin our model, as there are transitions from the initial state a to a, b, and c.\n2.\nThe formula\n¬F(i)\nstates that the initial state is not a ﬁnal, accepting state. This is true in our\nmodel as b and c are the only ﬁnal states and a is the intitial one.\n3.\nThe formula\n∀x∀y∀z (R(x, y) ∧R(x, z) →y = z)\nmakes use of the equality predicate and states that the transition relation is\ndeterministic: all transitions from any state can go to at most one state (there\nmay be no transitions from a state as well). This is false in our model since\nstate a has transitions to b and c.\n4.\nThe formula\n∀x∃y R(x, y)\nstates that the model is free of states that deadlock: all states have a transition\nto some state. This is true in our model: a can move to a, b or c; and b and c\ncan move to c.\nExample 2.16 Let F\ndef\n= {e, ·} and P\ndef\n= {≤}, where e is a constant, · is a",
    "4.\nThe formula\n∀x∃y R(x, y)\nstates that the model is free of states that deadlock: all states have a transition\nto some state. This is true in our model: a can move to a, b or c; and b and c\ncan move to c.\nExample 2.16 Let F\ndef\n= {e, ·} and P\ndef\n= {≤}, where e is a constant, · is a\nfunction of two arguments and ≤is a predicate in need of two arguments as\nwell. Again, we write · and ≤in inﬁx notation as in (t1 · t2) ≤(t · t).\n126\n2 Predicate logic\nThe model M we have in mind has as set A all binary strings, ﬁnite\nwords over the alphabet {0, 1}, including the empty string denoted by ϵ. The\ninterpretation eM of e is just the empty word ϵ. The interpretation ·M of · is\nthe concatenation of words. For example, 0110 ·M 1110 equals 01101110. In\ngeneral, if a1a2 . . . ak and b1b2 . . . bn are such words with ai, bj ∈{0, 1}, then\na1a2 . . . ak ·M b1b2 . . . bn equals a1a2 . . . akb1b2 . . . bn. Finally, we interpret ≤\nas the preﬁx ordering of words. We say that s1 is a preﬁx of s2 if there is\na binary word s3 such that s1 ·M s3 equals s2. For example, 011 is a preﬁx\nof 011001 and of 011, but 010 is neither. Thus, ≤M is the set {(s1, s2) |\ns1 is a preﬁx of s2}. Here are again some informal model checks:\n1.\nIn our model, the formula\n∀x ((x ≤x · e) ∧(x · e ≤x))\nsays that every word is a preﬁx of itself concatenated with the empty word and\nconversely. Clearly, this holds in our model, for s ·M ϵ is just s and every word\nis a preﬁx of itself.\n2.\nIn our model, the formula\n∃y ∀x (y ≤x)\nsays that there exists a word s that is a preﬁx of every other word. This is true,\nfor we may chose ϵ as such a word (there is no other choice in this case).\n3.\nIn our model, the formula\n∀x ∃y (y ≤x)\nsays that every word has a preﬁx. This is clearly the case and there are in\ngeneral multiple choices for y, which are dependent on x.\n4.\nIn our model, the formula ∀x ∀y ∀z ((x ≤y) →(x · z ≤y · z)) says that when-\never a word s1 is a preﬁx of s2, then s1s has to be a preﬁx of s2s for every word",
    "says that every word has a preﬁx. This is clearly the case and there are in\ngeneral multiple choices for y, which are dependent on x.\n4.\nIn our model, the formula ∀x ∀y ∀z ((x ≤y) →(x · z ≤y · z)) says that when-\never a word s1 is a preﬁx of s2, then s1s has to be a preﬁx of s2s for every word\ns. This is clearly not the case. For example, take s1 as 01, s2 as 011 and s to\nbe 0.\n5.\nIn our model, the formula\n¬∃x ∀y ((x ≤y) →(y ≤x))\nsays that there is no word s such that whenever s is a preﬁx of some other word\ns1, it is the case that s1 is a preﬁx of s as well. This is true since there cannot\nbe such an s. Assume, for the sake of argument, that there were such a word s.\nThen s is clearly a preﬁx of s0, but s0 cannot be a preﬁx of s since s0 contains\none more bit than s.\nIt is crucial to realise that the notion of a model is extremely liberal and\nopen-ended. All it takes is to choose a non-empty set A, whose elements\n2.4 Semantics of predicate logic\n127\nmodel real-world objects, and a set of concrete functions and relations, one\nfor each function, respectively predicate, symbol. The only mild requirement\nimposed on all of this is that the concrete functions and relations on A have\nthe same number of arguments as their syntactic counterparts.\nHowever, you, as a designer or implementor of such a model, have the\nresponsibility of choosing your model wisely. Your model should be a suf-\nﬁciently accurate picture of whatever it is you want to model, but at the\nsame time it should abstract away (= ignore) aspects of the world which are\nirrelevant from the perspective of your task at hand.\nFor example, if you build a database of family relationships, then it would\nbe foolish to interpret father-of(x, y) by something like ‘x is the daughter\nof y.’ By the same token, you probably would not want to have a predicate\nfor ‘is taller than,’ since your focus in this model is merely on relationships\ndeﬁned by birth. Of course, there are circumstances in which you may want",
    "of y.’ By the same token, you probably would not want to have a predicate\nfor ‘is taller than,’ since your focus in this model is merely on relationships\ndeﬁned by birth. Of course, there are circumstances in which you may want\nto add additional features to your database.\nGiven a model M for a pair (F, P) of function and predicate symbols,\nwe are now almost in a position to formally compute a truth value for all\nformulas in predicate logic which involve only function and predicate sym-\nbols from (F, P). There is still one thing, though, that we need to discuss.\nGiven a formula ∀x φ or ∃x φ, we intend to check whether φ holds for all,\nrespectively some, value a in our model. While this is intuitive, we have no\nway of expressing this in our syntax: the formula φ usually has x as a free\nvariable; φ[a/x] is well-intended, but ill-formed since φ[a/x] is not a logical\nformula, for a is not a term but an element of our model.\nTherefore we are forced to interpret formulas relative to an environment.\nYou may think of environments in a variety of ways. Essentially, they are\nlook-up tables for all variables; such a table l associates with every variable\nx a value l(x) of the model. So you can also say that environments are\nfunctions l: var →A from the set of variables var to the universe of values\nA of the underlying model. Given such a look-up table, we can assign truth\nvalues to all formulas. However, for some of these computations we need\nupdated look-up tables.\nDeﬁnition 2.17 A look-up table or environment for a universe A of con-\ncrete values is a function l: var →A from the set of variables var to A. For\nsuch an l, we denote by l[x \u000f→a] the look-up table which maps x to a and\nany other variable y to l(y).\nFinally, we are able to give a semantics to formulas of predicate logic. For\npropositional logic, we did this by computing a truth value. Clearly, it suﬃces\nto know in which cases this value is T.\n128\n2 Predicate logic",
    "any other variable y to l(y).\nFinally, we are able to give a semantics to formulas of predicate logic. For\npropositional logic, we did this by computing a truth value. Clearly, it suﬃces\nto know in which cases this value is T.\n128\n2 Predicate logic\nDeﬁnition 2.18 Given a model M for a pair (F, P) and given an environ-\nment l, we deﬁne the satisfaction relation M ⊨l φ for each logical formula\nφ over the pair (F, P) and look-up table l by structural induction on φ. If\nM ⊨l φ holds, we say that φ computes to T in the model M with respect to\nthe environment l.\nP:\nIf φ is of the form P(t1, t2, . . . , tn), then we interpret the terms t1, t2, . . . , tn in\nour set A by replacing all variables with their values according to l. In this way\nwe compute concrete values a1, a2, . . . , an of A for each of these terms, where\nwe interpret any function symbol f ∈F by f M. Now M ⊨l P(t1, t2, . . . , tn)\nholds iﬀ(a1, a2, . . . , an) is in the set P M.\n∀x:\nThe relation M ⊨l ∀x ψ holds iﬀM ⊨l[x\u0005→a] ψ holds for all a ∈A.\n∃x:\nDually, M ⊨l ∃x ψ holds iﬀM ⊨l[x\u0005→a] ψ holds for some a ∈A.\n¬:\nThe relation M ⊨l ¬ψ holds iﬀit is not the case that M ⊨l ψ holds.\n∨:\nThe relation M ⊨l ψ1 ∨ψ2 holds iﬀM ⊨l ψ1 or M ⊨l ψ2 holds.\n∧:\nThe relation M ⊨l ψ1 ∧ψ2 holds iﬀM ⊨l ψ1 and M ⊨l ψ2 hold.\n→:\nThe relation M ⊨l ψ1 →ψ2 holds iﬀM ⊨l ψ2 holds whenever M ⊨l ψ1 holds.\nWe sometimes write M ̸⊨l φ to denote that M ⊨l φ does not hold.\nThere is a straightforward inductive argument on the height of the parse\ntree of a formula which says that M ⊨l φ holds iﬀM ⊨l′ φ holds, whenever\nl and l′ are two environments which are identical on the set of free variables\nof φ. In particular, if φ has no free variables at all, we then call φ a sentence;\nwe conclude that M ⊨l φ holds, or does not hold, regardless of the choice of\nl. Thus, for sentences φ we often elide l and write M ⊨φ since the choice of\nan environment l is then irrelevant.\nExample 2.19 Let us illustrate the deﬁnitions above by means of an-",
    "we conclude that M ⊨l φ holds, or does not hold, regardless of the choice of\nl. Thus, for sentences φ we often elide l and write M ⊨φ since the choice of\nan environment l is then irrelevant.\nExample 2.19 Let us illustrate the deﬁnitions above by means of an-\nother simple example. Let F\ndef\n= {alma} and P\ndef\n= {loves} where alma is a\nconstant and loves a predicate with two arguments. The model M we\nchoose here consists of the privacy-respecting set A\ndef\n= {a, b, c}, the constant\nfunction almaM def\n= a and the predicate lovesM def\n= {(a, a), (b, a), (c, a)}, which\nhas two arguments as required. We want to check whether the model M\nsatisﬁes\nNone of Alma’s lovers’ lovers love her.\nFirst, we need to express the, morally worrying, sentence in predicate logic.\nHere is such an encoding (as we already discussed, diﬀerent but logically\nequivalent encodings are possible):\n∀x ∀y (loves(x, alma) ∧loves(y, x) →¬loves(y, alma)) .\n(2.8)\n2.4 Semantics of predicate logic\n129\nDoes the model M satisfy this formula? Well, it does not; for we may choose\na for x and b for y. Since (a, a) is in the set lovesM and (b, a) is in the\nset lovesM, we would need that the latter does not hold since it is the\ninterpretation of loves(y, alma); this cannot be.\nAnd what changes if we modify M to M′, where we keep A and almaM,\nbut redeﬁne the interpretation of loves as lovesM′ def\n= {(b, a), (c, b)}? Well,\nnow there is exactly one lover of Alma’s lovers, namely c; but c is not one\nof Alma’s lovers. Thus, the formula in (2.8) holds in the model M′.\n2.4.2 Semantic entailment\nIn propositional logic, the semantic entailment φ1, φ2, . . . , φn ⊨ψ holds iﬀ:\nwhenever all φ1, φ2, . . . , φn evaluate to T, the formula ψ evaluates to T as well.\nHow can we deﬁne such a notion for formulas in predicate logic, considering\nthat M ⊨l φ is indexed with an environment?\nDeﬁnition 2.20 Let Γ be a (possibly inﬁnite) set of formulas in predicate\nlogic and ψ a formula of predicate logic.\n1.",
    "How can we deﬁne such a notion for formulas in predicate logic, considering\nthat M ⊨l φ is indexed with an environment?\nDeﬁnition 2.20 Let Γ be a (possibly inﬁnite) set of formulas in predicate\nlogic and ψ a formula of predicate logic.\n1.\nSemantic entailment Γ ⊨ψ holds iﬀfor all models M and look-up tables l,\nwhenever M ⊨l φ holds for all φ ∈Γ, then M ⊨l ψ holds as well.\n2.\nFormula ψ is satisﬁable iﬀthere is some model M and some environment l such\nthat M ⊨l ψ holds.\n3.\nFormula ψ is valid iﬀM ⊨l ψ holds for all models M and environments l in\nwhich we can check ψ.\n4.\nThe set Γ is consistent or satisﬁable iﬀthere is a model M and a look-up table\nl such that M ⊨l φ holds for all φ ∈Γ.\nIn predicate logic, the symbol ⊨is overloaded: it denotes model checks ‘M ⊨\nφ’ and semantic entailment ‘φ1, φ2, . . . , φn ⊨ψ.’ Computationally, each of\nthese notions means trouble. First, establishing M ⊨φ will cause problems,\nif done on a machine, as soon as the universe of values A of M is inﬁnite.\nIn that case, checking the sentence ∀x ψ, where x is free in ψ, amounts to\nverifying M ⊨[x\u0006→a] ψ for inﬁnitely many elements a.\nSecond, and much more seriously, in trying to verify that φ1, φ2, . . . , φn ⊨\nψ holds, we have to check things out for all possible models, all models which\nare equipped with the right structure (i.e. they have functions and predicates\nwith the matching number of arguments). This task is impossible to perform\nmechanically. This should be contrasted to the situation in propositional\nlogic, where the computation of the truth tables for the propositions involved\nwas the basis for computing this relationship successfully.\n130\n2 Predicate logic\nHowever, we can sometimes reason that certain semantic entailments are\nvalid. We do this by providing an argument that does not depend on the\nactual model at hand. Of course, this works only for a very limited number\nof cases. The most prominent ones are the quantiﬁer equivalences which we",
    "However, we can sometimes reason that certain semantic entailments are\nvalid. We do this by providing an argument that does not depend on the\nactual model at hand. Of course, this works only for a very limited number\nof cases. The most prominent ones are the quantiﬁer equivalences which we\nalready encountered in the section on natural deduction. Let us look at a\ncouple of examples of semantic entailment.\nExample 2.21 The justiﬁcation of the semantic entailment\n∀x (P(x) →Q(x)) ⊨∀x P(x) →∀x Q(x)\nis as follows. Let M be a model satisfying ∀x (P(x) →Q(x)). We need to\nshow that M satisﬁes ∀x P(x) →∀x Q(x) as well. On inspecting the deﬁni-\ntion of M ⊨ψ1 →ψ2, we see that we are done if not every element of our\nmodel satisﬁes P. Otherwise, every element does satisfy P. But since M\nsatisﬁes ∀x (P(x) →Q(x)), the latter fact forces every element of our model\nto satisfy Q as well. By combining these two cases (i.e. either all elements of\nM satisfy P, or not) we have shown that M satisﬁes ∀x P(x) →∀x Q(x).\nWhat about the converse of the above? Is\n∀x P(x) →∀x Q(x) ⊨∀x (P(x) →Q(x))\nvalid as well? Hardly! Suppose that M′ is a model satisfying ∀x P(x) →\n∀x Q(x). If A′ is its underlying set and P M′ and QM′ are the corresponding\ninterpretations of P and Q, then M′ ⊨∀x P(x) →∀x Q(x) simply says that,\nif P M′ equals A′, then QM′ must equal A′ as well. However, if P M′ does not\nequal A′, then this implication is vacuously true (remember that F →· = T\nno matter what · actually is). In this case we do not get any additional\nconstraints on our model M′. After these observations, it is now easy to\nconstruct a counter-example model. Let A′ def\n= {a, b}, P M′ def\n= {a} and QM′ def\n=\n{b}. Then M′ ⊨∀x P(x) →∀x Q(x) holds, but M′ ⊨∀x (P(x) →Q(x)) does\nnot.\n2.4.3 The semantics of equality\nWe have already pointed out the open-ended nature of the semantics of\npredicate logic. Given a predicate logic over a set of function symbols F and",
    "= {a, b}, P M′ def\n= {a} and QM′ def\n=\n{b}. Then M′ ⊨∀x P(x) →∀x Q(x) holds, but M′ ⊨∀x (P(x) →Q(x)) does\nnot.\n2.4.3 The semantics of equality\nWe have already pointed out the open-ended nature of the semantics of\npredicate logic. Given a predicate logic over a set of function symbols F and\na set of predicate symbols P, we need only a non-empty set A equipped with\nconcrete functions or elements fM (for f ∈F) and concrete predicates P M\n(for P ∈P) in A which have the right arities agreed upon in our speciﬁcation.\nOf course, we also stressed that most models have natural interpretations of\n2.5 Undecidability of predicate logic\n131\nfunctions and predicates, but central notions like that of semantic entailment\n(φ1, φ2, . . . , φn ⊨ψ) really depend on all possible models, even the ones that\ndon’t seem to make any sense.\nApparently there is no way out of this peculiarity. For example, where\nwould you draw the line between a model that makes sense and one that\ndoesn’t? And would any such choice, or set of criteria, not be subjective? Such\nconstraints could also forbid a modiﬁcation of your model if this alteration\nwere caused by a slight adjustment of the problem domain you intended to\nmodel. You see that there are a lot of good reasons for maintaining such a\nliberal stance towards the notion of models in predicate logic.\nHowever, there is one famous exception. Often one presents predicate logic\nsuch that there is always a special predicate = available to denote equality\n(recall Section 2.3.1); it has two arguments and t1 = t2 has the intended\nmeaning that the terms t1 and t2 compute the same thing. We discussed its\nproof rule in natural deduction already in Section 2.3.1.\nSemantically, one recognises the special role of equality by imposing on\nan interpretation function =M to be actual equality on the set A of M.\nThus, (a, b) is in the set =M iﬀa and b are the same elements in the set A.\nFor example, given A\ndef\n= {a, b, c}, the interpretation =M of equality is forced",
    "an interpretation function =M to be actual equality on the set A of M.\nThus, (a, b) is in the set =M iﬀa and b are the same elements in the set A.\nFor example, given A\ndef\n= {a, b, c}, the interpretation =M of equality is forced\nto be {(a, a), (b, b), (c, c)}. Hence the semantics of equality is easy, for it is\nalways modelled extensionally.\n2.5 Undecidability of predicate logic\nWe continue our introduction to predicate logic with some negative results.\nGiven a formula φ in propositional logic we can, at least in principle, de-\ntermine whether ⊨φ holds: if φ has n propositional atoms, then the truth\ntable of φ contains 2n lines; and ⊨φ holds if, and only if, the column for φ\n(of length 2n) contains only T entries.\nThe bad news is that such a mechanical procedure, working for all for-\nmulas φ, cannot be provided in predicate logic. We will give a formal proof\nof this negative result, though we rely on an informal (yet intuitive) notion\nof computability.\nThe problem of determining whether a predicate logic formula is valid is\nknown as a decision problem. A solution to a decision problem is a program\n(written in Java, C, or any other common language) that takes problem\ninstances as input and always terminates, producing a correct ‘yes’ or ‘no’\noutput. In the case of the decision problem for predicate logic, the input to\nthe program is an arbitrary formula φ of predicate logic and the program\n132\n2 Predicate logic\nis correct if it produces ‘yes’ whenever the input formula is valid and ‘no’\nwhenever it is not. Note that the program which solves a decision problem\nmust terminate for all well-formed input: a program which goes on thinking\nabout it for ever is not allowed. The decision problem at hand is this:\nValidity in predicate logic.\nGiven a logical formula φ in predicate logic, does\n⊨φ hold, yes or no?\nWe now show that this problem is not solvable; we cannot write a correct\nC or Java program that works for all φ. It is important to be clear about",
    "Validity in predicate logic.\nGiven a logical formula φ in predicate logic, does\n⊨φ hold, yes or no?\nWe now show that this problem is not solvable; we cannot write a correct\nC or Java program that works for all φ. It is important to be clear about\nexactly what we are stating. Naturally, there are some φ which can easily be\nseen to be valid; and others which can easily be seen to be invalid. However,\nthere are also some φ for which it is not easy. Every φ can, in principle, be\ndiscovered to be valid or not, if you are prepared to work arbitrarily hard at\nit; but there is no uniform mechanical procedure for determining whether φ\nis valid which will work for all φ.\nWe prove this by a well-known technique called problem reduction. That\nis, we take some other problem, of which we already know that it is not\nsolvable, and we then show that the solvability of our problem entails the\nsolvability of the other one. This is a beautiful application of the proof rules\n¬i and ¬e, since we can then infer that our own problem cannot be solvable\nas well.\nThe problem that is known not to be solvable, the Post correspondence\nproblem, is interesting in its own right and, upon ﬁrst reﬂection, does not\nseem to have a lot to do with predicate logic.\nThe Post correspondence problem.\nGiven a ﬁnite sequence of pairs\n(s1, t1), (s2, t2), . . . , (sk, tk) such that all si and ti are binary strings of pos-\nitive length, is there a sequence of indices i1, i2, . . . , in with n ≥1 such that\nthe concatenation of strings si1si2 . . . sin equals ti1ti2 . . . tin?\nHere is an instance of the problem which we can solve successfully: the\nconcrete correspondence problem instance C is given by a sequence of three\npairs C\ndef\n= ((1, 101), (10, 00), (011, 11)) so\ns1\ndef\n= 1\ns2\ndef\n= 10\ns3\ndef\n= 011\nt1\ndef\n= 101\nt2\ndef\n= 00\nt3\ndef\n= 11.\nA solution to the problem is the sequence of indices (1, 3, 2, 3) since s1s3s2s3\nand t1t3t2t3 both equal 101110011. Maybe you think that this problem must",
    "pairs C\ndef\n= ((1, 101), (10, 00), (011, 11)) so\ns1\ndef\n= 1\ns2\ndef\n= 10\ns3\ndef\n= 011\nt1\ndef\n= 101\nt2\ndef\n= 00\nt3\ndef\n= 11.\nA solution to the problem is the sequence of indices (1, 3, 2, 3) since s1s3s2s3\nand t1t3t2t3 both equal 101110011. Maybe you think that this problem must\nsurely be solvable; but remember that a computational solution would have\n2.5 Undecidability of predicate logic\n133\nto be a program that solves all such problem instances. Things get a bit\ntougher already if we look at this (solvable) problem:\ns1\ndef\n= 001\ns2\ndef\n= 01\ns3\ndef\n= 01\ns4\ndef\n= 10\nt1\ndef\n= 0\nt2\ndef\n= 011\nt3\ndef\n= 101\nt4\ndef\n= 001\nwhich you are invited to solve by hand, or by writing a program for this\nspeciﬁc instance.\nNote that the same number can occur in the sequence of indices, as hap-\npened in the ﬁrst example in which 3 occurs twice. This means that the\nsearch space we are dealing with is inﬁnite, which should give us some indi-\ncation that the problem is unsolvable. However, we do not formally prove it\nin this book. The proof of the following theorem is due to the mathematician\nA. Church.\nTheorem 2.22 The decision problem of validity in predicate logic is unde-\ncidable: no program exists which, given any φ, decides whether ⊨φ.\nPROOF: As said before, we pretend that validity is decidable for predicate\nlogic and thereby solve the (insoluble) Post correspondence problem. Given\na correspondence problem instance C:\ns1 s2 . . . sk\nt1 t2 . . . tk\nwe need to be able to construct, within ﬁnite space and time and uniformly\nso for all instances, some formula φ of predicate logic such that ⊨φ holds\niﬀthe correspondence problem instance C above has a solution.\nAs function symbols, we choose a constant e and two function symbols\nf0 and f1 each of which requires one argument. We think of e as the empty\nstring, or word, and f0 and f1 symbolically stand for concatenation with 0,\nrespectively 1. So if b1b2 . . . bl is a binary string of bits, we can code that up",
    "f0 and f1 each of which requires one argument. We think of e as the empty\nstring, or word, and f0 and f1 symbolically stand for concatenation with 0,\nrespectively 1. So if b1b2 . . . bl is a binary string of bits, we can code that up\nas the term fbl(fbl−1 . . . (fb2(fb1(e))) . . . ). Note that this coding spells that\nword backwards. To facilitate reading those formulas, we abbreviate terms\nlike fbl(fbl−1 . . . (fb2(fb1(t))) . . . ) by fb1b2...bl(t).\nWe also require a predicate symbol P which expects two arguments.\nThe intended meaning of P(s, t) is that there is some sequence of indices\n(i1, i2, . . . , im) such that s is the term representing si1si2 . . . sim and t rep-\nresents ti1ti2 . . . tim. Thus, s constructs a string using the same sequence of\nindices as does t; only s uses the si whereas t uses the ti.\n134\n2 Predicate logic\nOur sentence φ has the coarse structure φ1 ∧φ2 →φ3 where we set\nφ1\ndef\n=\nk\u0001\ni=1\nP(fsi(e), fti(e))\nφ2\ndef\n= ∀v ∀w\n\u0002\nP(v, w) →\nk\u0001\ni=1\nP(fsi(v), fti(w))\n\u0003\nφ3\ndef\n= ∃z P(z, z) .\nOur claim is ⊨φ holds iﬀthe Post correspondence problem C has a solution.\nFirst, let us assume that ⊨φ holds. Our strategy is to ﬁnd a model for\nφ which tells us there is a solution to the correspondence problem C simply\nby inspecting what it means for φ to satisfy that particular model. The\nuniverse of concrete values A of that model is the set of all ﬁnite, binary\nstrings (including the empty string denoted by ϵ).\nThe interpretation eM of the constant e is just that empty string ϵ. The\ninterpretation of f0 is the unary function fM\n0\nwhich appends a 0 to a given\nstring, fM\n0 (s)\ndef\n= s0; similarly, fM\n1 (s)\ndef\n= s1 appends a 1 to a given string.\nThe interpretation of P on M is just what we expect it to be:\nP M def\n= {(s, t) | there is a sequence of indices (i1, i2, . . . , im) such that\ns equals si1si2 . . . sim and t equals ti1ti2 . . . tim}\nwhere s and t are binary strings and the si and ti are the data of the",
    "The interpretation of P on M is just what we expect it to be:\nP M def\n= {(s, t) | there is a sequence of indices (i1, i2, . . . , im) such that\ns equals si1si2 . . . sim and t equals ti1ti2 . . . tim}\nwhere s and t are binary strings and the si and ti are the data of the\ncorrespondence problem C. A pair of strings (s, t) lies in P M iﬀ, using the\nsame sequence of indices (i1, i2, . . . , im), s is built using the corresponding\nsi and t is built using the respective ti.\nSince ⊨φ holds we infer that M ⊨φ holds, too. We claim that M ⊨\nφ2 holds as well, which says that whenever the pair (s, t) is in P M, then\nthe pair (s si, t ti) is also in P M for i = 1, 2, . . . , k (you can verify that is\nsays this by inspecting the deﬁnition of P M). Now (s, t) ∈P M implies that\nthere is some sequence (i1, i2, . . . , im) such that s equals si1si2 . . . sim and t\nequals ti1ti2 . . . tim. We simply choose the new sequence (i1, i2, . . . , im, i) and\nobserve that s si equals si1si2 . . . simsi and t ti equals ti1ti2 . . . timti and so\nM ⊨φ2 holds as claimed. (Why does M ⊨φ1 hold?)\nSince M ⊨φ1 ∧φ2 →φ3 and M ⊨φ1 ∧φ2 hold, it follows that M ⊨φ3\nholds as well. By deﬁnition of φ3 and P M, this tells us there is a solution\nto C.\nConversely, let us assume that the Post correspondence problem C has\nsome solution, namely the sequence of indices (i1, i2, . . . , in). Now we have to\nshow that, if M′ is any model having a constant eM′, two unary functions,\n2.5 Undecidability of predicate logic\n135\nfM′\n0\nand fM′\n1\n, and a binary predicate P M′, then that model has to satisfy\nφ. Notice that the root of the parse tree of φ is an implication, so this is\nthe crucial clause for the deﬁnition of M′ ⊨φ. By that very deﬁnition, we\nare already done if M′ ̸⊨φ1, or if M′ ̸⊨φ2. The harder part is therefore the\none where M′ ⊨φ1 ∧φ2, for in that case we need to verify M′ ⊨φ3 as well.\nThe way we proceed here is by interpreting ﬁnite, binary strings in the",
    "the crucial clause for the deﬁnition of M′ ⊨φ. By that very deﬁnition, we\nare already done if M′ ̸⊨φ1, or if M′ ̸⊨φ2. The harder part is therefore the\none where M′ ⊨φ1 ∧φ2, for in that case we need to verify M′ ⊨φ3 as well.\nThe way we proceed here is by interpreting ﬁnite, binary strings in the\ndomain of values A′ of the model M′. This is not unlike the coding of an\ninterpreter for one programming language in another. The interpretation is\ndone by a function interpret which is deﬁned inductively on the data structure\nof ﬁnite, binary strings:\ninterpret(ϵ)\ndef\n= eM′\ninterpret(s0)\ndef\n= fM′\n0\n(interpret(s))\ninterpret(s1)\ndef\n= fM′\n1\n(interpret(s)) .\nNote that interpret(s) is deﬁned inductively on the length of s. This interpre-\ntation is, like the coding above, backwards; for example, the string 0100110\ngets interpreted as fM′\n0\n(fM′\n1\n(fM′\n1\n(fM′\n0\n(fM′\n0\n(fM′\n1\n(fM′\n0\n(eM′))))))). Note that\ninterpret(b1b2 . . . bl) = fM′\nbl (fM′\nbl−1(. . . (fb1(eM′) . . . ))) is just the meaning of\nfs(e) in A′, where s equals b1b2 . . . bl. Using that and the fact that M′ ⊨φ1,\nwe conclude that (interpret(si), interpret(ti)) ∈P M′ for i = 1, 2, . . . , k. Sim-\nilarly, since M′ ⊨φ2, we know that for all (s, t) ∈P M′ we have that\n(interpret(ssi), interpret(tti)) ∈P M′ for i = 1, 2, . . . , k. Using these two facts,\nstarting with (s, t) = (si1, ti1), we repeatedly use the latter observation to\nobtain\n(interpret(si1si2 . . . sin), interpret(ti1ti2 . . . tin)) ∈P M′.\n(2.9)\nSince si1si2 . . . sin and ti1ti2 . . . tin together form a solution of C, they are\nequal; and therefore interpret(si1si2 . . . sin) and interpret(ti1ti2 . . . tin) are the\nsame elements in A′, for interpreting the same thing gets you the same result.\nHence (2.9) veriﬁes ∃z P(z, z) in M′ and thus M′ ⊨φ3.\n2\nThere are two more negative results which we now get quite easily. Recall\nthat a formula φ is satisﬁable if there is some model M and some environ-",
    "same elements in A′, for interpreting the same thing gets you the same result.\nHence (2.9) veriﬁes ∃z P(z, z) in M′ and thus M′ ⊨φ3.\n2\nThere are two more negative results which we now get quite easily. Recall\nthat a formula φ is satisﬁable if there is some model M and some environ-\nment l such that M ⊨l φ holds. This property is not to be taken for granted;\nthe formula ∃x (P(x) ∧¬P(x)) is clearly unsatisﬁable. More interesting is\nthe observation that φ is unsatisﬁable if, and only if, ¬φ is valid, i.e. holds\nin all models. This is an immediate consequence of the deﬁnitional clause\nM ⊨l ¬φ for negation. Since we can’t compute validity, it follows that we\ncannot compute satisﬁability either.\n136\n2 Predicate logic\nThe other undecidability result comes from the soundness and complete-\nness of predicate logic which, in special form for sentences, reads as\n⊨φ iﬀ\n⊢φ\n(2.10)\nwhich we do not prove in this text. Since we can’t decide validity, we cannot\ndecide provability either, on the basis of (2.10). One might reﬂect on that\nlast negative result a bit. It means bad news if one wants to implement\nperfect theorem provers which can mechanically produce a proof of a given\nformula, or refute it. It means good news, though, if we like the thought\nthat machines still need a little bit of human help. Creativity seems to have\nlimits if we leave it to machines alone.\n2.6 Expressiveness of predicate logic\nPredicate logic is much more expressive than propositional logic, having\npredicate and function symbols, as well as quantiﬁers. This expressivess\ncomes at the cost of making validity, satisﬁability and provability undecid-\nable. The good news, though, is that checking formulas on models is practi-\ncal; SQL queries over relational databases or XQueries over XML documents\nare examples of this in practice.\nSoftware models, design standards, and execution models of hardware or\nprograms often are described in terms of directed graphs. Such models M",
    "cal; SQL queries over relational databases or XQueries over XML documents\nare examples of this in practice.\nSoftware models, design standards, and execution models of hardware or\nprograms often are described in terms of directed graphs. Such models M\nare interpretations of a two-argument predicate symbol R over a concrete\nset A of ‘states.’\nExample 2.23 Given a set of states A = {s0, s1, s2, s3}, let RM be the\nset {(s0, s1), (s1, s0), (s1, s1), (s1, s2), (s2, s0), (s3, s0), (s3, s2)}. We may de-\npict this model as a directed graph in Figure 2.5, where an edge (a transi-\ntion) leads from a node s to a node s′ iﬀ(s, s′) ∈RM. In that case, we often\ndenote this as s →s′.\nThe validation of many applications requires to show that a ‘bad’ state\ncannot be reached from a ‘good’ state. What ‘good’ and ‘bad’ mean will\ndepend on the context. For example, a good state may be one in which an\ninteger expression, say x ∗(y −1), evaluates to a value that serves as a safe\nindex into an array a of length 10. A bad state would then be one in which\nthis integer expression evaluates to an unsafe value, say 11, causing an ‘out-\nof-bounds exception.’ In its essence, deciding whether from a good state one\ncan reach a bad state is the reachability problem in directed graphs.\n2.6 Expressiveness of predicate logic\n137\ns1\ns0\ns3\ns2\nFigure 2.5. A directed graph, which is a model M for a predicate sym-\nbol R with two arguments. A pair of nodes (n, n′) is in the interpretation\nRM of R iff there is a transition (an edge) from node n to node n′ in\nthat graph.\nReachability: Given nodes n and n′ in a directed graph, is there a ﬁnite\npath of transitions from n to n′?\nIn Figure 2.5, state s2 is reachable from state s0, e.g. through the path\ns0 →s1 →s2. By convention, every state reaches itself by a path of length\n0. State s3, however, is not reachable from s0; only states s0, s1, and s2\nare reachable from s0. Given the evident importance of this concept, can",
    "In Figure 2.5, state s2 is reachable from state s0, e.g. through the path\ns0 →s1 →s2. By convention, every state reaches itself by a path of length\n0. State s3, however, is not reachable from s0; only states s0, s1, and s2\nare reachable from s0. Given the evident importance of this concept, can\nwe express reachability in predicate logic – which is, after all, so expressive\nthat it is undecidable? To put this question more precisely: can we ﬁnd a\npredicate-logic formula φ with u and v as its only free variables and R as\nits only predicate symbol (of arity 2) such that φ holds in directed graphs\niﬀthere is a path in that graph from the node associated to u to the node\nassociated to v? For example, we might try to write:\nu = v ∨∃x(R(u, x) ∧R(x, v)) ∨∃x1∃x2(R(u, x1) ∧R(x1, x2) ∧R(x2, v)) ∨. . .\nThis is inﬁnite, so it’s not a well-formed formula. The question is: can we\nﬁnd a well-formed formula with the same meaning?\nSurprisingly, this is not the case. To show this we need to record an im-\nportant consequence of the completeness of natural deduction for predicate\nlogic.\nTheorem 2.24 (Compactness Theorem) Let Γ be a set of sentences of\npredicate logic. If all ﬁnite subsets of Γ are satisﬁable, then so is Γ.\nPROOF:\nWe use proof by contradiction: Assume that Γ is not satisﬁable.\nThen the semantic entailment Γ ⊨⊥holds as there is no model in which\nall φ ∈Γ are true. By completeness, this means that the sequent Γ ⊢⊥\nis valid. (Note that this uses a slightly more general notion of sequent in\nwhich we may have inﬁnitely many premises at our disposal. Soundness and\n138\n2 Predicate logic\ncompleteness remain true for that reading.) Thus, this sequent has a proof\nin natural deduction; this proof – being a ﬁnite piece of text – can use\nonly ﬁnitely many premises ∆from Γ. But then ∆⊢⊥is valid, too, and\nso ∆⊨⊥follows by soundness. But the latter contradicts the fact that all\nﬁnite subsets of Γ are consistent.\n2\nFrom this theorem one may derive a number of useful techniques. We men-",
    "only ﬁnitely many premises ∆from Γ. But then ∆⊢⊥is valid, too, and\nso ∆⊨⊥follows by soundness. But the latter contradicts the fact that all\nﬁnite subsets of Γ are consistent.\n2\nFrom this theorem one may derive a number of useful techniques. We men-\ntion a technique for ensuring the existence of models of inﬁnite size.\nTheorem 2.25 (L¨owenheim-Skolem Theorem) Let ψ be a sentence of\npredicate logic such for any natural number n ≥1 there is a model of ψ with\nat least n elements. Then ψ has a model with inﬁnitely many elements.\nPROOF:\nThe formula φn\ndef\n= ∃x1∃x2 . . . ∃xn\n\u0004\n1≤i<j≤n ¬(xi = xj) speciﬁes\nthat there are at least n elements. Consider the set of sentences Γ\ndef\n=\n{ψ} ∪{φn | n ≥1} and let ∆be any if its ﬁnite subsets. Let k ≥1 be such\nthat n ≤k for all n with φn ∈∆. Since the latter set is ﬁnite, such a k has to\nexist. By assumption, {ψ, φk} is satisﬁable; but φk →φn is valid for all n ≤k\n(why?). Therefore, ∆is satisﬁable as well. The compactness theorem then\nimplies that Γ is satisﬁable by some model M; in particular, M ⊨ψ holds.\nSince M satisﬁes φn for all n ≥1, it cannot have ﬁnitely many elements. 2\nWe can now show that reachability is not expressible in predicate logic.\nTheorem 2.26 Reachability is not expressible in predicate logic: there is\nno predicate-logic formula φ with u and v as its only free variables and R as\nits only predicate symbol (of arity 2) such that φ holds in directed graphs\niﬀthere is a path in that graph from the node associated to u to the node\nassociated to v.\nPROOF:\nSuppose there is a formula φ expressing the existence of a path\nfrom the node associated to u to the node associated to v. Let c and c′ be\nconstants. Let φn be the formula expressing that there is a path of length n\nfrom c to c′: we deﬁne φ0 as c = c′, φ1 as R(c, c′) and, for n > 1,\nφn\ndef\n= ∃x1 . . . ∃xn−1(R(c, x1) ∧R(x1, x2) ∧· · · ∧R(xn−1, c′)).\nLet ∆= {¬φi | i ≥0} ∪{φ[c/u][c′/v]}. All formulas in ∆are sentences and",
    "constants. Let φn be the formula expressing that there is a path of length n\nfrom c to c′: we deﬁne φ0 as c = c′, φ1 as R(c, c′) and, for n > 1,\nφn\ndef\n= ∃x1 . . . ∃xn−1(R(c, x1) ∧R(x1, x2) ∧· · · ∧R(xn−1, c′)).\nLet ∆= {¬φi | i ≥0} ∪{φ[c/u][c′/v]}. All formulas in ∆are sentences and\n∆is unsatisﬁable, since the ‘conjunction’ of all sentences in ∆says that\nthere is no path of length 0, no path of length 1, etc. from the node denoted\nby c to the node denoted by c′, but there is a ﬁnite path from c to c′ as\nφ[c/u][c′/v] is true.\n2.6 Expressiveness of predicate logic\n139\nHowever, every ﬁnite subset of ∆is satisﬁable since there are paths of any\nﬁnite length. Therefore, by the Compactness Theorem, ∆itself is satisﬁable.\nThis is a contradiction. Therefore, there cannot be such a formula φ.\n2\n2.6.1 Existential second-order logic\nIf predicate logic cannot express reachability in graphs, then what can, and\nat what cost? We seek an extension of predicate logic that can specify such\nimportant properties, rather than inventing an entirely new syntax, seman-\ntics and proof theory from scratch. This can be realized by applying quan-\ntiﬁers not only to variables, but also to predicate symbols. For a predicate\nsymbol P with n ≥1 arguments, consider formulas of the form\n∃P φ\n(2.11)\nwhere φ is a formula of predicate logic in which P occurs. Formulas of that\nform are the ones of existential second-order logic. An example of arity 2 is\n∃P ∀x∀y∀z (C1 ∧C2 ∧C3 ∧C4)\n(2.12)\nwhere each Ci is a Horn clause4\nC1\ndef\n= P(x, x)\nC2\ndef\n= P(x, y) ∧P(y, z) →P(x, z)\nC3\ndef\n= P(u, v) →⊥\nC4\ndef\n= R(x, y) →P(x, y).\nIf we think of R and P as two transition relations on a set of states, then\nC4 says that any R-edge is also a P-edge, C1 states that P is reﬂexive, C2\nspeciﬁes that P is transitive, and C3 ensures that there is no P-path from\nthe node associated to u to the node associated to v.\nGiven a model M with interpretations for all function and predicate sym-",
    "C4 says that any R-edge is also a P-edge, C1 states that P is reﬂexive, C2\nspeciﬁes that P is transitive, and C3 ensures that there is no P-path from\nthe node associated to u to the node associated to v.\nGiven a model M with interpretations for all function and predicate sym-\nbols of φ in (2.11), except P, let MT be that same model augmented with\nan interpretation T ⊆A × A of P, i.e. P MT = T. For any look-up table l,\nthe semantics of ∃P φ is then\nM ⊨l ∃P φ\niﬀ\nfor some T ⊆A × A, MT ⊨l φ.\n(2.13)\n4 Meaning, a Horn clause after all atomic subformulas are replaced with propositional atoms.\n140\n2 Predicate logic\nExample 2.27 Let ∃P φ be the formula in (2.12) and consider the model\nM of Example 2.23 and Figure 2.5. Let l be a look-up table with l(u) = s0\nand l(v) = s3. Does M ⊨l ∃P φ hold? For that, we need an interpretation\nT ⊆A × A of P such that MT ⊨l ∀x∀y∀x (C1 ∧C2 ∧C3 ∧C4) holds. That\nis, we need to ﬁnd a reﬂexive and transitive relation T ⊆A × A that con-\ntains RM but not (s0, s3). Please verify that T\ndef\n= {(s, s′) ∈A × A | s′ ̸= s3}\n∪{(s3, s3)} is such a T. Therefore, M ⊨l ∃P φ holds.\nIn the exercises you are asked to show that the formula in (2.12) holds in\na directed graph iﬀthere isn’t a ﬁnite path from node l(u) to node l(v) in\nthat graph. Therefore, this formula speciﬁes unreachability.\n2.6.2 Universal second-order logic\nOf course, we can negate (2.12) and obtain\n∀P ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)\n(2.14)\nby relying on the familiar de Morgan laws. This is a formula of universal\nsecond-order logic. This formula expresses reachability.\nTheorem 2.28 Let M = (A, RM) be any model. Then the formula\nin (2.14) holds under look-up table l in M iﬀl(v) is R-reachable from l(u)\nin M.\nPROOF:\n1.\nFirst, assume that MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds for all inter-\npretations T of P. Then it also holds for the interpretation which is the re-\nﬂexive, transitive closure of RM. But for that T, MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨",
    "in M.\nPROOF:\n1.\nFirst, assume that MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds for all inter-\npretations T of P. Then it also holds for the interpretation which is the re-\nﬂexive, transitive closure of RM. But for that T, MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨\n¬C3 ∨¬C4) can hold only if MT ⊨l ¬C3 holds, as all other clauses Ci (i ̸= 3)\nare false. But this means that MT ⊨l P(u, v) has to hold. So (l(u), l(v)) ∈T\nfollows, meaning that there is a ﬁnite path from l(u) to l(v).\n2.\nConversely, let l(v) be R-reachable from l(u) in M.\n– For any interpretation T of P which is not reﬂexive, not transitive or does\nnot contain RM the relation MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds,\nsince T makes one of the clauses ¬C1, ¬C2 or ¬C4 true.\n– The other possibility is that T be a reﬂexive, transitive relation containing\nRM. Then T contains the reﬂexive, transitive closure of RM. But (l(u), l(v)) is\nin that closure by assumption. Therefore, ¬C3 is made true in the interpreta-\ntion T under look-up table l, and so MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)\nholds.\n2.7 Micromodels of software\n141\nIn summary, MT ⊨l ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4) holds for all inter-\npretations T ⊆A × A. Therefore, M ⊨l ∀P ∃x∃y∃z (¬C1 ∨¬C2 ∨¬C3 ∨¬C4)\nholds.\n2\nIt is beyond the scope of this text to show that reachability can also be\nexpressed in existential second-order logic, but this is indeed the case. It is\nan important open problem to determine whether existential second-order\nlogic is closed under negation, i.e. whether for all such formulas ∃P φ there\nis a formula ∃Q ψ of existential second-order logic such that the latter is\nsemantically equivalent to the negation of the former.\nIf we allow existential and universal quantiﬁers to apply to predicate sym-\nbols in the same formula, we arrive at fully-ﬂedged second-order logic, e.g.\n∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))).\n(2.15)\nWe have ∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))) iﬀ\nthere is some T such that for all U we have (MT )U ⊨∀x∀y (Q(x, y) →",
    "bols in the same formula, we arrive at fully-ﬂedged second-order logic, e.g.\n∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))).\n(2.15)\nWe have ∃P∀Q (∀x∀y (Q(x, y) →Q(y, x)) →∀u∀v (Q(u, v) →P(u, v))) iﬀ\nthere is some T such that for all U we have (MT )U ⊨∀x∀y (Q(x, y) →\nQ(y, x)) →∀u∀v (Q(u, v) →P(u, v)), the latter being a model check in ﬁrst-\norder logic.\nIf one wants to quantify over relations of relations, one gets third-order\nlogic etc. Higher-order logics require great care in their design. Typical re-\nsults such as completeness and compactness may quickly fail to hold. Even\nworse, a naive higher-order logic may be inconsistent at the meta-level. Re-\nlated problems were discovered in naive set theory, e.g. in the attempt to\ndeﬁne the ‘set’ A that contains as elements those sets X that do not contain\nthemselves as an element:\nA\ndef\n= {X | X ̸∈X}.\n(2.16)\nWe won’t study higher-order logics in this text, but remark that many the-\norem provers or deductive frameworks rely on higher-order logical frame-\nworks.\n2.7 Micromodels of software\nTwo of the central concepts developed so far are\nr model checking: given a formula φ of predicate logic and a matching model M\ndetermine whether M ⊨φ holds; and\nr semantic entailment: given a set of formulas Γ of predicate logic, is Γ ⊨φ valid?\n142\n2 Predicate logic\nHow can we put these concepts to use in the modelling and reasoning about\nsoftware? In the case of semantic entailment, Γ should contain all the re-\nquirements we impose on a software design and φ may be a property we\nthink should hold in any implementation that meets the requirements Γ.\nSemantic entailment therefore matches well with software speciﬁcation and\nvalidation; alas, it is undecidable in general. Since model checking is de-\ncidable, why not put all the requirements into a model M and then check\nM ⊨φ? The diﬃculty with this approach is that, by comitting to a particu-\nlar model M, we are comitting to a lot of detail which doesn’t form part of",
    "validation; alas, it is undecidable in general. Since model checking is de-\ncidable, why not put all the requirements into a model M and then check\nM ⊨φ? The diﬃculty with this approach is that, by comitting to a particu-\nlar model M, we are comitting to a lot of detail which doesn’t form part of\nthe requirements. Typically, the model instantiates a number of parameters\nwhich were left free in the requirements. From this point of view, semantic\nentailment is better, because it allows a variety of models with a variety of\ndiﬀerent values for those parameters.\nWe seek to combine semantic entailment and model checking in a way\nwhich attempts to give us the advantages of both. We will extract from\nthe requirements a relatively small number of small models, and check that\nthey satisfy the property φ to be proved. This satisfaction checking has the\ntractability of model checking, while the fact that we range over a set of mod-\nels (albeit a small one) allows us to consider diﬀerent values of parameters\nwhich are not set in the requirements.\nThis approach is implemented in a tool called Alloy, due to D. Jackson.\nThe models we consider are what he calls ‘micromodels’ of software.\n2.7.1 State machines\nWe illlustrate this approach by revisiting Example 2.15 from page 125. Its\nmodels are state machines with F = {i} and P = {R, F}, where i is a con-\nstant, F a predicate symbol with one argument and R a predicate symbol\nwith two arguments. A (concrete) model M contains a set of concrete el-\nements A – which may be a set of states of a computer program. The in-\nterpretations iM ∈A, RM ∈A × A, and F M ⊆A are understood to be a\ndesignated initial state, a state transition relation, and a set of ﬁnal (ac-\ncepting) states, respectively. Model M is concrete since there is nothing left\nun-speciﬁed and all checks M ⊨φ have deﬁnite answers: they either hold or\nthey don’t.\nIn practice not all functional or other requirements of a software sys-",
    "cepting) states, respectively. Model M is concrete since there is nothing left\nun-speciﬁed and all checks M ⊨φ have deﬁnite answers: they either hold or\nthey don’t.\nIn practice not all functional or other requirements of a software sys-\ntem are known in advance, and they are likely to change during its life-\ncycle. For example, we may not know how many states there will be; and\nsome transitions may be mandatory whereas others may be optional in an\nimplementation. Conceptually, we seek a description M of all compliant\n2.7 Micromodels of software\n143\nimplementations Mi (i ∈I) of some software system. Given some matching\nproperty ψ, we then want to know\nr (assertion checking) whether ψ holds in all implementations Mi ∈M; or\nr (consistency checking) whether ψ holds in some implementation Mi ∈M.\nFor example, let M be the set of all concrete models of state machines, as\nabove. A possible assertion check ψ is ‘Final states are never initial states.’\nAn example of a consistency check ψ is ‘There are state machines that\ncontain a non-ﬁnal but deadlocked state.’\nAs remarked earlier, if M were the set of all state machines, then checking\nproperties would risk being undecidable, and would at least be intractable.\nIf M consists of a single model, then checking properties would be decidable;\nbut a single model is not general enough. It would comit us to instantiating\nseveral parameters which are not part of the requirements of a state machine,\nsuch as its size and detailed construction. A better idea is to ﬁx a ﬁnite bound\non the size of models, and check whether all models of that size that satisfy\nthe requirements also satisfy the property under consideration.\nr If we get a positive answer, we are somewhat conﬁdent that the property holds\nin all models. In this case, the answer is not conclusive, because there could be\na larger model which fails the property, but nevertheless a positive answer gives\nus some conﬁdence.",
    "r If we get a positive answer, we are somewhat conﬁdent that the property holds\nin all models. In this case, the answer is not conclusive, because there could be\na larger model which fails the property, but nevertheless a positive answer gives\nus some conﬁdence.\nr If we get a negative answer, then we have found a model in M which violates\nthe property. In that case, we have a conclusive answer, and can inspect the\nmodel in question.\nD. Jackson’s small scope hypothesis states that negative answers tend to\noccur in small models already, boosting the conﬁdence we may have in a\npositive answer. Here is how one could write the requirements for M for\nstate machines in Alloy:\nsig State {}\nsig StateMachine {\nA : set State,\ni : A,\nF : set A,\nR : A -> A\n}\nThe model speciﬁes two signatures. Signature State is simple in that it has\nno internal structure, denoted by {}. Although the states of real systems may\n144\n2 Predicate logic\nwell have internal structure, our Alloy declaration abstracts it away. The\nsecond signature StateMachine has internal, composite structure, saying\nthat every state machine has a set of states A, an initial state i from A, a set\nof ﬁnal states F from A, and a transition relation R of type A -> A. If we read\n-> as the cartesian product ×, we see that this internal structure is simply\nthe structural information needed for models of Example 2.15 (page 125).\nConcrete models of state machines are instances of signature StateMachine.\nIt is useful to think of signatures as sets whose elements are the instances of\nthat signature. Elements possess all the structure declared in their signature.\nGiven these signatures, we can code and check an assertion:\nassert FinalNotInitial {\nall M : StateMachine | no M.i & M.F\n} check FinalNotIntial for 3 but 1 StateMachine\ndeclares an assertion named FinalNotInitial whose body speciﬁes that\nfor all models M of type StateMachine the property no M.i & M.F is true.",
    "assert FinalNotInitial {\nall M : StateMachine | no M.i & M.F\n} check FinalNotIntial for 3 but 1 StateMachine\ndeclares an assertion named FinalNotInitial whose body speciﬁes that\nfor all models M of type StateMachine the property no M.i & M.F is true.\nRead & for set intersection and no S (‘there is no S’) for ‘set S is empty.’\nAlloy identiﬁes elements a with singleton sets {a}, so this set intersection\nis well typed. The relational dot operator . enables access to the internal\ncomponents of a state machine: M.i is the initial state of M and M.F is its set\nof ﬁnal states etc. Therefore, the expression no M.i & M.F states ‘No initial\nstate of M is also a ﬁnal state of M.’ Finally, the check directive informs the\nanalyzer of Alloy that it should try to ﬁnd a counterexample of the assertion\nFinalNotInitial with at most three elements for every signature, except\nfor StateMachine which should have at most one.\nThe results of Alloy’s assertion check are shown in Figure 2.7. This visual-\nization has been customized to decorate initial and ﬁnal states with respec-\ntive labels i and F. The transition relation is shown as a labeled graph and\nthere is only one transition (from State 0 back to State 0) in this exam-\nple. Please verify that this is a counterexample to the claim of the assertion\nFinalNotInitial within the speciﬁed scopes. Alloy’s GUI lets you search\nfor additional witnesses (here: counterexamples), if they exist.\nSimilarly, we can check a property of state machines for consistency with\nour model. Alloy uses the keyword fun for consistency checks. e.g.\nfun AGuidedSimulation(M : StateMachine, s : M.A) {\nno s.(M.R)\nnot s in M.F\n# M.A = 3\n} run AGiudedSimulation for 3 but 1 StateMachine\n2.7 Micromodels of software\n145\nmodule AboutStateMachines\nsig State {}\n-- simple states\nsig StateMachine { -- composite state machines\nA : set State,\n-- set of states of a state machine\ni : A,\n-- initial state of a state machine\nF : set A,\n-- set of final states of a state machine",
    "2.7 Micromodels of software\n145\nmodule AboutStateMachines\nsig State {}\n-- simple states\nsig StateMachine { -- composite state machines\nA : set State,\n-- set of states of a state machine\ni : A,\n-- initial state of a state machine\nF : set A,\n-- set of final states of a state machine\nR : A -> A\n-- transition relation of a state machine\n}\n-- Claim that final states are never initial: false.\nassert FinalNotInitial {\nall M : StateMachine | no M.i & M.F\n} check FinalNotInitial for 3 but 1 StateMachine\n-- Is there a three-state machine with a non-final deadlock? True.\nfun AGuidedSimulation(M : StateMachine, s : M.A) {\nno s.(M.R)\nnot s in M.F\n# M.A = 3\n} run AGuidedSimulation for 3 but 1 StateMachine\nFigure 2.6. The complete Alloy module for models of state machines,\nwith one assertion and one consistency check. The lexeme -- enables\ncomments on the same line.\nState_0\nR\nState_1\n(F)\nState_2\n(i, F)\nFigure 2.7. Alloy’s analyzer finds a state machine model (with one\ntransition only) within the specified scope such that the assertion\nFinalNotInitial is false: the initial state State 2 is also final.\nThis consistency check is named AGuidedSimulation and followed by an\nordered ﬁnite list of parameter/type pairs; the ﬁrst parameter is M of type\nStateMachine, the second one is s of type M.A – i.e. s is a state of M. The\nbody of a consistency check is a ﬁnite list of constraints (here three), which\nare conjoined implicitly. In this case, we want to ﬁnd a model with instances\nof the parameters M and s such that s is a non-ﬁnal state of M, the second\nconstraint not s in M.F plus the type information s : M.A; and there is\nno transition out of s, the ﬁrst constraint no s.(M.R).\nThe latter requires further explanation. The keyword no denotes ‘there\nis no;’ here it is applied to the set s.(M.R), expressing that there are no\n146\n2 Predicate logic\nState_0\nState_2\n(i)\nR\nState_1\nR\nFigure 2.8. Alloy’s analyzer finds a state machine model within the",
    "The latter requires further explanation. The keyword no denotes ‘there\nis no;’ here it is applied to the set s.(M.R), expressing that there are no\n146\n2 Predicate logic\nState_0\nState_2\n(i)\nR\nState_1\nR\nFigure 2.8. Alloy’s analyzer finds a state machine model within the\nspecified scope such that the consistency check AGuidedSimulation is\ntrue: there is a non-final deadlocked state, here State 2.\nelements in s.(M.R). Since M.R is the transition relation of M, we need to\nunderstand how s.(M.R) constructs a set. Well, s is an element of M.A and\nM.R has type M.A -> M.A. Therefore, we may form the set of all elements s’\nsuch that there is a M.R-transition from s to s’; this is the set s.(M.R). The\nthird constraint states that M has exactly three states: in Alloy,\n# S = k\ndeclares that the set S has exactly k elements.\nThe\nrun\ndirective\ninstructs\nto\ncheck\nthe\nconsistency\nof\nAGuidedSimulation for at most one state machine and at most three\nstates; the constraint analyzer of Alloy returns the witness (here: an exam-\nple) of Figure 2.8. Please check that this witness satisﬁes all constraints of\nthe consistency check and that it is within the speciﬁed scopes.\nThe complete model of state machines with these two checks is depicted in\nFigure 2.6. The keyword plus name module AboutStateMachines identify\nthis under-speciﬁed model M, rightly suggesting that Alloy is a modular\nspeciﬁcation and analysis platform.\n2.7.2 Alma – re-visited\nRecall Example 2.19 from page 128. Its model had three elements and did\nnot satisfy the formula in (2.8). We can now write a module in Alloy which\nchecks whether all smaller models have to satisfy (2.8). The code is given in\nFigure 2.9. It names the module AboutAlma and deﬁnes a simple signature of\ntype Person. Then it declares a signature SoapOpera which has a cast – a\nset of type Person – a designated cast member alma, and a relation loves\nof type cast -> cast. We check the assertion OfLovers in a scope of at",
    "Figure 2.9. It names the module AboutAlma and deﬁnes a simple signature of\ntype Person. Then it declares a signature SoapOpera which has a cast – a\nset of type Person – a designated cast member alma, and a relation loves\nof type cast -> cast. We check the assertion OfLovers in a scope of at\nmost two persons and at most one soap opera. The body of that assertion\nis the typed version of (2.8) and deserves a closer look:\n1.\nExpressions of the form all x : T | F state that formula F is true for all\ninstances x of type T. So the assertion states that with S {...} is true for all\nsoap operas S.\n2.7 Micromodels of software\n147\nmodule AboutAlma\nsig Person {}\nsig SoapOpera {\ncast : set Person,\nalma : cast,\nloves : cast -> cast\n}\nassert OfLovers {\nall S : SoapOpera |\nwith S {\nall x, y : cast |\nalma in x.loves && x in y.loves => not alma in y.loves\n}\n}\ncheck OfLovers for 2 but 1 SoapOpera\nFigure 2.9. In this module, the analysis of OfLovers checks whether\nthere is a model of ≤2 persons and ≤1 soap operas for which the\nquery in (2.8), page 128, is false.\nPerson_1\n(cast, alma)\nloves\nPerson_0\nFigure 2.10. Alloy’s analyzer finds a counterexample to the formula in\n(2.8): Alma is the only cast member and loves herself.\n2.\nThe expression with S {...} is a convenient notation that allows us to write\nloves and cast instead of the needed S.loves and S.cast (respectively) within\nits curly brackets.\n3.\nIts body ... states that for all x, and y in the cast of S, if alma is loved by x\nand x is loved by y, then – the symbol => expresses implication – alma is not\nloved by y.\nAlloy’s analysis ﬁnds a counterexample to this assertion, shown in Fig-\nure 2.10. It is a counterexample since alma is her own lover, and therefore\nalso one of her lover’s lovers’. Apparently, we have underspeciﬁed our model:\nwe implicitly made the domain-speciﬁc assumption that self-love makes for\n148\n2 Predicate logic\nPerson_1\n(cast)\nPerson_0\n(cast, alma)\nloves\nPerson_2\n(cast)\nloves\nloves",
    "also one of her lover’s lovers’. Apparently, we have underspeciﬁed our model:\nwe implicitly made the domain-speciﬁc assumption that self-love makes for\n148\n2 Predicate logic\nPerson_1\n(cast)\nPerson_0\n(cast, alma)\nloves\nPerson_2\n(cast)\nloves\nloves\nFigure 2.11. Alloy’s analyzer finds a counterexample to the formula in\n(2.8) that meets the constraint of NoSelfLove with three cast members.\nThe bidirectional arrow indicates that Person 1 loves Person 2 and vice\nversa.\na poor script of jealousy and intrigue, but did not rule out self-love in our\nAlloy module. To remedy this, we can add a fact to the module; facts may\nhave names and restrict the set of possible models: assertions and consis-\ntency checks are conducted only over concrete models that satisfy all facts\nof the module. Adding the declaration\nfact NoSelfLove {\nall S : SoapOpera, p : S.cast | not p in p.(S.loves)\n}\nto the module AboutAlma enforces that no member of any soap-opera cast\nloves him or herself. We re-check the assertion and the analyzer informs us\nthat no solution was found. This suggests that our model from Example 2.19\nis indeed a minimal one in the presence of that domain assumption. If we\nretain that fact, but change the occurrence of 2 in the check directive to 3,\nwe get a counterexample, depicted in Figure 2.11. Can you see why it is a\ncounterexample?\n2.7.3 A software micromodel\nSo far we used Alloy to generate instances of models of ﬁrst-order logic that\nsatisfy certain constraints expressed as formulas of ﬁrst-order logic. Now we\napply Alloy and its constraint analyzer to a more serious task: we model a\nsoftware system. The intended beneﬁts provided by a system model are\n1.\nit captures formally static and dynamic system structure and behaviour;\n2.\nit can verify consistency of the constrained design space;\n2.7 Micromodels of software\n149\n3.\nit is executable, so it allows guided simulations through a potentially very com-\nplex design space; and\n4.",
    "1.\nit captures formally static and dynamic system structure and behaviour;\n2.\nit can verify consistency of the constrained design space;\n2.7 Micromodels of software\n149\n3.\nit is executable, so it allows guided simulations through a potentially very com-\nplex design space; and\n4.\nit can boost our conﬁdence into the correctness of claims about static and\ndynamic aspects of all its compliant implementations.\nMoreover, formal models attached to software products can be seen as a\nreliability contract; a promise that the software implements the structure and\nbehaviour of the model and is expected to meet all of the assertions certiﬁed\ntherein. (However, this may not be very useful for extremely under-speciﬁed\nmodels.)\nWe will model a software package dependency system. This system is used\nwhen software packages are installed or upgraded. The system checks to see\nif prerequisites in the form of libraries or other packages are present. The\nrequirements on a software package dependency system are not straightfor-\nward. As most computer users know, the upgrading process can go wrong\nin various ways. For example, upgrading a package can involve replacing\nshared libraries with newer versions. But other packages which rely on the\nolder versions of the shared libraries may then cease to work.\nSoftware package dependency systems are used in several computer sys-\ntems, such as Red Hat Linux, .NET’s Global Assembly Cache and others.\nUsers often have to guess how technical questions get resolved within the de-\npendency system. To the best of our knowledge, there is no publicly available\nformal and executable model of any particular dependency system to which\napplication programmers could turn if they had such non-trivial technical\nquestions about its inner workings.\nIn our model, applications are built out of components. Components oﬀer\nservices to other components. A service can be a number of things. Typically,",
    "application programmers could turn if they had such non-trivial technical\nquestions about its inner workings.\nIn our model, applications are built out of components. Components oﬀer\nservices to other components. A service can be a number of things. Typically,\na service is a method (a modular piece of program code), a ﬁeld entry, or a\ntype – e.g. the type of a class in an object-oriented programming language.\nComponents typically require the import of services from other components.\nTechnically speaking, such import services resolve all un-resolved references\nwithin that component, making the component linkable. A component also\nhas a name and may have a special service, called ‘main.’\nWe model components as a signature in Alloy:\nsig Component {\nname: Name,\n-- name of the component\nmain: option Service, -- component may have a ‘main’ service\nexport: set Service,\n-- services the component exports\nimport: set Service,\n-- services the component imports\nversion: Number\n-- version number of the component\n}{ no import & export }\n150\n2 Predicate logic\nThe signatures Service and Name won’t require any composite structure for\nour modelling purposes. The signature Number will get an ordering later on.\nA component is an instance of Component and therefore has a name, a set of\nservices export it oﬀers to other components, and a set import of services\nit needs to import from other components. Last but not least, a component\nhas a version number. Observe the role of the modiﬁers set and option\nabove.\nA declaration i : set S means that i is a subset of set S; but a declara-\ntion i : option S means that i is a subset of S with at most one element.\nThus, option enables us to model an element that may (non-empty, sin-\ngleton set) or may not (empty set) be present; a very useful ability indeed.\nFinally, a declaration i : S states that i is a subset of S containing ex-\nactly one element; this really speciﬁes a scalar/element of type S since Alloy\nidentiﬁes elements a with sets {a}.",
    "gleton set) or may not (empty set) be present; a very useful ability indeed.\nFinally, a declaration i : S states that i is a subset of S containing ex-\nactly one element; this really speciﬁes a scalar/element of type S since Alloy\nidentiﬁes elements a with sets {a}.\nWe can constrain all instances of a signature with C by adding { C } to\nits signature declaration. We did this for the signature Component, where C\nis the constraint no import & export, stating that, in all components, the\nintersection (&) of import and export is empty (no).\nA Package Dependency System (PDS) consists of a set of components:\nsig PDS {\ncomponents : set Component\n...\n}{ components.import in components.export }\nand other structure that we specify later on. The primary concern in a PDS\nis that its set of components be coherent: at all times, all imports of all of its\ncomponents can be serviced within that PDS. This requirement is enforced\nfor all instances of PDS by adding the constraint components.import in\ncomponents.export to its signature. Here components is a set of compo-\nnents and Alloy deﬁnes the meaning of components.import as the union of\nall sets c.import, where c is an element of components. Therefore the re-\nquirement states that, for all c in components, all of c’s needed services can\nbe provided by some component in components as well. This is exactly the\nintegrity constraint we need for the set of components of a PDS. Observe that\nthis requirement does not specify which component provides which service,\nwhich would be an unacceptable imposition on implementation freedom.\nGiven this integrity constraint we can already model the installation\n(adding) or removal of a component in a PDS, without having speciﬁed the\nremaining structure of a PDS. This is possible since, in the context of these\noperations, we may abstract a PDS into its set of components. We model\n2.7 Micromodels of software\n151\nthe addition of a component to a PDS as a parametrized fun-statement with",
    "remaining structure of a PDS. This is possible since, in the context of these\noperations, we may abstract a PDS into its set of components. We model\n2.7 Micromodels of software\n151\nthe addition of a component to a PDS as a parametrized fun-statement with\nname AddComponent and three parameters\nfun AddComponent(P, P’: PDS, c: Component) {\nnot c in P.components\nP’.components = P.components + c\n} run AddComponent for 3\nwhere P is intended to be the PDS prior to the execution of that operation,\nP’ models the PDS after that execution, and c models the component that is\nto be added. This intent interprets the parametric constraint AddComponent\nas an operation leading from one ‘state’ to another (obtained by removing\nc from the PDS P). The body of AddComponent states two constraints, con-\njoined implicitly. Thus, this operation applies only if the component c is not\nalready in the set of components of the PDS (not c in P.components; an\nexample of a precondition) and if the PDS adds only c and does not lose\nany other components (P’.components = P.components + c; an example\nof a postcondition).\nTo get a feel for the complexities and vexations of designing software sys-\ntems, consider our conscious or implicit decision to enforce that all instances\nof PDS have a coherent set of components. This sounds like a very good idea,\nbut what if a ‘real’ and faulty PDS ever gets to a state in which it is inco-\nherent? We would then be prevented from adding components that may re-\nstore its coherence! Therefore, the aspects of our model do not include issues\nsuch as repair – which may indeed by an important software management\naspect.\nThe speciﬁcation for the removal of a component is very similar to the\none for AddComponent:\nfun RemoveComponent(P, P’: PDS, c: Component) {\nc in P.components\nP’.components = P.components - c\n} run RemoveComponent for 3\nexcept that the precondition now insists that c be in the set of components",
    "The speciﬁcation for the removal of a component is very similar to the\none for AddComponent:\nfun RemoveComponent(P, P’: PDS, c: Component) {\nc in P.components\nP’.components = P.components - c\n} run RemoveComponent for 3\nexcept that the precondition now insists that c be in the set of components\nof the PDS prior to the removal; and the postcondition speciﬁes that the\nPDS lost component c but did not add or lose any other components. The\nexpression S - T denotes exactly those ‘elements’ of S that are not in T.\nIt remains to complete the signature for PDS. Three additions are\nmade.\n1.\nA relation schedule assigns to each PDS component and any of its import\nservices a component in that PDS that provides that service.\n152\n2 Predicate logic\nfact SoundPDSs {\nall P : PDS |\nwith P {\nall c : components, s : Service |\n--1\nlet c’ = c.schedule[s] {\n(some c’ iff s in c.import) && (some c’ => s in c’.export)\n}\nall c : components | c.requires = c.schedule[Service]\n--2\n}\n}\nFigure 2.12. A fact that constrains the state and schedulers of all PDSs.\n2.\nDerived from schedule we obtain a relation requires between components of\nthe PDS that expresses the dependencies between these components based on\nthe schedule.\n3.\nFinally, we add constraints that ensure the integrity and correct handling of\nschedule and requires for all instances of PDS.\nThe complete signature of PDS is\nsig PDS {\ncomponents : set Component,\nschedule\n: components -> Service ->? components,\nrequires\n: components -> components\n}\nFor any P : PDS, the expression P.schedule denotes a relation of type\nP.components -> Service ->? P.components. The ? is a multiplicity con-\nstraint, saying that each component of the PDS and each service get related\nto at most one component. This will ensure that the scheduler is deter-\nministic and that it may not schedule anything – e.g. when the service is\nnot needed by the component in the ﬁrst argument. In Alloy there are also",
    "straint, saying that each component of the PDS and each service get related\nto at most one component. This will ensure that the scheduler is deter-\nministic and that it may not schedule anything – e.g. when the service is\nnot needed by the component in the ﬁrst argument. In Alloy there are also\nmultiplicity markings ! for ‘exactly one’ and + for ‘one or more.’ The ab-\nsence of such markings means ‘zero or more.’ For example, the declaration\nof requires uses that default reading.\nWe use a fact-statement to constrain even further the structure and\nbehaviour of all PDSs, depicted in Figure 2.12. The fact named SoundPDSs\nquantiﬁes the constraints over all instances of PDSs (all P : PDS | ...)\nand uses with P {...} to avoid the use of navigation expressions of the\nform P.e. The body of that fact lists two constraints --1 and --2:\n2.7 Micromodels of software\n153\n--1 states two constraints within a let-expression of the form let x\n= E {...}. Such a let-expression declares all free occurrences of x in\n{...} to be equal to E. Note that [] is a version of the dot operator\n. with lower binding priority, so c.schedule[s] is syntactic sugar for\ns.(c.schedule).\nr In the ﬁrst constraint, component c and a service s have another component c’\nscheduled (some c’ is true iﬀset c’ is non-empty) if and only if s is actually in\nthe import set of c. Only needed services are scheduled!\nr In the second constraint, if c’ is scheduled to provide service s for c, then s is\nin the export set of c’ – we can only schedule components that can provide the\nscheduled services!\n--2 deﬁnes requires in terms of schedule: a component c requires all those\ncomponents that are scheduled to provide some service for c.\nOur complete Alloy model for PDSs is shown in Figure 2.13. Using Al-\nloy’s constraint analyzer we validate that all our fun-statements, notably\nthe operations of removing and adding components to a PDS, are logically\nconsistent for this design.",
    "Our complete Alloy model for PDSs is shown in Figure 2.13. Using Al-\nloy’s constraint analyzer we validate that all our fun-statements, notably\nthe operations of removing and adding components to a PDS, are logically\nconsistent for this design.\nThe assertion AddingIsFunctionalForPDSs claims that the execution of\nthe operation which adds a component to a PDS renders a unique result\nPDS. Alloy’s analyzer ﬁnds a counterexample to this claim, where P has\nno components, so nothing is scheduled or required; and P’ and P’’ have\nComponent 2 as only component, added to P, so this component is required\nand scheduled in those PDSs.\nSince P’ and P’’ seem to be equal, how can this be a counterexample?\nWell, we ran the analysis in scope 3, so PDS = {PDS 0, PDS 1, PDS 2} and\nAlloy chose PDS 0 as P, PDS 1 as P’, and PDS 2 as P’’. Since the set PDS\ncontains three elements, Alloy ‘thinks’ that they are all diﬀerent from each\nother. This is the interpretation of equality enforced by predicate logic. Ob-\nviously, what is needed here is a\nstructural equality of types: we want to\nensure that the addition of a component results into a PDS with unique\nstructure. A fun-statement can be used to specify structural equality:\nfun StructurallyEqual(P, P’ : PDS) {\nP.components = P’.components\nP.schedule = P’.schedule\nP.requires = P’.requires\n} run StructurallyEqual for 2\nWe then simply replace the expression P’ = P’’ in AdditionIsFunctional\nwith the expression StructurallyEqual(P’,P’’), increase the scope for\n154\n2 Predicate logic\nmodule PDS\nopen std/ord\n-- opens specification template for linear order\nsig Component {\nname: Name,\nmain: option Service,\nexport: set Service,\nimport: set Service,\nversion: Number\n}{ no import & export }\nsig PDS {\ncomponents: set Component,\nschedule: components -> Service ->? components,\nrequires: components -> components\n}{ components.import in components.export }\nfact SoundPDSs {\nall P : PDS |\nwith P {\nall c : components, s : Service |\n--1\nlet c’ = c.schedule[s] {",
    "}{ no import & export }\nsig PDS {\ncomponents: set Component,\nschedule: components -> Service ->? components,\nrequires: components -> components\n}{ components.import in components.export }\nfact SoundPDSs {\nall P : PDS |\nwith P {\nall c : components, s : Service |\n--1\nlet c’ = c.schedule[s] {\n(some c’ iff s in c.import) && (some c’ => s in c’.export) }\nall c : components | c.requires = c.schedule[Service]\n}\n--2\n}\nsig Name, Number, Service {}\nfun AddComponent(P, P’: PDS, c: Component) {\nnot c in P.components\nP’.components = P.components + c\n} run AddComponent for 3 but 2 PDS\nfun RemoveComponent(P, P’: PDS, c : Component) {\nc in P.components\nP’.components = P.components - c\n} run RemoveComponent for 3 but 2 PDS\nfun HighestVersionPolicy(P: PDS) {\nwith P {\nall s : Service, c : components, c’ : c.schedule[s],\nc’’ : components - c’ {\ns in c’’.export && c’’.name = c’.name =>\nc’’.version in c’.version.^(Ord[Number].prev) } }\n} run HighestVersionPolicy for 3 but 1 PDS\nfun AGuidedSimulation(P,P’,P’’ : PDS, c1, c2 : Component) {\nAddComponent(P,P’,c1)\nRemoveComponent(P,P’’,c2)\nHighestVersionPolicy(P)\nHighestVersionPolicy(P’)\nHighestVersionPolicy(P’’)\n} run AGuidedSimulation for 3\nassert AddingIsFunctionalForPDSs {\nall P, P’, P’’: PDS, c: Component\n{\nAddComponent(P,P’,c) &&\nAddComponent(P,P’’,c) => P’ = P’’ }\n} check AddingIsFunctionalForPDSs for 3\nFigure 2.13. Our Alloy model of the PDS.\n2.7 Micromodels of software\n155\nthat assertion to 7, re-built the model, and re-analyze that assertion.\nPerhaps surprisingly, we ﬁnd as counterexample a PDS 0 with two com-\nponents Component 0 and Component 1 such that Component 0.import =\n{ Service 2 }\nand\nComponent 1.import = { Service 1 }.\nSince\nService 2 is contained in Component 2.export, we have two struc-\nturally diﬀerent legitimate post states which are obtained by adding\nComponent 2 but which diﬀer in their scheduler. In P’ we have the same\nscheduling instances as in PDS 0. Yet P’’ schedules Component 2 to",
    "Since\nService 2 is contained in Component 2.export, we have two struc-\nturally diﬀerent legitimate post states which are obtained by adding\nComponent 2 but which diﬀer in their scheduler. In P’ we have the same\nscheduling instances as in PDS 0. Yet P’’ schedules Component 2 to\nprovide service Service 2 for Component 0; and Component 0 still provides\nService 1 to Component 1. This analysis reveals that the addition of\ncomponents creates opportunities to reschedule services, for better (e.g.\noptimizations) or for worse (e.g. security breaches).\nThe utility of a micromodel of software resides perhaps more in the ability\nto explore it through guided simulations, as opposed to verifying some of\nits properties with absolute certainty. We demonstrate this by generating\na simulation that shows the removal and the addition of a component to a\nPDS such that the scheduler always schedules components with the highest\nversion number possible in all PDSs. Therefore we know that such a schedul-\ning policy is consistent for these two operations; it is by no means the only\nsuch policy and is not guaranteed to ensure that applications won’t break\nwhen using scheduled services. The fun-statement\nfun HighestVersionPolicy(P: PDS) {\nwith P {\nall s : Service, c : components, c’ : c.schedule[s],\nc’’ : components - c’ {\ns in c’’.export && c’’.name = c’.name =>\nc’’.version in c’.version.^(Ord[Number].prev)\n}\n}\n} run HighestVersionPolicy for 3 but 1 PDS\nspeciﬁes that, among those suppliers with identical name, the scheduler\nchooses one with the highest available version number. The expression\nc’.version.^(Ord[Number].prev)\nneeds explaining: c’.version is the version number of c’, an element of\ntype Number. The symbol ^ can be applied to a binary relation r : T -> T\nsuch that ^r has again type T -> T and denotes the transitive closure of r.\nIn this case, T equals Number and r equals Ord[Number].prev.\n156\n2 Predicate logic",
    "type Number. The symbol ^ can be applied to a binary relation r : T -> T\nsuch that ^r has again type T -> T and denotes the transitive closure of r.\nIn this case, T equals Number and r equals Ord[Number].prev.\n156\n2 Predicate logic\nBut what shall me make of the latter expression? It assumes that the mod-\nule contains a statement open std/ord which opens the signature speciﬁca-\ntions from another module in ﬁle ord.als of the library std. That module\ncontains a signature named Ord which has a type variable as a parameter; it\nis polymorphic. The expression Ord[Number] instantiates that type variable\nwith the type Number, and then invokes the prev relation of that signa-\nture with that type, where prev is constrained in std/ord to be a linear\norder. The net eﬀect is that we create a linear order on Number such that\nn.prev is the previous element of n with respect to that order. Therefore,\nn.^prev lists all elements that are smaller than n in that order. Please reread\nthe body of that fun-statement to convince yourself that it states what is\nintended.\nSince fun-statements can be invoked with instances of their parameters,\nwe can write the desired simulation based on HighestVersionPolicy:\nfun AGuidedSimulation(P,P’,P’’ : PDS, c1, c2 : Component) {\nAddComponent(P,P’,c1)\nRemoveComponent(P,P’’,c2)\nHighestVersionPolicy(P)\nHighestVersionPolicy(P’) HighestVersionPolicy(P’’)\n} run AGuidedSimulation for 3\nAlloy’s analyzer generates a scenario for this simulation, which amounts\nto two diﬀerent operation snapshots originating in P such that all three\nparticipating PDSs schedule according to HighestVersionPolicy. Can you\nspot why we had to work with two components c1 and c2?\nWe conclude this case study by pointing out limitations of Alloy and its\nanalyzer. In order to be able to use a SAT solver for propositional logic\nas an analysis engine, we can only check or run formulas of existential or\nuniversal second-order logic in the bodies of assertions or in the bodies of",
    "We conclude this case study by pointing out limitations of Alloy and its\nanalyzer. In order to be able to use a SAT solver for propositional logic\nas an analysis engine, we can only check or run formulas of existential or\nuniversal second-order logic in the bodies of assertions or in the bodies of\nfun-statements (if they are wrapped in existential quantiﬁers for all param-\neters). For example, we cannot even check whether there is an instance of\nAddComponent such that for the resulting PDS a certain scheduling policy is\nimpossible. For less explicit reasons it also seems unlikely that we can check\nin Alloy that every coherent set of components is realizable as P.components\nfor some PDS P. This deﬁciency is due to the inherent complexity of such\nproblems and theorem provers may have to be used if such properties need\nto be guaranteed. On the other hand, the expressiveness of Alloy allows for\nthe rapid prototyping of models and the exploration of simulations and pos-\nsible counterexamples which should enhance once understanding of a design\nand so improve that design’s reliability.\n2.8 Exercises\n157\n2.8 Exercises\nExercises 2.1\n1.\n*\nUse the predicates\nA(x, y) :\nx admires y\nB(x, y) :\nx attended y\nP(x) :\nx is a professor\nS(x) :\nx is a student\nL(x) :\nx is a lecture\nand the nullary function symbol (constant)\nm :\nMary\nto translate the following into predicate logic:\n(a) Mary admires every professor.\n(The answer is not ∀x A(m, P(x)).)\n(b) Some professor admires Mary.\n(c) Mary admires herself.\n(d) No student attended every lecture.\n(e) No lecture was attended by every student.\n(f) No lecture was attended by any student.\n2. Use the predicate speciﬁcations\nB(x, y) :\nx beats y\nF(x) :\nx is an (American) football team\nQ(x, y) :\nx is quarterback of y\nL(x, y) :\nx loses to y\nand the constant symbols\nc :\nWildcats\nj :\nJayhawks\nto translate the following into predicate logic.\n(a) Every football team has a quarterback.",
    "2. Use the predicate speciﬁcations\nB(x, y) :\nx beats y\nF(x) :\nx is an (American) football team\nQ(x, y) :\nx is quarterback of y\nL(x, y) :\nx loses to y\nand the constant symbols\nc :\nWildcats\nj :\nJayhawks\nto translate the following into predicate logic.\n(a) Every football team has a quarterback.\n(b) If the Jayhawks beat the Wildcats, then the Jayhawks do not lose to every\nfootball team.\n(c) The Wildcats beat some team, which beat the Jayhawks.\n3.\n*\nFind appropriate predicates and their speciﬁcation to translate the following\ninto predicate logic:\n(a) All red things are in the box.\n(b) Only red things are in the box.\n(c) No animal is both a cat and a dog.\n(d) Every prize was won by a boy.\n(e) A boy won every prize.\n158\n2 Predicate logic\n4. Let F(x, y) mean that x is the father of y; M(x, y) denotes x is the mother of y.\nSimilarly, H(x, y), S(x, y), and B(x, y) say that x is the husband/sister/brother\nof y, respectively. You may also use constants to denote individuals, like ‘Ed’\nand ‘Patsy.’ However, you are not allowed to use any predicate symbols other\nthan the above to translate the following sentences into predicate logic:\n(a) Everybody has a mother.\n(b) Everybody has a father and a mother.\n(c) Whoever has a mother has a father.\n(d) Ed is a grandfather.\n(e) All fathers are parents.\n(f) All husbands are spouses.\n(g) No uncle is an aunt.\n(h) All brothers are siblings.\n(i) Nobody’s grandmother is anybody’s father.\n(j) Ed and Patsy are husband and wife.\n(k) Carl is Monique’s brother-in-law.\n5. The following sentences are taken from the RFC3157 Internet Taskforce Docu-\nment ‘Securely Available Credentials – Requirements.’ Specify each sentence in\npredicate logic, deﬁning predicate symbols as appropriate:\n(a) An attacker can persuade a server that a successful login has occurred, even\nif it hasn’t.\n(b) An attacker can overwrite someone else’s credentials on the server.\n(c) All users enter passwords instead of names.",
    "predicate logic, deﬁning predicate symbols as appropriate:\n(a) An attacker can persuade a server that a successful login has occurred, even\nif it hasn’t.\n(b) An attacker can overwrite someone else’s credentials on the server.\n(c) All users enter passwords instead of names.\n(d) Credential transfer both to and from a device MUST be supported.\n(e) Credentials MUST NOT be forced by the protocol to be present in cleartext\nat any device other than the end user’s.\n(f) The protocol MUST support a range of cryptographic algorithms, includ-\ning syymetric and asymmetric algorithms, hash algorithms, and MAC algo-\nrithms.\n(g) Credentials MUST only be downloadable following user authentication or\nelse only downloadable in a format that requires completion of user authen-\ntication for deciphering.\n(h) Diﬀerent end user devices MAY be used to download, upload, or manage the\nsame set of credentials.\nExercises 2.2\n1. Let F be {d, f, g}, where d is a constant, f a function symbol with two arguments\nand g a function symbol with three arguments.\n(a) Which of the following strings are terms over F? Draw the parse tree of those\nstrings which are indeed terms:\ni. g(d, d)\nii.\n*\nf(x, g(y, z), d)\n2.8 Exercises\n159\n∗\n−\nx\n2\n+\ns\ny\nx\nFigure 2.14. A parse tree representing an arithmetic term.\niii.\n*\ng(x, f(y, z), d)\niv. g(x, h(y, z), d)\nv. f(f(g(d, x), f(g(d, x), y, g(y, d)), g(d, d)), g(f(d, d, x), d), z)\n(b) The length of a term over F is the length of its string representation, where we\ncount all commas and parentheses. For example, the length of f(x, g(y, z), z)\nis 13. List all variable-free terms over F of length less than 10.\n(c)\n*\nThe height of a term over F is deﬁned as 1 plus the length of the longest\npath in its parse tree, as in Deﬁnition 1.32. List all variable-free terms over\nF of height less than 4.\n2. Draw the parse tree of the term (2 −s(x)) + (y ∗x), considering that −, +, and\n∗are used in inﬁx in this term. Compare your solution with the parse tree in\nFigure 2.14.",
    "path in its parse tree, as in Deﬁnition 1.32. List all variable-free terms over\nF of height less than 4.\n2. Draw the parse tree of the term (2 −s(x)) + (y ∗x), considering that −, +, and\n∗are used in inﬁx in this term. Compare your solution with the parse tree in\nFigure 2.14.\n3. Which of the following strings are formulas in predicate logic? Specify a reason\nfor failure for strings which aren’t, draw parse trees of all strings which are.\n(a)\n*\nLet m be a constant, f a function symbol with one argument and S and B\ntwo predicate symbols, each with two arguments:\ni. S(m, x)\nii. B(m, f(m))\niii. f(m)\niv. B(B(m, x), y)\nv. S(B(m), z)\nvi. (B(x, y) →(∃z S(z, y)))\nvii. (S(x, y) →S(y, f(f(x))))\nviii. (B(x) →B(B(x))).\n(b) Let c and d be constants, f a function symbol with one argument, g a function\nsymbol with two arguments and h a function symbol with three arguments.\nFurther, P and Q are predicate symbols with three arguments:\n160\n2 Predicate logic\ni. ∀x P(f(d), h(g(c, x), d, y))\nii. ∀x P(f(d), h(P(x, y), d, y))\niii. ∀x Q(g(h(x, f(d), x), g(x, x)), h(x, x, x), c)\niv. ∃z (Q(z, z, z) →P(z))\nv. ∀x ∀y (g(x, y) →P(x, y, x))\nvi. Q(c, d, c).\n4. Let φ be ∃x (P(y, z) ∧(∀y (¬Q(y, x) ∨P(y, z)))), where P and Q are predicate\nsymbols with two arguments.\n(a)\n*\nDraw the parse tree of φ.\n(b)\n*\nIdentify all bound and free variable leaves in φ.\n(c) Is there a variable in φ which has free and bound occurrences?\n(d)\n*\nConsider the terms w (w is a variable), f(x) and g(y, z), where f and g are\nfunction symbols with arity 1 and 2, respectively.\ni. Compute φ[w/x], φ[w/y], φ[f(x)/y] and φ[g(y, z)/z].\nii. Which of w, f(x) and g(y, z) are free for x in φ?\niii. Which of w, f(x) and g(y, z) are free for y in φ?\n(e) What is the scope of ∃x in φ?\n(f)\n*\nSuppose that we change φ to ∃x (P(y, z) ∧(∀x (¬Q(x, x) ∨P(x, z)))). What\nis the scope of ∃x now?\n5. (a) Let P be a predicate symbol with arity 3. Draw the parse tree of ψ\ndef\n=\n¬(∀x ((∃y P(x, y, z)) ∧(∀z P(x, y, z)))).",
    "(e) What is the scope of ∃x in φ?\n(f)\n*\nSuppose that we change φ to ∃x (P(y, z) ∧(∀x (¬Q(x, x) ∨P(x, z)))). What\nis the scope of ∃x now?\n5. (a) Let P be a predicate symbol with arity 3. Draw the parse tree of ψ\ndef\n=\n¬(∀x ((∃y P(x, y, z)) ∧(∀z P(x, y, z)))).\n(b) Indicate the free and bound variables in that parse tree.\n(c) List all variables which occur free and bound therein.\n(d) Compute ψ[t/x], ψ[t/y] and ψ[t/z], where t\ndef\n= g(f(g(y, y)), y). Is t free for x\nin ψ; free for y in ψ; free for z in ψ?\n6. Rename the variables for φ in Example 2.9 (page 106) such that the resulting\nformula ψ has the same meaning as φ, but f(y, y) is free for x in ψ.\nExercises 2.3\n1. Prove the validity of the following sequents using, among others, the rules =i\nand =e. Make sure that you indicate for each application of =e what the rule\ninstances φ, t1 and t2 are.\n(a) (y = 0) ∧(y = x) ⊢0 = x\n(b) t1 = t2 ⊢(t + t2) = (t + t1)\n(c) (x = 0) ∨((x + x) > 0) ⊢(y = (x + x)) →((y > 0) ∨(y = (0 + x))).\n2. Recall that we use = to express the equality of elements in our models. Consider\nthe formula ∃x ∃y (¬(x = y) ∧(∀z ((z = x) ∨(z = y)))). Can you say, in plain\nEnglish, what this formula speciﬁes?\n3. Try to write down a sentence of predicate logic which intuitively holds in a\nmodel iﬀthe model has (respectively)\n(a)\n*\nexactly three distinct elements\n(b) at most three distinct elements\n(c)\n*\nonly ﬁnitely many distinct elements.\n2.8 Exercises\n161\nWhat ‘limitation’ of predicate logic causes problems in ﬁnding such a sentence\nfor the last item?\n4. (a) Find a (propositional) proof for φ →(q1 ∧q2) |−(φ →q1) ∧(φ →q2).\n(b) Find a (predicate) proof for φ →∀x Q(x) |−∀x (φ →Q(x)), provided that\nx is not free in φ.\n(Hint: whenever you used ∧rules in the (propositional) proof of the previous\nitem, use ∀rules in the (predicate) proof.)\n(c) Find a proof for ∀x (P(x) →Q(x)) |−∀x P(x) →∀x Q(x).\n(Hint: try (p1 →q1) ∧(p2 →q2) |−p1 ∧p2 →q1 ∧q2 ﬁrst.)",
    "x is not free in φ.\n(Hint: whenever you used ∧rules in the (propositional) proof of the previous\nitem, use ∀rules in the (predicate) proof.)\n(c) Find a proof for ∀x (P(x) →Q(x)) |−∀x P(x) →∀x Q(x).\n(Hint: try (p1 →q1) ∧(p2 →q2) |−p1 ∧p2 →q1 ∧q2 ﬁrst.)\n5. Find a propositional logic sequent that corresponds to ∃x ¬φ ⊢¬∀x φ. Prove it.\n6. Provide proofs for the following sequents:\n(a) ∀x P(x) ⊢∀y P(y); using ∀x P(x) as a premise, your proof needs to end with\nan application of ∀i which requires the formula P(y0).\n(b) ∀x (P(x) →Q(x)) ⊢(∀x ¬Q(x)) →(∀x ¬P(x))\n(c) ∀x (P(x) →¬Q(x)) ⊢¬(∃x (P(x) ∧Q(x))).\n7. The sequents below look a bit tedious, but in proving their validity you make\nsure that you really understand how to nest the proof rules:\n(a)\n*\n∀x ∀y P(x, y) |−∀u ∀v P(u, v)\n(b) ∃x ∃y F(x, y) |−∃u ∃v F(u, v)\n(c)\n*\n∃x ∀y P(x, y) |−∀y ∃x P(x, y).\n8. In this exercise, whenever you use a proof rule for quantiﬁers, you should men-\ntion how its side condition (if applicable) is satisﬁed.\n(a) Prove 2(b-h) of Theorem 2.13 from page 117.\n(b) Prove one direction of 1(b) of Theorem 2.13: ¬∃x φ ⊢∀x ¬φ.\n(c) Prove 3(a) of Theorem 2.13: (∀x φ) ∧(∀x ψ) ⊣⊢∀x (φ ∧ψ); recall that you\nhave to do two separate proofs.\n(d) Prove both directions of 4(a) of Theorem 2.13: ∀x ∀y φ ⊣⊢∀y ∀x φ.\n9. Prove the validity of the following sequents in predicate logic, where F, G, P,\nand Q have arity 1, and S has arity 0 (a ‘propositional atom’):\n(a)\n*\n∃x (S →Q(x)) |−S →∃x Q(x)\n(b) S →∃x Q(x) |−∃x (S →Q(x))\n(c) ∃x P(x) →S |−∀x (P(x) →S)\n(d)\n*\n∀x P(x) →S |−∃x (P(x) →S)\n(e) ∀x (P(x) ∨Q(x)) |−∀x P(x) ∨∃x Q(x)\n(f) ∀x ∃y (P(x) ∨Q(y)) |−∃y ∀x (P(x) ∨Q(y))\n(g) ∀x (¬P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))\n(h) ∀x (P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))\n(i) ∃x (¬P(x) ∧¬Q(x)) ⊢∃x (¬(P(x) ∧Q(x)))\n(j) ∃x (¬P(x) ∨Q(x)) ⊢∃x (¬(P(x) ∧¬Q(x)))\n(k)\n*\n∀x (P(x) ∧Q(x)) |−∀x P(x) ∧∀x Q(x).\n(l)\n*\n∀x P(x) ∨∀x Q(x) |−∀x (P(x) ∨Q(x)).\n(m)\n*\n∃x (P(x) ∧Q(x)) |−∃x P(x) ∧∃x Q(x).\n(n)\n*\n∃x F(x) ∨∃x G(x) |−∃x (F(x) ∨G(x)).",
    "(h) ∀x (P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))\n(i) ∃x (¬P(x) ∧¬Q(x)) ⊢∃x (¬(P(x) ∧Q(x)))\n(j) ∃x (¬P(x) ∨Q(x)) ⊢∃x (¬(P(x) ∧¬Q(x)))\n(k)\n*\n∀x (P(x) ∧Q(x)) |−∀x P(x) ∧∀x Q(x).\n(l)\n*\n∀x P(x) ∨∀x Q(x) |−∀x (P(x) ∨Q(x)).\n(m)\n*\n∃x (P(x) ∧Q(x)) |−∃x P(x) ∧∃x Q(x).\n(n)\n*\n∃x F(x) ∨∃x G(x) |−∃x (F(x) ∨G(x)).\n(o) ∀x ∀y (S(y) →F(x)) |−∃yS(y) →∀x F(x).\n162\n2 Predicate logic\n(p)\n*\n¬∀x ¬P(x) |−∃x P(x).\n(q)\n*\n∀x ¬P(x) |−¬∃x P(x).\n(r)\n*\n¬∃x P(x) |−∀x ¬P(x).\n10. Just like natural deduction proofs for propositional logic, certain things that\nlook easy can be hard to prove for predicate logic. Typically, these involve the\n¬¬e rule. The patterns are the same as in propositional logic:\n(a) Proving that p ∨q |−¬(¬p ∧¬q) is valid is quite easy. Try it.\n(b) Show that ∃x P(x) |−¬∀x ¬P(x) is valid.\n(c) Proving that ¬(¬p ∧¬q) |−p ∨q is valid is hard; you have to try to prove\n¬¬(p ∨q) ﬁrst and then use the ¬¬e rule. Do it.\n(d) Re-express the sequent from the previous item such that p and q are unary\npredicates and both formulas are universally quantiﬁed. Prove its validity.\n11. The proofs of the sequents below combine the proof rules for equality and\nquantiﬁers. We write φ ↔ψ as an abbreviation for (φ →ψ) ∧(ψ →φ). Find\nproofs for\n(a)\n*\nP(b) |−∀x (x = b →P(x))\n(b) P(b), ∀x∀y (P(x) ∧P(y) →x = y) |−∀x (P(x) ↔x = b)\n(c)\n*\n∃x ∃y (H(x, y) ∨H(y, x)), ¬∃x H(x, x) |−∃x∃y ¬(x = y)\n(d) ∀x (P(x) ↔x = b) |−P(b) ∧∀x∀y (P(x) ∧P(y) →x = y).\n12.\n*\nProve the validity of S →∀x Q(x) |−∀x (S →Q(x)), where S has arity 0 (a\n‘propositional atom’).\n13. By natural deduction, show the validity of\n(a)\n*\n∀x P(a, x, x), ∀x ∀y ∀z (P(x, y, z) →P(f(x), y, f(z)))\n|−P(f(a), a, f(a))\n(b)\n*\n∀x P(a, x, x), ∀x ∀y ∀z (P(x, y, z) →P(f(x), y, f(z)))\n|−∃z P(f(a), z, f(f(a)))\n(c)\n*\n∀y Q(b, y), ∀x ∀y (Q(x, y) →Q(s(x), s(y)))\n|−∃z (Q(b, z) ∧Q(z, s(s(b))))\n(d) ∀x ∀y ∀z (S(x, y) ∧S(y, z) →S(x, z)), ∀x ¬S(x, x)\n⊢∀x ∀y (S(x, y) →¬S(y, x))\n(e) ∀x (P(x) ∨Q(x)), ∃x ¬Q(x), ∀x (R(x) →¬P(x)) ⊢∃x ¬R(x)",
    "*\n∀x P(a, x, x), ∀x ∀y ∀z (P(x, y, z) →P(f(x), y, f(z)))\n|−∃z P(f(a), z, f(f(a)))\n(c)\n*\n∀y Q(b, y), ∀x ∀y (Q(x, y) →Q(s(x), s(y)))\n|−∃z (Q(b, z) ∧Q(z, s(s(b))))\n(d) ∀x ∀y ∀z (S(x, y) ∧S(y, z) →S(x, z)), ∀x ¬S(x, x)\n⊢∀x ∀y (S(x, y) →¬S(y, x))\n(e) ∀x (P(x) ∨Q(x)), ∃x ¬Q(x), ∀x (R(x) →¬P(x)) ⊢∃x ¬R(x)\n(f) ∀x (P(x) →(Q(x) ∨R(x))), ¬∃x (P(x) ∧R(x)) ⊢∀x (P(x) →Q(x))\n(g) ∃x ∃y (S(x, y) ∨S(y, x)) ⊢∃x ∃y S(x, y)\n(h) ∃x (P(x) ∧Q(x)), ∀y (P(x) →R(x)) ⊢∃x (R(x) ∧Q(x)).\n14. Translate the following argument into a sequent in predicate logic using a suit-\nable set of predicate symbols:\nIf there are any tax payers, then all politicians are tax payers.\nIf there are any philanthropists, then all tax payers are philan-\nthropists. So, if there are any tax-paying philanthropists, then\nall politicians are philanthropists.\nNow come up with a proof of that sequent in predicate logic.\n2.8 Exercises\n163\n15. Discuss in what sense the equivalences of Theorem 2.13 (page 117) form the\nbasis of an algorithm which, given φ, pushes quantiﬁers to the top of the for-\nmula’s parse tree. If the result is ψ, what can you say about commonalities and\ndiﬀerences between φ and ψ?\nExercises 2.4\n1.\n*\nConsider the formula φ\ndef\n= ∀x ∀y Q(g(x, y), g(y, y), z), where Q and g have arity\n3 and 2, respectively. Find two models M and M′ with respective environments\nl and l′ such that M ⊨l φ but M′ ̸⊨l′ φ.\n2. Consider the sentence φ\ndef\n= ∀x ∃y ∃z (P(x, y) ∧P(z, y) ∧(P(x, z) →P(z, x))).\nWhich of the following models satisﬁes φ?\n(a) The model M consists of the set of natural numbers with P M def\n= {(m, n) |\nm < n}.\n(b) The model M′ consists of the set of natural numbers with P M′ def\n= {(m, 2 ∗\nm) | m natural number}.\n(c) The model M′′ consists of the set of natural numbers with P M′′ def\n= {(m, n) |\nm < n + 1}.\n3. Let P be a predicate with two arguments. Find a model which satisﬁes the\nsentence ∀x ¬P(x, x); also ﬁnd one which doesn’t.\n4. Consider the sentence ∀x(∃yP(x, y) ∧(∃zP(z, x) →∀yP(x, y))). Please simu-",
    "(c) The model M′′ consists of the set of natural numbers with P M′′ def\n= {(m, n) |\nm < n + 1}.\n3. Let P be a predicate with two arguments. Find a model which satisﬁes the\nsentence ∀x ¬P(x, x); also ﬁnd one which doesn’t.\n4. Consider the sentence ∀x(∃yP(x, y) ∧(∃zP(z, x) →∀yP(x, y))). Please simu-\nlate the evaluation of this sentence in a model and look-up table of your choice,\nfocusing on how the initial look-up table l grows and shrinks like a stack when\nyou evaluate its subformulas according to the deﬁnition of the satisfaction\nrelation.\n5. Let φ be the sentence ∀x ∀y ∃z (R(x, y) →R(y, z)), where R is a predicate sym-\nbol of two arguments.\n(a)\n*\nLet A\ndef\n= {a, b, c, d} and RM def\n= {(b, c), (b, b), (b, a)}. Do we have M ⊨φ? Jus-\ntify your answer, whatever it is.\n(b)\n*\nLet A′ def\n= {a, b, c} and RM′ def\n= {(b, c), (a, b), (c, b)}. Do we have M′ ⊨φ? Jus-\ntify your answer, whatever it is.\n6.\n*\nConsider the three sentences\nφ1\ndef\n= ∀x P(x, x)\nφ2\ndef\n= ∀x ∀y (P(x, y) →P(y, x))\nφ3\ndef\n= ∀x ∀y ∀z ((P(x, y) ∧P(y, z) →P(x, z)))\nwhich express that the binary predicate P is reﬂexive, symmetric and transitive,\nrespectively. Show that none of these sentences is semantically entailed by the\nother ones by choosing for each pair of sentences above a model which satisﬁes\nthese two, but not the third sentence – essentially, you are asked to ﬁnd three\nbinary relations, each satisfying just two of these properties.\n164\n2 Predicate logic\n7. Show the semantic entailment ∀x ¬φ ⊨¬∃x φ; for that you have to take any\nmodel which satisﬁes ∀x ¬φ and you have to reason why this model must also\nsatisfy ¬∃x φ. You should do this in a similar way to the examples in Sec-\ntion 2.4.2.\n8.\n*\nShow the semantic entailment ∀x P(x) ∨∀x Q(x) ⊨∀x (P(x) ∨Q(x)).\n9. Let φ and ψ and η be sentences of predicate logic.\n(a) If ψ is semantically entailed by φ, is it necessarily the case that ψ is not\nsemantically entailed by ¬φ?\n(b)\n*\nIf ψ is semantically entailed by φ ∧η, is it necessarily the case that ψ is",
    "9. Let φ and ψ and η be sentences of predicate logic.\n(a) If ψ is semantically entailed by φ, is it necessarily the case that ψ is not\nsemantically entailed by ¬φ?\n(b)\n*\nIf ψ is semantically entailed by φ ∧η, is it necessarily the case that ψ is\nsemantically entailed by φ and semantically entailed by η?\n(c) If ψ is semantically entailed by φ or by η, is it necessarily the case that ψ\nis semantically entailed by φ ∨η?\n(d) Explain why ψ is semantically entailed by φ iﬀφ →ψ is valid.\n10. Is ∀x (P(x) ∨Q(x)) ⊨∀x P(x) ∨∀x Q(x) a semantic entailment? Justify your\nanswer.\n11. For each set of formulas below show that they are consistent:\n(a) ∀x ¬S(x, x), ∃x P(x), ∀x ∃y S(x, y), ∀x (P(x) →∃y S(y, x))\n(b)\n*\n∀x ¬S(x, x), ∀x ∃y S(x, y),\n∀x ∀y ∀z ((S(x, y) ∧S(y, z)) →S(x, z))\n(c) (∀x (P(x) ∨Q(x))) →∃y R(y), ∀x (R(x) →Q(x)), ∃y (¬Q(y) ∧P(y))\n(d)\n*\n∃x S(x, x), ∀x ∀y (S(x, y) →(x = y)).\n12. For each of the formulas of predicate logic below, either ﬁnd a model which\ndoes not satisfy it, or prove it is valid:\n(a) (∀x ∀y (S(x, y) →S(y, x))) →(∀x ¬S(x, x))\n(b)\n*\n∃y ((∀x P(x)) →P(y))\n(c) (∀x (P(x) →∃y Q(y))) →(∀x ∃y (P(x) →Q(y)))\n(d) (∀x ∃y (P(x) →Q(y))) →(∀x (P(x) →∃y Q(y)))\n(e) ∀x ∀y (S(x, y) →(∃z (S(x, z) ∧S(z, y))))\n(f) (∀x ∀y (S(x, y) →(x = y))) →(∀z ¬S(z, z))\n(g)\n*\n(∀x ∃y (S(x, y) ∧((S(x, y) ∧S(y, x)) →(x = y)))) →\n(¬∃z ∀w (S(z, w))).\n(h) ∀x ∀y ((P(x) →P(y)) ∧(P(y) →P(x)))\n(i) (∀x ((P(x) →Q(x)) ∧(Q(x) →P(x)))) →((∀x P(x)) →(∀x Q(x)))\n(j) ((∀x P(x)) →(∀x Q(x))) →(∀x ((P(x) →Q(x)) ∧(Q(x) →P(x))))\n(k) Diﬃcult: (∀x ∃y (P(x) →Q(y))) →(∃y ∀x (P(x) →Q(y))).\nExercises 2.5\n1. Assuming that our proof calculus for predicate logic is sound (see exercise 3\nbelow), show that the validity of the following sequents cannot be proved by\nﬁnding for each sequent a model such that all formulas to the left of ⊢evaluate\nto T and the sole formula to the right of ⊢evaluates to F (explain why this\nguarantees the non-existence of a proof):\n2.8 Exercises\n165\n(a) ∀x (P(x) ∨Q(x)) ⊢∀x P(x) ∨∀x Q(x)\n(b)\n*",
    "ﬁnding for each sequent a model such that all formulas to the left of ⊢evaluate\nto T and the sole formula to the right of ⊢evaluates to F (explain why this\nguarantees the non-existence of a proof):\n2.8 Exercises\n165\n(a) ∀x (P(x) ∨Q(x)) ⊢∀x P(x) ∨∀x Q(x)\n(b)\n*\n∀x (P(x) →R(x)), ∀x (Q(x) →R(x)) ⊢∃x (P(x) ∧Q(x))\n(c) (∀x P(x)) →L ⊢∀x (P(x) →L), where L has arity 0\n(d)\n*\n∀x ∃y S(x, y) ⊢∃y ∀x S(x, y)\n(e) ∃x P(x), ∃y Q(y) ⊢∃z (P(z) ∧Q(z)).\n(f)\n*\n∃x (¬P(x) ∧Q(x)) ⊢∀x (P(x) →Q(x))\n(g)\n*\n∃x (¬P(x) ∨¬Q(x)) ⊢∀x (P(x) ∨Q(x)).\n2. Assuming that ⊢is sound and complete for ⊨in ﬁrst-order logic, explain in detail\nwhy the undecidability of ⊨implies that satisﬁability, validity, and provability\nare all undecidable for that logic.\n3. To show the soundness of our natural deduction rules for predicate logic, it\nintuitively suﬃces to show that the conclusion of a proof rule is true provided\nthat all its premises are true. What additional complication arises due to the\npresence of variables and quantiﬁers? Can you precisely formalise the necessary\ninduction hypothesis for proving soundness?\nExercises 2.6\n1. In Example 2.23, page 136, does M ⊨l ∃P φ hold if l satisﬁes\n(a)\n*\nl(u) = s3 and l(v) = s1;\n(b) l(u) = s1 and l(v) = s3?\nJustify your answers.\n2. Prove that M ⊨l ∃P ∀x∀y∀z (C1 ∧C2 ∧C3 ∧C4) holds iﬀstate l(v) is not reach-\nable from state l(u) in the model M, where the Ci are the ones of (2.12) on\npage 139.\n3. Does Theorem 2.26 from page 138 apply or remain valid if we allow φ to contain\nfunction symbols of any ﬁnite arity?\n4.\n*\nIn the directed graph of Figure 2.5 from page 137, how many paths are there\nthat witness the reachability of node s3 from s2?\n5. Let P and R be predicate symbols of arity 2. Write formulas of existential second-\norder logic of the form ∃P ψ that hold in all models of the form M = (A, RM)\niﬀ\n(a)\n*\nR contains a reﬂexive and symmetric relation;\n(b) R contains an equivalence relation\n(c) there is an R-path that visits each node of the graph exactly once – such a",
    "order logic of the form ∃P ψ that hold in all models of the form M = (A, RM)\niﬀ\n(a)\n*\nR contains a reﬂexive and symmetric relation;\n(b) R contains an equivalence relation\n(c) there is an R-path that visits each node of the graph exactly once – such a\npath is called Hamiltonian\n(d) R can be extended to an equivalence relation: there is some equivalence\nrelation T with RM ⊆T\n(e)\n*\nthe relation ‘there is an R-path of length 2’ is transitive.\n6.\n*\nShow informally that (2.16) on page 141 gives rise to Russell’s paradox: A has\nto be, and cannot be, an element of A.\n7. The second item in the proof of Theorem 2.28 (page 140) relies on the fact\nthat if a binary relation R is contained in a reﬂexive, transitive relation T of\n166\n2 Predicate logic\nthe same type, then T also contains the reﬂexive, transitive closure of R. Prove\nthis.\n8. For the model of Example 2.23 and Figure 2.5 (page 137), determine which model\nchecks hold and justify your answer:\n(a)\n*\n∃P (∀x∀y P(x, y) →¬P(y, x)) ∧(∀u∀v R(u, v) →P(v, u));\n(b) ∀P (∃x∃y∃z P(x, y) ∧P(y, z) ∧¬P(x, z)) →(∀u∀v R(u, v) →P(u, v)); and\n(c) ∀P (∀x ¬P(x, x)) ∨(∀u∀v R(u, v) →P(u, v)).\n9. Express the following statements about a binary relation R in predicate\nlogic, universal second-order logic, or existential second-order logic – if at all\npossible:\n(a) All symmetric, transitive relations either don’t contain R or are equivalence\nrelations.\n(b)\n*\nAll nodes are on at least one R-cycle.\n(c) There is a smallest relation containing R which is symmetric.\n(d) There is a smallest relation containing R which is reﬂexive.\n(e)\n*\nThe relation R is a maximal equivalence relation: R is an equivalence relation;\nand there is no relation contained in R that is an equivalence relation.\nExercises 2.7\n1. (a)\n*\nExplain why the model of Figure 2.11 (page 148) is a counterexample to\nOfLovers in the presence of the fact NoSelfLove.\n(b) Can you identify the set {a, b, c} from Example 2.19 (page 128) with the",
    "and there is no relation contained in R that is an equivalence relation.\nExercises 2.7\n1. (a)\n*\nExplain why the model of Figure 2.11 (page 148) is a counterexample to\nOfLovers in the presence of the fact NoSelfLove.\n(b) Can you identify the set {a, b, c} from Example 2.19 (page 128) with the\nmodel of Figure 2.11 such that these two models are structurally the same?\nJustify your answer.\n(c)\n*\nExplain informally why no model with less than three elements can sat-\nisfy (2.8) from page 128 and the fact NoSelfLove.\n2. Use the following fragment of an Alloy module\nmodule AboutGraphs\nsig Element {}\nsig Graph {\nnodes : set Element,\nedges : nodes -> nodes\n}\nfor these modelling tasks:\n(a) Recall Exercise 6 from page 163 and its three sentences, where P(x, y) spec-\niﬁes that there is an edge from x to y. For each sentence, write a consistency\ncheck that attempts to generate a model of a graph in which that sentence\nis false, but the other two are true. Analyze it within Alloy. What it the\nsmallest scope, if any, in which the analyzer ﬁnds a model for this?\n2.8 Exercises\n167\n(b)\n*\n(Recall that the expression # S = n speciﬁes that set S has n elements.)\nUse Alloy to generate a graph with seven nodes such that each node can\nreach exactly ﬁve nodes on ﬁnite paths (not necessarily the same ﬁve\nnodes).\n(c) A cycle of length n is a set of n nodes and a path through each of them,\nbeginning and ending with the same node. Generate a cycle of length 4.\n3. An undirected graph has a set of nodes and a set of edges, except that every\nedge connects two nodes without any sense of direction.\n(a) Adjust the Alloy module from the previous item – e.g. by adding an appro-\npriate fact – to ‘simulate’ undirected graphs.\n(b) Write some consistency and assertion checks and analyze them to boost the\nconﬁdence you may have in your Alloy module of undirected graphs.\n4.\n*\nA colorable graph consists of a set of nodes, a binary symmetric relation (the",
    "priate fact – to ‘simulate’ undirected graphs.\n(b) Write some consistency and assertion checks and analyze them to boost the\nconﬁdence you may have in your Alloy module of undirected graphs.\n4.\n*\nA colorable graph consists of a set of nodes, a binary symmetric relation (the\nedges) between nodes and a function that assigns to each node a color. This\nfunction is subject to the constraint that no nodes have the same color if they\nare related by an edge.\n(a) Write a signature AboutColoredGraphs for this structure and these con-\nstraints.\n(b) Write a fun-statement that generates a graph whose nodes are colored by\ntwo colors only. Such a graph is 2-colorable.\n(c) For eack k = 3, 4 write a fun-statement that generates a graph whose nodes\nare colored by k colors such that all k colors are being used. Such a graph is\nk-colorable.\n(d) Test these three functions in a module.\n(e) Try to write a fun-statement that generates a graph that is 3-colorable but\ndeﬁnitely not 2-colorable. What does Alloy’s model builder report? Consider\nthe formula obtained from that fun-statement’s body by existentially quan-\ntifying that body with all its parameters. Determine whether is belongs to\npredicate logic, existential or universal second-order logic.\n5.\n*\nA Kripke model is a state machine with a non-empty set of initial states init, a\nmapping prop from states to atomic properties (specifying which properties are\ntrue at which states), a state transition relation next, and a set of ﬁnal states\nfinal (states that don’t have a next state). With a module KripkeModel:\n(a) Write a signature StateMachine and some basic facts that reﬂect this struc-\nture and these constraints.\n(b) Write a fun-statement Reaches which takes a state machine as ﬁrst parame-\nter and a set of states as a second parameter such that the second parameter\ndenotes the ﬁrst parameter’s set of states reachable from any initial state.\nNote: Given the type declaration r : T -> T, the expression *r has type T",
    "ter and a set of states as a second parameter such that the second parameter\ndenotes the ﬁrst parameter’s set of states reachable from any initial state.\nNote: Given the type declaration r : T -> T, the expression *r has type T\n-> T as well and denotes the reﬂexive, transitive closure of r.\n(c) Write these fun-statements and check their consistency:\ni. DeadlockFree(m: StateMachine), among the reachable states of m only\nthe final ones can deadlock;\n168\n2 Predicate logic\nState_0\nprop: Prop_1\nnext\nState_2\nprop: Prop_0\nnext\nState_1\nFigure 2.15. A snapshot of a non-deterministic state machine in which\nno non-final state deadlocks and where states that satisfy the same\nproperties are identical.\nii. Deterministic(m: StateMachine), at all reachable states of m the state\ntransition relation is deterministic: each state has at most one outgoing\ntransition;\niii. Reachability(m: StateMachine, p: Prop), some state which has\nproperty p can be reached in m; and\niv. Liveness(m: StateMachine, p: Prop),\nno\nmatter\nwhich\nstate\nm\nreaches, it can – from that state – reach a state in which p holds.\n(d) i. Write an assertion Implies which says that whenever a state machine\nsatisﬁes Liveness for a property then it also satisﬁes Reachability for\nthat property.\nii. Analyze that assertion in a scope of your choice. What conclusions can you\ndraw from the analysis’ ﬁndings?\n(e) Write an assertion Converse which states that Reachability of a property\nimplies its Liveness. Analyze it in a scope of 3. What do you conclude, based\non the analysis’ result?\n(f) Write a fun-statement that, when analyzed, generates a statemachine with\ntwo propositions and three states such that it satisﬁes the statement of the\nsentence in the caption of Figure 2.15.\n6.\n*\nGroups are the bread and butter of cryptography and group operations are ap-\nplied in the silent background when you use PUTTY, Secure Socket Layers etc.\nA group is a tuple (G, ⋆, 1), where ⋆: G × G →G is a function and 1 ∈G such\nthat",
    "sentence in the caption of Figure 2.15.\n6.\n*\nGroups are the bread and butter of cryptography and group operations are ap-\nplied in the silent background when you use PUTTY, Secure Socket Layers etc.\nA group is a tuple (G, ⋆, 1), where ⋆: G × G →G is a function and 1 ∈G such\nthat\nG1 for every x ∈G there is some y ∈G such that x ⋆y = y ⋆x = 1 (any such y\nis called an inverse of x);\nG2 for all x, y, z ∈G, we have x ⋆(y ⋆z) = (x ⋆y) ⋆z; and\nG3 for all x ∈G, we have x ⋆1 = 1 ⋆x = x.\n(a) Specify a signature for groups that realizes this functionality and its con-\nstraints.\n(b) Write a fun-statement AGroup that generates a group with three elements.\n(c) Write an assertion Inverse saying that inverse elements are unique. Check\nit in the scope of 5. Report your ﬁndings. What would the small scope hy-\npothesis suggest?\n2.8 Exercises\n169\n(d)\ni. Write an assertion Commutative saying that all groups are commutative.\nA group is commutative iﬀx ⋆y = y ⋆x for all its elements x and y.\nii. Check the assertion Commutative in scope 5 and report your ﬁndings.\nWhat would the small scope hypothesis suggest?\niii. Re-check assertion Commutative in scope 6 and record how long the tool\ntakes to ﬁnd a solution. What lesson(s) do you learn from this?\n(e) For the functions and assertions above, is it safe to restrict the scope for\ngroups to 1? And how does one do this in Alloy?\n7. In Alloy, one can extend a signature. For example, we may declare\nsig Program extends PDS {\nm : components\n-- initial main of PDS\n}\nThis declares instances of Program to be of type PDS, but to also possess a\ndesignated component named m. Observe how the occurrence of components\nin m : components refers to the set of components of a program, viewed as a\nPDS5. In this exercise, you are asked to modify the Alloy module of Figure 2.13\non page 154.\n(a) Include a signature Program as above. Add a fact stating that all programs’\ndesignated component has a main method; and for all programs, their set",
    "PDS5. In this exercise, you are asked to modify the Alloy module of Figure 2.13\non page 154.\n(a) Include a signature Program as above. Add a fact stating that all programs’\ndesignated component has a main method; and for all programs, their set\nof components is the reﬂexive, transitive closure of their relation requires\napplied to the designated component m. Alloy uses *r to denote the reﬂexive,\ntransitive closure of relation r.\n(b) Write a guided simulation that, if consistent, produces a model with three\nPDSs, exactly one of them being a program. The program has four compo-\nnents – including the designated m – all of which schedule services from the\nremaining three components. Use Alloy’s analyzer to detemine whether your\nsimulation is consistent and compliant with the speciﬁcation given in this\nitem.\n(c) Let’s say that a component of a program is garbage for that program if\nno service reachable from the main service of m via requires schedules that\ncomponent. Explain whether, and if so how, the constraints of AddComponent\nand RemoveComponent already enforce the presence of ‘garbage collection’ if\nthe instances of P and P’ are constrained to be programs.\n8. Recall our discussion of existential and universal second-order logic from Sec-\ntion 2.6. Then study the structure of the fun-statements and assertions in Fig-\nure 2.13 on page 154. As you may know, Alloy analyzes such statements by de-\nriving from them a formula for which it tries to ﬁnd a model within the speciﬁed\nscope: the negation of the body of an assertion; or the body of a fun-statement,\nexistentially quantiﬁed with all its parameters. For each of these derived formulas,\n5 In most object-oriented languages, e.g. Java, extends creates a new type. In Alloy 2.0 and 2.1, it\ncreates a subset of a type and not a new type as such, where the subset has additional structure\nand may need to satisfy additional constraints.\n170\n2 Predicate logic",
    "5 In most object-oriented languages, e.g. Java, extends creates a new type. In Alloy 2.0 and 2.1, it\ncreates a subset of a type and not a new type as such, where the subset has additional structure\nand may need to satisfy additional constraints.\n170\n2 Predicate logic\ndetermine whether they can be expressed in ﬁrst-order logic, existential second-\norder logic or universal second-order logic.\n9. Recalling the comment on page 142 that Alloy combines model checking M ⊨φ\nand validity checking Γ ⊨φ, can you discuss to what extent this is so?\n2.9 Bibliographic notes\nMany design decisions have been taken in the development of predicate\nlogic in the form known today. The Greeks and the medievals had systems\nin which many of the examples and exercises in this book could be rep-\nresented, but nothing that we would recognise as predicate logic emerged\nuntil the work of Gottlob Frege in 1879, printed in [Fre03]. An account of\nthe contributions of the many other people involved in the development of\nlogic can be found in the ﬁrst few pages of W. Hodges’ chapter in [Hod83].\nThere are many books covering classical logic and its use in computer sci-\nence; we give a few incomplete pointers to the literature. The books [SA91],\n[vD89] and [Gal87] cover more theoretical applications than those in this\nbook, including type theory, logic programming, algebraic speciﬁcation and\nterm-rewriting systems. An approach focusing on automatic theorem prov-\ning is taken by [Fit96]. Books which study the mathematical aspects of\npredicate logic in greater detail, such as completeness of the proof systems\nand incompleteness of ﬁrst-order arithmetic, include [Ham78] and [Hod83].\nMost of these books present other proof systems besides natural deduction\nsuch as axiomatic systems and tableau systems. Although natural deduction\nhas the advantages of elegance and simplicity over axiomatic methods, there\nare few expositions of it in logic books aimed at a computer science audi-",
    "Most of these books present other proof systems besides natural deduction\nsuch as axiomatic systems and tableau systems. Although natural deduction\nhas the advantages of elegance and simplicity over axiomatic methods, there\nare few expositions of it in logic books aimed at a computer science audi-\nence. One exception to this is the book [BEKV94], which is the ﬁrst one to\npresent the rules for quantiﬁers in the form we used here. A natural deduc-\ntion theorem prover called Jape has been developed, in which one can vary\nthe set of available rules and specify new ones6.\nA standard reference for computability theory is [BJ80]. A proof for the\nundecidability of the Post correspondence problem can be found in the text\nbook [Tay98]. The second instance of a Post correspondence problem is taken\nfrom [Sch92]. A text on the fundamentals of databases systems is [EN94].\nThe discussion of Section 2.6 is largely based on the text [Pap94] which\nwe highly recommend if you mean to ﬁnd out more about the intimate\nconnections between logic and computational complexity.\n6 www.comlab.ox.ac.uk/oucl/users/bernard.sufrin/jape.html\n2.9 Bibliographic notes\n171\nThe source code of all complete Alloy modules from this chapter (work-\ning under Alloy 2.0 and 2.1) as well as source code compliant with Alloy\n3.0 are available under ‘ancillary material’ at the book’s website. The PDS\nmodel grew out of a coursework set in the Fall 2002 for C475 Software En-\ngineering Environments, co-taught by Susan Eisenbach and the ﬁrst author;\na published model customized for the .NET global assembly cache will\nappeared in [EJC03]. The modelling language Alloy and its constraint\nanalyzer [JSS01] have been developed by D. Jackson and his Software\nDesign Group at the Laboratory for Computer Science at the Massachusetts\nInstitute of Technology. The tool has a dedicated repository website at\nalloy.mit.edu.\nMore information on typed higher-order logics and their use in the",
    "analyzer [JSS01] have been developed by D. Jackson and his Software\nDesign Group at the Laboratory for Computer Science at the Massachusetts\nInstitute of Technology. The tool has a dedicated repository website at\nalloy.mit.edu.\nMore information on typed higher-order logics and their use in the\nmodelling and verifying of programming frameworks can be found on F.\nPfenning’s course homepage7 on Computation and Deduction.\n7 www-2.cs.cmu.edu/~fp/courses/comp-ded/\n3\nVerification by model checking\n3.1 Motivation for verification\nThere is a great advantage in being able to verify the correctness of computer\nsystems, whether they are hardware, software, or a combination. This is most\nobvious in the case of safety-critical systems, but also applies to those that\nare commercially critical, such as mass-produced chips, mission critical, etc.\nFormal veriﬁcation methods have quite recently become usable by industry\nand there is a growing demand for professionals able to apply them. In this\nchapter, and the next one, we examine two applications of logics to the\nquestion of verifying the correctness of computer systems, or programs.\nFormal veriﬁcation techniques can be thought of as comprising three\nparts:\nr a framework for modelling systems, typically a description language of some sort;\nr a speciﬁcation language for describing the properties to be veriﬁed;\nr a veriﬁcation method to establish whether the description of a system satisﬁes\nthe speciﬁcation.\nApproaches to veriﬁcation can be classiﬁed according to the following\ncriteria:\nProof-based vs. model-based. In a proof-based approach, the system\ndescription is a set of formulas Γ (in a suitable logic) and the speciﬁcation\nis another formula φ. The veriﬁcation method consists of trying to ﬁnd\na proof that Γ |−φ. This typically requires guidance and expertise from\nthe user.\nIn a model-based approach, the system is represented by a model M for\nan appropriate logic. The speciﬁcation is again represented by a formula",
    "is another formula φ. The veriﬁcation method consists of trying to ﬁnd\na proof that Γ |−φ. This typically requires guidance and expertise from\nthe user.\nIn a model-based approach, the system is represented by a model M for\nan appropriate logic. The speciﬁcation is again represented by a formula\nφ and the veriﬁcation method consists of computing whether a model\nM satisﬁes φ (written M ⊨φ). This computation is usually automatic\nfor ﬁnite models.\n172\n3.1 Motivation for verification\n173\nIn Chapters 1 and 2, we could see that logical proof systems are often\nsound and complete, meaning that Γ |−φ (provability) holds if, and only\nif, Γ ⊨φ (semantic entailment) holds, where the latter is deﬁned as fol-\nlows: for all models M, if for all ψ ∈Γ we have M ⊨ψ, then M ⊨φ.\nThus, we see that the model-based approach is potentially simpler than\nthe proof-based approach, for it is based on a single model M rather\nthan a possibly inﬁnite class of them.\nDegree of automation. Approaches\ndiﬀer\non\nhow\nautomatic\nthe\nmethod is; the extremes are fully automatic and fully manual. Many\nof the computer-assisted techniques are somewhere in the middle.\nFull- vs. property-veriﬁcation. The speciﬁcation may describe a sin-\ngle property of the system, or it may describe its full behaviour. The\nlatter is typically expensive to verify.\nIntended domain of application, which may be hardware or software;\nsequential or concurrent; reactive or terminating; etc. A reactive system\nis one which reacts to its environment and is not meant to terminate\n(e.g., operating systems, embedded systems and computer hardware).\nPre- vs. post-development. Veriﬁcation is of greater advantage if in-\ntroduced early in the course of system development, because errors\ncaught earlier in the production cycle are less costly to rectify. (It is\nalleged that Intel lost millions of dollars by releasing their Pentium chip\nwith the FDIV error.)\nThis chapter concerns a veriﬁcation method called model checking. In",
    "caught earlier in the production cycle are less costly to rectify. (It is\nalleged that Intel lost millions of dollars by releasing their Pentium chip\nwith the FDIV error.)\nThis chapter concerns a veriﬁcation method called model checking. In\nterms of the above classiﬁcation, model checking is an automatic, model-\nbased, property-veriﬁcation approach. It is intended to be used for concur-\nrent, reactive systems and originated as a post-development methodology.\nConcurrency bugs are among the most diﬃcult to ﬁnd by testing (the activ-\nity of running several simulations of important scenarios), since they tend to\nbe non-reproducible or not covered by test cases, so it is well worth having\na veriﬁcation technique that can help one to ﬁnd them.\nThe Alloy system described in Chapter 2 is also an automatic, model-\nbased, property-veriﬁcation approach. The way models are used is slightly\ndiﬀerent, however. Alloy ﬁnds models which form counterexamples to asser-\ntions made by the user. Model checking starts with a model described by\nthe user, and discovers whether hypotheses asserted by the user are valid\non the model. If they are not, it can produce counterexamples, consisting of\nexecution traces. Another diﬀerence between Alloy and model checking is\nthat model checking (unlike Alloy) focuses explicitly on temporal properties\nand the temporal evolution of systems.\n174\n3 Verification by model checking\nBy contrast, Chapter 4 describes a very diﬀerent veriﬁcation technique\nwhich in terms of the above classiﬁcation is a proof-based, computer-assisted,\nproperty-veriﬁcation approach. It is intended to be used for programs which\nwe expect to terminate and produce a result.\nModel checking is based on temporal logic. The idea of temporal logic is\nthat a formula is not statically true or false in a model, as it is in propo-\nsitional and predicate logic. Instead, the models of temporal logic contain\nseveral states and a formula can be true in some states and false in others.",
    "that a formula is not statically true or false in a model, as it is in propo-\nsitional and predicate logic. Instead, the models of temporal logic contain\nseveral states and a formula can be true in some states and false in others.\nThus, the static notion of truth is replaced by a dynamic one, in which the\nformulas may change their truth values as the system evolves from state\nto state. In model checking, the models M are transition systems and the\nproperties φ are formulas in temporal logic. To verify that a system satisﬁes\na property, we must do three things:\nr model the system using the description language of a model checker, arriving at\na model M;\nr code the property using the speciﬁcation language of the model checker, resulting\nin a temporal logic formula φ;\nr Run the model checker with inputs M and φ.\nThe model checker outputs the answer ‘yes’ if M ⊨φ and ‘no’ otherwise; in\nthe latter case, most model checkers also produce a trace of system behaviour\nwhich causes this failure. This automatic generation of such ‘counter traces’\nis an important tool in the design and debugging of systems.\nSince model checking is a model-based approach, in terms of the classiﬁca-\ntion given earlier, it follows that in this chapter, unlike in the previous two,\nwe will not be concerned with semantic entailment (Γ ⊨φ), or with proof\ntheory (Γ ⊢φ), such as the development of a natural deduction calculus for\ntemporal logic. We will work solely with the notion of satisfaction, i.e. the\nsatisfaction relation between a model and a formula (M ⊨φ).\nThere is a whole zoo of temporal logics that people have proposed and\nused for various things. The abundance of such formalisms may be organised\nby classifying them according to their particular view of ‘time.’ Linear-\ntime logics think of time as a set of paths, where a path is a sequence of\ntime instances. Branching-time logics represent time as a tree, rooted at the\npresent moment and branching out into the future. Branching time appears",
    "by classifying them according to their particular view of ‘time.’ Linear-\ntime logics think of time as a set of paths, where a path is a sequence of\ntime instances. Branching-time logics represent time as a tree, rooted at the\npresent moment and branching out into the future. Branching time appears\nto make the non-deterministic nature of the future more explicit. Another\nquality of time is whether we think of it as being continuous or discrete.\nThe former would be suggested if we study an analogue computer, the latter\nmight be preferred for a synchronous network.\n3.2 Linear-time temporal logic\n175\nTemporal logics have a dynamic aspect to them, since the truth of a\nformula is not ﬁxed in a model, as it is in predicate or propositional logic,\nbut depends on the time-point inside the model. In this chapter, we study\na logic where time is linear, called Linear-time Temporal Logic (LTL), and\nanother where time is branching, namely Computation Tree Logic (CTL).\nThese logics have proven to be extremely fruitful in verifying hardware and\ncommunication protocols; and people are beginning to apply them to the\nveriﬁcation of software. Model checking is the process of computing an answer\nto the question of whether M, s ⊨φ holds, where φ is a formula of one of\nthese logics, M is an appropriate model of the system under consideration,\ns is a state of that model and ⊨is the underlying satisfaction relation.\nModels like M should not be confused with an actual physical system.\nModels are abstractions that omit lots of real features of a physical system,\nwhich are irrelevant to the checking of φ. This is similar to the abstractions\nthat one does in calculus or mechanics. There we talk about straight lines,\nperfect circles, or an experiment without friction. These abstractions are\nvery powerful, for they allow us to focus on the essentials of our particular\nconcern.\n3.2 Linear-time temporal logic\nLinear-time temporal logic, or LTL for short, is a temporal logic, with con-",
    "perfect circles, or an experiment without friction. These abstractions are\nvery powerful, for they allow us to focus on the essentials of our particular\nconcern.\n3.2 Linear-time temporal logic\nLinear-time temporal logic, or LTL for short, is a temporal logic, with con-\nnectives that allow us to refer to the future. It models time as a sequence of\nstates, extending inﬁnitely into the future. This sequence of states is some-\ntimes called a computation path, or simply a path. In general, the future is\nnot determined, so we consider several paths, representing diﬀerent possible\nfutures, any one of which might be the ‘actual’ path that is realised.\nWe work with a ﬁxed set Atoms of atomic formulas (such as p, q, r, . . . , or\np1, p2, . . . ). These atoms stand for atomic facts which may hold of a system,\nlike ‘Printer Q5 is busy,’ or ‘Process 3259 is suspended,’ or ‘The content of\nregister R1 is the integer value 6.’ The choice of atomic descriptions obvi-\nously depends on our particular interest in a system at hand.\n3.2.1 Syntax of LTL\nDeﬁnition 3.1 Linear-time temporal logic (LTL) has the following syntax\ngiven in Backus Naur form:\nφ ::= ⊤| ⊥| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ)\n| (X φ) | (F φ) | (G φ) | (φ U φ) | (φ W φ) | (φ R φ)\n(3.1)\nwhere p is any propositional atom from some set Atoms.\n176\n3 Verification by model checking\n→\nr\n∨\nF\np\nG\nq\n¬\nU\np\nFigure 3.1. The parse tree of (F (p →G r) ∨((¬q) U p)).\nThus, the symbols ⊤and ⊥are LTL formulas, as are all atoms from Atoms;\nand ¬φ is an LTL formula if φ is one, etc. The connectives X, F, G, U, R,\nand W are called temporal connectives. X means ‘neXt state,’ F means ‘some\nFuture state,’ and G means ‘all future states (Globally).’ The next three, U,\nR and W are called ‘Until,’ ‘Release’ and ‘Weak-until’ respectively. We will\nlook at the precise meaning of all these connectives in the next section; for\nnow, we concentrate on their syntax.\nHere are some examples of LTL formulas:\nr (((F p) ∧(G q)) →(p W r))",
    "R and W are called ‘Until,’ ‘Release’ and ‘Weak-until’ respectively. We will\nlook at the precise meaning of all these connectives in the next section; for\nnow, we concentrate on their syntax.\nHere are some examples of LTL formulas:\nr (((F p) ∧(G q)) →(p W r))\nr (F (p →(G r)) ∨((¬q) U p)), the parse tree of this formula is illustrated in\nFigure 3.1.\nr (p W (q W r))\nr ((G (F p)) →(F (q ∨s))).\nIt’s boring to write all those brackets, and makes the formulas hard to read.\nMany of them can be omitted without introducing ambiguities; for example,\n(p →(F q)) could be written p →F q without ambiguity. Others, however,\nare required to resolve ambiguities. In order to omit some of those, we assume\nsimilar binding priorities for the LTL connectives to those we assumed for\npropositional and predicate logic.\n3.2 Linear-time temporal logic\n177\nq\np\nG\nr\nF\n→\n∨\np\nU\n¬\nFigure 3.2. The parse tree of F p →G r ∨¬q U p, assuming binding pri-\norities of Convention 3.2.\nConvention 3.2 The unary connectives (consisting of ¬ and the temporal\nconnectives X, F and G) bind most tightly. Next in the order come U, R\nand W; then come ∧and ∨; and after that comes →.\nThese binding priorities allow us to drop some brackets without introduc-\ning ambiguity. The examples above can be written:\nr F p ∧G q →p W r\nr F (p →G r) ∨¬q U p\nr p W (q W r)\nr G F p →F (q ∨s).\nThe brackets we retained were in order to override the priorities of Conven-\ntion 3.2, or to disambiguate cases which the convention does not resolve.\nFor example, with no brackets at all, the second formula would become\nF p →G r ∨¬q U p, corresponding to the parse tree of Figure 3.2, which is\nquite diﬀerent.\nThe following are not well-formed formulas:\nr U r – since U is binary, not unary\nr p G q – since G is unary, not binary.\n178\n3 Verification by model checking\nDeﬁnition 3.3 A subformula of an LTL formula φ is any formula ψ whose\nparse tree is a subtree of φ’s parse tree.",
    "quite diﬀerent.\nThe following are not well-formed formulas:\nr U r – since U is binary, not unary\nr p G q – since G is unary, not binary.\n178\n3 Verification by model checking\nDeﬁnition 3.3 A subformula of an LTL formula φ is any formula ψ whose\nparse tree is a subtree of φ’s parse tree.\nThe subformulas of p W (q U r), e.g., are p, q, r, q U r and p W (q U r).\n3.2.2 Semantics of LTL\nThe kinds of systems we are interested in verifying using LTL may be\nmodelled as transition systems. A transition system models a system by\nmeans of states (static structure) and transitions (dynamic structure). More\nformally:\nDeﬁnition 3.4 A transition system M = (S, →, L) is a set of states S\nendowed with a transition relation\n→(a binary relation on S), such\nthat every s ∈S has some s′ ∈S with s →s′, and a labelling function\nL: S →P(Atoms).\nTransition systems are also simply called models in this chapter. So a model\nhas a collection of states S, a relation →, saying how the system can move\nfrom state to state, and, associated with each state s, one has the set of\natomic propositions L(s) which are true at that particular state. We write\nP(Atoms) for the power set of Atoms, a collection of atomic descriptions.\nFor example, the power set of {p, q} is {∅, {p}, {q}, {p, q}}. A good way of\nthinking about L is that it is just an assignment of truth values to all the\npropositional atoms, as it was the case for propositional logic (we called\nthat a valuation). The diﬀerence now is that we have more than one state,\nso this assignment depends on which state s the system is in: L(s) contains\nall atoms which are true in state s.\nWe may conveniently express all the information about a (ﬁnite) tran-\nsition system M using directed graphs whose nodes (which we call states)\ncontain all propositional atoms that are true in that state. For example, if\nour system has only three states s0, s1 and s2; if the only possible transi-\ntions between states are s0 →s1, s0 →s2, s1 →s0, s1 →s2 and s2 →s2;",
    "sition system M using directed graphs whose nodes (which we call states)\ncontain all propositional atoms that are true in that state. For example, if\nour system has only three states s0, s1 and s2; if the only possible transi-\ntions between states are s0 →s1, s0 →s2, s1 →s0, s1 →s2 and s2 →s2;\nand if L(s0) = {p, q}, L(s1) = {q, r} and L(s2) = {r}, then we can condense\nall this information into Figure 3.3. We prefer to present models by means\nof such pictures whenever that is feasible.\nThe requirement in Deﬁnition 3.4 that for every s ∈S there is at least\none s′ ∈S such that s →s′ means that no state of the system can ‘dead-\nlock.’ This is a technical convenience, and in fact it does not represent any\nreal restriction on the systems we can model. If a system did deadlock, we\ncould always add an extra state sd representing deadlock, together with new\n3.2 Linear-time temporal logic\n179\ns0\np, q\ns1\nq, r\ns2\nr\nFigure 3.3. A concise representation of a transition system M =\n(S, →,L) as a directed graph. We label state s with l iff l ∈L(s).\ns1\ns3\ns0\ns2\ns4\ns1\ns3\ns0\ns2\ns4\nsd\nFigure 3.4. On the left, we have a system with a state s4 that does not\nhave any further transitions. On the right, we expand that system with a\n‘deadlock’ state sd such that no state can deadlock; of course, it is then\nour understanding that reaching the ‘deadlock’ state sd corresponds to\ndeadlock in the original system.\ntransitions s →sd for each s which was a deadlock in the old system, as well\nas sd →sd. See Figure 3.4 for such an example.\nDeﬁnition 3.5 A path in a model M = (S, →, L) is an inﬁnite sequence of\nstates s1, s2, s3, . . . in S such that, for each i ≥1, si →si+1. We write the\npath as s1 →s2 →. . . .\nConsider the path π = s1 →s2 →. . . . It represents a possible future of\nour system: ﬁrst it is in state s1, then it is in state s2, and so on. We write\nπi for the suﬃx starting at si, e.g., π3 is s3 →s4 →. . . .\n180\n3 Verification by model checking\np, q\nr\nr\nr\nq, r\np, q\nq, r\ns0\ns2\ns2\ns2\ns0\ns1",
    "Consider the path π = s1 →s2 →. . . . It represents a possible future of\nour system: ﬁrst it is in state s1, then it is in state s2, and so on. We write\nπi for the suﬃx starting at si, e.g., π3 is s3 →s4 →. . . .\n180\n3 Verification by model checking\np, q\nr\nr\nr\nq, r\np, q\nq, r\ns0\ns2\ns2\ns2\ns0\ns1\ns1\nr\ns2\nr\ns2\nFigure 3.5. Unwinding the system of Figure 3.3 as an infinite tree of\nall computation paths beginning in a particular state.\nIt is useful to visualise all possible computation paths from a given state\ns by unwinding the transition system to obtain an inﬁnite computation tree.\nFor example, if we unwind the state graph of Figure 3.3 for the designated\nstarting state s0, then we get the inﬁnite tree in Figure 3.5. The execu-\ntion paths of a model M are explicitly represented in the tree obtained by\nunwinding the model.\nDeﬁnition 3.6 Let M = (S, →, L) be a model and π = s1 →. . . be a path\nin M. Whether π satisﬁes an LTL formula is deﬁned by the satisfaction\nrelation ⊨as follows:\n1.\nπ ⊨⊤\n2.\nπ ̸⊨⊥\n3.\nπ ⊨p iﬀp ∈L(s1)\n4.\nπ ⊨¬φ iﬀπ ̸⊨φ\n5.\nπ ⊨φ1 ∧φ2 iﬀπ ⊨φ1 and π ⊨φ2\n6.\nπ ⊨φ1 ∨φ2 iﬀπ ⊨φ1 or π ⊨φ2\n7.\nπ ⊨φ1 →φ2 iﬀπ ⊨φ2 whenever π ⊨φ1\n8.\nπ ⊨X φ iﬀπ2 ⊨φ\n9.\nπ ⊨G φ iﬀ, for all i ≥1, πi ⊨φ\n3.2 Linear-time temporal logic\n181\ns0\ns1\ns2\ns3\ns4\ns5\ns6\ns7\ns8\ns9\ns10\n\u0001\n\u0002\u0003\n\u0004\np\nq\n. . .\nFigure 3.6. An illustration of the meaning of Until in the semantics of\nLTL. Suppose p is satisfied at (and only at) s3, s4, s5, s6, s7, s8 and q is\nsatisfied at (and only at) s9. Only the states s3 to s9 each satisfy p U q\nalong the path shown.\n10.\nπ ⊨F φ iﬀthere is some i ≥1 such that πi ⊨φ\n11.\nπ ⊨φ U ψ iﬀthere is some i ≥1 such that πi ⊨ψ and for all j = 1, . . . , i −1\nwe have πj ⊨φ\n12.\nπ ⊨φ W ψ iﬀeither there is some i ≥1 such that πi ⊨ψ and for all j =\n1, . . . , i −1 we have πj ⊨φ; or for all k ≥1 we have πk ⊨φ\n13.\nπ ⊨φ R ψ iﬀeither there is some i ≥1 such that πi ⊨φ and for all j = 1, . . . , i\nwe have πj ⊨ψ, or for all k ≥1 we have πk ⊨ψ.",
    "we have πj ⊨φ\n12.\nπ ⊨φ W ψ iﬀeither there is some i ≥1 such that πi ⊨ψ and for all j =\n1, . . . , i −1 we have πj ⊨φ; or for all k ≥1 we have πk ⊨φ\n13.\nπ ⊨φ R ψ iﬀeither there is some i ≥1 such that πi ⊨φ and for all j = 1, . . . , i\nwe have πj ⊨ψ, or for all k ≥1 we have πk ⊨ψ.\nClauses 1 and 2 reﬂect the facts that ⊤is always true, and ⊥is always false.\nClauses 3–7 are similar to the corresponding clauses we saw in propositional\nlogic. Clause 8 removes the ﬁrst state from the path, in order to create a\npath starting at the ‘next’ (second) state.\nNotice that clause 3 means that atoms are evaluated in the ﬁrst state along\nthe path in consideration. However, that doesn’t mean that all the atoms\noccuring in an LTL formula refer to the ﬁrst state of the path; if they are in\nthe scope of a temporal connective, e.g., in G (p →X q), then the calculation\nof satisfaction involves taking suﬃces of the path in consideration, and the\natoms refer to the ﬁrst state of those suﬃces.\nLet’s now look at clauses 11–13, which deal with the binary temporal\nconnectives. U, which stands for ‘Until,’ is the most commonly encountered\none of these. The formula φ1 U φ2 holds on a path if it is the case that φ1\nholds continuously until φ2 holds. Moreover, φ1 U φ2 actually demands that\nφ2 does hold in some future state. See Figure 3.6 for illustration: each of the\nstates s3 to s9 satisﬁes p U q along the path shown, but s0 to s2 don’t.\nThe other binary connectives are W, standing for ‘Weak-until,’ and R,\nstanding for ‘Release.’ Weak-until is just like U, except that φ W ψ does not\nrequire that ψ is eventually satisﬁed along the path in question, which is\nrequired by φ U ψ. Release R is the dual of U; that is, φ R ψ is equivalent to\n¬(¬φ U ¬ψ). It is called ‘Release’ because clause 11 determines that ψ must\nremain true up to and including the moment when φ becomes true (if there\nis one); φ ‘releases’ ψ. R and W are actually quite similar; the diﬀerences",
    "¬(¬φ U ¬ψ). It is called ‘Release’ because clause 11 determines that ψ must\nremain true up to and including the moment when φ becomes true (if there\nis one); φ ‘releases’ ψ. R and W are actually quite similar; the diﬀerences\nare that they swap the roles of φ and ψ, and the clause for W has an i −1\n182\n3 Verification by model checking\nwhere R has i. Since they are similar, why do we need both? We don’t; they\nare interdeﬁnable, as we will see later. However, it’s useful to have both. R\nis useful because it is the dual of U, while W is useful because it is a weak\nform of U.\nNote that neither the strong version (U) or the weak version (W) of until\nsays anything about what happens after the until has been realised. This\nis in contrast with some of the readings of ‘until’ in natural language. For\nexample, in the sentence ‘I smoked until I was 22’ it is not only expressed\nthat the person referred to continually smoked up until he or she was 22\nyears old, but we also would interpret such a sentence as saying that this\nperson gave up smoking from that point onwards. This is diﬀerent from the\nsemantics of until in temporal logic. We could express the sentence about\nsmoking by combining U with other connectives; for example, by asserting\nthat it was once true that s U (t ∧G ¬s), where s represents ‘I smoke’ and\nt represents ‘I am 22.’\nRemark 3.7 Notice that, in clauses 9–13 above, the future includes the\npresent. This means that, when we say ‘in all future states,’ we are including\nthe present state as a future state. It is a matter of convention whether we\ndo this, or not. As an exercise, you may consider developing a version of\nLTL in which the future excludes the present. A consequence of adopting\nthe convention that the future shall include the present is that the formulas\nG p →p, p →q U p and p →F p are true in every state of every model.\nSo far we have deﬁned a satisfaction relation between paths and LTL for-",
    "LTL in which the future excludes the present. A consequence of adopting\nthe convention that the future shall include the present is that the formulas\nG p →p, p →q U p and p →F p are true in every state of every model.\nSo far we have deﬁned a satisfaction relation between paths and LTL for-\nmulas. However, to verify systems, we would like to say that a model as\na whole satisﬁes an LTL formula. This is deﬁned to hold whenever every\npossible execution path of the model satisﬁes the formula.\nDeﬁnition 3.8 Suppose M = (S, →, L) is a model, s ∈S, and φ an LTL\nformula. We write M, s ⊨φ if, for every execution path π of M starting at\ns, we have π ⊨φ.\nIf M is clear from the context, we may abbreviate M, s ⊨φ by s ⊨φ.\nIt should be clear that we have outlined the formal foundations of a pro-\ncedure that, given φ, M and s, can check whether M, s ⊨φ holds. Later\nin this chapter, we will examine algorithms which implement this calcula-\ntion. Let us now look at some example checks for the system in Figures 3.3\nand 3.5.\n1.\nM, s0 ⊨p ∧q holds since the atomic symbols p and q are contained in the node\nof s0: π ⊨p ∧q for every path π beginning in s0.\n3.2 Linear-time temporal logic\n183\n2.\nM, s0 ⊨¬r holds since the atomic symbol r is not contained in node s0.\n3.\nM, s0 ⊨⊤holds by deﬁnition.\n4.\nM, s0 ⊨X r holds since all paths from s0 have either s1 or s2 as their next\nstate, and each of those states satisﬁes r.\n5.\nM, s0 ⊨X (q ∧r) does not hold since we have the rightmost computation path\ns0 →s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 contains r, but\nnot q.\n6.\nM, s0 ⊨G ¬(p ∧r) holds since all computation paths beginning in s0 satisfy\nG ¬(p ∧r), i.e. they satisfy ¬(p ∧r) in each state along the path. Notice that\nG φ holds in a state if, and only if, φ holds in all states reachable from the\ngiven state.\n7.\nFor similar reasons, M, s2 ⊨G r holds (note the s2 instead of s0).\n8.\nFor any state s of M, we have M, s ⊨F (¬q ∧r) →F G r. This says that if",
    "G φ holds in a state if, and only if, φ holds in all states reachable from the\ngiven state.\n7.\nFor similar reasons, M, s2 ⊨G r holds (note the s2 instead of s0).\n8.\nFor any state s of M, we have M, s ⊨F (¬q ∧r) →F G r. This says that if\nany path π beginning in s gets to a state satisfying (¬q ∧r), then the path\nπ satisﬁes F G r. Indeed this is true, since if the path has a state satisfying\n(¬q ∧r) then (since that state must be s2) the path does satisfy F G r. Notice\nwhat F G r says about a path: eventually, you have continuously r.\n9.\nThe formula G F p expresses that p occurs along the path in question inﬁnitely\noften. Intuitively, it’s saying: no matter how far along the path you go (that’s\nthe G part) you will ﬁnd you still have a p in front of you (that’s the F part).\nFor example, the path s0 →s1 →s0 →s1 →. . . satisﬁes G F p. But the path\ns0 →s2 →s2 →s2 →. . . doesn’t.\n10.\nIn our model, if a path from s0 has inﬁnitely many ps on it then it must be the\npath s0 →s1 →s0 →s1 →. . . , and in that case it also has inﬁnitely many rs\non it. So, M, s0 ⊨G F p →G F r. But it is not the case the other way around!\nIt is not the case that M, s0 ⊨G F r →G F p, because we can ﬁnd a path from\ns0 which has inﬁnitely many rs but only one p.\n3.2.3 Practical patterns of specifications\nWhat kind of practically relevant properties can we check with formulas of\nLTL? We list a few of the common patterns. Suppose atomic descriptions\ninclude some words such as busy and requested. We may require some of\nthe following properties of real systems:\nr It is impossible to get to a state where started holds, but ready does not hold:\nG¬(started ∧¬ready)\nThe negation of this formula expresses that it is possible to get to such a state,\nbut this is only so if interpreted on paths (π ⊨φ). We cannot assert such a\npossibility if interpreted on states (s ⊨φ) since we cannot express the existence\nof paths; for that interpretation, the negation of the formula above asserts that",
    "but this is only so if interpreted on paths (π ⊨φ). We cannot assert such a\npossibility if interpreted on states (s ⊨φ) since we cannot express the existence\nof paths; for that interpretation, the negation of the formula above asserts that\nall paths will eventually get to such a state.\n184\n3 Verification by model checking\nr For any state, if a request (of some resource) occurs, then it will eventually be\nacknowledged:\nG (requested →F acknowledged).\nr A certain process is enabled inﬁnitely often on every computation path:\nG F enabled.\nr Whatever happens, a certain process will eventually be permanently deadlocked:\nF G deadlock.\nr If the process is enabled inﬁnitely often, then it runs inﬁnitely often.\nG F enabled →G F running.\nr An upwards travelling lift at the second ﬂoor does not change its direction when\nit has passengers wishing to go to the ﬁfth ﬂoor:\nG (floor2 ∧directionup ∧ButtonPressed5 →(directionup U floor5))\nHere, our atomic descriptions are boolean expressions built from system vari-\nables, e.g., floor2.\nThere are some things which are not possible to say in LTL, however. One\nbig class of such things are statements which assert the existence of a path,\nsuch as these ones:\nr From any state it is possible to get to a restart state (i.e., there is a path from\nall states to a state satisfying restart).\nr The lift can remain idle on the third ﬂoor with its doors closed (i.e., from the\nstate in which it is on the third ﬂoor, there is a path along which it stays there).\nLTL can’t express these because it cannot directly assert the existence of\npaths. In Section 3.4, we look at Computation Tree Logic (CTL) which has\noperators for quantifying over paths, and can express these properties.\n3.2.4 Important equivalences between LTL formulas\nDeﬁnition 3.9 We say that two LTL formulas φ and ψ are semantically\nequivalent, or simply equivalent, writing φ ≡ψ, if for all models M and all\npaths π in M: π ⊨φ iﬀπ ⊨ψ.",
    "operators for quantifying over paths, and can express these properties.\n3.2.4 Important equivalences between LTL formulas\nDeﬁnition 3.9 We say that two LTL formulas φ and ψ are semantically\nequivalent, or simply equivalent, writing φ ≡ψ, if for all models M and all\npaths π in M: π ⊨φ iﬀπ ⊨ψ.\nThe equivalence of φ and ψ means that φ and ψ are semantically inter-\nchangeable. If φ is a subformula of some bigger formula χ, and ψ ≡φ, then\nwe can make the substitution of ψ for φ in χ without changing the meaning\nof χ. In propositional logic, we saw that ∧and ∨are duals of each other,\nmeaning that if you push a ¬ past a ∧, it becomes a ∨, and vice versa:\n¬(φ ∧ψ) ≡¬φ ∨¬ψ\n¬(φ ∨ψ) ≡¬φ ∧¬ψ.\n(Because ∧and ∨are binary, pushing a negation downwards in the parse\ntree past one of them also has the eﬀect of duplicating that negation.)\n3.2 Linear-time temporal logic\n185\nSimilarly, F and G are duals of each other, and X is dual with itself:\n¬G φ ≡F ¬φ\n¬F φ ≡G ¬φ\n¬X φ ≡X ¬φ.\nAlso U and R are duals of each other:\n¬(φ U ψ) ≡¬φ R ¬ψ\n¬(φ R ψ) ≡¬φ U ¬ψ.\nWe should give formal proofs of these equivalences. But they are easy, so we\nleave them as an exercise to the reader. ‘Morally’ there ought to be a dual\nfor W, and you can invent one if you like. Work out what it might mean,\nand then pick a symbol based on the ﬁrst letter of the meaning. However, it\nmight not be very useful.\nIt’s also the case that F distributes over ∨and G over ∧, i.e.,\nF (φ ∨ψ) ≡F φ ∨F ψ\nG (φ ∧ψ) ≡G φ ∧G ψ.\nCompare this with the quantiﬁer equivalences in Section 2.3.2. But F does\nnot distribute over ∧. What this means is that there is a model with a\npath which distinguishes F (φ ∧ψ) and F φ ∧F ψ, for some φ, ψ. Take the\npath s0 →s1 →s0 →s1 →. . . from the system of Figure 3.3, for example;\nit satisﬁes F p ∧F r but it doesn’t satisfy F (p ∧r).\nHere are two more equivalences in LTL:\nF φ ≡⊤U φ\nG φ ≡⊥R φ.\nThe ﬁrst one exploits the fact that the clause for Until states two things:",
    "path s0 →s1 →s0 →s1 →. . . from the system of Figure 3.3, for example;\nit satisﬁes F p ∧F r but it doesn’t satisfy F (p ∧r).\nHere are two more equivalences in LTL:\nF φ ≡⊤U φ\nG φ ≡⊥R φ.\nThe ﬁrst one exploits the fact that the clause for Until states two things:\nthe second formula φ must become true; and until then, the ﬁrst formula ⊤\nmust hold. So, if we put ‘no constraint’ for the ﬁrst formula, it boils down\nto asking that the second formula holds, which is what F asks. (The formula\n⊤represent ‘no constraint.’ If you ask me to bring it about that ⊤holds,\nI need do nothing, it enforces no constraint. In the same sense, ⊥is ‘every\nconstraint.’ If you ask me to bring it about that ⊥holds, I’ll have to meet\nevery constraint there is, which is impossible.)\nThe second formula, that G φ ≡⊥R φ, can be obtained from the ﬁrst by\nputting a ¬ in front of each side, and applying the duality rules. Another\nmore intuitive way of seeing this is to recall the meaning of ‘release:’ ⊥\nreleases φ, but ⊥will never be true, so φ doesn’t get released.\nAnother pair of equivalences relates the strong and weak versions of Until,\nU and W. Strong until may be seen as weak until plus the constraint that\nthe eventuality must actually occur:\nφ U ψ ≡φ W ψ ∧F ψ .\n(3.2)\n186\n3 Verification by model checking\nTo prove equivalence (3.2), suppose ﬁrst that a path satisﬁes φ U ψ. Then,\nfrom clause 11, we have i ≥1 such that πi ⊨ψ and for all j = 1, . . . , i −1\nwe have πj ⊨φ. From clause 12, this proves φ W ψ, and from clause 10 it\nproves F ψ. Thus for all paths π, if π ⊨φ U ψ then π ⊨φ W ψ ∧F ψ. As an\nexercise, the reader can prove it the other way around.\nWriting W in terms of U is also possible: W is like U but also allows the\npossibility of the eventuality never occurring:\nφ W ψ ≡φ U ψ ∨G φ.\n(3.3)\nInspection of clauses 12 and 13 reveals that R and W are rather similar. The\ndiﬀerences are that they swap the roles of their arguments φ and ψ; and the",
    "Writing W in terms of U is also possible: W is like U but also allows the\npossibility of the eventuality never occurring:\nφ W ψ ≡φ U ψ ∨G φ.\n(3.3)\nInspection of clauses 12 and 13 reveals that R and W are rather similar. The\ndiﬀerences are that they swap the roles of their arguments φ and ψ; and the\nclause for W has an i −1 where R has i. Therefore, it is not surprising that\nthey are expressible in terms of each other, as follows:\nφ W ψ ≡ψ R (φ ∨ψ)\n(3.4)\nφ R ψ ≡ψ W (φ ∧ψ).\n(3.5)\n3.2.5 Adequate sets of connectives for LTL\nRecall that φ ≡ψ holds iﬀany path in any transition system which sat-\nisﬁes φ also satisﬁes ψ, and vice versa. As in propositional logic, there is\nsome redundancy among the connectives. For example, in Chapter 1 we saw\nthat the set {⊥, ∧, ¬} forms an adequate set of connectives, since the other\nconnectives ∨, →, ⊤, etc., can be written in terms of those three.\nSmall adequate sets of connectives also exist in LTL. Here is a summary\nof the situation.\nr X is completely orthogonal to the other connectives. That is to say, its presence\ndoesn’t help in deﬁning any of the other ones in terms of each other. Moreover,\nX cannot be derived from any combination of the others.\nr Each of the sets {U, X}, {R, X}, {W, X} is adequate. To see this, we note that\n– R and W may be deﬁned from U, by the duality φ R ψ ≡¬(¬φ U ¬ψ) and\nequivalence (3.4) followed by the duality, respectively.\n– U and W may be deﬁned from R, by the duality φ U ψ ≡¬(¬φ R ¬ψ) and\nequivalence (3.4), respectively.\n– R and U may be deﬁned from W, by equivalence (3.5) and the duality φ U\nψ ≡¬(¬φ R ¬ψ) followed by equivalence (3.5).\nSometimes it is useful to look at adequate sets of connectives which do not\nrely on the availability of negation. That’s because it is often convenient to\nassume formulas are written in negation-normal form, where all the negation\nsymbols are applied to propositional atoms (i.e., they are near the leaves\n3.3 Model checking: systems, tools, properties\n187",
    "rely on the availability of negation. That’s because it is often convenient to\nassume formulas are written in negation-normal form, where all the negation\nsymbols are applied to propositional atoms (i.e., they are near the leaves\n3.3 Model checking: systems, tools, properties\n187\nof the parse tree). In this case, these sets are adequate for the fragment\nwithout X, and no strict subset is: {U, R}, {U, W}, {U, G}, {R, F}, {W, F}.\nBut {R, G} and {W, G} are not adequate. Note that one cannot deﬁne G\nwith {U,F}, and one cannot deﬁne F with {R,G} or {W,G}.\nWe ﬁnally state and prove a useful equivalence about U.\nTheorem 3.10 The equivalence φ U ψ ≡¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ holds\nfor all LTL formulas φ and ψ.\nPROOF: Take any path s0 →s1 →s2 →. . . in any model.\nFirst, suppose s0 ⊨φ U ψ holds. Let n be the smallest number such that\nsn ⊨ψ; such a number has to exist since s0 ⊨φ U ψ; then, for each k < n,\nsk ⊨φ. We immediately have s0 ⊨F ψ, so it remains to show s0 ⊨¬(¬ψ U\n(¬φ ∧¬ψ)), which, if we expand, means:\n(∗) for each i > 0, if si ⊨¬φ ∧¬ψ, then there is some j < i with sj ⊨ψ.\nTake any i > 0 with si ⊨¬φ ∧¬ψ; i > n, so we can take j\ndef\n= n and have\nsj ⊨ψ.\nConversely, suppose s0 ⊨¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ holds; we prove s0 ⊨φ U\nψ. Since s0 ⊨F ψ, we have a minimal n as before. We show that, for any\ni < n, si ⊨φ. Suppose si ⊨¬φ; since n is minimal, we know si ⊨¬ψ, so\nby (∗) there is some j < i < n with sj ⊨ψ, contradicting the minimality\nof n.\n2\n3.3 Model checking: systems, tools, properties\n3.3.1 Example: mutual exclusion\nLet us now look at a larger example of veriﬁcation using LTL, having to do\nwith mutual exclusion. When concurrent processes share a resource (such as\na ﬁle on a disk or a database entry), it may be necessary to ensure that they\ndo not have access to it at the same time. Several processes simultaneously\nediting the same ﬁle would not be desirable.\nWe therefore identify certain critical sections of each process’ code and",
    "a ﬁle on a disk or a database entry), it may be necessary to ensure that they\ndo not have access to it at the same time. Several processes simultaneously\nediting the same ﬁle would not be desirable.\nWe therefore identify certain critical sections of each process’ code and\narrange that only one process can be in its critical section at a time. The\ncritical section should include all the access to the shared resource (though it\nshould be as small as possible so that no unnecessary exclusion takes place).\nThe problem we are faced with is to ﬁnd a protocol for determining which\nprocess is allowed to enter its critical section at which time. Once we have\nfound one which we think works, we verify our solution by checking that it\nhas some expected properties, such as the following ones:\nSafety: Only one process is in its critical section at any time.\n188\n3 Verification by model checking\nn1n2\nt1n2\nc1n2\nt1t2\nn1t2\nn1c2\nt1c2\nc1t2\ns0\ns1\ns2\ns4\ns3\ns5\ns6\ns7\nFigure 3.7. A first-attempt model for mutual exclusion.\nThis safety property is not enough, since a protocol which permanently\nexcluded every process from its critical section would be safe, but not very\nuseful. Therefore, we should also require:\nLiveness: Whenever any process requests to enter its critical section, it\nwill eventually be permitted to do so.\nNon-blocking: A process can always request to enter its critical section.\nSome rather crude protocols might work on the basis that they cycle through\nthe processes, making each one in turn enter its critical section. Since it\nmight be naturally the case that some of them request access to the shared\nresource more often than others, we should make sure our protocol has the\nproperty:\nNo strict sequencing: Processes need not enter their critical section in\nstrict sequence.\nThe first modelling attempt\nWe will model two processes, each of\nwhich is in its non-critical state (n), or trying to enter its critical state (t),",
    "property:\nNo strict sequencing: Processes need not enter their critical section in\nstrict sequence.\nThe first modelling attempt\nWe will model two processes, each of\nwhich is in its non-critical state (n), or trying to enter its critical state (t),\nor in its critical state (c). Each individual process undergoes transitions in\nthe cycle n →t →c →n →. . . , but the two processes interleave with each\nother. Consider the protocol given by the transition system M in Figure 3.7.\n(As usual, we write p1p2 . . . pm in a node s to denote that p1, p2, . . . , pm\nare the only propositional atoms true at s.) The two processes start oﬀin\ntheir non-critical sections (global state s0). State s0 is the only initial state,\nindicated by the incoming edge with no source. Either of them may now\n3.3 Model checking: systems, tools, properties\n189\nmove to its trying state, but only one of them can ever make a transition at\na time (asynchronous interleaving). At each step, an (unspeciﬁed) scheduler\ndetermines which process may run. So there is a transition arrow from s0 to\ns1 and s5. From s1 (i.e., process 1 trying, process 2 non-critical) again two\nthings can happen: either process 1 moves again (we go to s2), or process 2\nmoves (we go to s3). Notice that not every process can move in every state.\nFor example, process 1 cannot move in state s7, since it cannot go into its\ncritical section until process 2 comes out of its critical section.\nWe would like to check the four properties by ﬁrst describing them as\ntemporal logic formulas. Unfortunately, they are not all expressible as LTL\nformulas. Let us look at them case-by-case.\nSafety: This is expressible in LTL, as G ¬(c1 ∧c2). Clearly, G ¬(c1 ∧c2)\nis satisﬁed in the initial state (indeed, in every state).\nLiveness: This is also expressible: G (t1 →F c1). However, it is not sat-\nisﬁed by the initial state, for we can ﬁnd a path starting at the\ninitial state along which there is a state, namely s1, in which t1 is",
    "is satisﬁed in the initial state (indeed, in every state).\nLiveness: This is also expressible: G (t1 →F c1). However, it is not sat-\nisﬁed by the initial state, for we can ﬁnd a path starting at the\ninitial state along which there is a state, namely s1, in which t1 is\ntrue but from there along the path c1 is false. The path in question\nis s0 →s1 →s3 →s7 →s1 →s3 →s7 . . . on which c1 is always false.\nNon-blocking: Let’s just consider process 1. We would like to express the\nproperty as: for every state satisfying n1, there is a successor satisfying\nt1. Unfortunately, this existence quantiﬁer on paths (‘there is a successor\nsatisfying. . . ’) cannot be expressed in LTL. It can be expressed in the\nlogic CTL, which we will turn to in the next section (for the impatient,\nsee page 215).\nNo strict sequencing: We might consider expressing this as saying: there\nis a path with two distinct states satisfying c1 such that no state in\nbetween them has that property. However, we cannot express ‘there\nexists a path,’ so let us consider the complement formula instead. The\ncomplement says that all paths having a c1 period which ends can-\nnot have a further c1 state until a c2 state occurs. We write this as:\nG (c1 →c1 W (¬c1 ∧¬c1 W c2)). This says that anytime we get into a\nc1 state, either that condition persists indeﬁnitely, or it ends with a non-\nc1 state and in that case there is no further c1 state unless and until we\nobtain a c2 state.\nThis formula is false, as exempliﬁed by the path s0 →s5 →s3 →s4 →\ns5 →s3 →s4 . . . . Therefore the original condition expressing that strict\nsequencing need not occur, is true.\nBefore further considering the mutual exclusion example, some comments\nabout expressing properties in LTL are appropriate. Notice that in the\n190\n3 Verification by model checking\nno-strict-sequencing property, we overcame the problem of not being able to\nexpress the existence of paths by instead expressing the complement prop-",
    "about expressing properties in LTL are appropriate. Notice that in the\n190\n3 Verification by model checking\nno-strict-sequencing property, we overcame the problem of not being able to\nexpress the existence of paths by instead expressing the complement prop-\nerty, which of course talks about all paths. Then we can perform our check,\nand simply reverse the answer; if the complement property is false, we de-\nclare our property to be true, and vice versa.\nWhy was that tactic not available to us to express the non-blocking prop-\nerty? The reason is that it says: every path to a n1 state may be continued\nby a one-step path to a t1 state. The presence of both universal and exis-\ntential quantiﬁers is the problem. In the no-strict-sequencing property, we\nhad only an existential quantiﬁer; thus, taking the complement property\nturned it into a universal path quantiﬁer, which can be expressed in LTL.\nBut where we have alternating quantiﬁers, taking the complement property\ndoesn’t help in general.\nLet’s go back to the mutual exclusion example. The reason liveness failed\nin our ﬁrst attempt at modelling mutual exclusion is that non-determinism\nmeans it might continually favour one process over another. The problem is\nthat the state s3 does not distinguish between which of the processes ﬁrst\nwent into its trying state. We can solve this by splitting s3 into two states.\nThe second modelling attempt\nThe two states s3 and s9 in Figure 3.8\nboth correspond to the state s3 in our ﬁrst modelling attempt. They both\nrecord that the two processes are in their trying states, but in s3 it is im-\nplicitly recorded that it is process 1’s turn, whereas in s9 it is process 2’s\nturn. Note that states s3 and s9 both have the labelling t1t2; the deﬁnition of\ntransition systems does not preclude this. We can think of there being some\nother, hidden, variables which are not part of the initial labelling, which\ndistinguish s3 and s9.",
    "turn. Note that states s3 and s9 both have the labelling t1t2; the deﬁnition of\ntransition systems does not preclude this. We can think of there being some\nother, hidden, variables which are not part of the initial labelling, which\ndistinguish s3 and s9.\nRemark 3.11 The four properties of safety, liveness, non-blocking and no-\nstrict-sequencing are satisﬁed by the model in Figure 3.8. (Since the non-\nblocking property has not yet been written in temporal logic, we can only\ncheck it informally.)\nIn this second modelling attempt, our transition system is still slightly\nover-simpliﬁed, because we are assuming that it will move to a diﬀerent\nstate on every tick of the clock (there are no transitions to the same state).\nWe may wish to model that a process can stay in its critical state for several\nticks, but if we include an arrow from s4, or s7, to itself, we will again violate\nliveness. This problem will be solved later in this chapter when we consider\n‘fairness constraints’ (Section 3.6.2).\n3.3 Model checking: systems, tools, properties\n191\ns0 n1n2\ns1\ns2\ns4\ns3\nt1n2\nc1n2\nc1t2\ns5\nn1c2\ns7\nn1t2\nt1c2\nt1t2\nt1t2\ns9\ns6\nFigure 3.8. A second-attempt model for mutual exclusion. There are\nnow two states representing t1t2, namely s3 and s9.\n3.3.2 The NuSMV model checker\nSo far, this chapter has been quite theoretical; and the sections after this\none continue in this vein. However, one of the exciting things about model\nchecking is that it is also a practical subject, for there are several eﬃcient\nimplementations which can check large systems in realistic time. In this\nsection, we look at the NuSMV model-checking system. NuSMV stands for\n‘New Symbolic Model Veriﬁer.’ NuSMV is an Open Source product, is ac-\ntively supported and has a substantial user community. For details on how\nto obtain it, see the bibliographic notes at the end of the chapter.\nNuSMV (sometimes called simply SMV) provides a language for describ-",
    "‘New Symbolic Model Veriﬁer.’ NuSMV is an Open Source product, is ac-\ntively supported and has a substantial user community. For details on how\nto obtain it, see the bibliographic notes at the end of the chapter.\nNuSMV (sometimes called simply SMV) provides a language for describ-\ning the models we have been drawing as diagrams and it directly checks the\nvalidity of LTL (and also CTL) formulas on those models. SMV takes as\ninput a text consisting of a program describing a model and some speciﬁca-\ntions (temporal logic formulas). It produces as output either the word ‘true’\nif the speciﬁcations hold, or a trace showing why the speciﬁcation is false\nfor the model represented by our program.\nSMV programs consist of one or more modules. As in the programming\nlanguage C, or Java, one of the modules must be called main. Modules can\ndeclare variables and assign to them. Assignments usually give the initial\nvalue of a variable and its next value as an expression in terms of the current\nvalues of variables. This expression can be non-deterministic (denoted by\nseveral expressions in braces, or no assignment at all). Non-determinism is\nused to model the environment and for abstraction.\n192\n3 Verification by model checking\nThe following input to SMV:\nMODULE main\nVAR\nrequest : boolean;\nstatus : {ready,busy};\nASSIGN\ninit(status) := ready;\nnext(status) := case\nrequest : busy;\n1 : {ready,busy};\nesac;\nLTLSPEC\nG(request -> F status=busy)\nconsists of a program and a speciﬁcation. The program has two variables,\nrequest of type boolean and status of enumeration type {ready, busy}:\n0 denotes ‘false’ and 1 represents ‘true.’ The initial and subsequent values\nof variable request are not determined within this program; this conserva-\ntively models that these values are determined by an external environment.\nThis under-speciﬁcation of request implies that the value of variable status\nis partially determined: initially, it is ready; and it becomes busy whenever",
    "tively models that these values are determined by an external environment.\nThis under-speciﬁcation of request implies that the value of variable status\nis partially determined: initially, it is ready; and it becomes busy whenever\nrequest is true. If request is false, the next value of status is not deter-\nmined.\nNote that the case 1: signiﬁes the default case, and that case statements\nare evaluated from the top down: if several expressions to the left of a ‘:’ are\ntrue, then the command corresponding to the ﬁrst, top-most true expression\nwill be executed. The program therefore denotes the transition system shown\nin Figure 3.9; there are four states, each one corresponding to a possible value\nof the two binary variables. Note that we wrote ‘busy’ as a shorthand for\n‘status=busy’ and ‘req’ for ‘request is true.’\nIt takes a while to get used to the syntax of SMV and its meaning. Since\nvariable request functions as a genuine environment in this model, the\nprogram and the transition system are non-deterministic: i.e., the ‘next\nstate’ is not uniquely deﬁned. Any state transition based on the behaviour\nof status comes in a pair: to a successor state where request is false, or\ntrue, respectively. For example, the state ‘¬req, busy’ has four states it can\nmove to (itself and three others).\nLTL speciﬁcations are introduced by the keyword LTLSPEC and are sim-\nply LTL formulas. Notice that SMV uses &, |, -> and ! for ∧, ∨, →and\n¬, respectively, since they are available on standard keyboards. We may\n3.3 Model checking: systems, tools, properties\n193\nreq\nready\nbusy\nreq\n¬req\nbusy\nready\n¬req\nFigure 3.9. The model corresponding to the SMV program in the text.\neasily verify that the speciﬁcation of our module main holds of the model in\nFigure 3.9.\nModules in SMV\nSMV supports breaking a system description into sev-\neral modules, to aid readability and to verify interaction properties. A mod-\nule is instantiated when a variable having that module name as its type is",
    "Figure 3.9.\nModules in SMV\nSMV supports breaking a system description into sev-\neral modules, to aid readability and to verify interaction properties. A mod-\nule is instantiated when a variable having that module name as its type is\ndeclared. This deﬁnes a set of variables, one for each one declared in the\nmodule description. In the example below, which is one of the ones dis-\ntributed with SMV, a counter which repeatedly counts from 000 through to\n111 is described by three single-bit counters. The module counter cell is\ninstantiated three times, with the names bit0, bit1 and bit2. The counter\nmodule has one formal parameter, carry in, which is given the actual value\n1 in bit0, and bit0.carry out in the instance bit1. Hence, the carry in of\nmodule bit1 is the carry out of module bit0. Note that we use the period\n‘.’ in m.v to access the variable v in module m. This notation is also used by\nAlloy (see Chapter 2) and a host of programming languages to access ﬁelds\nin record structures, or methods in objects. The keyword DEFINE is used\nto assign the expression value & carry in to the symbol carry out (such\ndeﬁnitions are just a means for referring to the current value of a certain\nexpression).\nMODULE main\nVAR\nbit0 : counter_cell(1);\nbit1 : counter_cell(bit0.carry_out);\nbit2 : counter_cell(bit1.carry_out);\nLTLSPEC\nG F bit2.carry_out\n194\n3 Verification by model checking\nMODULE counter_cell(carry_in)\nVAR\nvalue : boolean;\nASSIGN\ninit(value) := 0;\nnext(value) := (value + carry_in) mod 2;\nDEFINE\ncarry_out := value & carry_in;\nThe eﬀect of the DEFINE statement could have been obtained by declaring\na new variable and assigning its value thus:\nVAR\ncarry_out : boolean;\nASSIGN\ncarry_out := value & carry_in;\nNotice that, in this assignment, the current value of the variable is assigned.\nDeﬁned symbols are usually preferable to variables, since they don’t increase\nthe state space by declaring new variables. However, they cannot be assigned",
    "VAR\ncarry_out : boolean;\nASSIGN\ncarry_out := value & carry_in;\nNotice that, in this assignment, the current value of the variable is assigned.\nDeﬁned symbols are usually preferable to variables, since they don’t increase\nthe state space by declaring new variables. However, they cannot be assigned\nnon-deterministically since they refer only to another expression.\nSynchronous and asynchronous composition\nBy default, modules\nin SMV are composed synchronously: this means that there is a global clock\nand, each time it ticks, each of the modules executes in parallel. By use of\nthe process keyword, it is possible to compose the modules asynchronously.\nIn that case, they run at diﬀerent ‘speeds,’ interleaving arbitrarily. At each\ntick of the clock, one of them is non-deterministically chosen and executed\nfor one cycle. Asynchronous interleaving composition is useful for describing\ncommunication protocols, asynchronous circuits and other systems whose\nactions are not synchronised to a global clock.\nThe bit counter above is synchronous, whereas the examples below of\nmutual exclusion and the alternating bit protocol are asynchronous.\n3.3.3 Running NuSMV\nThe normal use of NuSMV is to run it in batch mode, from a Unix shell or\ncommand prompt in Windows. The command line\nNuSMV counter3.smv\n3.3 Model checking: systems, tools, properties\n195\nwill analyse the code in the ﬁle counter3.smv and report on the speciﬁca-\ntions it contains. One can also run NuSMV interactively. In that case, the\ncommand line\nNuSMV -int counter3.smv\nenters NuSMV’s command-line interpreter. From there, there is a variety\nof commands you can use which allow you to compile the description and\nrun the speciﬁcation checks, as well as inspect partial results and set various\nparameters. See the NuSMV user manual for more details.\nNuSMV also supports bounded model checking, invoked by the command-\nline option -bmc. Bounded model checking looks for counterexamples in",
    "run the speciﬁcation checks, as well as inspect partial results and set various\nparameters. See the NuSMV user manual for more details.\nNuSMV also supports bounded model checking, invoked by the command-\nline option -bmc. Bounded model checking looks for counterexamples in\norder of size, starting with counterexamples of length 1, then 2, etc., up\nto a given threshold (10 by default). Note that bounded model checking\nis incomplete: failure to ﬁnd a counterexample does not mean that there\nis none, but only that there is none of length up to the threshold. For\nrelated reasons, this incompleteness features also in Alloy and its constraint\nanalyzer. Thus, while a negative answer can be relied on (if NuSMV ﬁnds a\ncounterexample, it is valid), a positive one cannot. References on bounded\nmodel checking can be found in the bibliographic notes on page 254. Later\non, we use bounded model checking to prove the optimality of a scheduler.\n3.3.4 Mutual exclusion revisited\nFigure 3.10 gives the SMV code for a mutual exclusion protocol. This code\nconsists of two modules, main and prc. The module main has the variable\nturn, which determines whose turn it is to enter the critical section if both\nare trying to enter (recall the discussion about the states s3 and s9 in Sec-\ntion 3.3.1).\nThe module main also has two instantiations of prc. In each of these\ninstantiations, st is the status of a process (saying whether it is in its critical\nsection, or not, or trying) and other-st is the status of the other process\n(notice how this is passed as a parameter in the third and fourth lines of\nmain).\nThe value of st evolves in the way described in a previous section: when\nit is n, it may stay as n or move to t. When it is t, if the other one is n, it will\ngo straight to c, but if the other one is t, it will check whose turn it is before\ngoing to c. Then, when it is c, it may move back to n. Each instantiation of\nprc gives the turn to the other one when it gets to its critical section.",
    "go straight to c, but if the other one is t, it will check whose turn it is before\ngoing to c. Then, when it is c, it may move back to n. Each instantiation of\nprc gives the turn to the other one when it gets to its critical section.\nAn important feature of SMV is that we can restrict its search tree to\nexecution paths along which an arbitrary boolean formula about the state\n196\n3 Verification by model checking\nMODULE main\nVAR\npr1: process prc(pr2.st, turn, 0);\npr2: process prc(pr1.st, turn, 1);\nturn: boolean;\nASSIGN\ninit(turn) := 0;\n-- safety\nLTLSPEC\nG!((pr1.st = c) & (pr2.st = c))\n-- liveness\nLTLSPEC\nG((pr1.st = t) -> F (pr1.st = c))\nLTLSPEC\nG((pr2.st = t) -> F (pr2.st = c))\n-- ‘negation’ of strict sequencing (desired to be false)\nLTLSPEC G(pr1.st=c -> ( G pr1.st=c | (pr1.st=c U\n(!pr1.st=c & G !pr1.st=c | ((!pr1.st=c) U pr2.st=c)))))\nMODULE prc(other-st, turn, myturn)\nVAR\nst: {n, t, c};\nASSIGN\ninit(st) := n;\nnext(st) :=\ncase\n(st = n)\n: {t,n};\n(st = t) & (other-st = n)\n: c;\n(st = t) & (other-st = t) & (turn = myturn): c;\n(st = c)\n: {c,n};\n1\n: st;\nesac;\nnext(turn) :=\ncase\nturn = myturn & st = c : !turn;\n1\n: turn;\nesac;\nFAIRNESS running\nFAIRNESS\n!(st = c)\nFigure 3.10. SMV code for mutual exclusion. Because W is not sup-\nported by SMV, we had to make use of equivalence (3.3) to write the\nno-strict-sequencing formula as an equivalent but longer formula in-\nvolving U.\n3.3 Model checking: systems, tools, properties\n197\nφ is true inﬁnitely often. Because this is often used to model fair access to\nresources, it is called a fairness constraint and introduced by the keyword\nFAIRNESS. Thus, the occurrence of FAIRNESS φ means that SMV, when\nchecking a speciﬁcation ψ, will ignore any path along which φ is not satisﬁed\ninﬁnitely often.\nIn the module prc, we restrict model checks to computation paths along\nwhich st is inﬁnitely often not equal to c. This is because our code allows\nthe process to stay in its critical section as long as it likes. Thus, there",
    "inﬁnitely often.\nIn the module prc, we restrict model checks to computation paths along\nwhich st is inﬁnitely often not equal to c. This is because our code allows\nthe process to stay in its critical section as long as it likes. Thus, there\nis another opportunity for liveness to fail: if process 2 stays in its critical\nsection forever, process 1 will never be able to enter. Again, we ought not\nto take this kind of violation into account, since it is patently unfair if a\nprocess is allowed to stay in its critical section for ever. We are looking for\nmore subtle violations of the speciﬁcations, if there are any. To avoid the\none above, we stipulate the fairness constraint !(st=c).\nIf the module in question has been declared with the process keyword,\nthen at each time point SMV will non-deterministically decide whether or\nnot to select it for execution, as explained earlier. We may wish to ignore\npaths in which a module is starved of processor time. The reserved word\nrunning can be used instead of a formula in a fairness constraint: writing\nFAIRNESS running restricts attention to execution paths along which the\nmodule in which it appears is selected for execution inﬁnitely often.\nIn prc, we restrict ourselves to such paths, since, without this restriction,\nit would be easy to violate the liveness constraint if an instance of prc were\nnever selected for execution. We assume the scheduler is fair; this assumption\nis codiﬁed by two FAIRNESS clauses. We return to the issue of fairness, and\nthe question of how our model-checking algorithm copes with it, in the next\nsection.\nPlease run this program in NuSMV to see which speciﬁcations hold for\nit.\nThe transition system corresponding to this program is shown in\nFigure 3.11. Each state shows the values of the variables; for example, ct1\nis the state in which process 1 and 2 are critical and trying, respectively,\nand turn=1. The labels on the transitions show which process was selected",
    "it.\nThe transition system corresponding to this program is shown in\nFigure 3.11. Each state shows the values of the variables; for example, ct1\nis the state in which process 1 and 2 are critical and trying, respectively,\nand turn=1. The labels on the transitions show which process was selected\nfor execution. In general, each state has several transitions, some in which\nprocess 1 moves and others in which process 2 moves.\nThis model is a bit diﬀerent from the previous model given for mutual\nexclusion in Figure 3.8, for these two reasons:\nr Because the boolean variable turn has been explicitly introduced to distinguish\nbetween states s3 and s9 of Figure 3.8, we now distinguish between certain states\n198\n3 Verification by model checking\ncn0\ntn0\ntc0\ntt0\nnn0\nct0\n1,2\n2\n1\n1\n1\n2\n1,2\n1\n2\n2\n2\n2\n1\n1\n2\n1\n1,2\n1\n1,2\n1,2\n1\n2\n2\n2\n1\n2\n1\n1\n1\n2\n1\n2\n2\nnn1\ntn1\ncn1\nct1\nnt1\ntt1\nnc1\ntc1\n1,2\n2\nnc0\nnt0\n1,2\n2\n1\n1,2\n1,2\n1\n1,2\nFigure 3.11. The transition system corresponding to the SMV code\nin Figure 3.10. The labels on the transitions denote the process which\nmakes the move. The label 1, 2 means that either process could make\nthat move.\n3.3 Model checking: systems, tools, properties\n199\n(for example, ct0 and ct1) which were identical before. However, these states\nare not distinguished if you look just at the transitions from them. Therefore,\nthey satisfy the same LTL formulas which don’t mention turn. Those states are\ndistinguished only by the way they can arise.\nr We have eliminated an over-simpliﬁcation made in the model of Figure 3.8. Recall\nthat we assumed the system would move to a diﬀerent state on every tick of the\nclock (there were no transitions from a state to itself). In Figure 3.11, we allow\ntransitions from each state to itself, representing that a process was chosen for\nexecution and did some private computation, but did not move in or out of its\ncritical section. Of course, by doing this we have introduced paths in which one",
    "transitions from each state to itself, representing that a process was chosen for\nexecution and did some private computation, but did not move in or out of its\ncritical section. Of course, by doing this we have introduced paths in which one\nprocess gets stuck in its critical section, whence the need to invoke a fairness\nconstraint to eliminate such paths.\n3.3.5 The ferryman\nYou may recall the puzzle of a ferryman, goat, cabbage, and wolf all on one\nside of a river. The ferryman can cross the river with at most one passenger\nin his boat. There is a behavioural conﬂict between:\n1.\nthe goat and the cabbage; and\n2.\nthe goat and the wolf;\nif they are on the same river bank but the ferryman crosses the river or stays\non the other bank.\nCan the ferryman transport all goods to the other side, without any con-\nﬂicts occurring? This is a planning problem, but it can be solved by model\nchecking. We describe a transition system in which the states represent which\ngoods are at which side of the river. Then we ask if the goal state is reach-\nable from the initial state: Is there a path from the initial state such that it\nhas a state along it at which all the goods are on the other side, and during\nthe transitions to that state the goods are never left in an unsafe, conﬂicting\nsituation?\nWe model all possible behaviour (including that which results in conﬂicts)\nas a NuSMV program (Figure 3.12). The location of each agent is modelled\nas a boolean variable: 0 denotes that the agent is on the initial bank, and\n1 the destination bank. Thus, ferryman = 0 means that the ferryman is\non the initial bank, ferryman = 1 that he is on the destination bank, and\nsimilarly for the variables goat, cabbage and wolf.\nThe variable carry takes a value indicating whether the goat, cabbage,\nwolf or nothing is carried by the ferryman. The deﬁnition of next(carry)\nworks as follows. It is non-deterministic, but the set from which a value is",
    "similarly for the variables goat, cabbage and wolf.\nThe variable carry takes a value indicating whether the goat, cabbage,\nwolf or nothing is carried by the ferryman. The deﬁnition of next(carry)\nworks as follows. It is non-deterministic, but the set from which a value is\nnon-deterministically chosen is determined by the values of ferryman, goat,\nMODULE main\nVAR\nferryman : boolean;\ngoat\n: boolean;\ncabbage\n: boolean;\nwolf\n: boolean;\ncarry\n: {g,c,w,0};\nASSIGN\ninit(ferryman) := 0; init(goat)\n:= 0;\ninit(cabbage)\n:= 0; init(wolf)\n:= 0;\ninit(carry)\n:= 0;\nnext(ferryman) := 0,1;\nnext(carry) := case\nferryman=goat : g;\n1\n: 0;\nesac union\ncase\nferryman=cabbage : c;\n1\n: 0;\nesac union\ncase\nferryman=wolf : w;\n1\n: 0;\nesac union 0;\nnext(goat) := case\nferryman=goat\n& next(carry)=g : next(ferryman);\n1\n: goat;\nesac;\nnext(cabbage) := case\nferryman=cabbage & next(carry)=c : next(ferryman);\n1\n: cabbage;\nesac;\nnext(wolf) := case\nferryman=wolf & next(carry)=w : next(ferryman);\n1\n: wolf;\nesac;\nLTLSPEC !((\n(goat=cabbage | goat=wolf) -> goat=ferryman)\nU (cabbage & goat & wolf & ferryman))\nFigure 3.12. NuSMV code for the ferryman planning problem.\n3.3 Model checking: systems, tools, properties\n201\netc., and always includes 0. If ferryman = goat (i.e., they are on the same\nside) then g is a member of the set from which next(carry) is chosen. The\nsituation for cabbage and wolf is similar. Thus, if ferryman = goat = wolf ̸=\ncabbage then that set is {g, w, 0}. The next value assigned to ferryman is\nnon-deterministic: he can choose to cross or not to cross the river. But the\nnext values of goat, cabbage and wolf are deterministic, since whether they\nare carried or not is determined by the ferryman’s choice, represented by the\nnon-deterministic assignment to carry; these values follow the same pattern.\nNote how the boolean guards refer to state bits at the next state. The\nSMV compiler does a dependency analysis and rejects circular dependencies",
    "are carried or not is determined by the ferryman’s choice, represented by the\nnon-deterministic assignment to carry; these values follow the same pattern.\nNote how the boolean guards refer to state bits at the next state. The\nSMV compiler does a dependency analysis and rejects circular dependencies\non next values. (The dependency analysis is rather pessimistic: sometimes\nNuSMV complains of circularity even in situations when it could be resolved.\nThe original CMU-SMV is more liberal in this respect.)\nRunning NuSMV\nWe seek a path satisfying φ U ψ, where ψ asserts the\nﬁnal goal state, and φ expresses the safety condition (if the goat is with\nthe cabbage or the wolf, then the ferryman is there, too, to prevent any\nuntoward behaviour). Thus, we assert that all paths satisfy ¬(φ U ψ), i.e.,\nno path satisﬁes φ U ψ. We hope this is not the case, and NuSMV will give\nus an example path which does satisfy φ U ψ. Indeed, running NuSMV gives\nus the path of Figure 3.13, which represents a solution to the puzzle.\nThe beginning of the generated path represents the usual solution to this\npuzzle: the ferryman takes the goat ﬁrst, then goes back for the cabbage. To\navoid leaving the goat and the cabbage together, he takes the goat back, and\npicks up the wolf. Now the wolf and the cabbage are on the destination side,\nand he goes back again to get the goat. This brings us to State 1.9, where\nthe ferryman appears to take a well-earned break. But the path continues.\nStates 1.10 to 1.15 show that he takes his charges back to the original side\nof the bank; ﬁrst the cabbage, then the wolf, then the goat. Unfortunately\nit appears that the ferryman’s clever plan up to state 1.9 is now spoiled,\nbecause the goat meets an unhappy end in state 1.11.\nWhat went wrong? Nothing, actually. NuSMV has given us an inﬁnite\npath, which loops around the 15 illustrated states. Along the inﬁnite path,\nthe ferryman repeatedly takes his goods across (safely), and then back again",
    "because the goat meets an unhappy end in state 1.11.\nWhat went wrong? Nothing, actually. NuSMV has given us an inﬁnite\npath, which loops around the 15 illustrated states. Along the inﬁnite path,\nthe ferryman repeatedly takes his goods across (safely), and then back again\n(unsafely). This path does indeed satisfy the speciﬁcation φ U ψ, which as-\nserts the safety of the forward journey but says nothing about what happens\nafter that. In other words, the path is correct; it satisﬁes φ U ψ (with ψ oc-\ncurring at state 8). What happens along the path after that has no bearing\non φ U ψ.\n202\n3 Verification by model checking\nacws-0116% nusmv\nferryman.smv\n*** This is NuSMV 2.1.2 (compiled 2002-11-22 12:00:00)\n*** For more information of NuSMV see <http://nusmv.irst.itc.it>\n*** or email to <nusmv-users@irst.itc.it>.\n*** Please report bugs to <nusmv-users@irst.itc.it>.\n-- specification !(((goat = cabbage | goat = wolf) -> goat = ferryman)\nU (((cabbage & goat) & wolf) & ferryman)) is false\n-- as demonstrated by the following execution sequence\n-- loop starts here --\n-> State 1.1 <-\nferryman = 0\n-> State 1.8 <-\ngoat = 0\nferryman = 1\ncabbage = 0\ngoat = 1\nwolf = 0\ncarry = g\ncarry = 0\n-> State 1.9 <-\n-> State 1.2 <-\n-> State 1.10 <-\nferryman = 1\nferryman = 0\ngoat = 1\ncabbage = 0\ncarry = g\ncarry = c\n-> State 1.3 <-\n-> State 1.11 <-\nferryman = 0\nferryman = 1\ncarry = 0\ncarry = 0\n-> State 1.4 <-\n-> State 1.12 <-\nferryman = 1\nferryman = 0\ncabbage = 1\nwolf = 0\ncarry = c\ncarry = w\n-> State 1.5 <-\n-> State 1.13 <-\nferryman = 0\nferryman = 1\ngoat = 0\ncarry = 0\ncarry = g\n-> State 1.14 <-\n-> State 1.6 <-\nferryman = 0\nferryman = 1\ngoat = 0\nwolf = 1\ncarry = g\ncarry = w\n-> State 1.15 <-\n-> State 1.7 <-\ncarry = 0\nferryman = 0\ncarry = 0\nFigure 3.13. A solution path to the ferryman puzzle. It is unnecessar-\nily long. Using bounded model checking will refine it into an optimal\nsolution.\nInvoking bounded model checking will produce the shortest possible path",
    "carry = w\n-> State 1.15 <-\n-> State 1.7 <-\ncarry = 0\nferryman = 0\ncarry = 0\nFigure 3.13. A solution path to the ferryman puzzle. It is unnecessar-\nily long. Using bounded model checking will refine it into an optimal\nsolution.\nInvoking bounded model checking will produce the shortest possible path\nto violate the property; in this case, it is states 1.1 to 1.8 of the illus-\ntrated path. It is the shortest, optimal solution to our planning problem\nsince the model check NuSMV -bmc -bmc_length 7 ferryman.smv shows\nthat the LTL formula holds in that model, meaning that no solution with\nfewer than seven transitions is possible.\n3.3 Model checking: systems, tools, properties\n203\nOne might wish to verify whether there is a solution which involves three\njourneys for the goat. This can be done by altering the LTL formula. In-\nstead of seeking a path satisfying φ U ψ, where φ equals (goat = cabbage ∨\ngoat = wolf) →goat = ferryman and ψ equals cabbage ∧goat ∧wolf ∧\nferryman, we now seek a path satisfying (φ U ψ) ∧G (goat →G goat). The\nlast bit says that once the goat has crossed, he remains across; otherwise,\nthe goat makes at least three trips. NuSMV veriﬁes that the negation of this\nformula is true, conﬁrming that there is no such solution.\n3.3.6 The alternating bit protocol\nThe alternating bit protocol (ABP) is a protocol for transmitting messages\nalong a ‘lossy line,’ i.e., a line which may lose or duplicate messages. The\nprotocol guarantees that, providing the line doesn’t lose inﬁnitely many mes-\nsages, communication between the sender and the receiver will be successful.\n(We allow the line to lose or duplicate messages, but it may not corrupt mes-\nsages; however, there is no way of guaranteeing successful transmission along\na line which can corrupt.)\nThe ABP works as follows. There are four entities, or agents: the sender,\nthe receiver, the message channel and the acknowledgement channel. The\nsender transmits the ﬁrst part of the message together with the ‘control’",
    "a line which can corrupt.)\nThe ABP works as follows. There are four entities, or agents: the sender,\nthe receiver, the message channel and the acknowledgement channel. The\nsender transmits the ﬁrst part of the message together with the ‘control’\nbit 0. If, and when, the receiver receives a message with the control bit 0,\nit sends 0 along the acknowledgement channel. When the sender receives\nthis acknowledgement, it sends the next packet with the control bit 1. If\nand when the receiver receives this, it acknowledges by sending a 1 on the\nacknowledgement channel. By alternating the control bit, both receiver and\nsender can guard against duplicating messages and losing messages (i.e.,\nthey ignore messages that have the unexpected control bit).\nIf the sender doesn’t get the expected acknowledgement, it continually re-\nsends the message, until the acknowledgement arrives. If the receiver doesn’t\nget a message with the expected control bit, it continually resends the pre-\nvious acknowledgement.\nFairness is also important for the ABP. It comes in because, although\nwe want to model the fact that the channel can lose messages, we want to\nassume that, if we send a message often enough, eventually it will arrive.\nIn other words, the channel cannot lose an inﬁnite sequence of messages. If\nwe did not make this assumption, then the channels could lose all messages\nand, in that case, the ABP would not work.\nLet us see this in the concrete setting of SMV. We may assume that\nthe text to be sent is divided up into single-bit messages, which are sent\n204\n3 Verification by model checking\nMODULE sender(ack)\nVAR\nst\n: {sending,sent};\nmessage1 : boolean;\nmessage2 : boolean;\nASSIGN\ninit(st) := sending;\nnext(st) := case\nack = message2 & !(st=sent) : sent;\n1\n: sending;\nesac;\nnext(message1) :=\ncase\nst = sent : {0,1};\n1\n: message1;\nesac;\nnext(message2) :=\ncase\nst = sent : !message2;\n1\n: message2;\nesac;\nFAIRNESS running\nLTLSPEC G F st=sent\nFigure 3.14. The ABP sender in SMV.",
    "init(st) := sending;\nnext(st) := case\nack = message2 & !(st=sent) : sent;\n1\n: sending;\nesac;\nnext(message1) :=\ncase\nst = sent : {0,1};\n1\n: message1;\nesac;\nnext(message2) :=\ncase\nst = sent : !message2;\n1\n: message2;\nesac;\nFAIRNESS running\nLTLSPEC G F st=sent\nFigure 3.14. The ABP sender in SMV.\nsequentially. The variable message1 is the current bit of the message be-\ning sent, whereas message2 is the control bit. The deﬁnition of the mod-\nule sender is given in Figure 3.14. This module spends most of its time in\nst=sending, going only brieﬂy to st=sent when it receives an acknowledge-\nment corresponding to the control bit of the message it has been sending.\nThe variables message1 and message2 represent the actual data being sent\nand the control bit, respectively. On successful transmission, the module ob-\ntains a new message to send and returns to st=sending. The new message1\nis obtained non-deterministically (i.e., from the environment); message2 al-\nternates in value. We impose FAIRNESS running, i.e., the sender must be\nselected to run inﬁnitely often. The LTLSPEC tests that we can always suc-\nceed in sending the current message. The module receiver is programmed\nin a similar way, in Figure 3.15.\nWe also need to describe the two channels, in Figure 3.16. The acknowl-\nedgement channel is an instance of the one-bit channel one-bit-chan below.\nIts lossy character is speciﬁed by the assignment to forget. The value of\n3.3 Model checking: systems, tools, properties\n205\nMODULE receiver(message1,message2)\nVAR\nst\n: {receiving,received};\nack\n: boolean;\nexpected : boolean;\nASSIGN\ninit(st) := receiving;\nnext(st) := case\nmessage2=expected & !(st=received) : received;\n1\n: receiving;\nesac;\nnext(ack) :=\ncase\nst = received : message2;\n1\n: ack;\nesac;\nnext(expected) :=\ncase\nst = received : !expected;\n1\n: expected;\nesac;\nFAIRNESS running\nLTLSPEC G F st=received\nFigure 3.15. The ABP receiver in SMV.\ninput should be transmitted to output, unless forget is true. The two-bit",
    "esac;\nnext(ack) :=\ncase\nst = received : message2;\n1\n: ack;\nesac;\nnext(expected) :=\ncase\nst = received : !expected;\n1\n: expected;\nesac;\nFAIRNESS running\nLTLSPEC G F st=received\nFigure 3.15. The ABP receiver in SMV.\ninput should be transmitted to output, unless forget is true. The two-bit\nchannel two-bit-chan, used to send messages, is similar. Again, the non-\ndeterministic variable forget determines whether the current bit is lost or\nnot. Either both parts of the message get through, or neither of them does\n(the channel is assumed not to corrupt messages).\nThe channels have fairness constraint which are intended to model the fact\nthat, although channels can lose messages, we assume that they inﬁnitely\noften transmit the message correctly. (If this were not the case, then we\ncould ﬁnd an uninteresting violation of the liveness property, for example a\npath along which all messages from a certain time onwards get lost.)\nIt is interesting to note that the fairness constraint ‘inﬁnitely often\n!forget’ is not suﬃcient to prove the desired properties, for although it\nforces the channel to transmit inﬁnitely often, it doesn’t prevent it from\n(say) dropping all the 0 bits and transmitting all the 1 bits. That is why\nwe use the stronger fairness constraints shown. Some systems allow fairness\n206\n3 Verification by model checking\nMODULE one-bit-chan(input)\nVAR\noutput : boolean;\nforget : boolean;\nASSIGN\nnext(output) := case\nforget : output;\n1:\ninput;\nesac;\nFAIRNESS running\nFAIRNESS input & !forget\nFAIRNESS !input & !forget\nMODULE two-bit-chan(input1,input2)\nVAR\nforget : boolean;\noutput1 : boolean;\noutput2 : boolean;\nASSIGN\nnext(output1) := case\nforget : output1;\n1:\ninput1;\nesac;\nnext(output2) := case\nforget : output2;\n1:\ninput2;\nesac;\nFAIRNESS running\nFAIRNESS input1 & !forget\nFAIRNESS !input1 & !forget\nFAIRNESS input2 & !forget\nFAIRNESS !input2 & !forget\nFigure 3.16. The two modules for the two ABP channels in SMV.",
    "next(output1) := case\nforget : output1;\n1:\ninput1;\nesac;\nnext(output2) := case\nforget : output2;\n1:\ninput2;\nesac;\nFAIRNESS running\nFAIRNESS input1 & !forget\nFAIRNESS !input1 & !forget\nFAIRNESS input2 & !forget\nFAIRNESS !input2 & !forget\nFigure 3.16. The two modules for the two ABP channels in SMV.\ncontraints of the form ‘inﬁnitely often p implies inﬁnitely often q’, which\nwould be more satisfactory here, but is not allowed by SMV.\nFinally, we tie it all together with the module main (Figure 3.17). Its role\nis to connect together the components of the system, and giving them initial\nvalues of their parameters. Since the ﬁrst control bit is 0, we also initialise\nthe receiver to expect a 0. The receiver should start oﬀby sending 1 as its\n3.4 Branching-time logic\n207\nMODULE main\nVAR\ns : process sender(ack_chan.output);\nr : process receiver(msg_chan.output1,msg_chan.output2);\nmsg_chan : process two-bit-chan(s.message1,s.message2);\nack_chan : process one-bit-chan(r.ack);\nASSIGN\ninit(s.message2) := 0;\ninit(r.expected) := 0;\ninit(r.ack)\n:= 1;\ninit(msg_chan.output2) := 1;\ninit(ack_chan.output) := 1;\nLTLSPEC\nG (s.st=sent & s.message1=1 -> msg_chan.output1=1)\nFigure 3.17. The main ABP module.\nacknowledgement, so that sender does not think that its very ﬁrst message\nis being acknowledged before anything has happened. For the same reason,\nthe output of the channels is initialised to 1.\nThe speciﬁcations for ABP.\nOur SMV program satisﬁes the following spec-\niﬁcations:\nSafety: If the message bit 1 has been sent and the correct acknowledge-\nment has been returned, then a 1 was indeed received by the receiver:\nG (S.st=sent & S.message1=1 -> msg chan.output1=1).\nLiveness: Messages get through eventually. Thus, for any state there is\ninevitably a future state in which the current message has got through. In\nthe module sender, we speciﬁed G F st=sent. (This speciﬁcation could\nequivalently have been written in the main module, as G F S.st=sent.)",
    "Liveness: Messages get through eventually. Thus, for any state there is\ninevitably a future state in which the current message has got through. In\nthe module sender, we speciﬁed G F st=sent. (This speciﬁcation could\nequivalently have been written in the main module, as G F S.st=sent.)\nSimilarly, acknowledgements get through eventually. In the module\nreceiver, we write G F st=received.\n3.4 Branching-time logic\nIn our analysis of LTL (linear-time temporal logic) in the preceding sections,\nwe noted that LTL formulas are evaluated on paths. We deﬁned that a state\nof a system satisﬁes an LTL formula if all paths from the given state satisfy\nit. Thus, LTL implicitly quantiﬁes universally over paths. Therefore, prop-\nerties which assert the existence of a path cannot be expressed in LTL. This\nproblem can partly be alleviated by considering the negation of the property\nin question, and interpreting the result accordingly. To check whether there\n208\n3 Verification by model checking\nexists a path from s satisfying the LTL formula φ, we check whether all paths\nsatisfy ¬φ; a positive answer to this is a negative answer to our original ques-\ntion, and vice versa. We used this approach when analysing the ferryman\npuzzle in the previous section. However, as already noted, properties which\nmix universal and existential path quantiﬁers cannot in general be model\nchecked using this approach, because the complement formula still has a mix.\nBranching-time logics solve this problem by allowing us to quantify ex-\nplicitly over paths. We will examine a logic known as Computation Tree\nLogic, or CTL. In CTL, as well as the temporal operators U, F, G and X of\nLTL we also have quantiﬁers A and E which express ‘all paths’ and ‘exists\na path’, respectively. For example, we can write:\nr There is a reachable state satisfying q: this is written EF q.\nr From all reachable states satisfying p, it is possible to maintain p continuously",
    "LTL we also have quantiﬁers A and E which express ‘all paths’ and ‘exists\na path’, respectively. For example, we can write:\nr There is a reachable state satisfying q: this is written EF q.\nr From all reachable states satisfying p, it is possible to maintain p continuously\nuntil reaching a state satisfying q: this is written AG (p →E[p U q]).\nr Whenever a state satisfying p is reached, the system can exhibit q continuously\nforevermore: AG (p →EG q).\nr There is a reachable state from which all reachable states satisfy p: EF AG p.\n3.4.1 Syntax of CTL\nComputation Tree Logic, or CTL for short, is a branching-time logic, mean-\ning that its model of time is a tree-like structure in which the future is not\ndetermined; there are diﬀerent paths in the future, any one of which might\nbe the ‘actual’ path that is realised.\nAs before, we work with a ﬁxed set of atomic formulas/descriptions (such\nas p, q, r, . . . , or p1, p2, . . . ).\nDeﬁnition 3.12 We deﬁne CTL formulas inductively via a Backus Naur\nform as done for LTL:\nφ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | AX φ | EX φ |\nAF φ | EF φ | AG φ | EG φ | A[φ U φ] | E[φ U φ]\nwhere p ranges over a set of atomic formulas.\nNotice that each of the CTL temporal connectives is a pair of symbols.\nThe ﬁrst of the pair is one of A and E. A means ‘along All paths’ (inevitably)\nand E means ‘along at least (there Exists) one path’ (possibly). The second\none of the pair is X, F, G, or U, meaning ‘neXt state,’ ‘some Future state,’\n‘all future states (Globally)’ and Until, respectively. The pair of symbols\nin E[φ1 U φ2], for example, is EU. In CTL, pairs of symbols like EU are\n3.4 Branching-time logic\n209\nindivisible. Notice that AU and EU are binary. The symbols X, F, G and\nU cannot occur without being preceded by an A or an E; similarly, every A\nor E must have one of X, F, G and U to accompany it.\nUsually weak-until (W) and release (R) are not included in CTL, but they\nare derivable (see Section 3.4.5).",
    "U cannot occur without being preceded by an A or an E; similarly, every A\nor E must have one of X, F, G and U to accompany it.\nUsually weak-until (W) and release (R) are not included in CTL, but they\nare derivable (see Section 3.4.5).\nConvention 3.13 We assume similar binding priorities for the CTL con-\nnectives to what we did for propositional and predicate logic. The unary\nconnectives (consisting of ¬ and the temporal connectives AG, EG, AF, EF,\nAX and EX) bind most tightly. Next in the order come ∧and ∨; and after\nthat come →, AU and EU .\nNaturally, we can use brackets in order to override these priorities. Let\nus see some examples of well-formed CTL formulas and some examples\nwhich are not well-formed, in order to understand the syntax. Suppose\nthat p, q and r are atomic formulas. The following are well-formed CTL\nformulas:\nr AG (q →EG r), note that this is not the same as AG q →EG r, for according to\nConvention 3.13, the latter formula means (AG q) →(EG r)\nr EF E[r U q]\nr A[p U EF r]\nr EF EG p →AF r, again, note that this binds as (EF EG p) →AF r, not\nEF (EG p →AF r) or EF EG (p →AF r)\nr A[p1 U A[p2 U p3]]\nr E[A[p1 U p2] U p3]\nr AG (p →A[p U (¬p ∧A[¬p U q])]).\nIt is worth spending some time seeing how the syntax rules allow us to\nconstruct each of these. The following are not well-formed formulas:\nr EF G r\nr A¬G ¬p\nr F [r U q]\nr EF (r U q)\nr AEF r\nr A[(r U q) ∧(p U r)].\nIt is especially worth understanding why the syntax rules don’t allow us to\nconstruct these. For example, take EF (r U q). The problem with this string\nis that U can occur only when paired with an A or an E. The E we have is\npaired with the F. To make this into a well-formed CTL formula, we would\nhave to write EF E[r U q] or EF A[r U q].\n210\n3 Verification by model checking\nAU\nEU\nAX\n¬\n¬\nEX\np\np\n∧\nq\np\nFigure 3.18. The parse tree of a CTL formula without infix notation.\nNotice that we use square brackets after the A or E, when the paired",
    "have to write EF E[r U q] or EF A[r U q].\n210\n3 Verification by model checking\nAU\nEU\nAX\n¬\n¬\nEX\np\np\n∧\nq\np\nFigure 3.18. The parse tree of a CTL formula without infix notation.\nNotice that we use square brackets after the A or E, when the paired\noperator is a U. There is no strong reason for this; you could use ordinary\nround brackets instead. However, it often helps one to read the formula\n(because we can more easily spot where the corresponding close bracket is).\nAnother reason for using the square brackets is that SMV insists on it.\nThe reason A[(r U q) ∧(p U r)] is not a well-formed formula is that the\nsyntax does not allow us to put a boolean connective (like ∧) directly inside\nA[ ] or E[ ]. Occurrences of A or E must be followed by one of G, F, X or U;\nwhen they are followed by U, it must be in the form A[φ U ψ]. Now, the φ\nand the ψ may contain ∧, since they are arbitrary formulas; so A[(p ∧q) U\n(¬r →q)] is a well-formed formula.\nObserve that AU and EU are binary connectives which mix inﬁx and\npreﬁx notation. In pure inﬁx, we would write φ1 AU φ2, whereas in pure\npreﬁx we would write AU(φ1, φ2).\nAs with any formal language, and as we did in the previous two chapters,\nit is useful to draw parse trees for well-formed formulas. The parse tree for\nA[AX ¬p U E[EX (p ∧q) U ¬p]] is shown in Figure 3.18.\nDeﬁnition 3.14 A subformula of a CTL formula φ is any formula ψ whose\nparse tree is a subtree of φ’s parse tree.\n3.4 Branching-time logic\n211\n3.4.2 Semantics of computation tree logic\nCTL formulas are interpreted over transition systems (Deﬁnition 3.4). Let\nM = (S, →, L) be such a model, s ∈S and φ a CTL formula. The deﬁnition\nof whether M, s ⊨φ holds is recursive on the structure of φ, and can be\nroughly understood as follows:\nr If φ is atomic, satisfaction is determined by L.\nr If the top-level connective of φ (i.e., the connective occurring top-most in the\nparse tree of φ) is a boolean connective (∧, ∨, ¬, ⊤etc.) then the satisfaction",
    "roughly understood as follows:\nr If φ is atomic, satisfaction is determined by L.\nr If the top-level connective of φ (i.e., the connective occurring top-most in the\nparse tree of φ) is a boolean connective (∧, ∨, ¬, ⊤etc.) then the satisfaction\nquestion is answered by the usual truth-table deﬁnition and further recursion\ndown φ.\nr If the top level connective is an operator beginning A, then satisfaction holds if\nall paths from s satisfy the ‘LTL formula’ resulting from removing the A symbol.\nr Similarly, if the top level connective begins with E, then satisfaction holds if\nsome path from s satisfy the ‘LTL formula’ resulting from removing the E.\nIn the last two cases, the result of removing A or E is not strictly an LTL\nformula, for it may contain further As or Es below. However, these will be\ndealt with by the recursion.\nThe formal deﬁnition of M, s ⊨φ is a bit more verbose:\nDeﬁnition 3.15 Let M = (S, →, L) be a model for CTL, s in S, φ a CTL\nformula. The relation M, s ⊨φ is deﬁned by structural induction on φ:\n1.\nM, s ⊨⊤and M, s ̸⊨⊥\n2.\nM, s ⊨p iﬀp ∈L(s)\n3.\nM, s ⊨¬φ iﬀM, s ̸⊨φ\n4.\nM, s ⊨φ1 ∧φ2 iﬀM, s ⊨φ1 and M, s ⊨φ2\n5.\nM, s ⊨φ1 ∨φ2 iﬀM, s ⊨φ1 or M, s ⊨φ2\n6.\nM, s ⊨φ1 →φ2 iﬀM, s ̸⊨φ1 or M, s ⊨φ2.\n7.\nM, s ⊨AX φ iﬀfor all s1 such that s →s1 we have M, s1 ⊨φ. Thus, AX says:\n‘in every next state.’\n8.\nM, s ⊨EX φ iﬀfor some s1 such that s →s1 we have M, s1 ⊨φ. Thus, EX\nsays: ‘in some next state.’ E is dual to A – in exactly the same way that ∃is\ndual to ∀in predicate logic.\n9.\nM, s ⊨AG φ holds iﬀfor all paths s1 →s2 →s3 →. . ., where s1 equals s, and\nall si along the path, we have M, si ⊨φ. Mnemonically: for All computation\npaths beginning in s the property φ holds Globally. Note that ‘along the path’\nincludes the path’s initial state s.\n10.\nM, s ⊨EG φ holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals s,\nand for all si along the path, we have M, si ⊨φ. Mnemonically: there Exists\na path beginning in s such that φ holds Globally along the path.\n212",
    "includes the path’s initial state s.\n10.\nM, s ⊨EG φ holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals s,\nand for all si along the path, we have M, si ⊨φ. Mnemonically: there Exists\na path beginning in s such that φ holds Globally along the path.\n212\n3 Verification by model checking\nφ\nFigure 3.19. A system whose starting state satisfies EF φ.\n11.\nM, s ⊨AF φ holds iﬀfor all paths s1 →s2 →. . ., where s1 equals s, there is\nsome si such that M, si ⊨φ. Mnemonically: for All computation paths begin-\nning in s there will be some Future state where φ holds.\n12.\nM, s ⊨EF φ holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals s,\nand for some si along the path, we have M, si ⊨φ. Mnemonically: there Exists\na computation path beginning in s such that φ holds in some Future state;\n13.\nM, s ⊨A[φ1 U φ2] holds iﬀfor all paths s1 →s2 →s3 →. . ., where s1 equals\ns, that path satisﬁes φ1 U φ2, i.e., there is some si along the path, such that\nM, si ⊨φ2, and, for each j < i, we have M, sj ⊨φ1. Mnemonically: All com-\nputation paths beginning in s satisfy that φ1 Until φ2 holds on it.\n14.\nM, s ⊨E[φ1 U φ2] holds iﬀthere is a path s1 →s2 →s3 →. . ., where s1 equals\ns, and that path satisﬁes φ1 U φ2 as speciﬁed in 13. Mnemonically: there Exists\na computation path beginning in s such that φ1 Until φ2 holds on it.\nClauses 9–14 above refer to computation paths in models. It is there-\nfore useful to visualise all possible computation paths from a given state\ns by unwinding the transition system to obtain an inﬁnite computation\ntree, whence ‘computation tree logic.’ The diagrams in Figures 3.19–3.22\nshow schematically systems whose starting states satisfy the formulas EF φ,\nEG φ, AG φ and AF φ, respectively. Of course, we could add more φ to any\nof these diagrams and still preserve the satisfaction – although there is noth-\ning to add for AG . The diagrams illustrate a ‘least’ way of satisfying the\nformulas.\n3.4 Branching-time logic\n213\nφ\nφ\nφ",
    "EG φ, AG φ and AF φ, respectively. Of course, we could add more φ to any\nof these diagrams and still preserve the satisfaction – although there is noth-\ning to add for AG . The diagrams illustrate a ‘least’ way of satisfying the\nformulas.\n3.4 Branching-time logic\n213\nφ\nφ\nφ\nFigure 3.20. A system whose starting state satisfies EG φ.\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nφ\nFigure 3.21. A system whose starting state satisfies AG φ.\nRecall the transition system of Figure 3.3 for the designated starting state\ns0, and the inﬁnite tree illustrated in Figure 3.5. Let us now look at some\nexample checks for this system.\n1.\nM, s0 ⊨p ∧q holds since the atomic symbols p and q are contained in the node\nof s0.\n2.\nM, s0 ⊨¬r holds since the atomic symbol r is not contained in node s0.\n214\n3 Verification by model checking\nφ\nφ\nφ\nφ\nφ\nFigure 3.22. A system whose starting state satisfies AF φ.\n3.\nM, s0 ⊨⊤holds by deﬁnition.\n4.\nM, s0 ⊨EX (q ∧r) holds since we have the leftmost computation path s0 →\ns1 →s0 →s1 →. . . in Figure 3.5, whose second node s1 contains q and r.\n5.\nM, s0 ⊨¬AX (q ∧r) holds since we have the rightmost computation path s0 →\ns2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 only contains r, but\nnot q.\n6.\nM, s0 ⊨¬EF (p ∧r) holds since there is no computation path beginning in s0\nsuch that we could reach a state where p ∧r would hold. This is so because\nthere is simply no state whatsoever in this system where p and r hold at the\nsame time.\n7.\nM, s2 ⊨EG r holds since there is a computation path s2 →s2 →s2 →. . .\nbeginning in s2 such that r holds in all future states of that path; this is\nthe only computation path beginning at s2 and so M, s2 ⊨AG r holds as well.\n8.\nM, s0 ⊨AF r holds since, for all computation paths beginning in s0, the system\nreaches a state (s1 or s2) such that r holds.\n9.\nM, s0 ⊨E[(p ∧q) U r] holds since we have the rightmost computation path\ns0 →s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 (i = 1) satisﬁes",
    "8.\nM, s0 ⊨AF r holds since, for all computation paths beginning in s0, the system\nreaches a state (s1 or s2) such that r holds.\n9.\nM, s0 ⊨E[(p ∧q) U r] holds since we have the rightmost computation path\ns0 →s2 →s2 →s2 →. . . in Figure 3.5, whose second node s2 (i = 1) satisﬁes\nr, but all previous nodes (only j = 0, i.e., node s0) satisfy p ∧q.\n10.\nM, s0 ⊨A[p U r] holds since p holds at s0 and r holds in any possible successor\nstate of s0, so p U r is true for all computation paths beginning in s0 (so we\nmay choose i = 1 independently of the path).\n11.\nM, s0 ⊨AG (p ∨q ∨r →EF EG r) holds since in all states reachable from s0\nand satisfying p ∨q ∨r (all states in this case) the system can reach a state\nsatisfying EG r (in this case state s2).\n3.4 Branching-time logic\n215\n3.4.3 Practical patterns of specifications\nIt’s useful to look at some typical examples of formulas, and compare the sit-\nuation with LTL (Section 3.2.3). Suppose atomic descriptions include some\nwords such as busy and requested.\nr It is possible to get to a state where started holds, but ready doesn’t:\nEF (started ∧¬ready). To express impossibility, we simply negate the formula.\nr For any state, if a request (of some resource) occurs, then it will eventually be\nacknowledged:\nAG (requested →AF acknowledged).\nr The property that if the process is enabled inﬁnitely often, then it runs in-\nﬁnitely often, is not expressible in CTL. In particular, it is not expressed by\nAG AF enabled →AG AF running, or indeed any other insertion of A or E into\nthe corresponding LTL formula. The CTL formula just given expresses that if\nevery path has inﬁnitely often enabled, then every path is inﬁnitely often taken;\nthis is much weaker than asserting that every path which has inﬁnitely often\nenabled is inﬁnitely often taken.\nr A certain process is enabled inﬁnitely often on every computation path:\nAG (AF enabled).\nr Whatever happens, a certain process will eventually be permanently deadlocked:\nAF (AG deadlock).",
    "enabled is inﬁnitely often taken.\nr A certain process is enabled inﬁnitely often on every computation path:\nAG (AF enabled).\nr Whatever happens, a certain process will eventually be permanently deadlocked:\nAF (AG deadlock).\nr From any state it is possible to get to a restart state:\nAG (EF restart).\nr An upwards travelling lift at the second ﬂoor does not change its direction when\nit has passengers wishing to go to the ﬁfth ﬂoor:\nAG (floor2 ∧directionup ∧ButtonPressed5 →A[directionup U floor5])\nHere, our atomic descriptions are boolean expressions built from system vari-\nables, e.g., floor2.\nr The lift can remain idle on the third ﬂoor with its doors closed:\nAG (floor3 ∧idle ∧doorclosed →EG (floor3 ∧idle ∧doorclosed)).\nr A process can always request to enter its critical section. Recall that this was\nnot expressible in LTL. Using the propositions of Figure 3.8, this may be written\nAG (n1 →EX t1) in CTL.\nr Processes need not enter their critical section in strict sequence. This was also\nnot expressible in LTL, though we expressed its negation. CTL allows us to\nexpress it directly: EF (c1 ∧E[c1 U (¬c1 ∧E[¬c2 U c1])]).\n3.4.4 Important equivalences between CTL formulas\nDeﬁnition 3.16 Two CTL formulas φ and ψ are said to be semantically\nequivalent if any state in any model which satisﬁes one of them also satisﬁes\nthe other; we denote this by φ ≡ψ.\n216\n3 Verification by model checking\nWe have already noticed that A is a universal quantiﬁer on paths and E\nis the corresponding existential quantiﬁer. Moreover, G and F are also uni-\nversal and existential quantiﬁers, ranging over the states along a particular\npath. In view of these facts, it is not surprising to ﬁnd that de Morgan rules\nexist:\n¬AF φ ≡EG ¬φ\n¬EF φ ≡AG ¬φ\n(3.6)\n¬AX φ ≡EX ¬φ.\nWe also have the equivalences\nAF φ ≡A[⊤U φ]\nEF φ ≡E[⊤U φ]\nwhich are similar to the corresponding equivalences in LTL.\n3.4.5 Adequate sets of CTL connectives\nAs in propositional logic and in LTL, there is some redundancy among the",
    "exist:\n¬AF φ ≡EG ¬φ\n¬EF φ ≡AG ¬φ\n(3.6)\n¬AX φ ≡EX ¬φ.\nWe also have the equivalences\nAF φ ≡A[⊤U φ]\nEF φ ≡E[⊤U φ]\nwhich are similar to the corresponding equivalences in LTL.\n3.4.5 Adequate sets of CTL connectives\nAs in propositional logic and in LTL, there is some redundancy among the\nCTL connectives. For example, the connective AX can be written ¬EX ¬;\nand AG, AF, EG and EF can be written in terms of AU and EU as follows:\nﬁrst, write AG φ as ¬EF ¬φ and EG φ as ¬AF ¬φ, using (3.6), and then use\nAF φ ≡A[⊤U φ] and EF φ ≡E[⊤U φ]. Therefore AU, EU and EX form\nan adequate set of temporal connectives.\nAlso EG, EU, and EX form an adequate set, for we have the equivalence\nA[φ U ψ] ≡¬(E[¬ψ U (¬φ ∧¬ψ)] ∨EG ¬ψ)\n(3.7)\nwhich can be proved as follows:\nA[φ U ψ] ≡A[¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ]\n≡¬E¬[¬(¬ψ U (¬φ ∧¬ψ)) ∧F ψ]\n≡¬E[(¬ψ U (¬φ ∧¬ψ)) ∨G ¬ψ]\n≡¬(E[¬ψ U (¬φ ∧¬ψ)] ∨EG ¬ψ).\nThe ﬁrst line is by Theorem 3.10, and the remainder by elementary manipu-\nlation. (This proof involves intermediate formulas which violate the syntactic\nformation rules of CTL; however, it is valid in the logic CTL* introduced in\nthe next section.) More generally, we have:\nTheorem 3.17 A set of temporal connectives in CTL is adequate if, and\nonly if, it contains at least one of {AX , EX }, at least one of {EG , AF , AU }\nand EU .\n3.5 CTL* and the expressive powers of LTL and CTL\n217\nThis theorem is proved in a paper referenced in the bibliographic notes\nat the end of the chapter. The connective EU plays a special role in that\ntheorem because neither weak-until W nor release R are primitive in CTL\n(Deﬁnition 3.12). The temporal connectives AR, ER, AW and EW are all\ndeﬁnable in CTL:\nr A[φ R ψ] = ¬E[¬φ U ¬ψ]\nr E[φ R ψ] = ¬A[¬φ U ¬ψ]\nr A[φ W ψ] = A[ψ R (φ ∨ψ)], and then use the ﬁrst equation above\nr E[φ W ψ] = E[ψ R (φ ∨ψ)], and then use the second one.\nThese deﬁnitions are justiﬁed by LTL equivalences in Sections 3.2.4\nand 3.2.5. Some other noteworthy equivalences in CTL are the following:\nAG φ ≡φ ∧AX AG φ",
    "r A[φ W ψ] = A[ψ R (φ ∨ψ)], and then use the ﬁrst equation above\nr E[φ W ψ] = E[ψ R (φ ∨ψ)], and then use the second one.\nThese deﬁnitions are justiﬁed by LTL equivalences in Sections 3.2.4\nand 3.2.5. Some other noteworthy equivalences in CTL are the following:\nAG φ ≡φ ∧AX AG φ\nEG φ ≡φ ∧EX EG φ\nAF φ ≡φ ∨AX AF φ\nEF φ ≡φ ∨EX EF φ\nA[φ U ψ] ≡ψ ∨(φ ∧AX A[φ U ψ])\nE[φ U ψ] ≡ψ ∨(φ ∧EX E[φ U ψ]).\nFor example, the intuition for the third one is the following: in order to have\nAF φ in a particular state, φ must be true at some point along each path\nfrom that state. To achieve this, we either have φ true now, in the current\nstate; or we postpone it, in which case we must have AF φ in each of the next\nstates. Notice how this equivalence appears to deﬁne AF in terms of AX\nand AF itself, an apparently circular deﬁnition. In fact, these equivalences\ncan be used to deﬁne the six connectives on the left in terms of AX and\nEX , in a non-circular way. This is called the ﬁxed-point characterisation of\nCTL; it is the mathematical foundation for the model-checking algorithm\ndeveloped in Section 3.6.1; and we return to it later (Section 3.7).\n3.5 CTL* and the expressive powers of LTL and CTL\nCTL allows explicit quantiﬁcation over paths, and in this respect it is more\nexpressive than LTL, as we have seen. However, it does not allow one to\nselect a range of paths by describing them with a formula, as LTL does.\nIn that respect, LTL is more expressive. For example, in LTL we can say\n‘all paths which have a p along them also have a q along them,’ by writing\nF p →F q. It is not possible to write this in CTL because of the constraint\nthat every F has an associated A or E. The formula AF p →AF q means\n218\n3 Verification by model checking\nsomething quite diﬀerent: it says ‘if all paths have a p along them, then\nall paths have a q along them.’ One might write AG (p →AF q), which is\ncloser, since it says that every way of extending every path to a p eventually",
    "218\n3 Verification by model checking\nsomething quite diﬀerent: it says ‘if all paths have a p along them, then\nall paths have a q along them.’ One might write AG (p →AF q), which is\ncloser, since it says that every way of extending every path to a p eventually\nmeets a q, but that is still not capturing the meaning of F p →F q.\nCTL* is a logic which combines the expressive powers of LTL and CTL,\nby dropping the CTL constraint that every temporal operator (X, U, F, G)\nhas to be associated with a unique path quantiﬁer (A, E). It allows us to\nwrite formulas such as\nr A[(p U r) ∨(q U r)]: along all paths, either p is true until r, or q is true until r.\nr A[X p ∨X X p]: along all paths, p is true in the next state, or the next but one.\nr E[G F p]: there is a path along which p is inﬁnitely often true.\nThese formulas are not equivalent to, respectively, A[(p ∨q) U r)], AX p ∨\nAX AX p and EG EF p. It turns out that the ﬁrst of them can be written\nas a (rather long) CTL formula. The second and third do not have a CTL\nequivalent.\nThe syntax of CTL* involves two classes of formulas:\nr state formulas, which are evaluated in states:\nφ ::= ⊤| p | (¬φ) | (φ ∧φ) | A[α] | E[α]\nwhere p is any atomic formula and α any path formula; and\nr path formulas, which are evaluated along paths:\nα ::= φ | (¬α) | (α ∧α) | (α U α) | (G α) | (F α) | (X α)\nwhere φ is any state formula. This is an example of an inductive deﬁnition\nwhich is mutually recursive: the deﬁnition of each class depends upon the\ndeﬁnition of the other, with base cases p and ⊤.\nLTL and CTL as subsets of CTL*\nAlthough the syntax of LTL does\nnot include A and E, the semantic viewpoint of LTL is that we consider\nall paths. Therefore, the LTL formula α is equivalent to the CTL* formula\nA[α]. Thus, LTL can be viewed as a subset of CTL*.\nCTL is also a subset of CTL*, since it is the fragment of CTL* in which\nwe restrict the form of path formulas to\nα ::= (φ U φ) | (G φ) | (F φ) | (X φ)",
    "all paths. Therefore, the LTL formula α is equivalent to the CTL* formula\nA[α]. Thus, LTL can be viewed as a subset of CTL*.\nCTL is also a subset of CTL*, since it is the fragment of CTL* in which\nwe restrict the form of path formulas to\nα ::= (φ U φ) | (G φ) | (F φ) | (X φ)\nFigure 3.23 shows the relationship among the expressive powers of CTL,\nLTL and CTL*. Here are some examples of formulas in each of the subsets\n3.5 CTL* and the expressive powers of LTL and CTL\n219\nLTL\nψ1\nψ2\nψ3\nψ4\nCTL\nCTL*\nFigure 3.23. The expressive powers of CTL, LTL and CTL*.\nshown:\nIn CTL but not in LTL: ψ1\ndef\n= AG EF p. This expresses: wherever we\nhave got to, we can always get to a state in which p is true. This is\nalso useful, e.g., in ﬁnding deadlocks in protocols.\nThe proof that AG EF p is not expressible in LTL is as follows. Let φ be\nan LTL formula such that A[φ] is allegedly equivalent to AG EF p. Since\nM, s ⊨AG EF p in the left-hand diagram below, we have M, s ⊨A[φ].\nNow let M′ be as shown in the right-hand diagram. The paths from s\nin M′ are a subset of those from s in M, so we have M′, s ⊨A[φ]. Yet,\nit is not the case that M′, s ⊨AG EF p; a contradiction.\n¬p\n¬p\np\ns\ns\nt\nIn CTL*, but neither in CTL nor in LTL: ψ4\ndef\n= E[G F p],\nsaying\nthat\nthere is a path with inﬁnitely many p.\nThe proof that this is not expressible in CTL is quite complex and may\nbe found in the papers co-authored by E. A. Emerson with others, given\nin the references. (Why is it not expressible in LTL?)\nIn LTL but not in CTL: ψ3\ndef\n= A[G F p →F q], saying that if there are in-\nﬁnitely many p along the path, then there is an occurrence of q. This\nis an interesting thing to be able to say; for example, many fairness\nconstraints are of the form ‘inﬁnitely often requested implies eventually\nacknowledged’.\nIn LTL and CTL: ψ2\ndef\n= AG (p →AF q) in CTL, or G (p →F q) in LTL:\nany p is eventually followed by a q.\nRemark 3.18 We just saw that some (but not all) LTL formulas can be",
    "constraints are of the form ‘inﬁnitely often requested implies eventually\nacknowledged’.\nIn LTL and CTL: ψ2\ndef\n= AG (p →AF q) in CTL, or G (p →F q) in LTL:\nany p is eventually followed by a q.\nRemark 3.18 We just saw that some (but not all) LTL formulas can be\nconverted into CTL formulas by adding an A to each temporal operator. For\n220\n3 Verification by model checking\na positive example, the LTL formula G (p →F q) is equivalent to the CTL\nformula AG (p →AF q). We discuss two more negative examples:\nr F G p and AF AG p are not equivalent, since F G p is satisﬁed, whereas AF AG p\nis not satisﬁed, in the model\np\n¬p\np\nIn fact, AF AG p is strictly stronger than F G p.\nr While the LTL formulas X F p and F X p are equivalent, and they are equivalent\nto the CTL formula AX AF p, they are not equivalent to AF AX p. The latter\nis strictly stronger, and has quite a strange meaning (try working it out).\nRemark 3.19 There is a considerable literature comparing linear-time and\nbranching-time logics. The question of which one is ‘better’ has been debated\nfor about 20 years. We have seen that they have incomparable expressive\npowers. CTL* is more expressive than either of them, but is computationally\nmuch more expensive (as will be seen in Section 3.6). The choice between\nLTL and CTL depends on the application at hand, and on personal prefer-\nence. LTL lacks CTL’s ability to quantify over paths, and CTL lacks LTL’s\nﬁner-grained ability to describe individual paths. To many people, LTL ap-\npears to be more straightforward to use; as noted above, CTL formulas like\nAF AX p seem hard to understand.\n3.5.1 Boolean combinations of temporal formulas in CTL\nCompared with CTL*, the syntax of CTL is restricted in two ways: it does\nnot allow boolean combinations of path formulas and it does not allow nest-\ning of the path modalities X, F and G. Indeed, we have already seen exam-\nples of the inexpressibility in CTL of nesting of path modalities, namely the\nformulas ψ3 and ψ4 above.",
    "not allow boolean combinations of path formulas and it does not allow nest-\ning of the path modalities X, F and G. Indeed, we have already seen exam-\nples of the inexpressibility in CTL of nesting of path modalities, namely the\nformulas ψ3 and ψ4 above.\nIn this section, we see that the ﬁrst of these restrictions is only apparent;\nwe can ﬁnd equivalents in CTL for formulas having boolean combinations\nof path formulas. The idea is to translate any CTL formula having boolean\ncombinations of path formulas into a CTL formula that doesn’t. For exam-\nple, we may see that E[F p ∧F q] ≡EF [p ∧EF q] ∨EF [q ∧EF p] since, if\nwe have F p ∧F q along any path, then either the p must come before the q,\nor the other way around, corresponding to the two disjuncts on the right.\n(If the p and q occur simultaneously, then both disjuncts are true.)\n3.6 Model-checking algorithms\n221\nSince U is like F (only with the extra complication of its ﬁrst argument),\nwe ﬁnd the following equivalence:\nE[(p1 U q1) ∧(p2 U q2)] ≡E[(p1 ∧p2) U (q1 ∧E[p2 U q2])]\n∨E[(p1 ∧p2) U (q2 ∧E[p1 U q1])].\nAnd from the CTL equivalence A[p U q] ≡¬(E[¬q U (¬p ∧¬q)] ∨EG ¬q)\n(see Theorem 3.10) we can obtain E[¬(p U q)]\n≡\nE[¬q U (¬p ∧¬q)] ∨\nEG ¬q. Other identities we need in this translation include E[¬X p]\n≡\nEX ¬p.\n3.5.2 Past operators in LTL\nThe temporal operators X, U, F, etc. which we have seen so far refer to the\nfuture. Sometimes we want to encode properties that refer to the past, such\nas: ‘whenever q occurs, then there was some p in the past.’ To do this, we\nmay add the operators Y, S, O, H. They stand for yesterday, since, once, and\nhistorically, and are the past analogues of X, U, F, G, respectively. Thus,\nthe example formula may be written G (q →O p).\nNuSMV supports past operators in LTL. One could also add past opera-\ntors to CTL (AY, ES, etc.) but NuSMV does not support them.\nSomewhat counter-intuitively, past operators do not increase the expres-",
    "the example formula may be written G (q →O p).\nNuSMV supports past operators in LTL. One could also add past opera-\ntors to CTL (AY, ES, etc.) but NuSMV does not support them.\nSomewhat counter-intuitively, past operators do not increase the expres-\nsive power of LTL. That is to say, every LTL formula with past operators\ncan be written equivalently without them. The example formula above can\nbe written ¬p W q, or equivalently ¬(¬q U (p ∧¬q)) if one wants to avoid\nW. This result is surprising, because it seems that being able to talk about\nthe past as well as the future allows more expressivity than talking about\nthe future alone. However, recall that LTL equivalence is quite crude: it says\nthat the two formulas are satisﬁed by exactly the same set of paths. The\npast operators allow us to travel backwards along the path, but only to reach\npoints we could have reached by travelling forwards from its beginning. In\ncontrast, adding past operators to CTL does increase its expressive power,\nbecause they can allow us to examine states not forward-reachable from the\npresent one.\n3.6 Model-checking algorithms\nThe semantic deﬁnitions for LTL and CTL presented in Sections 3.2 and 3.4\nallow us to test whether the initial states of a given system satisfy an LTL or\nCTL formula. This is the basic model-checking question. In general, inter-\nesting transition systems will have a huge number of states and the formula\n222\n3 Verification by model checking\nwe are interested in checking may be quite long. It is therefore well worth\ntrying to ﬁnd eﬃcient algorithms.\nAlthough LTL is generally preferred by speciﬁers, as already noted, we\nstart with CTL model checking because its algorithm is simpler.\n3.6.1 The CTL model-checking algorithm\nHumans may ﬁnd it easier to do model checks on the unwindings of models\ninto inﬁnite trees, given a designated initial state, for then all possible paths\nare plainly visible. However, if we think of implementing a model checker",
    "3.6.1 The CTL model-checking algorithm\nHumans may ﬁnd it easier to do model checks on the unwindings of models\ninto inﬁnite trees, given a designated initial state, for then all possible paths\nare plainly visible. However, if we think of implementing a model checker\non a computer, we certainly cannot unwind transition systems into inﬁ-\nnite trees. We need to do checks on ﬁnite data structures. For this reason,\nwe now have to develop new insights into the semantics of CTL. Such a\ndeeper understanding will provide the basis for an eﬃcient algorithm which,\ngiven M, s ∈S and φ, computes whether M, s ⊨φ holds. In the case that\nφ is not satisﬁed, such an algorithm can be augmented to produce an ac-\ntual path (= run) of the system demonstrating that M cannot satisfy φ.\nThat way, we may debug a system by trying to ﬁx what enables runs which\nrefute φ.\nThere are various ways in which one could consider\nM, s0\n?\n⊨φ\nas a computational problem. For example, one could have the model M, the\nformula φ and a state s0 as input; one would then expect a reply of the form\n‘yes’ (M, s0 ⊨φ holds), or ‘no’ (M, s0 ⊨φ does not hold). Alternatively, the\ninputs could be just M and φ, where the output would be all states s of the\nmodel M which satisfy φ.\nIt turns out that it is easier to provide an algorithm for solving the second\nof these two problems. This automatically gives us a solution to the ﬁrst one,\nsince we can simply check whether s0 is an element of the output set.\nThe labelling algorithm\nWe present an algorithm which, given a model\nand a CTL formula, outputs the set of states of the model that satisfy the\nformula. The algorithm does not need to be able to handle every CTL con-\nnective explicitly, since we have already seen that the connectives ⊥, ¬ and\n∧form an adequate set as far as the propositional connectives are concerned;\nand AF , EU and EX form an adequate set of temporal connectives. Given\nan arbitrary CTL formula φ, we would simply pre-process φ in order to write",
    "∧form an adequate set as far as the propositional connectives are concerned;\nand AF , EU and EX form an adequate set of temporal connectives. Given\nan arbitrary CTL formula φ, we would simply pre-process φ in order to write\nit in an equivalent form in terms of the adequate set of connectives, and then\n3.6 Model-checking algorithms\n223\nRepeat. . .\nAFψ1\nAFψ1\nAFψ1\nAFψ1\nAFψ1\nAFψ1\nAFψ1\n. . . until no change.\nFigure 3.24. The iteration step of the procedure for labelling states with\nsubformulas of the form AF ψ1.\ncall the model-checking algorithm. Here is the algorithm:\nINPUT: a CTL model M = (S, →, L) and a CTL formula φ.\nOUTPUT: the set of states of M which satisfy φ.\nFirst, change φ to the output of TRANSLATE (φ), i.e., we write φ in terms\nof the connectives AF, EU, EX, ∧, ¬ and ⊥using the equivalences given\nearlier in the chapter. Next, label the states of M with the subformulas of φ\nthat are satisﬁed there, starting with the smallest subformulas and working\noutwards towards φ.\nSuppose ψ is a subformula of φ and states satisfying all the immediate\nsubformulas of ψ have already been labelled. We determine by a case analysis\nwhich states to label with ψ. If ψ is\nr ⊥: then no states are labelled with ⊥.\nr p: then label s with p if p ∈L(s).\nr ψ1 ∧ψ2: label s with ψ1 ∧ψ2 if s is already labelled both with ψ1 and with ψ2.\nr ¬ψ1: label s with ¬ψ1 if s is not already labelled with ψ1.\nr AF ψ1:\n– If any state s is labelled with ψ1, label it with AF ψ1.\n– Repeat: label any state with AF ψ1 if all successor states are labelled with\nAF ψ1, until there is no change. This step is illustrated in Figure 3.24.\nr E[ψ1 U ψ2]:\n– If any state s is labelled with ψ2, label it with E[ψ1 U ψ2].\n– Repeat: label any state with E[ψ1 U ψ2] if it is labelled with ψ1 and at least\none of its successors is labelled with E[ψ1 U ψ2], until there is no change. This\nstep is illustrated in Figure 3.25.\nr EX ψ1: label any state with EX ψ1 if one of its successors is labelled with ψ1.\n224",
    "– Repeat: label any state with E[ψ1 U ψ2] if it is labelled with ψ1 and at least\none of its successors is labelled with E[ψ1 U ψ2], until there is no change. This\nstep is illustrated in Figure 3.25.\nr EX ψ1: label any state with EX ψ1 if one of its successors is labelled with ψ1.\n224\n3 Verification by model checking\nψ1\nψ1\nE[ψ1 U ψ2]\nE[ψ1 U ψ2]\nE[ψ1 U ψ2]\nFigure 3.25. The iteration step of the procedure for labelling states with\nsubformulas of the form E[ψ1 U ψ2].\nHaving performed the labelling for all the subformulas of φ (including φ\nitself), we output the states which are labelled φ.\nThe complexity of this algorithm is O(f · V · (V + E)), where f is the\nnumber of connectives in the formula, V is the number of states and E is\nthe number of transitions; the algorithm is linear in the size of the formula\nand quadratic in the size of the model.\nHandling EG directly\nInstead of using a minimal adequate set of con-\nnectives, it would have been possible to write similar routines for the other\nconnectives. Indeed, this would probably be more eﬃcient. The connectives\nAG and EG require a slightly diﬀerent approach from that for the others,\nhowever. Here is the algorithm to deal with EG ψ1 directly:\nr EG ψ1:\n– Label all the states with EG ψ1.\n– If any state s is not labelled with ψ1, delete the label EG ψ1.\n– Repeat: delete the label EG ψ1 from any state if none of its successors is\nlabelled with EG ψ1; until there is no change.\nHere, we label all the states with the subformula EG ψ1 and then whittle\ndown this labelled set, instead of building it up from nothing as we did in\nthe case for EU. Actually, there is no real diﬀerence between this procedure\nfor EG ψ and what you would do if you translated it into ¬AF ¬ψ as far as\nthe ﬁnal result is concerned.\nA variant which is more efficient\nWe can improve the eﬃciency of\nour labelling algorithm by using a cleverer way of handling EG. Instead of\nusing EX, EU and AF as the adequate set, we use EX, EU and EG instead.",
    "the ﬁnal result is concerned.\nA variant which is more efficient\nWe can improve the eﬃciency of\nour labelling algorithm by using a cleverer way of handling EG. Instead of\nusing EX, EU and AF as the adequate set, we use EX, EU and EG instead.\nFor EX and EU we do as before (but take care to search the model by\n3.6 Model-checking algorithms\n225\nstates satisfying ψ\n⊨EG ψ\nSCC\nSCC\nSCC\nFigure 3.26. A better way of handling EG.\nbackwards breadth-ﬁrst search, for this ensures that we won’t have to pass\nover any node twice). For the EG ψ case:\nr Restrict the graph to states satisfying ψ, i.e., delete all other states and their\ntransitions;\nr Find the maximal strongly connected components (SCCs); these are maximal\nregions of the state space in which every state is linked with (= has a ﬁnite path\nto) every other one in that region.\nr Use backwards breadth-ﬁrst search on the restricted graph to ﬁnd any state that\ncan reach an SCC; see Figure 3.26.\nThe complexity of this algorithm is O(f · (V + E)), i.e., linear both in the\nsize of the model and in the size of the formula.\nExample 3.20 We applied the basic algorithm to our second model of mu-\ntual exclusion with the formula E[¬c2 U c1]; see Figure 3.27. The algorithm\nlabels all states which satisfy c1 during phase 1 with E[¬c2 U c1]. This labels\ns2 and s4. During phase 2, it labels all states which do not satisfy c2 and\nhave a successor state that is already labelled. This labels states s1 and s3.\nDuring phase 3, we label s0 because it does not satisfy c2 and has a succes-\nsor state (s1) which is already labelled. Thereafter, the algorithm terminates\nbecause no additional states get labelled: all unlabelled states either satisfy\nc2, or must pass through such a state to reach a labelled state.\nThe pseudo-code of the CTL model-checking algorithm\nWe\npresent the pseudo-code for the basic labelling algorithm. The main function\nSAT (for ‘satisﬁes’) takes as input a CTL formula. The program SAT expects",
    "c2, or must pass through such a state to reach a labelled state.\nThe pseudo-code of the CTL model-checking algorithm\nWe\npresent the pseudo-code for the basic labelling algorithm. The main function\nSAT (for ‘satisﬁes’) takes as input a CTL formula. The program SAT expects\na parse tree of some CTL formula constructed by means of the grammar in\nDeﬁnition 3.12. This expectation reﬂects an important precondition on the\ncorrectness of the algorithm SAT. For example, the program simply would\nnot know what to do with an input of the form X (⊤∧EF p3), since this is\nnot a CTL formula.\n226\n3 Verification by model checking\ns5\ns0\n0: t1n2\n0: c1n2\n0: t1t2\n0: c1t2\n2: E[¬c2 U c1]\ns3\ns1\ns2\ns6\ns9\ns4\ns7\n1: E[¬c2 U c1]\n1: E[¬c2 U c1]\n2: E[¬c2 U c1]\n3: E[¬c2 U c1]\n0: n1n2\n0: n1t2\n0: t1t2\n0: t1c2\n0: n1c2\nFigure 3.27. An example run of the labelling algorithm in our second\nmodel of mutual exclusion applied to the formula E[¬c2 U c1].\nThe pseudo-code we write for SAT looks a bit like fragments of C or\nJava code; we use functions with a keyword return that indicates which\nresult the function should return. We will also use natural language to\nindicate the case analysis over the root node of the parse tree of φ. The\ndeclaration local var declares some fresh variables local to the current in-\nstance of the procedure in question, whereas repeat until executes the\ncommand which follows it repeatedly, until the condition becomes true. Ad-\nditionally, we employ suggestive notation for the operations on sets, like\nintersection, set complement and so forth. In reality we would need an ab-\nstract data type, together with implementations of these operations, but for\nnow we are interested only in the mechanism in principle of the algorithm\nfor SAT; any (correct and eﬃcient) implementation of sets would do and\nwe study such an implementation in Chapter 6. We assume that SAT has\naccess to all the relevant parts of the model: S, →and L. In particular,",
    "now we are interested only in the mechanism in principle of the algorithm\nfor SAT; any (correct and eﬃcient) implementation of sets would do and\nwe study such an implementation in Chapter 6. We assume that SAT has\naccess to all the relevant parts of the model: S, →and L. In particular,\nwe ignore the fact that SAT would require a description of M as input as\nwell. We simply assume that SAT operates directly on any such given model.\nNote how SAT translates φ into an equivalent formula of the adequate set\nchosen.\n3.6 Model-checking algorithms\n227\nfunction SAT (φ)\n/* determines the set of states satisfying φ */\nbegin\ncase\nφ is ⊤: return S\nφ is ⊥: return ∅\nφ is atomic: return {s ∈S | φ ∈L(s)}\nφ is ¬φ1 : return S −SAT (φ1)\nφ is φ1 ∧φ2 : return SAT (φ1) ∩SAT (φ2)\nφ is φ1 ∨φ2 : return SAT (φ1) ∪SAT (φ2)\nφ is φ1 →φ2 : return SAT (¬φ1 ∨φ2)\nφ is AX φ1 : return SAT (¬EX ¬φ1)\nφ is EX φ1 : return SATEX(φ1)\nφ is A[φ1 U φ2] : return SAT(¬(E[¬φ2 U (¬φ1 ∧¬φ2)] ∨EG ¬φ2))\nφ is E[φ1 U φ2] : return SATEU(φ1, φ2)\nφ is EF φ1 : return SAT (E(⊤U φ1))\nφ is EG φ1 : return SAT(¬AF ¬φ1)\nφ is AF φ1 : return SATAF (φ1)\nφ is AG φ1 : return SAT (¬EF ¬φ1)\nend case\nend function\nFigure 3.28. The function SAT. It takes a CTL formula as input and\nreturns the set of states satisfying the formula. It calls the functions\nSATEX, SATEU and SATAF, respectively, if EX , EU or AF is the root of the\ninput’s parse tree.\nThe algorithm is presented in Figure 3.28 and its subfunctions in Fig-\nures 3.29–3.31. They use program variables X, Y , V and W which are sets\nof states. The program for SAT handles the easy cases directly and passes\nmore complicated cases on to special procedures, which in turn might call\nSAT recursively on subexpressions. These special procedures rely on imple-\nmentations of the functions\npre∃(Y ) = {s ∈S | exists s′, (s →s′ and s′ ∈Y )}\npre∀(Y ) = {s ∈S | for all s′, (s →s′ implies s′ ∈Y )}.\n‘Pre’ denotes travelling backwards along the transition relation. Both func-",
    "SAT recursively on subexpressions. These special procedures rely on imple-\nmentations of the functions\npre∃(Y ) = {s ∈S | exists s′, (s →s′ and s′ ∈Y )}\npre∀(Y ) = {s ∈S | for all s′, (s →s′ implies s′ ∈Y )}.\n‘Pre’ denotes travelling backwards along the transition relation. Both func-\ntions compute a pre-image of a set of states. The function pre∃(instrumental\nin SATEX and SATEU) takes a subset Y of states and returns the set of states\nwhich can make a transition into Y . The function pre∀, used in SATAF, takes\n228\n3 Verification by model checking\nfunction SATEX (φ)\n/* determines the set of states satisfying EX φ */\nlocal var X, Y\nbegin\nX := SAT (φ);\nY := pre∃(X);\nreturn Y\nend\nFigure 3.29. The function SATEX. It computes the states satisfying φ by\ncalling SAT. Then, it looks backwards along →to find the states satisfying\nEX φ.\nfunction SATAF (φ)\n/* determines the set of states satisfying AF φ */\nlocal var X, Y\nbegin\nX := S;\nY := SAT (φ);\nrepeat until X = Y\nbegin\nX := Y ;\nY := Y ∪pre∀(Y )\nend\nreturn Y\nend\nFigure 3.30. The function SATAF. It computes the states satisfying φ by\ncalling SAT. Then, it accumulates states satisfying AF φ in the manner\ndescribed in the labelling algorithm.\na set Y and returns the set of states which make transitions only into Y .\nObserve that pre∀can be expressed in terms of complementation and pre∃,\nas follows:\npre∀(Y ) = S −pre∃(S −Y )\n(3.8)\nwhere we write S −Y for the set of all s ∈S which are not in Y .\nThe correctness of this pseudocode and the model checking algorithm is\ndiscussed in Section 3.7.\n3.6 Model-checking algorithms\n229\nfunction SATEU (φ, ψ)\n/* determines the set of states satisfying E[φ U ψ] */\nlocal var W, X, Y\nbegin\nW := SAT (φ);\nX := S;\nY := SAT (ψ);\nrepeat until X = Y\nbegin\nX := Y ;\nY := Y ∪(W ∩pre∃(Y ))\nend\nreturn Y\nend\nFigure 3.31. The function SATEU. It computes the states satisfying φ by\ncalling SAT. Then, it accumulates states satisfying E[φ U ψ] in the manner\ndescribed in the labelling algorithm.",
    "begin\nW := SAT (φ);\nX := S;\nY := SAT (ψ);\nrepeat until X = Y\nbegin\nX := Y ;\nY := Y ∪(W ∩pre∃(Y ))\nend\nreturn Y\nend\nFigure 3.31. The function SATEU. It computes the states satisfying φ by\ncalling SAT. Then, it accumulates states satisfying E[φ U ψ] in the manner\ndescribed in the labelling algorithm.\nThe ‘state explosion’ problem\nAlthough the labelling algorithm (with\nthe clever way of handling EG) is linear in the size of the model, unfortu-\nnately the size of the model is itself more often than not exponential in the\nnumber of variables and the number of components of the system which\nexecute in parallel. This means that, for example, adding a boolean variable\nto your program will double the complexity of verifying a property of it.\nThe tendency of state spaces to become very large is known as the state\nexplosion problem. A lot of research has gone into ﬁnding ways of overcoming\nit, including the use of:\nr Eﬃcient data structures, called ordered binary decision diagrams (OBDDs),\nwhich represent sets of states instead of individual states. We study these in\nChapter 6 in detail. SMV is implemented using OBDDs.\nr Abstraction: one may interpret a model abstractly, uniformly or for a speciﬁc\nproperty.\nr Partial order reduction: for asynchronous systems, several interleavings of com-\nponent traces may be equivalent as far as satisfaction of the formula to be checked\nis concerned. This can often substantially reduce the size of the model-checking\nproblem.\nr Induction: model-checking systems with (e.g.) large numbers of identical, or sim-\nilar, components can often be implemented by ‘induction’ on this number.\n230\n3 Verification by model checking\nr Composition: break the veriﬁcation problem down into several simpler veriﬁca-\ntion problems.\nThe last four issues are beyond the scope of this book, but references may\nbe found at the end of this chapter.\n3.6.2 CTL model checking with fairness\nThe veriﬁcation of M, s0 ⊨φ might fail because the model M may contain",
    "tion problems.\nThe last four issues are beyond the scope of this book, but references may\nbe found at the end of this chapter.\n3.6.2 CTL model checking with fairness\nThe veriﬁcation of M, s0 ⊨φ might fail because the model M may contain\nbehaviour which is unrealistic, or guaranteed not to occur in the actual sys-\ntem being analysed. For example, in the mutual exclusion case, we expressed\nthat the process prc can stay in its critical section (st=c) as long as it needs.\nWe modelled this by the non-deterministic assignment\nnext(st) :=\ncase\n...\n(st = c)\n: {c,n};\n...\nesac;\nHowever, if we really allow process 2 to stay in its critical section as\nlong as it likes, then we have a path which violates the liveness constraint\nAG (t1 →AF c1), since, if process 2 stays forever in its critical section, t1\ncan be true without c1 ever becoming true.\nWe would like to ignore this path, i.e., we would like to assume that the\nprocess can stay in its critical section as long as it needs, but will eventually\nexit from its critical section after some ﬁnite time.\nIn LTL, we could handle this by verifying a formula like FG¬c2 →φ,\nwhere φ is the formula we actually want to verify. This whole formula asserts\nthat all paths which satisfy inﬁnitely often ¬c2 also satisfy φ. However,\nwe cannot do this in CTL because we cannot write formulas of the form\nFG¬c2 →φ in CTL. The logic CTL is not expressive enough to allow us\nto pick out the ‘fair’ paths, i.e., those in which process 2 always eventually\nleaves its critical section.\nIt is for that reason that SMV allows us to impose fairness constraints\non top of the transition system it describes. These assumptions state that\na given formula is true inﬁnitely often along every computation path. We\ncall such paths fair computation paths. The presence of fairness constraints\nmeans that, when evaluating the truth of CTL formulas in speciﬁcations,\nthe connectives A and E range only over fair paths.\n3.6 Model-checking algorithms\n231",
    "call such paths fair computation paths. The presence of fairness constraints\nmeans that, when evaluating the truth of CTL formulas in speciﬁcations,\nthe connectives A and E range only over fair paths.\n3.6 Model-checking algorithms\n231\nWe therefore impose the fairness constraint that !st=c be true inﬁnitely\noften. This means that, whatever state the process is in, there will be a\nstate in the future in which it is not in its critical section. Similar fairness\nconstraints were used for the Alternating Bit Protocol.\nFairness constraints of the form (where φ is a state formula)\nProperty φ is true inﬁnitely often\nare known as simple fairness constraints. Other types include those of the\nform\nIf φ is true inﬁnitely often, then ψ is also true inﬁnitely often.\nSMV can deal only with simple fairness constraints; but how does it do\nthat? To answer that, we now explain how we may adapt our model-checking\nalgorithm so that A and E are assumed to range only over fair computation\npaths.\nDeﬁnition 3.21 Let C\ndef\n= {ψ1, ψ2, . . . , ψn} be a set of n fairness constraints.\nA computation path s0 →s1 →. . . is fair with respect to these fairness\nconstraints iﬀfor each i there are inﬁnitely many j such that sj ⊨ψi, that\nis, each ψi is true inﬁnitely often along the path. Let us write AC and EC\nfor the operators A and E restricted to fair paths.\nFor example, M, s0 ⊨ACG φ iﬀφ is true in every state along all fair paths;\nand similarly for ACF, ACU, etc. Notice that these operators explicitly de-\npend on the chosen set C of fairness constraints. We already know that ECU,\nECG and ECX form an adequate set; this can be shown in the same man-\nner as was done for the temporal connectives without fairness constraints\n(Section 3.4.4). We also have that\nEC[φ U ψ] ≡E[φ U (ψ ∧ECG ⊤)]\nECX φ ≡EX (φ ∧ECG ⊤).\nTo see this, observe that a computation path is fair iﬀany suﬃx of it is\nfair. Therefore, we need only provide an algorithm for ECG φ. It is similar",
    "(Section 3.4.4). We also have that\nEC[φ U ψ] ≡E[φ U (ψ ∧ECG ⊤)]\nECX φ ≡EX (φ ∧ECG ⊤).\nTo see this, observe that a computation path is fair iﬀany suﬃx of it is\nfair. Therefore, we need only provide an algorithm for ECG φ. It is similar\nto Algorithm 2 for EG, given earlier in this chapter:\nr Restrict the graph to states satisfying φ; of the resulting graph, we want to know\nfrom which states there is a fair path.\nr Find the maximal strongly connected components (SCCs) of the restricted graph;\nr Remove an SCC if, for some ψi, it does not contain a state satisfying ψi. The\nresulting SCCs are the fair SCCs. Any state of the restricted graph that can\nreach one has a fair path from it.\n232\n3 Verification by model checking\nstates satisfying φ\n⊨EfGφ\nψ2\nψ1\nψ3\nfair SCC\nfair SCC\nFigure 3.32. Computing the states satisfying ECG φ. A state satisfies\nECG φ iff, in the graph resulting from the restriction to states satisfying\nφ, the state has a fair path from it. A fair path is one which leads to an\nSCC with a cycle passing through at least one state that satisfies each\nfairness constraint; in the example, C equals {ψ1, ψ2, ψ3}.\nr Use backwards breadth-ﬁrst search to ﬁnd the states on the restricted graph that\ncan reach a fair SCC.\nSee Figure 3.32. The complexity of this algorithm is O(n · f · (V + E)), i.e.,\nstill linear in the size of the model and formula.\nIt should be noted that writing fairness conditions using SMV’s FAIR-\nNESS keyword is necessary only for CTL model checking. In the case of LTL,\nwe can assert the fairness condition as part of the formula to be checked.\nFor example, if we wish to check the LTL formula ψ under the assumption\nthat φ is inﬁnitely often true, we check G F φ →ψ. This means: all paths\nsatisfying inﬁnitely often φ also satisfy ψ. It is not possible to express this\nin CTL. In particular, any way of adding As or Es to G F φ →ψ will result\nin a formula with a diﬀerent meaning from the intended one. For example,",
    "that φ is inﬁnitely often true, we check G F φ →ψ. This means: all paths\nsatisfying inﬁnitely often φ also satisfy ψ. It is not possible to express this\nin CTL. In particular, any way of adding As or Es to G F φ →ψ will result\nin a formula with a diﬀerent meaning from the intended one. For example,\nAG AF φ →ψ means that if all paths are fair then ψ holds, rather than what\nwas intended: ψ holds along all paths which are fair.\n3.6.3 The LTL model-checking algorithm\nThe algorithm presented in the sections above for CTL model checking\nis quite intuitive: given a system and a CTL formula, it labels states of\nthe system with the subformulas of the formula which are satisﬁed there.\nThe state-labelling approach is appropriate because subformulas of the for-\nmula may be evaluated in states of the system. This is not the case for\nLTL: subformulas of the formula must be evaluated not in states but along\npaths of the system. Therefore, LTL model checking has to adopt a diﬀerent\nstrategy.\nThere are several algorithms for LTL model checking described in the\nliterature. Although they diﬀer in detail, nearly all of them adopt the same\n3.6 Model-checking algorithms\n233\nbasic strategy. We explain that strategy ﬁrst; then, we describe some algo-\nrithms in more detail.\nThe basic strategy\nLet M = (S, →, L) be a model, s ∈S, and φ an LTL\nformula. We determine whether M, s ⊨φ, i.e., whether φ is satisﬁed along\nall paths of M starting at s. Almost all LTL model checking algorithms\nproceed along the following three steps.\n1.\nConstruct an automaton, also known as a tableau, for the formula ¬φ. The\nautomaton for ψ is called Aψ. Thus, we construct A¬φ. The automaton has a\nnotion of accepting a trace. A trace is a sequence of valuations of the proposi-\ntional atoms. From a path, we can abstract its trace. The construction has the\nproperty that for all paths π: π ⊨ψ iﬀthe trace of π is accepted by Aψ. In other\nwords, the automaton Aψ encodes precisely the traces which satisfy ψ.",
    "tional atoms. From a path, we can abstract its trace. The construction has the\nproperty that for all paths π: π ⊨ψ iﬀthe trace of π is accepted by Aψ. In other\nwords, the automaton Aψ encodes precisely the traces which satisfy ψ.\nThus, the automaton A¬φ which we construct for ¬φ has the property that it\nencodes all the traces satisfying ¬φ; i.e., all the traces which do not satisfy φ.\n2.\nCombine the automaton A¬φ with the model M of the system. The combina-\ntion operation results in a transition system whose paths are both paths of the\nautomaton and paths of the system.\n3.\nDiscover whether there is any path from a state derived from s in the combined\ntransition system. Such a path, if there is one, can be interpreted as a path in\nM beginning at s which does not satisfy φ.\nIf there was no such path, then output: ‘Yes, M, s ⊨φ.’ Otherwise, if there is\nsuch a path, output ‘No, M, s ̸⊨φ.’ In the latter case, the counterexample can\nbe extracted from the path found.\nLet us consider an example. The system is described by the SMV program\nand its model M, shown in Figure 3.33. We consider the formula ¬(a U b).\nSince it is not the case that all paths of M satisfy the formula (for example,\nthe path q3, q2, q2 . . . does not satisfy it) we expect the model check to\nfail.\nIn accordance with Step 1, we construct an automaton AaUb which char-\nacterises precisely the traces which satisfy a U b. (We use the fact that\n¬¬(a U b) is equivalent to a U b.) Such an automaton is shown in Figure\n3.34. We will look at how to construct it later; for now, we just try to un-\nderstand how and why it works.\nA trace t is accepted by an automaton like the one of Figure 3.34 if there\nexists a path π through the automaton such that:\nr π starts in an initial state (i.e. one containing φ);\nr it respects the transition relation of the automaton;\nr t is the trace of π; matches the corresponding state of π;\n234\n3 Verification by model checking\ninit(a) := 1;\ninit(b) := 0;\nnext(a) := case\n!a : 0;\nb\n: 1;\n1",
    "r π starts in an initial state (i.e. one containing φ);\nr it respects the transition relation of the automaton;\nr t is the trace of π; matches the corresponding state of π;\n234\n3 Verification by model checking\ninit(a) := 1;\ninit(b) := 0;\nnext(a) := case\n!a : 0;\nb\n: 1;\n1\n: {0,1};\nesac;\nnext(b) := case\na & next(a) : !b;\n!a : 1;\n1\n: {0,1};\nesac;\nq3\nq1\nq2\nab\nq4\nab\nab\nab\nFigure 3.33. An SMV program and its model M.\na b φ\nq4\na b φ\na b φ\nq1\nq′\n3\na b φ\nq3\na b φ\nq2\nFigure 3.34. Automaton accepting precisely traces satisfying φ\ndef\n= a U b.\nThe transitions with no arrows can be taken in either direction. The\nacceptance condition is that the path of the automaton cannot loop\nindefinitely through q3.\nr the path respects a certain ‘accepting condition.’ For the automaton of Fig-\nure 3.34, the accepting condition is that the path should not end q3, q3, q3 . . . ,\nindeﬁnitely.\nFor example, suppose t is a b, a b, a b, a b, a b, a b, a b, a b, . . . , eventually re-\npeating forevermore the state a b. Then we choose the path q3, q3, q3, q4, q4,\nq1, q′\n3, q′\n3 . . . . We start in q3 because the ﬁrst state is a b and it is an initial\n3.6 Model-checking algorithms\n235\nstate. The next states we choose just follow the valuation of the states of\nπ. For example, at q1 the next valuation is a b and the transitions allow us\nto choose q3 or q′\n3. We choose q′\n3, and loop there forevermore. This path\nmeets the conditions, and therefore the trace t is accepted. Observe that the\ndeﬁnition states ‘there exists a path.’ In the example above, there are also\npaths which don’t meet the conditions:\nr Any path beginning q3, q′\n3, . . . doesn’t meet the condition that we have to respect\nthe transition relation.\nr The path q3, q3, q3, q4, q4, q1, q3, q3 . . . doesn’t meet the condition that we must\nnot end on a loop of q3.\nThese paths need not bother us, because it is suﬃcient to ﬁnd one which\ndoes meet the conditions in order to declare that π is accepted.",
    "the transition relation.\nr The path q3, q3, q3, q4, q4, q1, q3, q3 . . . doesn’t meet the condition that we must\nnot end on a loop of q3.\nThese paths need not bother us, because it is suﬃcient to ﬁnd one which\ndoes meet the conditions in order to declare that π is accepted.\nWhy does the automaton of Figure 3.34 work as intended? To understand\nit, observe that it has enough states to distinguish the values of the propo-\nsitions – that is, a state for each of the valuations {a b, a b, a b, a b}, and in\nfact two states for the valuation a b. One state for each of {a b, a b, a b} is\nintuitively enough, because those valuations determine whether a U b holds.\nBut a U b could be false or true in a b, so we have to consider the two cases.\nThe presence of φ\ndef\n= a U b in a state indicates that either we are still ex-\npecting φ to become true, or we have just obtained it. Whereas φ indicates\nwe no longer expect φ, and have not just obtained it. The transitions of the\nautomaton are such that the only way out of q3 is to obtain b, i.e., to move to\nq2 or q4. Apart from that, the transitions are liberal, allowing any path to be\nfollowed; each of q1, q2, q3 can transition to any valuation, and so can q3, q′\n3\ntaken together, provided we are careful to choose the right one to enter.\nThe acceptance condition, which allows any path except one looping indeﬁ-\nnitely on q3, guarantees that the promise of a U b to deliver b is eventually\nfulﬁlled.\nUsing this automaton AaUb, we proceed to Step 2. To combine the au-\ntomaton AaUb with the model of the system M shown in Figure 3.33, it is\nconvenient ﬁrst to redraw M with two versions of q3; see Figure 3.35(left).\nIt is an equivalent system; all ways into q3 now non-deterministically choose\nq3 or q′\n3, and which ever one we choose leads to the same successors. But it\nallows us to superimpose it on AaUb and select the transitions common to\nboth, obtaining the combined system of Figure 3.35(right).",
    "It is an equivalent system; all ways into q3 now non-deterministically choose\nq3 or q′\n3, and which ever one we choose leads to the same successors. But it\nallows us to superimpose it on AaUb and select the transitions common to\nboth, obtaining the combined system of Figure 3.35(right).\nStep 3 now asks whether there is a path from q of the combined automa-\nton. As can be seen, there are two kinds of path in the combined system:\nq3, (q4, q3, )∗q2, q2 . . . , and q3, q4, (q3, q4, )∗q′\n3, q1, q2, q2, . . .\nwhere (q3, q4)∗\ndenotes either the empty string or q3, q4 or q3, q4, q3, q4 etc. Thus, according\n236\n3 Verification by model checking\na b\na b\na b\nq1\na b\na b\nq2\na b φ\na b φ\na b φ\nq1\na b φ\na b φ\nq2\nq3\nq′\n3\nq3\nq′\n3\nq4\nq4\nFigure 3.35. Left: the system M of Figure 3.33, redrawn with an ex-\npanded state space; right: the expanded M and AaUb combined.\nto Step 3, and as we expected, ¬(a U b) is not satisﬁed in all paths of the\noriginal system M.\nConstructing the automaton\nLet us look in more detail at how the\nautomaton is constructed. Given an LTL formula φ, we wish to construct\nan automaton Aφ such that Aφ accepts precisely those runs on which φ\nholds. We assume that φ contains only the temporal connectives U and X;\nrecall that the other temporal connectives can be written in terms of these\ntwo.\nDeﬁne the closure C(φ) of formula φ as the set of subformulas of φ\nand their complements, identifying ¬¬ψ and ψ. For example, C(a U b) =\n{a, b, ¬a, ¬b, a U b, ¬(a U b)}. The states of Aφ, denoted by q, q′ etc., are\nthe maximal subsets of C(φ) which satisfy the following conditions:\nr For all (non-negated) ψ ∈C(φ), either ψ ∈q or ¬ψ ∈q, but not both.\nr ψ1 ∨ψ2 ∈q holds iﬀψ1 ∈q or ψ2 ∈q, whenever ψ1 ∨ψ2 ∈C(φ).\nr Conditions for other boolean combinations are similar.\nr If ψ1 U ψ2 ∈q, then ψ2 ∈q or ψ1 ∈q.\nr If ¬(ψ1 U ψ2) ∈q, then ¬ψ2 ∈q.\nIntuitively, these conditions imply that the states of Aφ are capable of saying\nwhich subformulas of φ are true.\n3.6 Model-checking algorithms\n237",
    "r Conditions for other boolean combinations are similar.\nr If ψ1 U ψ2 ∈q, then ψ2 ∈q or ψ1 ∈q.\nr If ¬(ψ1 U ψ2) ∈q, then ¬ψ2 ∈q.\nIntuitively, these conditions imply that the states of Aφ are capable of saying\nwhich subformulas of φ are true.\n3.6 Model-checking algorithms\n237\nThe initial states of Aφ are those states containing φ. For transition rela-\ntion δ of Aφ we have (q, q′) ∈δ iﬀall of the following conditions hold:\nr if X ψ ∈q then ψ ∈q′;\nr if ¬X ψ ∈q then ¬ψ ∈q′;\nr If ψ1 U ψ2 ∈q and ψ2 /∈q then ψ1 U ψ2 ∈q′;\nr If ¬(ψ1 U ψ2) ∈q and ψ1 ∈q then ¬(ψ1 U ψ2) ∈q′.\nThese last two conditions are justiﬁed by the recursion laws\nψ1 U ψ2 = ψ2 ∨(ψ1 ∧X (ψ1 U ψ2))\n¬(ψ1 U ψ2) = ¬ψ2 ∧(¬ψ1 ∨X ¬(ψ1 U ψ2)) .\nIn particular, they ensure that whenever some state contains ψ1 U ψ2, sub-\nsequent states contain ψ1 for as long as they do not contain ψ2.\nAs we have deﬁned Aφ so far, not all paths through Aφ satisfy φ. We use\nadditional acceptance conditions to guarantee the ‘eventualities’ ψ promised\nby the formula ψ1 U ψ2, namely that Aφ cannot stay for ever in states satis-\nfying ψ1 without ever obtaining ψ2. Recall that, for the automaton of Figure\n3.34 for a U b, we stipulated the acceptance condition that the path through\nthe automaton should not end q3, q3, . . . .\nThe acceptance conditions of Aφ are deﬁned so that they ensure that\nevery state containing some formula χ U ψ will eventually be followed by\nsome state containing ψ. Let χ1 U ψ1, . . . , χk U ψk be all subformulas of\nthis form in C(φ). We stipulate the following acceptance condition: a run\nis accepted if, for every i such that 1 ≤i ≤k, the run has inﬁnitely many\nstates satisfying ¬(χi U ψi) ∨ψi. To understand why this condition has the\ndesired eﬀect, imagine the circumstances in which it is false. Suppose we\nhave a run having only ﬁnitely many states satisfying ¬(χi U ψi) ∨ψi. Let\nus advance through all those ﬁnitely many states, taking the suﬃx of the run",
    "states satisfying ¬(χi U ψi) ∨ψi. To understand why this condition has the\ndesired eﬀect, imagine the circumstances in which it is false. Suppose we\nhave a run having only ﬁnitely many states satisfying ¬(χi U ψi) ∨ψi. Let\nus advance through all those ﬁnitely many states, taking the suﬃx of the run\nnone of whose states satisﬁes ¬(χi U ψi) ∨ψi, i.e., all of whose states satisfy\n(χi U ψi) ∧¬ψi. That is precisely the sort of run we want to eliminate.\nIf we carry out this construction on a U b, we obtain the automaton shown\nin Figure 3.34. Another example is shown in Figure 3.36, for the formula\n(p U q) ∨(¬p U q). Since that formula has two U subformulas, there are two\nsets speciﬁed in the acceptance condition, namely, the states satisfying p U q\nand the states satisfying ¬p U q.\nHow LTL model checking is implemented in NuSMV\nIn the sec-\ntions above, we described an algorithm for LTL model checking. Given an\nLTL formula φ and a system M and a state s of M, we may check whether\nM, s ⊨φ holds by constructing the automaton A¬φ, combining it with M,\n238\n3 Verification by model checking\n¬(p U q),\n¬(¬p U q),\n¬p, ¬q, ¬φ\n¬(p U q),\n¬(¬p U q),\np U q,\n¬p U q,\n¬p, q, φ\np U q,\n¬(¬p U q),\np, ¬q, φ\np U q,\n¬p U q,\np, q, φ\n¬(p U q),\n¬p U q,\n¬p, ¬q, φ\nq1\nq2\nq3\nq4\nq5\nq6\np, ¬q, ¬φ\nFigure 3.36. Automaton accepting precisely traces satisfying φ\ndef\n= (p U\nq) ∨(¬p U q). The transitions with no arrows can be taken in either direc-\ntion. The acceptance condition asserts that every run must pass infinitely\noften through the set {q1, q3, q4, q5, q6}, and also the set {q1, q2, q3, q5, q6}.\nand checking whether there is a path of the resulting system which satisﬁes\nthe acceptance condition of A¬φ.\nIt is possible to implement the check for such a path in terms of CTL\nmodel checking, and this is in fact what NuSMV does. The combined system\nM × A¬φ is represented as the system to be model checked in NuSMV,\nand the formula to be checked is simply EG ⊤. Thus, we ask the question:",
    "It is possible to implement the check for such a path in terms of CTL\nmodel checking, and this is in fact what NuSMV does. The combined system\nM × A¬φ is represented as the system to be model checked in NuSMV,\nand the formula to be checked is simply EG ⊤. Thus, we ask the question:\ndoes the combined system have a path. The acceptance conditions of A¬φ\nare represented as implicit fairness conditions for the CTL model-checking\nprocedure. Explicitly, this amounts to asserting ‘FAIRNESS ¬(χ U ψ) ∨ψ’\nfor each formula χ U ψ occurring in C(φ).\n3.7 The fixed-point characterisation of CTL\nOn page 227, we presented an algorithm which, given a CTL formula φ and\na model M = (S, →, L), computes the set of states s ∈S satisfying φ. We\nwrite this set as [[φ]]. The algorithm works recursively on the structure of\nφ. For formulas φ of height 1 (⊥, ⊤or p), [[φ]] is computed directly. Other\n3.7 The fixed-point characterisation of CTL\n239\nformulas are composed of smaller subformulas combined by a connective of\nCTL. For example, if φ is ψ1 ∨ψ2, then the algorithm computes the sets\n[[ψ1]] and [[ψ2]] and combines them in a certain way (in this case, by taking\nthe union) in order to obtain [[ψ1 ∨ψ2]].\nThe more interesting cases arise when we deal with a formula such as\nEX ψ, involving a temporal operator. The algorithm computes the set [[ψ]]\nand then computes the set of all states which have a transition to a state in\n[[ψ]]. This is in accord with the semantics of EX ψ: M, s ⊨EX ψ iﬀthere is\na state s′ with s →s′ and M, s′ ⊨ψ.\nFor most of these logical operators, we may easily continue this discussion\nto see that the algorithms work just as expected. However, the cases EU,\nAF and EG (where we needed to iterate a certain labelling policy until it\nstabilised) are not so obvious to reason about. The topic of this section is to\ndevelop the semantic insights into these operators that allow us to provide a\ncomplete proof for their termination and correctness. Inspecting the pseudo-",
    "stabilised) are not so obvious to reason about. The topic of this section is to\ndevelop the semantic insights into these operators that allow us to provide a\ncomplete proof for their termination and correctness. Inspecting the pseudo-\ncode in Figure 3.28, we see that most of these clauses just do the obvious\nand correct thing according to the semantics of CTL. For example, try out\nwhat SAT does when you call it with φ1 →φ2.\nOur aim in this section is to prove the termination and correctness\nof SATAF and SATEU. In fact, we will also write a procedure SATEG and\nprove its termination and correctness1. The procedure SATEG is given in\nFigure 3.37 and is based on the intuitions given in Section 3.6.1: note how\ndeleting the label if none of the successor states is labelled is coded as\nintersecting the labelled set with the set of states which have a labelled\nsuccessor.\nThe semantics of EG φ says that s0 ⊨EG φ holds iﬀthere exists a com-\nputation path s0 →s1 →s2 →. . . such that si ⊨φ holds for all i ≥0. We\ncould instead express it as follows: EG φ holds if φ holds and EG φ holds\nin one of the successor states to the current state. This suggests the equiv-\nalence EG φ ≡φ ∧EX EG φ which can easily be proved from the semantic\ndeﬁnitions of the connectives.\nObserving that [[EX ψ]] = pre∃([[ψ]]) we see that the equivalence above\ncan be written as [[EG φ]] = [[φ]] ∩pre∃([[EG φ]]). This does not look like a\nvery promising way of calculating EG φ, because we need to know EG φ in\norder to work out the right-hand side. Fortunately, there is a way around\nthis apparent circularity, known as computing ﬁxed points, and that is the\nsubject of this section.\n1 Section 3.6.1 handles EG φ by translating it into ¬AF ¬φ, but we already noted in Section 3.6.1\nthat EG could be handled directly.\n240\n3 Verification by model checking\nfunction SATEG (φ)\n/* determines the set of states satisfying EG φ */\nlocal var X, Y\nbegin\nY := SAT (φ);\nX := ∅;\nrepeat until X = Y\nbegin\nX := Y ;\nY := Y ∩pre∃(Y )",
    "that EG could be handled directly.\n240\n3 Verification by model checking\nfunction SATEG (φ)\n/* determines the set of states satisfying EG φ */\nlocal var X, Y\nbegin\nY := SAT (φ);\nX := ∅;\nrepeat until X = Y\nbegin\nX := Y ;\nY := Y ∩pre∃(Y )\nend\nreturn Y\nend\nFigure 3.37. The pseudo-code for SATEG.\n3.7.1 Monotone functions\nDeﬁnition 3.22 Let S be a set of states and F : P(S) →P(S) a function\non the power set of S.\n1.\nWe say that F is monotone iﬀX ⊆Y implies F(X) ⊆F(Y ) for all subsets X\nand Y of S.\n2.\nA subset X of S is called a ﬁxed point of F iﬀF(X) = X.\nFor an example, let S\ndef\n= {s0, s1} and F(Y )\ndef\n= Y ∪{s0} for all subsets Y\nof S. Since Y ⊆Y ′ implies Y ∪{s0} ⊆Y ′ ∪{s0}, we see that F is monotone.\nThe ﬁxed points of F are all subsets of S containing s0. Thus, F has two\nﬁxed points, the sets {s0} and {s0, s1}. Notice that F has a least (= {s0})\nand a greatest (= {s0, s1}) ﬁxed point.\nAn example of a function G: P(S) →P(S), which is not monotone, is\ngiven by\nG(Y )\ndef\n= if Y = {s0} then {s1} else {s0}.\nSo G maps {s0} to {s1} and all other sets to {s0}. The function G is\nnot monotone since {s0} ⊆{s0, s1} but G({s0}) = {s1} is not a subset of\nG({s0, s1}) = {s0}. Note that G has no ﬁxed points whatsoever.\nThe reasons for exploring monotone functions on P(S) in the context of\nproving the correctness of SAT are:\n1.\nthat monotone functions always have a least and a greatest ﬁxed point;\n2.\nthat the meanings of EG, AF and EU can be expressed via greatest, respectively\nleast, ﬁxed points of monotone functions on P(S);\n3.7 The fixed-point characterisation of CTL\n241\n3.\nthat these ﬁxed-points can be easily computed, and;\n4.\nthat the procedures SATEU and SATAF code up such ﬁxed-point computations,\nand are correct by item 2.\nNotation 3.23 F i(X) means\nF(F(. . . F\n\u0001\n\u0002\u0003\n\u0004\ni times\n(X) . . . ))\nThus, the function F i is just ‘F applied i many times.’\nFor example, for the function F(Y )\ndef\n= Y ∪{s0}, we obtain F 2(Y ) =",
    "4.\nthat the procedures SATEU and SATAF code up such ﬁxed-point computations,\nand are correct by item 2.\nNotation 3.23 F i(X) means\nF(F(. . . F\n\u0001\n\u0002\u0003\n\u0004\ni times\n(X) . . . ))\nThus, the function F i is just ‘F applied i many times.’\nFor example, for the function F(Y )\ndef\n= Y ∪{s0}, we obtain F 2(Y ) =\nF(F(Y )) = (Y ∪{s0}) ∪{s0} = Y ∪{s0} = F(Y ). In this case, F 2 = F and\ntherefore F i = F for all i ≥1. It is not always the case that the sequence of\nfunctions (F 1, F 2, F 3, . . . ) stabilises in such a way. For example, this won’t\nhappen for the function G deﬁned above (see Exercise 1(d) on page 253).\nThe following fact is a special case of a fundamental insight, often referred\nto as the Knaster–Tarski Theorem.\nTheorem 3.24 Let S be a set {s0, s1, . . . , sn} with n + 1 elements. If\nF : P(S) →P(S) is a monotone function, then F n+1(∅) is the least ﬁxed\npoint of F and F n+1(S) is the greatest ﬁxed point of F.\nPROOF: Since ∅⊆F(∅), we get F(∅) ⊆F(F(∅)), i.e., F 1(∅) ⊆F 2(∅), for F\nis monotone. We can now use mathematical induction to show that\nF 1(∅) ⊆F 2(∅) ⊆F 3(∅) ⊆. . . ⊆F i(∅)\nfor all i ≥1. In particular, taking i\ndef\n= n + 1, we claim that one of the expres-\nsions F k(∅) above is already a ﬁxed point of F. Otherwise, F 1(∅) needs to\ncontain at least one element (for then ∅̸= F(∅)). By the same token, F 2(∅)\nneeds to have at least two elements since it must be bigger than F 1(∅). Con-\ntinuing this argument, we see that F n+2(∅) would have to contain at least\nn + 2 many elements. The latter is impossible since S has only n + 1 ele-\nments. Therefore, F(F k(∅)) = F k(∅) for some 0 ≤k ≤n + 1, which readily\nimplies that F n+1(∅) is a ﬁxed point of F as well.\nNow suppose that X is another ﬁxed point of F. We need to show that\nF n+1(∅) is a subset of X; but, since ∅⊆X, we conclude F(∅) ⊆F(X) =\nX, for F is monotone and X a ﬁxed point of F. By induction, we obtain\nF i(∅) ⊆X for all i ≥0. So, for i\ndef\n= n + 1, we get F n+1(∅) ⊆X.",
    "Now suppose that X is another ﬁxed point of F. We need to show that\nF n+1(∅) is a subset of X; but, since ∅⊆X, we conclude F(∅) ⊆F(X) =\nX, for F is monotone and X a ﬁxed point of F. By induction, we obtain\nF i(∅) ⊆X for all i ≥0. So, for i\ndef\n= n + 1, we get F n+1(∅) ⊆X.\nThe proof of the statements about the greatest ﬁxed point is dual to the\none above. Simply replace ⊆by ⊇, ∅by S and ‘bigger’ by ‘smaller.’\n2\n242\n3 Verification by model checking\nThis theorem about the existence of least and greatest ﬁxed points of\nmonotone functions F : P(S) →P(S) not only asserted the existence of\nsuch ﬁxed points; it also provided a recipe for computing them, and cor-\nrectly so. For example, in computing the least ﬁxed point of F, all we have\nto do is apply F to the empty set ∅and keep applying F to the result un-\ntil the latter becomes invariant under the application of F. The theorem\nabove further ensures that this process is guaranteed to terminate. More-\nover, we can specify an upper bound n + 1 to the worst-case number of\niterations necessary for reaching this ﬁxed point, assuming that S has n + 1\nelements.\n3.7.2 The correctness of SATEG\nWe saw at the end of the last section that [[EG φ]] = [[φ]] ∩pre∃([[EG φ]]). This\nimplies that EG φ is a ﬁxed point of the function F(X) = [[φ]] ∩pre∃(X). In\nfact, F is monotone, EG φ is its greatest ﬁxed point and therefore EG φ can\nbe computed using Theorem 3.24.\nTheorem 3.25 Let F be as deﬁned above and let S have n + 1 elements.\nThen F is monotone, [[EG φ]] is the greatest ﬁxed point of F, and [[EG φ]] =\nF n+1(S).\nPROOF:\n1.\nIn order to show that F is monotone, we take any two subsets X and Y of S\nsuch that X ⊆Y and we need to show that F(X) is a subset of F(Y ). Given s0\nsuch that there is some s1 ∈X with s0 →s1, we certainly have s0 →s1, where\ns1 ∈Y , for X is a subset of Y . Thus, we showed pre∃(X) ⊆pre∃(Y ) from which\nwe readily conclude that F(X) = [[φ]] ∩pre∃(X) ⊆[[φ]] ∩pre∃(Y ) = F(Y ).\n2.",
    "such that there is some s1 ∈X with s0 →s1, we certainly have s0 →s1, where\ns1 ∈Y , for X is a subset of Y . Thus, we showed pre∃(X) ⊆pre∃(Y ) from which\nwe readily conclude that F(X) = [[φ]] ∩pre∃(X) ⊆[[φ]] ∩pre∃(Y ) = F(Y ).\n2.\nWe have already seen that [[EG φ]] is a ﬁxed point of F. To show that it is the\ngreatest ﬁxed point, it suﬃces to show here that any set X with F(X) = X has\nto be contained in [[EG φ]]. So let s0 be an element of such a ﬁxed point X. We\nneed to show that s0 is in [[EG φ]] as well. For that we use the fact that\ns0 ∈X = F(X) = [[φ]] ∩pre∃(X)\nto infer that s0 ∈[[φ]] and s0 →s1 for some s1 ∈X; but, since s1 is in X,\nwe may apply that same argument to s1 ∈X = F(X) = [[φ]] ∩pre∃(X) and we\nget s1 ∈[[φ]] and s1 →s2 for some s2 ∈X. By mathematical induction, we can\ntherefore construct an inﬁnite path s0 →s1 →· · · →sn →sn+1 →. . . such that\nsi ∈[[φ]] for all i ≥0. By the deﬁnition of [[EG φ]], this entails s0 ∈[[EG φ]].\n3.\nThe last item is now immediately accessible from the previous one and Theo-\nrem 3.24.\n2\n3.7 The fixed-point characterisation of CTL\n243\nNow we can see that the procedure SATEG is correctly coded and termi-\nnates. First, note that the line Y := Y ∩pre∃(Y ) in the procedure SATEG\n(Figure 3.37) could be changed to Y := SAT(φ) ∩pre∃(Y ) without changing\nthe eﬀect of the procedure. To see this, note that the ﬁrst time round the\nloop, Y is SAT(φ); and in subsequent loops, Y ⊆SAT(φ), so it doesn’t matter\nwhether we intersect with Y or SAT(φ)2. With the change, it is clear that\nSATEG is calculating the greatest ﬁxed point of F; therefore its correctness\nfollows from Theorem 3.25.\n3.7.3 The correctness of SATEU\nProving the correctness of SATEU is similar. We start by noting the equiv-\nalence E[φ U ψ] ≡ψ ∨(φ ∧EX E[φ U ψ]) and we write it as [[E[φ U ψ]]] =\n[[ψ]] ∪([[φ]] ∩pre∃[[E[φ U ψ]]]). That tells us that [[E[φ U ψ]]] is a ﬁxed point\nof the function G(X) = [[ψ]] ∪([[φ]] ∩pre∃(X)). As before, we can prove that",
    "alence E[φ U ψ] ≡ψ ∨(φ ∧EX E[φ U ψ]) and we write it as [[E[φ U ψ]]] =\n[[ψ]] ∪([[φ]] ∩pre∃[[E[φ U ψ]]]). That tells us that [[E[φ U ψ]]] is a ﬁxed point\nof the function G(X) = [[ψ]] ∪([[φ]] ∩pre∃(X)). As before, we can prove that\nthis function is monotone. It turns out that [[E[φ U ψ]]] is its least ﬁxed\npoint and that the function SATEU is actually computing it in the manner of\nTheorem 3.24.\nTheorem 3.26 Let G be deﬁned as above and let S have n + 1 elements.\nThen G is monotone, [[E(φ U ψ)]] is the least ﬁxed point of G, and we have\n[[E(φ U ψ)]] = Gn+1(∅).\n2 If you are sceptical, try computing the values Y0, Y1, Y2, . . . , where Yi represents the value of Y\nafter i iterations round the loop. The program before the change computes as follows:\nY0 = SAT(φ)\nY1 = Y0 ∩pre∃(Y0)\nY2 = Y1 ∩pre∃(Y1)\n= Y0 ∩pre∃(Y0) ∩pre∃(Y0 ∩pre∃(Y0))\n= Y0 ∩pre∃(Y0 ∩pre∃(Y0)).\nThe last of these equalities follows from the monotonicity of pre∃.\nY3 = Y2 ∩pre∃(Y2)\n= Y0 ∩pre∃(Y0 ∩pre∃(Y0)) ∩pre∃(Y0 ∩pre∃(Y0 ∩pre∃(Y0)))\n= Y0 ∩pre∃(Y0 ∩pre∃(Y0 ∩pre∃(Y0))).\nAgain the last one follows by monotonicity. Now look at what the program does after the change:\nY0 = SAT(φ)\nY1 = SAT(φ) ∩pre∃(Y0)\n= Y0 ∩pre∃(Y0)\nY2 = Y0 ∩pre∃(Y1)\nY3 = Y0 ∩pre∃(Y1)\n= Y0 ∩pre∃(Y0 ∩pre∃(Y0)).\nA formal proof would follow by induction on i.\n244\n3 Verification by model checking\nPROOF:\n1.\nAgain, we need to show that X ⊆Y implies G(X) ⊆G(Y ); but that is essen-\ntially the same argument as for F, since the function which sends X to pre∃(X)\nis monotone and all that G now does is to perform the intersection and union\nof that set with constant sets [[φ]] and [[ψ]].\n2.\nIf S has n + 1 elements, then the least ﬁxed point of G equals Gn+1(∅) by\nTheorem 3.24. Therefore it suﬃces to show that this set equals [[E(φ U ψ)]].\nSimply observe what kind of states we obtain by iterating G on the empty set\n∅: G1(∅) = [[ψ]] ∪([[φ]] ∩pre∃([[∅]])) = [[ψ]] ∪([[φ]] ∩∅) = [[ψ]] ∪∅= [[ψ]], which are",
    "Theorem 3.24. Therefore it suﬃces to show that this set equals [[E(φ U ψ)]].\nSimply observe what kind of states we obtain by iterating G on the empty set\n∅: G1(∅) = [[ψ]] ∪([[φ]] ∩pre∃([[∅]])) = [[ψ]] ∪([[φ]] ∩∅) = [[ψ]] ∪∅= [[ψ]], which are\nall states s0 ∈[[E(φ U ψ)]], where we chose i = 0 according to the deﬁnition of\nUntil. Now,\nG2(∅) = [[ψ]] ∪([[φ]] ∩pre∃(G1(∅)))\ntells us that the elements of G2(∅) are all those s0 ∈[[E(φ U ψ)]] where we chose\ni ≤1. By mathematical induction, we see that Gk+1(∅) is the set of all states\ns0 for which we chose i ≤k to secure s0 ∈[[E(φ U ψ)]]. Since this holds for all\nk, we see that [[E(φ U ψ)]] is nothing but the union of all sets Gk+1(∅) with\nk ≥0; but, since Gn+1(∅) is a ﬁxed point of G, we see that this union is just\nGn+1(∅).\n2\nThe correctness of the coding of SATEU follows similarly to that of\nSATEG. We change the line Y := Y ∪(W ∩pre∃(Y )) into Y := SAT(ψ) ∪\n(W ∩pre∃(Y )) and observe that this does not change the result of the pro-\ncedure, because the ﬁrst time round the loop, Y is SAT(ψ); and, since Y is\nalways increasing, it makes no diﬀerence whether we perform a union with\nY or with SAT(ψ). Having made that change, it is then clear that SATEU is\njust computing the least ﬁxed point of G using Theorem 3.24.\nWe illustrate these results about the functions F\nand G above\nthrough an example. Consider the system in Figure 3.38. We begin\nby computing the set [[EF p]]. By the deﬁnition of EF\nthis is just\n[[E(⊤U p)]]. So we have φ1\ndef\n= ⊤and φ2\ndef\n= p. From Figure 3.38, we ob-\ntain [[p]] = {s3} and of course [[⊤]] = S. Thus, the function G above\nequals G(X) = {s3} ∪pre∃(X). Since [[E(⊤U p)]] equals the least ﬁxed\npoint of G, we need to iterate G on ∅until this process stabilises.\nFirst, G1(∅) = {s3} ∪pre∃(∅) = {s3}. Second, G2(∅) = G(G1(∅)) = {s3} ∪\npre∃({s3}) = {s1, s3}. Third, G3(∅) = G(G2(∅)) = {s3} ∪pre∃({s1, s3}) =\n{s0, s1, s2, s3}. Fourth, G4(∅) = G(G3(∅)) = {s3} ∪pre∃({s0, s1, s2, s3}) =",
    "point of G, we need to iterate G on ∅until this process stabilises.\nFirst, G1(∅) = {s3} ∪pre∃(∅) = {s3}. Second, G2(∅) = G(G1(∅)) = {s3} ∪\npre∃({s3}) = {s1, s3}. Third, G3(∅) = G(G2(∅)) = {s3} ∪pre∃({s1, s3}) =\n{s0, s1, s2, s3}. Fourth, G4(∅) = G(G3(∅)) = {s3} ∪pre∃({s0, s1, s2, s3}) =\n{s0, s1, s2, s3}. Therefore, {s0, s1, s2, s3} is the least ﬁxed point of G,\nwhich equals [[E(⊤U p)]] by Theorem 3.20. But then [[E(⊤U p)]] =\n[[EF p]].\n3.8 Exercises\n245\ns0\ns1\ns4\nq\nq\np\ns3\ns2\nFigure 3.38. A system for which we compute invariants.\nThe other example we study is the computation of the set [[EG q]]. By\nTheorem 3.25, that set is the greatest ﬁxed point of the function F above,\nwhere φ\ndef\n= q. From Figure 3.38 we see that [[q]] = {s0, s4} and so F(X) =\n[[q]] ∩pre∃(X) = {s0, s4} ∩pre∃(X). Since [[EG q]] equals the greatest ﬁxed\npoint of F, we need to iterate F on S until this process stabilises. First,\nF 1(S) = {s0, s4} ∩pre∃(S) = {s0, s4} ∩S since every s has some s′ with s →\ns′. Thus, F 1(S) = {s0, s4}.\nSecond, F 2(S) = F(F 1(S)) = {s0, s4} ∩pre∃({s0, s4}) = {s0, s4}. There-\nfore, {s0, s4} is the greatest ﬁxed point of F, which equals [[EG q]] by Theo-\nrem 3.25.\n3.8 Exercises\nExercises 3.1\n1. Read Section 2.7 in case you have not yet done so and classify Alloy and its\nconstraint analyser according to the classiﬁcation criteria for formal methods\nproposed on page 172.\n2. Visit and browse the websites3 and4 to ﬁnd formal methods that interest you for\nwhatever reason. Then classify them according to the criteria from page 172.\nExercises 3.2\n1. Draw parse trees for the LTL formulas:\n(a) F p ∧G q →p W r\n(b) F (p →G r) ∨¬q U p\n(c) p W (q W r)\n(d) G F p →F (q ∨s)\n3 www.afm.sbu.ac.uk\n4 www.cs.indiana.edu/formal-methods-education/\n246\n3 Verification by model checking\nq3\nq1\nq2\nab\nq4\nab\nab\nab\nFigure 3.39. A model M.\n2. Consider the system of Figure 3.39. For each of the formulas φ:\n(a) G a\n(b) a U b\n(c) a U X (a ∧¬b)\n(d) X ¬b ∧G (¬a ∨¬b)\n(e) X (a ∧b) ∧F (¬a ∧¬b)",
    "4 www.cs.indiana.edu/formal-methods-education/\n246\n3 Verification by model checking\nq3\nq1\nq2\nab\nq4\nab\nab\nab\nFigure 3.39. A model M.\n2. Consider the system of Figure 3.39. For each of the formulas φ:\n(a) G a\n(b) a U b\n(c) a U X (a ∧¬b)\n(d) X ¬b ∧G (¬a ∨¬b)\n(e) X (a ∧b) ∧F (¬a ∧¬b)\n(i) Find a path from the initial state q3 which satisﬁes φ.\n(ii) Determine whether M, q3 ⊨φ.\n3. Working from the clauses of Deﬁnition 3.1 (page 175), prove the equivalences:\nφ U ψ ≡φ W ψ ∧F ψ\nφ W ψ ≡φ U ψ ∨G φ\nφ W ψ ≡ψ R (φ ∨ψ)\nφ R ψ ≡ψ W (φ ∧ψ) .\n4. Prove that φ U ψ ≡ψ R (φ ∨ψ) ∧F ψ.\n5. List all subformulas of the LTL formula ¬p U (F r ∨G ¬q →q W ¬r).\n6. ‘Morally’ there ought to be a dual for W. Work out what it might mean, and\nthen pick a symbol based on the ﬁrst letter of the meaning.\n7. Prove that for all paths π of all models, π ⊨φ W ψ ∧F ψ implies π ⊨φ U ψ.\nThat is, prove the remaining half of equivalence (3.2) on page 185.\n8. Recall the algorithm NNF on page 62 which computes the negation normal form\nof propositional logic formulas. Extend this algorithm to LTL: you need to add\nprogram clauses for the additional connectives X, F, G and U, R and W; these\nclauses have to animate the semantic equivalences that we presented in this\nsection.\n3.8 Exercises\n247\nExercises 3.3\n1. Consider the model in Figure 3.9 (page 193).\n(a)\n*\nVerify that G(req -> F busy) holds in all initial states.\n(b) Does ¬(req U ¬busy) hold in all initial states of that model?\n(c) NuSMV has the capability of referring to the next value of a declared vari-\nable v by writing next(v). Consider the model obtained from Figure 3.9\nby removing the self-loop on state !req & busy. Use the NuSMV feature\nnext(...) to code that modiﬁed model as an NuSMV program with the\nspeciﬁcation G(req -> F busy). Then run it.\n2. Verify Remark 3.11 from page 190.\n3.\n*\nDraw the transition system described by the ABP program.\nRemarks: There are 28 reachable states of the ABP program. (Looking at the",
    "next(...) to code that modiﬁed model as an NuSMV program with the\nspeciﬁcation G(req -> F busy). Then run it.\n2. Verify Remark 3.11 from page 190.\n3.\n*\nDraw the transition system described by the ABP program.\nRemarks: There are 28 reachable states of the ABP program. (Looking at the\nprogram, you can see that the state is described by nine boolean variables, namely\nS.st, S.message1, S.message2, R.st, R.ack, R.expected, msg chan.output1,\nmsg chan.output2 and ﬁnally ack chan.output. Therefore, there are 29 = 512\nstates in total. However, only 28 of them can be reached from the initial state\nby following a ﬁnite path.)\nIf you abstract away from the contents of the message (e.g., by setting\nS.message1 and msg chan.output1 to be constant 0), then there are only 12\nreachable states. This is what you are asked to draw.\nExercises 3.4\n1. Write the parse trees for the following CTL formulas:\n(a)\n*\nEG r\n(b)\n*\nAG (q →EG r)\n(c)\n*\nA[p U EF r]\n(d)\n*\nEF EG p →AF r, recall Convention 3.13\n(e) A[p U A[q U r]]\n(f) E[A[p U q] U r]\n(g) AG (p →A[p U (¬p ∧A[¬p U q])]).\n2. Explain why the following are not well-formed CTL formulas:\n(a)\n*\nF G r\n(b) X X r\n(c) A¬G ¬p\n(d) F [r U q]\n(e) EX X r\n(f)\n*\nAEF r\n(g)\n*\nAF [(r U q) ∧(p U r)].\n3. State which of the strings below are well-formed CTL formulas. For those which\nare well-formed, draw the parse tree. For those which are not well-formed,\nexplain why not.\n248\n3 Verification by model checking\nr\np, q\nq, r\ns3\np, t, r\ns1\ns2\ns0\nFigure 3.40. A model with four states.\n(a) ¬(¬p) ∨(r ∧s)\n(b) X q\n(c)\n*\n¬AX q\n(d) p U (AX ⊥)\n(e)\n*\nE[(AX q) U (¬(¬p) ∨(⊤∧s))]\n(f)\n*\n(F r) ∧(AG q)\n(g) ¬(AG q) ∨(EG q).\n4.\n*\nList all subformulas of the formula AG (p →A[p U (¬p ∧A[¬p U q])]).\n5. Does E[req U ¬busy] hold in all initial states of the model in Figure 3.9 on\npage 193?\n6. Consider the system M in Figure 3.40.\n(a) Beginning from state s0, unwind this system into an inﬁnite tree, and draw\nall computation paths up to length 4 (= the ﬁrst four layers of that tree).",
    "5. Does E[req U ¬busy] hold in all initial states of the model in Figure 3.9 on\npage 193?\n6. Consider the system M in Figure 3.40.\n(a) Beginning from state s0, unwind this system into an inﬁnite tree, and draw\nall computation paths up to length 4 (= the ﬁrst four layers of that tree).\n(b) Determine whether M, s0 ⊨φ and M, s2 ⊨φ hold and justify your answer,\nwhere φ is the LTL or CTL formula:\n(i)\n*\n¬p →r\n(ii) F t\n(iii)\n*\n¬EG r\n(iv) E (t U q)\n(v) F q\n(vi) EF q\n(vii) EG r\n(viii) G (r ∨q).\n7. Let M = (S, →, L) be any model for CTL and let [[φ]] denote the set of all s ∈S\nsuch that M, s ⊨φ. Prove the following set identities by inspecting the clauses\nof Deﬁnition 3.15 from page 211.\n(a)\n*\n[[⊤]] = S,\n(b) [[⊥]] = ∅\n3.8 Exercises\n249\nr\np, q\nq, r\np, t\ns2\ns3\ns1\ns0\nFigure 3.41. Another model with four states.\n(c) [[¬φ]] = S −[[φ]],\n(d) [[φ1 ∧φ2]] = [[φ1]] ∩[[φ2]]\n(e) [[φ1 ∨φ2]] = [[φ1]] ∪[[φ2]]\n(f)\n*\n[[φ1 →φ2]] = (S −[[φ1]]) ∪[[φ2]]\n(g)\n*\n[[AX φ]] = S −[[EX ¬φ]]\n(h) [[A(φ2 U φ2)]] = [[¬(E(¬φ1 U (¬φ1 ∧¬φ2)) ∨EG ¬φ2)]].\n8. Consider the model M in Figure 3.41. Check whether M, s0 ⊨φ and M, s2 ⊨φ\nhold for the CTL formulas φ:\n(a) AF q\n(b) AG (EF (p ∨r))\n(c) EX (EX r)\n(d) AG (AF q).\n9.\n*\nThe meaning of the temporal operators F, G and U in LTL and AU, EU, AG,\nEG, AF and EF in CTL was deﬁned to be such that ‘the present includes the\nfuture.’ For example, EF p is true for a state if p is true for that state already.\nOften one would like corresponding operators such that the future excludes the\npresent. Use suitable connectives of the grammar on page 208 to deﬁne such\n(six) modiﬁed connectives as derived operators in CTL.\n10. Which of the following pairs of CTL formulas are equivalent? For those which\nare not, exhibit a model of one of the pair which is not a model of the\nother:\n(a) EF φ and EG φ\n(b)\n*\nEF φ ∨EF ψ and EF (φ ∨ψ)\n(c)\n*\nAF φ ∨AF ψ and AF (φ ∨ψ)\n(d) AF ¬φ and ¬EG φ\n(e)\n*\nEF ¬φ and ¬AF φ\n(f) A[φ1 U A[φ2 U φ3]] and A[A[φ1 U φ2] U φ3], hint: it might make it simpler",
    "are not, exhibit a model of one of the pair which is not a model of the\nother:\n(a) EF φ and EG φ\n(b)\n*\nEF φ ∨EF ψ and EF (φ ∨ψ)\n(c)\n*\nAF φ ∨AF ψ and AF (φ ∨ψ)\n(d) AF ¬φ and ¬EG φ\n(e)\n*\nEF ¬φ and ¬AF φ\n(f) A[φ1 U A[φ2 U φ3]] and A[A[φ1 U φ2] U φ3], hint: it might make it simpler\nif you think ﬁrst about models that have just one path\n(g) ⊤and AG φ →EG φ\n(h)\n*\n⊤and EG φ →AG φ.\n11. Find operators to replace the ?, to make the following equivalences:\n250\n3 Verification by model checking\n(a)\n*\nAG (φ ∧ψ) ≡AG φ ? AG ψ\n(b) EF ¬φ ≡¬??φ\n12. State explicitly the meaning of the temporal connectives AR etc., as deﬁned on\npage 217.\n13. Prove the equivalences (3.6) on page 216.\n14.\n*\nWrite pseudo-code for a recursive function TRANSLATE which takes as input\nan arbitrary CTL formula φ and returns as output an equivalent CTL formula\nψ whose only operators are among the set {⊥, ¬, ∧, AF , EU , EX }.\nExercises 3.5\n1. Express the following properties in CTL and LTL whenever possible. If neither\nis possible, try to express the property in CTL*:\n(a)\n*\nWhenever p is followed by q (after ﬁnitely many steps), then the system\nenters an ‘interval’ in which no r occurs until t.\n(b) Event p precedes s and t on all computation paths. (You may ﬁnd it easier\nto code the negation of that speciﬁcation ﬁrst.)\n(c) After p, q is never true. (Where this constraint is meant to apply on all\ncomputation paths.)\n(d) Between the events q and r, event p is never true.\n(e) Transitions to states satisfying p occur at most twice.\n(f)\n*\nProperty p is true for every second state along a path.\n2. Explain in detail why the LTL and CTL formulas for the practical speciﬁcation\npatterns of pages 183 and 215 capture the stated ‘informal’ properties expressed\nin plain English.\n3. Consider the set of LTL/CTL formulas F = {F p →F q, AF p →AF q, AG (p →\nAF q)}.\n(a) Is there a model such that all formulas hold in it?\n(b) For each φ ∈F, is there a model such that φ is the only formula in F satisﬁed\nin that model?",
    "in plain English.\n3. Consider the set of LTL/CTL formulas F = {F p →F q, AF p →AF q, AG (p →\nAF q)}.\n(a) Is there a model such that all formulas hold in it?\n(b) For each φ ∈F, is there a model such that φ is the only formula in F satisﬁed\nin that model?\n(c) Find a model in which no formula of F holds.\n4. Consider the CTL formula AG (p →AF (s ∧AX (AF t))). Explain what exactly\nit expresses in terms of the order of occurrence of events p, s and t.\n5. Extend the algorithm NNF from page 62 which computes the negation normal\nform of propositional logic formulas to CTL*. Since CTL* is deﬁned in terms\nof two syntactic categories (state formulas and path formulas), this requires two\nseparate versions of NNF which call each other in a way that is reﬂected by the\nsyntax of CTL* given on page 218.\n6. Find a transition system which distinguishes the following pairs of CTL* formu-\nlas, i.e., show that they are not equivalent:\n(a) AF G p and AF AG p\n(b)\n*\nAG F p and AG EF p\n(c) A[(p U r) ∨(q U r)] and A[(p ∨q) U r)]\n3.8 Exercises\n251\n(d)\n*\nA[X p ∨X X p] and AX p ∨AX AX p\n(e) E[G F p] and EG EF p.\n7. The translation from CTL with boolean combinations of path formulas to plain\nCTL introduced in Section 3.5.1 is not complete. Invent CTL equivalents for:\n(a)\n*\nE[F p ∧(q U r)]\n(b)\n*\nE[F p ∧G q].\nIn this way, we have dealt with all formulas of the form E[φ ∧ψ]. Formulas of the\nform E[φ ∨ψ] can be rewritten as E[φ] ∨E[ψ] and A[φ] can be written ¬E[¬φ].\nUse this translation to write the following in CTL:\n(c) E[(p U q) ∧F p]\n(d)\n*\nA[(p U q) ∧G p]\n(e)\n*\nA[F p →F q].\n8. The aim of this exercise is to demonstrate the expansion given for AW at the\nend of the last section, i.e., A[p W q] ≡¬E[¬q U ¬(p ∨q)].\n(a) Show that the following LTL formulas are valid (i.e., true in any state of any\nmodel):\n(i) ¬q U (¬p ∧¬q) →¬G p\n(ii) G ¬q ∧F ¬p →¬q U (¬p ∧¬q).\n(b) Expand ¬((p U q) ∨G p) using de Morgan rules and the LTL equivalence\n¬(φ U ψ) ≡(¬ψ U (¬φ ∧¬ψ)) ∨¬F ψ.",
    "(a) Show that the following LTL formulas are valid (i.e., true in any state of any\nmodel):\n(i) ¬q U (¬p ∧¬q) →¬G p\n(ii) G ¬q ∧F ¬p →¬q U (¬p ∧¬q).\n(b) Expand ¬((p U q) ∨G p) using de Morgan rules and the LTL equivalence\n¬(φ U ψ) ≡(¬ψ U (¬φ ∧¬ψ)) ∨¬F ψ.\n(c) Using your expansion and the facts (i) and (ii) above, show ¬((p U q) ∨\nG p) ≡¬q U ¬(p ∧q) and hence show that the desired expansion of AW\nabove is correct.\nExercises 3.6\n1.\n*\nVerify φ1 to φ4 for the transition system given in Figure 3.11 on page 198. Which\nof them require the fairness constraints of the SMV program in Figure 3.10?\n2. Try to write a CTL formula that enforces non-blocking and no-strict-sequencing\nat the same time, for the SMV program in Figure 3.10 (page 196).\n3.\n*\nApply the labelling algorithm to check the formulas φ1, φ2, φ3 and φ4 of the\nmutual exclusion model in Figure 3.7 (page 188).\n4. Apply the labelling algorithm to check the formulas φ1, φ2, φ3 and φ4 of the\nmutual exclusion model in Figure 3.8 (page 191).\n5. Prove that (3.8) on page 228 holds in all models. Does your proof require that\nfor every state s there is some state s′ with s →s′?\n6. Inspecting the deﬁnition of the labelling algorithm, explain what happens if you\nperform it on the formula p ∧¬p (in any state, in any model).\n7. Modify the pseudo-code for SAT on page 227 by writing a special procedure for\nAG ψ1, without rewriting it in terms of other formulas5.\n5 Question: will your routine be more like the routine for AF, or more like that for EG on page 224?\nWhy?\n252\n3 Verification by model checking\n8.\n*\nWrite the pseudo-code for SATEG, based on the description in terms of deleting\nlabels given in Section 3.6.1.\n9.\n*\nFor mutual exclusion, draw a transition system which forces the two processes\nto enter their critical section in strict sequence and show that φ4 is false of its\ninitial state.\n10. Use the deﬁnition of ⊨between states and CTL formulas to explain why s ⊨",
    "labels given in Section 3.6.1.\n9.\n*\nFor mutual exclusion, draw a transition system which forces the two processes\nto enter their critical section in strict sequence and show that φ4 is false of its\ninitial state.\n10. Use the deﬁnition of ⊨between states and CTL formulas to explain why s ⊨\nAG AF φ means that φ is true inﬁnitely often along every path starting at s.\n11.\n*\nShow that a CTL formula φ is true on inﬁnitely many states of a computa-\ntion path s0 →s1 →s2 →. . . iﬀfor all n ≥0 there is some m ≥n such that\nsm ⊨φ.\n12. Run the NuSMV system on some examples. Try commenting out, or deleting,\nsome of the fairness constraints, if applicable, and see the counter examples\nNuSMV generates. NuSMV is very easy to run.\n13. In the one-bit channel, there are two fairness constraints. We could have written\nthis as a single one, inserting ‘&’ between running and the long formula, or we\ncould have separated the long formula into two and made it into a total of three\nfairness constraints.\nIn general, what is the diﬀerence between the single fairness constraint φ1 ∧φ2 ∧\n· · · ∧φn and the n fairness constraints φ1, φ2, . . . , φn? Write an SMV program\nwith a fairness constraint a & b which is not equivalent to the two fairness\nconstraints a and b. (You can actually do it in four lines of SMV.)\n14. Explain the construction of formula φ4, used to express that the processes need\nnot enter their critical section in strict sequence. Does it rely on the fact that\nthe safety property φ1 holds?\n15.\n*\nCompute the ECG ⊤labels for Figure 3.11, given the fairness constraints of the\ncode in Figure 3.10 on page 196.\nExercises 3.7\n1. Consider the functions\nH1, H2, H3 : P({1, 2, 3, 4, 5, 6, 7, 8, 9, 10}) →P({1, 2, 3, 4, 5, 6, 7, 8, 9, 10})\ndeﬁned by\nH1(Y )\ndef\n= Y −{1, 4, 7}\nH2(Y )\ndef\n= {2, 5, 9} −Y\nH3(Y )\ndef\n= {1, 2, 3, 4, 5} ∩({2, 4, 8} ∪Y )\nfor all Y ⊆{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.\n(a)\n*\nWhich of these functions are monotone; which ones aren’t? Justify your an-\nswer in each case.\n(b)\n*",
    "deﬁned by\nH1(Y )\ndef\n= Y −{1, 4, 7}\nH2(Y )\ndef\n= {2, 5, 9} −Y\nH3(Y )\ndef\n= {1, 2, 3, 4, 5} ∩({2, 4, 8} ∪Y )\nfor all Y ⊆{1, 2, 3, 4, 5, 6, 7, 8, 9, 10}.\n(a)\n*\nWhich of these functions are monotone; which ones aren’t? Justify your an-\nswer in each case.\n(b)\n*\nCompute the least and greatest ﬁxed points of H3 using the iterations Hi\n3\nwith i = 1, 2, . . . and Theorem 3.24.\n3.8 Exercises\n253\nq\np\nq\nq\nFigure 3.42. Another system for which we compute invariants.\n(c) Does H2 have any ﬁxed points?\n(d) Recall G: P({s0, s1}) →P({s0, s1}) with\nG(Y )\ndef\n= if Y = {s0} then {s1} else {s0} .\nUse mathematical induction to show that Gi equals G for all odd numbers\ni ≥1. What does Gi look like for even numbers i?\n2.\n*\nLet A and B be two subsets of S and let F : P(S) →P(S) be a monotone\nfunction. Show that:\n(a) F1 : P(S) →P(S) with F1(Y )\ndef\n= A ∩F(Y ) is monotone;\n(b) F2 : P(S) →P(S) with F2(Y )\ndef\n= A ∪(B ∩F(Y )) is monotone.\n3. Use Theorems 3.25 and 3.26 to compute the following sets (the underlying model\nis in Figure 3.42):\n(a) [[EF p]]\n(b) [[EG q]].\n4. Using the function F(X) = [[φ]] ∪pre∀(X) prove that [[AF φ]] is the least ﬁxed\npoint of F. Hence argue that the procedure SATAF is correct and terminates.\n5.\n*\nOne may also compute AG φ directly as a ﬁxed point. Consider the function\nH : P(S) →P(S) with H(X) = [[φ]] ∩pre∀(X). Show that H is monotone and\nthat [[AG φ]] is the greatest ﬁxed point of H. Use that insight to write a procedure\nSATAG.\n6. Similarly, one may compute A[φ1 U φ2] directly as a ﬁxed point, using\nK : P(S) →P(S), where K(X) = [[φ2]] ∪([[φ1]] ∩pre∀(X)). Show that K is\nmonotone and that [[A[φ1 U φ2]]] is the least ﬁxed point of K. Use that insight\nto write a procedure SATAU. Can you use that routine to handle all calls of the\nform AF φ as well?\n7. Prove that [[A[φ1 U φ2]]] = [[φ2 ∨(φ1 ∧AX (A[φ1 U φ2]))]].\n8. Prove that [[AG φ]] = [[φ ∧AX (AG φ)]].\n9. Show that the repeat-statements in the code for SATEU and SATEG always termi-",
    "to write a procedure SATAU. Can you use that routine to handle all calls of the\nform AF φ as well?\n7. Prove that [[A[φ1 U φ2]]] = [[φ2 ∨(φ1 ∧AX (A[φ1 U φ2]))]].\n8. Prove that [[AG φ]] = [[φ ∧AX (AG φ)]].\n9. Show that the repeat-statements in the code for SATEU and SATEG always termi-\nnate. Use this fact to reason informally that the main program SAT terminates\nfor all valid CTL formulas φ. Note that some subclauses, like the one for AU,\ncall SAT recursively and with a more complex formula. Why does this not aﬀect\ntermination?\n254\n3 Verification by model checking\n3.9 Bibliographic notes\nTemporal logic was invented by the philosopher A. Prior in the 1960s; his\nlogic was similar to what we now call LTL. The ﬁrst use of temporal logic for\nreasoning about concurrent programs was by A. Pnueli [Pnu81]. The logic\nCTL was invented by E. Clarke and E. A. Emerson (during the early 1980s);\nand CTL* was invented by E. A. Emerson and J. Halpern (in 1986) to unify\nCTL and LTL.\nCTL model checking was invented by E. Clarke and E. A. Emerson [CE81]\nand by J. Quielle and J. Sifakis [QS81]. The technique we described for LTL\nmodel checking was invented by M. Vardi and P. Wolper [VW84]. Surveys\nof some of these ideas can be found in [CGL93] and [CGP99]. The theorem\nabout adequate sets of CTL connectives is proved in [Mar01].\nThe original SMV system was written by K. McMillan [McM93] and is\navailable with source code from Carnegie Mellon University6. NuSMV7 is a\nreimplementation, developed in Trento by A. Cimatti, and M. Roveri and is\naimed at being customisable and extensible. Extensive documentation about\nNuSMV can be found at that site. NuSMV supports essentially the same\nsystem description language as CMU SMV, but it has an improved user in-\nterface and a greater variety of algorithms. For example, whereas CMU SMV\nchecks only CTL speciﬁcation, NuSMV supports LTL and CTL. NuSMV im-\nplements bounded model checking [BCCZ99]. Cadence SMV8 is an entirely",
    "system description language as CMU SMV, but it has an improved user in-\nterface and a greater variety of algorithms. For example, whereas CMU SMV\nchecks only CTL speciﬁcation, NuSMV supports LTL and CTL. NuSMV im-\nplements bounded model checking [BCCZ99]. Cadence SMV8 is an entirely\nnew model checker focused on compositional systems and abstraction as\nways of addressing the state explosion problem. It was also developed by\nK. McMillan and its description language resembles but much extends the\noriginal SMV.\nA website which gathers frequently used speciﬁcation patterns in various\nframeworks (such as CTL, LTL and regular expressions) is maintained by\nM. Dwyer, G. Avrunin, J. Corbett and L. Dillon9.\nCurrent research in model checking includes attempts to exploit abstrac-\ntions, symmetries and compositionality [CGL94, Lon83, Dam96] in order to\nreduce the impact of the state explosion problem.\nThe model checker Spin, which is geared towards asynchronous systems\nand is based on the temporal logic LTL, can be found at the Spin website10. A\nmodel checker called FDR2 based on the process algebra CSP is available11.\n6 www.cs.cmu.edu/~modelcheck/\n7 nusmv.irst.itc.it\n8 www-cad.eecs.berkeley.edu/~kenmcmil/\n9 patterns.projects.cis.ksu.edu/\n10 netlib.bell-labs.com/netlib/spin/whatispin.html\n11 www.fsel.com.fdr2 download.html\n3.9 Bibliographic notes\n255\nThe Edinburgh Concurrency Workbench12 and the Concurrency Workbench\nof North Carolina13 are similar software tools for the design and analysis of\nconcurrent systems. An example of a customisable and extensible modular\nmodel checking frameworks for the veriﬁcation of concurrent software is\nBogor14.\nThere are many textbooks about veriﬁcation of reactive systems; we men-\ntion [MP91, MP95, Ros97, Hol90]. The SMV code contained in this chapter\ncan be downloaded from www.cs.bham.ac.uk/research/lics/.\n12 www.dcs.ed.ac.uk/home/cwb\n13 www.cs.sunysb.edu/~cwb\n14 http://bogor.projects.cis.ksu.edu/\n4\nProgram verification",
    "tion [MP91, MP95, Ros97, Hol90]. The SMV code contained in this chapter\ncan be downloaded from www.cs.bham.ac.uk/research/lics/.\n12 www.dcs.ed.ac.uk/home/cwb\n13 www.cs.sunysb.edu/~cwb\n14 http://bogor.projects.cis.ksu.edu/\n4\nProgram verification\nThe methods of the previous chapter are suitable for verifying systems of\ncommunicating processes, where control is the main issue, but there are no\ncomplex data. We relied on the fact that those (abstracted) systems are\nin a ﬁnite state. These assumptions are not valid for sequential programs\nrunning on a single processor, the topic of this chapter. In those cases, the\nprograms may manipulate non-trivial data and – once we admit variables of\ntype integer, list, or tree – we are in the domain of machines with inﬁnite\nstate space.\nIn terms of the classiﬁcation of veriﬁcation methods given at the beginning\nof the last chapter, the methods of this chapter are\nProof-based. We do not exhaustively check every state that the system\ncan get in to, as one does with model checking; this would be impossi-\nble, given that program variables can have inﬁnitely many interacting\nvalues. Instead, we construct a proof that the system satisﬁes the prop-\nerty at hand, using a proof calculus. This is analogous to the situation\nin Chapter 2, where using a suitable proof calculus avoided the prob-\nlem of having to check inﬁnitely many models of a set of predicate logic\nformulas in order to establish the validity of a sequent.\nSemi-automatic. Although many of the steps involved in proving that\na program satisﬁes its speciﬁcation are mechanical, there are some steps\nthat involve some intelligence and that cannot be carried out algorith-\nmically by a computer. As we will see, there are often good heuristics\nto help the programmer complete these tasks. This contrasts with the\nsituation of the last chapter, which was fully automatic.\nProperty-oriented. Just like in the previous chapter, we verify proper-",
    "mically by a computer. As we will see, there are often good heuristics\nto help the programmer complete these tasks. This contrasts with the\nsituation of the last chapter, which was fully automatic.\nProperty-oriented. Just like in the previous chapter, we verify proper-\nties of a program rather than a full speciﬁcation of its behaviour.\n256\n4.1 Why should we specify and verify code?\n257\nApplication domain. The domain of application in this chapter is se-\nquential transformational programs. ‘Sequential’ means that we assume\nthe program runs on a single processor and that there are no concur-\nrency issues. ‘Transformational’ means that the program takes an input\nand, after some computation, is expected to terminate with an output.\nFor example, methods of objects in Java are often programmed in this\nstyle. This contrasts with the previous chapter which focuses on reactive\nsystems that are not intended to terminate and that react continually\nwith their environment.\nPre/post-development. The techniques of this chapter should be used\nduring the coding process for small fragments of program that perform\nan identiﬁable (and hence, speciﬁable) task and hence should be used\nduring the development process in order to avoid functional bugs.\n4.1 Why should we specify and verify code?\nThe task of specifying and verifying code is often perceived as an unwel-\ncome addition to the programmer’s job and a dispensable one. Arguments\nin favour of veriﬁcation include the following:\nr Documentation: The speciﬁcation of a program is an important component\nin its documentation and the process of documenting a program may raise or\nresolve important issues. The logical structure of the formal speciﬁcation, written\nas a formula in a suitable logic, typically serves as a guiding principle in trying\nto write an implementation in which it holds.\nr Time-to-market: Debugging big systems during the testing phase is costly and",
    "resolve important issues. The logical structure of the formal speciﬁcation, written\nas a formula in a suitable logic, typically serves as a guiding principle in trying\nto write an implementation in which it holds.\nr Time-to-market: Debugging big systems during the testing phase is costly and\ntime-consuming and local ‘ﬁxes’ often introduce new bugs at other places. Ex-\nperience has shown that verifying programs with respect to formal speciﬁcations\ncan signiﬁcantly cut down the duration of software development and maintenance\nby eliminating most errors in the planning phase and helping in the clariﬁcation\nof the roles and structural aspects of system components.\nr Refactoring: Properly speciﬁed and veriﬁed software is easier to reuse, since\nwe have a clear speciﬁcation of what it is meant to do.\nr Certiﬁcation audits: Safety-critical computer systems – such as the control\nof cooling systems in nuclear power stations, or cockpits of modern aircrafts –\ndemand that their software be speciﬁed and veriﬁed with as much rigour and\nformality as possible. Other programs may be commercially critical, such as ac-\ncountancy software used by banks, and they should be delivered with a warranty:\na guarantee for correct performance within proper use. The proof that a program\nmeets its speciﬁcations is indeed such a warranty.\n258\n4 Program verification\nThe degree to which the software industry accepts the beneﬁts of proper\nveriﬁcation of code depends on the perceived extra cost of producing it and\nthe perceived beneﬁts of having it. As veriﬁcation technology improves, the\ncosts are declining; and as the complexity of software and the extent to which\nsociety depends on it increase, the beneﬁts are becoming more important.\nThus, we can expect that the importance of veriﬁcation to industry will\ncontinue to increase over the next decades. Microsoft’s emergent technology\nA# combines program veriﬁcation, testing, and model-checking techniques",
    "society depends on it increase, the beneﬁts are becoming more important.\nThus, we can expect that the importance of veriﬁcation to industry will\ncontinue to increase over the next decades. Microsoft’s emergent technology\nA# combines program veriﬁcation, testing, and model-checking techniques\nin an integrated in-house development environment.\nCurrently, many companies struggle with a legacy of ancient code with-\nout proper documentation which has to be adapted to new hardware and\nnetwork environments, as well as ever-changing requirements. Often, the\noriginal programmers who might still remember what certain pieces of code\nare for have moved, or died. Software systems now often have a longer\nlife-expectancy than humans, which necessitates a durable, transparent and\nportable design and implementation process; the year-2000 problem was just\none such example. Software veriﬁcation provides some of this.\n4.2 A framework for software verification\nSuppose you are working for a software company and your task is to write\nprograms which are meant to solve sophisticated problems, or computations.\nTypically, such a project involves an outside customer – a utility company,\nfor example – who has written up an informal description, in plain English,\nof the real-world task that is at hand. In this case, it could be the devel-\nopment and maintenance of a database of electricity accounts with all the\npossible applications of that – automated billing, customer service etc. Since\nthe informality of such descriptions may cause ambiguities which eventually\ncould result in serious and expensive design ﬂaws, it is desirable to condense\nall the requirements of such a project into formal speciﬁcations. These formal\nspeciﬁcations are usually symbolic encodings of real-world constraints into\nsome sort of logic. Thus, a framework for producing the software could be:\nr Convert the informal description R of requirements for an application domain",
    "speciﬁcations are usually symbolic encodings of real-world constraints into\nsome sort of logic. Thus, a framework for producing the software could be:\nr Convert the informal description R of requirements for an application domain\ninto an ‘equivalent’ formula φR of some symbolic logic;\nr Write a program P which is meant to realise φR in the programming environment\nsupplied by your company, or wanted by the particular customer;\nr Prove that the program P satisﬁes the formula φR.\nThis scheme is quite crude – for example, constraints may be actual design\ndecisions for interfaces and data types, or the speciﬁcation may ‘evolve’\n4.2 A framework for software verification\n259\nand may partly be ‘unknown’ in big projects – but it serves well as a ﬁrst\napproximation to trying to deﬁne good programming methodology. Several\nvariations of such a sequence of activities are conceivable. For example,\nyou, as a programmer, might have been given only the formula φR, so you\nmight have little if any insight into the real-world problem which you are\nsupposed to solve. Technically, this poses no problem, but often it is handy\nto have both informal and formal descriptions available. Moreover, crafting\nthe informal requirements R is often a mutual process between the client\nand the programmer, whereby the attempt at formalising R can uncover\nambiguities or undesired consequences and hence lead to revisions of R.\nThis ‘going back and forth’ between the realms of informal and formal\nspeciﬁcations is necessary since it is impossible to ‘verify’ whether an infor-\nmal requirement R is equivalent to a formal description φR. The meaning\nof R as a piece of natural language is grounded in common sense and gen-\neral knowledge about the real-world domain and often based on heuristics\nor quantitative reasoning. The meaning of a logic formula φR, on the other\nhand, is deﬁned in a precise mathematical, qualitative and compositional",
    "of R as a piece of natural language is grounded in common sense and gen-\neral knowledge about the real-world domain and often based on heuristics\nor quantitative reasoning. The meaning of a logic formula φR, on the other\nhand, is deﬁned in a precise mathematical, qualitative and compositional\nway by structural induction on the parse tree of φR – the ﬁrst three chap-\nters contain examples of this.\nThus, the process of ﬁnding a suitable formalisation φR of R requires\nthe utmost care; otherwise it is always possible that φR speciﬁes behaviour\nwhich is diﬀerent from the one described in R. To make matters worse, the\nrequirements R are often inconsistent; customers usually have a fairly vague\nconception of what exactly a program should do for them. Thus, producing\na clear and coherent description R of the requirements for an application do-\nmain is already a crucial step in successful programming; this phase ideally is\nundertaken by customers and project managers around a table, or in a video\nconference, talking to each other. We address this ﬁrst item only implicitly\nin this text, but you should certainly be aware of its importance in practice.\nThe next phase of the software development framework involves construct-\ning the program P and after that the last task is to verify that P satisﬁes φR.\nHere again, our framework is oversimplifying what goes on in practice, since\noften proving that P satisﬁes its speciﬁcation φR goes hand-in-hand with\ninventing a suitable P. This correspondence between proving and program-\nming can be stated quite precisely, but that is beyond the scope of this book.\n4.2.1 A core programming language\nThe programming language which we set out to study here is the typical\ncore language of most imperative programming languages. Modulo trivial\n260\n4 Program verification\nsyntactic variations, it is a subset of Pascal, C, C++ and Java. Our lan-\nguage consists of assignments to integer- and boolean-valued variables, if-",
    "core language of most imperative programming languages. Modulo trivial\n260\n4 Program verification\nsyntactic variations, it is a subset of Pascal, C, C++ and Java. Our lan-\nguage consists of assignments to integer- and boolean-valued variables, if-\nstatements, while-statements and sequential compositions. Everything that\ncan be computed by large languages like C and Java can also be computed\nby our language, though perhaps not as conveniently, because it does not\nhave any objects, procedures, threads or recursive data structures. While\nthis makes it seem unrealistic compared with fully blown commercial lan-\nguages, it allows us to focus our discussion on the process of formal program\nveriﬁcation. The features missing from our language could be implemented\non top of it; that is the justiﬁcation for saying that they do not add to the\npower of the language, but only to the convenience of using it. Verifying\nprograms using those features would require non-trivial extensions of the\nproof calculus we present here. In particular, dynamic scoping of variables\npresents hard problems for program-veriﬁcation methods, but this is beyond\nthe scope of this book.\nOur core language has three syntactic domains: integer expressions,\nboolean expressions and commands – the latter we consider to be our\nprograms. Integer expressions are built in the familiar way from variables\nx, y, z, . . . , numerals 0, 1, 2, . . . , −1, −2, . . . and basic operations like addition\n(+) and multiplication (∗). For example,\n5\nx\n4 + (x −3)\nx + (x ∗(y −(5 + z)))\nare all valid integer expressions. Our grammar for generating integer expres-\nsions is\nE ::=\nn | x | (−E) | (E + E) | (E −E) | (E ∗E)\n(4.1)\nwhere n is any numeral in {. . . , −2, −1, 0, 1, 2, . . . } and x is any variable.\nNote that we write multiplication in ‘mathematics’ as 2 · 3, whereas our\ncore language writes 2 ∗3 instead.\nConvention 4.1 In the grammar above, negation −binds more tightly",
    "(4.1)\nwhere n is any numeral in {. . . , −2, −1, 0, 1, 2, . . . } and x is any variable.\nNote that we write multiplication in ‘mathematics’ as 2 · 3, whereas our\ncore language writes 2 ∗3 instead.\nConvention 4.1 In the grammar above, negation −binds more tightly\nthan multiplication ∗, which binds more tightly than subtraction −and\naddition +.\nSince if-statements and while-statements contain conditions in them, we\nalso need a syntactic domain B of boolean expressions. The grammar in\n4.2 A framework for software verification\n261\nBackus Naur form\nB ::= true | false | (!B) | (B & B)\n| (B || B) | (E < E)\n(4.2)\nuses ! for the negation, & for conjunction and || for disjunction of\nboolean expressions. This grammar may be freely expanded by operators\nwhich are deﬁnable in terms of the above. For example, the test for equal-\nity1 E1 == E2 may be expressed via !(E1 < E2) & !(E2 < E1). We gener-\nally make use of shorthand notation whenever this is convenient. We also\nwrite (E1 != E2) to abbreviate !(E1 == E2). We will also assume the usual\nbinding priorities for logical operators stated in Convention 1.3 on page 5.\nBoolean expressions are built on top of integer expressions since the last\nclause of (4.2) mentions integer expressions.\nHaving integer and boolean expressions at hand, we can now deﬁne the\nsyntactic domain of commands. Since commands are built from simpler com-\nmands using assignments and the control structures, you may think of com-\nmands as the actual programs. We choose as grammar for commands\nC\n::=\nx = E | C; C | if B {C} else {C} | while B {C}\n(4.3)\nwhere the braces { and } are to mark the extent of the blocks of code in the\nif-statement and the while-statement, as in languages such as C and Java.\nThey can be omitted if the blocks consist of a single statement. The intuitive\nmeaning of the programming constructs is the following:\n1.\nThe atomic command x = E is the usual assignment statement; it evaluates",
    "if-statement and the while-statement, as in languages such as C and Java.\nThey can be omitted if the blocks consist of a single statement. The intuitive\nmeaning of the programming constructs is the following:\n1.\nThe atomic command x = E is the usual assignment statement; it evaluates\nthe integer expression E in the current state of the store and then overwrites\nthe current value stored in x with the result of that evaluation.\n2.\nThe compound command C1; C2 is the sequential composition of the commands\nC1 and C2. It begins by executing C1 in the current state of the store. If that\nexecution terminates, then it executes C2 in the storage state resulting from the\nexecution of C1. Otherwise – if the execution of C1 does not terminate – the\nrun of C1; C2 also does not terminate. Sequential composition is an example of\na control structure since it implements a certain policy of ﬂow of control in a\ncomputation.\n1 In common with languages like C and Java, we use a single equals sign = to mean assignment\nand a double sign == to mean equality. Earlier languages like Pascal used := for assignment and\nsimple = for equality; it is a great pity that C and its successors did not keep this convention.\nThe reason that = is a bad symbol for assignment is that assignment is not symmetric: if we\ninterpret x = y as the assignment, then x becomes y which is not the same thing as y becoming\nx; yet, x = y and y = x are the same thing if we mean equality. The two dots in := helped\nremind the reader that this is an asymmetric assignment operation rather than a symmetric\nassertion of equality. However, the notation = for assignment is now commonplace, so we will\nuse it.\n262\n4 Program verification\n3.\nAnother control structure is if B {C1} else {C2}. It ﬁrst evaluates the boolean\nexpression B in the current state of the store; if that result is true, then C1 is\nexecuted; if B evaluated to false, then C2 is executed.\n4.\nThe third control construct while B {C} allows us to write statements which",
    "Another control structure is if B {C1} else {C2}. It ﬁrst evaluates the boolean\nexpression B in the current state of the store; if that result is true, then C1 is\nexecuted; if B evaluated to false, then C2 is executed.\n4.\nThe third control construct while B {C} allows us to write statements which\nare executed repeatedly. Its meaning is that:\na the boolean expression B is evaluated in the current state of the store;\nb if B evaluates to false, then the command terminates,\nc otherwise, the command C will be executed. If that execution terminates,\nthen we resume at step (a) with a re-evaluation of B as the updated state\nof the store may have changed its value.\nThe point of the while-statement is that it repeatedly executes the command\nC as long as B evaluates to true. If B never becomes false, or if one of the\nexecutions of C does not terminate, then the while-statement will not termi-\nnate. While-statements are the only real source of non-termination in our core\nprogramming language.\nExample 4.2 The factorial n! of a natural number n is deﬁned induc-\ntively by\n0!\ndef\n= 1\n(4.4)\n(n + 1)!\ndef\n= (n + 1) · n!\nFor example, unwinding this deﬁnition for n being 4, we get 4!\ndef\n= 4 · 3! =\n· · · = 4 · 3 · 2 · 1 · 0! = 24. The following program Fac1:\ny = 1;\nz = 0;\nwhile (z != x) {\nz = z + 1;\ny = y * z;\n}\nis intended to compute the factorial2 of x and to store the result in y. We\nwill prove that Fac1 really does this later in the chapter.\n4.2.2 Hoare triples\nProgram fragments generated by (4.3) commence running in a ‘state’ of the\nmachine. After doing some computation, they might terminate. If they do,\nthen the result is another, usually diﬀerent, state. Since our programming\n2 Please note the diﬀerence between the formula x! = y, saying that the factorial of x is equal to\ny, and the piece of code x != y which says that x is not equal to y.\n4.2 A framework for software verification\n263\nlanguage does not have any procedures or local variables, the ‘state’ of the",
    "2 Please note the diﬀerence between the formula x! = y, saying that the factorial of x is equal to\ny, and the piece of code x != y which says that x is not equal to y.\n4.2 A framework for software verification\n263\nlanguage does not have any procedures or local variables, the ‘state’ of the\nmachine can be represented simply as a vector of values of all the variables\nused in the program.\nWhat syntax should we use for φR, the formal speciﬁcations of require-\nments for such programs? Because we are interested in the output of the\nprogram, the language should allow us to talk about the variables in the\nstate after the program has executed, using operators like = to express\nequality and < for less than. You should be aware of the overloading of\n=. In code, it represents an assignment instruction; in logical formulas, it\nstands for equality, which we write == within program code.\nFor example, if the informal requirement R says that we should\nCompute a number y whose square is less than the input x.\nthen an appropriate speciﬁcation may be y · y < x. But what if the input x\nis −4? There is no number whose square is less than a negative number, so\nit is not possible to write the program in a way that it will work with all\npossible inputs. If we go back to the client and say this, he or she is quite\nlikely to respond by saying that the requirement is only that the program\nwork for positive numbers; i.e., he or she revises the informal requirement\nso that it now says\nIf the input x is a positive number, compute a number whose square\nis less than x.\nThis means we need to be able to talk not just about the state after the\nprogram executes, but also about the state before it executes. The assertions\nwe make will therefore be triples, typically looking like\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n(4.5)\nwhich (roughly) means:\nIf the program P is run in a state that satisﬁes φ, then the state\nresulting from P’s execution will satisfy ψ.\nThe speciﬁcation of the program P, to calculate a number whose square is",
    "we make will therefore be triples, typically looking like\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n(4.5)\nwhich (roughly) means:\nIf the program P is run in a state that satisﬁes φ, then the state\nresulting from P’s execution will satisfy ψ.\nThe speciﬁcation of the program P, to calculate a number whose square is\nless than x, now looks like this:\n\u0001\nx > 0\n\u0002\nP\n\u0001\ny · y < x\n\u0002\n.\n(4.6)\nIt means that, if we run P in a state such that x > 0, then the resulting\nstate will be such that y · y < x. It does not tell us what happens if we run\nP in a state in which x ≤0, the client required nothing for non-positive\nvalues of x. Thus, the programmer is free to do what he or she wants in that\ncase. A program which produces ‘garbage’ in the case that x ≤0 satisﬁes\nthe speciﬁcation, as long as it works correctly for x > 0.\n264\n4 Program verification\nLet us make these notions more precise.\nDeﬁnition 4.3 1.\nThe form\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nof our speciﬁcation is called a Hoare\ntriple, after the computer scientist C. A. R. Hoare.\n2.\nIn (4.5), the formula φ is called the precondition of P and ψ is called the\npostcondition.\n3.\nA store or state of core programs is a function l that assigns to each variable\nx an integer l(x).\n4.\nFor a formula φ of predicate logic with function symbols −(unary), +, −, and ∗\n(binary); and a binary predicate symbols < and =, we say that a state l satisﬁes\nφ or l is a φ-state – written l ⊨φ – iﬀM ⊨l φ from page 128 holds, where l\nis viewed as a look-up table and the model M has as set A all integers and\ninterprets the function and predicate symbols in their standard manner.\n5.\nFor Hoare triples in (4.5), we demand that quantiﬁers in φ and ψ only bind\nvariables that do not occur in the program P.\nExample 4.4 For any state l for which l(x) = −2, l(y) = 5, and l(z) = −1,\nthe relation\n1.\nl ⊨¬(x + y < z) holds since x + y evaluates to −2 + 5 = 3, z evaluates to l(z) =\n−1, and 3 is not strictly less than −1;\n2.\nl ⊨y −x ∗z < z does not hold, since the lefthand expression evaluates to 5 −",
    "Example 4.4 For any state l for which l(x) = −2, l(y) = 5, and l(z) = −1,\nthe relation\n1.\nl ⊨¬(x + y < z) holds since x + y evaluates to −2 + 5 = 3, z evaluates to l(z) =\n−1, and 3 is not strictly less than −1;\n2.\nl ⊨y −x ∗z < z does not hold, since the lefthand expression evaluates to 5 −\n(−2) · (−1) = 3 which is not strictly less than l(z) = −1;\n3.\nl ⊨∀u (y < u →y ∗z < u ∗z) does not hold; for u being 7, l ⊨y < u holds, but\nl ⊨y ∗z < u ∗z does not.\nOften, we do not want to put any constraints on the initial state; we\nsimply wish to say that, no matter what state we start the program in, the\nresulting state should satisfy ψ. In that case the precondition can be set to\n⊤, which is – as in previous chapters – a formula which is true in any state.\nNote that the triple in (4.6) does not specify a unique program P, or\na unique behaviour. For example, the program which simply does y = 0;\nsatisﬁes the speciﬁcation – since 0 · 0 is less than any positive number – as\ndoes the program\ny = 0;\nwhile (y * y < x) {\ny = y + 1;\n}\ny = y - 1;\nThis program ﬁnds the greatest y whose square is less than x; the while-\nstatement overshoots a bit, but then we ﬁx it after the while-statement.3\n3 We could avoid this inelegance by using the repeat construct of exercise 3 on page 299.\n4.2 A framework for software verification\n265\nNote that these two programs have diﬀerent behaviour. For example, if x is\n22, the ﬁrst one will compute y = 0 and the second will render y = 4; but\nboth of them satisfy the speciﬁcation.\nOur agenda, then, is to develop a notion of proof which allows us to\nprove that a program P satisﬁes the speciﬁcation given by a precondition\nφ and a postcondition ψ in (4.5). Recall that we developed proof calculi\nfor propositional and predicate logic where such proofs could be accom-\nplished by investigating the structure of the formula one wanted to prove.\nFor example, for proving an implication φ →ψ one had to assume φ and",
    "φ and a postcondition ψ in (4.5). Recall that we developed proof calculi\nfor propositional and predicate logic where such proofs could be accom-\nplished by investigating the structure of the formula one wanted to prove.\nFor example, for proving an implication φ →ψ one had to assume φ and\nmanage to show ψ; then the proof could be ﬁnished with the proof rule for\nimplies-introduction. The proof calculi which we are about to develop follow\nsimilar lines. Yet, they are diﬀerent from the logics we previously studied\nsince they prove triples which are built from two diﬀerent sorts of things:\nlogical formulas φ and ψ versus a piece of code P. Our proof calculi have to\naddress each of these appropriately. Nonetheless, we retain proof strategies\nwhich are compositional, but now in the structure of P. Note that this is\nan important advantage in the veriﬁcation of big projects, where code is\nbuilt from a multitude of modules such that the correctness of certain parts\nwill depend on the correctness of certain others. Thus, your code might\ncall subroutines which other members of your project are about to code,\nbut you can already check the correctness of your code by assuming that\nthe subroutines meet their own speciﬁcations. We will explore this topic in\nSection 4.5.\n4.2.3 Partial and total correctness\nOur explanation of when the triple\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds was rather informal. In\nparticular, it did not say what we should conclude if P does not terminate.\nIn fact there are two ways of handling this situation. Partial correctness\nmeans that we do not require the program to terminate, whereas in total\ncorrectness we insist upon its termination.\nDeﬁnition 4.5 (Partial correctness) We say that the triple\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis satisﬁed under partial correctness if, for all states which satisfy φ, the\nstate resulting from P’s execution satisﬁes the postcondition ψ, provided\nthat P actually terminates. In this case, the relation ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds.",
    "\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis satisﬁed under partial correctness if, for all states which satisfy φ, the\nstate resulting from P’s execution satisﬁes the postcondition ψ, provided\nthat P actually terminates. In this case, the relation ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds.\nWe call ⊨par the satisfaction relation for partial correctness.\nThus, we insist on ψ being true of the resulting state only if the program P\nhas terminated on an input satisfying φ. Partial correctness is rather a weak\nrequirement, since any program which does not terminate at all satisﬁes its\n266\n4 Program verification\nspeciﬁcation. In particular, the program\nwhile true { x = 0; }\n– which endlessly ‘loops’ and never terminates – satisﬁes all speciﬁcations,\nsince partial correctness only says what must happen if the program termi-\nnates.\nTotal correctness, on the other hand, requires that the program terminates\nin order for it to satisfy a speciﬁcation.\nDeﬁnition 4.6 (Total correctness) We say that the triple\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis\nsatisﬁed under total correctness if, for all states in which P is executed which\nsatisfy the precondition φ, P is guaranteed to terminate and the resulting\nstate satisﬁes the postcondition ψ. In this case, we say that ⊨tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds and call ⊨tot the satisfaction relation of total correctness.\nA program which ‘loops’ forever on all input does not satisfy any spec-\niﬁcation under total correctness. Clearly, total correctness is more useful\nthan partial correctness, so the reader may wonder why partial correctness\nis introduced at all. Proving total correctness usually beneﬁts from prov-\ning partial correctness ﬁrst and then proving termination. So, although our\nprimary interest is in proving total correctness, it often happens that we\nhave to or may wish to split this into separate proofs of partial correctness\nand of termination. Most of this chapter is devoted to the proof of partial\ncorrectness, though we return to the issue of termination in Section 4.4.",
    "have to or may wish to split this into separate proofs of partial correctness\nand of termination. Most of this chapter is devoted to the proof of partial\ncorrectness, though we return to the issue of termination in Section 4.4.\nBefore we delve into the issue of crafting sound and complete proof calculi\nfor partial and total correctness, let us brieﬂy give examples of typical sorts\nof speciﬁcations which we would like to be able to prove.\nExamples 4.7\n1.\nLet Succ be the program\na = x + 1;\nif (a - 1 == 0) {\ny = 1;\n} else {\ny = a;\n}\nThe program Succ satisﬁes the speciﬁcation\n\u0001\n⊤\n\u0002\nSucc\n\u0001\ny = (x + 1)\n\u0002\nunder par-\ntial and total correctness, so if we think of x as input and y as output, then\nSucc computes the successor function. Note that this code is far from optimal.\n4.2 A framework for software verification\n267\nIn fact, it is a rather roundabout way of implementing the successor function.\nDespite this non-optimality, our proof rules need to be able to prove this pro-\ngram behaviour.\n2.\nThe program Fac1 from Example 4.2 terminates only if x is initially non-\nnegative – why? Let us look at what properties of Fac1 we expect to be able to\nprove.\nWe should be able to prove that ⊨tot\n\u0001\nx ≥0\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nholds. It states\nthat, provided x ≥0, Fac1 terminates with the result y = x!. However, the\nstronger statement that ⊨tot\n\u0001\n⊤\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nholds should not be provable,\nbecause Fac1 does not terminate for negative values of x.\nFor partial correctness, both statements ⊨par\n\u0001\nx ≥0\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nand\n⊨par\n\u0001\n⊤\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nshould be provable since they hold.\nDeﬁnition 4.8 1.\nIf the partial correctness of triples\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\ncan be proved\nin the partial-correctness calculus we develop in this chapter, we say that the\nsequent ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid.\n2.\nSimilarly, if it can be proved in the total-correctness calculus to be developed\nin this chapter, we say that the sequent ⊢tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid.\nThus, ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002",
    "in the partial-correctness calculus we develop in this chapter, we say that the\nsequent ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid.\n2.\nSimilarly, if it can be proved in the total-correctness calculus to be developed\nin this chapter, we say that the sequent ⊢tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid.\nThus, ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds if P is partially correct, while the validity of\n⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nmeans that P can be proved to be partially-correct by our\ncalculus. The ﬁrst one means it is actually correct, while the second one\nmeans it is provably correct according to our calculus.\nIf our calculus is any good, then the relation ⊢par should be contained in\n⊨par! More precisely, we will say that our calculus is sound if, whenever it\ntells us something can be proved, that thing is indeed true. Thus, it is sound\nif it doesn’t tell us that false things can be proved. Formally, we write that\n⊢par is sound if\n⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds whenever ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid\nfor all φ, ψ and P; and, similarly, ⊢tot is sound if\n⊨tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds whenever ⊢tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid\nfor all φ, ψ and P. We say that a calculus is complete if it is able to prove\neverything that is true. Formally, ⊢par is complete if\n⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nis valid whenever ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds\nfor all φ, ψ and P; and similarly for ⊢tot being complete.\nIn Chapters 1 and 2, we said that soundness is relatively easy to show,\nsince typically the soundness of individual proof rules can be established\nindependently of the others. Completeness, on the other hand, is harder to\n268\n4 Program verification\nshow since it depends on the entire set of proof rules cooperating together.\nThe same situation holds for the program logic we introduce in this chapter.\nEstablishing its soundness is simply a matter of considering each rule in\nturn – done in exercise 3 on page 303 – whereas establishing its (relative)\ncompleteness is harder and beyond the scope of this book.\n4.2.4 Program variables and logical variables",
    "Establishing its soundness is simply a matter of considering each rule in\nturn – done in exercise 3 on page 303 – whereas establishing its (relative)\ncompleteness is harder and beyond the scope of this book.\n4.2.4 Program variables and logical variables\nThe variables which we have seen so far in the programs that we verify\nare called program variables. They can also appear in the preconditions and\npostconditions of speciﬁcations. Sometimes, in order to formulate speciﬁca-\ntions, we need to use other variables which do not appear in programs.\nExamples 4.9\n1.\nAnother version of the factorial program might have been Fac2:\ny = 1;\nwhile (x != 0) {\ny = y * x;\nx = x - 1;\n}\nUnlike the previous version, it ‘consumes’ the input x. Nevertheless, it cor-\nrectly calculates the factorial of x and stores the value in y; and we would\nlike to express that as a Hoare triple. However, it is not a good idea to write\n\u0001\nx ≥0\n\u0002\nFac2\n\u0001\ny = x!\n\u0002\nbecause, if the program terminates, then x will be 0 and\ny will be the factorial of the initial value of x.\nWe need a way of remembering the initial value of x, to cope with the fact\nthat it is modiﬁed by the program. Logical variables achieve just that: in the\nspeciﬁcation\n\u0001\nx = x0 ∧x ≥0\n\u0002\nFac2\n\u0001\ny = x0!\n\u0002\nthe x0 is a logical variable and\nwe read it as being universally quantiﬁed in the precondition. Therefore, this\nspeciﬁcation reads: for all integers x0, if x equals x0, x ≥0 and we run the\nprogram such that it terminates, then the resulting state will satisfy y equals\nx0!. This works since x0 cannot be modiﬁed by Fac2 as x0 does not occur in\nFac2.\n2.\nConsider the program Sum:\nz = 0;\nwhile (x > 0) {\nz = z + x;\nx = x - 1;\n}\nThis program adds up the ﬁrst x integers and stores the result in z.\nThus,\n\u0001\nx = 3\n\u0002\nSum\n\u0001\nz = 6\n\u0002\n,\n\u0001\nx = 8\n\u0002\nSum\n\u0001\nz = 36\n\u0002\netc. We know from The-\norem 1.31 on page 41 that 1 + 2 + · · · + x = x(x + 1)/2 for all x ≥0, so\n4.3 Proof calculus for partial correctness\n269",
    "z = z + x;\nx = x - 1;\n}\nThis program adds up the ﬁrst x integers and stores the result in z.\nThus,\n\u0001\nx = 3\n\u0002\nSum\n\u0001\nz = 6\n\u0002\n,\n\u0001\nx = 8\n\u0002\nSum\n\u0001\nz = 36\n\u0002\netc. We know from The-\norem 1.31 on page 41 that 1 + 2 + · · · + x = x(x + 1)/2 for all x ≥0, so\n4.3 Proof calculus for partial correctness\n269\nwe would like to express, as a Hoare triple, that the value of z upon\ntermination is x0(x0 + 1)/2 where x0 is the initial value of x. Thus, we write\n\u0001\nx = x0 ∧x ≥0\n\u0002\nSum\n\u0001\nz = x0(x0 + 1)/2\n\u0002\n.\nVariables like x0 in these examples are called logical variables, because they\noccur only in the logical formulas that constitute the precondition and post-\ncondition; they do not occur in the code to be veriﬁed. The state of the\nsystem gives a value to each program variable, but not for the logical vari-\nables. Logical variables take a similar role to the dummy variables of the\nrules for ∀i and ∃e in Chapter 2.\nDeﬁnition 4.10 For a Hoare triple\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n, its set of logical variables\nare those variables that are free in φ or ψ; and don’t occur in P.\n4.3 Proof calculus for partial correctness\nThe proof calculus which we now present goes back to R. Floyd and C.\nA. R. Hoare. In the next subsection, we specify proof rules for each of the\ngrammar clauses for commands. We could go on to use these proof rules\ndirectly, but it turns out to be more convenient to present them in a diﬀerent\nform, suitable for the construction of proofs known as proof tableaux. This\nis what we do in the subsection following the next one.\n4.3.1 Proof rules\nThe proof rules for our calculus are given in Figure 4.1. They should be\ninterpreted as rules that allow us to pass from simple assertions of the form\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nto more complex ones. The rule for assignment is an axiom as\nit has no premises. This allows us to construct some triples out of noth-\ning, to get the proof going. Complete proofs are trees, see page 274 for an\nexample.\nComposition.\nGiven speciﬁcations for the program fragments C1 and C2,\nsay\n\u0001\nφ",
    "\u0002\nto more complex ones. The rule for assignment is an axiom as\nit has no premises. This allows us to construct some triples out of noth-\ning, to get the proof going. Complete proofs are trees, see page 274 for an\nexample.\nComposition.\nGiven speciﬁcations for the program fragments C1 and C2,\nsay\n\u0001\nφ\n\u0002\nC1\n\u0001\nη\n\u0002\nand\n\u0001\nη\n\u0002\nC2\n\u0001\nψ\n\u0002\n,\nwhere the postcondition of C1 is also the precondition of C2, the proof\nrule for sequential composition shown in Figure 4.1 allows us to derive a\nspeciﬁcation for C1; C2, namely\n\u0001\nφ\n\u0002\nC1; C2\n\u0001\nψ\n\u0002\n.\n270\n4 Program verification\n\u0001\nφ\n\u0002\nC1\n\u0001\nη\n\u0002\n\u0001\nη\n\u0002\nC2\n\u0001\nψ\n\u0002\n\u0001\nφ\n\u0002\nC1; C2\n\u0001\nψ\n\u0002\nComposition\n\u0001\nψ[E/x]\n\u0002\nx = E\n\u0001\nψ\n\u0002 Assignment\n\u0001\nφ ∧B\n\u0002\nC1\n\u0001\nψ\n\u0002\n\u0001\nφ ∧¬B\n\u0002\nC2\n\u0001\nψ\n\u0002\n\u0001\nφ\n\u0002\nif B {C1} else {C2}\n\u0001\nψ\n\u0002\nIf-statement\n\u0001\nψ ∧B\n\u0002\nC\n\u0001\nψ\n\u0002\n\u0001\nψ\n\u0002\nwhile B {C}\n\u0001\nψ ∧¬B\n\u0002 Partial-while\n⊢AR φ′ →φ\n\u0001\nφ\n\u0002\nC\n\u0001\nψ\n\u0002\n⊢AR ψ →ψ′\n\u0001\nφ′\u0002\nC\n\u0001\nψ′\u0002\nImplied\nFigure 4.1. Proof rules for partial correctness of Hoare triples.\nThus, if we know that C1 takes φ-states to η-states and C2 takes η-states\nto ψ-states, then running C1 and C2 in that sequence will take φ-states to\nψ-states.\nUsing the proof rules of Figure 4.1 in program veriﬁcation, we have to\nread them bottom-up: e.g. in order to prove\n\u0001\nφ\n\u0002\nC1; C2\n\u0001\nψ\n\u0002\n, we need to ﬁnd\nan appropriate η and prove\n\u0001\nφ\n\u0002\nC1\n\u0001\nη\n\u0002\nand\n\u0001\nη\n\u0002\nC2\n\u0001\nψ\n\u0002\n. If C1; C2 runs on\ninput satisfying φ and we need to show that the store satisﬁes ψ after its\nexecution, then we hope to show this by splitting the problem into two. After\nthe execution of C1, we have a store satisfying η which, considered as input\nfor C2, should result in an output satisfying ψ. We call η a midcondition.\nAssignment.\nThe rule for assignment has no premises and is therefore an\naxiom of our logic. It tells us that, if we wish to show that ψ holds in the state\nafter the assignment x = E, we must show that ψ[E/x] holds before the\nassignment; ψ[E/x] denotes the formula obtained by taking ψ and replacing\nall free occurrences of x with E as deﬁned on page 105. We read the stroke",
    "after the assignment x = E, we must show that ψ[E/x] holds before the\nassignment; ψ[E/x] denotes the formula obtained by taking ψ and replacing\nall free occurrences of x with E as deﬁned on page 105. We read the stroke\nas ‘in place of;’ thus, ψ[E/x] is ψ with E in place of x. Several explanations\nmay be required to understand this rule.\nr At ﬁrst sight, it looks as if the rule has been stated in reverse; one might expect\nthat, if ψ holds in a state in which we perform the assignment x = E, then surely\n4.3 Proof calculus for partial correctness\n271\nψ[E/x] holds in the resulting state, i.e. we just replace x by E. This is wrong. It\nis true that the assignment x = E replaces the value of x in the starting state\nby E, but that does not mean that we replace occurrences of x in a condition on\nthe starting state by E.\nFor example, let ψ be x = 6 and E be 5. Then\n\u0001\nψ\n\u0002\nx = 5\n\u0001\nψ[x/E]\n\u0002\ndoes not\nhold: given a state in which x equals 6, the execution of x = 5 results in a\nstate in which x equals 5. But ψ[x/E] is the formula 5 = 6 which holds in no\nstate.\nThe right way to understand the Assignment rule is to think about what you\nwould have to prove about the initial state in order to prove that ψ holds in\nthe resulting state. Since ψ will – in general – be saying something about the\nvalue of x, whatever it says about that value must have been true of E, since\nin the resulting state the value of x is E. Thus, ψ with E in place of x – which\nsays whatever ψ says about x but applied to E – must be true in the initial\nstate.\nr The axiom\n\u0001\nψ[E/x]\n\u0002\nx = E\n\u0001\nψ\n\u0002\nis best applied backwards than forwards in the\nveriﬁcation process. That is to say, if we know ψ and we wish to ﬁnd φ such\nthat\n\u0001\nφ\n\u0002\nx = E\n\u0001\nψ\n\u0002\n, it is easy: we simply set φ to be ψ[E/x]; but, if we know\nφ and we want to ﬁnd ψ such that\n\u0001\nφ\n\u0002\nx = E\n\u0001\nψ\n\u0002\n, there is no easy way of\ngetting a suitable ψ. This backwards characteristic of the assignment and the",
    "that\n\u0001\nφ\n\u0002\nx = E\n\u0001\nψ\n\u0002\n, it is easy: we simply set φ to be ψ[E/x]; but, if we know\nφ and we want to ﬁnd ψ such that\n\u0001\nφ\n\u0002\nx = E\n\u0001\nψ\n\u0002\n, there is no easy way of\ngetting a suitable ψ. This backwards characteristic of the assignment and the\ncomposition rule will be important when we look at how to construct proofs;\nwe will work from the end of a program to its beginning.\nr If we apply this axiom in this backwards fashion, then it is completely\nmechanical to apply. It just involves doing a substitution. That means we could\nget a computer to do it for us. Unfortunately, that is not true for all the rules;\napplication of the rule for while-statements, for example, requires ingenuity.\nTherefore a computer can at best assist us in performing a proof by carrying\nout the mechanical steps, such as application of the assignment axiom, while\nleaving the steps that involve ingenuity to the programmer.\nr Observe that, in computing ψ[E/x] from ψ, we replace all the free occurrences of\nx in ψ. Note that there cannot be problems caused by bound occurrences, as seen\nin Example 2.9 on page 106, provided that preconditions and postconditions quan-\ntify over logical variables only. For obvious reasons, this is recommended practice.\nExamples 4.11\n1.\nSuppose P is the program x = 2. The following are instances of axiom\nAssignment:\na\n\u0001\n2 = 2\n\u0002\nP\n\u0001\nx = 2\n\u0002\nb\n\u0001\n2 = 4\n\u0002\nP\n\u0001\nx = 4\n\u0002\nc\n\u0001\n2 = y\n\u0002\nP\n\u0001\nx = y\n\u0002\nd\n\u0001\n2 > 0\n\u0002\nP\n\u0001\nx > 0\n\u0002\n.\n272\n4 Program verification\nThese are all correct statements. Reading them backwards, we see that they\nsay:\na If you want to prove x = 2 after the assignment x = 2, then we must be able\nto prove that 2 = 2 before it. Of course, 2 is equal to 2, so proving it shouldn’t\npresent a problem.\nb If you wanted to prove that x = 4 after the assignment, the only way in which\nit would work is if 2 = 4; however, unfortunately it is not. More generally,\n\u0001\n⊥\n\u0002\nx = E\n\u0001\nψ\n\u0002\nholds for any E and ψ – why?\nc If you want to prove x = y after the assignment, you will need to prove that",
    "b If you wanted to prove that x = 4 after the assignment, the only way in which\nit would work is if 2 = 4; however, unfortunately it is not. More generally,\n\u0001\n⊥\n\u0002\nx = E\n\u0001\nψ\n\u0002\nholds for any E and ψ – why?\nc If you want to prove x = y after the assignment, you will need to prove that\n2 = y before it.\nd To prove x > 0, we’d better have 2 > 0 prior to the execution of P.\n2.\nSuppose P is x = x + 1. By choosing various postconditions, we obtain the fol-\nlowing instances of the assignment axiom:\na\n\u0001\nx + 1 = 2\n\u0002\nP\n\u0001\nx = 2\n\u0002\nb\n\u0001\nx + 1 = y\n\u0002\nP\n\u0001\nx = y\n\u0002\nc\n\u0001\nx + 1 + 5 = y\n\u0002\nP\n\u0001\nx + 5 = y\n\u0002\nd\n\u0001\nx + 1 > 0 ∧y > 0\n\u0002\nP\n\u0001\nx > 0 ∧y > 0\n\u0002\n.\nNote that the precondition obtained by performing the substitution can often be\nsimpliﬁed. The proof rule for implications below will allow such simpliﬁcations\nwhich are needed to make preconditions appreciable by human consumers.\nIf-statements.\nThe proof rule for if-statements allows us to prove a triple\nof the form\n\u0001\nφ\n\u0002\nif B {C1} else {C2}\n\u0001\nψ\n\u0002\nby decomposing it into two triples, subgoals corresponding to the cases of\nB evaluating to true and to false. Typically, the precondition φ will not tell\nus anything about the value of the boolean expression B, so we have to\nconsider both cases. If B is true in the state we start in, then C1 is executed\nand hence C1 will have to translate φ states to ψ states; alternatively, if\nB is false, then C2 will be executed and will have to do that job. Thus,\nwe have to prove that\n\u0001\nφ ∧B\n\u0002\nC1\n\u0001\nψ\n\u0002\nand\n\u0001\nφ ∧¬B\n\u0002\nC2\n\u0001\nψ\n\u0002\n. Note that the\npreconditions are augmented by the knowledge that B is true and false,\nrespectively. This additional information is often crucial for completing the\nrespective subproofs.\nWhile-statements.\nThe rule for while-statements given in Figure 4.1 is ar-\nguably the most complicated one. The reason is that the while-statement\nis the most complicated construct in our language. It is the only command\nthat ‘loops,’ i.e. executes the same piece of code several times. Also, unlike",
    "While-statements.\nThe rule for while-statements given in Figure 4.1 is ar-\nguably the most complicated one. The reason is that the while-statement\nis the most complicated construct in our language. It is the only command\nthat ‘loops,’ i.e. executes the same piece of code several times. Also, unlike\nas the for-statement in languages like Java we cannot generally predict how\n4.3 Proof calculus for partial correctness\n273\nmany times while-statements will ‘loop’ around, or even whether they will\nterminate at all.\nThe key ingredient in the proof rule for Partial-while is the ‘invariant’ ψ.\nIn general, the body C of the command while (B) {C} changes the values\nof the variables it manipulates; but the invariant expresses a relationship\nbetween those values which is preserved by any execution of C. In the proof\nrule, ψ expresses this invariant; the rule’s premise,\n\u0001\nψ ∧B\n\u0002\nC\n\u0001\nψ\n\u0002\n, states\nthat, if ψ and B are true before we execute C, and C terminates, then ψ\nwill be true after it. The conclusion of Partial-while states that, no matter\nhow many times the body C is executed, if ψ is true initially and the while-\nstatement terminates, then ψ will be true at the end. Moreover, since the\nwhile-statement has terminated, B will be false.\nImplied.\nOne ﬁnal rule is required in our calculus: the rule Implied of Figure\n4.1. It tells us that, if we have proved\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nand we have a formula φ′\nwhich implies φ and another one ψ′ which is implied by ψ, then we should\nalso be allowed to prove that\n\u0001\nφ′\u0002\nP\n\u0001\nψ′\u0002\n. A sequent ⊢ARφ →φ′ is valid iﬀ\nthere is a proof of φ′ in the natural deduction calculus for predicate logic,\nwhere φ and standard laws of arithmetic – e.g. ∀x (x = x + 0) – are premises.\nNote that the rule Implied allows the precondition to be strengthened (thus,\nwe assume more than we need to), while the postcondition is weakened (i.e.\nwe conclude less than we are entitled to). If we tried to do it the other way",
    "Note that the rule Implied allows the precondition to be strengthened (thus,\nwe assume more than we need to), while the postcondition is weakened (i.e.\nwe conclude less than we are entitled to). If we tried to do it the other way\naround, weakening the precondition or strengthening the postcondition, then\nwe would conclude things which are incorrect – see exercise 9(a) on page 300.\nThe rule Implied acts as a link between program logic and a suitable\nextension of predicate logic. It allows us to import proofs in predicate logic\nenlarged with the basic facts of arithmetic, which are required for reasoning\nabout integer expressions, into the proofs in program logic.\n4.3.2 Proof tableaux\nThe proof rules presented in Figure 4.1 are not in a form which is easy\nto use in examples. To illustrate this point, we present an example of a\nproof in Figure 4.2; it is a proof of the triple\n\u0001\n⊤\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nwhere Fac1\nis the factorial program given in Example 4.2. This proof abbreviates rule\nnames; and drops the bars and names for Assignment as well as sequents\nfor ⊢AR in all applications of the Implied rule. We have not yet presented\nenough information for the reader to complete such a proof on her own,\nbut she can at least use the proof rules in Figure 4.1 to check whether all\nrule instances of that proof are permissible, i.e. match the required pat-\ntern.\n274\n4 Program verification\n\u0001\n1 = 1\u0002\ny = 1\u0001\ny = 1\u0002\ni\n\u0001\n⊤\u0002\ny = 1\u0001\ny = 1\u0002\n\u0001\ny = 1 ∧0 = 0\u0002\nz = 0\u0001\ny = 1 ∧z = 0\u0002\ni\n\u0001\ny = 1\u0002\nz = 0\u0001\ny = 1 ∧z = 0\u0002\nc\n\u0001\n⊤\u0002\ny = 1; z = 0\u0001\ny = 1 ∧z = 0\u0002\n\u0001\ny · (z + 1) = (z + 1)!\u0002\nz = z+1\u0001\ny · z = z!\u0002\ni\n\u0001\ny = z! ∧z ̸= x\u0002\nz = z+1\u0001\ny · z = z!\u0002\n\u0001\ny · z = z!\u0002\ny = y*z\u0001\ny = z!\u0002\nc\n\u0001\ny = z! ∧z ̸= x\u0002\nz = z+1; y = y*z\u0001\ny = z!\u0002\nw\n\u0001\ny = z!\u0002\nwhile (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = z! ∧z = x\u0002\ni\n\u0001\ny = 1 ∧z = 0\u0002\nwhile (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = x!\u0002\nc\n\u0001\n⊤\u0002\ny = 1; z = 0; while (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = x!\u0002\nFigure 4.2. A partial-correctness proof for Fac1 in tree form.",
    "c\n\u0001\ny = z! ∧z ̸= x\u0002\nz = z+1; y = y*z\u0001\ny = z!\u0002\nw\n\u0001\ny = z!\u0002\nwhile (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = z! ∧z = x\u0002\ni\n\u0001\ny = 1 ∧z = 0\u0002\nwhile (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = x!\u0002\nc\n\u0001\n⊤\u0002\ny = 1; z = 0; while (z\n!=\nx) {z = z+1; y = y*z}\u0001\ny = x!\u0002\nFigure 4.2. A partial-correctness proof for Fac1 in tree form.\n4.3 Proof calculus for partial correctness\n275\nIt should be clear that proofs in this form are unwieldy to work with.\nThey will tend to be very wide and a lot of information is copied from one\nline to the next. Proving properties of programs which are longer than Fac1\nwould be very diﬃcult in this style. In Chapters 1, 2 and 5 we abandon\nrepresentation of proofs as trees for similar reasons. The rule for sequential\ncomposition suggests a more convenient way of presenting proofs in pro-\ngram logic, called proof tableaux. We can think of any program of our core\nprogramming language as a sequence\nC1;\nC2;\n·\n·\n·\nCn\nwhere none of the commands Ci is a composition of smaller programs, i.e. all\nof the Ci above are either assignments, if-statements or while-statements. Of\ncourse, we allow the if-statements and while-statements to have embedded\ncompositions.\nLet P stand for the program C1; C2; . . . ; Cn−1; Cn. Suppose that we want\nto show the validity of ⊢par\n\u0001\nφ0\n\u0002\nP\n\u0001\nφn\n\u0002\nfor a precondition φ0 and a postcon-\ndition φn. Then, we may split this problem into smaller ones by trying to\nﬁnd formulas φj (0 < j < n) and prove the validity of ⊢par\n\u0001\nφi\n\u0002\nCi+1\n\u0001\nφi+1\n\u0002\nfor i = 0, 1, . . . , n −1. This suggests that we should design a proof calcu-\nlus which presents a proof of ⊢par\n\u0001\nφ0\n\u0002\nP\n\u0001\nψn\n\u0002\nby interleaving formulas with\ncode as in\n\u0001\nφ0\n\u0002\nC1;\u0001\nφ1\n\u0002\njustification\nC2;\n·\n·\n· \u0001\nφn−1\n\u0002\njustification\nCn;\u0001\nφn\n\u0002\njustification\n276\n4 Program verification\nAgainst each formula, we write a justiﬁcation, whose nature will be clariﬁed\nshortly. Proof tableaux thus consist of the program code interleaved with\nformulas, which we call midconditions, that should hold at the point they\nare written.",
    "Cn;\u0001\nφn\n\u0002\njustification\n276\n4 Program verification\nAgainst each formula, we write a justiﬁcation, whose nature will be clariﬁed\nshortly. Proof tableaux thus consist of the program code interleaved with\nformulas, which we call midconditions, that should hold at the point they\nare written.\nEach of the transitions\n\u0001\nφi\n\u0002\nCi+1\n\u0001\nφi+1\n\u0002\nwill appeal to one of the rules of Figure 4.1, depending on whether Ci+1 is\nan assignment, an if-statement or a while-statement. Note that this notation\nfor proofs makes the proof rule for composition in Figure 4.1 implicit.\nHow should the intermediate formulas φi be found? In principle, it seems\nas though one could start from φ0 and, using C1, obtain φ1 and continue\nworking downwards. However, because the assignment rule works backwards,\nit turns out that it is more convenient to start with φn and work upwards,\nusing Cn to obtain φn−1 etc.\nDeﬁnition 4.12 The process of obtaining φi from Ci+1 and φi+1 is called\ncomputing the weakest precondition of Ci+1, given the postcondition φi+1.\nThat is to say, we are looking for the logically weakest formula whose truth\nat the beginning of the execution of Ci+1 is enough to guarantee φi+14.\nThe construction of a proof tableau for\n\u0001\nφ\n\u0002\nC1; . . . ; Cn\n\u0001\nψ\n\u0002\ntypically con-\nsists of starting with the postcondition ψ and pushing it upwards through\nCn, then Cn−1, . . . , until a formula φ′ emerges at the top. Ideally, the formula\nφ′ represents the weakest precondition which guarantees that the ψ will hold\nif the composed program C1; C2; . . . ; Cn−1; Cn is executed and terminates.\nThe weakest precondition φ′ is then checked to see whether it follows from\nthe given precondition φ. Thus, we appeal to the Implied rule of Figure 4.1.\nBefore a discussion of how to ﬁnd invariants for while-statement, we now\nlook at the assignment and the if-statement to see how the weakest precon-\ndition is calculated for each one.\nAssignment.\nThe assignment axiom is easily adapted to work for proof",
    "Before a discussion of how to ﬁnd invariants for while-statement, we now\nlook at the assignment and the if-statement to see how the weakest precon-\ndition is calculated for each one.\nAssignment.\nThe assignment axiom is easily adapted to work for proof\ntableaux. We write it thus:\n4 φ is weaker than ψ means that φ is implied by ψ in predicate logic enlarged with the basic\nfacts about arithmetic: the sequent ⊢AR ψ →φ is valid. We want the weakest formula, because\nwe want to impose as few constraints as possible on the preceding code. In some cases, espe-\ncially those involving while-statements, it might not be possible to extract the logically weakest\nformula. We just need one which is suﬃciently weak to allow us to complete the proof at hand.\n4.3 Proof calculus for partial correctness\n277\n\u0001\nψ[E/x]\n\u0002\nx = E\n\u0001\nψ\n\u0002\nAssignment\nThe justiﬁcation is written against the ψ, since, once the proof has been con-\nstructed, we want to read it in a forwards direction. The construction itself\nproceeds in a backwards direction, because that is the way the assignment\naxiom facilitates.\nImplied.\nIn tableau form, the Implied rule allows us to write one formula φ2\ndirectly underneath another one φ1 with no code in between, provided that\nφ1 implies φ2 in that the sequent ⊢AR φ1 →φ2 is valid. Thus, the Implied\nrule acts as an interface between predicate logic with arithmetic and program\nlogic. This is a surprising and crucial insight. Our proof calculus for partial\ncorrectness is a hybrid system which interfaces with another proof calculus\nvia the Implied proof rule only.\nWhen we appeal to the Implied rule, we will usually not explicitly write\nout the proof of the implication in predicate logic, for this chapter focuses\non the program logic. Mostly, the implications we typically encounter will\nbe easy to verify.\nThe Implied rule is often used to simplify formulas that are generated by\napplications of the other rules. It is also used when the weakest precondition",
    "on the program logic. Mostly, the implications we typically encounter will\nbe easy to verify.\nThe Implied rule is often used to simplify formulas that are generated by\napplications of the other rules. It is also used when the weakest precondition\nφ′ emerges by pushing the postcondition upwards through the whole pro-\ngram. We use the Implied rule to show that the given precondition implies\nthe weakest precondition. Let’s look at some examples of this.\nExamples 4.13\n1.\nWe show that ⊢par\n\u0001\ny = 5\n\u0002\nx = y + 1\n\u0001\nx = 6\n\u0002\nis valid:\n\u0001\ny = 5\n\u0002\n\u0001\ny + 1 = 6\n\u0002\nImplied\nx = y + 1\n\u0001\nx = 6\n\u0002\nAssignment\nThe proof is constructed from the bottom upwards. We start with\n\u0001\nx = 6\n\u0002\nand, using the assignment axiom, we push it upwards through x = y + 1. This\nmeans substituting y + 1 for all occurrences of x, resulting in\n\u0001\ny + 1 = 6\n\u0002\n. Now,\nwe compare this with the given precondition\n\u0001\ny = 5\n\u0002\n. The given precondition\nand the arithmetic fact 5 + 1 = 6 imply it, so we have ﬁnished the proof.\n278\n4 Program verification\nAlthough the proof is constructed bottom-up, its justiﬁcations make sense\nwhen read top-down: the second line is implied by the ﬁrst and the fourth\nfollows from the second by the intervening assignment.\n2.\nWe prove the validity of ⊢par\n\u0001\ny < 3\n\u0002\ny = y + 1\n\u0001\ny < 4\n\u0002\n:\n\u0001\ny < 3\n\u0002\n\u0001\ny + 1 < 4\n\u0002\nImplied\ny = y + 1;\n\u0001\ny < 4\n\u0002\nAssignment\nNotice that Implied always refers to the immediately preceding line. As already\nremarked, proofs in program logic generally combine two logical levels: the ﬁrst\nlevel is directly concerned with proof rules for programming constructs such as\nthe assignment statement; the second level is ordinary entailment familiar to\nus from Chapters 1 and 2 plus facts from arithmetic – here that y < 3 implies\ny + 1 < 3 + 1 = 4.\nWe may use ordinary logical and arithmetic implications to change a certain\ncondition φ to any condition φ′ which is implied by φ for reasons which have\nnothing to do with the given code. In the example above, φ was y < 3 and the",
    "y + 1 < 3 + 1 = 4.\nWe may use ordinary logical and arithmetic implications to change a certain\ncondition φ to any condition φ′ which is implied by φ for reasons which have\nnothing to do with the given code. In the example above, φ was y < 3 and the\nimplied formula φ′ was then y + 1 < 4. The validity of ⊢AR (y < 3) →(y + 1 <\n4) is rooted in general facts about integers and the relation < deﬁned on them.\nCompletely formal proofs would require separate proofs attached to all instances\nof the rule Implied. As already said, we won’t do that here as this chapter focuses\non aspects of proofs which deal directly with code.\n3.\nFor the sequential composition of assignment statements\nz = x;\nz = z + y;\nu = z;\nour goal is to show that u stores the sum of x and y after this sequence of\nassignments terminates. Let us write P for the code above. Thus, we mean to\nprove ⊢par\n\u0001\n⊤\n\u0002\nP\n\u0001\nu = x + y\n\u0002\n.\nWe construct the proof by starting with the postcondition u = x + y and\npushing it up through the assignments, in reverse order, using the assignment\nrule.\n– Pushing it up through u = z involves replacing all occurrences of u by z,\nresulting in z = x + y. We thus have the proof fragment\n\u0001\nz = x + y\n\u0002\nu = z;\n\u0001\nu = x + y\n\u0002\nAssignment\n– Pushing z = x + y upwards through z = z + y involves replacing z by z + y,\nresulting in z + y = x + y.\n4.3 Proof calculus for partial correctness\n279\n– Pushing that upwards through z = x involves replacing z by x, resulting in\nx + y = x + y. The proof fragment now looks like this:\n\u0001\nx + y = x + y\n\u0002\nz = x;\n\u0001\nz + y = x + y\n\u0002\nAssignment\nz = z + y;\n\u0001\nz = x + y\n\u0002\nAssignment\nu = z;\n\u0001\nu = x + y\n\u0002\nAssignment\nThe weakest precondition that thus emerges is x + y = x + y; we have to check\nthat this follows from the given precondition ⊤. This means checking that any\nstate that satisﬁes ⊤also satisﬁes x + y = x + y. Well, ⊤is satisﬁed in all states,\nbut so is x + y = x + y, so the sequent ⊢AR ⊤→(x + y = x + y) is valid.\nThe ﬁnal completed proof therefore looks like this:\n\u0001",
    "that this follows from the given precondition ⊤. This means checking that any\nstate that satisﬁes ⊤also satisﬁes x + y = x + y. Well, ⊤is satisﬁed in all states,\nbut so is x + y = x + y, so the sequent ⊢AR ⊤→(x + y = x + y) is valid.\nThe ﬁnal completed proof therefore looks like this:\n\u0001\n⊤\n\u0002\n\u0001\nx + y = x + y\n\u0002\nImplied\nz = x;\n\u0001\nz + y = x + y\n\u0002\nAssignment\nz = z + y;\n\u0001\nz = x + y\n\u0002\nAssignment\nu = z;\n\u0001\nu = x + y\n\u0002\nAssignment\nand we can now read it from the top down.\nThe application of the axiom Assignment requires some care. We describe\ntwo pitfalls which the unwary may fall into, if the rule is not applied correctly.\nr Consider the example ‘proof’\n\u0001\nx + 1 = x + 1\n\u0002\nx = x + 1;\n\u0001\nx = x + 1\n\u0002\nAssignment\nwhich uses the rule for assignment incorrectly. Pattern matching with the assign-\nment axiom means that ψ has to be x = x + 1, the expression E is x + 1 and\nψ[E/x] is x + 1 = x + 1. However, ψ[E/x] is obtained by replacing all occur-\nrences of x in ψ by E, thus, ψ[E/x] would have to be equal to x + 1 = x + 1 + 1.\nTherefore, the corrected proof\n280\n4 Program verification\n\u0001\nx + 1 = x + 1 + 1\n\u0002\nx = x + 1;\n\u0001\nx = x + 1\n\u0002\nAssignment\nshows that ⊢par\n\u0001\nx + 1 = x + 1 + 1\n\u0002\nx = x + 1\n\u0001\nx = x + 1\n\u0002\nis valid.\nAs an aside, this corrected proof is not very useful. The triple says that, if\nx + 1 = (x + 1) + 1 holds in a state and the assignment x = x + 1 is executed\nand terminates, then the resulting state satisﬁes x = x + 1; but, since the precon-\ndition x + 1 = x + 1 + 1 can never be true, this triple tells us nothing informative\nabout the assignment.\nr Another way of using the proof rule for assignment incorrectly is by allowing ad-\nditional assignments to happen in between ψ[E/x] and x = E, as in the ‘proof’\n\u0001\nx + 2 = y + 1\n\u0002\ny = y + 1000001;\nx = x + 2;\n\u0001\nx = y + 1\n\u0002\nAssignment\nThis is not a correct application of the assignment rule, since an additional\nassignment happens in line 2 right before the actual assignment to which the",
    "\u0001\nx + 2 = y + 1\n\u0002\ny = y + 1000001;\nx = x + 2;\n\u0001\nx = y + 1\n\u0002\nAssignment\nThis is not a correct application of the assignment rule, since an additional\nassignment happens in line 2 right before the actual assignment to which the\ninference in line 4 applies. This additional assignment makes this reasoning un-\nsound: line 2 overwrites the current value in y to which the equation in line 1\nis referring. Clearly, x + 2 = y + 1 won’t be true any longer. Therefore, we are\nallowed to use the proof rule for assignment only if there is no additional code\nbetween the precondition ψ[E/x] and the assignment x = E.\nIf-statements.\nWe now consider how to push a postcondition upwards\nthrough an if-statement. Suppose we are given a condition ψ and a pro-\ngram fragment if (B) {C1} else {C2}. We wish to calculate the weakest\nφ such that\n\u0001\nφ\n\u0002\nif (B) {C1} else {C2}\n\u0001\nψ\n\u0002\n.\nThis φ may be calculated as follows.\n1.\nPush ψ upwards through C1; let’s call the result φ1. (Note that, since C1 may\nbe a sequence of other commands, this will involve appealing to other rules. If\nC1 contains another if-statement, then this step will involve a ‘recursive call’\nto the rule for if-statements.)\n2.\nSimilarly, push ψ upwards through C2; call the result φ2.\n3.\nSet φ to be (B →φ1) ∧(¬B →φ2).\nExample 4.14 Let us see this proof rule at work on the non-optimal code\nfor Succ given earlier in the chapter. Here is the code again:\n4.3 Proof calculus for partial correctness\n281\na = x + 1;\nif (a - 1 == 0)\n{\ny = 1;\n} else {\ny = a;\n}\nWe want to show that ⊢par\n\u0001\n⊤\n\u0002\nSucc\n\u0001\ny = x+1\n\u0002\nis valid. Note that this\nprogram is the sequential composition of an assignment and an if-statement.\nThus, we need to obtain a suitable midcondition to put between the if-\nstatement and the assignment.\nWe push the postcondition y = x + 1 upwards through the two branches\nof the if-statement, obtaining\nr φ1 is 1 = x + 1;\nr φ2 is a = x + 1;\nand obtain the midcondition (a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →",
    "statement and the assignment.\nWe push the postcondition y = x + 1 upwards through the two branches\nof the if-statement, obtaining\nr φ1 is 1 = x + 1;\nr φ2 is a = x + 1;\nand obtain the midcondition (a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →\na = x + 1) by appealing to a slightly diﬀerent version of the rule\nIf-statement: \u0001\nφ1\n\u0002\nC1\n\u0001\nψ\n\u0002\n\u0001\nφ2\n\u0002\nC2\n\u0001\nψ\n\u0002\n\u0001\n(B →φ1) ∧(¬B →φ2)\n\u0002\nif B {C1} else {C2}\n\u0001\nψ\n\u0002 If-Statement\n(4.7)\nHowever, this rule can be derived using the proof rules discussed so far; see\nexercise 9(c) on page 301. The partial proof now looks like this:\n(⊤)\n(?)\n?\na = x + 1;\n((a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →a = x + 1))\n?\nif (a - 1 == 0) {\n(1 = x + 1)\nIf-Statement\ny = 1;\n(y = x + 1)\nAssignment\n} else {\n(a = x + 1)\nIf-Statement\ny = a;\n(y = x + 1)\nAssignment\n}\n(y = x + 1)\nIf-Statement\nContinuing this example, we push the long formula above the if-statement\nthrough the assignment, to obtain\n(x + 1 −1 = 0 →1 = x + 1) ∧(¬(x + 1 −1 = 0) →x + 1 = x + 1)\n(4.8)\n282\n4 Program verification\nWe need to show that this is implied by the given precondition ⊤, i.e. that\nit is true in any state. Indeed, simplifying (4.8) gives\n(x = 0 →1 = x + 1) ∧(¬(x = 0) →x + 1 = x + 1)\nand both these conjuncts, and therefore their conjunction, are clearly valid\nimplications. The above proof now is completed as:\n\u0001\n⊤\n\u0002\n\u0001\n(x + 1 −1 = 0 →1 = x + 1) ∧(¬(x + 1 −1 = 0) →x + 1 = x + 1)\n\u0002\nImplied\na = x + 1;\n\u0001\n(a −1 = 0 →1 = x + 1) ∧(¬(a −1 = 0) →a = x + 1)\n\u0002\nAssignment\nif (a - 1 == 0) {\n\u0001\n1 = x + 1\n\u0002\nIf-Statement\ny = 1;\n\u0001\ny = x + 1\n\u0002\nAssignment\n} else {\n\u0001\na = x + 1\n\u0002\nIf-Statement\ny = a;\n\u0001\ny = x + 1\n\u0002\nAssignment\n} \u0001\ny = x + 1\n\u0002\nIf-Statement\nWhile-statements.\nRecall that the proof rule for partial correctness of\nwhile-statements was presented in the following form in Figure 4.1 – here\nwe have written η instead of ψ:\n\u0001\nη ∧B\n\u0002\nC\n\u0001\nη\n\u0002\n\u0001\nη\n\u0002\nwhile B {C}\n\u0001\nη ∧¬B\n\u0002 Partial-while.\n(4.9)\nBefore we look at how Partial-while will be represented in proof tableaux,",
    "while-statements was presented in the following form in Figure 4.1 – here\nwe have written η instead of ψ:\n\u0001\nη ∧B\n\u0002\nC\n\u0001\nη\n\u0002\n\u0001\nη\n\u0002\nwhile B {C}\n\u0001\nη ∧¬B\n\u0002 Partial-while.\n(4.9)\nBefore we look at how Partial-while will be represented in proof tableaux,\nlet us look in more detail at the ideas behind this proof rule. The formula η is\nchosen to be an invariant of the body C of the while-statement: provided the\nboolean guard B is true, if η is true before we start C, and C terminates,\nthen it is also true at the end. This is what the premise\n\u0001\nη ∧B\n\u0002\nC\n\u0001\nη\n\u0002\nexpresses.\nNow suppose the while-statement executes a terminating run from a state\nthat satisﬁes η; and that the premise of (4.9) holds.\nr If B is false as soon as we embark on the while-statement, then we do not execute\nC at all. Nothing has happened to change the truth value of η, so we end the\nwhile-statement with η ∧¬B.\n4.3 Proof calculus for partial correctness\n283\nr If B is true when we embark on the while-statement, we execute C. By the\npremise of the rule in (4.9), we know η is true at the end of C.\n– if B is now false, we stop with η ∧¬B.\n– if B is true, we execute C again; η is again re-established. No matter how\nmany times we execute C in this way, η is re-established at the end of each\nexecution of C. The while-statement terminates if, and only if, B is false after\nsome ﬁnite (zero including) number of executions of C, in which case we have\nη ∧¬B.\nThis argument shows that Partial-while is sound with respect to the sat-\nisfaction relation for partial correctness, in the sense that anything we prove\nusing it is indeed true. However, as it stands it allows us to prove only things\nof the form\n\u0001\nη\n\u0002\nwhile (B) {C}\n\u0001\nη ∧¬B\n\u0002\n, i.e. triples in which the postcon-\ndition is the same as the precondition conjoined with ¬B. Suppose that we\nare required to prove\n\u0001\nφ\n\u0002\nwhile (B) {C}\n\u0001\nψ\n\u0002\n(4.10)\nfor some φ and ψ which are not related in that way. How can we use\nPartial-while in a situation like this?",
    "\u0002\nwhile (B) {C}\n\u0001\nη ∧¬B\n\u0002\n, i.e. triples in which the postcon-\ndition is the same as the precondition conjoined with ¬B. Suppose that we\nare required to prove\n\u0001\nφ\n\u0002\nwhile (B) {C}\n\u0001\nψ\n\u0002\n(4.10)\nfor some φ and ψ which are not related in that way. How can we use\nPartial-while in a situation like this?\nThe answer is that we must discover a suitable η, such that\n1.\n⊢AR φ →η,\n2.\n⊢AR η ∧¬B →ψ and\n3.\n⊢par\n\u0001\nη\n\u0002\nwhile (B) {C}\n\u0001\nη ∧¬B\n\u0002\nare all valid, where the latter is shown by means of Partial-while. Then,\nImplied infers that (4.10) is a valid partial-correctness triple.\nThe crucial thing, then, is the discovery of a suitable invariant η. It is a\nnecessary step in order to use the proof rule Partial-while and in general it\nrequires intelligence and ingenuity. This contrasts markedly with the case of\nthe proof rules for if-statements and assignments, which are purely mechan-\nical in nature: their usage is just a matter of symbol-pushing and does not\nrequire any deeper insight.\nDiscovery of a suitable invariant requires careful thought about what the\nwhile-statement is really doing. Indeed the eminent computer scientist, the\nlate E. Dijkstra, said that to understand a while-statement is tantamount\nto knowing what its invariant is with respect to given preconditions and\npostconditions for that while-statement.\nThis is because a suitable invariant can be interpreted as saying that the\nintended computation performed by the while-statement is correct up to\nthe current step of the execution. It then follows that, when the execution\n284\n4 Program verification\nterminates, the entire computation is correct. Let us formalize invariants\nand then study how to discover them.\nDeﬁnition 4.15 An invariant of the while-statement while (B) {C} is a\nformula η such that ⊨par\n\u0001\nη ∧B\n\u0002\nC\n\u0001\nη\n\u0002\nholds; i.e. for all states l, if η and B\nare true in l and C is executed from state l and terminates, then η is again\ntrue in the resulting state.",
    "and then study how to discover them.\nDeﬁnition 4.15 An invariant of the while-statement while (B) {C} is a\nformula η such that ⊨par\n\u0001\nη ∧B\n\u0002\nC\n\u0001\nη\n\u0002\nholds; i.e. for all states l, if η and B\nare true in l and C is executed from state l and terminates, then η is again\ntrue in the resulting state.\nNote that η does not have to be true continuously during the execution of\nC; in general, it will not be. All we require is that, if it is true before C is\nexecuted, then it is true (if and) when C terminates.\nFor any given while-statement there are several invariants. For example,\n⊤is an invariant for any while-statement; so is ⊥, since the premise of the\nimplication ‘if ⊥∧B is true, then . . . ’ is false, so that implication is true.\nThe formula ¬B is also an invariant of while (B) do {C}; but most of\nthese invariants are useless to us, because we are looking for an invariant\nη for which the sequents ⊢AR φ →η and ⊢AR η ∧¬B →ψ, are valid, where\nφ and ψ are the preconditions and postconditions of the while-statement.\nUsually, this will single out just one of all the possible invariants – up to\nlogical equivalence.\nA useful invariant expresses a relationship between the variables manip-\nulated by the body of the while-statement which is preserved by the exe-\ncution of the body, even though the values of the variables themselves may\nchange. The invariant can often be found by constructing a trace of the\nwhile-statement in action.\nExample 4.16 Consider the program Fac1 from page 262, annotated with\nlocation labels for our discussion:\ny = 1;\nz = 0;\nl1:\nwhile (z != x) {\nz = z + 1;\ny = y * z;\nl2:\n}\nSuppose program execution begins in a store in which x equals 6. When the\nprogram ﬂow ﬁrst encounters the while-statement at location l1, z equals\n0 and y equals 1, so the condition z ̸= x is true and the body is executed.\nThereafter at location l2, z equals 1 and y equals 1 and the boolean guard\nis still true, so the body is executed again. Continuing in this way, we obtain",
    "0 and y equals 1, so the condition z ̸= x is true and the body is executed.\nThereafter at location l2, z equals 1 and y equals 1 and the boolean guard\nis still true, so the body is executed again. Continuing in this way, we obtain\n4.3 Proof calculus for partial correctness\n285\nthe following trace:\nafter iteration\nz at l1\ny at l1\nB at l1\n0\n0\n1\ntrue\n1\n1\n1\ntrue\n2\n2\n2\ntrue\n3\n3\n6\ntrue\n4\n4\n24\ntrue\n5\n5\n120\ntrue\n6\n6\n720\nfalse\nThe program execution stops when the boolean guard becomes false.\nThe invariant of this example is easy to see: it is ‘y = z!’. Every time\nwe complete an execution of the body of the while-statement, this fact is\ntrue, even though the values of y and z have been changed. Moreover, this\ninvariant has the needed properties. It is\nr weak enough to be implied by the precondition of the while-statement, which\nwe will shortly discover to be y = 1 ∧z = 0 based on the initial assignments and\ntheir precondition 0!\ndef\n= 1,\nr but also strong enough that, together with the negation of the boolean guard, it\nimplies the postcondition ‘y = x!’.\nThat is to say, the sequents\n⊢AR (y = 1 ∧z = 0) →(y = z!) and ⊢AR (y = z! ∧x = z) →(y = x!)\n(4.11)\nare valid.\nAs in this example, a suitable invariant is often discovered by looking at\nthe logical structure of the postcondition. A complete proof of the factorial\nexample in tree form, using this invariant, was given in Figure 4.2.\nHow should we use the while-rule in proof tableaux? We need to think\nabout how to push an arbitrary postcondition ψ upwards through a while-\nstatement to meet the precondition φ. The steps are:\n1.\nGuess a formula η which you hope is a suitable invariant.\n2.\nTry to prove that ⊢AR η ∧¬B →ψ and ⊢AR φ →η are valid, where B is the\nboolean guard of the while-statement. If both proofs succeed, go to 3. Otherwise\n(if at least one proof fails), go back to 1.\n3.\nPush η upwards through the body C of the while-statement; this involves ap-",
    "2.\nTry to prove that ⊢AR η ∧¬B →ψ and ⊢AR φ →η are valid, where B is the\nboolean guard of the while-statement. If both proofs succeed, go to 3. Otherwise\n(if at least one proof fails), go back to 1.\n3.\nPush η upwards through the body C of the while-statement; this involves ap-\nplying other rules dictated by the form of C. Let us name the formula that\nemerges η′.\n286\n4 Program verification\n4.\nTry to prove that ⊢AR η ∧B →η′ is valid; this proves that η is indeed an in-\nvariant. If you succeed, go to 5. Otherwise, go back to 1.\n5.\nNow write η above the while-statement and write φ above that η, annotating\nthat η with an instance of Implied based on the successful proof of the validity\nof ⊢AR φ →η in 2. Mission accomplished!\nExample 4.17 We continue the example of the factorial. The partial proof\nobtained by pushing y = x! upwards through the while-statement – thus\nchecking the hypothesis that y = z! is an invariant – is as follows:\ny = 1;\nz = 0;\n\u0001\ny = z!\n\u0002\n?\nwhile (z != x) {\n\u0001\ny = z! ∧z ̸= x\n\u0002\nInvariant Hyp. ∧guard\n\u0001\ny · (z + 1) = (z + 1)!\n\u0002\nImplied\nz = z + 1;\n\u0001\ny · z = z!\n\u0002\nAssignment\ny = y * z;\n\u0001\ny = z!\n\u0002\nAssignment\n} \u0001\ny = x!\n\u0002\n?\nWhether y = z! is a suitable invariant depends on three things:\nr The ability to prove that it is indeed an invariant, i.e. that y = z! implies y · (z +\n1) = (z + 1)!. This is the case, since we just multiply each side of y = z! by z + 1\nand appeal to the inductive deﬁnition of (z + 1)! in Example 4.2.\nr The ability to prove that η is strong enough that it and the negation of the\nboolean guard together imply the postcondition; this is also the case, for y = z!\nand x = z imply y = x!.\nr The ability to prove that η is weak enough to be established by the code leading\nup to the while-statement. This is what we prove by continuing to push the result\nupwards through the code preceding the while-statement.\nContinuing, then: pushing y = z! through z = 0 results in y = 0! and push-",
    "r The ability to prove that η is weak enough to be established by the code leading\nup to the while-statement. This is what we prove by continuing to push the result\nupwards through the code preceding the while-statement.\nContinuing, then: pushing y = z! through z = 0 results in y = 0! and push-\ning that through y = 1 renders 1 = 0!. The latter holds in all states as 0! is\n4.3 Proof calculus for partial correctness\n287\ndeﬁned to be 1, so it is implied by ⊤; our completed proof is:\n\u0001\n⊤\n\u0002\n\u0001\n1 = 0!\n\u0002\nImplied\ny = 1;\n\u0001\ny = 0!\n\u0002\nAssignment\nz = 0;\n\u0001\ny = z!\n\u0002\nAssignment\nwhile (z != x) {\n\u0001\ny = z! ∧z ̸= x\n\u0002\nInvariant Hyp. ∧guard\n\u0001\ny · (z + 1) = (z + 1)!\n\u0002\nImplied\nz = z + 1;\n\u0001\ny · z = z!\n\u0002\nAssignment\ny = y * z;\n\u0001\ny = z!\n\u0002\nAssignment\n} \u0001\ny = z! ∧¬(z ̸= x)\n\u0002\nPartial-while\n\u0001\ny = x!\n\u0002\nImplied\n4.3.3 A case study: minimal-sum section\nWe practice the proof rule for while-statements once again by verifying a\nprogram which computes the minimal-sum section of an array of integers.\nFor that, let us extend our core programming language with arrays of inte-\ngers5. For example, we may declare an array\nint a[n];\nwhose name is a and whose ﬁelds are accessed by a[0], a[1], . . . , a[n-1],\nwhere n is some constant. Generally, we allow any integer expression E to\ncompute the ﬁeld index, as in a[E]. It is the programmer’s responsibility to\nmake sure that the value computed by E is always within the array bounds.\nDeﬁnition 4.18 Let a[0], . . . , a[n −1] be the integer values of an array a.\nA section of a is a continuous piece a[i], . . . , a[j], where 0 ≤i ≤j < n. We\n5 We only read from arrays in the program Min Sum which follows. Writing to arrays introduces\nadditional problems because an array element can have several syntactically diﬀerent names and\nthis has to be taken into account by the calculus.\n288\n4 Program verification\nwrite Si,j for the sum of that section: a[i] + a[i + 1] + · · · + a[j]. A minimal-\nsum section is a section a[i], . . . , a[j] of a such that the sum Si,j is less than",
    "this has to be taken into account by the calculus.\n288\n4 Program verification\nwrite Si,j for the sum of that section: a[i] + a[i + 1] + · · · + a[j]. A minimal-\nsum section is a section a[i], . . . , a[j] of a such that the sum Si,j is less than\nor equal to the sum Si′,j′ of any other section a[i′], . . . , a[j′] of a.\nExample 4.19 Let us illustrate these concepts on the example integer array\n[−1, 3, 15, −6, 4, −5]. Both [3, 15, −6] and [−6] are sections, but [3, −6, 4]\nisn’t since 15 is missing. A minimal-sum section for this particular array is\n[−6, 4, −5] with sum −7; it is the only minimal-sum section in this case.\nIn general, minimal-sum sections need not be unique. For example, the\narray [1, −1, 3, −1, 1] has two minimal-sum sections [1, −1] and [−1, 1] with\nminimal sum 0.\nThe task at hand is to\nr write a program Min Sum, written in our core programming language extended\nwith integer arrays, which computes the sum of a minimal-sum section of a given\narray;\nr make the informal requirement of this problem, given in the previous item, into\na formal speciﬁcation about the behaviour of Min Sum;\nr use our proof calculus for partial correctness to show that Min Sum satisﬁes those\nformal speciﬁcations provided that it terminates.\nThere is an obvious program to do the job: we could list all the possible\nsections of a given array, then traverse that list to compute the sum of\neach section and keep the recent minimal sum in a storage location. For the\nexample array [−1, 3, −2], this results in the list\n[−1], [−1, 3], [−1, 3, −2], [3], [3, −2], [−2]\nand we see that only the last section [−2] produces the minimal sum −2.\nThis idea can easily be coded in our core programming language, but it\nhas a serious drawback: the number of sections of a given array of size n is\nproportional to the square of n; if we also have to sum all those, then our task\nhas worst-case time complexity of the order n · n2 = n3. Computationally,",
    "has a serious drawback: the number of sections of a given array of size n is\nproportional to the square of n; if we also have to sum all those, then our task\nhas worst-case time complexity of the order n · n2 = n3. Computationally,\nthis is an expensive price to pay, so we should inspect the problem more\nclosely in order to see whether we can do better.\nCan we compute the minimal sum over all sections in time proportional\nto n, by passing through the array just once? Intuitively, this seems diﬃcult,\nsince if we store just the minimal sum seen so far as we pass through the\narray, we may miss the opportunity of some large negative numbers later on\nbecause of some large positive numbers we encounter en route. For example,\n4.3 Proof calculus for partial correctness\n289\nsuppose the array is\n[−8, 3, −65, 20, 45, −100, −8, 17, −4, −14].\nShould we settle for −8 + 3 −65, or should we try to take advantage of the\n−100 – remembering that we can pass through the array only once? In this\ncase, the whole array is a section that gives us the smallest sum, but it\nis diﬃcult to see how a program which passes through the array just once\ncould detect this.\nThe solution is to store two values during the pass: the minimal sum seen\nso far (s in the program below) and also the minimal sum seen so far of\nall sections which end at the current point in the array (t below). Here is a\nprogram that is intended to do this:\nk = 1;\nt = a[0];\ns = a[0];\nwhile (k != n) {\nt = min(t + a[k], a[k]);\ns = min(s,t);\nk = k + 1;\n}\nwhere min is a function which computes the minimum of its two arguments\nas speciﬁed in exercise 10 on page 301. The variable k proceeds through\nthe range of indexes of the array and t stores the minimal sum of sections\nthat end at a[k] – whenever the control ﬂow of the program is about to\nevaluate the boolean expression of its while-statement. As each new value is\nexamined, we can either add it to the current minimal sum, or decide that a",
    "the range of indexes of the array and t stores the minimal sum of sections\nthat end at a[k] – whenever the control ﬂow of the program is about to\nevaluate the boolean expression of its while-statement. As each new value is\nexamined, we can either add it to the current minimal sum, or decide that a\nlower minimal sum can be obtained by starting a new section. The variable\ns stores the minimal sum seen so far; it is computed as the minimum we\nhave seen so far in the last step, or the minimal sum of sections that end at\nthe current point.\nAs you can see, it not intuitively clear that this program is correct, war-\nranting the use of our partial-correctness calculus to prove its correctness.\nTesting the program with a few examples is not suﬃcient to ﬁnd all mis-\ntakes, however, and the reader would rightly not be convinced that this\nprogram really does compute the minimal-sum section in all cases. So let\nus try to use the partial-correctness calculus introduced in this chapter to\nprove it.\n290\n4 Program verification\nWe formalise our requirement of the program as two speciﬁcations6, writ-\nten as Hoare triples.\nS1.\n\u0001\n⊤\n\u0002\nMin Sum\n\u0001\n∀i, j (0 ≤i ≤j < n →s ≤Si,j)\n\u0002\n.\nIt says that, after the program terminates, s is less than or equal to, the\nsum of any section of the array. Note that i and j are logical variables\nin that they don’t occur as program variables.\nS2.\n\u0001\n⊤\n\u0002\nMin Sum\n\u0001\n∃i, j (0 ≤i ≤j < n ∧s = Si,j)\n\u0002\n,\nwhich says that there is a section whose sum is s.\nIf there is a section whose sum is s and no section has a sum less than s,\nthen s is the sum of a minimal-sum section: the ‘conjunction’ of S1 and S2\ngive us the property we want.\nLet us ﬁrst prove S1. This begins with seeking a suitable invariant. As\nalways, the following characteristics of invariants are a useful guide:\nr Invariants express the fact that the computation performed so far by the while-\nstatement is correct.\nr Invariants typically have the same form as the desired postcondition of the while-\nstatement.",
    "always, the following characteristics of invariants are a useful guide:\nr Invariants express the fact that the computation performed so far by the while-\nstatement is correct.\nr Invariants typically have the same form as the desired postcondition of the while-\nstatement.\nr Invariants express relationships between the variables manipulated by the while-\nstatement which are re-established each time the body of the while-statement is\nexecuted.\nA suitable invariant in this case appears to be\nInv1(s, k)\ndef\n= ∀i, j (0 ≤i ≤j < k →s ≤Si,j)\n(4.12)\nsince it says that s is less than, or equal to, the minimal sum observed up\nto the current stage of the computation, represented by k. Note that it has\nthe same form as the desired postcondition: we replaced the n by k, since\nthe ﬁnal value of k is n. Notice that i and j are quantiﬁed in the formula,\nbecause they are logical variables; k is a program variable. This justiﬁes the\nnotation Inv1(s, k) which highlights that the formula has only the program\nvariables s and k as free variables and is similar to the use of fun-statements\nin Alloy in Chapter 2.\nIf we start work on producing a proof tableau with this invariant, we\nwill soon ﬁnd that it is not strong enough to do the job. Intuitively, this is\nbecause it ignores the value of t, which stores the minimal sum of all sections\nending just before a[k], which is crucial in the idea behind the program. A\nsuitable invariant expressing that t is correct up to the current point of the\n6 The notation ∀i, j abbreviates ∀i∀j, and similarly for ∃i, j.\n4.3 Proof calculus for partial correctness\n291\n(⊤)\n(Inv1(a[0], 1) ∧Inv2(a[0], 1))\nImplied\nk = 1;\n(Inv1(a[0], k) ∧Inv2(a[0], k))\nAssignment\nt = a[0];\n(Inv1(a[0], k) ∧Inv2(t, k))\nAssignment\ns = a[0];\n(Inv1(s, k) ∧Inv2(t, k))\nAssignment\nwhile (k != n) {\n(Inv1(s, k) ∧Inv2(t, k) ∧k ̸= n)\nInvariant Hyp. ∧guard\n(Inv1(min(s, min(t + a[k], a[k])), k + 1)\n∧Inv2(min(t + a[k], a[k]), k + 1))\nImplied (Lemma 4.20)\nt = min(t + a[k], a[k]);",
    "t = a[0];\n(Inv1(a[0], k) ∧Inv2(t, k))\nAssignment\ns = a[0];\n(Inv1(s, k) ∧Inv2(t, k))\nAssignment\nwhile (k != n) {\n(Inv1(s, k) ∧Inv2(t, k) ∧k ̸= n)\nInvariant Hyp. ∧guard\n(Inv1(min(s, min(t + a[k], a[k])), k + 1)\n∧Inv2(min(t + a[k], a[k]), k + 1))\nImplied (Lemma 4.20)\nt = min(t + a[k], a[k]);\n(Inv1(min(s, t), k + 1) ∧Inv2(t, k + 1))\nAssignment\ns = min(s,t);\n(Inv1(s, k + 1) ∧Inv2(t, k + 1))\nAssignment\nk = k + 1;\n(Inv1(s, k) ∧Inv2(t, k))\nAssignment\n}\n(Inv1(s, k) ∧Inv2(t, k) ∧¬¬(k = n))\nPartial-while\n(Inv1(s, n))\nImplied\nFigure 4.3. Tableau proof for specification S1 of Min Sum.\ncomputation is\nInv2(t, k)\ndef\n= ∀i (0 ≤i < k →t ≤Si,k−1)\n(4.13)\nsaying that t is not greater than the sum of any section ending in a[k −1].\nOur invariant is the conjunction of these formulas, namely\nInv1(s, k) ∧Inv2(t, k).\n(4.14)\nThe completed proof tableau of S1 for Min Sum is given in Figure 4.3. The\ntableau is constructed by\nr Proving that the candidate invariant (4.14) is indeed an invariant. This involves\npushing it upwards through the body of the while-statement and showing that\nwhat emerges follows from the invariant and the boolean guard. This non-trivial\nimplication is shown in the proof of Lemma 4.20.\nr Proving that the invariant, together with the negation of the boolean guard, is\nstrong enough to prove the desired postcondition. This is the last implication of\nthe proof tableau.\n292\n4 Program verification\nr Proving that the invariant is established by the code before the while-statement.\nWe simply push it upwards through the three initial assignments and check that\nthe resulting formula is implied by the precondition of the speciﬁcation, here ⊤.\nAs so often the case, in constructing the tableau, we ﬁnd that two formulas\nmeet; and we have to prove that the ﬁrst one implies the second one. Some-\ntimes this is easy and we can just note the implication in the tableau. For\nexample, we readily see that ⊤implies Inv1(a[0], 1) ∧Inv2(a[0], 1): k being",
    "meet; and we have to prove that the ﬁrst one implies the second one. Some-\ntimes this is easy and we can just note the implication in the tableau. For\nexample, we readily see that ⊤implies Inv1(a[0], 1) ∧Inv2(a[0], 1): k being\n1 forces i and j to be zero in order that the assumptions in Inv1(a[0], k)\nand Inv2(a[0], k) be true. But this means that their conclusions are true as\nwell. However, the proof obligation that the invariant hypothesis imply the\nprecondition computed within the body of the while-statement reveals the\ncomplexity and ingenuity of this program and its justiﬁcation needs to be\ntaken oﬀ-line:\nLemma 4.20 Let s and t be any integers, n the length of the array a,\nand k an index of that array in the range of 0 < k < n. Then Inv1(s, k) ∧\nInv2(t, k) ∧k ̸= n implies\n1.\nInv1(min(s, min(t + a[k], a[k])), k + 1) as well as\n2.\nInv2(min(t + a[k], a[k]), k + 1).\nPROOF:\n1.\nTake any i with 0 ≤i < k + 1; we will prove that min(t + a[k], a[k]) ≤Si,k. If\ni < k, then Si,k = Si,k−1 + a[k], so what we have to prove is min(t + a[k], a[k]) ≤\nSi,k−1 + a[k]; but we know t ≤Si,k−1, so the result follows by adding a[k] to\neach side. Otherwise, i = k, Si,k = a[k] and the result follows.\n2.\nTake any i and j with 0 ≤i ≤j < k + 1; we prove that min(s, t + a[k], a[k]) ≤\nSi,j. If i ≤j < k, then the result is immediate. Otherwise, i ≤j = k and the\nresult follows from part 1 of the lemma.\n2\n4.4 Proof calculus for total correctness\nIn the preceding section, we developed a calculus for proving partial correct-\nness of triples\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n. In that setting, proofs come with a disclaimer: only\nif the program P terminates an execution does a proof of ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\ntell\nus anything about that execution. Partial correctness does not tell us any-\nthing if P ‘loops’ indeﬁnitely. In this section, we extend our proof calculus\nfor partial correctness so that it also proves that programs terminate. In the\nprevious section, we already pointed out that only the syntactic construct",
    "thing if P ‘loops’ indeﬁnitely. In this section, we extend our proof calculus\nfor partial correctness so that it also proves that programs terminate. In the\nprevious section, we already pointed out that only the syntactic construct\nwhile B {C} could be responsible for non-termination.\n4.4 Proof calculus for total correctness\n293\nTherefore, the proof calculus for total correctness is the same as\nfor partial correctness for all the rules except the rule for while-\nstatements.\nA proof of total correctness for a while-statement will consist of two parts:\nthe proof of partial correctness and a proof that the given while-statement\nterminates. Usually, it is a good idea to prove partial correctness ﬁrst since\nthis often provides helpful insights for a termination proof. However, some\nprograms require termination proofs as premises for establishing partial cor-\nrectness, as can be seen in exercise 1(d) on page 303.\nThe proof of termination usually has the following form. We identify an\ninteger expression whose value can be shown to decrease every time we\nexecute the body of the while-statement in question, but which is always\nnon-negative. If we can ﬁnd an expression with these properties, it follows\nthat the while-statement must terminate; because the expression can only\nbe decremented a ﬁnite number of times before it becomes 0. That is because\nthere is only a ﬁnite number of integer values between 0 and the initial value\nof the expression.\nSuch integer expressions are called variants. As an example, for the pro-\ngram Fac1 of Example 4.2, a suitable variant is x −z. The value of this\nexpression is decremented every time the body of the while-statement is\nexecuted. When it is 0, the while-statement terminates.\nWe can codify this intuition in the following rule for total correctness\nwhich replaces the rule for the while statement:\n\u0001\nη ∧B ∧0 ≤E = E0\n\u0002\nC\n\u0001\nη ∧0 ≤E < E0\n\u0002\n\u0001\nη ∧0 ≤E\n\u0002\nwhile B {C}\n\u0001\nη ∧¬B\n\u0002\nTotal-while.\n(4.15)",
    "executed. When it is 0, the while-statement terminates.\nWe can codify this intuition in the following rule for total correctness\nwhich replaces the rule for the while statement:\n\u0001\nη ∧B ∧0 ≤E = E0\n\u0002\nC\n\u0001\nη ∧0 ≤E < E0\n\u0002\n\u0001\nη ∧0 ≤E\n\u0002\nwhile B {C}\n\u0001\nη ∧¬B\n\u0002\nTotal-while.\n(4.15)\nIn this rule, E is the expression whose value decreases with each execution\nof the body C. This is coded by saying that, if its value equals that of the\nlogical variable E0 before the execution of C, then it is strictly less than E0\nafter it – yet still it remains non-negative. As before, η is the invariant.\nWe use the rule Total-while in tableaux similarly to how we use Partial-\nwhile, but note that the body of the rule C must now be shown to satisfy\n\u0001\nη ∧B ∧0 ≤E = E0\n\u0002\nC\n\u0001\nη ∧0 ≤E < E0\n\u0002\n.\nWhen we push η ∧0 ≤E < E0 upwards through the body, we have to prove\nthat what emerges from the top is implied by η ∧B ∧0 ≤E = E0; and\nthe weakest precondition for the entire while-statement, which gets writ-\nten above that while-statement, is η ∧0 ≤E.\n294\n4 Program verification\nLet us illustrate this rule by proving that ⊢tot\n\u0001\nx ≥0\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nis\nvalid, where Fac1 is given in Example 4.2, as follows:\ny = 1;\nz = 0;\nwhile (x != z) {\nz = z + 1;\ny = y * z;\n}\nAs already mentioned, x −z is a suitable variant. The invariant (y = z!) of\nthe partial correctness proof is retained. We obtain the following complete\nproof for total correctness:\n(x ≥0)\n(1 = 0! ∧0 ≤x −0)\nImplied\ny = 1;\n(y = 0! ∧0 ≤x −0)\nAssignment\nz = 0;\n(y = z! ∧0 ≤x −z)\nAssignment\nwhile (x != z) {\n(y = z! ∧x ̸= z ∧0 ≤x −z = E0)\nInvariant Hyp. ∧guard\n(y · (z + 1) = (z + 1)! ∧0 ≤x −(z + 1) < E0)\nImplied\nz = z + 1;\n(y · z = z! ∧0 ≤x −z < E0)\nAssignment\ny = y * z;\n(y = z! ∧0 ≤x −z < E0)\nAssignment\n}\n(y = z! ∧x = z)\nTotal-while\n(y = x!)\nImplied\nand so ⊢tot\n\u0001\nx ≥0\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nis valid. Two comments are in order:\nr Notice that the precondition x ≥0 is crucial in securing the fact that 0 ≤x −z",
    "z = z + 1;\n(y · z = z! ∧0 ≤x −z < E0)\nAssignment\ny = y * z;\n(y = z! ∧0 ≤x −z < E0)\nAssignment\n}\n(y = z! ∧x = z)\nTotal-while\n(y = x!)\nImplied\nand so ⊢tot\n\u0001\nx ≥0\n\u0002\nFac1\n\u0001\ny = x!\n\u0002\nis valid. Two comments are in order:\nr Notice that the precondition x ≥0 is crucial in securing the fact that 0 ≤x −z\nholds right before the while-statements gets executed: it implies the precondition\n1 = 0! ∧0 ≤x −0 computed by our proof. In fact, observe that Fac1 does not\nterminate if x is negative initially.\nr The application of Implied within the body of the while-statement is valid, but\nit makes vital use of the fact that the boolean guard is true. This is an exam-\nple of a while-statement whose boolean guard is needed in reasoning about the\ncorrectness of every iteration of that while-statement.\n4.4 Proof calculus for total correctness\n295\nOne may wonder whether there is a program that, given a while-statement\nand a precondition as input, decides whether that while-statement termi-\nnates on all runs whose initial states satisfy that precondition. One can prove\nthat there cannot be such a program. This suggests that the automatic ex-\ntraction of useful termination expressions E cannot be realized either. Like\nmost other such universal problems discussed in this text, the wish to com-\npletely mechanise such decision or extraction procedures cannot be realised.\nHence, ﬁnding a working variant E is a creative activity which requires skill,\nintuition and practice.\nLet us consider an example program, Collatz, that conveys the challenge\none may face in ﬁnding suitable termination variants E:\nc = x;\nwhile (c != 1) {\nif (c % 2 == 0) { c = c / 2; }\nelse { c = 3*c + 1; }\n}\nThis program records the initial value of x in c and then iterates an if-\nstatement until, and if, the value of c equals 1. The if-statement tests\nwhether c is even – divisible by 2 – if so, c stores its current value divided\nby 2; if not, c stores ‘three times its current value plus 1.’ The expression",
    "statement until, and if, the value of c equals 1. The if-statement tests\nwhether c is even – divisible by 2 – if so, c stores its current value divided\nby 2; if not, c stores ‘three times its current value plus 1.’ The expression\nc / 2 denotes integer division, so 11 / 2 renders 5 as does 10 / 2.\nTo get a feel for this algorithm, consider an execution trace in which the\nvalue of x is 5: the value of c evolves as 5 16 8 4 2 1. For another example,\nif the value of x is initially 172, the evolution of c is\n172 86 43 130 65 196 98 49 148 74 37 112 56 28 14 7 22\n11 34 17 52 26 13 40 20 10 5 16 8 4 2 1\nThis execution requires 32 iterations of the while-statement to reach a ter-\nminating state in which the value of c equals 1. Notice how this trace reaches\n5, from where on the continuation is as if 5 were the initial value of x.\nFor the initial value 123456789 of x we abstract the evolution of c with +\n(its value increases in the else-branch) and −(its value decreases in the\nif-branch):\n+ - - - - - - + - - - + - + - - + - + - + - + - + - + - - + - - -\n- + - - - - + - - + - - + - - + - + - - - + - + - - - - - + - - +\n- + - - + - - - - + - - - - - - + - - + - + - - + - + - + - - + -\n+ - + - + - - + - - - + - + - + - - + - + - - + - + - + - + - + -\n+ - - - + - + - + - + - - - - + - - + - - + - - - - + - - - + - +\n- + - - - - - + - - - -\n296\n4 Program verification\nThis requires 177 iterations of the while-statement to reach a terminating\nstate. Although it is re-assuring that some program runs terminate, the\nirregular pattern of + and −above make it seem very hard, if not impossible,\nto come up with a variant that proves the termination of Collatz on all\nexecutions in which the initial value of x is positive.\nFinally, let’s consider a really big integer:\n32498723462509735034567279652376420563047563456356347563\\\\\n96598734085384756074086560785607840745067340563457640875\\\\\n62984573756306537856405634056245634578692825623542135761\\\\\n9519765129854122965424895465956457",
    "Finally, let’s consider a really big integer:\n32498723462509735034567279652376420563047563456356347563\\\\\n96598734085384756074086560785607840745067340563457640875\\\\\n62984573756306537856405634056245634578692825623542135761\\\\\n9519765129854122965424895465956457\nwhere \\\\ denotes concatenation of digits. Although this is a very large num-\nber indeed, our program Collatz requires only 4940 iterations to terminate.\nUnfortunately, nobody knows a suitable variant for this program that could\nprove the validity of ⊢tot\n\u0001\n0 < x\n\u0002\nCollatz\n\u0001\n⊤\n\u0002\n. Observe how the use of ⊤as\na postcondition emphasizes that this Hoare triple is merely concerned about\nprogram termination as such. Ironically, there is also no known initial value\nof x greater than 0 for which Collatz doesn’t terminate. In fact, things are\neven subtler than they may appear: if we replace 3*c + 1 in Collatz with a\ndiﬀerent such linear expression in c, the program may not terminate despite\nmeeting the precondition 0 < x; see exercise 6 on page 303.\n4.5 Programming by contract\nFor a valid sequent ⊢tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n, the triple\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nmay be seen as a\ncontract between a supplier and a consumer of a program P. The supplier\ninsists that consumers run P only on initial state satisﬁes φ. In that case,\nthe supplier promises the consumer that the ﬁnal state of that run satisﬁes\nψ. For a valid ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\n, the latter guarantee applies only when a run\nterminates.\nFor imperative programming, the validation of Hoare triples can be in-\nterpreted as the validation of contracts for method or procedure calls. For\nexample, our program fragment Fac1 may be the ... in the method body\nint factorial (x: int) { ... return y; }\nThe code for this method can be annotated with its contractual assumptions\nand guarantees. These annotations can be checked oﬀ-line by humans, during\ncompile-time or even at run-time in languages such as Eiﬀel. A possible\nformat for such contracts for the method factorial is given in Figure 4.4.",
    "The code for this method can be annotated with its contractual assumptions\nand guarantees. These annotations can be checked oﬀ-line by humans, during\ncompile-time or even at run-time in languages such as Eiﬀel. A possible\nformat for such contracts for the method factorial is given in Figure 4.4.\n4.5 Programming by contract\n297\nmethod name:\nfactorial\ninput:\nx ofType int\nassumes:\n0 <= x\nguarantees:\ny = x!\noutput:\nofType int\nmodifies only:\ny\nFigure 4.4. A contract for the method factorial.\nThe keyword assumes states all preconditions, the keyword guarantees lists\nall postconditions. The keyword modifies only speciﬁes which program\nvariables may change their value during an execution of this method.\nLet us see why such contracts are useful. Suppose that your boss tells\nyou to write a method that computes\n\u0001n\nk\n\u0002\n– read ‘n choose k’ – a notion of\ncombinatorics where 1/\n\u000149\n6\n\u0002\nis your change of getting all six lottery numbers\nright out of 49 numbers total. Your boss also tells you that\n\u0003n\nk\n\u0004\n=\nn!\nk! · (n −k)!\n(4.16)\nholds. The method factorial and its contract (Figure 4.4) is at your dis-\nposal. Using (4.16) you can quickly compute some values, such as\n\u00015\n2\n\u0002\n=\n5!/(2! · 3!) = 10,\n\u000110\n0\n\u0002\n= 1, and\n\u000149\n6\n\u0002\n= 13983816. You then write a method\nchoose that makes calls to the method factorial, e.g. you may write\nint choose(n : int, k : int) {\nreturn factorial(n) / (factorial(k) * factorial (n - k));\n}\nThis method body consists of a return-statement only which makes three\ncalls to method factorial and then computes the result according to (4.16).\nSo far so good. But programming by contract is not just about writing\nprograms, it is also about writing the contracts for such programs! The\nstatic information about choose – e.g. its name – are quickly ﬁlled into that\ncontract. But what about the preconditions (assumes) and postconditions\n(guarantees)?\nAt the very least, you must state preconditions that ensure that all",
    "static information about choose – e.g. its name – are quickly ﬁlled into that\ncontract. But what about the preconditions (assumes) and postconditions\n(guarantees)?\nAt the very least, you must state preconditions that ensure that all\nmethod calls within this method’s body satisfy their preconditions. In this\ncase, we only call factorial whose precondition is that its input value be\nnon-negative. Therefore, we require that n, k, and n −k be non-negative.\nThe latter says that n is not smaller than k.\nWhat about the postconditions of choose? Since the method body de-\nclared no local variables, we use result to denote the return value of this\n298\n4 Program verification\nmethod. The postcondition then states that result equals\n\u0001n\nk\n\u0002\n– assuming\nthat you boss’ equation (4.16) is correct for your preconditions 0 ≤k, 0 ≤n,\nand k ≤n. The contract for choose is therefore\nmethod name:\nchoose\ninput:\nn ofType int, k ofType int\nassumes:\n0 <= k, 0 <= n, k <= n\nguarantees:\nresult = ‘n choose k’\noutput:\nofType int\nmodifies only local variables\nFrom this we learn that programming by contract uses contracts\n1.\nas assume-guarantee abstract interfaces to methods;\n2.\nto specify their method’s header information, output type, when calls to its\nmethod are ‘legal,’ what variables that method modiﬁes, and what its output\nsatisﬁes on all ‘legal’ calls;\n3.\nto enable us to prove the validity of a contract C for method m by ensuring that\nall method calls within m’s body meet the preconditions of these methods and\nusing that all such calls then meet their respective postconditions.\nProgramming by contract therefore gives rise to program validation by\ncontract. One proves the ‘Hoare triple’\n\u0001\nassume\n\u0002\nmethod\n\u0001\nguarantee\n\u0002\nvery\nmuch in the style developed in this chapter, except that for all method\ninvocations within that body we can assume that their Hoare triples are\ncorrect.\nExample 4.21 We have already used program validation by contract in our",
    "\u0001\nassume\n\u0002\nmethod\n\u0001\nguarantee\n\u0002\nvery\nmuch in the style developed in this chapter, except that for all method\ninvocations within that body we can assume that their Hoare triples are\ncorrect.\nExample 4.21 We have already used program validation by contract in our\nveriﬁcation of the program that computes the minimal sum for all sections\nof an array in Figure 4.3 on page 291. Let us focus on the proof fragment\n(Inv1(min(s, min(t + a[k], a[k])), k + 1) ∧Inv2(min(t + a[k], a[k]), k + 1))\nImplied (Lemma 4.20)\nt = min(t + a[k], a[k]);\n(Inv1(min(s, t), k + 1) ∧Inv2(t, k + 1))\nAssignment\ns = min(s,t);\n(Inv1(s, k + 1) ∧Inv2(t, k + 1))\nAssignment\nIts last line serves as the postcondition which gets pushed through the as-\nsignment s = min(s,t). But min(s,t) is a method call whose guarantees\nare speciﬁed as ‘result equals min(s, t),’ where min(s, t) is a mathematical\nnotation for the smaller of the numbers s and t. Thus, the rule Assignment\ndoes not substitute the syntax of the method invocation min(s,t) for all\noccurrences of s in Inv1(s, k + 1) ∧Inv2(t, k + 1), but changes all such s to\nthe guarantee min(s, t) of the method call min(s,t) – program validation\n4.6 Exercises\n299\nby contract in action! A similar comment applies for the assignment t =\nmin(t + a[k], a[k]).\nProgram validation by contract has to be used wisely to avoid circular\nreasoning. If each method is a node in a graph, let’s draw an edge from\nmethod n to method m iﬀwithin the body of n there is a call to method m.\nFor program validation by contract to be sound, we require that there be\nno cycles in this method-dependency graph.\n4.6 Exercises\nExercises 4.1\n1.\n*\nIf you already have written computer programs yourself, assemble for each pro-\ngramming language you used a list of features of its software development envi-\nronment (compiler, editor, linker, run-time environment etc) that may improve\nthe likelihood that your programs work correctly. Try to rate the eﬀectiveness of\neach such feature.",
    "gramming language you used a list of features of its software development envi-\nronment (compiler, editor, linker, run-time environment etc) that may improve\nthe likelihood that your programs work correctly. Try to rate the eﬀectiveness of\neach such feature.\n2. Repeat the previous exercise by listing and rating features that may decrease\nthe likelihood of procuding correct and reliable programs.\nExercises 4.2\n1.\n*\nIn what circumstances would if (B) {C1} else {C2} fail to terminate?\n2.\n*\nA familiar command missing from our language is the for-statement. It may be\nused to sum the elements in an array, for example, by programming as follows:\ns = 0;\nfor (i = 0; i <= max; i = i+1) {\ns = s + a[i];\n}\nAfter performing the initial assignment s = 0, this executes i = 0 ﬁrst, then\nexecutes the body s = s + a[i] and the incrementation i = i + 1 continually\nuntil i <= max becomes false. Explain how for (C1; B; C2) {C3} can be deﬁned\nas a derived program in our core language.\n3. Suppose that you need a language construct repeat {C} until (B) which re-\npeats C until B becomes true, i.e.\ni. executes C in the current state of the store;\nii. evaluates B in the resulting state of the store;\niii. if B is false, the program resumes with (i); otherwise, the program\nrepeat {C} until (B) terminates.\nThis construct sometimes allows more elegant code than a corresponding while-\nstatement.\n300\n4 Program verification\n(a) Deﬁne repeat C until B as a derived expression using our core language.\n(b) Can one deﬁne every repeat expression in our core language extended with\nfor-statements? (You might need the empty command skip which does noth-\ning.)\nExercises 4.3\n1. For any store l as in Example 4.4 (page 264), determine which of the relations\nbelow hold; justify your answers:\n(a)\n*\nl ⊨(x + y < z) →¬(x ∗y = z)\n(b) l ⊨∀u (u < y) ∨(u ∗z < y ∗z)\n(c)\n*\nl ⊨x + y −z < x ∗y ∗z.\n2.\n*\nFor any φ, ψ and P explain why ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds whenever the relation\n⊨tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds.",
    "below hold; justify your answers:\n(a)\n*\nl ⊨(x + y < z) →¬(x ∗y = z)\n(b) l ⊨∀u (u < y) ∨(u ∗z < y ∗z)\n(c)\n*\nl ⊨x + y −z < x ∗y ∗z.\n2.\n*\nFor any φ, ψ and P explain why ⊨par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds whenever the relation\n⊨tot\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\nholds.\n3. Let the relation P ⊢l ; l′ hold iﬀP’s execution in store l terminates, resulting\nin store l′. Use this formal judgment P ⊢l ; l′ along with the relation l ⊨φ to\ndeﬁne ⊨par and ⊨tot symbolically.\n4. Another reason for proving partial correctness in isolation is that some program\nfragments have the form while (true) {C}. Give useful examples of such pro-\ngram fragments in application programming.\n5.\n*\nUse the proof rule for assignment and logical implication as appropriate to show\nthe validity of\n(a) ⊢par\n\u0001\nx > 0\n\u0002\ny = x + 1\n\u0001\ny > 1\n\u0002\n(b) ⊢par\n\u0001\n⊤\n\u0002\ny = x; y = x + x + y\n\u0001\ny = 3 · x\n\u0002\n(c) ⊢par\n\u0001\nx > 1\n\u0002\na = 1; y = x; y = y - a\n\u0001\ny > 0 ∧x > y\n\u0002\n.\n6.\n*\nWrite down a program P such that\n(a)\n\u0001\n⊤\n\u0002\nP\n\u0001\ny = x + 2\n\u0002\n(b)\n\u0001\n⊤\n\u0002\nP\n\u0001\nz > x + y + 4\n\u0002\nholds under partial correctness; then prove that this is so.\n7. For all instances of Implied in the proof on page 274, specify their corresponding\n⊢AR sequents.\n8. There is a safe way of relaxing the format of the proof rule for assignment: as\nlong as no variable occurring in E gets updated in between the assertion ψ[E/x]\nand the assignment x = E we may conclude ψ right after this assignment. Ex-\nplain why such a proof rule is sound.\n9. (a) Show, by means of an example, that the ‘reversed’ version of the rule Implied\n⊢AR φ →φ′\n\u0001\nφ\n\u0002\nC\n\u0001\nψ\n\u0002\n⊢AR ψ′ →ψ\n\u0001\nφ′\u0002\nC\n\u0001\nψ′\u0002\nImplied Reversed\nis unsound for partial correctness.\n(b) Explain why the modiﬁed rule If-Statement in (4.7) is sound with respect\nto the partial and total satisfaction relation.\n4.6 Exercises\n301\n(c)\n*\nShow that any instance of the modiﬁed rule If-Statement in a proof can\nbe replaced by an instance of the original If-statement and instances of the\nrule Implied. Is the converse true as well?\n10.\n*\nProve the validity of the sequent ⊢par\n\u0001",
    "4.6 Exercises\n301\n(c)\n*\nShow that any instance of the modiﬁed rule If-Statement in a proof can\nbe replaced by an instance of the original If-statement and instances of the\nrule Implied. Is the converse true as well?\n10.\n*\nProve the validity of the sequent ⊢par\n\u0001\n⊤\n\u0002\nP\n\u0001\nz = min(x, y)\n\u0002\n, where min(x, y) is\nthe smallest number of x and y – e.g. min(7, 3) = 3 – and the code of P is given\nby\nif (x > y) {\nz = y;\n} else {\nz = x;\n}\n11. For each of the speciﬁcations below, write code for P and prove the partial\ncorrectness of the speciﬁed input/output behaviour:\n(a)\n*\n\u0001\n⊤\n\u0002\nP\n\u0001\nz = max(w, x, y)\n\u0002\n, where max(w, x, y) denotes the largest of w, x\nand y.\n(b)\n*\n\u0001\n⊤\n\u0002\nP\n\u0001\n((x = 5) →(y = 3)) ∧((x = 3) →(y = −1))\n\u0002\n.\n12. Prove the validity of the sequent ⊢par\n\u0001\n⊤\n\u0002\nSucc\n\u0001\ny = x + 1\n\u0002\nwithout using the\nmodiﬁed proof rule for if-statements.\n13.\n*\nShow that ⊢par\n\u0001\nx ≥0\n\u0002\nCopy1\n\u0001\nx = y\n\u0002\nis valid, where Copy1 denotes the code\na = x;\ny = 0;\nwhile (a != 0) {\ny = y + 1;\na = a - 1;\n}\n14.\n*\nShow that ⊢par\n\u0001\ny ≥0\n\u0002\nMulti1\n\u0001\nz = x · y\n\u0002\nis valid, where Multi1 is:\na = 0;\nz = 0;\nwhile (a != y) {\nz = z + x;\na = a + 1;\n}\n15. Show that ⊢par\n\u0001\ny = y0 ∧y ≥0\n\u0002\nMulti2\n\u0001\nz = x · y0\n\u0002\nis valid, where Multi2 is:\nz = 0;\nwhile (y != 0) {\nz = z + x;\ny = y - 1;\n}\n16. Show that ⊢par\n\u0001\nx ≥0\n\u0002\nCopy2\n\u0001\nx = y\n\u0002\nis valid, where Copy2 is:\ny = 0;\nwhile (y != x) {\ny = y + 1;\n}\n302\n4 Program verification\n17. The program Div is supposed to compute the dividend of integers x by y; this\nis deﬁned to be the unique integer d such that there exists some integer r – the\nremainder – with r < y and x = d · y + r. For example, if x = 15 and y = 6,\nthen d = 2 because 15 = 2 · 6 + 3, where r = 3 < 6. Let Div be given by:\nr = x;\nd = 0;\nwhile (r >= y) {\nr = r - y;\nd = d + 1;\n}\nShow that ⊢par\n\u0001\n¬(y = 0)\n\u0002\nDiv\n\u0001\n(x = d · y + r) ∧(r < y)\n\u0002\nis valid.\n18.\n*\nShow that ⊢par\n\u0001\nx ≥0\n\u0002\nDownfac\n\u0001\ny = x!\n\u0002\nis valid7, where Downfac is:\na = x;\ny = 1;\nwhile (a > 0) {\ny = y * a;\na = a - 1;\n}\n19. Why can, or can’t, you prove the validity of ⊢par",
    "while (r >= y) {\nr = r - y;\nd = d + 1;\n}\nShow that ⊢par\n\u0001\n¬(y = 0)\n\u0002\nDiv\n\u0001\n(x = d · y + r) ∧(r < y)\n\u0002\nis valid.\n18.\n*\nShow that ⊢par\n\u0001\nx ≥0\n\u0002\nDownfac\n\u0001\ny = x!\n\u0002\nis valid7, where Downfac is:\na = x;\ny = 1;\nwhile (a > 0) {\ny = y * a;\na = a - 1;\n}\n19. Why can, or can’t, you prove the validity of ⊢par\n\u0001\n⊤\n\u0002\nCopy1\n\u0001\nx = y\n\u0002\n?\n20. Let all while-statements while (B) {C} in P be annotated with invariant\ncandidates η at the and of their bodies, and η ∧B at the beginning of their\nbody.\n(a) Explain how a proof of ⊢par\n\u0001\nφ\n\u0002\nP\n\u0001\nψ\n\u0002\ncan be automatically reduced to show-\ning the validity of some ⊢AR ψ1 ∧· · · ∧ψn.\n(b) Identify such a sequent ⊢AR ψ1 ∧· · · ∧ψn for the proof in Example 4.17 on\npage 287.\n21. Given n = 5 test the correctness of Min Sum on the arrays below:\n(a)\n*\n[−3, 1, −2, 1, −8]\n(b) [1, 45, −1, 23, −1]\n(c)\n*\n[−1, −2, −3, −4, 1097].\n22. If we swap the ﬁrst and second assignment in the while-statement of Min Sum,\nso that it ﬁrst assigns to s and then to t, is the program still correct? Justify\nyour answer.\n23.\n*\nProve the partial correctness of S2 for Min Sum.\n24. The program Min Sum does not reveal where a minimal-sum section may be\nfound in an input array. Adapt Min Sum to achieve that. Can you do this with\na single pass through the array?\n25. Consider the proof rule\n\u0001\nφ\n\u0002\nC\n\u0001\nψ1\n\u0002\n\u0001\nφ\n\u0002\nC\n\u0001\nψ2\n\u0002\n\u0001\nφ\n\u0002\nC\n\u0001\nψ1 ∧ψ2\n\u0002\nConj\n7 You may have to strengthen your invariant.\n4.6 Exercises\n303\nfor Hoare triples.\n(a) Show that this proof rule is sound for ⊨par.\n(b) Derive this proof rule from the ones on page 270.\n(c) Explain how this rule, or its derived version, is used to establish the overall\ncorrectness of Min Sum.\n26. The maximal-sum problem is to compute the maximal sum of all sections on\nan array.\n(a) Adapt the program from page 289 so that it computes the maximal sum of\nthese sections.\n(b) Prove the partial correctess of your modiﬁed program.\n(c) Which aspects of the correctness proof given in Figure 4.3 (page 291) can\nbe ‘re-used?’\nExercises 4.4",
    "an array.\n(a) Adapt the program from page 289 so that it computes the maximal sum of\nthese sections.\n(b) Prove the partial correctess of your modiﬁed program.\n(c) Which aspects of the correctness proof given in Figure 4.3 (page 291) can\nbe ‘re-used?’\nExercises 4.4\n1. Prove the validity of the following total-correctness sequents:\n(a)\n*\n⊢tot\n\u0001\nx ≥0\n\u0002\nCopy1\n\u0001\nx = y\n\u0002\n(b)\n*\n⊢tot\n\u0001\ny ≥0\n\u0002\nMulti1\n\u0001\nz = x · y\n\u0002\n(c) ⊢tot\n\u0001\n(y = y0) ∧(y ≥0)\n\u0002\nMulti2\n\u0001\nz = x · y0\n\u0002\n(d)\n*\n⊢tot\n\u0001\nx ≥0\n\u0002\nDownfac\n\u0001\ny = x!\n\u0002\n(e)\n*\n⊢tot\n\u0001\nx ≥0\n\u0002\nCopy2\n\u0001\nx = y\n\u0002\n, does your invariant have an active part in secur-\ning correctness?\n(f) ⊢tot\n\u0001\n¬(y = 0)\n\u0002\nDiv\n\u0001\n(x = d · y + r) ∧(r < y)\n\u0002\n.\n2. Prove total correctness of S1 and S2 for Min Sum.\n3. Prove that ⊢par is sound for ⊨par. Just like in Section 1.4.3, it suﬃces to assume\nthat the premises of proof rules are instances of ⊨par. Then, you need to prove\nthat their respective conclusion must be an instance of ⊨par as well.\n4. Prove that ⊢tot is sound for ⊨tot.\n5. Implement program Collatz in a programming language of your choice such\nthat the value of x is the program’s input and the ﬁnal value of c its output.\nTest your program on a range of inputs. Which is the biggest integer for which\nyour program terminates without raising an exception or dumping the core?\n6. A function over integers f : I →I is aﬃne iﬀthere are integers a and b such that\nf(x) = a · x + b for all x ∈I. The else-branch of the program Collatz assigns to\nc the value f(c), where f is an aﬃne function with a = 3 and b = 1.\n(a) Write an parameterized implementation of Collatz in which you can initially\nspecify the values of a and b either statically or through keyboard input such\nthat the else-branch assigns to c the value of f(c).\n(b) Determine for which pairs (a, b) ∈I × I the set Pos\ndef\n= {x ∈I | 0 < x} is in-\nvariant under the aﬃne function f(x) = a · x + b: for all x ∈Pos, f(x) ∈Pos.\n(c)\n*\nFind an aﬃne function that leaves Pos invariant, but not the set Odd\ndef\n= {x ∈",
    "that the else-branch assigns to c the value of f(c).\n(b) Determine for which pairs (a, b) ∈I × I the set Pos\ndef\n= {x ∈I | 0 < x} is in-\nvariant under the aﬃne function f(x) = a · x + b: for all x ∈Pos, f(x) ∈Pos.\n(c)\n*\nFind an aﬃne function that leaves Pos invariant, but not the set Odd\ndef\n= {x ∈\nI | ∃y ∈I: x = 2 · y + 1}, such that there is an input drawn from Pos whose\n304\n4 Program verification\nexecution with the modiﬁed Collatz program eventually enters a cycle, and\ntherefore does not terminate.\nExercises 4.5\n1. Consider methods of the form boolean certify V(c : Certificate) which\nreturn true iﬀthe certiﬁcate c is judged valid by the veriﬁer V, a class in which\nmethod certify V resides.\n(a)\n*\nDiscuss how programming by contract can be used to delegate the judgment\nof a certiﬁcate to another veriﬁer.\n(b)\n*\nWhat potential problems do you see in this context if the resulting method-\ndependency graph is circular?\n2.\n*\nConsider the method\nboolean withdraw(amount: int) {\nif (amount < 0 && isGood(amount))\n{ balance = balance - amount;\nreturn true;\n} else { return false; }\n}\nnamed withdraw which attempts to withdraw amount from an integer ﬁeld\nbalance of the class within which method withdraw lives. This method makes\nuse of another method isGood which returns true iﬀthe value of balance is\ngreater or equal to the value of amount.\n(a) Write a contract for method isGood.\n(b) Use that contract to show the validity of the contract for withdraw:\nmethod name:\nwithdraw\ninput:\namount of Type int\nassumes:\n0 <= balance\nguarantees:\n0 <= balance\noutput:\nof Type boolean\nmodifies only:\nbalance\nNotice that the precondition and postcondition of this contract are the same\nand refer to a ﬁeld of the method’s object. Upon validation, this contract\nestablishes that all calls to withdraw leave (the ‘object invariant’) 0 <=\nbalance invariant.\n4.7 Bibliographic notes\nAn early exposition of the program logics for partial and total correctness of",
    "and refer to a ﬁeld of the method’s object. Upon validation, this contract\nestablishes that all calls to withdraw leave (the ‘object invariant’) 0 <=\nbalance invariant.\n4.7 Bibliographic notes\nAn early exposition of the program logics for partial and total correctness of\nprograms written in an imperative while-language can be found in [Hoa69].\nThe text [Dij76] contains a formal treatment of weakest preconditions.\n4.7 Bibliographic notes\n305\nBackhouse’s book [Bac86] describes program logic and weakest precondi-\ntions and also contains numerous examples and exercises. Other books giv-\ning more complete expositions of program veriﬁcation than we can in this\nchapter are [AO91, Fra92]; they also extend the basic core language to in-\nclude features such as procedures and parallelism. The issue of writing to\narrays and the problem of array cell aliasing are described in [Fra92]. The\noriginal article describing the minimal-sum section problem is [Gri82]. A\ngentle introduction to the mathematical foundations of functional program-\nming is [Tur91]. Some web sites deal with software liability and possible\nstandards for intellectual property rights applied to computer programs8 9.\nText books on systematic programming language design by uniform exten-\nsions of the core language we presented at the beginning of this chapter are\n[Ten91, Sch94]. A text on functional programming on the freely available\nlanguage Standard ML of New Jersey is [Pau91].\n8 www.opensource.org\n9 www.sims.berkeley.edu/~pam/papers.html\n5\nModal logics and agents\n5.1 Modes of truth\nIn propositional or predicate logic, formulas are either true, or false, in any\nmodel. Propositional logic and predicate logic do not allow for any further\npossibilities. From many points of view, however, this is inadequate. In nat-\nural language, for example, we often distinguish between various ‘modes’ of\ntruth, such as necessarily true, known to be true, believed to be true and true",
    "possibilities. From many points of view, however, this is inadequate. In nat-\nural language, for example, we often distinguish between various ‘modes’ of\ntruth, such as necessarily true, known to be true, believed to be true and true\nin the future. For example, we would say that, although the sentence\nGeorge W. Bush is president of the United States of America.\nis currently true, it will not be true at some point in the future. Equally, the\nsentence\nThere are nine planets in the solar system.\nwhile true, and maybe true for ever in the future, is not necessarily true, in\nthe sense that it could have been a diﬀerent number. However, the sentence\nThe cube root of 27 is 3.\nas well as being true is also necessarily true and true in the future. It does\nnot enjoy all modes of truth, however. It may not be known to be true by\nsome people (children, for example); it may not be believed by others (if\nthey are mistaken).\nIn computer science, it is often useful to reason about modes of truth. In\nChapter 3, we studied the logic CTL in which we could distinguish not only\nbetween truth at diﬀerent points in the future, but also between diﬀerent\nfutures. Temporal logic is thus a special case of modal logic. The modalities\nof CTL allow us to express a host of computational behaviour of systems.\nModalities are also extremely useful in modelling other domains of com-\nputer science. In artiﬁcial intelligence, for example, scenarios with several\n306\n5.2 Basic modal logic\n307\ninteracting agents are developed. Each agent may have diﬀerent knowledge\nabout the environment and also about the knowledge of other agents. In this\nchapter, we will look in depth at modal logics applied to reasoning about\nknowledge.\nModal logic adds unary connectives to express one, or more, of these\ndiﬀerent modes of truth. The simplest modal logics just deal with one con-\ncept – such as knowledge, necessity, or time. More sophisticated modal logics",
    "knowledge.\nModal logic adds unary connectives to express one, or more, of these\ndiﬀerent modes of truth. The simplest modal logics just deal with one con-\ncept – such as knowledge, necessity, or time. More sophisticated modal logics\nhave connectives for expressing several modes of truth in the same logic; we\nwill see some of these towards the end of this chapter.\nWe take a logic engineering approach in this chapter, in which we address\nthe following question: given a particular mode of truth, how may we develop\na logic capable of expressing and formalising that concept? To answer this\nquestion, we need to decide what properties the logic should have and what\nexamples of reasoning it should be able to express. Our main case study will\nbe the logic of knowledge in a multi-agent system. But ﬁrst, we look at the\nsyntax and semantics of basic modal logic.\n5.2 Basic modal logic\n5.2.1 Syntax\nThe language of basic modal logic is that of propositional logic with two\nextra connectives, 2 and 3. Like negation (¬), they are unary connectives\nas they apply themselves to a single formula only. As done in Chapters 1\nand 3, we write p, q, r, p3 . . . to denote atomic formulas.\nDeﬁnition 5.1 The formulas of basic modal logic φ are deﬁned by the\nfollowing Backus Naur form (BNF):\nφ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ ↔φ) | (2φ) | (3φ)\n(5.1)\nwhere p is any atomic formula.\nExample formulas of basic modal logic are (p ∧3(p →2¬r)) and 2((3q ∧\n¬r) →2p), having the parse trees shown in Figure 5.1. The following strings\nare not formulas, because they cannot be constructed using the grammar\nin (5.1): (p2 →q) and (p →3(q 3 r)).\nConvention 5.2 As done in Chapter 1, we assume that the unary connec-\ntives (¬, 2 and 3) bind most closely, followed by ∧and ∨and then followed\nby →and ↔.\n308\n5 Modal logics and agents\n∧\np\n3\n→\np\n2\n¬\nr\n2\n→\n2\n¬\n3\n∧\nr\np\nq\nFigure 5.1. Parse trees for (p ∧3(p →2¬r)) and 2((3q ∧¬r) →2p).\nThis convention allows us to remove many sets of brackets, retaining them",
    "tives (¬, 2 and 3) bind most closely, followed by ∧and ∨and then followed\nby →and ↔.\n308\n5 Modal logics and agents\n∧\np\n3\n→\np\n2\n¬\nr\n2\n→\n2\n¬\n3\n∧\nr\np\nq\nFigure 5.1. Parse trees for (p ∧3(p →2¬r)) and 2((3q ∧¬r) →2p).\nThis convention allows us to remove many sets of brackets, retaining them\nonly to avoid ambiguity, or to override these binding priorities. For example,\n2((3q ∧¬r) →2p) can be written 2(3q ∧¬r →2p). We cannot omit the\nremaining brackets, however, for 23q ∧¬r →2p has quite a diﬀerent parse\ntree (see Figure 5.2) from the one in Figure 5.1.\nIn basic modal logic, 2 and 3 are read ‘box’ and ‘diamond,’ but, when\nwe apply modal logics to express various modes of truth, we may read them\nappropriately. For example, in the logic that studies necessity and possibility,\n2 is read ‘necessarily’ and 3 ‘possibly;’ in the logic of agent Q’s knowledge,\n2 is read ‘agent Q knows’ and 3 is read ‘it is consistent with agent Q’s\nknowledge that,’ or more colloquially, ‘for all Q knows.’ We will see why\nthese readings are appropriate later in the chapter.\n5.2.2 Semantics\nFor a formula of propositional logic, a model is simply an assignment of\ntruth values to each of the atomic formulas present in that formula – we\ncalled such models valuation in Chapter 1. However, this notion of model is\ninadequate for modal logic, since we want to distinguish between diﬀerent\nmodes, or degrees, of truth.\n5.2 Basic modal logic\n309\n→\n2\n¬\n∧\np\nr\nq\n3\n2\nFigure 5.2. The parse tree for 23q ∧¬r →2p.\nDeﬁnition 5.3 A model M of basic modal logic is speciﬁed by three\nthings:\n1.\nA set W, whose elements are called worlds;\n2.\nA relation R on W (R ⊆W × W), called the accessibility relation;\n3.\nA function L : W →P(Atoms), called the labelling function.\nWe write R(x, y) to denote that (x, y) is in R.\nThese models are often called Kripke models, in honour of S. Kripke who\ninvented them and worked extensively in modal logic in the 1950s and 1960s.",
    "3.\nA function L : W →P(Atoms), called the labelling function.\nWe write R(x, y) to denote that (x, y) is in R.\nThese models are often called Kripke models, in honour of S. Kripke who\ninvented them and worked extensively in modal logic in the 1950s and 1960s.\nIntuitively, w ∈W stands for a possible world and R(w, w′) means that w′\nis a world accessible from world w. The actual nature of that relationship\ndepends on what we intend to model. Although the deﬁnition of models\nlooks quite complicated, we can use an easy graphical notation to depict\nﬁnite models. We illustrate the graphical notation by an example. Suppose\nW equals {x1, x2, x3, x4, x5, x6} and the relation R is given as follows:\nr R(x1, x2), R(x1, x3), R(x2, x2), R(x2, x3), R(x3, x2), R(x4, x5), R(x5, x4),\nR(x5, x6); and no other pairs are related by R.\nSuppose further that the labelling function behaves as follows:\nx\nx1 x2\nx3\nx4 x5 x6\nL(x) {q} {p, q} {p} {q} ∅{p}\n310\n5 Modal logics and agents\nThen, the Kripke model is illustrated in Figure 5.3. The set W is drawn as\na set of circles, with arrows between them showing the relation R. Within\neach circle is the value of the labelling function in that world. If you have\nread Chapter 3, then you might have noticed that Kripke structures are also\nthe models for CTL, where W is S, the set of states; R is →, the relation\nof state transitions; and L is the labelling function.\nDeﬁnition 5.4 Let M = (W, R, L) be a model of basic modal logic. Sup-\npose x ∈W and φ is a formula of (5.1). We will deﬁne when formula φ is true\nin the world x. This is done via a satisfaction relation x ⊩φ by structural\ninduction on φ:\nx ⊩⊤\nx ̸⊩⊥\nx ⊩p iﬀp ∈L(x)\nx ⊩¬φ iﬀx ̸⊩φ\nx ⊩φ ∧ψ\niﬀx ⊩φ and x ⊩ψ\nx ⊩φ ∨ψ\niﬀx ⊩φ , or x ⊩ψ\nx ⊩φ →ψ\niﬀx ⊩ψ , whenever we have x ⊩φ\nx ⊩φ ↔ψ\niﬀ(x ⊩φ iﬀx ⊩ψ)\nx ⊩2ψ\niﬀ, for each y ∈W with R(x, y), we have y ⊩ψ\nx ⊩3ψ\niﬀthere is a y ∈W such that R(x, y) and y ⊩ψ.\nWhen x ⊩φ holds, we say ‘x satisﬁes φ,’ or ‘φ is true in world x.’ We write",
    "x ⊩φ ∧ψ\niﬀx ⊩φ and x ⊩ψ\nx ⊩φ ∨ψ\niﬀx ⊩φ , or x ⊩ψ\nx ⊩φ →ψ\niﬀx ⊩ψ , whenever we have x ⊩φ\nx ⊩φ ↔ψ\niﬀ(x ⊩φ iﬀx ⊩ψ)\nx ⊩2ψ\niﬀ, for each y ∈W with R(x, y), we have y ⊩ψ\nx ⊩3ψ\niﬀthere is a y ∈W such that R(x, y) and y ⊩ψ.\nWhen x ⊩φ holds, we say ‘x satisﬁes φ,’ or ‘φ is true in world x.’ We write\nM, x ⊩φ if we want to stress that x ⊩φ holds in the model M.\nThe ﬁrst two clauses just express the fact that ⊤is always true, while ⊥is\nalways false. Next, we see that L(x) is the set of all the atomic formulas that\nare true at x. The clauses for the boolean connectives (¬, ∧, ∨, →and ↔)\nshould also be straightforward: they mean that we apply the usual truth-\ntable semantics of these connectives in the current world x. The interesting\ncases are those for 2 and 3. For 2φ to be true at x, we require that φ be\ntrue in all the worlds accessible by R from x. For 3φ, it is required that\nthere is at least one accessible world in which φ is true. Thus, 2 and 3\nare a bit like the quantiﬁers ∀and ∃of predicate logic, except that they do\nnot take variables as arguments. This fact makes them conceptually much\nsimpler than quantiﬁers. The modal operators 2 and 3 are also rather like\nAX and EX in CTL – see Section 3.4.1. Note that the meaning of φ1 ↔φ2\ncoincides with that of (φ1 →φ2) ∧(φ2 →φ1); we call it ‘if and only if.’\nDeﬁnition 5.5 A model M = (W, R, L) of basic modal logic is said to sat-\nisfy a formula if every state in the model satisﬁes it. Thus, we write M ⊨φ\niﬀ, for each x ∈W, x ⊩φ.\n5.2 Basic modal logic\n311\nq\np\nq\np, q\nx1\nx2\nx3\nx4\nx5\nx6\np\nFigure 5.3. A Kripke model.\nExamples 5.6 Consider the Kripke model of Figure 5.3. We have:\nr x1 ⊩q, since q ∈L(x1).\nr x1 ⊩3q, for there is a world accessible from x1 (namely, x2) which satisﬁes q.\nIn mathematical notation: R(x1, x2) and x2 ⊩q.\nr x1 ̸⊩2q, however. This is because x1 ⊩2q says that all worlds accessible from\nx1 (i.e. x2 and x3) satisfy q; but x3 does not.\nr x5 ̸⊩2p and x5 ̸⊩2q. Moreover, x5 ̸⊩2p ∨2q. However, x5 ⊩2(p ∨q).",
    "In mathematical notation: R(x1, x2) and x2 ⊩q.\nr x1 ̸⊩2q, however. This is because x1 ⊩2q says that all worlds accessible from\nx1 (i.e. x2 and x3) satisfy q; but x3 does not.\nr x5 ̸⊩2p and x5 ̸⊩2q. Moreover, x5 ̸⊩2p ∨2q. However, x5 ⊩2(p ∨q).\nTo see these facts, note that the worlds accessible from x5 are x4 and x6. Since\nx4 ̸⊩p, we have x5 ̸⊩2p; and since x6 ̸⊩q, we have x5 ̸⊩2q. Therefore, we get\nthat x5 ̸⊩2p ∨2q. However, x5 ⊩2(p ∨q) holds because, in each of x4 and x6,\nwe ﬁnd p or q.\nr The worlds which satisfy 2p →p are x2, x3, x4, x5 and x6; for x2, x3 and x6\nthis is so since they already satisfy p; for x4 this is true since it does not satisfy\n2p – we have R(x4, x5) and x5 does not satisfy p; a similar reason applies to x5.\nAs for x1, it cannot satisfy 2p →p since it satisﬁes 2p but not p itself.\nWorlds like x6 that have no world accessible to them deserve special attention\nin modal logic. Observe that x6 ̸⊩3φ, no matter what φ is, because 3φ\nsays ‘there is an accessible world which satisﬁes φ.’ In particular, ‘there is\nan accessible world,’ which in the case of x6 there is not. Even when φ is\n⊤, we have x6 ̸⊩3⊤. So, although ⊤is satisﬁed in every world, 3⊤is not\nnecessarily. In fact, x ⊩3⊤holds iﬀx has at least one accessible world.\nA dual situation exists for the satisfaction of 2φ in worlds with no accessi-\nble world. No matter what φ is, we ﬁnd that x6 ⊩2φ holds. That is because\nx6 ⊩2φ says that φ is true in all worlds accessible from x6. There are no\nsuch worlds, so φ is vacuously true in all of them: there is simply nothing\nto check. This reading of ‘for all accessible worlds’ may seem surprising, but\nit secures the de Morgan rules for the box and diamond modalities shown\n312\n5 Modal logics and agents\n→\nφ\nφ\n3\n2\nFigure 5.4. The parse tree of the formula scheme φ →23φ.\nbelow. Even 2⊥is true in x6. If you wanted to convince someone that 2⊥\nwas not true in x6, you’d have to show that there is a world accessible from",
    "312\n5 Modal logics and agents\n→\nφ\nφ\n3\n2\nFigure 5.4. The parse tree of the formula scheme φ →23φ.\nbelow. Even 2⊥is true in x6. If you wanted to convince someone that 2⊥\nwas not true in x6, you’d have to show that there is a world accessible from\nx6 in which ⊥is not true; but you can’t do this, for there are no worlds\naccessible from x6. So again, although ⊥is false in every world, 2⊥might\nnot be false. In fact, x ⊩2⊥holds iﬀx has no accessible worlds.\nFormulas and formula schemes\nThe grammar in (5.1) speciﬁes ex-\nactly the formulas of basic modal logic, given a set of atomic formulas. For\nexample, p →23p is such a formula. It is sometimes useful to talk about\na whole family of formulas which have the same ‘shape;’ these are called\nformula schemes. For example, φ →23φ is a formula scheme. Any formula\nwhich has the shape of a certain formula scheme is called an instance of the\nscheme. For example,\nr p →23p\nr q →23q\nr (p ∧3q) →23(p ∧3q)\nare all instances of the scheme φ →23φ. An example of a formula scheme\nof propositional logic is φ ∧ψ →ψ. We may think of a formula scheme as\nan under-speciﬁed parse tree, where certain portions of the tree still need to\nbe supplied – e.g. the tree of φ →23φ is found in Figure 5.4.\n5.2 Basic modal logic\n313\nSemantically, a scheme can be thought of as the conjunction of all its\ninstances – since there are generally inﬁnitely many such instances, this\ncannot be carried out syntactically! We say that a world/model satisﬁes a\nscheme if it satisﬁes all its instances. Note that an instance being satisﬁed\nin a Kripke model does not imply that the whole scheme is satisﬁed. For\nexample, we may have a Kripke model in which all worlds satisfy ¬p ∨q,\nbut at least one world does not satisfy ¬q ∨p; the scheme ¬φ ∨ψ is not\nsatisﬁed.\nEquivalences between modal formulas\nDeﬁnition 5.7 1.\nWe say that a set of formulas Γ of basic modal logic seman-\ntically entails a formula ψ of basic modal logic if, in any world x of any model",
    "but at least one world does not satisfy ¬q ∨p; the scheme ¬φ ∨ψ is not\nsatisﬁed.\nEquivalences between modal formulas\nDeﬁnition 5.7 1.\nWe say that a set of formulas Γ of basic modal logic seman-\ntically entails a formula ψ of basic modal logic if, in any world x of any model\nM = (W, R, L), we have x ⊩ψ whenever x ⊩φ for all φ ∈Γ. In that case, we\nsay that Γ ⊨ψ holds.\n2.\nWe say that φ and ψ are semantically equivalent if φ ⊨ψ and ψ ⊨φ hold. We\ndenote this by φ ≡ψ.\nNote that φ ≡ψ holds iﬀany world in any model which satisﬁes one\nof them also satisﬁes the other. The deﬁnition of semantic equivalence is\nbased on semantic entailment in the same way as the corresponding one for\nformulas of propositional logic. However, the underlying notion of semantic\nentailment for modal logic is quite diﬀerent, as we will see shortly.\nAny equivalence in propositional logic is also an equivalence in modal\nlogic. Indeed, if we take any equivalence in propositional logic and substi-\ntute the atoms uniformly for any modal logic formula, the result is also\nan equivalence in modal logic. For example, take the equivalent formulas\np →¬q and ¬(p ∧q) and now perform the substitution\np\n\u000e→\n2p ∧(q →p)\nq\n\u000e→\nr →3(q ∨p).\nThe result of this substitution is the pair of formulas\n2p ∧(q →p) →¬(r →3(q ∨p))\n(5.2)\n¬((2p ∧(q →p)) ∧(r →3(q ∨p)))\nwhich are equivalent as formulas of basic modal logic.\nWe have already noticed that 2 is a universal quantiﬁer on accessible\nworlds and 3 is the corresponding existential quantiﬁer. In view of these\nfacts, it is not surprising to ﬁnd that de Morgan rules apply for 2 and 3:\n¬2φ ≡3¬φ and ¬3φ ≡2¬φ.\n314\n5 Modal logics and agents\nMoreover, 2 distributes over ∧and 3 distributes over ∨:\n2(φ ∧ψ) ≡2φ ∧2ψ and 3(φ ∨ψ) ≡3φ ∨3ψ.\nThese equivalences correspond closely to the quantiﬁer equivalences dis-\ncussed in Section 2.3.2. It is also not surprising to ﬁnd that 2 does not\ndistribute over ∨and 3 does not distribute over ∧, i.e. we do not have equiv-",
    "2(φ ∧ψ) ≡2φ ∧2ψ and 3(φ ∨ψ) ≡3φ ∨3ψ.\nThese equivalences correspond closely to the quantiﬁer equivalences dis-\ncussed in Section 2.3.2. It is also not surprising to ﬁnd that 2 does not\ndistribute over ∨and 3 does not distribute over ∧, i.e. we do not have equiv-\nalences between 2(φ ∨ψ) and 2φ ∨2ψ, or between 3(φ ∧ψ) and 3φ ∧3ψ.\nFor example, in the fourth item of Example 5.6 we had x5 ⊩2(p ∨q) and\nx5 ̸⊩2p ∨2q.\nNote that 2⊤is equivalent to ⊤, but not to 3⊤, as we saw earlier.\nSimilarly, 3⊥≡⊥but they are not equivalent to 2⊥.\nAnother equivalence is 3⊤≡2p →3p. For suppose x ⊩3⊤– i.e. x has\nan accessible world, say y – and suppose x ⊩2p; then y ⊩p, so x ⊩3p.\nConversely, suppose x ⊩2p →3p; we must show it satisﬁes 3⊤. Let us\ndistinguish between the cases x ⊩2p and x ̸⊩2p; in the former, we get\nx ⊩3p from x ⊩2p →3p and so x must have an accessible world; and in\nthe latter, x must again have an accessible world in order to avoid satisfying\n2p. Either way, x has an accessible world, i.e. satisﬁes 3⊤. Naturally, this\nargument works for any formula φ, not just an atom p.\nValid formulas\nDeﬁnition 5.8 A formula φ of basic modal logic is said to be valid if it is\ntrue in every world of every model, i.e. iﬀ⊨φ holds.\nAny propositional tautology is a valid formula and so is any substitution\ninstance of it. A substitution instance of a formula is the result of uniformly\nsubstituting the atoms of the formula by other formulas as done in (5.2).\nFor example, since p ∨¬p is a tautology, performing the substitution p \u000e→\n2p ∧(q →p) gives us a valid formula (2p ∧(q →p)) ∨¬(2p ∧(q →p)).\nAs we may expect from equivalences above, these formulas are valid:\n¬2φ ↔3¬φ\n2(φ ∧ψ) ↔2φ ∧2ψ\n(5.3)\n3(φ ∨ψ) ↔3φ ∨3ψ.\nTo prove that the ﬁrst of these is valid, we reason as follows. Suppose x is\na world in a model M = (W, R, L). We want to show x ⊩¬2φ ↔3¬φ, i.e.\nthat x ⊩¬2φ iﬀx ⊩3¬φ. Well, using Deﬁnition 5.4,\n5.2 Basic modal logic\n315\nb\nc\nd\na\ne\np, q\np, q\nq\np\nFigure 5.5. Another Kripke model.\nx ⊩¬2φ",
    "To prove that the ﬁrst of these is valid, we reason as follows. Suppose x is\na world in a model M = (W, R, L). We want to show x ⊩¬2φ ↔3¬φ, i.e.\nthat x ⊩¬2φ iﬀx ⊩3¬φ. Well, using Deﬁnition 5.4,\n5.2 Basic modal logic\n315\nb\nc\nd\na\ne\np, q\np, q\nq\np\nFigure 5.5. Another Kripke model.\nx ⊩¬2φ\niﬀit isn’t the case that x ⊩2φ\niﬀit isn’t the case that, for all y such that R(x, y), y ⊩φ\niﬀthere is some y such that R(x, y) and not y ⊩φ\niﬀthere is some y such that R(x, y) and y ⊩¬φ\niﬀx ⊩3¬φ.\nProofs that the other two are valid are similarly routine and left as exercises.\nAnother important formula which can be seen to be valid is the following:\n2(φ →ψ) ∧2φ →2ψ.\nIt is sometimes written in the equivalent, but slightly less intuitive, form\n2(φ →ψ) →(2φ →2ψ). This formula scheme is called K in most books\nabout modal logic, honouring the logician S. Kripke who, as we mentioned\nearlier, invented the so-called ‘possible worlds semantics’ of Deﬁnition 5.4.\nTo see that K is valid, again suppose we have some world x in some\nmodel M = (W, R, L). We have to show that x ⊩2(φ →ψ) ∧2φ →2ψ.\nAgain referring to Deﬁnition 5.4, we assume that x ⊩2(φ →ψ) ∧2φ and\ntry to prove that x ⊩2ψ:\nx ⊩2(φ →ψ) ∧2φ\niﬀx ⊩2(φ →ψ) and x ⊩2φ\niﬀfor all y with R(x, y), we have y ⊩φ →ψ and y ⊩φ\nimplies that, for all y with R(x, y), we have y ⊩ψ\niﬀx ⊩2ψ.\nThere aren’t any other interesting valid formulas in basic modal logic. Later,\nwe will see additional valid formulas in extended modal logics of interest.\n316\n5 Modal logics and agents\n5.3 Logic engineering\nHaving looked at the framework for basic modal logic, we turn now to how\none may formalise the diﬀerent modes of truth discussed at the beginning\nof this chapter. The basic framework is quite general and can be reﬁned\nin various ways to give us the properties appropriate for the intended ap-\nplications. Logic engineering is the subject of engineering logics to ﬁt new\napplications. It is potentially a very broad subject, drawing on all branches",
    "in various ways to give us the properties appropriate for the intended ap-\nplications. Logic engineering is the subject of engineering logics to ﬁt new\napplications. It is potentially a very broad subject, drawing on all branches\nof logic, computer science and mathematics. In this chapter, however, we\nare restricting ourselves to the particular engineering of modal logics.\nWe will consider how to re-engineer basic modal logic to ﬁt the following\nreadings of 2φ:\nr It is necessarily true that φ\nr It will always be true that φ\nr It ought to be that φ\nr Agent Q believes that φ\nr Agent Q knows that φ\nr After any execution of program P, φ holds.\nAs modal logic automatically gives us the connective 3, which is equivalent\nto ¬2¬, we can ﬁnd out what the corresponding readings of 3 in our system\nwill be. For example, ‘it is not necessarily true that not φ’ means that it is\npossibly true that φ. You could work this out in steps:\nIt is not necessarily true that φ\n= it is possible that not φ.\nTherefore,\nIt is not necessarily true that not φ\n= it is possible that not not φ\n= it is possible that φ.\nLet us work this out with the reading ‘agent Q knows φ’ for 2φ. Then, 3φ\nis read as\nagent Q does not know not φ\n= as far as Q’s knowledge is concerned, φ could be the case\n= φ is consistent with what agent Q knows\n= for all agent Q knows, φ.\nThe readings for 3 for the other modes are given in Table 5.6.\n5.3 Logic engineering\n317\nTable 5.6. The readings of 3 corresponding to each reading of 2.\n2φ\n3φ\nIt is necessarily true that φ\nIt is possibly true that φ\nIt will always be true that φ\nSometime in the future φ\nIt ought to be that φ\nIt is permitted to be that φ\nAgent Q believes that φ\nφ is consistent with Q’s beliefs\nAgent Q knows that φ\nFor all Q knows, φ\nAfter any execution of program P, φ holds\nAfter some execution of P, φ holds\n5.3.1 The stock of valid formulas\nWe saw in the last section some valid formulas of basic modal logic, such",
    "Agent Q believes that φ\nφ is consistent with Q’s beliefs\nAgent Q knows that φ\nFor all Q knows, φ\nAfter any execution of program P, φ holds\nAfter some execution of P, φ holds\n5.3.1 The stock of valid formulas\nWe saw in the last section some valid formulas of basic modal logic, such\nas instances of the axiom scheme K: 2(φ →ψ) →(2φ →2ψ) and of the\nschemes in (5.3). Many other formulas, such as\nr 2p →p\nr 2p →22p\nr ¬2p →2¬2p\nr 3⊤\nare not valid. For example, for each one of these, there is a world in the\nKripke model of Figure 5.3 which does not satisfy the formula. The world\nx1 satisﬁes 2p, but it does not satisfy p, so it does not satisfy 2p →p. If we\nadd R(x2, x1) to our model, then x1 still satisﬁes 2p but does not satisfy\n22p. Thus, x1 fails to satisfy 2p →22p. If we change L(x4) to {p, q}, then\nx4 does not satisfy ¬2p →2¬2p, because it satisﬁes ¬2p, but it does not\nsatisfy 2¬2p – the path R(x4, x5)R(x5, x4) serves as a counter example.\nFinally, x6 does not satisfy 3⊤, for this formula states that there is an\naccessible world satisfying ⊤, which is not the case.\nIf we are to build a logic capturing the concept of necessity, however, we\nmust surely have that 2p →p is valid; for anything which is necessarily true\nis also simply true. Similarly, we would expect 2p →p to be valid in the\ncase that 2p means ‘agent Q knows p,’ for anything which is known must\nalso be true. We cannot know something which is false. We can, however,\nbelieve falsehoods, so in the case of a logic of belief, we would not expect\n2p →p to be valid.\nPart of the job of logic engineering is to determine what formula schemes\nshould be valid and to craft the logic in such a way that precisely those ones\nare valid.\nTable 5.7 shows six interesting readings for 2 and eight formula schemes.\nFor each reading and each formula scheme, we decide whether we should\nexpect the scheme to be valid. Notice that we should only put a tick if the\n318\n5 Modal logics and agents\n2φ\n2φ →φ\n2φ →22φ\n3φ →23φ\n3⊤\n2φ →3φ\n2φ ∨2¬φ",
    "Table 5.7 shows six interesting readings for 2 and eight formula schemes.\nFor each reading and each formula scheme, we decide whether we should\nexpect the scheme to be valid. Notice that we should only put a tick if the\n318\n5 Modal logics and agents\n2φ\n2φ →φ\n2φ →22φ\n3φ →23φ\n3⊤\n2φ →3φ\n2φ ∨2¬φ\n2(φ →ψ) ∧2φ →2ψ\n3φ ∧3ψ →3(φ ∧ψ)\nIt is necessarily true that φ\n√\n√\n√\n√\n√\n×\n√\n×\nIt will always be true that φ\n×\n√\n×\n×\n×\n×\n√\n×\nIt ought to be that φ\n×\n×\n×\n√\n√\n×\n√\n×\nAgent Q believes that φ\n×\n√\n√\n√\n√\n×\n√\n×\nAgent Q knows that φ\n√\n√\n√\n√\n√\n×\n√\n×\nAfter any execut’n of prgrm P, φ holds\n×\n×\n×\n×\n×\n×\n√\n×\nTable 5.7. Which formula schemes should hold for these readings of 2?\nformula should be valid for all cases of φ and ψ. If it could be valid for some\ncases, but not for others, we put a cross.\nThere are many points worth noting about Table 5.7. First, observe that\nit is rather debatable whether to put a tick, or a cross, in some of the cells.\nWe need to be precise about the concept of truth we are trying to formalise,\nin order to resolve any ambiguity.\nNecessity. When we ask ourselves whether 2φ →22φ and 3φ →23φ\nshould be valid, it seems to depend on what notion of necessity we are\nreferring to. These formulas are valid if that which is necessary is nec-\nessarily necessary. If we are dealing with physical necessity, then this\namounts to: are the laws of the universe themselves physically neces-\nsary, i.e. do they entail that they should be the laws of the universe?\nThe answer seems to be no. However, if we meant logical necessity, it\nseems that we should give the answer yes, for the laws of logic are meant\nto be those assertions whose truth cannot be denied. The row is ﬁlled\non the understanding that we mean logical necessity.\nAlways in the future. We must be precise about whether or not the\nfuture includes the present; this is precisely what the formula 2φ →\nφ states. It is a matter of convention whether the future includes the",
    "on the understanding that we mean logical necessity.\nAlways in the future. We must be precise about whether or not the\nfuture includes the present; this is precisely what the formula 2φ →\nφ states. It is a matter of convention whether the future includes the\npresent, or not. In Chapter 3, we saw that CTL adopts the convention\nthat it does. For variety, therefore, let us assume that the future does not\ninclude the present in this row of the table. That means that 2φ →φ\nfails. What about 3⊤? It says that there is a future world in which ⊤\nis true. In particular, then, there is a future world, i.e. time has no end.\nWhether we regard this as true or not depends on exactly what notion\nof ‘the future’ we are trying to model. We assumed the validity of 3⊤\n5.3 Logic engineering\n319\nin Chapter 3 on CTL since this resulted in an easier presentation of our\nmodel-checking algorithms, but we might choose to model it otherwise,\nas in Table 5.7.\nOught. In this case the formulas 2φ →22φ and 3φ →23φ state that\nthe moral codes we adopt are themselves forced upon us by morality.\nThis seems not to be the case; for example, we may believe that ‘It\nought to be the case that we wear a seat-belt,’ but this does not compel\nus to believe that ‘It ought to be the case that we ought to wear a seat-\nbelt.’ However, anything which ought to be so should be permitted to\nbe so; therefore, 2φ →3φ.\nBelief. To decide whether 3⊤, let us express it as ¬2⊥, for this is seman-\ntically equivalent. It says that agent Q does not believe any contradic-\ntions. Here we must be precise about whether we are modelling human\nbeings, with all their foibles and often plainly contradictory beliefs, or\nwhether we are modelling idealised agents that are logically omniscient –\ni.e. capable of working out the logical consequences of their beliefs. We\nopt to model the latter concept. The same issue arises when we consider,\nfor example, 3φ →23φ, which – when we rewrite it as ¬2ψ →2¬2ψ –",
    "whether we are modelling idealised agents that are logically omniscient –\ni.e. capable of working out the logical consequences of their beliefs. We\nopt to model the latter concept. The same issue arises when we consider,\nfor example, 3φ →23φ, which – when we rewrite it as ¬2ψ →2¬2ψ –\nsays that, if agent Q doesn’t believe something, then he believes that he\ndoesn’t believe it. Validity of the formula 2φ ∨2¬φ would mean that Q\nhas an opinion on every matter; we suppose this is unlikely. What about\n3φ ∧3ψ →3(φ ∧ψ)? Let us rewrite it as ¬3(φ ∧ψ) →¬(3φ ∧3ψ),\ni.e. 2(¬φ ∨¬ψ) →(2¬φ ∨2¬ψ) or – if we subsume the negations into\nthe φ and ψ – the formula 2(φ ∨ψ) →(2φ ∨2ψ). This seems not to\nbe valid, for agent Q may be in a situation in which she or he believes\nthat there is a key in the red box, or in the green box, without believing\nthat it is in the red box and also without believing that it is in the green\nbox.\nKnowledge. It seems to diﬀer from belief only in respect of the ﬁrst for-\nmula in Table 5.7; while agent Q can have false beliefs, he can only know\nthat which is true. In the case of knowledge, the formulas 2φ →22φ\nand ¬2ψ →2¬2ψ are called positive introspection and negative intro-\nspection, respectively, since they state that the agent can introspect upon\nher knowledge; if she knows something, she knows that she knows it; and\nif she does not know something, she again knows that she doesn’t know\nit. Clearly, this represents idealised knowledge, since most humans – with\nall their hang-ups and infelicities – do not satisfy these properties. The\nformula scheme K is sometimes referred to as logical omniscience in the\nlogic of knowledge, since it says that the agent’s knowledge is closed\nunder logical consequence. This means that the agent knows all the\n320\n5 Modal logics and agents\nconsequences of anything he knows, which is unfortunately (or fortu-\nnately?) true only for idealised agents, not humans.\nExecution of programs. Not many of our formulas seem to hold in this",
    "under logical consequence. This means that the agent knows all the\n320\n5 Modal logics and agents\nconsequences of anything he knows, which is unfortunately (or fortu-\nnately?) true only for idealised agents, not humans.\nExecution of programs. Not many of our formulas seem to hold in this\ncase. The scheme 2φ →22φ says that running the program twice is the\nsame as running it once, which is plainly wrong in the case of a program\nwhich deducts money from your bank account. The formula 3⊤says\nthat there is an execution of the program which terminates; this is false\nfor some programs.\nThe formula schemes 3⊤and 2φ →3φ were seen to be equivalent in the\npreceding section and, indeed, we see that they get the same pattern of ticks\nand crosses. We can also show that 2φ →φ entails 3⊤– i.e. (2φ →φ) →\n3⊤is valid – so whenever the former gets a tick, so should the latter. This\nis indeed the case, as you can verify in Table 5.7.\n5.3.2 Important properties of the accessibility relation\nSo far, we have been engineering logics at the level of deciding what formulas\nshould be valid for the various readings of 2. We can also engineer logics\nat the level of Kripke models. For each of our six readings of 2, there is a\ncorresponding reading of the accessibility relation R which will then suggest\nthat R enjoys certain properties such as reﬂexivity or transitivity.\nLet us start with necessity. The clauses\nx ⊩2ψ\niﬀfor each y ∈W with R(x, y) we have y ⊩ψ\nx ⊩3ψ\niﬀthere is a y ∈W such that R(x, y) and y ⊩ψ\nfrom Deﬁnition 5.4 tell us that φ is necessarily true at x if φ is true in all\nworlds y accessible from x in a certain way; but accessible in what way?\nIntuitively, necessarily φ is true if φ is true in all possible worlds; so R(x, y)\nshould be interpreted as meaning that y is a possible world according to the\ninformation in x.\nIn the case of knowledge, we think of R(x, y) as saying: y could be the\nactual world according to agent Q’s knowledge at x. In other words, if the",
    "should be interpreted as meaning that y is a possible world according to the\ninformation in x.\nIn the case of knowledge, we think of R(x, y) as saying: y could be the\nactual world according to agent Q’s knowledge at x. In other words, if the\nactual world is x, then agent Q – who is not omniscient – cannot rule out\nthe possibility of it being y. If we plug this deﬁnition into the clause above\nfor x ⊩2φ, we ﬁnd that agent Q knows φ iﬀφ is true in all the worlds that,\nfor all he knows, could be the actual world. The meaning of R for each of\nthe six readings of 2 is shown in Table 5.8.\nRecall that a given binary relation R may be:\nr reﬂexive: if, for every x ∈W, we have R(x, x);\nr symmetric: if, for every x, y ∈W, we have R(x, y) implies R(y, x);\n5.3 Logic engineering\n321\nTable 5.8. For each reading of 2, the meaning of R is given.\n2φ\nR(x, y)\nIt is necessarily true that φ\ny is possible world according to the in-\nformation at x\nIt will always be true that φ\ny is a future world of x\nIt ought to be that φ\ny is an acceptable world according to\nthe information at x\nAgent Q believes that φ\ny could be the actual world according\nto Q’s beliefs at x\nAgent Q knows that φ\ny could be the actual world according\nto Q’s knowledge at x\nAfter any execution of P, φ holds\ny is a possible resulting state after ex-\necution of P at x\nr serial: if, for every x there is a y such that R(x, y);\nr transitive: if, for every x, y, z ∈W, we have R(x, y) and R(y, z) imply R(x, z);\nr Euclidean: if, for every x, y, z ∈W with R(x, y) and R(x, z), we have R(y, z);\nr functional: if, for each x there is a unique y such that R(x, y);\nr linear: if, for every x, y, z ∈W, we have that R(x, y) and R(x, z) together imply\nthat R(y, z), or y equals z, or R(z, y);\nr total: if for every x, y ∈W we have R(x, y) or R(y, x); and\nr an equivalence relation: if it is reﬂexive, symmetric and transitive.\nNow, let us consider this question: according to the various readings of",
    "that R(y, z), or y equals z, or R(z, y);\nr total: if for every x, y ∈W we have R(x, y) or R(y, x); and\nr an equivalence relation: if it is reﬂexive, symmetric and transitive.\nNow, let us consider this question: according to the various readings of\nR, which of these properties do we expect R to have?\nExample 5.9 If 2φ means ‘agent Q knows φ,’ then R(x, y) means y could\nbe the actual world according to Q’s knowledge at x.\nr Should R be reﬂexive? This would say: x could be the actual world according\nto Q’s knowledge at x. In other words, Q cannot know that things are diﬀerent\nfrom how they really are – i.e., Q cannot have false knowledge. This is a desirable\nproperty for R to have. Moreover, it seems to rest on the same intuition – i.e. the\nimpossibility of false knowledge – as the validity of the formula 2φ →φ. Indeed,\nthe validity of this formula and the property of reﬂexivity are closely related, as\nwe see later on.\nr Should R be transitive? It would say: if y is possible according to Q’s knowledge\nat x and z is possible according to her knowledge at y, then z is possible according\nto her knowledge at x.\nWell, this seems to be true. For suppose it was not true, i.e. at x she knew\nsomething preventing z from being the real world. Then, she would know she\nknew this thing at x; therefore, she would know something at y which prevented\nz from being the real world; which contradicts our premise.\n322\n5 Modal logics and agents\nIn this argument, we relied on positive introspection, i.e. the formula 2φ →22φ.\nAgain, we will shortly see that there is a close correspondence between R being\ntransitive and the validity of this formula.\n5.3.3 Correspondence theory\nWe saw in the preceding section that there appeared to be a correspondence\nbetween the validity of 2φ →φ and the property that the accessibility re-\nlation R is reﬂexive. The connection between them is that both relied on\nthe intuition that anything which is known by an agent is true. Moreover,",
    "We saw in the preceding section that there appeared to be a correspondence\nbetween the validity of 2φ →φ and the property that the accessibility re-\nlation R is reﬂexive. The connection between them is that both relied on\nthe intuition that anything which is known by an agent is true. Moreover,\nthere also seemed to be a correspondence between 2φ →22φ and R being\ntransitive; they both seem to assert the property of positive introspection,\ni.e. that which is known is known to be known.\nIn this section, we will see that there is a precise mathematical relation-\nship between these formulas and properties of R. Indeed, to every formula\nscheme there corresponds a property of R. From the point of view of logic\nengineering, it is important to see this relationship, because it helps one to\nunderstand the logic being studied. For example, if you believe that a cer-\ntain formula scheme should be accepted in the system of modal logic you are\nengineering, then it is well worth looking at the corresponding property of\nR and checking that this property makes sense for the application, too. Al-\nternatively, the meaning of some formulas may seem diﬃcult to understand,\nso looking at their corresponding properties of R can help.\nTo state the relationship between formula schemes and their correspond-\ning properties, we need the notion of a (modal) frame.\nDeﬁnition 5.10 A frame F = (W, R) is a set W of worlds and a binary\nrelation R on W.\nA frame is like a Kripke model (Deﬁnition 5.3), except that it has no la-\nbelling function. From any model we can extract a frame, by just forgetting\nabout the labelling function; for example, Figure 5.9 shows the frame ex-\ntracted from the Kripke model of Figure 5.3. A frame is just a set of worlds\nand an accessibility relationship between them. It has no information about\nwhat atomic formulas are true at the various worlds. However, it is useful to\nsay sometimes that the frame, as a whole, satisﬁes a formula. This is deﬁned\nas follows.",
    "and an accessibility relationship between them. It has no information about\nwhat atomic formulas are true at the various worlds. However, it is useful to\nsay sometimes that the frame, as a whole, satisﬁes a formula. This is deﬁned\nas follows.\nDeﬁnition 5.11 A frame F = (W, R) satisﬁes a formula of basic modal\nlogic φ if, for each labelling function L : W →P(Atoms) and each w ∈W,\n5.3 Logic engineering\n323\nx1\nx2\nx3\nx4\nx5\nx6\nFigure 5.9. The frame of the model in Figure 5.3.\nx4\nx5\nx6\nFigure 5.10. Another frame.\nthe relation M, w ⊩φ holds, where M = (W, R, L) – recall the deﬁnition of\nM, w ⊩φ on page 310. In that case, we say that F ⊨φ holds.\nOne can show that, if a frame satisﬁes a formula, then it also satisﬁes\nevery substitution instance of that formula. Conversely, if a frame satisﬁes\nan instance of a formula scheme, it satisﬁes the whole scheme. This con-\ntrasts markedly with models. For example, the model of Figure 5.3 satisﬁes\np ∨3p ∨33p, but doesn’t satisfy every instance of φ ∨3φ ∨33φ; for ex-\nample, x6 does not satisfy q ∨3q ∨33q. Since frames don’t contain any\ninformation about the truth or falsity of propositional atoms, they can’t\ndistinguish between diﬀerent atoms; so, if a frame satisﬁes a formula, it also\nsatisﬁes the formula scheme obtained by substituting its atoms p, q, . . . by\nφ, ψ, . . .\nExamples 5.12 Consider the frame F in Figure 5.10.\n1.\nF satisﬁes the formula 2p →p. To see this, we have to consider any labelling\nfunction of the frame – there are eight such labelling functions, since p could\nbe true or false in each of the three worlds – and show that each world satisﬁes\nthe formula for each labelling. Rather than really doing this literally, let us\n324\n5 Modal logics and agents\nx4\nx5\nx6\np\np\nFigure 5.11. A model.\ngive a generic argument: let x be any world. Suppose that x ⊩2p; we want to\nshow x ⊩p. We know that R(x, x) because each x is accessible from itself in the\ndiagram; so, it follows from the clause for 2 in Deﬁnition 5.4 that x ⊩p.\n2.",
    "324\n5 Modal logics and agents\nx4\nx5\nx6\np\np\nFigure 5.11. A model.\ngive a generic argument: let x be any world. Suppose that x ⊩2p; we want to\nshow x ⊩p. We know that R(x, x) because each x is accessible from itself in the\ndiagram; so, it follows from the clause for 2 in Deﬁnition 5.4 that x ⊩p.\n2.\nTherefore, our frame F satisﬁes any formula of this shape, i.e. it satisﬁes the\nformula scheme 2φ →φ.\n3.\nThe frame does not satisfy the formula 2p →22p. For suppose we take the\nlabelling of Figure 5.11; then x4 ⊩2p, but x4 ̸⊩22p.\nIf you think about why the frame of Figure 5.10 satisﬁed 2p →p and why\nit did not satisfy 2p →22p, you will probably guess the following:\nTheorem 5.13 Let F = (W, R) be a frame.\n1.\nThe following statements are equivalent:\n– R is reﬂexive;\n– F satisﬁes 2φ →φ;\n– F satisﬁes 2p →p;\n2.\nThe following statements are equivalent:\n– R is transitive;\n– F satisﬁes 2φ →22φ;\n– F satisﬁes 2p →22p.\nPROOF: Each item 1 and 2 requires us to prove three things: (a) that, if R\nhas the property, then the frame satisﬁes the formula scheme; and (b) that,\nif the frame satisﬁes the formula scheme, it satisﬁes the instance of it; and\n(c) that, if the frame satisﬁes a formula instance, then R has the property.\n1.\n(a) Suppose R is reﬂexive. Let L be a labelling function, so now M = (W, R, L)\nis a model of basic modal logic. We need to show M ⊨2φ →φ. That means\nwe need to show x ⊩2φ →φ for any x ∈W, so pick any x. Use the clause\nfor implication in Deﬁnition 5.4. Suppose x ⊩2φ; since R(x, x), it immediately\nfollows from the clause for 2 in Deﬁnition 5.4 that x ⊩p. Therefore, we have\nshown x ⊩2φ →φ.\n(b) We just set φ to be p.\n5.3 Logic engineering\n325\nTable 5.12. Properties of R corresponding to some formulas.\nname\nformula scheme\nproperty of R\nT\n2φ →φ\nreﬂexive\nB\nφ →23φ\nsymmetric\nD\n2φ →3φ\nserial\n4\n2φ →22φ\ntransitive\n5\n3φ →23φ\nEuclidean\n2φ ↔3φ\nfunctional\n2(φ ∧2φ →ψ) ∨2(ψ ∧2ψ →φ)\nlinear\n(c) Suppose the frame satisﬁes 2p →p. Take any x; we’re going to show R(x, x).",
    "name\nformula scheme\nproperty of R\nT\n2φ →φ\nreﬂexive\nB\nφ →23φ\nsymmetric\nD\n2φ →3φ\nserial\n4\n2φ →22φ\ntransitive\n5\n3φ →23φ\nEuclidean\n2φ ↔3φ\nfunctional\n2(φ ∧2φ →ψ) ∨2(ψ ∧2ψ →φ)\nlinear\n(c) Suppose the frame satisﬁes 2p →p. Take any x; we’re going to show R(x, x).\nTake a labelling function L such that p ̸∈L(x) and p ∈L(y) for all worlds y\nexcept x. Proof by contradiction: Assume we don’t have R(x, x). Then, x ⊩2p,\nsince all the worlds accessible from x satisfy p – this is because all the worlds\nexcept x satisfy p; but since F satisﬁes 2p →p, it follows that x ⊩2p →p;\ntherefore, putting x ⊩2p and x ⊩2p →p together, we get x ⊩p. This is a\ncontradiction to the assumption that we don’t have R(x, x), since we said that\np ̸∈L(x). So we must have R(x, x) in our frame!\n2.\n(a) Suppose R is transitive. Let L be a labelling function and M = (W, R, L). We\nneed to show M ⊩2φ →22φ. That means we need to show x ⊩2φ →22φ\nfor any x ∈W. Suppose x ⊩2φ; we need to show x ⊩22φ. That is, using the\nclause for 2 in Deﬁnition 5.4, that any y such that R(x, y) satisﬁes 2φ; that is,\nfor any y, z with R(x, y) and R(y, z), we have z ⊩φ.\nWell, suppose we did have y and z with R(x, y) and R(y, z). By the fact that R\nis transitive, we obtain R(x, z). But we’re supposing that x ⊩2φ, so from the\nmeaning of 2 we get z ⊩φ, which is what we needed to prove.\n(b) Again, just set φ to be p.\n(c) Suppose the frame satisﬁes 2p →22p. Take any x, y and z with R(x, y)\nand R(y, z); we are going to show R(x, z).\nDeﬁne a labelling function L such that p ̸∈L(z) and p ∈L(w) for all worlds\nw except z. Suppose we don’t have R(x, z); then x ⊩2p, since w ⊩p for all\nw ̸= z. Using the axiom 2p →22p, it follows that x ⊩22p. So y ⊩2p holds\nsince R(x, y). The latter and R(y, z) then render z ⊩p, a contradiction. Thus,\nwe must have R(x, z).\n2\nThis picture is completed in Table 5.12, which shows, for a collection of\nformulas, the corresponding property of R. What this table means mathe-\nmatically is the following:",
    "since R(x, y). The latter and R(y, z) then render z ⊩p, a contradiction. Thus,\nwe must have R(x, z).\n2\nThis picture is completed in Table 5.12, which shows, for a collection of\nformulas, the corresponding property of R. What this table means mathe-\nmatically is the following:\nTheorem 5.14 A frame F = (W, R) satisﬁes a formula scheme in Table\n5.12 iﬀR has the corresponding property in that table.\nThe names of the formulas in the left-hand column are historical, but have\nstuck and are still used widely in books.\n326\n5 Modal logics and agents\n5.3.4 Some modal logics\nThe logic engineering approach of this section encourages us to design logics\nby picking and choosing a set L of formula schemes, according to the ap-\nplication at hand. Some examples of formula schemes that we may wish to\nconsider for a given application are those in Tables 5.7 and 5.12.\nDeﬁnition 5.15 Let L be a set of formula schemes of modal logic and\nΓ ∪{ψ} a set of formulas of basic modal logic.\n1.\nThe set Γ is closed under substitution instances iﬀwhenever φ ∈Γ, then any\nsubstitution instance of φ is also in Γ.\n2.\nLet Lc be the smallest set containing all instances of L.\n3.\nΓ semantically entails ψ in L iﬀΓ ∪Lc semantically entails ψ in basic modal\nlogic. In that case, we say that Γ ⊨L ψ holds.\nThus, we have Γ ⊨L ψ if every Kripke model and every world x satisfying\nΓ ∪Lc therein also satisﬁes ψ. Note that for L = ∅this deﬁnition is consistent\nwith the one of Deﬁnition 5.7, since we then have Γ ∪Lc = Γ. For logic\nengineering, we require that L be\nr closed under substitution instances; otherwise, we won’t be able to characterize\nLc in terms of properties of the accessibility relation; and\nr consistent in that there is a frame F such that F ⊨φ holds for all φ ∈L; oth-\nerwise, Γ ⊨L ψ holds for all Γ and ψ! In most applications of logic engineering,\nconsistency is easy to establish.\nWe now study a few important modal logics that extend basic modal logic\nwith a consistent set of formula schemes L.",
    "erwise, Γ ⊨L ψ holds for all Γ and ψ! In most applications of logic engineering,\nconsistency is easy to establish.\nWe now study a few important modal logics that extend basic modal logic\nwith a consistent set of formula schemes L.\nThe modal logic K\nThe weakest modal logic doesn’t have any chosen\nformula schemes, like those of Tables 5.7 and 5.12. So L = ∅and this modal\nlogic is called K as it satisﬁes all instances of the formula scheme K; modal\nlogics with this property are called normal and all modal logics we study in\nthis text are normal.\nThe modal logic KT45\nA well-known modal logic is KT45 – also called\nS5 in the technical literature – where L = {T, 4, 5} with T, 4 and 5 from\nTable 5.12. This logic is used to reason about knowledge; 2φ means that\nthe agent Q knows φ. Table 5.12 tell us, respectively, that\nT. Truth: the agent Q knows only true things.\n4. Positive introspection: if the agent Q knows something, then she knows\nthat she knows it.\n5. Negative introspection: if the agent Q doesn’t know something, then\nshe knows that she doesn’t know it.\n5.3 Logic engineering\n327\nIn this application, the formula scheme K means logical omniscience: the\nagent’s knowledge is closed under logical consequence. Note that these prop-\nerties represent idealisations of knowledge. Human knowledge has none of\nthese properties! Even computer agents may not have them all. There are\nseveral attempts in the literature to deﬁne logics of knowledge that are more\nrealistic, but we will not consider them here.\nThe semantics of the logic KT45 must consider only relations R which\nare: reﬂexive (T), transitive (4) and Euclidean (5).\nFact 5.16 A relation is reﬂexive, transitive and Euclidean iﬀit is reﬂexive,\ntransitive and symmetric, i.e. if it is an equivalence relation.\nKT45 is simpler than K in the sense that it has few essentially diﬀerent ways\nof composing modalities.\nTheorem 5.17 Any sequence of modal operators and negations in KT45",
    "transitive and symmetric, i.e. if it is an equivalence relation.\nKT45 is simpler than K in the sense that it has few essentially diﬀerent ways\nof composing modalities.\nTheorem 5.17 Any sequence of modal operators and negations in KT45\nis equivalent to one of the following: −, 2, 3, ¬, ¬2 and ¬3, where −\nindicates the absence of any negation or modality.\nThe modal logic KT4\nThe modal logic KT4, that is L equals {T, 4},\nis also called S4 in the literature. Correspondence theory tells us that its\nmodels are precisely the Kripke models M = (W, R, L), where R is reﬂexive\nand transitive. Such structures are often very useful in computer science. For\nexample, if φ stands for the type of a piece of code – φ could be int × int →\nbool, indicating some code which expects a pair of integers as input and\noutputs a boolean value – then 2φ could stand for residual code of type φ.\nThus, in the current world x this code would not have to be executed, but\ncould be saved (= residualised) for execution at a later computation stage.\nThe formula scheme 2φ →φ, the axiom T, then means that code may be\nexecuted right away, whereas the formula scheme 2φ →22φ, the axiom 4,\nallows that residual code remain residual, i.e. we can repeatedly postpone its\nexecution in future computation stages. Such type systems have important\napplications in the specialisation and partial evaluation of code. We refer\nthe interested reader to the bibliographic notes at the end of the chapter.\nTheorem 5.18 Any sequence of modal operators and negations in KT4 is\nequivalent to one of the following: −, 2, 3, 23, 32, 232, 323, ¬, ¬2,\n¬3, ¬23, ¬32, ¬232 and ¬323.\nIntuitionistic propositional logic\nIn Chapter 1, we gave a natural de-\nduction system for propositional logic which was sound and complete with\n328\n5 Modal logics and agents\nrespect to semantic entailment based on truth tables. We also pointed out\nthat the proof rules PBC, LEM and ¬¬e are questionable in certain com-",
    "In Chapter 1, we gave a natural de-\nduction system for propositional logic which was sound and complete with\n328\n5 Modal logics and agents\nrespect to semantic entailment based on truth tables. We also pointed out\nthat the proof rules PBC, LEM and ¬¬e are questionable in certain com-\nputational situations. If we disallow their usage in natural deduction proofs,\nwe obtain a logic, called intuitionistic propositional logic, together with its\nown proof theory. So far so good; but it is less clear what sort of semantics\none could have for such a logic – again with soundness and completeness in\nmind. This is where certain models of KT4 will do the job quite nicely. Recall\nthat correspondence theory implies that a model M = (W, R, L) of KT4 is\nsuch that R is reﬂexive and transitive. The only additional requirement we\nimpose on a model for intuitionistic propositional logic is that its labelling\nfunction L be monotone in R: R(x, y) implies that L(x) is a subset of L(y).\nThis models that the truth of atomic positive formulas persist throughout\nthe worlds that are reachable from a given world.\nDeﬁnition 5.19 A model of intuitionistic propositional logic is a model\nM = (W, R, L) of KT4 such that R(x, y) always implies L(x) ⊆L(y). Given\na propositional logic formula as in (1.3), we deﬁne x ⊩φ as in Deﬁnition 5.4\nexception for the clauses →and ¬. For φ1 →φ2 we deﬁne x ⊩φ1 →φ2 iﬀ\nfor all y with R(x, y) we have y ⊩φ2 whenever we have y ⊩φ1. For ¬φ we\ndeﬁne x ⊩¬φ iﬀfor all y with R(x, y) we have y ̸⊩φ.\nAs an example, consider the model W = {x, y} with accessibility relation\nR = {(x, x), (x, y), (y, y)}, which is indeed reﬂexive and transitive. For a la-\nbelling function L with L(x) = ∅and L(y) = {p}, we claim that x ̸⊩p ∨¬p.\n(Recall that p ∨¬p is an instance of LEM which we proved in Chapter 1 with\nthe full natural deduction calculus.) We do not have x ⊩p, for p is not in\nthe set L(x) which is empty. Thus, Deﬁnition 5.4 for the case ∨implies that",
    "belling function L with L(x) = ∅and L(y) = {p}, we claim that x ̸⊩p ∨¬p.\n(Recall that p ∨¬p is an instance of LEM which we proved in Chapter 1 with\nthe full natural deduction calculus.) We do not have x ⊩p, for p is not in\nthe set L(x) which is empty. Thus, Deﬁnition 5.4 for the case ∨implies that\nx ⊩p ∨¬p can hold only if x ⊩¬p holds. But x ⊩¬p simply does not hold,\nsince there is a world y with R(x, y) such that y ⊩p holds, for p ∈L(y). The\navailability of possible worlds in the models of KT4 together with a ‘modal\ninterpretation’ of →and ¬ breaks down the validity of the theorem LEM in\nclassical logic.\nOne can now deﬁne semantic entailment in the same manner as for modal\nlogics. Then, one can prove soundness and completeness of the reduced nat-\nural deduction system with respect to this semantic entailment, but those\nproofs are beyond the scope of this book.\n5.4 Natural deduction\nVerifying semantic entailment Γ ⊨L ψ by appealing to its deﬁnition directly\nwould be rather diﬃcult. We would have to consider every Kripke model\n5.4 Natural deduction\n329\nthat satisﬁes all formulas of Γ and every world in it. Fortunately, we have a\nmuch more usable approach, which is an extension, respectively adaptation,\nof the systems of natural deduction met in Chapters 1 and 2. Recall that\nwe presented natural deduction proofs as linear representations of proof\ntrees which may involve proof boxes which control the scope of assumptions,\nor quantiﬁers. The proof boxes have formulas and/or other boxes inside\nthem. There are rules which dictate how to construct proofs. Boxes open\nwith an assumption; when a box is closed – in accordance with a rule –\nwe say that its assumption is discharged. Formulas may be repeated and\nbrought into boxes, but may not be brought out of boxes. Every formula\nmust have some justiﬁcation to its right: a justiﬁcation can be the name\nof a rule, or the word ‘assumption,’ or an instance of the proof rule copy;\nsee e.g. page 13.",
    "brought into boxes, but may not be brought out of boxes. Every formula\nmust have some justiﬁcation to its right: a justiﬁcation can be the name\nof a rule, or the word ‘assumption,’ or an instance of the proof rule copy;\nsee e.g. page 13.\nNatural deduction works in a very similar way for modal logic. The main\ndiﬀerence is that we introduce a new kind of proof box, to be drawn with\ndashed lines. This is required for the rules for the connective 2. The dashed\nproof box has a completely diﬀerent role from the solid one. As we saw\nin Chapter 1, going into a solid proof box means making an assumption.\nGoing into a dashed box means reasoning in an arbitrary accessible world.\nIf at any point in a proof we have 2φ, we could open a dashed box and put\nφ in it. Then, we could work on this φ, to obtain, for example, ψ. Now we\ncould come out of the dashed box and, since we have shown ψ in an arbi-\ntrary accessible world, we may deduce 2ψ in the world outside the dashed\nbox.\nThus, the rules for bringing formulas into dashed boxes and taking for-\nmulas out of them are the following:\nr Wherever 2φ occurs in a proof, φ may be put into a subsequent dashed box.\nr Wherever ψ occurs at the end of a dashed box, 2ψ may be put after that dashed\nbox.\nWe have thus added two rules, 2 introduction and 2 elimination:\n...\nφ\n2φ\n2i\n2φ\n...\nφ\n...\n2e\n330\n5 Modal logics and agents\nIn modal logic, natural deduction proofs contain both solid and dashed\nboxes, nested in any way. Note that there are no explicit rules for 3, which\nmust be written ¬2¬ in proofs.\nThe extra rules for KT45\nThe rules 2i and 2e are suﬃcient for cap-\nturing semantic entailment of the modal logic K. Stronger modal logics, e.g.\nKT45, require extra rules if one wants to capture their semantic entailment\nvia proofs. In the case of KT45, this extra strength is expressed by rule\nschemes for the axioms T, 4 and 5:\n2φ\nφ\nT\n2φ\n22φ\n4\n¬2φ\n2¬2φ\n5\nAn equivalent alternative to the rules 4 and 5 would be to stipulate relax-",
    "KT45, require extra rules if one wants to capture their semantic entailment\nvia proofs. In the case of KT45, this extra strength is expressed by rule\nschemes for the axioms T, 4 and 5:\n2φ\nφ\nT\n2φ\n22φ\n4\n¬2φ\n2¬2φ\n5\nAn equivalent alternative to the rules 4 and 5 would be to stipulate relax-\nations of the rules about moving formulas in and out of dashed boxes. Since\nrule 4 allows us to double-up boxes, we could instead think of it as allowing\nus to move formulas beginning with 2 into dashed boxes. Similarly, axiom\n5 has the eﬀect of allowing us to move formulas beginning with ¬2 into\ndashed boxes. Since 5 is a scheme and since φ and ¬¬φ are equivalent in ba-\nsic modal logic, we could write ¬φ instead of φ throughout without changing\nthe expressive power and meaning of that axiom.\nDeﬁnition 5.20 Let L be a set of formula schemes. We say that Γ ⊢L ψ is\nvalid if ψ has a proof in the natural deduction system for basic modal logic\nextended with the axioms from L and premises from Γ.\nExamples 5.21 We show that the following sequents are valid:\n1.\n|−K 2p ∧2q →2(p ∧q).\n1\n2p ∧2q\nassumption\n2\n2p\n∧e1 1\n3\n2q\n∧e2 1\n4\np\n2e 2\n5\nq\n2e 3\n6\np ∧q\n∧i 4, 5\n7\n2(p ∧q)\n2i 4−6\n8\n2p ∧2q →2(p ∧q)\n→i 1−7\n5.5 Reasoning about knowledge in a multi-agent system\n331\n2.\n|−KT45 p →23p.\n1\np\nassumption\n2\n2¬p\nassumption\n3\n¬p\nT 2\n4\n⊥\n¬e 1, 3\n5\n¬2¬p\n¬i 2−4\n6\n2¬2¬p\naxiom 5 on line 5\n7\np →2¬2¬p\n→i 1−6\n3.\n|−KT45 232p →2p.\n1\n2¬2¬2p\nassumption\n2\n¬2¬2p\n2e 1\n3\n¬2p\nassumption\n4\n2¬2p\naxiom 5 on line 3\n5\n⊥\n¬e 4, 2\n6\n¬¬2p\n¬i 3−5\n7\n2p\n¬¬e 6\n8\np\nT 7\n9\n2p\n2i 2−8\n10\n2¬2¬2p →2p\n→i 1−9\n5.5 Reasoning about knowledge in\na multi-agent system\nIn a multi-agent system, diﬀerent agents have diﬀerent knowledge of the\nworld. An agent may need to reason about its own knowledge about the\nworld; it may also need to reason about what other agents know about\nthe world. For example, in a bargaining situation, the seller of a car must\nconsider what a buyer knows about the car’s value. The buyer must also",
    "world. An agent may need to reason about its own knowledge about the\nworld; it may also need to reason about what other agents know about\nthe world. For example, in a bargaining situation, the seller of a car must\nconsider what a buyer knows about the car’s value. The buyer must also\nconsider what the seller knows about what the buyer knows about that\nvalue and so on.\nReasoning about knowledge refers to the idea that agents in a group take\ninto account not only the facts of the world, but also the knowledge of other\nagents in the group. Applications of this idea include: games, economics,\n332\n5 Modal logics and agents\ncryptography and protocols. It is not very easy for humans to follow the\nthread of such nested sentences as\nDean doesn’t know whether Nixon knows that Dean knows that\nNixon knows that McCord burgled O’Brien’s oﬃce at Watergate.\nHowever, computer agents are better than humans in this respect.\n5.5.1 Some examples\nWe start with some classic examples about reasoning in a multi-agent envi-\nronment. Then, in the next section, we engineer a modal logic which allows\nfor a formal representation of these examples via sequents and which solves\nthem by proving them in a natural deduction system.\nThe wise-men puzzle\nThere are three wise men. It’s common knowl-\nedge – known by everyone and known to be known by everyone, etc. – that\nthere are three red hats and two white hats. The king puts a hat on each\nof the wise men in such a way that they are not able to see their own hat,\nand asks each one in turn whether they know the colour of the hat on their\nhead. Suppose the ﬁrst man says he does not know; then the second says he\ndoes not know either.\nIt follows that the third man must be able to say that he knows the colour\nof his hat. Why is this? What colour has the third man’s hat?\nTo answer these questions, let us enumerate the seven possibilities which\nexist: they are\nR R R\nR R W\nR W R\nR W W\nW R R\nW R W\nW W R",
    "does not know either.\nIt follows that the third man must be able to say that he knows the colour\nof his hat. Why is this? What colour has the third man’s hat?\nTo answer these questions, let us enumerate the seven possibilities which\nexist: they are\nR R R\nR R W\nR W R\nR W W\nW R R\nW R W\nW W R\nwhere, for example, R W W refers to the situation that the ﬁrst, second\nand third men have red, white and white hats, respectively. The eighth\npossibility, W W W, is ruled out as there are only two white hats.\nNow let’s think of it from the second and third men’s point of view.\nWhen they hear the ﬁrst man speak, they can rule out the possibility of\nthe true situation being R W W, because if it were this situation, then the\nﬁrst man, seeing that the others were wearing white hats and knowing that\nthere are only two white hats, would have concluded that his hat must be\nred. As he said that he did not know, the true situation cannot be R W W.\nNotice that the second and third men must be intelligent in order to perform\n5.5 Reasoning about knowledge in a multi-agent system\n333\nthis reasoning; and they must know that the ﬁrst man is intelligent and\ntruthful as well. In the puzzle, we assume the truthfulness and intelligence\nand perceptiveness of the men are common knowledge – known by everyone\nand known to be known by everyone, etc.\nWhen the third man hears the second man speak, he can rule out the\npossibility of the true situation being W R W, for similar reasons: if it were\nthat, the second man would have said that he knew his hat was red, but\nhe did not say this. Moreover, the third man can also rule out the situation\nR R W when he hears the second man’s answer, for this reason: if the second\nman had seen that the ﬁrst was wearing red and the third white, he would\nhave known that it must be R W W or R R W; but he would have known\nfrom the ﬁrst man’s answer that it couldn’t be R W W, so he would have\nconcluded it was R R W and that he was wearing a red hat; but he did not",
    "man had seen that the ﬁrst was wearing red and the third white, he would\nhave known that it must be R W W or R R W; but he would have known\nfrom the ﬁrst man’s answer that it couldn’t be R W W, so he would have\nconcluded it was R R W and that he was wearing a red hat; but he did not\ndraw this conclusion, so, reasons the third man, it cannot be R R W.\nHaving heard the ﬁrst and second men speak, the third man has elimi-\nnated R W W, W R W and R R W; leaving only R R R, R W R, W R R and\nW W R. In all of these he is wearing a red hat, so he concludes that he must\nbe wearing a red hat.\nNotice that the men learn a lot from hearing the other men speak. We\nemphasise again the importance of the assumption that they tell the truth\nabout their state of knowledge and are perceptive and intelligent enough to\ncome to correct conclusions. Indeed, it is not enough that the three men\nare truthful, perceptive and intelligent; they must be known to be so by the\nothers and, in later examples, this fact must also be known etc. Therefore,\nwe assume that all this is common knowledge.\nThe muddy-children puzzle\nThis is one of the many variations on the\nwise-men puzzle; a diﬀerence is that the questions are asked in parallel rather\nthan sequentially. There is a large group of children playing in the garden –\ntheir perceptiveness, truthfulness and intelligence being common knowledge,\nit goes without saying. A certain number of children, say k ≥1, get mud on\ntheir foreheads. Each child can see the mud on others, but not on his own\nforehead. If k > 1, then each child can see another with mud on its forehead,\nso each one knows that at least one in the group is muddy. Consider these\ntwo scenarios:\nScenario 1. The father repeatedly asks the question ‘Does any of you\nknow whether you have mud on your own forehead?’ The ﬁrst time they\nall answer ‘no;’ but, unlike in the wise-men example, they don’t learn\nanything by hearing the others answer ‘no,’ so they go on answering ‘no’",
    "two scenarios:\nScenario 1. The father repeatedly asks the question ‘Does any of you\nknow whether you have mud on your own forehead?’ The ﬁrst time they\nall answer ‘no;’ but, unlike in the wise-men example, they don’t learn\nanything by hearing the others answer ‘no,’ so they go on answering ‘no’\nto the father’s repeated questions.\n334\n5 Modal logics and agents\nScenario 2. The father ﬁrst announces that at least one of them is\nmuddy – which is something they know already; and then, as before,\nhe repeatedly asks them ‘Does any of you know whether you have mud\non your own forehead?’ The ﬁrst time they all answer ‘no.’ Indeed, they\ngo on answering ‘no’ to the ﬁrst k −1 repetitions of that same question;\nbut at the kth those with muddy foreheads are able to answer ‘yes.’\nAt ﬁrst sight, it seems rather puzzling that the two scenarios are diﬀerent,\ngiven that the only diﬀerence in the events leading up to them is that in the\nsecond one the father announces something that they already know. It would\nbe wrong, however, to conclude that the children learn nothing from this\nannouncement. Although everyone knows the content of the announcement,\nthe father’s saying it makes it common knowledge among them, so now\nthey all know that everyone else knows it, etc. This is the crucial diﬀerence\nbetween the two scenarios.\nTo understand scenario 2, consider a few cases of k.\nk = 1, i.e. just one child has mud. That child is immediately able to\nanswer ‘yes,’ since she has heard the father and doesn’t see any other\nchild with mud.\nk = 2, say only the children Ramon and Candy have mud. Everyone\nanswers ‘no’ the ﬁrst time. Now Ramon thinks: since Candy answered\n‘no’ the ﬁrst time, she must see someone with mud. Well, the only person\nI can see with mud is Candy, so if she can see someone else it must be me.\nSo Ramon answers ‘yes’ the second time. Candy reasons symmetrically\nabout Ramon and also answers ‘yes’ the second time round.",
    "‘no’ the ﬁrst time, she must see someone with mud. Well, the only person\nI can see with mud is Candy, so if she can see someone else it must be me.\nSo Ramon answers ‘yes’ the second time. Candy reasons symmetrically\nabout Ramon and also answers ‘yes’ the second time round.\nk = 3, say only the children Alice, Bob, and Charlie have mud. Everyone\nanswers ‘no’ the ﬁrst two times. But now Alice thinks: if it was just\nBob and Charlie with mud, they would have answered ‘yes’ the second\ntime; making the argument for k = 2 above. So there must be a third\nperson with mud; since I can see only Bob and Charlie having mud,\nthe third person must be me. So Alice answers ‘yes’ the third time. For\nsymmetrical reasons, so do Bob and Charlie.\nAnd similarly for other cases of k.\nTo see that it was not common knowledge before the father’s announce-\nment that one of the children was muddy, consider again k = 2, with Ramon\nand Candy. Of course, Ramon and Candy both know someone is muddy –\nthey see each other; but, for example, Ramon doesn’t know that Candy\nknows that someone is dirty. For all Ramon knows, Candy might be the\nonly dirty one and therefore not be able to see a dirty child.\n5.5 Reasoning about knowledge in a multi-agent system\n335\n5.5.2 The modal logic KT45n\nWe now generalise the modal logic KT45 given in Section 5.3.4. Instead of\nhaving just one 2, it will have many, one for each agent i from a ﬁxed set\nA = {1, 2, . . . , n} of agents. We write those modal connectives as Ki (for\neach agent i ∈A); the K is to emphasise the application to knowledge. We\nassume a collection p, q, r, . . . of atomic formulas. The formula Ki p means\nthat agent i knows p; so, for example, K1 p ∧K1¬K2K1 p means that agent 1\nknows p, but knows that agent 2 doesn’t know he knows it.\nWe also have the modal connectives EG, where G is any subset of A. The\nformula EG p means everyone in the group G knows p. If G = {1, 2, 3, . . . , n},",
    "that agent i knows p; so, for example, K1 p ∧K1¬K2K1 p means that agent 1\nknows p, but knows that agent 2 doesn’t know he knows it.\nWe also have the modal connectives EG, where G is any subset of A. The\nformula EG p means everyone in the group G knows p. If G = {1, 2, 3, . . . , n},\nthen EG p is equivalent to K1 p ∧K2 p ∧· · · ∧Kn p. We assume similar bind-\ning priorities to those put forward on page 307.\nConvention 5.22 The binding priorities of KT45n are the ones of basic\nmodal logic, if we think of each modality Ki, EG and CG as ‘being’ 2.\nOne might think that φ could not be more widely known than everyone\nknowing it, but this is not the case. It could be, for example, that everyone\nknows φ, but they might not know that they all know it. If φ is supposed\nto be a secret, it might be that you and your friend both know it, but your\nfriend does not know that you know it and you don’t know that your friend\nknows it. Thus, EGEG φ is a state of knowledge even greater than EG φ and\nEGEGEG φ is greater still. We say that φ is common knowledge among G,\nwritten CG φ, if everyone knows φ and everyone knows that everyone knows\nit; and everyone knows that; and knows that etc. So we may think of CG φ\nas an inﬁnite conjunction\nEG φ ∧EGEG φ ∧EGEGEG φ ∧. . . .\nHowever, since our logics only have ﬁnite conjunctions, we cannot reduce\nCG to something which is already in the logic. We have to express the\ninﬁnite aspect of CG via its semantics and retain it as an additional modal\nconnective. Finally, DG φ means the knowledge of φ is distributed among\nthe group G; although no-one in G may know it, they would be able to\nwork it out if they put their heads together and combined the information\ndistributed among them.\nDeﬁnition 5.23 A formula φ in the multi-modal logic of KT45n is deﬁned\nby the following grammar:\nφ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ ↔φ) |\n(Ki φ) | (EG φ) | (CG φ) | (DG φ)\n336\n5 Modal logics and agents\nq\nq\np, q\nx1\nx2\nx4\nx5\np\nx6\np\nx3\nR1\nR1, R2\nR1, R3\nR1, R2",
    "distributed among them.\nDeﬁnition 5.23 A formula φ in the multi-modal logic of KT45n is deﬁned\nby the following grammar:\nφ ::= ⊥| ⊤| p | (¬φ) | (φ ∧φ) | (φ ∨φ) | (φ →φ) | (φ ↔φ) |\n(Ki φ) | (EG φ) | (CG φ) | (DG φ)\n336\n5 Modal logics and agents\nq\nq\np, q\nx1\nx2\nx4\nx5\np\nx6\np\nx3\nR1\nR1, R2\nR1, R3\nR1, R2\nR3\nFigure 5.13. A KT45n model for n = 3.\nwhere p is any atomic formula, i ∈A and G ⊆A. We simply write E, C and\nD without subscripts if we refer to EA, CA and DA.\nCompare this deﬁnition with Deﬁnition 5.1. Instead of 2, we have several\nmodalities Ki and we also have EG, CG and DG for each G ⊆A. Actually,\nall of these connectives will shortly be seen to be ‘box-like’ rather than\n‘diamond-like’, in the sense that they distribute over ∧rather than over ∨–\ncompare this to the discussion of equivalences on page 308. The ‘diamond-\nlike’ correspondents of these connectives are not explicitly in the language,\nbut may of course be obtained using negations, i.e. ¬Ki¬, ¬CG¬ etc.\nDeﬁnition 5.24 A model M = (W, (Ri)i∈A, L) of the multi-modal logic\nKT45n with the set A of n agents is speciﬁed by three things:\n1.\na set W of possible worlds;\n2.\nfor each i ∈A, an equivalence relation Ri on W (Ri ⊆W × W), called the\naccessibility relations; and\n3.\na labelling function L : W →P(Atoms).\nCompare this with Deﬁnition 5.3. The diﬀerence is that, instead of just one\naccessibility relation, we now have a family, one for each agent in A; and we\nassume the accessibility relations are equivalence relations.\nWe exploit these properties of Ri in the graphical illustrations of Kripke\nmodels for KT45n. For example, a model of KT453 with set of worlds\n{x1, x2, x3, x4, x5, x6} is shown in Figure 5.13. The links between the worlds\nhave to be labelled with the name of the accessibility relation, since we have\nseveral relations. For example, x1 and x2 are related by R1, whereas x4 and\n5.5 Reasoning about knowledge in a multi-agent system\n337",
    "{x1, x2, x3, x4, x5, x6} is shown in Figure 5.13. The links between the worlds\nhave to be labelled with the name of the accessibility relation, since we have\nseveral relations. For example, x1 and x2 are related by R1, whereas x4 and\n5.5 Reasoning about knowledge in a multi-agent system\n337\nx5 are related both by R1 and by R2. We simplify by no longer requiring ar-\nrows on the links. This is because we know that the relations are symmetric,\nso the links are bi-directional. Moreover, the relations are also reﬂexive, so\nthere should be loops like the one on x4 in Figure 5.11 in all the worlds and\nfor all of the relations. We can simply omit these from the diagram, since we\ndon’t need to distinguish between worlds which are self-related and those\nwhich are not.\nDeﬁnition 5.25 Take a model M = (W, (Ri)i∈A, L) of KT45n and a world\nx ∈W. We deﬁne when φ is true in x via a satisfaction relation x ⊩φ by\ninduction on φ:\nx ⊩p iﬀp ∈L(x)\nx ⊩¬φ iﬀx ̸⊩φ\nx ⊩φ ∧ψ\niﬀx ⊩φ and x ⊩ψ\nx ⊩φ ∨ψ\niﬀx ⊩φ or x ⊩ψ\nx ⊩φ →ψ\niﬀx ⊩ψ whenever we have x ⊩φ\nx ⊩Ki ψ\niﬀ, for each y ∈W, Ri(x, y) implies y ⊩ψ\nx ⊩EG ψ\niﬀ, for each i ∈G, x ⊩Ki ψ\nx ⊩CG ψ\niﬀ, for each k ≥1, we have x ⊩Ek\nGψ,\nwhere Ek\nG means EGEG . . . EG – k times\nx ⊩DG ψ\niﬀ, for each y ∈W, we have y ⊩ψ,\nwhenever Ri(x, y) for all i ∈G.\nAgain, we write M, x ⊩φ if we want to emphasise the model M.\nCompare this with Deﬁnition 5.4. The cases for the boolean connectives\nare the same as for basic modal logic. Each Ki behaves like a 2, but refers to\nits own accessibility relation Ri. As already stated, there are no equivalents\nof 3, but we can recover them as ¬Ki¬. The connective EG is deﬁned in\nterms of the Ki and CG is deﬁned in terms of EG.\nMany of the results we had for basic modal logic with a single accessi-\nbility relation also hold in this more general setting of several accessibility\nrelations. Summarising,\nr a frame F for KT45n (W, (Ri)i∈A) for the modal logic KT45n is a set W of",
    "terms of the Ki and CG is deﬁned in terms of EG.\nMany of the results we had for basic modal logic with a single accessi-\nbility relation also hold in this more general setting of several accessibility\nrelations. Summarising,\nr a frame F for KT45n (W, (Ri)i∈A) for the modal logic KT45n is a set W of\nworlds and, for each i ∈A, an equivalence relation Ri on W.\nr a frame F = (W, (Ri)i∈A) for KT45n is said to satisfy φ if, for each labelling\nfunction L: W →P(Atoms) and each w ∈W, we have M, w ⊩φ holds, where\nM = (W, (Ri)i∈A, L). In that case, we say that F ⊨φ holds.\nThe following theorem is useful for answering questions about formu-\nlas involving E and C. Let M = (W, (Ri)i∈A, L) be a model for KT45n\n338\n5 Modal logics and agents\nand x, y ∈W. We say that y is G-reachable in k steps from x if there are\nw1, w2, . . . , wk−1 ∈W and i1, i2, . . . , ik in G such that\nx Ri1 w1 Ri2 w2 . . . Rik−1 wk−1 Rik y\nmeaning Ri1(x, w1), Ri2(w1, w2), . . . , Rik(wk, y). We also say that y is G-\nreachable from x if there is some k such that it is G-reachable in k steps.\nTheorem 5.26\n1.\nx ⊩Ek\nGφ iﬀ, for all y that are G-reachable from x in k steps, we have y ⊩φ.\n2.\nx ⊩CG φ iﬀ, for all y that are G-reachable from x, we have y ⊩φ.\nPROOF:\n1.\nFirst, suppose y ⊩φ for all y G-reachable from x in k steps. We will prove\nthat x ⊩Ek\nGφ holds. It is suﬃcient to show that x ⊩Ki1Ki2 . . . Kik φ for any\ni1, i2, . . . , ik ∈G. Take any i1, i2, . . . , ik ∈G and any w1, w2,. . . , wk−1 and y\nsuch that there is a path of the form x Ri1 w1 Ri2 w2 . . . Rik−1 wk−1 Rik y. Since\ny is G-reachable from x in k steps, we have y ⊩φ by our assumption, so x ⊩\nKi1Ki2 . . . Kik φ as required.\nConversely, suppose x ⊩Ek\nGφ holds and y is G-reachable from x in k steps. We\nmust show that y ⊩φ holds. Take i1, i2, . . . , ik by G-reachability; since x ⊩Ek\nGφ\nimplies x ⊩Ki1Ki2 . . . Kik φ, we have y ⊩φ.\n2.\nThis argument is similar.\nSome valid formulas in KT45n\nThe formula K holds for the connec-",
    "Gφ holds and y is G-reachable from x in k steps. We\nmust show that y ⊩φ holds. Take i1, i2, . . . , ik by G-reachability; since x ⊩Ek\nGφ\nimplies x ⊩Ki1Ki2 . . . Kik φ, we have y ⊩φ.\n2.\nThis argument is similar.\nSome valid formulas in KT45n\nThe formula K holds for the connec-\ntives Ki, EG, CG and DG, i.e. we have the corresponding formula schemes\nKi φ ∧Ki (φ →ψ) →Ki ψ\nEG φ ∧EG (φ →ψ) →EG ψ\nCG φ ∧CG (φ →ψ) →CG ψ\nDG φ ∧DG (φ →ψ) →DG ψ.\nThis means that these diﬀerent ‘levels’ of knowledge are closed under log-\nical consequence. For example, if certain facts are common knowledge and\nsome other fact follows logically from them, then that fact is also common\nknowledge.\nObserve that E, C and D are ‘box-like’ connectives, in the sense that\nthey quantify universally over certain accessibility relations. That is to say,\nwe may deﬁne the relations REG, RDG and RCG in terms of the relations\nRi, as follows:\nREG(x, y)\niﬀ\nRi(x, y)\nfor some i ∈G\nRDG(x, y)\niﬀ\nRi(x, y)\nfor all i ∈G\nRCG(x, y)\niﬀ\nRk\nEG(x, y)\nfor each k ≥1.\n5.5 Reasoning about knowledge in a multi-agent system\n339\nIt follows from this that EG, DG and CG satisfy the K formula with respect\nto the accessibility relations REG, RDG and RCG, respectively.\nWhat about other valid formulas? Since we have stipulated that the rela-\ntions Ri are equivalence relations, it follows from the multi-modal analogues\nof Theorem 5.13 and Table 5.12 that the following formulas are valid in\nKT45n for each agent i:\nKi φ →KiKi φ\npositive introspection\n¬Ki φ →Ki¬Ki φ\nnegative introspection\nKi φ →φ\ntruth.\nThese formulas also hold for DG, since RDG is also an equivalence rela-\ntion, but these don’t automatically generalise for EG and CG. For example,\nEG φ →EGEG φ is not valid; if it were valid, it would imply that common\nknowledge was nothing more than knowledge by everybody. The scheme\n¬EG φ →EG¬EG φ is also not valid. The failure of these formulas to be\nvalid can be traced to the fact that REG is not necessarily an equivalence",
    "EG φ →EGEG φ is not valid; if it were valid, it would imply that common\nknowledge was nothing more than knowledge by everybody. The scheme\n¬EG φ →EG¬EG φ is also not valid. The failure of these formulas to be\nvalid can be traced to the fact that REG is not necessarily an equivalence\nrelation, even though each Ri is an equivalence relation. However, REG is\nreﬂexive, so EG φ →φ is valid, provided that G ̸= ∅. If G = ∅, then EG φ\nholds vacuously, even if φ is false.\nSince RCG is an equivalence relation, the formulas T, 4 and 5 above do\nhold for CG, although the third one still requires the condition that G ̸= ∅.\n5.5.3 Natural deduction for KT45n\nThe proof system for KT45 is easily extended to KT45n; but for simplicity,\nwe omit reference to the connective D.\n1.\nThe dashed boxes now come in diﬀerent ‘ﬂavours’ for diﬀerent modal connec-\ntives; we’ll indicate the modality in the top left corner of the dashed box.\n2.\nThe axioms T, 4 and 5 can be used for any Ki, whereas axioms 4 and 5 can be\nused for CG, but not for EG – recall the discussion in Section 5.5.2.\n3.\nIn the rule CE, we may deduce Ek\nGφ from CG φ for any k; or we could go\ndirectly to Ki1 . . . Kik φ for any agents i1, . . . , ik∈G by using the rule CK.\nStrictly speaking, these rules are a whole set of such rules, one for each choice\nof k and i1, . . . , ik, but we refer to all of them as CE and CK respectively.\n4.\nApplying rule EKi, we may deduce Ki φ from EG φ for any i ∈G. From\n\u0001\ni∈G Ki φ we may deduce EG φ by virtue of rule KE. Note that the proof\nrule EKi is like a generalised and-elimination rule, whereas KE behaves like\nan and-introduction rule.\nThe proof rules for KT45n are summarised in Figure 5.14. As before, we\ncan think of the rules K4 and K5 and C4 and C5 as relaxations of the\n340\n5 Modal logics and agents\nKi\n...\nφ\nKiφ\nKii\nEG\n...\nφ\nEGφ\nEGi\nCG\n...\nφ\nCGφ\nCGi\nKiφ\nKi\n...\nφ\n...\nKie\nEGφ\nEG\n...\nφ\n...\nEGe\nCGφ\nCG\n...\nφ\n...\nCGe\nKi φ for each i ∈G\nEG φ\nKE\nEG φ\ni ∈G\nKi φ\nEKi\nCG φ\nEG . . . EG φ\nCE\nCG φ\nij ∈G",
    "can think of the rules K4 and K5 and C4 and C5 as relaxations of the\n340\n5 Modal logics and agents\nKi\n...\nφ\nKiφ\nKii\nEG\n...\nφ\nEGφ\nEGi\nCG\n...\nφ\nCGφ\nCGi\nKiφ\nKi\n...\nφ\n...\nKie\nEGφ\nEG\n...\nφ\n...\nEGe\nCGφ\nCG\n...\nφ\n...\nCGe\nKi φ for each i ∈G\nEG φ\nKE\nEG φ\ni ∈G\nKi φ\nEKi\nCG φ\nEG . . . EG φ\nCE\nCG φ\nij ∈G\nKi1 . . . Kik φ\nCK\nCG φ\nCGCG φ\nC4\n¬CG φ\nCG¬CG φ\nC5\nKi φ\nφ\nKT\nKi φ\nKiKi φ\nK4\n¬Ki φ\nKi¬Ki φ\nK5\nFigure 5.14. Natural deduction rules for KT45n.\nrules about moving formulas in and out of dashed proof boxes. Since rule\nK4 allows us to double-up Ki, we could instead think of it as allowing us\nto move formulas beginning with Ki into Ki-dashed boxes. Similarly, rule\nC5 has the eﬀect of allowing us to move formulas beginning with ¬CG into\nCG-dashed boxes.\nAn intuitive way of thinking about the dashed boxes is that formulas in\nthem are known to the agent in question. When you open a Ki-dashed box,\nyou are considering what agent i knows. It’s quite intuitive that an ordinary\nformula φ cannot be brought into such a dashed box, because the mere truth\nof φ does not mean that agent i knows it. In particular, you can’t use the\nrule ¬i if one of the premises of the rule is outside the dashed box you’re\nworking in.\n5.5 Reasoning about knowledge in a multi-agent system\n341\n1\nC(p ∨q)\npremise\n2\nK1(K2 p ∨K2 ¬p)\npremise\n3\nK1¬K2 q\npremise\n4\nK1K2 (p ∨q)\nCK 1\nK1\n5\nK2 (p ∨q)\nK1e 4\n6\nK2 p ∨K2 ¬p\nK1e 2\n7\n¬K2 q\nK1e 3\n8\nK2 p\nassumption\n9\np\naxiom T 8\n10\n11\n12\n13\n14\nK2 ¬p\nassumption\nK2\n¬p\nK2e 8\np ∨q\nK2e 5\nq\nprop 9, 10\nK2 q\nK2i 9−11\n⊥\n¬e 12, 7\np\n⊥e 13\n15\np\n∨e 6, 8−14, 8−14\n16\nK1 p\nK1i 5−15\nFigure 5.15. A proof of C(p ∨q), K1(K2p ∨K2 ¬p), K1¬K2 q |−K1 p.\nObserve the power of C φ in the premises: we can bring φ into any dashed\nbox by the application of the rules CK and Kie, no matter how deeply nested\nboxes are. The rule Ek φ, on the other hand, ensures that φ can be brought\ninto any dashed box with nesting ≤k. Compare this with Theorem 5.26.\nExample 5.27 We show that the sequent1 C(p ∨q), K1(K2p ∨K2 ¬p),",
    "box by the application of the rules CK and Kie, no matter how deeply nested\nboxes are. The rule Ek φ, on the other hand, ensures that φ can be brought\ninto any dashed box with nesting ≤k. Compare this with Theorem 5.26.\nExample 5.27 We show that the sequent1 C(p ∨q), K1(K2p ∨K2 ¬p),\nK1¬K2 q |−K1 p is valid in the modal logic KT45n. That means: if it is com-\nmon knowledge that p ∨q; and agent 1 knows that agent 2 knows whether\np is the case and also knows that agent 2 doesn’t know that q is true; then\nagent 1 knows that p is true. See Figure 5.15 for a proof. In line 12, we\nderived q from ¬p and p ∨q. Rather than show the full derivation in propo-\nsitional logic, which is not the focus here, we summarise by writing ‘prop’\nas the justiﬁcation for an inference in propositional logic.\n1 In this section we simply write |−for |−KT45n, unless indicated otherwise.\n342\n5 Modal logics and agents\n5.5.4 Formalising the examples\nNow that we have set up the modal logic KT45n, we can turn our attention to\nthe question of how to represent the wise-men and muddy-children puzzles in\nthis logic. Unfortunately, in spite of its sophistication, our logic is too simple\nto capture all the nuances of those examples. Although it has connectives\nfor representing diﬀerent items of knowledge held by diﬀerent agents, it\ndoes not have any temporal aspect, so it cannot directly capture the way\nin which the agents’ knowledge changes as time proceeds. We will overcome\nthis limitation by considering several ‘snapshots’ during which time is ﬁxed.\nThe wise-men puzzle\nRecall that there are three wise men; and it’s\ncommon knowledge that there are three red hats and two white hats. The\nking puts a hat on each of the wise men and asks them sequentially whether\nthey know the colour of the hat on their head – they are unable to see their\nown hat. We suppose the ﬁrst man says he does not know; then the second\nsays he does not know. We want to prove that, whatever the distribution of",
    "king puts a hat on each of the wise men and asks them sequentially whether\nthey know the colour of the hat on their head – they are unable to see their\nown hat. We suppose the ﬁrst man says he does not know; then the second\nsays he does not know. We want to prove that, whatever the distribution of\nhats, the third man now knows his hat is red.\nLet pi mean that man i has a red hat; so ¬pi means that man i has a\nwhite hat. Let Γ be the set of formulas\n{C(p1 ∨p2 ∨p3),\nC(p1 →K2 p1), C(¬p1 →K2 ¬p1),\nC(p1 →K3 p1), C(¬p1 →K3 ¬p1),\nC(p2 →K1 p2), C(¬p2 →K1 ¬p2),\nC(p2 →K3 p2), C(¬p2 →K3 ¬p2),\nC(p3 →K1 p3), C(¬p3 →K1 ¬p3),\nC(p3 →K2 p3), C(¬p3 →K2 ¬p3)}.\nThis corresponds to the initial set-up: it is common knowledge that one of\nthe hats must be red and that each man can see the colour of the other\nmen’s hats.\nThe announcement that the ﬁrst man doesn’t know the colour of his hat\namounts to the formula\nC(¬K1 p1 ∧¬K1 ¬p1)\nand similarly for the second man.\nA naive attempt at formalising the wise-men problem might go something\nlike this: we simply prove\nΓ, C(¬K1 p1 ∧¬K1 ¬p1), C(¬K2 p2 ∧¬K2 ¬p2) |−K3 p3\n5.5 Reasoning about knowledge in a multi-agent system\n343\ni.e. if Γ is true and the announcements are made, then the third man knows\nhis hat is red. However, this fails to capture the fact that time passes between\nthe announcements. The fact that C¬K1 p1 is true after the ﬁrst announce-\nment does not mean it is true after some subsequent announcement. For\nexample, if someone announces p1, then Cp1 becomes true.\nThe reason that this formalisation is incorrect is that, although knowledge\naccrues with time, lack of knowledge does not accrue with time. If I know φ,\nthen (assuming that φ doesn’t change) I will know it at the next time-point;\nbut if I do not know φ, it may be that I do know it at the next time point,\nsince I may acquire more knowledge.\nTo formalise the wise-men problem correctly, we need to break it into two",
    "then (assuming that φ doesn’t change) I will know it at the next time-point;\nbut if I do not know φ, it may be that I do know it at the next time point,\nsince I may acquire more knowledge.\nTo formalise the wise-men problem correctly, we need to break it into two\nentailments, one corresponding to each announcement. When the ﬁrst man\nannounces he does not know the colour of his hat, a certain positive formula\nφ becomes common knowledge. Our informal reasoning explained that all\nmen could then rule out the state RWW which, given p1 ∨p2 ∨p3, led them\nto the common knowledge of p2 ∨p3. Thus, φ is just p2 ∨p3 and we need to\nprove the entailment\nEntailment 1. Γ, C(¬K1 p1 ∧¬K1 ¬p1) |−C(p2 ∨p3).\nA proof of this sequent can be found in Figure 5.16.\nSince p2 ∨p3 is a positive formula, it persists with time and can be used\nin conjunction with the second announcement to prove the desired conclu-\nsion:\nEntailment 2. Γ, C(p2 ∨p3), C(¬K2 p2, ∧¬K2 ¬p2) |−K3 p3.\nThis method requires some careful thought: given an announcement of\nnegative information such as a man declaring that he does not know what\nthe colour of his hat is, we need to work out what positive-knowledge formula\ncan be derived from this and such new knowledge has to be suﬃcient to make\neven more progress towards solving the puzzle in the next round.\nRoutine proof segments like those in lines 11–16 of Figure 5.16 may be\nabbreviated into one step as long as all participating proof rules are recorded.\nThe resulting shorter representation can be seen in Figure 5.17.\nIn Figure 5.16, notice that the premises in lines 2 and 5 are not used.\nThe premises in lines 2 and 3 stand for any such formula for a given value\nof i and j, provided i ̸= j; this explains the inference made in line 8. In\nFigure 5.18, again notice that the premises in lines 1 and 5 are not used.\nObserve also that axiom T in conjunction with CK allows us to infer φ\nfrom any Cφ, although we had to split this up into two separate steps in",
    "of i and j, provided i ̸= j; this explains the inference made in line 8. In\nFigure 5.18, again notice that the premises in lines 1 and 5 are not used.\nObserve also that axiom T in conjunction with CK allows us to infer φ\nfrom any Cφ, although we had to split this up into two separate steps in\nlines 16 and 17. Practical implementations would probably allow for hybrid\nrules which condense such reasoning into one step.\n344\n5 Modal logics and agents\n1\nC(p1 ∨p2 ∨p3)\npremise\n2\nC(pi →Kj pi)\npremise, (i ̸= j)\n3\nC(¬pi →Kj ¬pi)\npremise, (i ̸= j)\n4\nC¬K1 p1\npremise\n5\nC¬K1 ¬p1\npremise\nC\n6\n7\n¬p2 ∧¬p3\nassumption\n8\n¬p2 →K1 ¬p2\nCe 3 (i, j) = (2, 1)\n9\n¬p3 →K1 ¬p3\nCe 3 (i, j) = (3, 1)\n10\nK1 ¬p2 ∧K1 ¬p3\nprop 7, 8, 9\n11\nK1 ¬p2\n∧e1 10\n12\nK1 ¬p3\n∧e2 10\nK1\n13\n14\n¬p2\nK1e 11\n15\n¬p3\nK1e 12\n16\n¬p2 ∧¬p3\n∧i 14, 15\n17\np1 ∨p2 ∨p3\nCe 1\n18\np1\nprop 16, 17\n19\nK1 p1\nK1i 13−18\n20\n¬K1 p1\nCe 4\n21\n⊥\n¬e 19, 20\n22\n¬(¬p2 ∧¬p3)\n¬i 7−21\n23\np2 ∨p3\nprop 22\n24\nC(p2 ∨p3)\nCi 6−23\nFigure 5.16. Proof of the sequent ‘Entailment 1’ for the wise-men puzzle.\nThe muddy-children puzzle\nSuppose there are n children. Let pi mean\nthat the ith child has mud on its forehead. We consider Scenario 2, in which\nthe father announces that one of the children is muddy. Similarly to the case\nfor the wise men, it is common knowledge that each child can see the other\nchildren, so it knows whether the others have mud, or not. Thus, for example,\n5.5 Reasoning about knowledge in a multi-agent system\n345\n1\nC(p1 ∨p2 ∨p3)\npremise\n2\nC(pi →Kj pi)\npremise, (i ̸= j)\n3\nC(¬pi →Kj ¬pi)\npremise, (i ̸= j)\n4\nC¬K1 p1\npremise\n5\nC¬K1 ¬p1\npremise\nC\n6\n7\n¬p2 ∧¬p3\nassumption\n8\n¬p2 →K1 ¬p2\nCe 3 (i, j) = (2, 1)\n9\n¬p3 →K1 ¬p3\nCe 3 (i, j) = (3, 1)\n10\nK1 ¬p2 ∧K1 ¬p3\nprop 7, 8, 9\nK1\n11\n12\n¬p2 ∧¬p3\n∧e1, K1e, ∧i\n13\np1 ∨p2 ∨p3\nCe 1\n14\np1\nprop 12, 13\n15\nK1 p1\nK1i 11−14\n16\n¬K1 p1\nCe 4\n17\n⊥\n¬e 15, 16\n18\n¬(¬p2 ∧¬p3)\n¬i 7−17\n19\np2 ∨p3\nprop 18\n20\nC(p2 ∨p3)\nCi 6−19\nFigure 5.17. A more compact representation of the proof in Figure 5.16.",
    "10\nK1 ¬p2 ∧K1 ¬p3\nprop 7, 8, 9\nK1\n11\n12\n¬p2 ∧¬p3\n∧e1, K1e, ∧i\n13\np1 ∨p2 ∨p3\nCe 1\n14\np1\nprop 12, 13\n15\nK1 p1\nK1i 11−14\n16\n¬K1 p1\nCe 4\n17\n⊥\n¬e 15, 16\n18\n¬(¬p2 ∧¬p3)\n¬i 7−17\n19\np2 ∨p3\nprop 18\n20\nC(p2 ∨p3)\nCi 6−19\nFigure 5.17. A more compact representation of the proof in Figure 5.16.\nwe have that C(p1 →K2 p1), which says that it is common knowledge that,\nif child 1 is muddy, then child 2 knows this and also C(¬p1 →K2 ¬p1). Let\nΓ be the collection of formulas:\nC(p1 ∨p2 ∨· · · ∨pn)\n\u0002\ni̸=j\nC(pi →Kj pi)\n\u0002\ni̸=j\nC(¬pi →Kj ¬pi).\n346\n5 Modal logics and agents\n1\nC(p1 ∨p2 ∨p3)\npremise\n2\nC(pi →Kj pi)\npremise, (i ̸= j)\n3\nC(¬pi →Kj ¬pi)\npremise, (i ̸= j)\n4\nC¬K2 p2\npremise\n5\nC¬K2 ¬p2\npremise\n6\nC(p2 ∨p3)\npremise\nK3\n7\n8\n¬p3\nassumption\n9\n¬p3 →K2 ¬p3\nCK 3 (i, j) = (3, 2)\n10\nK2 ¬p3\n→e 9, 8\nK2\n11\n12\n¬p3\nK2e 10\n13\np2 ∨p3\nCe 6\n14\np2\nprop 12, 13\n15\nK2 p2\nK2i 11−14\n16\nKi ¬K2 p2\nCK 4, for each i\n17\n¬K2 p2\nKT 16\n18\n⊥\n¬e 15, 17\n19\np3\nPBC 8−18\n20\nK3 p3\nK3i 7−19\nFigure 5.18. Proof of the sequent ‘Entailment 2’ for the wise-men puzzle.\nNote that \u0001\ni̸=j ψ(i,j) is a shorthand for the ﬁnite conjunction of all formulas\nψ(i,j), where i is diﬀerent from j. Let G be any set of children. We will\nrequire formulas of the form\nαG\ndef\n=\n\u0002\ni∈G\npi ∧\n\u0002\ni̸∈G\n¬pi.\nThe formula αG states that it is precisely the children in G that have muddy\nforeheads.\n5.5 Reasoning about knowledge in a multi-agent system\n347\n1\n¬p1 ∧¬p2 ∧· · · ∧pi ∧· · · ∧¬pn\nα{i}\n2\nC(p1 ∨· · · ∨pn)\nin Γ\n3\n¬pj\n∧e 1, for each j ̸= i\n4\n¬pj →Ki ¬pj\nin Γ, for each j ̸= i\n5\nKi ¬pj\n→e 4, 3, for each j ̸= i\n6\nKi (p1 ∨· · · ∨pn)\nCK 2\nKi\n7\n8\np1 ∨· · · ∨pn\nKi e 6\n9\n¬pj\nKi e 5, for each j ̸= i\n10\npi\nprop 9, 8\n11\nKi pi\nKi i\nFigure 5.19. Proof of the sequent ‘Entailment 1’ for the muddy-children\npuzzle.\nSuppose now that k = 1, i.e. that one child has mud on its forehead. We\nwould like to show that that child knows that it is the one. We prove the\nfollowing entailment.\nEntailment 1. Γ, α{i} |−Ki pi.",
    "11\nKi pi\nKi i\nFigure 5.19. Proof of the sequent ‘Entailment 1’ for the muddy-children\npuzzle.\nSuppose now that k = 1, i.e. that one child has mud on its forehead. We\nwould like to show that that child knows that it is the one. We prove the\nfollowing entailment.\nEntailment 1. Γ, α{i} |−Ki pi.\nThis says that, if the actual situation is one in which only one child\ncalled i has mud, then that child will know it. Our proof follows exactly\nthe same lines as the intuition: i sees that no other children have mud,\nbut knows that at least one has mud, so knows it must be itself who has\na muddy forehead. The proof is given in Figure 5.19.\nNote that the comment ‘for each j ̸= i’ means that we supply this argu-\nment for any such j. Thus, we can form the conjunction of all these inferences\nwhich we left implicit in the inference on line 10.\nWhat if there is more than one child with mud? In this case, the children\nall announce in the ﬁrst parallel round that they do not know whether they\nare muddy or not, corresponding to the formula\nA\ndef\n= C(¬K1 p1 ∧¬K1 ¬p1) ∧· · · ∧C(¬Kn pn ∧¬Kn ¬pn).\nWe saw in the wise-men example that it is dangerous to put the announce-\nment A alongside the premises Γ, because the truth of A, which has negative\nclaims about the children’s knowledge, cannot be guaranteed to persist with\n348\n5 Modal logics and agents\ntime. So we seek some positive formula which represents what the children\nlearn upon hearing the announcement. As in the wise-men example, this for-\nmula is implicit in the informal reasoning about the muddy children given\nin Section 5.5.1: if it is common knowledge that there are at least k muddy\nchildren, then, after an announcement of the form A, it will be common\nknowledge that there are at least k + 1 muddy children.\nTherefore, after the ﬁrst announcement A, the set of premises is\nΓ,\n\u0002\n1≤i≤n\nC¬α{i}.\nThis is Γ together with the common knowledge that the set of muddy chil-\ndren is not a singleton set.",
    "knowledge that there are at least k + 1 muddy children.\nTherefore, after the ﬁrst announcement A, the set of premises is\nΓ,\n\u0002\n1≤i≤n\nC¬α{i}.\nThis is Γ together with the common knowledge that the set of muddy chil-\ndren is not a singleton set.\nAfter the second announcement A, the set of premises becomes\nΓ,\n\u0002\n1≤i≤n\nC¬α{i},\n\u0002\ni̸=j\nC¬α{i,j}\nwhich we may write as\nΓ,\n\u0002\n|G|≤2\nC¬αG.\nPlease try carefully to understand the notation:\nαG\nthe set of muddy children is precisely the set G\n¬αG\nthe set of muddy children is some other set than G\n\u0002\n|G|≤k\n¬αG\nthe set of muddy children is of size greater than k.\nThe entailment corresponding to the second round is:\nΓ, C(\n\u0002\n|G|≤2\n¬αG), αH |−\n\u0002\ni∈H\nKi pi,\nwhere |H| = 3 .\nThe entailment corresponding to the kth round is:\nEntailment 2. Γ, C(\u0001\n|G|≤k ¬αG), αH |−\u0001\ni∈H Ki pi, where |H| = k + 1.\nPlease try carefully to understand what this sequent is saying. ‘If all\nthe things in Γ are true and if it is common knowledge that the set of\nmuddy children is not of size less than or equal to k and if actually it is\nof size k + 1, then each of those k + 1 children can deduce that they are\nmuddy.’ Notice how this ﬁts with our intuitive account given earlier in\nthis text.\n5.5 Reasoning about knowledge in a multi-agent system\n349\n1\nαH\npremise\n2\nC¬αG\npremise as |G| ≤k\n3\npj\n∧e 1, for each j ∈G\n4\n¬pk\n∧e 1, for each k ̸∈H\n5\npj →Ki pj\nin Γ for each j ∈G\n6\nKi pj\n→e 5, 4, for each j ∈G\n7\n¬pk →Ki ¬pk\nin Γ for each k ̸∈H\n8\nKi ¬pk\n→e 7, 4, for each k ̸∈H\n9\nKi ¬αG\nCK 2\nKi\n10\n11\npj\nKi e 6 (j ∈G)\n12\n¬pk\nKi e 8 (k ̸∈H)\n13\n¬pi\nassumption\n14\nαG\n∧i 11, 12, 13\n15\n¬αG\nKi e 9\n16\n⊥\n¬e 14, 15\n17\n¬¬pi\n¬i 13−16\n18\npi\n¬¬e 17\nKi pi\n19\nKi i 10−18\nFigure 5.20. The proof of Γ, C(¬αG), αH |−Ki pi, used to prove ‘En-\ntailment 2’ for the muddy-children puzzle.\nTo prove Entailment 2, take any i ∈H. It is suﬃcient to prove that\nΓ, C(\n\u0002\n|G|≤k\n¬αG), αH |−Ki pi\nis valid, as the repeated use of ∧i over all values of i gives us a proof of",
    "19\nKi i 10−18\nFigure 5.20. The proof of Γ, C(¬αG), αH |−Ki pi, used to prove ‘En-\ntailment 2’ for the muddy-children puzzle.\nTo prove Entailment 2, take any i ∈H. It is suﬃcient to prove that\nΓ, C(\n\u0002\n|G|≤k\n¬αG), αH |−Ki pi\nis valid, as the repeated use of ∧i over all values of i gives us a proof of\nEntailment 2. Let G be H −{i}; the proof that Γ, C(¬αG), αH |−Ki pi\nis valid is given in Figure 5.20. Please study this proof in every detail\nand understand how it is just following the steps taken in the informal\nproof in Section 5.5.1.\n350\n5 Modal logics and agents\nThe line 14 of the proof in Figure 5.20 applies several instances of ∧i in\nsequence and is a legitimate step since the formulas in lines 11–13 had been\nshown ‘for each’ element in the respective set.\n5.6 Exercises\nExercises 5.1\n1. Think about the highly distributed computing environments of today with their\ndynamic communication and network topology. Come up with several kinds of\nmodes of truth pertaining to statements made about such environments.\n2. Let M be a model of ﬁrst-order logic and let φ range over formulas of ﬁrst-order\nlogic. Discuss in what sense statements of the form ‘Formula φ is true in model\nM.’ express a mode of truth.\nExercises 5.2\n1. Consider the Kripke model M depicted in Figure 5.5.\n(a) For each of the following, determine whether it holds:\ni. a ⊩p\nii. a ⊩2¬q\niii.\n*\na ⊩q\niv.\n*\na ⊩22q\nv. a ⊩3p\nvi.\n*\na ⊩23¬q\nvii. c ⊩3⊤\nviii. d ⊩3⊤\nix. d ⊩22q\nx.\n*\nc ⊩2⊥\nxi. b ⊩2⊥\nxii. a ⊩33(p ∧q) ∧3⊤.\n(b) Find for each of the following a world which satisﬁes it:\ni. 2¬p ∧22¬p\nii. 3q ∧¬2q\niii.\n*\n3p ∨3q\niv.\n*\n3(p ∨3q)\nv. 2p ∨2¬p\nvi. 2(p ∨¬p).\n(c) For each formula of the previous item, ﬁnd a world which does not satisfy\nthe formula.\n2. Find a Kripke model M and a formula scheme which is not satisﬁed in M, but\nwhich has true instances in M.\n5.6 Exercises\n351\n3. Consider the Kripke model M = (W, R, L) where W = {a, b, c, d, e}; R =",
    "(c) For each formula of the previous item, ﬁnd a world which does not satisfy\nthe formula.\n2. Find a Kripke model M and a formula scheme which is not satisﬁed in M, but\nwhich has true instances in M.\n5.6 Exercises\n351\n3. Consider the Kripke model M = (W, R, L) where W = {a, b, c, d, e}; R =\n{(a, c), (a, e), (b, a), (b, c), (d, e), (e, a)}; and L(a) = {p}, L(b) = {p, q}, L(c) =\n{p, q}, L(d) = {q} and L(e) = ∅.\n(a) Draw a graph for M.\n(b) Investigate which of the formulas in exercise 1(b) on page 350 have a world\nwhich satisﬁes it.\n4. (a) Think about what you have to do to decide whether p →23q is true in a\nmodel.\n(b)\n*\nFind a model in which it is true and one in which it is false.\n5. For each of the following pairs of formulas, can you ﬁnd a model and a world in\nit which distinguishes them, i.e. makes one of them true and one false? In that\ncase, you are showing that they do not entail each other. If you cannot, it might\nmean that the formulas are equivalent. Justify your answer.\n(a) 2p and 22p\n(b) 2¬p and ¬3p\n(c) 2(p ∧q) and 2p ∧2q\n(d)\n*\n3(p ∧q) and 3p ∧3q\n(e) 2(p ∨q) and 2p ∨2q\n(f)\n*\n3(p ∨q) and 3p ∨3q\n(g) 2(p →q) and 2p →2q\n(h) 3⊤and ⊤\n(i) 2⊤and ⊤\n(j) 3⊥and ⊥.\n6. Show that the following formulas of basic modal logic are valid:\n(a)\n*\n2(φ ∧ψ) ↔(2φ ∧2ψ)\n(b) 3(φ ∨ψ) ↔(3φ ∨3ψ)\n(c)\n*\n2⊤↔⊤\n(d) 3⊥↔⊥\n(e) 3⊤→(2φ →3φ)\n7. Inspect Deﬁnition 5.4. We said that we deﬁned x ⊩φ by structural induction on\nφ. Is this really correct? Note the implicit deﬁnition of a second relation x ̸⊩φ.\nWhy is this deﬁnition still correct and in what sense does it still rely on structural\ninduction?\nExercises 5.3\n1. For which of the readings of 2 in Table 5.7 are the formulas below valid?\n(a)\n*\n(φ →2φ) →(φ →3φ)\n(b) (2φ →(φ ∧22φ ∧3φ)) →((2φ →(φ ∧22φ)) ∧(3φ →23φ)).\n2. Dynamic logic: Let P range over the programs of our core language in Chapter 4.\nConsider a modal logic whose modal operators are ⟨P⟩and [P] for all such\nprograms P. Evaluate such formulas in stores l as in Deﬁnition 4.3 (page 264).\n352",
    "(b) (2φ →(φ ∧22φ ∧3φ)) →((2φ →(φ ∧22φ)) ∧(3φ →23φ)).\n2. Dynamic logic: Let P range over the programs of our core language in Chapter 4.\nConsider a modal logic whose modal operators are ⟨P⟩and [P] for all such\nprograms P. Evaluate such formulas in stores l as in Deﬁnition 4.3 (page 264).\n352\n5 Modal logics and agents\nThe relation l ⊨⟨P⟩φ holds iﬀprogram P has some execution beginning in\nstore l and terminating in a store satisfying φ.\n(a)\n*\nGiven that ¬⟨P⟩¬ equals [P], spell out the meaning of [P].\n(b) Say that φ is valid iﬀit holds in all suitable stores l. State the total cor-\nrectness of a Hoare triple as a validity problem in this modal logic.\n3. For all binary relations R below, determine which of the properties reﬂexive\nthrough to total from page 320 apply to R where R(x, y) means that\n(a)\n*\nx is strictly less than y, where x and y range over all natural numbers n ≥1\n(b) x divides y, where x and y range over integers – e.g. 5 divides 15, whereas\n7 does not\n(c) x is a brother of y\n(d)\n*\nthere exist positive real numbers a and b such that x equals a · y + b, where\nx and y range over real numbers.\n4.\n*\nProve the Fact 5.16.\n5. Prove the informal claim made in item 2 of Example 5.12 by structural induction\non formulas in (5.1).\n6. Prove Theorem 5.17. Use mathematical induction on the length of the sequence\nof negations and modal operators. Note that this requires a case analysis over\nthe topmost operator other than a negation, or a modality.\n7. Prove Theorem 5.14, but for the case in which R is reﬂexive, or transitive.\n8. Find a Kripke model in which all worlds satisfy ¬p ∨q, but at least one world\ndoes not satisfy ¬q ∨p; i.e. show that the scheme ¬φ ∨ψ is not satisﬁed.\n9. Below you ﬁnd a list of sequents Γ ⊢φ in propositional logic. Find out whether\nyou can prove them without the use of the rules PBC, LEM and ¬¬e. If you\ncannot succeed, then try to construct a model M = (W, R, L) for intuitionistic",
    "9. Below you ﬁnd a list of sequents Γ ⊢φ in propositional logic. Find out whether\nyou can prove them without the use of the rules PBC, LEM and ¬¬e. If you\ncannot succeed, then try to construct a model M = (W, R, L) for intuitionistic\npropositional logic such that one of its worlds satisﬁes all formulas in Γ, but\ndoes not satisfy φ. Assuming soundness, this would guarantee that the sequent\nin question does not have a proof in intuitionistic propositional logic.\n(a)\n*\n⊢(p →q) ∨(q →r)\n(b) The proof rule MT: p →q, ¬q ⊢¬p\n(c) ¬p ∨q ⊢p →q\n(d) p →q ⊢¬p ∨q\n(e) The proof rule ¬¬e: ¬¬p ⊢p\n(f)\n*\nThe proof rule ¬¬i: p ⊢¬¬p.\n10. Prove that the natural deduction rules for propositional logic without the rules\n¬¬e, LEM and PBC are sound for the possible world semantics of intuitionis-\ntic propositional logic. Why does this show that the excluded rules cannot be\nimplemented using the remaining ones?\n11. Interpreting 2φ as ‘agent Q believes φ,’ explain the meaning of the following\nformula schemes:\n(a) 2φ →3φ\n(b)\n*\n2φ ∨2¬φ\n(c) 2(φ →ψ) ∧2φ →2ψ.\n5.6 Exercises\n353\n12. In the second row of Table 5.7, we adopted the convention that the future\nexcludes the present. Which formula schemes would be satisﬁed in that row if\ninstead we adopted the more common convention that the future includes the\npresent?\n13. Consider the properties in Table 5.12. Which ones should we accept if we read\n2 as\n(a)\n*\nknowledge\n(b) belief\n(c)\n*\n‘always in the future?’\n14. Find a frame which is reﬂexive, transitive, but not symmetric. Show that your\nframe does not satisfy the formula p →23p, by providing a suitable labelling\nfunction and choosing a world which refutes p →23p. Can you ﬁnd a labelling\nfunction and world which does satisfy p →23p in your frame?\n15. Give two examples of frames which are Euclidean – i.e. their accessibility rela-\ntion is Euclidean – and two which are not. Explain intuitively why 3p →23p\nholds on the ﬁrst two, but not on the latter two.",
    "function and world which does satisfy p →23p in your frame?\n15. Give two examples of frames which are Euclidean – i.e. their accessibility rela-\ntion is Euclidean – and two which are not. Explain intuitively why 3p →23p\nholds on the ﬁrst two, but not on the latter two.\n16. For each of the following formulas, ﬁnd the property of R which corresponds to\nit.\n(a) φ →2φ\n(b)\n*\n2⊥\n(c)\n*\n32φ →23φ.\n17.\n*\nFind a formula whose corresponding property is density: for all x, z ∈W such\nthat R(x, z), there exists y ∈W such that R(x, y) and R(y, z).\n18. The modal logic KD45 is used to model belief; see Table 5.12 for the axiom\nschemes D, 4, and 5.\n(a) Explain how it diﬀers from KT45.\n(b) Show that ⊨KD45 2p →3p is valid. What is the signiﬁcance of this, in terms\nof knowledge and belief?\n(c) Explain why the condition of seriality is relevant to belief.\n19. Recall Deﬁnition 5.7. How would you deﬁne ≡L for a modal logic L?\nExercises 5.4\n1. Find natural deduction proofs for the following sequents over the basic modal\nlogic K.\n(a)\n*\n⊢K 2(p →q) |−2p →2q\n(b) ⊢K 2(p →q) |−3p →3q\n(c)\n*\n⊢K|−2(p →q) ∧2(q →r) →2(p →r)\n(d) ⊢K 2(p ∧q) |−2p ∧2q\n(e) ⊢K|−3⊤→(2p →3p)\n(f)\n*\n⊢K 3(p →q) |−2p →3q\n(g) ⊢K 3(p ∨q) |−3p ∨3q.\n354\n5 Modal logics and agents\n2. Find natural deduction proofs for the following, in modal logic KT45.\n(a) p →23p\n(b) 23p ↔3p\n(c)\n*\n32p ↔2p\n(d) 2(2p →2q) ∨2(2q →2p)\n(e) 2(3p →q) ↔2(p →2q).\n3. Study the proofs you gave for the previous exercise to see whether any of\nthese formula schemes could be valid in basic modal logic. Inspect where and\nhow these proofs used the axioms T, 4 and 5 to see whether you can ﬁnd a\ncounter example, i.e. a Kripke model and a world which does not satisfy the\nformula.\n4. Provide a sketch of an argument which shows that the natural deduction rules\nfor basic modal logic are sound with respect to the semantics x ⊩φ over Kripke\nstructures.\nExercises 5.5\n1. This exercise is about the wise-men puzzle. Justify your answers.",
    "formula.\n4. Provide a sketch of an argument which shows that the natural deduction rules\nfor basic modal logic are sound with respect to the semantics x ⊩φ over Kripke\nstructures.\nExercises 5.5\n1. This exercise is about the wise-men puzzle. Justify your answers.\n(a) Each man is asked the question ‘Do you know the colour of your hat?’\nSuppose that the ﬁrst man says ‘no,’ but the second one says ‘yes.’ Given\nthis information together with the common knowledge, can we infer the\ncolour of his hat?\n(b) Can we predict whether the third man will now answer ‘yes’ or ‘no?’\n(c) What would be the situation if the third man were blind? What about the\nﬁrst man?\n2. This exercise is about the muddy-children puzzle. Suppose k = 4, say children\na, b, c and d have mud on their foreheads. Explain why, before the father’s\nannouncement, it is not common knowledge that someone is dirty.\n3. Write formulas for the following:\n(a) Agent 1 knows that p.\n(b) Agent 1 knows that p or q.\n(c)\n*\nAgent 1 knows p or agent 1 knows q.\n(d) Agent 1 knows whether p.\n(e) Agent 1 doesn’t know whether p or q.\n(f) Agent 1 knows whether agent 2 knows p.\n(g)\n*\nAgent 1 knows whether agent 2 knows whether p.\n(h) No-one knows p.\n(i) Not everyone knows whether p.\n(j) Anyone who knows p knows q.\n(k)\n*\nSome people know p but don’t know q.\n(l) Everyone knows someone who knows p.\n4. Determine which of the following hold in the Kripke model of Figure 5.13 and\njustify your answer:\n5.6 Exercises\n355\n(a) x1 ⊩K1 p\n(b) x3 ⊩K1 (p ∨q)\n(c) x1 ⊩K2 q\n(d)\n*\nx3 ⊩E(p ∨q)\n(e) x1 ⊩Cq\n(f) x1 ⊩D{1,3}p\n(g) x1 ⊩D{1,2}p\n(h) x6 ⊩E¬q\n(i)\n*\nx6 ⊩C¬q\n(j) x6 ⊩C{3}¬q.\n5. For each of the following formulas, show that it is not valid by ﬁnding a Kripke\nmodel with a world not satisfying the formula:\n(a) EG φ →EGEG φ\n(b) ¬EG φ →EG¬EG φ.\nExplain why these two Kripke models show that the union of equivalence rela-\ntions is not necessarily an equivalence relation.\n6.\n*\nExplain why CG φ →CGCG φ and ¬CG φ →CG¬CG φ are valid.",
    "model with a world not satisfying the formula:\n(a) EG φ →EGEG φ\n(b) ¬EG φ →EG¬EG φ.\nExplain why these two Kripke models show that the union of equivalence rela-\ntions is not necessarily an equivalence relation.\n6.\n*\nExplain why CG φ →CGCG φ and ¬CG φ →CG¬CG φ are valid.\n7. Prove the second part of Theorem 5.26.\n8. Recall Section 3.7. Can you specify a monotone function over the power set\nof possible worlds which computes the set of worlds satisfying CG φ? Is this a\nleast, or a greatest, ﬁxed point?\n9. Use the natural deduction rules for propositional logic to justify the proof steps\nbelow which are only annotated with ‘prop.’\n(a) Line 11 in Figure 5.15.\n(b) Lines 10, 18 and 23 of the proof in Figure 5.16. Of course this requires three\nseparate proofs.\n(c) Line 14 of the proof in Figure 5.18.\n(d) Line 10 of the proof in Figure 5.19.\n10. Using the natural deduction rules for KT45n, prove the validity of\n(a) Ki (p ∧q) ↔Ki p ∧Ki q\n(b) C(p ∧q) ↔Cp ∧Cq\n(c)\n*\nKi Cp ↔Cp\n(d) C Ki p ↔Cp\n(e)\n*\n¬φ →Ki¬Ki φ.\nExplain what this formula means in terms of knowledge. Do you believe it?\n(f) ¬φ →K1K2¬K2K1 φ\n(g)\n*\n¬K1¬K1φ ↔K1 φ.\n11. Do a natural deduction proof for a simpler version of the wise-men problem:\nThere are two wise men; as usual, they can see each other’s hats but not their\nown. It is common knowledge that there’s only one white hat available and two\nred ones. So at least one of the men is wearing a red one. Man 1 informs the\nsecond that he doesn’t know which hat he is wearing. Man 2 says, ‘Aha, then\nI must be wearing a red hat.’\n356\n5 Modal logics and agents\n(a) Justify man 2’s conclusion informally.\n(b) Let p1, p2 respectively, mean man 1, 2 respectively, is wearing a red hat.\nSo ¬p1, ¬p2 mean they (respectively) are wearing a white one. Informally\njustify each of the following premises in terms of the description of the\nproblem:\ni. K2K1 (p1 ∨p2)\nii. K2(¬p2 →K1 ¬p2)\niii. K2¬K1 p1.\n(c) Using natural deduction, prove from these premises that K2 p2.",
    "So ¬p1, ¬p2 mean they (respectively) are wearing a white one. Informally\njustify each of the following premises in terms of the description of the\nproblem:\ni. K2K1 (p1 ∨p2)\nii. K2(¬p2 →K1 ¬p2)\niii. K2¬K1 p1.\n(c) Using natural deduction, prove from these premises that K2 p2.\n(d) Show that the third premise was essential, by exhibiting a model/world\nwhich satisﬁes the ﬁrst two, but not the conclusion.\n(e) Now is it easy to answer questions like ‘If man 2 were blind would he still be\nable to tell?’ and ‘if man 1 were blind, would man 2 still be able to tell?’?\n12. Recall our informal discussion on positive-knowledge formulas and negative-\nknowledge formulas. Give formal deﬁnitions of these notions.\n5.7 Bibliographic notes\nThe ﬁrst systematic approaches to modal logic were made by C. I. Lewis\nin the 1950s. The possible-worlds approach, which greatly simpliﬁed modal\nlogic and is now almost synonymous with it, was invented by S. Kripke.\nBooks devoted to modal logic include [Che80, Gol87, Pop94], where exten-\nsive references to the literature may be found. All these books discuss the\nsoundness and completeness of proof calculi for modal logics. They also in-\nvestigate which modal logics have the ﬁnite-model property: if a sequent\ndoes not have a proof, there is a ﬁnite model which demonstrates that. Not\nall modal logics enjoy this property, which is important for decidability.\nIntuitionistic propositional logic has the ﬁnite-model property; an anima-\ntion which generates such ﬁnite models (called PORGI) is available from\nA. Stoughton’s website2.\nThe idea of using modal logic to reason about knowledge is due to J.\nHintikka. A great deal of work on applying modal logic to multi-agent sys-\ntems has been done in [FHMV95] and [MvdH95] and other work by those\nauthors. Many examples in this chapter are taken from this literature (some\nof them are attributed to other people there), though our treatment of them\nis original.",
    "tems has been done in [FHMV95] and [MvdH95] and other work by those\nauthors. Many examples in this chapter are taken from this literature (some\nof them are attributed to other people there), though our treatment of them\nis original.\nThe natural deduction proof system for modal logic presented in this\nchapter is based on ideas in [Fit93].\n2 www.cis.ksu.edu/~allen/porgi.html\n5.7 Bibliographic notes\n357\nAn application of the modal logic KT4 (more precisely, its fragment with-\nout negation) as a type system for staged computation in a functional pro-\ngramming language can be found in [DP96].\nWe should stress that our framework was deliberately ‘classical;’ the thesis\n[Sim94] is a good source for discussions of intuitionistic modal logics; it also\ncontains a gentle introduction to basic ﬁrst-order modal logic.\n6\nBinary decision diagrams\n6.1 Representing boolean functions\nBoolean functions are an important descriptive formalism for many hard-\nware and software systems, such as synchronous and asynchronous circuits,\nreactive systems and ﬁnite-state programs. Representing those systems in a\ncomputer in order to reason about them requires an eﬃcient representation\nfor boolean functions. We look at such a representation in this chapter and\ndescribe in detail how the systems discussed in Chapter 3 can be veriﬁed\nusing the representation.\nDeﬁnition 6.1 A boolean variable x is a variable ranging over the values\n0 and 1. We write x1, x2, . . . and x, y, z, . . . to denote boolean variables. We\ndeﬁne the following functions on the set {0, 1}:\nr 0\ndef\n= 1 and 1\ndef\n= 0;\nr x · y\ndef\n= 1 if x and y have value 1; otherwise x · y\ndef\n= 0;\nr x + y\ndef\n= 0 if x and y have value 0; otherwise x + y\ndef\n= 1;\nr x ⊕y\ndef\n= 1 if exactly one of x and y equals 1.\nA boolean function f of n arguments is a function from {0, 1}n to {0, 1}.\nWe write f(x1, x2, . . . , xn), or f(V ), to indicate that a syntactic representa-\ntion of f depends on the boolean variables in V only.",
    "def\n= 1;\nr x ⊕y\ndef\n= 1 if exactly one of x and y equals 1.\nA boolean function f of n arguments is a function from {0, 1}n to {0, 1}.\nWe write f(x1, x2, . . . , xn), or f(V ), to indicate that a syntactic representa-\ntion of f depends on the boolean variables in V only.\nNote that ·, + and ⊕are boolean functions with two arguments, whereas\n¯ is a boolean function that takes one argument. The binary functions ·,\n+ and ⊕are written in inﬁx notation instead of preﬁx; i.e. we write x + y\ninstead of +(x, y), etc.\n358\n6.1 Representing boolean functions\n359\nExample 6.2 In terms of the four functions above, we can deﬁne other\nboolean functions such as\n(1)\nf(x, y)\ndef\n= x · (y + x)\n(2)\ng(x, y)\ndef\n= x · y + (1 ⊕x)\n(3)\nh(x, y, z)\ndef\n= x + y · (x ⊕y)\n(4)\nk()\ndef\n= 1 ⊕(0 · 1).\n6.1.1 Propositional formulas and truth tables\nTruth tables and propositional formulas are two diﬀerent representations of\nboolean functions. In propositional formulas, ∧denotes ·, ∨denotes +, ¬\ndenotes ¯ and ⊤and ⊥denote 1 and 0, respectively.\nBoolean functions are represented by truth tables in the obvious way; for\nexample, the function f(x, y)\ndef\n= x + y is represented by the truth table on\nthe left:\nx\ny\nf(x, y)\n1\n1\n0\n0\n1\n0\n1\n0\n0\n0\n0\n1\np\nq\nφ\nT\nT\nF\nF\nT\nF\nT\nF\nF\nF\nF\nT\nOn the right, we show the same truth table using the notation of Chapter 1; a\nformula having this truth table is ¬(p ∨q). In this chapter, we may mix these\ntwo notational systems of boolean formulas and formulas of propositional\nlogic whenever it is convenient. You should be able to translate expressions\neasily from one notation to the other and vice versa.\nAs representations of boolean functions, propositional formulas and truth\ntables have diﬀerent advantages and disadvantages. Truth tables are very\nspace-ineﬃcient: if one wanted to model the functionality of a sequential\ncircuit by a boolean function of 100 variables (a small chip component would\neasily require this many variables), then the truth table would require 2100",
    "space-ineﬃcient: if one wanted to model the functionality of a sequential\ncircuit by a boolean function of 100 variables (a small chip component would\neasily require this many variables), then the truth table would require 2100\n(which is more than 1030) lines. Alas, there is not enough storage space\n(whether paper or particle) in the universe to record the information of\n2100 diﬀerent bit vectors of length 100. Although they are space ineﬃcient,\noperations on truth tables are simple. Once you have computed a truth table,\nit is easy to see whether the boolean function represented is satisﬁable: you\njust look to see if there is a 1 in the last column of the table.\nComparing whether two truth tables represent the same boolean function\nalso seems easy: assuming the two tables are presented with the same order\n360\n6 Binary decision diagrams\nof valuations, we simply check that they are identical. Although these opera-\ntions seem simple, however, they are computationally intractable because of\nthe fact that the number of lines in the truth table is exponential in the num-\nber of variables. Checking satisﬁability of a function with n atoms requires\nof the order of 2n operations if the function is represented as a truth table.\nWe conclude that checking satisﬁability and equivalence is highly ineﬃcient\nwith the truth-table representation.\nRepresentation of boolean functions by propositional formulas is slightly\nbetter. Propositional formulas often provide a wonderfully compact and eﬃ-\ncient presentation of boolean functions. A formula with 100 variables might\nonly be about 200–300 characters long. However, deciding whether an arbi-\ntrary propositional formula is satisﬁable is a famous problem in computer\nscience: no eﬃcient algorithms for this task are known, and it is strongly\nsuspected that there aren’t any. Similarly, deciding whether two arbitrary\npropositional formulas f and g denote the same boolean function is sus-\npected to be exponentially expensive.",
    "science: no eﬃcient algorithms for this task are known, and it is strongly\nsuspected that there aren’t any. Similarly, deciding whether two arbitrary\npropositional formulas f and g denote the same boolean function is sus-\npected to be exponentially expensive.\nIt is straightforward to see how to perform the boolean operations ·, +, ⊕\nand ¯ on these two representations. In the case of truth tables, they involve\napplying the operation to each line; for example, given truth tables for f and\ng over the same set of variables (and in the same order), the truth table for\nf ⊕g is obtained by applying ⊕to the truth value of f and g in each line. If\nf and g do not have the same set of arguments, it is easy to pad them out\nby adding further arguments. In the case of representation by propositional\nformulas, the operations ·, ⊕, etc., are simply syntactic manipulations. For\nexample, given formulas φ and ψ representing the functions f and g, the\nformulas representing f · g and f ⊕g are, respectively, φ ∧ψ and (φ ∧¬ψ) ∨\n(¬φ ∧ψ).\nWe could also consider representing boolean functions by various sub-\nclasses of propositional formulas, such as conjunctive and disjunctive normal\nforms. In the case of disjunctive normal form (DNF, in which a formula is a\ndisjunction of conjunctions of literals), the representation is sometimes com-\npact, but in the worst cases it can be very lengthy. Checking satisﬁability is a\nstraightforward operation, however, because it is suﬃcient to ﬁnd a disjunct\nwhich does not have two complementary literals. Unfortunately, there is not\na similar way of checking validity. Performing + on two formulas in DNF\nsimply involves inserting ∨between them. Performing · is more complicated;\nwe cannot simply insert ∧between the two formulas, because the result will\nnot in general be in DNF, so we have to perform lengthy applications of\nthe distributivity rule φ ∧(ψ1 ∨ψ2) ≡(φ ∧ψ1) ∨(φ ∧ψ1). Computing the\nnegation of a DNF formula is also expensive. The DNF formula φ may be",
    "we cannot simply insert ∧between the two formulas, because the result will\nnot in general be in DNF, so we have to perform lengthy applications of\nthe distributivity rule φ ∧(ψ1 ∨ψ2) ≡(φ ∧ψ1) ∨(φ ∧ψ1). Computing the\nnegation of a DNF formula is also expensive. The DNF formula φ may be\n6.1 Representing boolean functions\n361\nRepresentation of\ntest for\nboolean operations\nboolean functions\ncompact?\nsatisf’ty\nvalidity\n·\n+\n¯\nProp. formulas\noften\nhard\nhard\neasy\neasy\neasy\nFormulas in DNF\nsometimes\neasy\nhard\nhard\neasy\nhard\nFormulas in CNF\nsometimes\nhard\neasy\neasy\nhard\nhard\nOrdered truth tables\nnever\nhard\nhard\nhard\nhard\nhard\nReduced OBDDs\noften\neasy\neasy\nmedium\nmedium\neasy\nFigure 6.1. Comparing efficiency of five representations of boolean formulas.\ny\ny\n0\n1\n0\n0\nx\nFigure 6.2. An example of a binary decision tree.\nquite short, whereas the length of the disjunctive normal form of ¬φ can be\nexponential in the length of φ.\nThe situation for representation in conjunctive normal form is the dual. A\nsummary of these remarks is contained in Figure 6.1 (for now, please ignore\nthe last row).\n6.1.2 Binary decision diagrams\nBinary decision diagrams (BDDs) are another way of representing boolean\nfunctions. A certain class of such diagrams will provide the implementational\nframework for our symbolic model-checking algorithm. Binary decision di-\nagrams were ﬁrst considered in a simpler form called binary decision trees.\nThese are trees whose non-terminal nodes are labelled with boolean vari-\nables x, y, z, . . . and whose terminal nodes are labelled with either 0 or 1.\nEach non-terminal node has two edges, one dashed line and one solid line.\nIn Figure 6.2 you can see such a binary decision tree with two layers of\nvariables x and y.\nDeﬁnition 6.3 Let T be a ﬁnite binary decision tree. Then T determines\na unique boolean function of the variables in non-terminal nodes, in the\nfollowing way. Given an assignment of 0s and 1s to the boolean variables\n362\n6 Binary decision diagrams\n1\n0\ny\nx\ny\n1\n0\ny",
    "variables x and y.\nDeﬁnition 6.3 Let T be a ﬁnite binary decision tree. Then T determines\na unique boolean function of the variables in non-terminal nodes, in the\nfollowing way. Given an assignment of 0s and 1s to the boolean variables\n362\n6 Binary decision diagrams\n1\n0\ny\nx\ny\n1\n0\ny\nx\nFigure 6.3. (a) Sharing the terminal nodes of the binary decision tree\nin Figure 6.2; (b) further optimisation by removing a redundant decision\npoint.\noccurring in T, we start at the root of T and take the dashed line when-\never the value of the variable at the current node is 0; otherwise, we travel\nalong the solid line. The function value is the value of the terminal node we\nreach.\nFor example, the binary decision tree of Figure 6.2 represents a boolean\nfunction f(x, y). To ﬁnd f(0, 1), start at the root of the tree. Since the value\nof x is 0 we follow the dashed line out of the node labelled x and arrive\nat the leftmost node labelled y. Since y’s value is 1, we follow the solid\nline out of that y-node and arrive at the leftmost terminal node labelled\n0. Thus, f(0, 1) equals 0. In computing f(0, 0), we similarly travel down\nthe tree, but now following two dashed lines to obtain 1 as a result. You\ncan see that the two other possibilities result in reaching the remaining\ntwo terminal nodes labelled 0. Thus, this binary decision tree computes the\nfunction f(x, y)\ndef\n= x + y.\nBinary decision trees are quite close to the representation of boolean func-\ntions as truth tables as far as their sizes are concerned. If the root of a binary\ndecision tree is an x-node then it has two subtrees (one for the value of x\nbeing 0 and another one for x having value 1). So if f depends on n boolean\nvariables, the corresponding binary decision tree will have at least 2n+1 −1\nnodes (see exercise 5 on page 399). Since f’s truth table has 2n lines, we\nsee that decision trees as such are not a more compact representation of\nboolean functions. However, binary decision trees often contain some redun-",
    "nodes (see exercise 5 on page 399). Since f’s truth table has 2n lines, we\nsee that decision trees as such are not a more compact representation of\nboolean functions. However, binary decision trees often contain some redun-\ndancy which we can exploit.\nSince 0 and 1 are the only terminal nodes of binary decision trees, we can\noptimise the representation by having pointers to just one copy of 0 and\none copy of 1. For example, the binary decision tree in Figure 6.2 can be\noptimised in this way and the resulting structure is depicted in Figure 6.3(a).\nNote that we saved storage space for two redundant terminal 0-nodes, but\nthat we still have as many edges (pointers) as before.\n6.1 Representing boolean functions\n363\nx\nx\n1\n0\nz\ny\ny\ny\ny\nFigure 6.4. A BDD with duplicated subBDDs.\nA second optimisation we can do is to remove unnecessary decision points\nin the tree. In Figure 6.3(a), the right-hand y is unnecessary, because we go\nto the same place whether it is 0 or 1. Therefore the structure could be\nfurther reduced, to the one shown on the right, (b).\nAll these structures are examples of binary decision diagrams (BDDs).\nThey are more general than binary decision trees; the sharing of the leaves\nmeans they are not trees. As a third optimisation, we also allow subBDDs to\nbe shared. A subBDD is the part of a BDD occurring below a given node. For\nexample, in the BDD of Figure 6.4, the two inner y-nodes perform the same\nrole, because the subBDDs below them have the same structure. Therefore,\none of them could be removed, resulting in the BDD in Figure 6.5(a). Indeed,\nthe left-most y-node could also be merged with the middle one; then the\nx-node above both of them would become redundant. Removing it would\nresult in the BDD on the right of Figure 6.5.\nTo summarise, we encountered three diﬀerent ways of reducing a BDD to\na more compact form:\nC1. Removal of duplicate terminals. If a BDD contains more than one",
    "x-node above both of them would become redundant. Removing it would\nresult in the BDD on the right of Figure 6.5.\nTo summarise, we encountered three diﬀerent ways of reducing a BDD to\na more compact form:\nC1. Removal of duplicate terminals. If a BDD contains more than one\nterminal 0-node, then we redirect all edges which point to such a 0-node to\njust one of them. We proceed in the same way with terminal nodes labelled\nwith 1.\nC2. Removal of redundant tests. If both outgoing edges of a node n\npoint to the same node m, then we eliminate that node n, sending all its\nincoming edges to m.\nC3. Removal of duplicate non-terminals. If two distinct nodes n and\nm in the BDD are the roots of structurally identical subBDDs, then we\n364\n6 Binary decision diagrams\nx\n1\n0\nz\ny\ny\ny\nx\n0\n1\nz\ny\ny\nx\nFigure 6.5. The BDD of Figure 6.4: (a) after removal of one of the\nduplicate y-nodes; (b) after removal of another duplicate y-node and\nthen a redundant x-decision point.\neliminate one of them, say m, and redirect all its incoming edges to the\nother one.\nNote that C1 is a special case of C3. In order to deﬁne BDDs precisely,\nwe need a few auxiliary notions.\nDeﬁnition 6.4 A directed graph is a set G and a binary relation →on G:\n→⊆G × G. A cycle in a directed graph is a ﬁnite path in that graph that\nbegins and ends at the same node, i.e. a path of the form v1 →v2 →· · · →\nvn →v1. A directed acyclic graph (dag) is a directed graph that does not\nhave any cycles. A node of a dag is initial if there are no edges pointing to\nthat node. A node is called terminal if there are no edges out of that node.\nThe directed graph in Figure 3.3 on page 179 has cycles, for example\nthe cycle s0 →s1 →s0, and is not a dag. If we interpret the links in BDDs\n(whether solid or dashed) as always going in a downwards direction, then\nthe BDDs of this chapter are also directed graphs. They are also acyclic and\nhave a unique initial node. The optimisations C1–C3 preserve the property",
    "the cycle s0 →s1 →s0, and is not a dag. If we interpret the links in BDDs\n(whether solid or dashed) as always going in a downwards direction, then\nthe BDDs of this chapter are also directed graphs. They are also acyclic and\nhave a unique initial node. The optimisations C1–C3 preserve the property\nof being a dag; and fully reduced BDDs have precisely two terminal nodes.\nWe now formally deﬁne BDDs as certain kinds of dags:\nDeﬁnition 6.5 A binary decision diagram (BDD) is a ﬁnite dag with\na unique initial node, where all terminal nodes are labelled with 0 or\n1 and all non-terminal nodes are labelled with a boolean variable. Each\n6.1 Representing boolean functions\n365\n0\n1\n0\nx\nFigure 6.6. The BDDs (a) B0, representing the constant 0 boolean\nfunction; similarly, the BDD B1 has only one node 1 and represents\nthe constant 1 boolean function; and (b) Bx, representing the boolean\nvariable x.\nnon-terminal node has exactly two edges from that node to others: one la-\nbelled 0 and one labelled 1 (we represent them as a dashed line and a solid\nline, respectively).\nA BDD is said to be reduced if none of the optimisations C1–C3 can be\napplied (i.e. no more reductions are possible).\nAll the decision structures we have seen in this chapter (Figures 6.2–6.5)\nare BDDs, as are the constant functions B0 and B1, and the function Bx\nfrom Figure 6.6. If B is a BDD where V = {x1, x2, . . . , xn} is the set of labels\nof non-terminal nodes, then B determines a boolean function f(V ) in the\nsame way as binary decision trees (see Deﬁnition 6.3): given an assignment\nof 0s and 1s to the variables in V , we compute the value of f by starting\nwith the unique initial node. If its variable has value 0, we follow the dashed\nline; otherwise we take the solid line. We continue for each node until we\nreach a terminal node. Since the BDD is ﬁnite by deﬁnition, we eventually\nreach a terminal node which is labelled with 0 or 1. That label is the result\nof f for that particular assignment of truth values.",
    "line; otherwise we take the solid line. We continue for each node until we\nreach a terminal node. Since the BDD is ﬁnite by deﬁnition, we eventually\nreach a terminal node which is labelled with 0 or 1. That label is the result\nof f for that particular assignment of truth values.\nThe deﬁnition of a BDD does not prohibit that a boolean variable occur\nmore than once on a path in the dag. For example, consider the BDD in\nFigure 6.7.\nSuch a representation is wasteful, however. The solid link from the left-\nmost x to the 1-terminal is never taken, for example, because one can only\nget to that x-node when x has value 0.\nThanks to the reductions C1–C3, BDDs can often be quite compact rep-\nresentations of boolean functions. Let us consider how to check satisﬁability\nand perform the boolean operations on functions represented as BDDs. A\nBDD represents a satisﬁable function if a 1-terminal node is reachable from\nthe root along a consistent path in a BDD which represents it. A consistent\npath is one which, for every variable, has only dashed lines or only solid lines\nleaving nodes labelled by that variable. (In other words, we cannot assign\n366\n6 Binary decision diagrams\nx\ny\nz\nx\ny\nx\n0\n1\nFigure 6.7. A BDD where some boolean variables occur more than\nonce on an evaluation path.\na variable the values 0 and 1 simultaneously.) Checking validity is similar,\nbut we check that no 0-terminal is reachable by a consistent path.\nThe operations · and + can be performed by ‘surgery’ on the component\nBDDs. Given BDDs Bf and Bg representing boolean functions f and g, a\nBDD representing f · g can be obtained by taking the BDD f and replacing\nall its 1-terminals by Bg. To see why this is so, consider how to get to a\n1-terminal in the resulting BDD. You have to satisfy the requirements for\ngetting to a 1 imposed by both of the BDDs. Similarly, a BDD for f + g\ncan be obtained by replacing all 0 terminals of Bf by Bg. Note that these",
    "all its 1-terminals by Bg. To see why this is so, consider how to get to a\n1-terminal in the resulting BDD. You have to satisfy the requirements for\ngetting to a 1 imposed by both of the BDDs. Similarly, a BDD for f + g\ncan be obtained by replacing all 0 terminals of Bf by Bg. Note that these\noperations are likely to generate BDDs with multiple occurrences of variables\nalong a path. Later, in Section 6.2, we will see deﬁnitions of + and · on BDDs\nthat don’t have this undesirable eﬀect.\nThe complementation operation ¯ is also possible: a BDD representing f\ncan be obtained by replacing all 0-terminals in Bf by 1-terminals and vice\nversa. Figure 6.8 shows the complement of the BDD in Figure 6.2.\n6.1.3 Ordered BDDs\nWe have seen that the representation of boolean functions by BDDs is often\ncompact, thanks to the sharing of information aﬀorded by the reductions\nC1–C3. However, BDDs with multiple occurrences of a boolean variable\nalong a path seem rather ineﬃcient. Moreover, there seems no easy way to\ntest for equivalence of BDDs. For example, the BDDs of Figures 6.7 and 6.9\nrepresent the same boolean function (the reader should check this). Neither\nof them can be optimised further by applying the rules C1–C3. However,\n6.1 Representing boolean functions\n367\nx\ny\ny\n0\n1\n1\n1\nFigure 6.8. The complement of the BDD in Figure 6.2.\ny\n0\n1\nx\ny\nz\nFigure 6.9. A BDD representing the same function as the BDD of\nFigure 6.7, but having the variable ordering [x, y, z].\ntesting whether they denote the same boolean function seems to involve as\nmuch computational eﬀort as computing the entire truth table for f(x, y, z).\nWe can improve matters by imposing an ordering on the variables occur-\nring along any path. We then adhere to that same ordering for all the BDDs\nwe manipulate.\nDeﬁnition 6.6 Let [x1, . . . , xn] be an ordered list of variables without du-\nplications and let B be a BDD all of whose variables occur somewhere in",
    "ring along any path. We then adhere to that same ordering for all the BDDs\nwe manipulate.\nDeﬁnition 6.6 Let [x1, . . . , xn] be an ordered list of variables without du-\nplications and let B be a BDD all of whose variables occur somewhere in\nthe list. We say that B has the ordering [x1, . . . , xn] if all variable labels of\nB occur in that list and, for every occurrence of xi followed by xj along any\npath in B, we have i < j.\nAn ordered BDD (OBDD) is a BDD which has an ordering for some list\nof variables.\nNote that the BDDs of Figures 6.3(a,b) and 6.9 are ordered (with ordering\n[x, y]). We don’t insist that every variable in the list is used in the paths.\nThus, the OBDDs of Figures 6.3 and 6.9 have the ordering [x, y, z] and so\n368\n6 Binary decision diagrams\n0\n1\nz\nx\ny\ny\nx\nFigure 6.10. A BDD which does not have an ordering of variables.\ndoes any list having x, y and z in it in that order, such as [u, x, y, v, z, w] and\n[x, u, y, z]. Even the BDDs B0 and B1 in Figure 6.6 are OBDDs, a suitable\nordering list being the empty list (there are no variables), or indeed any list.\nThe BDD Bx of Figure 6.6(b) is also an OBDD, with any list containing x\nas its ordering.\nThe BDD of Figure 6.7 is not ordered. To see why this is so, consider the\npath taken if the values of x and y are 0. We begin with the root, an x-\nnode, and reach a y-node and then an x-node again. Thus, no matter what\nlist arrangement we choose (remembering that no double occurrences are\nallowed), this path violates the ordering condition. Another example of a\nBDD that is not ordered can be seen in Figure 6.10. In that case, we cannot\nﬁnd an order since the path for (x, y, z) ⇒(0, 0, 0) – meaning that x, y and z\nare assigned 0 – shows that y needs to occur before x in such a list, whereas\nthe path for (x, y, z) ⇒(1, 1, 1) demands that x be before y.\nIt follows from the deﬁnition of OBDDs that one cannot have multiple\noccurrences of any variable along a path.",
    "are assigned 0 – shows that y needs to occur before x in such a list, whereas\nthe path for (x, y, z) ⇒(1, 1, 1) demands that x be before y.\nIt follows from the deﬁnition of OBDDs that one cannot have multiple\noccurrences of any variable along a path.\nWhen operations are performed on two OBDDs, we usually require that\nthey have compatible variable orderings. The orderings of B1 and B2 are\nsaid to be compatible if there are no variables x and y such that x comes\nbefore y in the ordering of B1 and y comes before x in the ordering of B2.\nThis commitment to an ordering gives us a unique representation of boolean\nfunctions as OBDDs. For example, the BDDs in Figures 6.8 and 6.9 have\ncompatible variable orderings.\nTheorem 6.7 The reduced OBDD representing a given function f is\nunique. That is to say, let B and B′ be two reduced OBDDs with\n6.1 Representing boolean functions\n369\ncompatible variable orderings. If B and B represent the same boolean func-\ntion, then they have identical structure.\nIn other words, with OBDDs we cannot get a situation like the one en-\ncountered earlier, in which we have two distinct reduced BDDs which repre-\nsent the same function, provided that the orderings are compatible. It follows\nthat checking equivalence of OBDDs is immediate. Checking whether two\nOBDDs (having compatible orderings) represent the same function is simply\na matter of checking whether they have the same structure1.\nA useful consequence of the theorem above is that, if we apply the reduc-\ntions C1–C3 to an OBDD until no further reductions are possible, then we\nare guaranteed that the result is always the same reduced OBDD. The order\nin which we applied the reductions does not matter. We therefore say that\nOBDDs have a canonical form, namely their unique reduced OBDD. Most\nother representations (conjunctive normal forms, etc.) do not have canonical\nforms.\nThe algorithms for · and + for BDDs, presented in Section 6.1.2, won’t",
    "in which we applied the reductions does not matter. We therefore say that\nOBDDs have a canonical form, namely their unique reduced OBDD. Most\nother representations (conjunctive normal forms, etc.) do not have canonical\nforms.\nThe algorithms for · and + for BDDs, presented in Section 6.1.2, won’t\nwork for OBDDs as they may introduce multiple occurrences of the same\nvariable on a path. We will soon develop more sophisticated algorithms\nfor these operations on OBDDs, which exploit the compatible ordering of\nvariables in paths.\nOBDDs allow compact representations of certain classes of boolean func-\ntions which only have exponential representations in other systems, such as\ntruth tables and conjunctive normal forms. As an example consider the even\nparity function feven(x1, x2, . . . , xn) which is deﬁned to be 1 if there is an\neven number of variables xi with value 1; otherwise, it is deﬁned to be 0.\nIts representation as an OBDD requires only 2n + 1 nodes. Its OBDD for\nn = 4 and the ordering [x1, x2, x3, x4] can be found in Figure 6.11.\nThe impact of the chosen variable ordering\nThe size of the OBDD\nrepresenting the parity functions is independent of the chosen variable or-\ndering. This is because the parity functions are themselves independent of\nthe order of variables: swapping the values of any two variables does not\nchange the value of the function; such functions are called symmetric.\nHowever, in general the chosen variable ordering makes a signiﬁcant dif-\nference to the size of the OBDD representing a given function. Consider\nthe boolean function (x1 + x2) · (x3 + x4) · · · · · (x2n−1 + x2n); it corresponds\nto a propositional formula in conjunctive normal form. If we choose the\n1 In an implementation this will amount to checking whether two pointers are equal.\n370\n6 Binary decision diagrams\n1\n0\nx1\nx2\nx3\nx4\nx3\nx2\nx4\nFigure 6.11. An OBDD for the even parity function for four bits.\n‘natural’ ordering [x1, x2, x3, x4, . . . ], then we can represent this function as",
    "1 In an implementation this will amount to checking whether two pointers are equal.\n370\n6 Binary decision diagrams\n1\n0\nx1\nx2\nx3\nx4\nx3\nx2\nx4\nFigure 6.11. An OBDD for the even parity function for four bits.\n‘natural’ ordering [x1, x2, x3, x4, . . . ], then we can represent this function as\nan OBDD with 2n + 2 nodes. Figure 6.12 shows the resulting OBDD for\nn = 3. Unfortunately, if we choose instead the ordering\n[x1, x3, . . . , x2n−1, x2, x4, . . . , x2n]\nthe resulting OBDD requires 2n+1 nodes; the OBDD for n = 3 can be seen\nin Figure 6.13.\nThe sensitivity of the size of an OBDD to the particular variable order-\ning is a price we pay for all the advantages that OBDDs have over BDDs.\nAlthough ﬁnding the optimal ordering is itself a computationally expensive\nproblem, there are good heuristics which will usually produce a fairly good\nordering. Later on we return to this issue in discussions of applications.\nThe importance of canonical representation\nThe importance of\nhaving a canonical form for OBDDs in conjunction with an eﬃcient test for\ndeciding whether two reduced OBDDs are isomorphic cannot be overesti-\nmated. It allows us to perform the following tests:\nAbsence of redundant variables. If the value of the boolean function\nf(x1, x2, . . . , xn) does not depend on the value of xi, then any reduced\nOBDD which represents f does not contain any xi-node.\nTest for semantic equivalence. If two functions f(x1, x2, . . . , xn) and\ng(x1, x2, . . . , xn) are represented by OBDDs Bf, respectively Bg, with a\ncompatible ordering of variables, then we can eﬃciently decide whether f\nand g are semantically equivalent. We reduce Bf and Bg (if necessary); f\n6.1 Representing boolean functions\n371\n0\n1\nx1\nx6\nx5\nx3\nx4\nx2\nFigure 6.12. The OBDD for (x1 + x2) · (x3 + x4) · (x5 + x6) with vari-\nable ordering [x1, x2, x3, x4, x5, x6].\nx1\nx3\nx3\nx5\nx5\nx5\nx2\nx2\nx2\nx4\nx4\n1\nx6\n0\nx2\nx5\nFigure 6.13. Changing the ordering may have dramatic effects on the",
    "6.1 Representing boolean functions\n371\n0\n1\nx1\nx6\nx5\nx3\nx4\nx2\nFigure 6.12. The OBDD for (x1 + x2) · (x3 + x4) · (x5 + x6) with vari-\nable ordering [x1, x2, x3, x4, x5, x6].\nx1\nx3\nx3\nx5\nx5\nx5\nx2\nx2\nx2\nx4\nx4\n1\nx6\n0\nx2\nx5\nFigure 6.13. Changing the ordering may have dramatic effects on the\nsize of an OBDD: the OBDD for (x1 + x2) · (x3 + x4) · (x5 + x6) with\nvariable ordering [x1, x3, x5, x2, x4, x6].\n372\n6 Binary decision diagrams\nand g denote the same boolean functions if, and only if, the reduced OBDDs\nhave identical structure.\nTest for validity. We can test a function f(x1, x2, . . . , xn) for validity (i.e.\nf always computes 1) in the following way. Compute a reduced OBDD for\nf. Then f is valid if, and only if, its reduced OBDD is B1.\nTest for implication. We can test whether f(x1, x2, . . . , xn) implies g(x1,\nx2, . . . , xn) (i.e. whenever f computes 1, then so does g) by computing the\nreduced OBDD for f · g. This is B0 iﬀthe implication holds.\nTest for satisﬁability. We can test a function f(x1, x2, . . . , xn) for satis-\nﬁability (f computes 1 for at least one assignment of 0 and 1 values to its\nvariables). The function f is satisﬁable iﬀits reduced OBDD is not B0.\n6.2 Algorithms for reduced OBDDs\n6.2.1 The algorithm reduce\nThe reductions C1–C3 are at the core of any serious use of OBDDs, for\nwhenever we construct a BDD we will want to convert it to its reduced form.\nIn this section, we describe an algorithm reduce which does this eﬃciently\nfor ordered BDDs.\nIf the ordering of B is [x1, x2, . . . , xl], then B has at most l + 1 layers. The\nalgorithm reduce now traverses B layer by layer in a bottom-up fashion,\nbeginning with the terminal nodes. In traversing B, it assigns an integer\nlabel id(n) to each node n of B, in such a way that the subOBDDs with\nroot nodes n and m denote the same boolean function if, and only if, id(n)\nequals id(m).\nSince reduce starts with the layer of terminal nodes, it assigns the ﬁrst",
    "label id(n) to each node n of B, in such a way that the subOBDDs with\nroot nodes n and m denote the same boolean function if, and only if, id(n)\nequals id(m).\nSince reduce starts with the layer of terminal nodes, it assigns the ﬁrst\nlabel (say #0) to the ﬁrst 0-node it encounters. All other terminal 0-nodes\ndenote the same function as the ﬁrst 0-node and therefore get the same label\n(compare with reduction C1). Similarly, the 1-nodes all get the next label,\nsay #1.\nNow let us inductively assume that reduce has already assigned integer\nlabels to all nodes of a layer > i (i.e. all terminal nodes and xj-nodes with\nj > i). We describe how nodes of layer i (i.e. xi-nodes) are being handled.\nDeﬁnition 6.8 Given a non-terminal node n in a BDD, we deﬁne lo(n) to\nbe the node pointed to via the dashed line from n. Dually, hi(n) is the node\npointed to via the solid line from n.\nLet us describe how the labelling is done. Given an xi-node n, there are\nthree ways in which it may get its label:\n6.2 Algorithms for reduced OBDDs\n373\n0\n1\n#0\n#1\n0\n1\n0\n1\nx3\nx3\nx2\nx2\nx1\n#0\n#1\n#0\n#1\n#2\n#2\n#3\n#2\n#4\n=⇒\nx3\nx2\nx1\n#2\n#3\n#4\nReduce\nFigure 6.14. An example execution of the algorithm reduce.\nr If the label id(lo(n)) is the same as id(hi(n)), then we set id(n) to be that label.\nThat is because the boolean function represented at n is the same function as the\none represented at lo(n) and hi(n). In other words, node n performs a redundant\ntest and can be eliminated by reduction C2.\nr If there is another node m such that n and m have the same variable xi, and\nid(lo(n)) = id(lo(m)) and id(hi(n)) = id(hi(m)), then we set id(n) to be id(m).\nThis is because the nodes n and m compute the same boolean function (compare\nwith reduction C3).\nr Otherwise, we set id(n) to the next unused integer label.\nNote that only the last case creates a new label. Consider the OBDD\nin left side of Figure 6.14; each node has an integer label obtained in the",
    "with reduction C3).\nr Otherwise, we set id(n) to the next unused integer label.\nNote that only the last case creates a new label. Consider the OBDD\nin left side of Figure 6.14; each node has an integer label obtained in the\nmanner just described. The algorithm reduce then ﬁnishes by redirecting\nedges bottom-up as outlined in C1–C3. The resulting reduced OBDD is in\nright of Figure 6.14. Since there are eﬃcient bottom-up traversal algorithms\nfor dags, reduce is an eﬃcient operation in the number of nodes of an\nOBDD.\n6.2.2 The algorithm apply\nAnother procedure at the heart of OBDDs is the algorithm apply. It is\nused to implement operations on boolean functions such as +, · , ⊕and\ncomplementation (via f ⊕1). Given OBDDs Bf and Bg for boolean formulas\nf and g, the call apply (op, Bf, Bg) computes the reduced OBDD of the\nboolean formula f op g, where op denotes any function from {0, 1} × {0, 1}\nto {0, 1}.\n374\n6 Binary decision diagrams\nThe intuition behind the apply algorithm is fairly simple. The algorithm\noperates recursively on the structure of the two OBDDs:\n1.\nlet v be the variable highest in the ordering (=leftmost in the list) which occurs\nin Bf or Bg.\n2.\nsplit the problem into two subproblems for v being 0 and v being 1 and solve\nrecursively;\n3.\nat the leaves, apply the boolean operation op directly.\nThe result will usually have to be reduced to make it into an OBDD. Some\nreduction can be done ‘on the ﬂy’ in step 2, by avoiding the creation of a new\nnode if both branches are equal (in which case return the common result),\nor if an equivalent node already exists (in which case, use it).\nLet us make all this more precise and detailed.\nDeﬁnition 6.9 Let f be a boolean formula and x a variable.\n1.\nWe denote by f[0/x] the boolean formula obtained by replacing all occurrences\nof x in f by 0. The formula f[1/x] is deﬁned similarly. The expressions f[0/x]\nand f[1/x] are called restrictions of f.\n2.",
    "Deﬁnition 6.9 Let f be a boolean formula and x a variable.\n1.\nWe denote by f[0/x] the boolean formula obtained by replacing all occurrences\nof x in f by 0. The formula f[1/x] is deﬁned similarly. The expressions f[0/x]\nand f[1/x] are called restrictions of f.\n2.\nWe say that two boolean formulas f and g are semantically equivalent if they\nrepresent the same boolean function (with respect to the boolean variables that\nthey depend upon). In that case, we write f ≡g.\nFor example, if f(x, y)\ndef\n= x · (y + x), then f[0/x](x, y) equals 0 · (y + 0),\nwhich is semantically equivalent to 0. Similarly, f[1/y](x, y) is x · (1 + x),\nwhich is semantically equivalent to x.\nRestrictions allow us to perform recursion on boolean formulas, by decom-\nposing boolean formulas into simpler ones. For example, if x is a variable in\nf, then f is equivalent to x · f[0/x] + x · f[1/x]. To see this, consider the case\nx = 0; the expression computes to f[0/x]. When x = 1 it yields f[1/x]. This\nobservation is known as the Shannon expansion, although it can already be\nfound in G. Boole’s book ‘The Laws of Thought’ from 1854.\nLemma 6.10 (Shannon expansion) For all boolean formulas f and all\nboolean variables x (even those not occurring in f) we have\nf ≡x · f[0/x] + x · f[1/x].\n(6.1)\nThe function apply is based on the Shannon expansion for f op g:\nf op g = xi · (f[0/xi] op g[0/xi]) + xi · (f[1/xi] op g[1/xi]).\n(6.2)\nThis is used as a control structure of apply which proceeds from the roots\n6.2 Algorithms for reduced OBDDs\n375\n0\n1\n0\n1\nx4\nx3\nx1\nR5\nR6\nR4\nR2\nR1\nR3\n+\nS1\nS3\nS4\nS5\nS2\nx4\nx2\nx1\nx3\nFigure 6.15. An example of two arguments for a call apply (+, Bf, Bg).\nof Bf and Bg downwards to construct nodes of the OBDD Bf op g. Let rf be\nthe root node of Bf and rg the root node of Bg.\n1.\nIf both rf and rg are terminal nodes with labels lf and lg, respectively (recall\nthat terminal labels are either 0 or 1), then we compute the value lf op lg and",
    "of Bf and Bg downwards to construct nodes of the OBDD Bf op g. Let rf be\nthe root node of Bf and rg the root node of Bg.\n1.\nIf both rf and rg are terminal nodes with labels lf and lg, respectively (recall\nthat terminal labels are either 0 or 1), then we compute the value lf op lg and\nlet the resulting OBDD be B0 if that value is 0 and B1 otherwise.\n2.\nIn the remaining cases, at least one of the root nodes is a non-terminal. Suppose\nthat both root nodes are xi-nodes. Then we create an xi-node n with a dashed\nline to apply (op, lo(rf), lo(rg)) and a solid line to apply (op, hi(rf), hi(rg)), i.e.\nwe call apply recursively on the basis of (6.2).\n3.\nIf rf is an xi-node, but rg is a terminal node or an xj-node with j > i,\nthen we know that there is no xi-node in Bg because the two OBDDs have\na compatible ordering of boolean variables. Thus, g is independent of xi\n(g ≡g[0/xi] ≡g[1/xi]). Therefore, we create an xi-node n with a dashed line\nto apply (op, lo(rf), rg) and a solid line to apply (op, hi(rf), rg).\n4.\nThe case in which rg is a non-terminal, but rf is a terminal or an xj-node with\nj > i, is handled symmetrically to case 3.\nThe result of this procedure might not be reduced; therefore apply ﬁnishes\nby calling the function reduce on the OBDD it constructed. An example of\napply (where op is +) can be seen in Figures 6.15–6.17. Figure 6.16 shows\nthe recursive descent control structure of apply and Figure 6.17 shows the\nﬁnal result. In this example, the result of apply (+, Bf, Bg) is Bf.\nFigure 6.16 shows that numerous calls to apply occur several times with\nthe same arguments. Eﬃciency could be gained if these were evaluated only\n376\n6 Binary decision diagrams\n(R1, S1)\nx1\nx2\nx3\n(R3, S3)\n(R2, S3)\n(R3, S2)\nx4\nx3\n(R5, S4)\n(R6, S5)\n(R4, S3)\n(R6, S3)\n(R4, S3)\n(R4, S3)\nx4\n(R5, S4)\n(R6, S5)\n(R6, S5)\nx4\n(R6, S5)\nx4\n(R5, S4)\n(R6, S4)\n(R6, S5)\nFigure 6.16. The recursive call structure of apply for the example in\nFigure 6.15 (without memoisation).\n0\n1\nx4\nx3\nx2\nx1",
    "(R1, S1)\nx1\nx2\nx3\n(R3, S3)\n(R2, S3)\n(R3, S2)\nx4\nx3\n(R5, S4)\n(R6, S5)\n(R4, S3)\n(R6, S3)\n(R4, S3)\n(R4, S3)\nx4\n(R5, S4)\n(R6, S5)\n(R6, S5)\nx4\n(R6, S5)\nx4\n(R5, S4)\n(R6, S4)\n(R6, S5)\nFigure 6.16. The recursive call structure of apply for the example in\nFigure 6.15 (without memoisation).\n0\n1\nx4\nx3\nx2\nx1\nFigure 6.17. The result of apply (+, Bf, Bg), where Bf and Bg are given\nin Figure 6.15.\n6.2 Algorithms for reduced OBDDs\n377\nthe ﬁrst time and the result remembered for future calls. This program-\nming technique is known as memoisation. As well as being more eﬃcient,\nit has the advantage that the resulting OBDD requires less reduction. (In\nthis example, using memoisation eliminates the need for the ﬁnal call to\nreduce altogether.) Without memoisation, apply is exponential in the size\nof its arguments, since each non-leaf call generates a further two calls. With\nmemoisation, the number of calls to apply is bounded by 2 · |Bf| · |Bg|, where\n|B| is the size of the BDD. This is a worst-time complexity; the actual per-\nformance is often much better than this.\n6.2.3 The algorithm restrict\nGiven an OBDD Bf representing a boolean formula f, we need an algo-\nrithm restrict such that the call restrict(0, x, Bf) computes the reduced\nOBDD representing f[0/x] using the same variable ordering as Bf. The al-\ngorithm for restrict(0, x, Bf) works as follows. For each node n labelled\nwith x, incoming edges are redirected to lo(n) and n is removed. Then we\ncall reduce on the resulting OBDD. The call restrict (1, x, Bf) proceeds\nsimilarly, only we now redirect incoming edges to hi(n).\n6.2.4 The algorithm exists\nA boolean function can be thought of as putting a constraint on the values\nof its argument variables. For example, the function x + (y · z) evaluates to 1\nonly if x is 1; or y is 0 and z is 1 – this is a constraint on x, y, and z.\nIt is useful to be able to express the relaxation of the constraint on a subset\nof the variables concerned. To allow this, we write ∃x. f for the boolean",
    "only if x is 1; or y is 0 and z is 1 – this is a constraint on x, y, and z.\nIt is useful to be able to express the relaxation of the constraint on a subset\nof the variables concerned. To allow this, we write ∃x. f for the boolean\nfunction f with the constraint on x relaxed. Formally, ∃x. f is deﬁned as\nf[0/x] + f[1/x]; that is, ∃x. f is true if f could be made true by putting x\nto 0 or to 1. Given that ∃x. f\ndef\n= f[0/x] + f[1/x] the exists algorithm can\nbe implemented in terms of the algorithms apply and restrict as\napply (+, restrict (0, x, Bf), restrict (1, x, Bf)) .\n(6.3)\nConsider, for example, the OBDD Bf for the function f\ndef\n= x1 · y1 + x2 ·\ny2 + x3 · y3, shown in Figure 6.19. Figure 6.20 shows restrict(0, x3, Bf)\nand restrict(1, x3, Bf) and the result of applying + to them. (In this case\nthe apply function happens to return its second argument.)\nWe can improve the eﬃciency of this algorithm. Consider what happens\nduring the apply stage of (6.3). In that case, the apply algorithm works on\ntwo BDDs which are identical all the way down to the level of the x-nodes;\n378\n6 Binary decision diagrams\n1\n0\nx\nx\nx\nz\ny\nFigure 6.18. An example of a BDD which is not a read-1-BDD.\n1\n0\nx1\nx2\nx3\ny1\ny2\ny3\nFigure 6.19. A BDD Bf to illustrate the exists algorithm.\ntherefore the returned BDD also has that structure down to the x-nodes.\nAt the x-nodes, the two argument BDDs diﬀer, so the apply algorithm\nwill compute the apply of + to these two subBDDs and return that as the\nsubBDD of the result. This is illustrated in Figure 6.20. Therefore, we can\ncompute the OBDD for ∃x. f by taking the OBDD for f and replacing each\nnode labelled with x by the result of calling apply on + and its two branches.\nThis can easily be generalised to a sequence of exists operations. We\nwrite ∃ˆx. f to mean ∃x1.∃x2. . . . ∃xn. f, where ˆx denotes (x1, x2, . . . , xn).\n6.2 Algorithms for reduced OBDDs\n379\n1\n0\nx1\nx2\ny1\ny2\ny3\n1\n0\n1\n0\nx1\nx2\ny1\ny2\nx1\nx2\ny1\ny2\ny3",
    "This can easily be generalised to a sequence of exists operations. We\nwrite ∃ˆx. f to mean ∃x1.∃x2. . . . ∃xn. f, where ˆx denotes (x1, x2, . . . , xn).\n6.2 Algorithms for reduced OBDDs\n379\n1\n0\nx1\nx2\ny1\ny2\ny3\n1\n0\n1\n0\nx1\nx2\ny1\ny2\nx1\nx2\ny1\ny2\ny3\nFigure 6.20. restrict(0, x3, Bf) and restrict(1, x3, Bf) and the result\nof applying + to them.\n1\n0\nx1\nx2\nx3\ny1\ny2\ny3\n1\n0\nx1\nx2\ny1\ny2\ny3\n1\n0\nx1\ny1\ny2\ny3\n∃x3\n⇒\n∃x2\n⇒\nFigure 6.21. OBDDs for f, ∃x3. f and ∃x2.∃x3. f.\nThe OBDD for this boolean function is obtained from the OBDD for f by\nreplacing every node labelled with an xi by the + of its two branches.\nFigure 6.21 shows the computation of ∃x3. f and ∃x2.∃x3. f (which is\nsemantically equivalent to x1 · y1 + y2 + y3) in this way.\nThe boolean quantiﬁer ∀is the dual of ∃:\n∀x.f\ndef\n= f[0/x] · f[1/x]\nasserting that f could be made false by putting x to 0 or to 1.\nThe translation of boolean formulas into OBDDs using the algorithms of\nthis section is summarised in Figure 6.22.\n380\n6 Binary decision diagrams\nBoolean formula f\nRepresenting OBDD Bf\n0\nB0 (Fig. 6.6)\n1\nB1 (Fig. 6.6)\nx\nBx (Fig. 6.6)\nf\nswap the 0- and 1-nodes in Bf\nf + g\napply (+, Bf, Bg)\nf · g\napply (· , Bf, Bg)\nf ⊕g\napply (⊕, Bf, Bg)\nf[1/x]\nrestrict (1, x, Bf)\nf[0/x]\nrestrict (0, x, Bf)\n∃x.f\napply (+, Bf[0/x], Bf[1/x])\n∀x.f\napply (· , Bf[0/x], Bf[1/x])\nFigure 6.22. Translating boolean formulas f to OBDDs Bf, given a\nfixed, global ordering on boolean variables.\nAlgorithm Input OBDD(s)\nOutput OBDD\nTime-complexity\nreduce\nB\nreduced B\nO(|B| · log |B|)\napply\nBf, Bg (reduced) Bf op g (reduced)\nO(|Bf| · |Bg|)\nrestrict Bf (reduced)\nBf[0/x] or Bf[1/x] (reduced) O(|Bf| · log |Bf|)\n∃\nBf (reduced)\nB∃x1.∃x2....∃xn.f (reduced)\nNP-complete\nFigure 6.23. Upper bounds in terms of the input OBDD(s) for the\nworst-case running times of our algorithms needed in our implementa-\ntion of boolean formulas.\n6.2.5 Assessment of OBDDs\nTime complexities for computing OBDDs\nWe can measure the com-",
    "Bf (reduced)\nB∃x1.∃x2....∃xn.f (reduced)\nNP-complete\nFigure 6.23. Upper bounds in terms of the input OBDD(s) for the\nworst-case running times of our algorithms needed in our implementa-\ntion of boolean formulas.\n6.2.5 Assessment of OBDDs\nTime complexities for computing OBDDs\nWe can measure the com-\nplexity of the algorithms of the preceding section by giving upper bounds\nfor the running time in terms of the sizes of the input OBDDs. The table\nin Figure 6.23 summarises these upper bounds (some of those upper bounds\nmay require more sophisticated versions of the algorithms than the versions\npresented in this chapter). All the operations except nested boolean quantiﬁ-\ncation are practically eﬃcient in the size of the participating OBDDs. Thus,\nmodelling very large systems with this approach will work if the OBDDs\n6.2 Algorithms for reduced OBDDs\n381\nwhich represent the systems don’t grow too large too fast. If we can some-\nhow control the size of OBDDs, e.g. by using good heuristics for the choice\nof variable ordering, then these operations are computationally feasible. It\nhas already been shown that OBDDs modelling certain classes of systems\nand networks don’t grow excessively.\nThe expensive computational operations are the nested boolean quantiﬁ-\ncations ∃z1. . . . ∃zn.f and ∀z1. . . . ∀zn.f. By exercise 1 on page 406, the com-\nputation of the OBDD for ∃z1. . . . ∃zn.f, given the OBDD for f, is an NP-\ncomplete problem2; thus, it is unlikely that there exists an algorithm with\na feasible worst-time complexity. This is not to say that boolean functions\nmodelling practical systems may not have eﬃcient nested boolean quan-\ntiﬁcations. The performance of our algorithms can be improved by using\nfurther optimisation techniques, such as parallelisation.\nNote that the operations apply, restrict, etc. are only eﬃcient in the\nsize of the input OBDDs. So if a function f does not have a compact repre-\nsentation as an OBDD, then computing with its OBDD will not be eﬃcient.",
    "further optimisation techniques, such as parallelisation.\nNote that the operations apply, restrict, etc. are only eﬃcient in the\nsize of the input OBDDs. So if a function f does not have a compact repre-\nsentation as an OBDD, then computing with its OBDD will not be eﬃcient.\nThere are such nasty functions; indeed, one of them is integer multiplication.\nLet bn−1bn−2 . . . b0 and an−1an−2 . . . a0 be two n-bit integers, where bn−1 and\nan−1 are the most signiﬁcant bits and b0 and a0 are the least signiﬁcant bits.\nThe multiplication of these two integers results in a 2n-bit integer. Thus, we\nmay think of multiplication as 2n many boolean functions fi in 2n variables\n(n bits for input b and n bits for input a), where fi denotes the ith output\nbit of the multiplication. The following negative result, due to R. E. Bryant,\nshows that OBDDs cannot be used for implementing integer multiplication.\nTheorem 6.11 Any OBDD representation of fn−1 has at least a number\nof vertices proportional to 1.09n, i.e. its size is exponential in n.\nExtensions and variations of OBDDs\nThere are many variations and\nextensions to the OBDD data structure. Many of them can implement cer-\ntain operations more eﬃciently than their OBDD counterparts, but it seems\nthat none of them perform as well as OBDDs overall. In particular, one fea-\nture which many of the variations lack is the canonical form; therefore they\nlack an eﬃcient algorithm for deciding when two objects denote the same\nboolean function.\nOne kind of variation allows non-terminal nodes to be labelled with bi-\nnary operators as well as boolean variables. Parity OBDDs are like OBDDs\nin that there is an ordering on variables and every variable may occur at\n2 Another NP-complete problem is to decide the satisﬁability of formulas of propositional logic.\n382\n6 Binary decision diagrams\nmost once on a path; but some non-terminal nodes may be labelled with ⊕,\nthe exclusive-or operation. The meaning is that the function represented by",
    "2 Another NP-complete problem is to decide the satisﬁability of formulas of propositional logic.\n382\n6 Binary decision diagrams\nmost once on a path; but some non-terminal nodes may be labelled with ⊕,\nthe exclusive-or operation. The meaning is that the function represented by\nthat node is the exclusive-or of the boolean functions determined by its chil-\ndren. Parity OBDDs have similar algorithms for apply, restrict, etc. with\nthe same performance, but they do not have a canonical form. Checking for\nequivalence cannot be done in constant time. There is, however, a cubic algo-\nrithm for determining equivalence; and there are also eﬃcient probabilistic\ntests. Another variation of OBDDs allows complementation nodes, with the\nobvious meaning. Again, the main disadvantage is the lack of canonical form.\nOne can also allow non-terminal nodes to be unlabelled and to branch\nto more than two children. This can then be understood either as non-\ndeterministic branching, or as probabilistic branching: throw a pair of dice\nto determine where to continue the path. Such methods may compute wrong\nresults; one then aims at repeating the test to keep the (probabilistic)\nerror as small as desired. This method of repeating probabilistic tests is\ncalled probabilistic ampliﬁcation. Unfortunately, the satisﬁability problem\nfor probabilistic branching OBDDs is NP-complete. On a good note, prob-\nabilistic branching OBDDs can verify integer multiplication.\nThe development of extensions or variations of OBDDS which are cus-\ntomised to certain classes of boolean functions is an important area of on-\ngoing research.\n6.3 Symbolic model checking\nThe use of BDDs in model checking resulted in a signiﬁcant breakthrough in\nveriﬁcation in the early 1990s, because they have allowed systems with much\nlarger state spaces to be veriﬁed. In this section, we describe in detail how\nthe model-checking algorithm presented in Chapter 3 can be implemented\nusing OBDDs as the basic data structure.",
    "veriﬁcation in the early 1990s, because they have allowed systems with much\nlarger state spaces to be veriﬁed. In this section, we describe in detail how\nthe model-checking algorithm presented in Chapter 3 can be implemented\nusing OBDDs as the basic data structure.\nThe pseudo-code presented in Figure 3.28 on page 227 takes as input a\nCTL formula φ and returns the set of states of the given model which satisfy\nφ. Inspection of the code shows that the algorithm consists of manipulating\nintermediate sets of states. We show in this section how the model and the\nintermediate sets of states can be stored as OBDDs; and how the operations\nrequired in that pseudo-code can be implemented in terms of the operations\non OBDDs which we have seen in this chapter.\nWe start by showing how sets of states are represented with OBDDs,\ntogether with some of the operations required. Then, we extend that to\nthe representation of the transition system; and ﬁnally, we show how the\nremainder of the required operations is implemented.\n6.3 Symbolic model checking\n383\nModel checking using OBDDs is called symbolic model checking. The term\nemphasises that individual states are not represented; rather, sets of states\nare represented symbolically, namely, those which satisfy the formula being\nchecked.\n6.3.1 Representing subsets of the set of states\nLet S be a ﬁnite set (we forget for the moment that it is a set of states). The\ntask is to represent the various subsets of S as OBDDs. Since OBDDs encode\nboolean functions, we need somehow to code the elements of S as boolean\nvalues. The way to do this in general is to assign to each element s ∈S a\nunique vector of boolean values (v1, v2, . . . , vn), each vi ∈{0, 1}. Then, we\nrepresent a subset T by the boolean function fT which maps (v1, v2, . . . , vn)\nonto 1 if s ∈T and maps it onto 0 otherwise.\nThere are 2n boolean vectors (v1, v2, . . . , vn) of length n. Therefore, n\nshould be chosen such that 2n−1 < |S| ≤2n, where |S| is the number of",
    "represent a subset T by the boolean function fT which maps (v1, v2, . . . , vn)\nonto 1 if s ∈T and maps it onto 0 otherwise.\nThere are 2n boolean vectors (v1, v2, . . . , vn) of length n. Therefore, n\nshould be chosen such that 2n−1 < |S| ≤2n, where |S| is the number of\nelements in S. If |S| is not an exact power of 2, there will be some vec-\ntors which do not correspond to any element of S; they are just ignored.\nThe function fT : {0, 1}n →{0, 1} which tells us, for each s, represented by\n(v1, v2, . . . , vn), whether it is in the set T or not, is called the characteristic\nfunction of T.\nIn the case that S is the set of states of a transition system M = (S, →, L)\n(see Deﬁnition 3.4), there is a natural way of choosing the representation\nof S as boolean vectors. The labelling function L : S →P(Atoms) (where\nP(Atoms) is the set of subsets of Atoms) gives us the encoding. We assume\na ﬁxed ordering on the set Atoms, say x1, x2, . . . , xn, and then represent\ns ∈S by the vector (v1, v2, . . . , vn), where, for each i, vi equals 1 if xi ∈\nL(s) and vi is 0 otherwise. In order to guarantee that each s has a unique\nrepresentation as a boolean vector, we require that, for all s1, s2 ∈S, L(s1) =\nL(s2) implies s1 = s2. If this is not the case, perhaps because 2|Atoms| < |S|,\nwe can add extra atomic propositions in order to make enough distinctions\n(Cf. introduction of the turn variable for mutual exclusion in Section 3.3.4.)\nFrom now on, we refer to a state s ∈S by its representing boolean vector\n(v1, v2, . . . , vn), where vi is 1 if xi ∈L(s) and 0 otherwise. As an OBDD,\nthis state is represented by the OBDD of the boolean function l1 · l2 · · · · · ln,\nwhere li is xi if xi ∈L(s) and xi otherwise. The set of states {s1, s2, . . . , sm}\nis represented by the OBDD of the boolean function\n(l11 · l12 · · · · · l1n) + (l21 · l22 · · · · · l2n) + · · · + (lm1 · lm2 · · · · · lmn)\nwhere li1 · li2 · · · · · lin represents state si.\n384\n6 Binary decision diagrams\ns2\nx1\ns0\nx2\ns1",
    "is represented by the OBDD of the boolean function\n(l11 · l12 · · · · · l1n) + (l21 · l22 · · · · · l2n) + · · · + (lm1 · lm2 · · · · · lmn)\nwhere li1 · li2 · · · · · lin represents state si.\n384\n6 Binary decision diagrams\ns2\nx1\ns0\nx2\ns1\nFigure 6.24. A simple CTL model (Example 6.12).\nset of\nrepresentation by\nrepresentation by\nstates\nboolean values\nboolean function\n∅\n0\n{s0}\n(1, 0)\nx1 · x2\n{s1}\n(0, 1)\nx1 · x2\n{s2}\n(0, 0)\nx1 · x2\n{s0, s1}\n(1, 0), (0, 1)\nx1 · x2 + x1 · x2\n{s0, s2}\n(1, 0), (0, 0)\nx1 · x2 + x1 · x2\n{s1, s2}\n(0, 1), (0, 0)\nx1 · x2 + x1 · x2\nS\n(1, 0), (0, 1), (0, 0)\nx1 · x2 + x1 · x2 + x1 · x2\nFigure 6.25. Representation of subsets of states of the model of Figure 6.24.\nThe key point which makes this representation interesting is that the\nOBDD representing a set of states may be quite small.\nExample 6.12 Consider the CTL model in Figure 6.24, given by:\nS\ndef\n= {s0, s1, s2}\n→\ndef\n= {(s0, s1), (s1, s2), (s2, s0), (s2, s2)}\nL(s0)\ndef\n= {x1}\nL(s1)\ndef\n= {x2}\nL(s2)\ndef\n= ∅.\nNote that it has the property that, for all states s1 and s2, L(s1) = L(s2)\nimplies s1 = s2, i.e. a state is determined entirely by the atomic formulas\ntrue in it. Sets of states may be represented by boolean values and by boolean\nformulas with the ordering [x1, x2], as shown in Figure 6.25.\nNotice that the vector (1, 1) and the corresponding function x1 · x2 are\nunused. Therefore, we are free to include it in the representation of a subset\n6.3 Symbolic model checking\n385\nx2\n0\n1\nx1\n0\n1\nx1\nx2\nx2\nFigure 6.26. Two OBDDs for the set {s0, s1} (Example 6.12).\nof S or not; so we may choose to include it or not in order to optimise the\nsize of the OBDD. For example, the subset {s0, s1} is better represented\nby the boolean function x1 + x2, since its OBDD is smaller than that for\nx1 · x2 + x1 · x2 (Figure 6.26).\nIn order to justify the claim that the representation of subsets of S as\nOBDDs will be suitable for the algorithm presented in Section 3.6.1, we need",
    "by the boolean function x1 + x2, since its OBDD is smaller than that for\nx1 · x2 + x1 · x2 (Figure 6.26).\nIn order to justify the claim that the representation of subsets of S as\nOBDDs will be suitable for the algorithm presented in Section 3.6.1, we need\nto look at how the operations on subsets which are used in that algorithm\ncan be implemented in terms of the operations we have deﬁned on OBDDs.\nThe operations in that algorithm are:\nr Intersection, union and complementation of subsets. It is clear that these are\nrepresented by the boolean functions ·, + and ¯ respectively. The implementation\nvia OBDDs of · and + uses the apply algorithm (Section 6.2.2).\nr The functions\npre∃(X) = {s ∈S | exists s′, (s →s′ and s′ ∈X)}\n(6.4)\npre∀(X) = {s | for all s′, (s →s′ implies s′ ∈X)}.\nThe function pre∃(instrumental in SATEX and SATEU) takes a subset X of states\nand returns the set of states which can make a transition into X. The function\npre∀, used in SATAF, takes a set X and returns the set of states which can make\na transition only into X. In order to see how these are implemented in terms of\nOBDDs, we need ﬁrst to look at how the transition relation itself is represented.\n6.3.2 Representing the transition relation\nThe transition relation →of a model M = (S, →, L) is a subset of S × S.\nWe have already seen that subsets of a given ﬁnite set may be represented\nas OBDDs by considering the characteristic function of a binary encoding.\nJust like in the case of subsets of S, the binary encoding is naturally given\nby the labelling function L. Since →is a subset of S × S, we need two copies\nof the boolean vectors. Thus, the link s →s′ is represented by the pair of\n386\n6 Binary decision diagrams\nx1\nx2\nx′\n1\nx′\n2\n→\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n1\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\nx1\nx′\n1\nx2\nx′\n2\n→\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0",
    "x1\nx2\nx′\n1\nx′\n2\n→\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n1\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\nx1\nx′\n1\nx2\nx′\n2\n→\n0\n0\n0\n0\n1\n0\n0\n0\n1\n0\n0\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n1\n0\n0\n1\n0\n1\n0\n1\n0\n0\n1\n1\n0\n0\n0\n1\n1\n1\n0\n1\n0\n0\n0\n0\n1\n0\n0\n1\n1\n1\n0\n1\n0\n0\n1\n0\n1\n1\n0\n1\n1\n0\n0\n0\n1\n1\n0\n1\n0\n1\n1\n1\n0\n0\n1\n1\n1\n1\n0\nFigure 6.27. The truth table for the transition relation of Figure 6.24\n(see Example 6.13). The left version shows the ordering of variables\n[x1, x2, x′\n1, x′\n2], while the right one orders the variables [x1, x′\n1, x2, x′\n2] (the\nrows are ordered lexicographically).\nboolean vectors ((v1, v2, . . . , vn), (v′\n1, v′\n2, . . . , v′\nn)), where vi is 1 if pi ∈L(s)\nand 0 otherwise; and similarly, v′\ni is 1 if pi ∈L(s′) and 0 otherwise. As an\nOBDD, the link is represented by the OBDD for the boolean function\n(l1 · l2 · · · · · ln) · (l′\n1 · l′\n2 · · · · · l′\nn)\nand a set of links (for example, the entire relation →) is the OBDD for the\n+ of such formulas.\nExample 6.13 To compute the OBDD for the transition relation of Fig-\nure 6.24, we ﬁrst show it as a truth table (Figure 6.27 (left)). Each 1 in\nthe ﬁnal column corresponds to a link in the transition relation and each 0\ncorresponds to the absence of a link. The boolean function is obtained by\ntaking the disjunction of the rows having 1 in the last column and is\nf→def\n= x1 · x2 · x′\n1 · x′\n2 + x1 · x2 · x′\n1 · x′\n2 + x1 · x2 · x′\n1 · x′\n2 + x1 · x2 · x′\n1 · x′\n2.\n(6.5)\nIt turns out that it is usually more eﬃcient to interleave unprimed and\nprimed variables in the OBDD variable ordering for →. We therefore use\n6.3 Symbolic model checking\n387\n0\n1\nx2\nx2\nx′\n2\nx′\n2\nx1\nx′\n1\nx′\n1\nFigure 6.28. An OBDD for the transition relation of Example 6.13.\n[x1, x′\n1, x2, x′\n2] rather than [x1, x2, x′\n1, x′\n2]. Figure 6.27 (right) shows the truth\ntable redrawn with the interleaved ordering of the columns and the rows\nreordered lexicographically. The resulting OBDD is shown in Figure 6.28.",
    "[x1, x′\n1, x2, x′\n2] rather than [x1, x2, x′\n1, x′\n2]. Figure 6.27 (right) shows the truth\ntable redrawn with the interleaved ordering of the columns and the rows\nreordered lexicographically. The resulting OBDD is shown in Figure 6.28.\n6.3.3 Implementing the functions pre∃and pre∀\nIt remains to show how an OBDD for pre∃(X) and pre∀(X) can be com-\nputed, given OBDDs BX for X and B→for the transition relation →. First\nwe observe that pre∀can be expressed in terms of complementation and\npre∃, as follows: pre∀(X) = S −pre∃(S −X), where we write S −Y for the\nset of all s ∈S which are not in Y . Therefore, we need only explain how to\ncompute the OBDD for pre∃(X) in terms of BX and B→. Now (6.4) suggests\nthat one should proceed as follows:\n1.\nRename the variables in BX to their primed versions; call the resulting OBDD\nBX′.\n2.\nCompute the OBDD for exists(ˆx′, apply(·, B→, BX′)) using the apply and\nexists algorithms (Sections 6.2.2 and 6.2.4).\n6.3.4 Synthesising OBDDs\nThe method used in Example 6.13 for producing an OBDD for the transi-\ntion relation was to compute ﬁrst the truth table and then an OBDD which\nmight not be in its fully reduced form; hence the need for a ﬁnal call to\n388\n6 Binary decision diagrams\nthe reduce function. However, this procedure would be unacceptable if ap-\nplied to realistically sized systems with a large number of variables, for the\ntruth table’s size is exponential in the number of boolean variables. The\nkey idea and attraction of applying OBDDs to ﬁnite systems is therefore to\ntake a system description in a language such as SMV and to synthesise the\nOBDD directly, without having to go via intermediate representations (such\nas binary decision trees or truth tables) which are exponential in size.\nSMV allows us to deﬁne the next value of a variable in terms of the\ncurrent values of variables (see the examples of code in Section 3.3.2)3. This\ncan be compiled into a set of boolean functions fi, one for each variable xi,",
    "as binary decision trees or truth tables) which are exponential in size.\nSMV allows us to deﬁne the next value of a variable in terms of the\ncurrent values of variables (see the examples of code in Section 3.3.2)3. This\ncan be compiled into a set of boolean functions fi, one for each variable xi,\nwhich deﬁne the next value of xi in terms of the current values of all the\nvariables. In order to cope with non-deterministic assignment (such as the\nassignment to status in the example on page 192), we extend the set of\nvariables by adding unconstrained variables which model the input. Each x′\ni\nis a deterministic function of this enlarged set of variables; thus, x′\ni ↔fi,\nwhere f ↔g = 1 if, and only if, f and g compute the same values, i.e. it is\na shorthand for f ⊕g.\nThe boolean function representing the transition relation is therefore of\nthe form\n\u0001\n1≤i≤n\nx′\ni ↔fi,\n(6.6)\nwhere \u0002\n1≤i≤n gi is a shorthand for g1 · g2 · . . . · gn. Note that the \u0002 ranges\nonly over the non-input variables. So, if u is an input variable, the boolean\nfunction does not contain any u′ ↔fu.\nFigure 6.22 showed how the reduced OBDD could be computed from the\nparse tree of such a boolean function. Thus, it is possible to compile SMV\nprograms into OBDDs such that their speciﬁcations can be executed accord-\ning to the pseudo-code of the function SAT, now interpreted over OBDDs.\nOn page 396 we will see that this OBDD implementation can be extended\nto simple fairness constraints.\nModelling sequential circuits\nAs a further application of OBDDs to\nveriﬁcation, we show how OBDDs representing circuits may be synthesised.\nSynchronous circuits.\nSuppose that we have a design of a sequential circuit\nsuch as the one in Figure 6.29. This is a synchronous circuit (meaning that\n3 SMV also allows next values to be deﬁned in terms of next values, i.e. the keyword next to appear\nin expressions on the right-hand side of :=. This is useful for describing synchronisations, for\nexample, but we ignore that feature here.",
    "3 SMV also allows next values to be deﬁned in terms of next values, i.e. the keyword next to appear\nin expressions on the right-hand side of :=. This is useful for describing synchronisations, for\nexample, but we ignore that feature here.\n6.3 Symbolic model checking\n389\nx2\nx1\nFigure 6.29. A simple synchronous circuit with two registers.\nall the state variables are updated synchronously in parallel) whose func-\ntionality can be described by saying what the values of the registers x1 and\nx2 in the next state of the circuit are. The function f→coding the possible\nnext states of the circuits is\n(x′\n1 ↔x1) · (x′\n2 ↔x1 ⊕x2).\n(6.7)\nThis may now be translated into an OBDD by the methods summarised in\nFigure 6.22.\nAsynchronous circuits.\nThe symbolic encoding of synchronous circuits is\nin its logical structure very similar to the encoding of f→for CTL models;\ncompare the codings in (6.7) and (6.6). In asynchronous circuits, or processes\nin SMV, the logical structure of f→changes. As before, we can construct\nfunctions fi which code the possible next state in the local component, or\nthe SMV process, i. For asynchronous systems, there are two principal ways\nof composing these functions into global system behaviour:\nr In a simultaneous model, a global transition is one in which any number of\ncomponents may make their local transition. This is modelled as\nf →def\n=\nn\n\u0001\ni=1\n((x′\ni ↔fi) + (x′\ni ↔xi)) .\n(6.8)\nr In an interleaving model, exactly one local component makes a local transition;\n390\n6 Binary decision diagrams\nall other local components remain in their local state:\nf →def\n=\nn\n\u0003\ni=1\n\n(x′\ni ↔fi) ·\n\u0001\nj̸=i\n(x′\nj ↔xj)\n\n.\n(6.9)\nObserve the duality in these approaches: the simultaneous model has an\nouter product, whereas the interleaving model has an outer sum. The latter,\nif used in ∃ˆx′.f (‘for some next state’), can be optimised since sums distribute\nover existential quantiﬁcation; in Chapter 2 this was the equivalence ∃x.(φ ∨",
    "outer product, whereas the interleaving model has an outer sum. The latter,\nif used in ∃ˆx′.f (‘for some next state’), can be optimised since sums distribute\nover existential quantiﬁcation; in Chapter 2 this was the equivalence ∃x.(φ ∨\nψ) ≡∃x.φ ∨∃x.ψ. Thus, global states reachable in one step are the ‘union’\nof all the states reachable in one step in the local components; compare the\nformulas in (6.8) and (6.9) with (6.6).\n6.4 A relational mu-calculus\nWe saw in Section 3.7 that evaluating the set of states satisfying a CTL for-\nmula in a model may involve the computation of a ﬁxed point of an operator.\nFor example, [[EF φ]] is the least ﬁxed point of the operator F : P(S) →P(S)\ngiven by F(X) = [[φ]] ∪pre∃(X).\nIn this section, we introduce a syntax for referring to ﬁxed points in the\ncontext of boolean formulas. Fixed-point invariants frequently occur in all\nsorts of applications (for example, the common-knowledge operator CG in\nChapter 5), so it makes sense to have an intermediate language for express-\ning such invariants syntactically. This language also provides a formalism\nfor describing interactions and dependences of such invariants. We will see\nshortly that symbolic model checking in the presence of simple fairness con-\nstraints exhibits such more complex relationships between invariants.\n6.4.1 Syntax and semantics\nDeﬁnition 6.14 The formulas of the relational mu-calculus are given by\nthe grammar\nv ::= x | Z\nf ::= 0 | 1 | v | f | f1 + f2 | f1 · f2 | f1 ⊕f2 |\n∃x.f | ∀x.f | µZ.f | νZ.f | f[ˆx := ˆx′]\n(6.10)\nwhere x and Z are boolean variables, and ˆx is a tuple of variables. In the\nformulas µZ.f and νZ.f, any occurrence of Z in f is required to fall within an\neven number of complementation symbols ¯; such an f is said to be formally\nmonotone in Z. (In exercise 7 on page 410 we consider what happens if we\ndo not require formal monotonicity.)\n6.4 A relational mu-calculus\n391\nConvention 6.15 The binding priorities for the grammar in (6.10) are that",
    "even number of complementation symbols ¯; such an f is said to be formally\nmonotone in Z. (In exercise 7 on page 410 we consider what happens if we\ndo not require formal monotonicity.)\n6.4 A relational mu-calculus\n391\nConvention 6.15 The binding priorities for the grammar in (6.10) are that\n¯, and [ˆx := ˆx′] have the highest priority; followed by ∃x and ∀y; then µZ\nand νZ; followed by · . The operators + and ⊕have the lowest binding\npriority.\nThe symbols µ and ν are called least ﬁxed-point and greatest ﬁxed-point\noperators, respectively. In the formula µZ.f, the interesting case is that in\nwhich f contains an occurrence of Z. In that case, f can be thought of as\na function, taking Z to f. The formula µZ.f is intended to mean the least\nﬁxed point of that function. Similarly, νZ.f is the greatest ﬁxed point of the\nfunction. We will see how this is done in the semantics.\nThe formula f[ˆx := ˆx′] expresses an explicit substitution which forces f\nto be evaluated using the values of x′\ni rather than xi. (Recall that the primed\nvariables refer to the next state.) Thus, this syntactic form is not a meta-\noperation denoting a substitution, but an explicit syntactic form in its own\nright. The substitution will be made on the semantic side, not the syntactic\nside. This diﬀerence will become clear when we present the semantics of ⊨.\nA valuation ρ for f is an assignment of values 0 or 1 to all variables v.\nWe deﬁne a satisfaction relation ρ ⊨f inductively over the structure of such\nformulas f, given a valuation ρ.\nDeﬁnition 6.16 Let ρ be a valuation and v a variable. We write ρ(v) for\nthe value of v assigned by ρ. We deﬁne ρ[v \u0011→0] to be the updated valuation\nwhich assigns 0 to v and ρ(w) to all other variables w. Dually, ρ[v \u0011→1]\nassigns 1 to v and ρ(w) to all other variables w.\nFor example, if ρ is the valuation represented by (x, y, Z) ⇒(1, 0, 1) –\nmeaning that ρ(x) = 1, ρ(y) = 0, ρ(Z) = 1 and ρ(v) = 0 for all other vari-",
    "which assigns 0 to v and ρ(w) to all other variables w. Dually, ρ[v \u0011→1]\nassigns 1 to v and ρ(w) to all other variables w.\nFor example, if ρ is the valuation represented by (x, y, Z) ⇒(1, 0, 1) –\nmeaning that ρ(x) = 1, ρ(y) = 0, ρ(Z) = 1 and ρ(v) = 0 for all other vari-\nables v – then ρ[x \u0011→0] is represented by (x, y, Z) ⇒(0, 0, 1), whereas\nρ[Z \u0011→0] is (x, y, Z) ⇒(1, 0, 0). The assumption that valuations assign val-\nues to all variables is rather mathematical, but avoids some complications\nwhich have to be addressed in implementations (see exercise 3 on page 409).\nUpdated valuations allow us to deﬁne the satisfaction relation for all for-\nmulas without ﬁxed points:\nDeﬁnition 6.17 We deﬁne a satisfaction relation ρ ⊨f for formulas f with-\nout ﬁxed-point subformulas with respect to a valuation ρ by structural in-\nduction:\nr ρ ̸⊨0\nr ρ ⊨1\nr ρ ⊨v iﬀρ(v) equals 1\n392\n6 Binary decision diagrams\nr ρ ⊨f iﬀρ ̸⊨f\nr ρ ⊨f + g iﬀρ ⊨f or ρ ⊨g\nr ρ ⊨f · g iﬀρ ⊨f and ρ ⊨g\nr ρ ⊨f ⊕g iﬀρ ⊨(f · g + f · g)\nr ρ ⊨∃x.f iﬀρ[x \u0011→0] ⊨f or ρ[x \u0011→1] ⊨f\nr ρ ⊨∀x.f iﬀρ[x \u0011→0] ⊨f and ρ[x \u0011→1] ⊨f\nr ρ ⊨f[ˆx := ˆx′] iﬀρ[ˆx := ˆx′] ⊨f,\nwhere ρ[ˆx := ˆx′] is the valuation which assigns the same values as ρ, but for\neach xi it assigns ρ(x′\ni).\nThe semantics of boolean quantiﬁcation closely resembles the one for the\nquantiﬁers of predicate logic. The crucial diﬀerence, however, is that boolean\nformulas are only interpreted over the ﬁxed universe of values {0, 1}, whereas\npredicate formulas may take on values in all sorts of ﬁnite or inﬁnite models.\nExample 6.18 Let ρ be such that ρ(x′\n1) equals 0 and ρ(x′\n2) is 1. We evaluate\nρ ⊨(x1 + x2)[ˆx := ˆx′] which holds iﬀρ[ˆx := ˆx′] ⊨(x1 + x2). Thus, we need\nρ[ˆx := ˆx′] ⊨x1 or ρ[ˆx := ˆx′] ⊨x2 to be the case. Now, ρ[ˆx := ˆx′] ⊨x1 cannot\nbe, for this would mean that ρ(x′\n1) equals 1. Since ρ[ˆx := ˆx′] ⊨x2 would\nimply that ρ[ˆx := ˆx′] ̸⊨x2, we infer that ρ[ˆx := ˆx′] ̸⊨x2 because ρ(x′\n2) equals\n1. In summary, we demonstrated that ρ ̸⊨(x1 + x2)[ˆx := ˆx′].",
    "be, for this would mean that ρ(x′\n1) equals 1. Since ρ[ˆx := ˆx′] ⊨x2 would\nimply that ρ[ˆx := ˆx′] ̸⊨x2, we infer that ρ[ˆx := ˆx′] ̸⊨x2 because ρ(x′\n2) equals\n1. In summary, we demonstrated that ρ ̸⊨(x1 + x2)[ˆx := ˆx′].\nWe now extend the deﬁnition of ⊨to the ﬁxed-point operators µ and ν.\nTheir semantics will have to reﬂect their meaning as least, respectively great-\nest, ﬁxed-point operators. We deﬁne the semantics of µZ.f via its syntactic\napproximants which unfold the meaning of µZ.f:\nµ0Z.f\ndef\n= 0\nµm+1Z.f\ndef\n= f[µmZ.f/Z]\n(m ≥0).\n(6.11)\nThe unfolding is achieved by a meta-operation [g/Z] which, when applied\nto a formula f, replaces all free occurrences of Z in f with g. Thus, we view\nµZ as a binding construct similar to the quantiﬁers ∀x and ∃x, and [g/Z]\nis similar to the substitution [t/x] in predicate logic. For example, (x1 +\n∃x2.(Z · x2))[x1/Z] is the formula x1 + ∃x2.(x1 · x2), whereas ((µZ.x1 + Z) ·\n(x1 + ∃x2.(Z · x2)))[x1/Z] equals (µZ.x1 + Z) · (x1 + ∃x2.(x1 · x2)). See ex-\nercise 3 on page 409 for a formal account of this meta-operation.\nWith these approximants we can deﬁne:\nρ ⊨µZ.f iﬀ(ρ ⊨µmZ.f for some m ≥0).\n(6.12)\n6.4 A relational mu-calculus\n393\nThus, to determine whether µZ.f is true with respect to a valuation ρ,\nwe have to ﬁnd some m ≥0 such that ρ ⊨µmZ.f holds. A sensible strategy\nis to try to prove this for the smallest such m possible, if indeed such an\nm can be found. For example, in attempting to show ρ ⊨µZ.Z, we try\nρ ⊨µ0Z.Z, which fails since the latter formula is just 0. Now, µ1Z.Z is\ndeﬁned to be Z[µ0Z.Z/Z] which is just µ0Z.Z again.\nWe can now use\nmathematical induction on m ≥0 to show that µmZ.Z equals µ0Z.Z for all\nm ≥0. By (6.12), this implies ρ ̸⊨µZ.Z.\nThe semantics for νZ.f is similar. First, let us deﬁne a family of approx-\nimants ν0Z.f, ν1Z.f, . . . by\nν0Z.f\ndef\n= 1\nνm+1Z.f\ndef\n= f[νmZ.f/Z]\n(m ≥0).\n(6.13)\nNote that this deﬁnition only diﬀers from the one for µmZ.f in that the\nﬁrst approximant is deﬁned to be 1 instead of 0.",
    "The semantics for νZ.f is similar. First, let us deﬁne a family of approx-\nimants ν0Z.f, ν1Z.f, . . . by\nν0Z.f\ndef\n= 1\nνm+1Z.f\ndef\n= f[νmZ.f/Z]\n(m ≥0).\n(6.13)\nNote that this deﬁnition only diﬀers from the one for µmZ.f in that the\nﬁrst approximant is deﬁned to be 1 instead of 0.\nRecall how the greatest ﬁxed point for EG φ requires that φ holds on all\nstates of some path. Such invariant behaviour cannot be expressed with a\ncondition such as in (6.12), but is adequately deﬁned by demanding that\nρ ⊨νZ.f iﬀ(ρ ⊨νmZ.f for all m ≥0).\n(6.14)\nA dual reasoning to the above shows that ρ ⊨νZ.Z holds, regardless of the\nnature of ρ.\nOne informal way of understanding the deﬁnitions in (6.12) and (6.14) is\nthat ρ ⊨µZ.f is false until, and if, it is proven to hold; whereas ρ ⊨νZ.f is\ntrue until, and if, it is proven to be false. The temporal aspect is encoded\nby the unfolding of the recursion in (6.11), or in (6.13).\nTo prove that this recursive way of specifying ρ ⊨f actually is well de-\nﬁned, one has to consider more general forms of induction which keep track\nnot only of the height of f’s parse tree, but also of the number of syntactic\napproximants µmZ.g and νnZ.h, their ‘degree’ (in this case, m and n), as\nwell as their ‘alternation’ (the body of a ﬁxed point may contain a free oc-\ncurrence of a variable for a recursion higher up in the parse tree). This can\nbe done, though we won’t discuss the details here.\n6.4.2 Coding CTL models and specifications\nGiven a CTL model M = (S, →, L), the µ and ν operators permit us to\ntranslate any CTL formula φ into a formula, fφ, of the relational mu-calculus\nsuch that fφ represents the set of states s ∈S with s ⊨φ. Since we already\nsaw how to represent subsets of states as such formulas, we can then capture\n394\n6 Binary decision diagrams\nthe model-checking problem\nM, I\n?\n⊨φ\n(6.15)\nof whether all initial states s ∈I satisfy φ, in purely symbolic form: we\nanswer in the aﬃrmative if fI · f\nφ is unsatisﬁable, where fI is the charac-",
    "394\n6 Binary decision diagrams\nthe model-checking problem\nM, I\n?\n⊨φ\n(6.15)\nof whether all initial states s ∈I satisfy φ, in purely symbolic form: we\nanswer in the aﬃrmative if fI · f\nφ is unsatisﬁable, where fI is the charac-\nteristic function of I ⊆S. Otherwise, the logical structure of fI · f\nφ may be\nexploited to extract debugging information for correcting the model M in\norder to make (6.15) true.\nRecall how we can represent the transition relation →as a boolean for-\nmula f→(see Section 6.3.2). As before, we assume that states are coded as\nbit vectors (v1, v2, . . . , vn) and so the free boolean variables of all functions\nfφ are subsumed by the vector ˆx. The coding of the CTL formula φ as a\nfunction fφ in the relational mu-calculus is now given inductively as follows:\nfx def\n= x\nfor variables x\nf⊥def\n= 0\nf¬φ def\n= fφ\nfφ∧ψ def\n= fφ · fψ\nfEX φ def\n= ∃ˆx′. (f→· fφ[ˆx := ˆx′]).\nThe clause for EX deserves explanation. The variables xi refer to the\ncurrent state, whereas x′\ni refer to the next state. The semantics of CTL says\nthat s ⊨EX φ if, and only if, there is some s′ with s →s′ and s′ ⊨φ. The\nboolean formula encodes this deﬁnition, computing 1 precisely when this is\nthe case. If ˆx models the current state s, then ˆx′ models a possible successor\nstate if f→, a function in (ˆx, ˆx′), holds. We use the nested boolean quantiﬁer\n∃ˆx′ in order to say ‘there is some successor state.’ Observe also the desired\neﬀect of [ˆx := ˆx′] performed on fφ, thereby ‘forcing’ φ to be true at some\nnext state4.\nThe clause for EF is more complicated and involves the µ operator. Recall\nthe equivalence\nEF φ ≡φ ∨EX EF φ.\n(6.16)\n4 Exercise 6 on page 409 should give you a feel for how the semantics of f[ˆx := ˆx′] does not inter-\nfere with potential ∃ˆx′ or ∀ˆx′ quantiﬁers within f. For example, to evaluate ρ ⊨(∃ˆx′.f)[ˆx := ˆx′],\nwe evaluate ρ[ˆx := ˆx′] ⊨∃ˆx′.f, which is true if we can ﬁnd some values (v1, v2, . . . , vn) ∈{0, 1}n\nsuch that ρ[ˆx := ˆx′][x′\n1 \n→v1][x′\n2 \n→v2] . . . [x′",
    "fere with potential ∃ˆx′ or ∀ˆx′ quantiﬁers within f. For example, to evaluate ρ ⊨(∃ˆx′.f)[ˆx := ˆx′],\nwe evaluate ρ[ˆx := ˆx′] ⊨∃ˆx′.f, which is true if we can ﬁnd some values (v1, v2, . . . , vn) ∈{0, 1}n\nsuch that ρ[ˆx := ˆx′][x′\n1 \n→v1][x′\n2 \n→v2] . . . [x′\nn \n→vn] ⊨f is true. Observe that the resulting en-\nvironment binds all x′\ni to vi, but for all other values it binds them according to ρ[ˆx := ˆx′]; since\nthe latter binds xi to ρ(x′\ni) which is the ‘old’ value of x′\ni, this is exactly what we desire in order\nto prevent a clash of variable names with the intended semantics.\nRecall that an OBDD implementation synthesises formulas in a bottom-up fashion, so a reduced\nOBDD for ∃ˆx′.f will not contain any x′\ni nodes as its function does not depend on those variables.\nThus, OBDDs also avoid such name clash problems.\n6.4 A relational mu-calculus\n395\nTherefore, fEF φ has to be equivalent to fφ + fEX EF φ which in turn is equiv-\nalent to fφ + ∃ˆx′. (f→· fEF φ[ˆx := ˆx′]). Now, since EF involves computing\nthe least ﬁxed point of the operator derived from the Equivalence (6.16), we\nobtain\nfEF φ def\n= µZ. (fφ + ∃ˆx′. (f→· Z[ˆx := ˆx′])).\n(6.17)\nNote that the substitution Z[ˆx := ˆx′] means that the boolean function Z\nshould be made to depend on the x′\ni variables, rather than the xi variables.\nThis is because the evaluation of ρ ⊨Z[ˆx := ˆx′] results in ρ[ˆx := ˆx′] ⊨Z,\nwhere the latter valuation satisﬁes ρ[ˆx := ˆx′](xi) = ρ(x′\ni). Then, we use the\nmodiﬁed valuation ρ[ˆx := ˆx′] to evaluate Z.\nSince EF φ is equivalent to E[⊤U φ], we can generalise our coding of EF φ\naccordingly:\nfE[φUψ] def\n= µZ. (fψ + fφ · ∃ˆx′. (f→· Z[ˆx := ˆx′])).\n(6.18)\nThe coding of AF is similar to the one for EF in (6.17), except that ‘for\nsome’ (boolean quantiﬁcation ∃ˆx′) gets replaced by ‘for all’ (boolean quantiﬁ-\ncation ∀ˆx′) and the ‘conjunction’ f→· Z[ˆx := ˆx′] turns into the ‘implication’\nf→+ Z[ˆx := ˆx′]:\nfAF φ def\n= µZ. (fφ + ∀ˆx′. (f→+ Z[ˆx := ˆx′])).\n(6.19)",
    "some’ (boolean quantiﬁcation ∃ˆx′) gets replaced by ‘for all’ (boolean quantiﬁ-\ncation ∀ˆx′) and the ‘conjunction’ f→· Z[ˆx := ˆx′] turns into the ‘implication’\nf→+ Z[ˆx := ˆx′]:\nfAF φ def\n= µZ. (fφ + ∀ˆx′. (f→+ Z[ˆx := ˆx′])).\n(6.19)\nNotice how the semantics of µZ.f in (6.12) reﬂects the intended meaning\nof the AF connective. The mth approximant of fAF φ, which we write as\nfAF φ\nm\n, represents those states where all paths reach a φ-state within m steps.\nThis leaves us with coding EG, for then we have provided such a coding\nfor an adequate fragment of CTL (recall Theorem 3.17 on page 216). Because\nEG involves computing greatest ﬁxed points, we make use of the ν operator:\nfEG φ def\n= νZ. (fφ · ∃ˆx′. (f→· Z[ˆx := ˆx′])).\n(6.20)\nObserve that this does follow the logical structure of the semantics of\nEG: we need to show φ in the present state and then we have to ﬁnd some\nsuccessor state satisfying EG φ. The crucial point is that this obligation\nnever ceases; this is exactly what we ensured in (6.14).\nLet us see these codings in action on the model of Figure 6.24. We\nwant to perform a symbolic model check of the formula EX (x1 ∨¬x2).\nYou should verify, using e.g. the labelling algorithm from Chapter 3, that\n[[EX (x1 ∨¬x2)]] = {s1, s2}. Our claim is that this set is computed symbol-\nically by the resulting formula fEX (x1∨¬x2). First, we compute the formula\n396\n6 Binary decision diagrams\nf→which represents the transition relation →:\nf→= (x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1)\nwhere u is an input variable used to model the non-determinism (compare\nthe form (6.6) for the transition relation in Section 6.3.4). Thus, we obtain\nfEX (x1∨¬x2) = ∃x′\n1.∃x′\n2.(f→· fx1∨¬x2[ˆx := ˆx′])\n= ∃x′\n1.∃x′\n2.((x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) · (x′\n1 + x′\n2)).\nTo see whether s0 satisﬁes EX (x1 ∨¬x2), we evaluate ρ0 ⊨fEX (x1∨¬x2),\nwhere ρ0(x1) = 1 and ρ0(x2) = 0 (the value of ρ0(u) does not matter). We\nﬁnd that this does not hold, whence s0 ̸⊨EX (x1 ∨¬x2). Likewise, we verify",
    "= ∃x′\n1.∃x′\n2.((x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) · (x′\n1 + x′\n2)).\nTo see whether s0 satisﬁes EX (x1 ∨¬x2), we evaluate ρ0 ⊨fEX (x1∨¬x2),\nwhere ρ0(x1) = 1 and ρ0(x2) = 0 (the value of ρ0(u) does not matter). We\nﬁnd that this does not hold, whence s0 ̸⊨EX (x1 ∨¬x2). Likewise, we verify\ns1 ⊨EX (x1 ∨¬x2) by showing ρ1 ⊨fEX (x1∨¬x2); and s2 ⊨EX (x1 ∨¬x2) by\nshowing ρ2 ⊨fEX (x1∨¬x2), where ρi is the valuation representing state si.\nAs a second example, we compute fAF (¬x1∧¬x2) for the model in\nFigure 6.24. First, note that all three5 states satisfy AF (¬x1 ∧¬x2), if we\napply the labelling algorithm to the explicit model. Let us verify that the\nsymbolic encoding matches this result. By (6.19), we have that fAF (¬x1∧¬x2)\nequals\nµZ.\n\b\n(x1 · x2) + ∀x′\n1.∀x′\n2.(x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) · Z[ˆx := ˆx′]\n\t\n. (6.21)\nBy (6.12), we have ρ ⊨fAF (¬x1∧¬x2) iﬀρ ⊨fAF (¬x1∧¬x2)\nm\nfor some m ≥0.\nClearly, we have ρ ̸⊨fAF (¬x1∧¬x2)\n0\n. Now, fAF (¬x1∧¬x2)\n1\nequals\n((x1 · x2) + ∀x′\n1.∀x′\n2.(x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) · Z[ˆx := ˆx′])[0/Z].\nSince [0/Z] is a meta-operation, the latter formula is just\n(x1 · x2) + ∀x′\n1.∀x′\n2.(x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) · 0[ˆx := ˆx′].\nThus, we need to evaluate the disjunction (x1 · x2) + ∀x′\n1.∀x′\n2.(x′\n1 ↔x1 · x2 ·\nu) · (x′\n2 ↔x1) · 0[ˆx := ˆx′] at ρ. In particular, if ρ(x1) = 0 and ρ(x2) = 0,\nthen ρ ⊨x1 · x2 and so ρ ⊨(x1 · x2) + ∀x′\n1.∀x′\n2.(x′\n1 ↔x1 · x2 · u) · (x′\n2 ↔x1) ·\n0[ˆx := ˆx′]. Thus, s2 ⊨AF (¬x1 ∧¬x2) holds.\nSimilar reasoning establishes that the formula in (6.21) renders a correct\ncoding for the remaining two states as well, which you are invited to verify\nas an exercise.\nSymbolic model checking with fairness\nIn Chapter 3, we sketched\nhow SMV could use fairness assumptions which were not expressible entirely\n5 Since we have added the variable u, there are actually six states; they all satisfy the formula.\n6.4 A relational mu-calculus\n397\nwithin CTL and its semantics. The addition of fairness could be achieved",
    "how SMV could use fairness assumptions which were not expressible entirely\n5 Since we have added the variable u, there are actually six states; they all satisfy the formula.\n6.4 A relational mu-calculus\n397\nwithin CTL and its semantics. The addition of fairness could be achieved\nby restricting the ordinary CTL semantics to fair computation paths, or fair\nstates. Formally, we were given a set C = {ψ1, ψ2, . . . , ψk} of CTL formulas,\ncalled the fairness constraints, and we wanted to check whether s ⊨φ holds\nfor a CTL formula φ and all initial states s, with the additional fairness\nconstraints in C. Since ⊥, ¬, ∧, EX, EU and EG form an adequate set of\nconnectives for CTL, we may restrict this discussion to only these operators.\nClearly, the propositional connectives won’t change their meaning with the\naddition of fairness constraints. Therefore, it suﬃces to provide symbolic\ncodings for the fair connectives ECX, ECU and ECG from Chapter 3. The\nkey is to represent the set of fair states symbolically as a boolean formula\nfair deﬁned as\nfair\ndef\n= fECG⊤\n(6.22)\nwhich uses the (yet to be deﬁned) function fECG φ with ⊤as an instance.\nAssuming that the coding of fECG φ is correct, we see that fair computes 1\nin a state s if, and only if, there is a fair path with respect to C that begins\nin s. We say that such an s is a fair state.\nAs for ECX, note that s ⊨ECXφ if, and only if, there is some next state s′\nwith s →s′ and s′ ⊨φ such that s′ is a fair state. This immediately renders\nfECXφ def\n= ∃ˆx′.(f→· (fφ · fair)[ˆx := ˆx′]).\n(6.23)\nSimilarly, we obtain\nfEC[φ1Uφ2] def\n= µZ. (fφ2 · fair + fφ1 · ∃ˆx′. (f→· Z[ˆx := ˆx′])).\n(6.24)\nThis leaves us with the task of coding fECG φ. It is this last connective\nwhich reveals the complexity of fairness checks at work. Because the coding\nof fECG φ is rather complex, we proceed in steps. It is convenient to have the\nEX and EU functionality also at the level of boolean formulas directly. For",
    "which reveals the complexity of fairness checks at work. Because the coding\nof fECG φ is rather complex, we proceed in steps. It is convenient to have the\nEX and EU functionality also at the level of boolean formulas directly. For\nexample, if f is a boolean function in ˆx, then checkEX (f) codes the boolean\nformula which computes 1 for those vectors ˆx which have a next state ˆx′ for\nwhich f computes 1:\ncheckEX (f)\ndef\n= ∃ˆx′.(f→· f[ˆx := ˆx′]).\n(6.25)\nThus, fECXφ equals checkEX (fφ · fair). We proceed in the same way for\nfunctions f and g in n arguments ˆx to obtain checkEU (f, g) which computes\n398\n6 Binary decision diagrams\n1 at ˆx if there is a path that realises the f U g pattern:\ncheckEU (f, g)\ndef\n= µY.g + (f · checkEX(Y )).\n(6.26)\nWith this in place, we can code fECGφ quite easily:\nfECGφ def\n= νZ.fφ ·\nk\n\u0001\ni=1\ncheckEX (checkEU (fφ, Z · fψi) · fair).\n(6.27)\nNote that this coding has a least ﬁxed point (checkEU) in the body of a\ngreatest ﬁxed point. This is computationally rather involved since the call of\ncheckEU contains Z, the recursion variable of the outer greatest ﬁxed point,\nas a free variable; thus these recursions are nested and inter-dependent;\nthe recursions ‘alternate.’ Observe how this coding operates: to have a fair\npath from ˆx on which φ holds globally, we need φ to hold at ˆx; and for\nall fairness constraints ψi there has to be a next state ˆx′, where the whole\nproperty is true again (enforced by the free Z) and each fairness constraint\nis realised eventually on that path. The recursion in Z constantly reiterates\nthis reasoning, so if this function computes 1, then there is a path on which\nφ holds globally and where each ψi is true inﬁnitely often.\n6.5 Exercises\nExercises 6.1\n1. Write down the truth tables for the boolean formulas in Example 6.2 on page 359.\nIn your table, you may use 0 and 1, or F and T, whatever you prefer. What truth\nvalue does the boolean formula of item (4) on page 359 compute?\n2. ⊕is the exclusive-or: x ⊕y\ndef",
    "6.5 Exercises\nExercises 6.1\n1. Write down the truth tables for the boolean formulas in Example 6.2 on page 359.\nIn your table, you may use 0 and 1, or F and T, whatever you prefer. What truth\nvalue does the boolean formula of item (4) on page 359 compute?\n2. ⊕is the exclusive-or: x ⊕y\ndef\n= 1 if the values of x and y are diﬀerent; otherwise,\nx ⊕y\ndef\n= 0. Express this in propositional logic, i.e. ﬁnd a formula φ having the\nsame truth table as ⊕.\n3.\n*\nWrite down a boolean formula f(x, y) in terms of ·, +, ¯, 0 and 1, such that f\nhas the same truth table as p →q.\n4. Write down a BNF for the syntax of boolean formulas based on the operations\nin Deﬁnition 6.1.\nExercises 6.2\n1.\n*\nSuppose we swap all dashed and solid lines in the binary decision tree of Fig-\nure 6.2. Write out the truth table of the resulting binary decision tree and ﬁnd\na formula for it.\n6.5 Exercises\n399\n2.\n*\nConsider the following truth table:\np\nq\nr\nφ\nT\nT\nT\nT\nT\nT\nF\nF\nT\nF\nT\nF\nT\nF\nF\nF\nF\nT\nT\nT\nF\nT\nF\nF\nF\nF\nT\nT\nF\nF\nF\nF\nWrite down a binary decision tree which represents the boolean function speciﬁed\nin this truth table.\n3. Construct a binary decision tree for the boolean function speciﬁed in Figure 6.2,\nbut now the root should be a y-node and its two successors should be x-nodes.\n4. Consider the following boolean function given by its truth table:\nx\ny\nz\nf(x, y, z)\n1\n1\n1\n0\n1\n1\n0\n1\n1\n0\n1\n0\n1\n0\n0\n1\n0\n1\n1\n0\n0\n1\n0\n0\n0\n0\n1\n0\n0\n0\n0\n1\n(a) Construct a binary decision tree for f(x, y, z) such that the root is an x-node\nfollowed by y- and then z-nodes.\n(b) Construct another binary decision tree for f(x, y, z), but now let its root be\na z-node followed by y- and then x-nodes.\n5. Let T be a binary decision tree for a boolean function f(x1, x2, . . . , xn) of n\nboolean variables. Suppose that every variable occurs exactly once as one travels\ndown on any path of the tree T. Use mathematical induction to show that T has\n2n+1 −1 nodes.\nExercises 6.3\n1.\n*",
    "5. Let T be a binary decision tree for a boolean function f(x1, x2, . . . , xn) of n\nboolean variables. Suppose that every variable occurs exactly once as one travels\ndown on any path of the tree T. Use mathematical induction to show that T has\n2n+1 −1 nodes.\nExercises 6.3\n1.\n*\nExplain why all reductions C1–C3 (page 363) on a BDD B result in BDDs which\nstill represent the same function as B.\n2. Consider the BDD in Figure 6.7.\n(a)\n*\nSpecify the truth table for the boolean function f(x, y, z) represented by\nthis BDD.\n400\n6 Binary decision diagrams\n(b) Find a BDD for that function which does not have multiple occurrences of\nvariables along any path.\n3. Let f be the function represented by the BDD of Figure 6.3(b). Using also the\nBDDs B0, B1 and Bx illustrated in Figure 6.6, ﬁnd BDDs representing\n(a) f · x\n(b) x + f\n(c) f · 0\n(d) f · 1.\nExercises 6.4\n1. Figure 6.9 (page 367) shows a BDD with ordering [x, y, z].\n(a)\n*\nFind an equivalent reduced BDD with ordering [z, y, x]. (Hint: ﬁnd ﬁrst the\ndecision tree with the ordering [z, y, x], and then reduce it using C1–C3.)\n(b) Carry out the same construction process for the variable ordering [y, z, x].\nDoes the reduced BDD have more or fewer nodes than the ones for the\norderings [x, y, z] and [z, y, x]?\n2. Consider the BDDs in Figures 6.4–6.10. Determine which of them are OBDDs.\nIf you ﬁnd an OBDD, you need to specify a list of its boolean variables without\ndouble occurrences which demonstrates that ordering.\n3. Consider the following boolean formulas. Compute their unique reduced OBDDs\nwith respect to the ordering [x, y, z]. It is advisable to ﬁrst compute a binary\ndecision tree and then to perform the removal of redundancies.\n(a) f(x, y)\ndef\n= x · y\n(b)\n*\nf(x, y)\ndef\n= x + y\n(c) f(x, y)\ndef\n= x ⊕y\n(d)\n*\nf(x, y, z)\ndef\n= (x ⊕y) · (x + z).\n4. Recall the derived connective φ ↔ψ from Chapter 1 saying that for all valuations\nφ is true if, and only if, ψ is true.",
    "decision tree and then to perform the removal of redundancies.\n(a) f(x, y)\ndef\n= x · y\n(b)\n*\nf(x, y)\ndef\n= x + y\n(c) f(x, y)\ndef\n= x ⊕y\n(d)\n*\nf(x, y, z)\ndef\n= (x ⊕y) · (x + z).\n4. Recall the derived connective φ ↔ψ from Chapter 1 saying that for all valuations\nφ is true if, and only if, ψ is true.\n(a) Deﬁne this operator for boolean formulas using the basic operations ·, +, ⊕\nand ¯ from Deﬁnition 6.1.\n(b) Draw a reduced OBDD for the formula g(x, y)\ndef\n= x ↔y using the ordering\n[y, x].\n5. Consider the even parity function introduced at the end of the last section.\n(a) Deﬁne the odd parity function fodd(x1, x2, . . . , xn).\n(b) Draw an OBDD for the odd parity function for n = 5 and the ordering\n[x3, x5, x1, x4, x2]. Would the overall structure of this OBDD change if you\nchanged the ordering?\n(c) Show that feven(x1, x2, . . . , xn) and fodd(x1, x2, . . . , xn) denote the same\nboolean function.\n6. Use Theorem 6.7 (page 368) to show that, if the reductions C1–C3 are applied\nuntil no more reduction is possible, the result is independent of the order in\nwhich they were applied.\n6.5 Exercises\n401\nExercises 6.5\n1. Given the boolean formula f(x1, x2, x3)\ndef\n= x1 · (x2 + x3), compute its reduced\nOBDD for the following orderings:\n(a) [x1, x2, x3]\n(b) [x3, x1, x2]\n(c) [x3, x2, x1].\n2. Compute the reduced OBDD for f(x, y, z) = x · (z + z) + y · x in any ordering\nyou like. Is there a z-node in that reduced OBDD?\n3. Consider the boolean formula f(x, y, z)\ndef\n= (x + y + z) · (x + y + z) · (x + y). For\nthe variable orderings below, compute the (unique) reduced OBDD Bf of f with\nrespect to that ordering. It is best to write down the binary decision tree for that\nordering and then to apply all possible reductions.\n(a) [x, y, z].\n(b) [y, x, z].\n(c) [z, x, y].\n(d) Find an ordering of variables for which the resulting reduced OBDD Bf has a\nminimal number of edges; i.e. there is no ordering for which the corresponding",
    "ordering and then to apply all possible reductions.\n(a) [x, y, z].\n(b) [y, x, z].\n(c) [z, x, y].\n(d) Find an ordering of variables for which the resulting reduced OBDD Bf has a\nminimal number of edges; i.e. there is no ordering for which the corresponding\nBf has fewer edges. (How many possible orderings for x, y and z are there?)\n4. Given the truth table\nx\ny\nz\nf(x, y, z)\n1\n1\n1\n0\n1\n1\n0\n1\n1\n0\n1\n1\n1\n0\n0\n0\n0\n1\n1\n0\n0\n1\n0\n1\n0\n0\n1\n0\n0\n0\n0\n1\ncompute the reduced OBDD with respect to the following ordering of variables:\n(a) [x, y, z]\n(b) [z, y, x]\n(c) [y, z, x]\n(d) [x, z, y].\n5. Given the ordering [p, q, r], compute the reduced BDDs for p ∧(q ∨r) and (p ∧\nq) ∨(p ∧r) and explain why they are identical.\n6.\n*\nConsider the BDD in Figure 6.11 (page 370).\n(a) Construct its truth table.\n(b) Compute its conjunctive normal form.\n(c) Compare the length of that normal form with the size of the BDD. What is\nyour assessment?\n402\n6 Binary decision diagrams\nExercises 6.6\n1. Perform the execution of reduce on the following OBDDs:\n(a) The binary decision tree for\ni. x ⊕y\nii. x · y\niii. x + y\niv. x ↔y.\n(b) The OBDD in Figure 6.2 (page 361).\n(c)\n*\nThe OBDD in Figure 6.4 (page 363).\nExercises 6.7\n1. Recall the Shannon expansion in (6.1) on page 374. Suppose that x does not\noccur in f at all. Why does (6.1) still hold?\n2. Let f(x, y, z)\ndef\n= y + z · x + z · y + y · x be a boolean formula. Compute f’s\nShannon expansion with respect to\n(a) x\n(b) y\n(c) z.\n3. Show that boolean formulas f and g are semantically equivalent if, and only if,\nthe boolean formula (f + g) · (f + g) computes 1 for all possible assignments of\n0s and 1s to their variables.\n4. We may use the Shannon expansion to deﬁne formally how BDDs determine\nboolean functions. Let B be a BDD. It is intuitively clear that B determines\na unique boolean function. Formally, we compute a function fn inductively\n(bottom-up) for all nodes n of B:\n– If n is a terminal node labelled 0, then fn is the constant 0 function.",
    "boolean functions. Let B be a BDD. It is intuitively clear that B determines\na unique boolean function. Formally, we compute a function fn inductively\n(bottom-up) for all nodes n of B:\n– If n is a terminal node labelled 0, then fn is the constant 0 function.\n– Dually, if n is a terminal 1-node, then fn is the constant 1 function.\n– If n is a non-terminal node labelled x, then we already have deﬁned the\nboolean functions flo(n) and fhi(n) and set fn to be x · flo(n) + x · fhi(n).\nIf i is the initial node of B, then fi is the boolean function represented by\nB. Observe that we could apply this deﬁnition as a symbolic evaluation of B\nresulting in a boolean formula. For example, the BDD of Figure 6.3(b) renders\nx · (y · 1 + y · 0) + x · 0. Compute the boolean formulas obtained in this way for\nthe following BDDs:\n(a) the BDD in Figure 6.5(b) (page 364)\n(b) the BDDs in Figure 6.6 (page 365)\n(c) the BDD in Figure 6.11 (page 370).\n5.\n*\nConsider a ternary (= takes three arguments) boolean connective f →(g, h)\nwhich is equivalent to g when f is true; otherwise, it is equivalent to h.\n(a) Deﬁne this connective using any of the operators +, ·, ⊕or ¯.\n(b) Recall exercise 4. Use the ternary operator above to write fn as an expres-\nsion of flo(n), fhi(n) and its label x.\n6.5 Exercises\n403\n0\n1\nz\nx\ny\ny\nz\ny\nz\nx\n0\n1\nFigure 6.30. The reduced OBDDs Bf and Bg (see exercises).\n(c) Use mathematical induction (on what?) to prove that, if the root of fn is\nan x-node, then fn is independent of any y which comes before x in an\nassumed variable ordering.\n6. Explain why apply (op, Bf, Bg), where Bf and Bg have compatible ordering,\nproduces an OBDD with an ordering compatible with that of Bf and Bg.\n7. Explain why the four cases of the control structure for apply are exhaustive,\ni.e. there are no other possible cases in its execution.\n8. Consider the reduced OBDDs Bf and Bg in Figure 6.30. Recall that, in order\nto compute the reduced OBDD for f op g, you need to",
    "7. Explain why the four cases of the control structure for apply are exhaustive,\ni.e. there are no other possible cases in its execution.\n8. Consider the reduced OBDDs Bf and Bg in Figure 6.30. Recall that, in order\nto compute the reduced OBDD for f op g, you need to\n– construct the tree showing the recursive descent of apply (op, Bf, Bg) as\ndone in Figure 6.16;\n– use that tree to simulate apply (op, Bf, Bg); and\n– reduce, if necessary, the resulting OBDD.\nPerform these steps on the OBDDs of Figure 6.30 for the operation ‘op’ being\n(a) +\n(b) ⊕\n(c) ·\n9. Let Bf be the OBDD in Figure 6.11 (page 370). Compute apply (⊕, Bf, B1) and\nreduce the resulting OBDD. If you did everything correctly, then this OBDD\nshould be isomorphic to the one obtained from swapping 0- and 1-nodes in\nFigure 6.11.\n10.\n*\nConsider the OBDD Bc in Figure 6.31 which represents the ‘don’t care’ condi-\ntions for comparing the boolean functions f and g represented in Figure 6.30.\nThis means that we want to compare whether f and g are equal for all values\nof variables except those for which c is true (i.e. we ‘don’t care’ when c is true).\n(a) Show that the boolean formula (f ⊕g) + c is valid (always computes 1)\nif, and only if, f and g are equivalent on all values for which c evaluates\nto 0.\n404\n6 Binary decision diagrams\n0\n1\nz\ny\ny\nx\nFigure 6.31. The reduced OBDD Bc representing the ‘don’t care’ con-\nditions for the equivalence test of the OBDDs in Figure 6.30.\n(b) Proceed in three steps as in exercise 8 on page 403 to compute the reduced\nOBDD for (f ⊕g) + c from the OBDDs for f, g and c. Which call to apply\nneeds to be ﬁrst?\n11. We say that v ∈{0, 1} is a (left)-controlling value for the operation op, if either\nv op x = 1 or v op x = 0 for all values of x. We say that v is a controlling value\nif it is a left- and right-controlling value.\n(a) Deﬁne the notion of a right-controlling value.\n(b) Give examples of operations with controlling values.",
    "v op x = 1 or v op x = 0 for all values of x. We say that v is a controlling value\nif it is a left- and right-controlling value.\n(a) Deﬁne the notion of a right-controlling value.\n(b) Give examples of operations with controlling values.\n(c) Describe informally how apply can be optimised when op has a controlling\nvalue.\n(d) Could one still do some optimisation if op had only a left- or right-controlling\nvalue?\n12. We showed that the worst-time complexity of apply is O(|Bf| · |Bg|). Show that\nthis upper bound is hard, i.e. it cannot be improved:\n(a) Consider the functions f(x1, x2, . . . , x2n+2m)\ndef\n= x1 · xn+m+1 + · · · + xn ·\nx2n+m and g(x1, x2, . . . , x2n+2m)\ndef\n= xn+1 · x2n+m+1 + · · · + xn+m · x2n+2m\nwhich are in sum-of-product form. Compute the sum-of-product form of\nf + g.\n(b) Choose the ordering [x1, x2, . . . , x2n+2m] and argue that the OBDDs Bf\nand Bg have 2n+1 and 2m+1 edges, respectively.\n(c) Use the result from part (a) to conclude that Bf+g has 2n+m+1 edges, i.e.\n0.5 · |Bf| · |Bg|.\nExercises 6.8\n1. Let f be the reduced OBDD represented in Figure 6.5(b) (page 364). Compute\nthe reduced OBDD for the restrictions:\n(a) f[0/x]\n(b)\n*\nf[1/x]\n6.5 Exercises\n405\n(c) f[1/y]\n(d)\n*\nf[0/z].\n2.\n*\nSuppose that we intend to modify the algorithm restrict so that it is capable\nof computing reduced OBDDs for a general composition f[g/x].\n(a) Generalise Equation (6.1) to reﬂect the intuitive meaning of the operation\n[g/x].\n(b) What fact about OBDDs causes problems for computing this composition\ndirectly?\n(c) How can we compute this composition given the algorithms discussed so far?\n3. We deﬁne read-1-BDDs as BDDs B where each boolean variable occurs at most\nonce on any evaluation path of B. In particular, read-1-BDDs need not possess\nan ordering on their boolean variables. Clearly, every OBDD is a read-1-BDD;\nbut not every read-1-BDD is an OBDD (see Figure 6.10). In Figure 6.18 we see\na BDD which is not a read-1-BDD; the path for (x, y, z) ⇒(1, 0, 1) ‘reads’ the",
    "an ordering on their boolean variables. Clearly, every OBDD is a read-1-BDD;\nbut not every read-1-BDD is an OBDD (see Figure 6.10). In Figure 6.18 we see\na BDD which is not a read-1-BDD; the path for (x, y, z) ⇒(1, 0, 1) ‘reads’ the\nvalue of x twice.\nCritically assess the implementation of boolean formulas via OBDDs to see which\nimplementation details could be carried out for read-1-BDDs as well. Which\nimplementation aspects would be problematic?\n4. (For those who have had a course on ﬁnite automata.) Every boolean function\nf in n arguments can be viewed as a subset Lf of {0, 1}n; deﬁned to be the\nset of all those bit vectors (v1, v2, . . . , vn) for which f computes 1. Since this\nis a ﬁnite set, Lf is a regular language and has therefore a deterministic ﬁnite\nautomaton with a minimal number of states which accepts Lf. Can you match\nsome of the OBDD operations with those known for ﬁnite automata? How close\nis the correspondence? (You may have to consider non-reduced OBDDs.)\n5. (a) Show that every boolean function in n arguments can be represented as a\nboolean formula of the grammar\nf ::= 0 | x | f | f1 + f2.\n(b) Why does this also imply that every such function can be represented by a\nreduced OBDD in any variable ordering?\n6. Use mathematical induction on n to prove that there are exactly 2(2n) many\ndiﬀerent boolean functions in n arguments.\nExercises 6.9\n1. Use the exists algorithm to compute the OBDDs for\n(a) ∃x3.f, given the OBDD for f in Figure 6.11 (page 370)\n(b) ∀y.g, given the OBDD for g in Figure 6.9 (page 367)\n(c) ∃x2.∃x3.x1 · y1 + x2 · y2 + x3 · y3.\n2. Let f be a boolean function depending on n variables.\n(a) Show:\n406\n6 Binary decision diagrams\ni. The formula ∃x.f depends on all those variables that f depends upon,\nexcept x.\nii. If f computes to 1 with respect to a valuation ρ, then ∃x.f computes 1\nwith respect to the same valuation.\niii. If ∃x.f computes to 1 with respect to a valuation ρ, then there is a valuation",
    "i. The formula ∃x.f depends on all those variables that f depends upon,\nexcept x.\nii. If f computes to 1 with respect to a valuation ρ, then ∃x.f computes 1\nwith respect to the same valuation.\niii. If ∃x.f computes to 1 with respect to a valuation ρ, then there is a valuation\nρ′ for f which agrees with ρ for all variables other than x such that f\ncomputes to 1 under ρ′.\n(b) Can the statements above be shown for the function value 0?\n3. Let φ be a boolean formula.\n(a)\n*\nShow that φ is satisﬁable if, and only if, ∃x.φ is satisﬁable.\n(b) Show that φ is valid if, and only if, ∀x.φ is valid.\n(c) Generalise the two facts above to nested quantiﬁcations ∃ˆx and ∀ˆx. (Use\ninduction on the number of quantiﬁed variables.)\n4. Show that ∀ˆx.f and ∃ˆx.f are semantically equivalent. Use induction on the\nnumber of arguments in the vector ˆx.\nExercises 6.10\n(For those who know about complexity classes.)\n1. Show that 3SAT can be reduced to nested existential boolean quantiﬁcation.\nGiven an instance of 3SAT, we may think of it as a boolean formula f in product-\nof-sums form g1 · g2 · · · · · gn, where each gi is of the form (l1 + l2 + l3) with each\nlj being a boolean variable or its complementation. For example, f could be\n(x + y + z) · (x5 + x + x7) · (x2 + z + x) · (x4 + x2 + x4).\n(a) Show that you can represent each function gi with an OBDD of no more\nthan three non-terminals, independently of the chosen ordering.\n(b) Introduce n new boolean variables z1, z2, . . . , zn. We write \n1≤i≤n fi for\nthe expression f1 + f2 + · · · + fn and \u0002\n1≤i≤n fi for f1 · f2 · · · · · fn. Consider\nthe boolean formula h, deﬁned as\n\u0003\n1≤i≤n\n\ngi · zi ·\n\u0001\n1≤j<i\nzj\n\n.\n(6.28)\nChoose any ordering of variables whose list begins as in [z1, z2, . . . , zn, . . . ].\nDraw the OBDD for h (draw only the root nodes for gi).\n(c) Argue that the OBDD above has at most 4n non-terminal nodes.\n(d) Show that f is satisﬁable if, and only if, the OBDD for ∃z1.∃z2. . . . .∃zn.h is\nnot equal to B1.",
    "Draw the OBDD for h (draw only the root nodes for gi).\n(c) Argue that the OBDD above has at most 4n non-terminal nodes.\n(d) Show that f is satisﬁable if, and only if, the OBDD for ∃z1.∃z2. . . . .∃zn.h is\nnot equal to B1.\n(e) Explain why the last item shows a reduction of 3SAT to nested existential\nquantiﬁcation.\n2. Show that the problem of ﬁnding an optimal ordering for representing boolean\nfunctions as OBDDs is in coNP.\n6.5 Exercises\n407\nx1\nx2\nx2\nx1\ns0\ns3\ns1\ns2\nx2\ns1\ns2\nx1\ns0\nFigure 6.32. (a) A CTL model with four states. (b) A CTL model with\nthree states.\n3. Recall that ∃x.f is deﬁned as f[1/x] + f[0/x]. Since we have eﬃcient algorithms\nfor restriction and +, we obtain hereby an eﬃcient algorithm for ∃z1. . . . ∃zn.f.\nThus, P equals NP! What is wrong with this argument?\nExercises 6.11\n1.\n*\nConsider the CTL model in Figure 6.32(a). Using the ordering [x1, x2], draw the\nOBDD for the subsets {s0, s1} and {s0, s2}.\n2. Consider the CTL model in Figure 6.32(b). Because the number of states is not\nan exact power of 2, there are more than one OBDDs representing any given\nset of states. Using again the ordering [x1, x2], draw all possible OBDDs for the\nsubsets {s0, s1} and {s0, s2}.\nExercises 6.12\n1. Consider the CTL model in Figure 6.32(a).\n(a) Work out the truth table for the transition relation, ordering the columns\n[x1, x′\n1, x2, x′\n2]. There should be as many 1s in the ﬁnal column as there are\narrows in the transition relation. There is no freedom in the representation\nin this case, since the number of states is an exact power of 2.\n(b) Draw the OBDD for this transition relation, using the variable ordering\n[x1, x′\n1, x2, x′\n2].\n2. Apply the algorithm of Section 3.6.1, but now interpreted over OBDDs in the\nordering [x1, x2], to compute the set of states of the CTL model in Figure 6.32(b)\nwhich satisfy\n(a) AG (x1 ∨¬x2)\n(b) E[x2 U x1].\nShow the OBDDs which are computed along the way.\n3. Explain why exists(ˆx′, apply(·, B→, BX′)) faithfully implements the meaning",
    "ordering [x1, x2], to compute the set of states of the CTL model in Figure 6.32(b)\nwhich satisfy\n(a) AG (x1 ∨¬x2)\n(b) E[x2 U x1].\nShow the OBDDs which are computed along the way.\n3. Explain why exists(ˆx′, apply(·, B→, BX′)) faithfully implements the meaning\nof pre∃(X).\n408\n6 Binary decision diagrams\nx1\nx2\nx3\nFigure 6.33. A synchronous circuit for a modulo 8 counter.\nExercises 6.13\n1. (a) Simulate the evolution of the circuit in Figure 6.29 (page 389) with initial\nstate 01. What do you think that it computes?\n(b) Write down the explicit CTL model (S, →, L) for this circuit.\n2. Consider the sequential synchronous circuit in Figure 6.33.\n(a) Construct the functions fi for i = 1, 2, 3.\n(b) Code the function f →.\n(c) Recall from Chapter 2 that (∃x.φ) ∧ψ is semantically equivalent to ∃x.(φ ∧\nψ) if x is not free in ψ.\ni. Why is this also true in our setting of boolean formulas?\nii. Apply this law to push the ∃quantiﬁcations in f →as far inwards as possi-\nble. This is an often useful optimisation in checking synchronous circuits.\n3. Consider the boolean formula for the 2-bit comparator:\nf(x1, x2, y1, y2)\ndef\n= (x1 ↔y1) · (x2 ↔y2).\n(a) Draw its OBDD for the ordering [x1, y1, x2, y2].\n(b) Draw its OBDD for the ordering [x1, x2, y1, y2] and compare that with the\none above.\n4. (a) Can you use (6.6) from page 388 to code the transition relation →of the\nmodel in Figure 6.24 on page 384?\n(b) Can you do it with equation (6.9) from page 390?\n(c) With equation (6.8) from page 389?\n6.5 Exercises\n409\nExercises 6.14\n1. Let ρ be the valuation for which (x, y, z) ⇒(0, 1, 1). Compute whether ρ ⊨f\nholds for the following boolean formulas:\n(a) x · (y + z · (y ⊕x))\n(b) ∃x.(y · (x + z + y) + x · y)\n(c) ∀x.(y · (x + z + y) + x · y)\n(d) ∃z.(x · z + ∀x.((y + (x + x) · z)))\n(e)\n*\n∀x.(y + z).\n2.\n*\nUse (6.14) from page 393 and the deﬁnition of the satisfaction relation for for-\nmulas of the relational mu-calculus to prove ρ ⊨νZ.Z for all valuations ρ. In",
    "(b) ∃x.(y · (x + z + y) + x · y)\n(c) ∀x.(y · (x + z + y) + x · y)\n(d) ∃z.(x · z + ∀x.((y + (x + x) · z)))\n(e)\n*\n∀x.(y + z).\n2.\n*\nUse (6.14) from page 393 and the deﬁnition of the satisfaction relation for for-\nmulas of the relational mu-calculus to prove ρ ⊨νZ.Z for all valuations ρ. In\nthis case, f equals Z and you need to show (6.14) by mathematical induction on\nm ≥0.\n3. An implementation which decides ⊨and ̸⊨for the relational mu-calculus ob-\nviously cannot represent valuations which assign semantic values 0 or 1 to all,\ni.e. inﬁnitely many variables. Thus, it makes sense to consider ⊨as a relation\nbetween pairs (ρ, f), where ρ only assigns semantic values to all free variables\nof f.\n(a) Assume that νZ and µZ, ∃x, ∀x, and [ˆx := ˆx′] are binding constructs similar\nto the quantiﬁers in predicate logic. Deﬁne formally the set of free variables\nfor a formula f of the relational mu-calculus. (Hint: You should deﬁne this\nby structural induction on f. Also, which variables get bound in f[ˆx := ˆx′]?)\n(b) Recall the notion of t being free for x in φ which we discussed in Section 2.2.4\nDeﬁne what ‘g is free for Z in f’ should mean and ﬁnd an example, where g\nis not free for Z in f.\n(c) Explain informally why we can decide whether ρ ⊨f holds, provided that ρ\nassigns values 0 or 1 to all free variables of f. Explain why this answer will\nbe independent of what ρ does to variables which are bound in f. Why is\nthis relevant for an implementation framework?\n4. Let ρ be the valuation for which (x, x′, y, y′) ⇒(0, 1, 1, 1). Determine whether ρ ⊨\nf holds for the following formulas f (recall that we write f ↔g as an abbreviation\nfor f ⊕g, meaning that f computes 1 iﬀg computes 1):\n(a) ∃x.(x′ ↔(y + y′ · x))\n(b) ∀x.(x′ ↔(y + y′ · x))\n(c) ∃x′.(x′ ↔(y + y′ · x))\n(d) ∀x′.(x′ ↔(y + y′ · x)).\n5. Let ρ be a valuation with ρ(x′\n1) = 1 and ρ(x′\n2) = 0. Determine whether ρ ⊨f\nholds for the following:\n(a) x1[ˆx := ˆx′]\n(b) (x1 + x2)[ˆx := ˆx′]\n(c) (x1 · x2)[ˆx := ˆx′].",
    "(a) ∃x.(x′ ↔(y + y′ · x))\n(b) ∀x.(x′ ↔(y + y′ · x))\n(c) ∃x′.(x′ ↔(y + y′ · x))\n(d) ∀x′.(x′ ↔(y + y′ · x)).\n5. Let ρ be a valuation with ρ(x′\n1) = 1 and ρ(x′\n2) = 0. Determine whether ρ ⊨f\nholds for the following:\n(a) x1[ˆx := ˆx′]\n(b) (x1 + x2)[ˆx := ˆx′]\n(c) (x1 · x2)[ˆx := ˆx′].\n6. Evaluate ρ ⊨(∃x1.(x1 + x2))[ˆx := ˆx′] and explain how the valuation ρ changes\nin that process. In particular, [ˆx := ˆx′] replaces xi by x′\ni, but why does this not\ninterfere with the binding quantiﬁer ∃x1?\n410\n6 Binary decision diagrams\n7. (a) How would you deﬁne the notion of semantic entailment for the relational\nmu-calculus?\n(b) Deﬁne formally when two formulas of the relational mu-calculus are seman-\ntically equivalent.\nExercises 6.15\n1. Using the model of Figure 6.24 (page 384), determine whether ρ ⊨f EX (x1∨¬x2)\nholds, where ρ is\n(a) (x1, x2) ⇒(1, 0)\n(b) (x1, x2) ⇒(0, 1)\n(c) (x1, x2) ⇒(0, 0).\n2. Let S be {s0, s1}, with s0 →s0, s0 →s1 and s1 →s0 as possible transitions\nand L(s0) = {x1} and L(s1) = ∅. Compute the boolean function f EX (EX ¬x1).\n3. Equations (6.17) (page 395), (6.19) and (6.20) deﬁne f EF φ, f AF φ and f EG φ.\nWrite down a similar equation to deﬁne f AG φ.\n4. Deﬁne a direct coding f AU φ by modifying (6.18) appropriately.\n5. Mimic the example checks on page 396 for the connective AU: consider the\nmodel of Figure 6.24 (page 384). Since [[E[(x1 ∨x2) U (¬x1 ∧¬x2)]]] equals the\nentire state set {s0, s1, s2}, your coding of f E[x1∨x2U¬x1∧¬x2] is correct if it\ncomputes 1 for all bit vectors diﬀerent from (1, 1).\n(a) Verify that your coding is indeed correct.\n(b) Find a boolean formula without ﬁxed points which is semantically equiva-\nlent to f E[(x1∨x2)U(¬x1∧¬x2)].\n6. (a) Use (6.20) on page 395 to compute f EG ¬x1 for the model in Figure 6.24.\n(b) Show that f EG ¬x1 faithfully models the set of all states which satisfy\nEG ¬x1.\n7. In the grammar (6.10) for the relational mu-calculus on page 390, it was stated",
    "lent to f E[(x1∨x2)U(¬x1∧¬x2)].\n6. (a) Use (6.20) on page 395 to compute f EG ¬x1 for the model in Figure 6.24.\n(b) Show that f EG ¬x1 faithfully models the set of all states which satisfy\nEG ¬x1.\n7. In the grammar (6.10) for the relational mu-calculus on page 390, it was stated\nthat, in the formulas µZ.f and νZ.f, any occurrence of Z in f is required to\nfall within an even number of complementation symbols ¯. What happens if we\ndrop this requirement?\n(a) Consider the expression µZ.Z. We already saw that our relation ρ is total in\nthe sense that either ρ ⊨f or ρ ̸⊨f holds for all choices of valuations ρ and\nrelational mu-calculus formulas f. But formulas like µZ.Z are not formally\nmonotone. Let ρ be any valuation. Use mutual mathematical induction to\nshow:\ni. ρ ̸⊨µmZ.Z for all even numbers m ≥0\nii. ρ ⊨µmZ.Z for all odd numbers m ≥1\nInfer from these two items that ρ ⊨µZ.Z holds according to (6.12).\n(b) Consider any environment ρ. Use mathematical induction on m (and maybe\nan analysis on ρ) to show:\nIf\nρ ⊨µmZ.(x1 + x2 · Z)\nfor\nsome\nm ≥0,\nthen\nρ ⊨\nµkZ.(x1 + x2 · Z) for all k ≥m.\n6.5 Exercises\n411\n(c) In general, if f is formally monotone in Z then ρ ⊨µmZ.f implies\nρ ⊨µm+1Z.f. Can you state a similar property for the greatest ﬁxed-point\noperator ν?\n8. Given the CTL model for the circuit in Figure 6.29 (page 389):\n(a)\n*\ncode the function f EX (x1∧¬x2)\n(b) code the function f AG (AF ¬x1∧¬x2)\n(c)\n*\nﬁnd a boolean formula without any ﬁxed points which is semantically equiv-\nalent to f AG (AF ¬x1∧¬x2).\n9. Consider the sequential synchronous circuit in Figure 6.33 (page 408). Evaluate\nρ ⊨f EX x2, where ρ equals\n(a) (x1, x2, x3) ⇒(1, 0, 1)\n(b) (x1, x2, x3) ⇒(0, 1, 0).\n10. Prove\nTheorem 6.19 Given a coding for a ﬁnite CTL model, let φ be a CTL formula\nfrom an adequate fragment. Then [[φ]] corresponds to the set of valuations ρ such\nthat ρ ⊨f φ.\nby structural induction on φ. You may ﬁrst want to show that the evaluation of",
    "(b) (x1, x2, x3) ⇒(0, 1, 0).\n10. Prove\nTheorem 6.19 Given a coding for a ﬁnite CTL model, let φ be a CTL formula\nfrom an adequate fragment. Then [[φ]] corresponds to the set of valuations ρ such\nthat ρ ⊨f φ.\nby structural induction on φ. You may ﬁrst want to show that the evaluation of\nρ ⊨f φ depends only on the values ρ(xi), i.e. it does not matter what ρ assigns\nto x′\ni or Z.\n11. Argue that Theorem 6.19 above remains valid for arbitrary CTL formulas as\nlong as we translate formulas φ which are not in the adequate fragment into\nsemantically equivalent formulas ψ in that fragment and deﬁne f φ to be f ψ.\n12. Derive the formula f AF (¬x1∧x2) for the model in Figure 6.32(b) on page 407\nand evaluate it for the valuation corresponding to state s2 to determine whether\ns2 ⊨AF (¬x1 ∧x2) holds.\n13. Repeat the last exercise with f E[x1∨¬x2Ux1].\n14. Recall the way the two labelling algorithms operate in Chapter 3. Does our\nsymbolic coding mimic either or both of them, or neither?\nExercises 6.16\n1. Consider the equations in (6.22) and (6.27). The former deﬁnes fair in terms of\nf ECG⊤, whereas the latter deﬁnes f ECG φ for general φ. Why is this unproblem-\natic, i.e. non-circular?\n2. Given a ﬁxed CTL model M = (S, →, L), we saw how to code formulas f φ\nrepresenting the set of states s ∈S with s ⊨φ, φ being a CTL formula of an\nadequate fragment.\n(a) Assume the coding without consideration of simple fairness constraints. Use\nstructural induction on the CTL formula φ to show that\ni. the free variables of f φ are among ˆx, where the latter is the vector of\nboolean variables which code states s ∈S;\nii. all ﬁxed-point subformulas of f φ are formally monotone.\n412\n6 Binary decision diagrams\n(b) Show these two assertions if f φ also encodes simple fairness constraints.\n3. Consider the pseudo-code for the function SAT on page 227. We now want to\nmodify it so that the resulting output is not a set, or an OBDD, but a formula",
    "412\n6 Binary decision diagrams\n(b) Show these two assertions if f φ also encodes simple fairness constraints.\n3. Consider the pseudo-code for the function SAT on page 227. We now want to\nmodify it so that the resulting output is not a set, or an OBDD, but a formula\nof the relational mu-calculus; thus, we complete the table in Figure 6.22 on\npage 380 to give formulas of the relational mu-calculus. For example, the output\nfor ⊤would be 1 and the output for EU ψ would be a recursive call to SAT\ninformed by (6.18). Do you have a need for a separate function which handles\nleast or greatest ﬁxed points?\n4. (a) Write pseudo-code for a function SATrel mu which takes as input a formula\nof the relational mu-calculus, f, and synthesises an OBDD Bf, represent-\ning f. Assume that there are no ﬁxed-point subexpressions of f such that\ntheir recursive body contains a recursion variable of an outwards ﬁxed point.\nThus, the formula in (6.27) is not allowed. The ﬁxed-point operators µ and\nν require separate subfunctions which iterate the ﬁxed-point meaning in-\nformed by (6.12), respectively (6.14). Some of your clauses may need further\ncomment. E.g. how do you handle the constructor [ˆx := ˆx′]?\n(b) Explain what goes wrong if the input to your code is the formula in (6.27).\n5. If f is a formula with a vector of n free boolean variables ˆx, then the iteration of\nµZ.f, whether as OBDD implementation, or as in (6.12), may require up to 2n\nrecursive unfoldings to compute its meaning. Clearly, this is unacceptable. Given\nthe symbolic encoding of a CTL model M = (S, →, L) and a set I ⊆S of initial\nstates, we seek a formula that represents all states which are reachable from I on\nsome ﬁnite computation path in M. Using the extended Until operator in (6.26),\nwe may express this as checkEU (f I, ⊤), where f I is the characteristic function\nof I. We can ‘speed up’ this iterative process with a technique called ‘iterative\nsquaring’:\nµY.(f →+ ∃ˆw.(Y [ˆx′ := ˆw] · Y [ˆx := ˆw])).\n(6.29)",
    "we may express this as checkEU (f I, ⊤), where f I is the characteristic function\nof I. We can ‘speed up’ this iterative process with a technique called ‘iterative\nsquaring’:\nµY.(f →+ ∃ˆw.(Y [ˆx′ := ˆw] · Y [ˆx := ˆw])).\n(6.29)\nNote that this formula depends on the same boolean variables as f →, i.e. the\npair (ˆx, ˆx′). Explain informally:\nIf we apply (6.12) m times to the formula in (6.29), then this\nhas the same semantic ‘eﬀect’ as applying this rule 2m times to\ncheckEU (f →, ⊤).\nThus, one may ﬁrst compute the set of states reachable from any initial state\nand then restrict model checking to those states. Note that this reduction does\nnot alter the semantics of s ⊨φ for initial states s, so it is a sound technique;\nit sometimes improves, other times worsens, the performance of symbolic model\nchecks.\n6.6 Bibliographic notes\n413\n6.6 Bibliographic notes\nOrdered binary decision diagrams are due to R. E. Bryant [Bry86]. Binary\ndecision diagrams were introduced by C. Y. Lee [Lee59] and S. B. Akers\n[Ake78]. For a nice survey of these ideas see [Bry92]. For the limitations\nof OBDDs as models for integer multiplication as well as interesting con-\nnections to VLSI design see [Bry91]. A general introduction to the topic of\ncomputational complexity and its tight connections to logic can be found\nin [Pap94]. The modal mu-calculus was invented by D. Kozen [Koz83]; for\nmore on that logic and its application to speciﬁcations and veriﬁcation see\n[Bra91].\nThe use of BDDs in model checking was proposed by the team of au-\nthors J. R. Burch, E. M. Clarke, K. L. McMillan, D. L. Dill and J. Hwang\n[BCM+90, CGL93, McM93].\nBibliography\nAke78. S. B. Akers. Binary decision diagrams. IEEE Transactions on\nComputers, C-27(6):509–516, 1978.\nAO91. K. R. Apt and E.-R. Olderog. Veriﬁcation of Sequential and\nConcurrent Programs. Springer-Verlag, 1991.\nBac86. R. C. Backhouse. Program Construction and Veriﬁcation. Prentice\nHall, 1986.",
    "Ake78. S. B. Akers. Binary decision diagrams. IEEE Transactions on\nComputers, C-27(6):509–516, 1978.\nAO91. K. R. Apt and E.-R. Olderog. Veriﬁcation of Sequential and\nConcurrent Programs. Springer-Verlag, 1991.\nBac86. R. C. Backhouse. Program Construction and Veriﬁcation. Prentice\nHall, 1986.\nBCCZ99. A. Biere, A. Cimatti, E. Clarke, and Y. Zhu. Symbolic model checking\nwithout BDDs. In Proceedings of Tools and Algorithms for the\nAnalysis and Construction of Systems (TACAS’99), volume 1579 of\nLecture Notes in Computer Science, pages 193–207, 1999.\nBCM+90. J. R. Burch, J. M. Clarke, K. L. McMillan, D. L. Dill, and J. Hwang.\nSymbolic model checking: 1020 states and beyond. In IEEE Symposium\non Logic in Computer Science. IEEE Computer Society Press, 1990.\nBEKV94. K. Broda, S. Eisenbach, H. Khoshnevisan, and S. Vickers. Reasoned\nProgramming. Prentice Hall, 1994.\nBJ80. G. Boolos and R. Jeﬀrey. Computability and Logic. Cambridge\nUniversity Press, 2nd edition, 1980.\nBoo54. G. Boole. An Investigation of the Laws of Thought. Dover, New York,\n1854.\nBra91. J. C. Bradﬁeld. Verifying Temporal Properties of Systems. Birkh¨auser,\nBoston, 1991.\nBry86. R. E. Bryant. Graph-based algorithms for boolean function\nmanipulation. IEEE Transactions on Compilers, C-35(8), 1986.\nBry91. R. E. Bryant. On the Complexity of VLSI Implementations and Graph\nRepresentations of Boolean Functions with Applications to Integer\nMultiplication. IEEE Transactions on Computers, 40(2):205–213,\nFebruary 1991.\nBry92. R. E. Bryant. Symbolic Boolean Manipulation with Ordered\nBinary-decision Diagrams. ACM Computing Surveys, 24(3):293–318,\nSeptember 1992.\nCE81. E. M. Clarke and E. A. Emerson. Synthesis of synchronization\nskeletons for branching time temporal logic. In D. Kozen, editor,\nLogic of Programs Workshop, number 131 in LNCS. Springer Verlag,\n1981.\n414\nBibliography\n415\nCGL93. E. Clarke, O. Grumberg, and D. Long. Veriﬁcation tools for\nﬁnite-state concurrent systems. In A Decade of Concurrency, number",
    "skeletons for branching time temporal logic. In D. Kozen, editor,\nLogic of Programs Workshop, number 131 in LNCS. Springer Verlag,\n1981.\n414\nBibliography\n415\nCGL93. E. Clarke, O. Grumberg, and D. Long. Veriﬁcation tools for\nﬁnite-state concurrent systems. In A Decade of Concurrency, number\n803 in Lecture Notes in Computer Science, pages 124–175. Springer\nVerlag, 1993.\nCGL94. E. M. Clarke, O. Grumberg, and D. E. Long. Model checking and\nAbstraction. ACM Transactions on Programming Languages and\nSystems, 16(5):1512–1542, September 1994.\nCGP99. E. M. Clarke, O. Grumberg, and D. A. Peled. Model Checking. MIT\nPress, 1999.\nChe80. B. F. Chellas. Modal Logic – an Introduction. Cambridge University\nPress, 1980.\nDam96. D. R. Dams. Abstract Interpretation and Partition Reﬁnement for\nModel Checking. PhD thesis, Institute for Programming Research and\nAlgorithmics. Eindhoven University of Technology, July 1996.\nDij76. E. W. Dijkstra. A Discipline of Programming. Prentice Hall, 1976.\nDP96. R. Davies and F. Pfenning. A Modal Analysis of Staged Computation.\nIn 23rd Annual ACM Symposium on Principles of Programming\nLanguages. ACM Press, January 1996.\nEJC03. S. Eisenbach, V. Jurisic, and C. Sadler. Modeling the evolution of\n.NET programs. In IFIP International Conference on Formal Methods\nfor Open Distributed Systems, LNCS. Springer Verlag, 2003.\nEN94. R. Elmasri and S. B. Navathe. Fundamentals of Database Systems.\nBenjamin/Cummings, 1994.\nFHMV95. R. Fagin, J. Y. Halpern, Y. Moses, and M. Y. Vardi. Reasoning about\nKnowledge. MIT Press, Cambridge, 1995.\nFit93. M. Fitting. Basic modal logic. In D. Gabbay, C. Hogger, and\nJ. Robinson, editors, Handbook of Logic in Artiﬁcial Intelligence and\nLogic Programming, volume 1. Oxford University Press, 1993.\nFit96. M. Fitting. First-Order Logic and Automated Theorem Proving.\nSpringer, 2nd edition, 1996.\nFra92. N. Francez. Program Veriﬁcation. Addison-Wesley, 1992.\nFre03. G. Frege. Grundgesetze der Arithmetik, begriﬀsschriftlich abgeleitet.",
    "Logic Programming, volume 1. Oxford University Press, 1993.\nFit96. M. Fitting. First-Order Logic and Automated Theorem Proving.\nSpringer, 2nd edition, 1996.\nFra92. N. Francez. Program Veriﬁcation. Addison-Wesley, 1992.\nFre03. G. Frege. Grundgesetze der Arithmetik, begriﬀsschriftlich abgeleitet.\n1903. Volumes I and II (Jena).\nGal87. J. H. Gallier. Logic for Computer Science. John Wiley, 1987.\nGen69. G. Gentzen. Investigations into logical deduction. In M. E. Szabo,\neditor, The Collected Papers of Gerhard Gentzen, chapter 3, pages\n68–129. North-Holland Publishing Company, 1969.\nGol87. R. Goldblatt. Logics of Time and Computation. CSLI Lecture Notes,\n1987.\nGri82. D. Gries. A note on a standard strategy for developing loop invariants\nand loops. Science of Computer Programming, 2:207–214, 1982.\nHam78. A. G. Hamilton. Logic for Mathematicians. Cambridge University\nPress, 1978.\nHoa69. C. A. R. Hoare. An axiomatic basis for computer programming.\nCommunications of the ACM, 12:576–580, 1969.\nHod77. W. Hodges. Logic. Penguin Books, 1977.\nHod83. W. Hodges. Elementary predicate logic. In D. Gabbay and\nF. Guenthner, editors, Handbook of Philosophical Logic, volume 1.\nDordrecht: D. Reidel, 1983.\n416\nBibliography\nHol90. G. Holzmann. Design and Validation of Computer Protocols. Prentice\nHall, 1990.\nJSS01. D. Jackson, I. Shlyakhter, and M. Sridharan. A Micromodularity\nMechanism. In Proceedings of the ACM SIGSOFT Conference on the\nFoundations of Software Engineering/European Software Engineering\nConference (FSE/ESEC’01), September 2001.\nKoz83. D. Kozen. Results on the propositional mu-calculus. Theoretical\nComputer Science, 27:333–354, 1983.\nLee59. C. Y. Lee. Representation of switching circuits by binary-decision\nprograms. Bell System Technical Journal, 38:985–999, 1959.\nLon83. D. E. Long. Model Checking, Abstraction, and Compositional\nVeriﬁcation. PhD thesis, School of Computer Science, Carnegie Mellon\nUniversity, July 1983.",
    "Lee59. C. Y. Lee. Representation of switching circuits by binary-decision\nprograms. Bell System Technical Journal, 38:985–999, 1959.\nLon83. D. E. Long. Model Checking, Abstraction, and Compositional\nVeriﬁcation. PhD thesis, School of Computer Science, Carnegie Mellon\nUniversity, July 1983.\nMar01. A. Martin. Adequate sets of temporal connectives in CTL. Electronic\nNotes in Theoretical Computer Science 52(1), 2001.\nMcM93. K. L. McMillan. Symbolic Model Checking. Kluwer Academic\nPublishers, 1993.\nMP91. Z. Manna and A. Pnueli. The Temporal Logic of Reactive and\nConcurrent Systems: Speciﬁcation. Springer-Verlag, 1991.\nMP95. Z. Manna and A. Pnueli. Temporal Veriﬁcation of Reactive Systems:\nSafety. Springer-Verlag, 1995.\nMvdH95. J.-J. Ch. Meyer and W. van der Hoek. Epistemic Logic for AI and\nComputer Science, volume 41 of Cambridge Tracts in Theoretical\nComputer Science. Cambridge University Press, 1995.\nPap94. C. H. Papadimitriou. Computational Complexity. Addison Wesley,\n1994.\nPau91. L.C. Paulson. ML for the Working Programmer. Cambridge University\nPress, 1991.\nPnu81. A. Pnueli. A temporal logic of programs. Theoretical Computer\nScience, 13:45–60, 1981.\nPop94. S. Popkorn. First Steps in Modal Logic. Cambridge University Press,\n1994.\nPra65. D. Prawitz. Natural Deduction: A Proof-Theoretical Study. Almqvist &\nWiksell, 1965.\nQS81. J. P. Quielle and J. Sifakis. Speciﬁcation and veriﬁcation of concurrent\nsystems in CESAR. In Proceedings of the Fifth International\nSymposium on Programming, 1981.\nRos97. A. W. Roscoe. The Theory and Practice of Concurrency. Prentice\nHall, 1997.\nSA91. V. Sperschneider and G. Antoniou. Logic, A Foundation for Computer\nScience. Addison Wesley, 1991.\nSch92. U. Schoening. Logik f¨ur Informatiker. B. I. Wissenschaftsverlag, 1992.\nSch94. D. A. Schmidt. The Structure of Typed Programming Languages.\nFoundations of Computing. The MIT Press, 1994.\nSim94. A. K. Simpson. The Proof Theory and Semantics of Intuitionistic",
    "Science. Addison Wesley, 1991.\nSch92. U. Schoening. Logik f¨ur Informatiker. B. I. Wissenschaftsverlag, 1992.\nSch94. D. A. Schmidt. The Structure of Typed Programming Languages.\nFoundations of Computing. The MIT Press, 1994.\nSim94. A. K. Simpson. The Proof Theory and Semantics of Intuitionistic\nModal Logic. PhD thesis, The University of Edinburgh, Department of\nComputer Science, 1994.\nSS90. G. St˚almarck and M. S˚aﬂund. Modeling and verifying systems and\nsoftware in propositional logic. In B. K. Daniels, editor, Safety of\nComputer Control Systems (SAFECOMP’90), pages 31–36. Pergamon\nPress, 1990.\nBibliography\n417\nTay98. R. G. Taylor. Models of Computation and Formal Languages. Oxford\nUniversity Press, 1998.\nTen91. R. D. Tennent. Semantics of Programming Languages. Prentice Hall,\n1991.\nTur91. R. Turner. Constructive Foundations for Functional Languages.\nMcGraw Hill, 1991.\nvD89. D. van Dalen. Logic and Structure. Universitext. Springer-Verlag, 3rd\nedition, 1989.\nVW84. M. Y. Vardi and Pierre Wolper. Automata-theoretic techniques for\nmodal logics of programs. In Proc. 16th ACM Symposium on Theory\nof Computing, pages 446–456, 1984.\nWei98. M. A. Weiss. Data Structures and Problem Solving Using Java.\nAddison-Wesley, 1998.\nIndex\nABP, 203\nacknowledgement channel, 203\nalternating the control bit, 203\nfairness, 203\nmain SMV program, 207\nabsorption laws, 88\nabstract data type\nsets, 226\nabstraction, 175, 229, 247\nand non-determinism, 191\naccessibility relation, 309, 320, 336\nadequate set of connectives\nfor CTL, 216, 222, 231, 397\nfor LTL, 186\nfor propositional logic, 69, 87, 91\nagent, 307, 319, 327\nalgebraic speciﬁcation, 170\nalgorithm\ndeterministic, 59\nalgorithm apply, 373\ncomplexity, 380\ncontrol structure, 374\nrecursive descent, 375\nalgorithm CNF, 59\nalgorithm reduce, 372\ncomplexity, 380\nalgorithm restrict, 377\ncomplexity, 380\nalgorithm reduce\nexample execution, 373\nAlloy\n[], 153\nfun-statement, 155\nwith, 146\nassertion, 144\ncheck directive, 144\nconsistency check, 144",
    "complexity, 380\ncontrol structure, 374\nrecursive descent, 375\nalgorithm CNF, 59\nalgorithm reduce, 372\ncomplexity, 380\nalgorithm restrict, 377\ncomplexity, 380\nalgorithm reduce\nexample execution, 373\nAlloy\n[], 153\nfun-statement, 155\nwith, 146\nassertion, 144\ncheck directive, 144\nconsistency check, 144\nconstrain signature, 150\ncounterexample, 144\ndot operator, 144\nextending signature, 169\nimplication, 147\nlet-expression, 153\nlimitations, 156\nmodiﬁers, 150\nmodule, 146\nopening module, 156\npolymorphism, 156\npostcondition, 151\nprecondition, 151\nreﬂexive, transitive closure, 169\nrun directive, 146\nsignature, 143\ninstance, 144\nsmall scope hypothesis, 143, 168\ntransitive closure, 155\nuniversal quantiﬁcation, 146\nalternating bit protocol, 203\nalways in the future, 318\nand-elimination, 6, 339\nand-introduction, 6, 339\napplication domain, 173, 257\napproach\nmodel-based, 173\nproof-based, 173\napproximants\nµmZ.f, 392\nνmZ.f, 393\narity, 99\narray, 299\nbounds, 136, 287\nﬁeld, 287\nof integers, 287\nsection, 287\nartiﬁcial intelligence, 306\nartiﬁcial language, 93\nassertion checking, 143\nassignment, 191\ninitial, 292\nnon-deterministic, 205, 230\nprogram notation, 261\nstatement, 261\nassociativity laws, 56, 88\nassumption, 4\ndischarging, 28, 329\ntemporary, 11, 121\n418\nIndex\n419\nasynchronous\ncircuit, 358\ninterleaving, 189\natomic formula, 335\nof modal logic, 307\nof predicate logic\nmeaning, 124\naxiom\n5, 331\nT, 341\n4, 327, 330, 339\n5, 330, 339\nT, 327, 330, 339, 343\nfor assignment, 270\nfor equality, 107\ninstance, 271\nBackus Naur form (BNF), 33\nbackwards breadth-ﬁrst search, 225,\n232\nbase case, 41, 42, 86\nbasic modal logic, 307\nBDD, 364\nhi(n), 372\nlo(n), 372\nas boolean function, 365\ncomplement, 367\nconsistent path, 365\nedge, 361\nexamples, 365\nhas an ordering, 367\nlayer of variables, 361\nline\ndashed, 362, 365\nsolid, 362, 365\nordered, 367\nread-1, 405\nreduced, 365\nremoval of duplicate non-terminals, 363\nremoval of duplicate terminals, 363\nremoval of redundant tests, 363\nsatisﬁable, 365\nsubBDD, 363",
    "edge, 361\nexamples, 365\nhas an ordering, 367\nlayer of variables, 361\nline\ndashed, 362, 365\nsolid, 362, 365\nordered, 367\nread-1, 405\nreduced, 365\nremoval of duplicate non-terminals, 363\nremoval of duplicate terminals, 363\nremoval of redundant tests, 363\nsatisﬁable, 365\nsubBDD, 363\nwhich is not a read-1-BDD, 378\nwhich is not an OBDD, 368\nwith duplicated subBDDs, 363\nbelief, 319\nbinary decision diagram, 364\nbinary decision tree, 361\nredundancies in, 362\nbinding priorities, 176, 209\nfor basic modal logic, 307\nfor integer expressions, 260\nfor KT45n, 335\nfor predicate logic, 101\nfor propositional logic, 5\nfor relational mu-calculus, 391\nbit, 133\ncontrol, 203\nleast signiﬁcant, 381\nmost signiﬁcant, 381\none-bit channel, 205\ntwo-bit channel, 205\nblocks of code, 261\nBoole, G., 91, 374\nboolean algebra, 19\nboolean connective, 210, 310\nboolean existential quantiﬁcation, 377\nboolean expression, 260, 272\nboolean forall quantiﬁcation, 379\nboolean formula\nindependent of a variable, 375\nsemantically equivalent, 374\ntruth table, 359\nboolean function\n‘don’t care’ conditions, 403\nas a binary decision tree, 361\nsymbolic representation, 358\nboolean guard, 282\nboolean variable, 358\nbottom-elimination, 21\nbottom-introduction (see ‘not-elimination’), 21\nbox-elimination, 329\nbox-introduction, 329\nbranching-time logic, 174\ncase\noverlap, 61\ncase analysis, 61, 62, 113\ncase-statement, 18, 192\ncharacteristic function, 383\nChurch, A., 133\ncircuit\n2-bit comparator, 408\nasynchronous, 194, 389\nsequential, 359\nsynchronous, 194, 358, 388, 411\ncircular deﬁnition, 217\nClarke, E., 254\nclassical logic, 30, 328\nclient, 259\nclock tick, 190\nCNF, 55\ncode\nspeciﬁcation, 257\nveriﬁcation, 257\ncoding\nAF , 395\nEF , 394\nEG , 395\nEU , 395\nEX , 394\nexamples of symbolic evaluation, 395\nfair EG , 398\nfair EU , 397\nfair EX , 397\nset of fair states, 397\ncommand, 261\natomic, 261\ncompound, 261\ncommon knowledge, 332, 335\nas invariant, 390\n420\nIndex\ncommunicating processes, 256\ncommunication protocol, 194",
    "AF , 395\nEF , 394\nEG , 395\nEU , 395\nEX , 394\nexamples of symbolic evaluation, 395\nfair EG , 398\nfair EU , 397\nfair EX , 397\nset of fair states, 397\ncommand, 261\natomic, 261\ncompound, 261\ncommon knowledge, 332, 335\nas invariant, 390\n420\nIndex\ncommunicating processes, 256\ncommunication protocol, 194\nCompactness Theorem, 137\ncompleteness\nof natural deduction for predicate logic, 96\nof natural deduction for propositional logic,\n54\ncomplexity\nexponential, 229\nof apply, 404\nof brute force minimal-sum section\nalgorithm, 288\nof fairness, 397\nof labelling algorithm, 224, 225\nof labelling EGC, 232\ncomposition\nsequential, 278\nsynchronous, 194\ncompositional semantics, 39\ncompositionality\nin model checking, 230\ncomputability, 131\ncomputation\nintractable, 49\ncomputation path, 180, 212\nfair, 231\ncomputation trace, 285\ncomputation tree logic, 175\ncomputation tree logic, 306, 318\ncomputational behaviour, 306\ncomputer program, 103\nconcatenation, 126, 132\nconclusion, 4, 273\nconcurrency, 257\nconjunct, 56\nconjunction, 4, 291\ninﬁnite, 335\nconnective\nadequate set, 226\nunary, 177, 209\nconsistency, 308, 316\nconsistency checking, 143\nconstant symbol, 157\nconstraints\ninconsistent, 74\nSAT solver, 69\ncontradiction, 20, 118, 319, 325\ncontrol structure, 261, 262\ncontrolling value, 404\ncopy rule, 20, 329\ncore programming language, 259, 287\ncorrespondence theory, 325\ncounter example, 123, 130, 317, 354\ncounter trace, 174\ncritical section, 187\nCTL, 175, 254, 306, 318\nas a subset of CTL*, 218\nexpressive power, 220\nmodalities, 306\nwith boolean combinations of path\nformulas, 220, 251\nCTL connectives\nfair, 397\nCTL formula\nsquare brackets, 210\nCTL*, 254\nDAG, 70\ndag, 364\ndashed box\nﬂavour, 339\ndata structure, 123\nde Morgan laws, 57, 216, 251\nfor modalities, 313\ndeadlock, 178, 184, 215, 219\ndebugging systems, 222\ndebugging systems, 257\ndecision problem, 131\nof validity in predicate logic, 133\ndecision procedure, 55\ndeclarative explanation, 26\ndeclarative sentence, 2, 93\ntruth value, 37\ndefault case, 192",
    "de Morgan laws, 57, 216, 251\nfor modalities, 313\ndeadlock, 178, 184, 215, 219\ndebugging systems, 222\ndebugging systems, 257\ndecision problem, 131\nof validity in predicate logic, 133\ndecision procedure, 55\ndeclarative explanation, 26\ndeclarative sentence, 2, 93\ntruth value, 37\ndefault case, 192\ndeﬁnition\ninductive, 33\ndescription\ninformal, 258\nlanguage, 172, 174\nDijkstra, E., 283\ndirected graph, 136, 178, 364\nacyclic, 69, 364\ncycle, 364\ndisjunction, 4\nof literals, 55, 57\ndistributivity laws\nof box modality, 314\nof F connective, 185\nof propositional logic, 19, 60, 88\ndividend, 302\ndomain assumption, 148\ndouble negation-elimination, 352\ndouble negation-introduction, 352\nelimination rule, 6, 107\nEmerson, E. A., 254\nencoding, 128\nentailment\nin program logics, 278\nenvironment\nand non-determinism, 191\nfor concurrent programs, 173\nfor predicate logic formulas, 127\nequality, 263\nintentional, 107\nprogram notation, 261\nstructural, 153\nsymbol, 107\nequivalence relation, 321, 327, 339\nequivalent formulas\nof basic modal logic, 314\nof CTL, 215–217\nIndex\n421\nof KT4, 327\nof KT45, 327\nof LTL, 184\nof predicate logic, 117\nof propositional logic, 16\nof relational mu-calculus, 410\nexclusive-or, 382, 398\nexistential quantiﬁer, 216\nexistential second-order logic, 139, 156\nexists-elimination, 113\nexists-introduction, 112\nfactorial\nof a natural number, 262\nprogram, 262, 284\nfairness\nnested ﬁxed points, 398\nsymbolic model checking, 396\nfairness constraint, 190, 197\nsimple, 231, 252\nFAIRNESS running, 204\nFibonacci numbers, 85\nﬁeld index, 287\nﬁnite automata, 405\nﬁnite data structure, 222\nﬁrst-order logic, 93\nﬁxed point, 240\ngreatest, 240, 241\nleast, 240, 241\nsemantics for CTL, 217, 238\nﬂow of control, 261\nFloyd, R., 269\nfor-statement, 299\nforall-elimination, 109\nforall-introduction, 110\nformal\npath, 218\nformula\natomic, 175\nheight, 44, 86\nHorn, 65\nill-formed, 177\nimmediate subformula, 223\nof basic modal logic, 314\nof CTL, 208\natomic, 208\nill-formed, 209\nwell-formed, 209\nof LTL\nvalid, 251",
    "Floyd, R., 269\nfor-statement, 299\nforall-elimination, 109\nforall-introduction, 110\nformal\npath, 218\nformula\natomic, 175\nheight, 44, 86\nHorn, 65\nill-formed, 177\nimmediate subformula, 223\nof basic modal logic, 314\nof CTL, 208\natomic, 208\nill-formed, 209\nwell-formed, 209\nof LTL\nvalid, 251\nof predicate logic, 100\nof propositional logic, 33, 50\nwell-formed, 32, 33, 44\nof relational mu-calculus, 390\npositive, 328, 343, 348\nscheme, 312, 317\nK, 315\nin propositional logic, 312\ninstance, 312\nsubformula, 35\nframe, 322\nfree for x in φ, 106, 109\nFrege, G., 170\nfunction\nin predicate logic, 124\nmonotone, 240\na non-example, 240\nnullary, 99\nrecursive, 250\nSAT, 225, 227\ntermination, 253\nSATaf, 228\nSATag, 253\nSATeg, 252\nSATeu, 229\nSATex, 228\nsymbol, 96, 98, 157\nbinary, 98\ntranslate, 250\nfunction pre∀(X), 227\nfunction pre∃(X), 227, 385\nfunction pre∀(X), 385\nfunction SAT\ncorrectness, 240\nfuture\nexcludes the present, 249, 353\nincludes the present, 182, 249, 353\nwhether it includes the present, 318\nG-reachable, 338\nin k steps, 338\nG¨odel, K., 96\nGentzen, G., 91\nGlobal Assembly Cache, 149\ngrammar, 33\nclause, 269\nguided simulation, 155\nHalpern, J., 254\nhigher-order logic, 141\nHoare triple, 264\nHoare, C. A. R., 264, 269\nHodges, W., 170\nHorn clause, 65, 139\nhybrid rule, 343\nif-statement, 280\nimplementation\ncompliant, 143\nimplication, 4\nlogical, 278\nimplies-elimination, 9\nimplies-introduction, 12\nin-order representation, 35\ninconsistency, 259\nindex, 132\ninduction\ncourse-of-values, 43\nhypothesis, 41, 42\nin model checking, 229\nmathematical, 40\ninductive step, 41\n422\nIndex\ninﬁx notation, 125, 210\ninformation\nnegative, 343\ninput parameter, 61\ninteger\nexpression, 260\ninteger label, 372\ninteger multiplication, 381\ninterface between logics, 277\ninterleaving\nformulas with code, 275\ntransitions, 188, 194\nintroduction rules, 6, 107\nintrospection\nnegative, 319, 326\npositive, 319, 326\nintuitionistic logic, 30, 120, 327\ninvariants, 273\ndiscovering, 283\nSAT solver, 69\niterative squaring, 412\nJape, 170",
    "interface between logics, 277\ninterleaving\nformulas with code, 275\ntransitions, 188, 194\nintroduction rules, 6, 107\nintrospection\nnegative, 319, 326\npositive, 319, 326\nintuitionistic logic, 30, 120, 327\ninvariants, 273\ndiscovering, 283\nSAT solver, 69\niterative squaring, 412\nJape, 170\njustiﬁcation, 276, 277, 329\nKnaster-Tarski Theorem, 241\nknowledge\ncommon, 333\ndistributed, 335\ndomain-speciﬁc, 102\nfalse, 321\nformula\npositive, 343\nidealised, 319, 327\nin a multi-agent system, 307\nmodality, 335\nof agent, 307, 319\nKozen, D., 413\nKripke model, 167, 309\nas a counter example, 354\nfor KT45n, 336\nKripke, S., 309, 315\nL¨owenheim-Skolem Theorem, 138\nlabel\nadding, 223\ndeleting, 224\nlabelling\nAF, 223\nEG, 224\nEGC, 231\nEU, 223\nEX, 223\nlabelling algorithm, 222\nlabelling function\ncoding subsets, 383\nfor Kripke model, 309\nfor LTL model, 178\nframe does not have one, 322\nlanguage construct, 299\nlaw of excluded middle, 25\nLEM\ninstance, 328\nlinear-time logic, 174\nlinear-time temporal logic, 175\nLinear-time temporal logic (LTL), 175\nliteral, 55, 62\nliveness, 190, 197\nproperty, 188, 189, 207, 230\nlogic engineering, 307, 316\nlogic programming, 49, 170\nlogical level, 278\nlogical variables\nof Hoare triple, 269\nlook-up table, 127, 140\nup-dated, 127\nLTL, 175\nLTLSPEC, 192, 204\nmachine state, 263\nMcMillan, K., 254\nmemoisation\nof computed OBDDs, 377\nmidcondition, 270, 276\nminimal-sum section, 288\nminimal-sum-section problem, 305\nmodal connective\nCG, 335\nKi, 335\nmodal logic, 306\nK, 326\nKT4, 327\nKT45, 326, 327\nnormal, 326\nS4, 327\nS5, 326\nmodality, 308\ndiamond, 308\npath, 220\nmodel\nfor propositional logic, 37\nof KT45n, 336\nof basic modal logic, 309, 324\nof CTL, 310\npictorial representation, 248, 249\nof intuitionistic propositional logic, 328\nof KT45, 337\nof KT45n, 337\nof LTL\npictorial representation, 179\nof predicate logic, 96, 124\nunder-speciﬁed, 143\nmodel checker, 174\nmodel checking, 141, 173, 175, 256\nalgorithm, 217, 225, 231, 318\ndebugging, 394\nexample, 182, 213\nwith fairness constraints, 230",
    "of KT45, 337\nof KT45n, 337\nof LTL\npictorial representation, 179\nof predicate logic, 96, 124\nunder-speciﬁed, 143\nmodel checker, 174\nmodel checking, 141, 173, 175, 256\nalgorithm, 217, 225, 231, 318\ndebugging, 394\nexample, 182, 213\nwith fairness constraints, 230\nmodel-based veriﬁcation, 172, 174\nmodule, 265\nmodulo 8 counter, 408\nmodus ponens, 9\nIndex\n423\nmodus tollens, 10, 352\nmuddy-children puzzle, 342, 344\nmultiplicity constraint, 152\nMutex model\npictorial representation, 188\nmutual exclusion, 187\nnatural deduction\nextension to predicate logic, 95\nfor modal logic, 332\nfor temporal logic, 174\ninventor, 91\nnatural deduction rules\nfor basic modal logic, 329\nfor KT45n, 340, 355\nfor predicate logic, 107\nfor propositional logic, 27\nnecessity\nlogical, 308, 318\nphysical, 318\nnegation, 4\nnegation-elimination (see\n‘bottom-elimination’), 21\nnegation-introduction, 22\nnested boolean quantiﬁcation, 394\nnetwork\nsynchronous, 174\nno strict sequencing, 188, 189, 215\nnode\ninitial, 364\nleaf, 103\nnon-terminal, 361\nshared, 70\nterminal, 362, 364\nnon-blocking protocol, 188, 189, 215\nnon-determinism, 190\nnon-termination, 262\nnormal form, 53, 55\nconjunctive, 55, 360\ndisjunctive, 360\nnegation, 60\nCTL*, 250\nLTL, 186, 246\nproduct-of-sums, 406\nsum-of-products, 404\nnot-elimination, 21\nnot-introduction, 22\nOBDD, 367\nabsence of redundant variables,\n370\ncanonical form, 369\ncomplementation, 385\ndeﬁnition, 367\nextensions, 381\nfor pre∃(X), 387\nfor pre∀(X), 387\ninteger multiplication, 381\nintersection, 385\nlimitations, 381\nmemoisation, 377\nnested boolean quantiﬁcation, 380\nof a transition relation, 386\nof an even parity function, 370\nof the odd parity function, 400\noptimal ordering, 406\nreduced, 368\nunique representation, 368\nreduced one for logical ‘iﬀ’, 400\nrepresenting subsets, 383\nrunning time of algorithms\nupper bounds, 380\nsensitivity of size, 370\nsynthesis of boolean formula, 380\ntest\nfor implication, 372\nfor satisﬁability, 372\nfor semantic equivalence, 370\nfor validity, 372\nunion, 385",
    "unique representation, 368\nreduced one for logical ‘iﬀ’, 400\nrepresenting subsets, 383\nrunning time of algorithms\nupper bounds, 380\nsensitivity of size, 370\nsynthesis of boolean formula, 380\ntest\nfor implication, 372\nfor satisﬁability, 372\nfor semantic equivalence, 370\nfor validity, 372\nunion, 385\nvariations, 381\nodd parity function, 400\nomniscience\nlogical, 319\nor-elimination, 17\nor-introduction, 16\noverloading\nof ⊨, 129\nof proof rules, 107\nparity function\neven, 369\nas OBDD, 370\nparity OBDD, 381\nparse tree\nfor a predicate logic formula, 103\nof a term, 159\nof a basic modal logic formula, 307\nof a CTL formula, 210\nof propositional logic formula, 34\nroot, 35\nsubtree, 35\nunderspeciﬁed, 312\npartial correctness, 265\npartial order reduction, 229\npattern\ncheckEU (f, g), 398\ncheckEX (f), 397\npattern matching, 6, 111, 279\nplace holder, 94\npossibility, 316\nlogical, 308\npossible world\nsemantics, 315\nPost correspondence problem, 132\npostcondition, 151\nin program logic, 264\nPrawitz, D., 91\nprecondition, 151\nin program logic, 264\nweakest, 276\nof algorithm, 63\n424\nIndex\npredicate, 93\nbinary, 95\nnumber of arguments, 95\nunary, 95\npredicate logic, 93\nconsistent set of formulas, 129\nsatisﬁability, 129\nsemantic entailment, 129\nvalidity, 129\npreﬁx, 126\nnotation, 210\nordering, 126\npremise, 270\nPrior, A., 254\nproblem\ninstance, 132\nreduction, 132\nprocedural interpretation, 26\nprocess\nconcurrent, 187\ninstantiation, 206\nprocessor, 257\nprogram\nbehaviour, 264\nbug, 257\ncode, 276\nconstruct, 261\ncorrectness, 63, 67, 225\nderived, 299\ndiverging, 266\ndocumentation, 257\nenvironment, 258\nﬁnite-state, 358\nfragment, 262\nlogic, 275\nmethodology, 259\nprocedures, 263\nsequential, 256\ntermination, 67, 89, 242, 265\nvariable, 227, 268\nveriﬁcation, 270\nformal, 260\nprogram execution, 316, 319\nprogramming by contract, 296\nEiﬀel, 296\nprogramming language\nimperative, 259\nproof\nbox\nfor →i, 11\nfor forall-introduction, 110\nfor modal logic, 329\nopening, 28\nside by side, 22\nby contradiction, 24\ncalculus, 256, 260",
    "variable, 227, 268\nveriﬁcation, 270\nformal, 260\nprogram execution, 316, 319\nprogramming by contract, 296\nEiﬀel, 296\nprogramming language\nimperative, 259\nproof\nbox\nfor →i, 11\nfor forall-introduction, 110\nfor modal logic, 329\nopening, 28\nside by side, 22\nby contradiction, 24\ncalculus, 256, 260\nconstruction, 269\nconstructive, 120\ndashed box, 329, 340\nfragment, 278\nindirect, 29\nof correctness, 239\nof termination, 266\npartial, 281\npartial correctness, 269, 300\nsearch, 49\nsolid box, 329\nstrategy, 115, 265\nsubproof, 272\ntableaux, 269\ntheory, 93, 122, 174\ntotal correctness, 292\nproof rules, 5\nfor implication, 273\nfor assignment, 269\nfor conjunction, 6\nfor disjunction, 16\nfor double negation, 8\nfor equality, 108\nfor existential quantiﬁcation, 112\nfor if-statements, 272, 280\nmodiﬁed, 281\nfor implication, 12, 277\nfor KT45n, 339\nfor negation, 20\nfor quantiﬁers, 112\nfor sequential composition, 269, 275\nfor universal quantiﬁcation, 109\nfor while-statements, 272, 282, 287\nschema, 111\nsubformula property, 113\nproof tableaux\ncomplete, 291\nproof-based veriﬁcation, 172, 256\nproposition, 2\npropositional logic, 93\nprotocol, 187, 188\nprovability\nundecidability of predicate logic, 136\nquantiﬁer, 310, 313\nequivalences, 185\nin predicate logic, 94\nbinding priorities, 101\nequivalences, 130\nmeaning, 123\nQuielle, J., 254\nreachability, 136, 137\nreasoning\nabout knowledge, 326, 331\nconstructive, 29\nin an arbitrary accessible world, 329\ninformal, 343\nquantitative, 259\nunsound, 280\nrecord\nﬁeld, 193\nrecursion\nmutual, 218\nrecursive call, 280\nreductio ad absurdum, 24, 119\nreduction to absurdity, 24\nreﬂexive, transitive closure, 167\nIndex\n425\nregular language, 405\nrelation\nbinary, 178\nEuclidean, 321, 327\nfunctional, 321\nlinear, 321\nreﬂexive, 140, 320, 324\nas formula, 109\nserial, 320, 353\nsymmetric, 320\nas formula, 109\ntotal, 321\ntransition, 178\ntransitive, 140, 320, 324\nas formula, 109\nrelational mu-calculus\nﬁxed-point operators, 392\nrequirement\ninformal, 258, 263, 288\nrequirements, 142",
    "functional, 321\nlinear, 321\nreﬂexive, 140, 320, 324\nas formula, 109\nserial, 320, 353\nsymmetric, 320\nas formula, 109\ntotal, 321\ntransition, 178\ntransitive, 140, 320, 324\nas formula, 109\nrelational mu-calculus\nﬁxed-point operators, 392\nrequirement\ninformal, 258, 263, 288\nrequirements, 142\nrestriction, 374\nright-associative, 5\nroot of a parse tree, 135\nrule\nderived, 23\nhybrid, 10\nRussell’s paradox, 165\nsafety property, 187, 189, 207\nSAT solver\ncubic, 76\nforcing rules, 71\npermanent marks, 75\ntemporary marks, 74\nsatisfaction\nin a frame, 322\nin a frame for KT45n, 337\nsatisfaction relation\nfor relational mu-calculus, 391\nfor basic modal logic, 310\nfor KT45, 337\nfor LTL, 180\nfor partial correctness, 265\nfor predicate logic, 128\nfor relational mu-calculus, 391\nfor total correctness, 266\nsatisﬁability, 360\n3SAT, 406\ndeciding, 65\nof a propositional logic formula,\n85\nundecidability of predicate logic,\n135\nSCC\nfair, 232\nscheduler\nfair, 197\nscope\nof a dummy variable, 117\nof a variable, 103, 113\nof an assumption, 28, 113, 329\nsearch space, 113, 133\nsecond-order logic, 141\nsemantic entailment\nfor predicate logic, 141\nfor basic modal logic, 313\nfor KT4, 328\nfor normal modal logics, 326\nfor predicate logic, 96\nfor propositional logic, 46\nfor relational mu-calculus, 410\nsemantic equivalence, 39\nsemantics\nof µZ.f, 392\nof νZ.f, 393\nof basic modal logic, 310\nof boolean quantiﬁcation, 392\nof CTL, 211\nof EG, 239\nof equality, 131\nof predicate logic, 122\nof propositional logic, 38\nof relational mu-calculus, 391\nof Until, 181\nsentence\natomic, 4\ncomponents, 93\ndeclarative, 93\nin predicate logic, 128\nsequent, 5\ninvalid, 116\nShannon expansion, 374\nside condition, 108, 110\nSifakis, J., 254\nsmall scope hypothesis, 143\nSMV, 254\nmain program for ABP, 207\nmodule, 193\nreceiver, 205\nsender, 204\nfor channel, 206\ninstantiation, 193\nprocess, 389\nprogram\nexample, 192\nfor Mutex, 195\nspeciﬁcation, 192\nsoftware\nlife-cycle, 142\nmicromodel, 142\nreliability, 149\nrequirements, 142\nspeciﬁcation, 142",
    "small scope hypothesis, 143\nSMV, 254\nmain program for ABP, 207\nmodule, 193\nreceiver, 205\nsender, 204\nfor channel, 206\ninstantiation, 193\nprocess, 389\nprogram\nexample, 192\nfor Mutex, 195\nspeciﬁcation, 192\nsoftware\nlife-cycle, 142\nmicromodel, 142\nreliability, 149\nrequirements, 142\nspeciﬁcation, 142\nvalidation, 142\nsoundness\nof forall-elimination, 109\nof natural deduction\nbasic modal logic, 354\npredicate logic, 96, 122\npropositional logic, 45\nof program logics, 267\nof proof rule for while-statements,\n282\nof the substitution principle, 108\n426\nIndex\nspeciﬁcation\nfor ABP, 207\nformal, 259\ninformal, 259\nlanguage, 172\nof a predicate, 157\npatterns, 254\npractical pattern, 183, 215\ntruth table, 58\nspeciﬁcations, 191\nSpin, 254\nstate\ncritical, 188\nexplosion, 229\nexplosion problem, 254\nfair, 397\nﬁnal, 142\nformula, 218\nglobal, 188\ngraph, 180\ninitial, 142, 189, 222, 247, 252, 264\nnon-critical, 188\nof a system, 269\nof core program, 264\nreachable, 247\nresulting, 263, 299\nspace, 229\nsplitting states, 190\ntransition, 142\ntrying, 188\nstate machine, 142\nstorage\nlocation, 288\nstate, 261\nstore\nof core program, 264\nstring, 247, 307\nbinary, 126, 132\nempty, 126\nstrongly connected component, 225\nstructural equality, 153\nstructural induction, 44, 51\nsubformula, 178\nsubstitution\nin predicate logic, 105\ninstance, 323\ninstance of tautology, 314\nprinciple, 108\nsymbolic model checking, 383\nsyntactic\ndomain, 260, 261\nsyntax\nof basic modal logic, 307\nof boolean expressions, 261\nof boolean formulas, 398\nof CTL, 208\nof CTL*, 218\nof KT45n, 335\nof LTL, 175\nof predicate logic, 100\nof propositional logic, 33\nof relational mu-calculus, 390\nof terms, 99\nsystem\nasynchronous, 254\ninterleaving model, 389\nsimultaneous model, 389\naxiomatic, 91\ncommercially critical, 172, 257\ncomponent, 206\nconcurrent, 173\ndebugging, 174\ndescription, 193\ndesign, 174\ndevelopment, 173\nelevator, 184, 215\nﬁnite-state, 256\nhybrid, 277\ninﬁnite-state, 256\nmission-critical, 172\nmulti-agent, 331\nphysical, 175\nreactive, 173, 257, 358",
    "axiomatic, 91\ncommercially critical, 172, 257\ncomponent, 206\nconcurrent, 173\ndebugging, 174\ndescription, 193\ndesign, 174\ndevelopment, 173\nelevator, 184, 215\nﬁnite-state, 256\nhybrid, 277\ninﬁnite-state, 256\nmission-critical, 172\nmulti-agent, 331\nphysical, 175\nreactive, 173, 257, 358\nsafety-critical, 172, 257\ntransition, 174\nveriﬁcation, 256\ntautology, 50\ntemporal connective\nAF, 212\nAG, 211\nAU, 212\nAX, 211\nEF, 212\nEG, 211\nEU, 212\nEX, 211\ntemporal connectives, 176\ntemporal logic, 174, 306\nterm, 99\ninterpretation, 128\nterm-rewriting system, 170\ntermination\nCollatz 3n + 1, 295\nproof, 266\ntertium non datur, 25\ntheorem, 13\nprover, 106, 136\nproving, 170\ntime\ncontinuous, 174\ndiscrete, 174\ntop\nmarking, 66\ntotal correctness, 265, 266\ntransition relation, 178\nfor SMV programs, 388\ntransition system, 174\nof ABP program, 247\nof Mutex code, 198\nof SMV program, 192\nunwinding, 180, 212, 222\nIndex\n427\ntranslation\nEnglish into predicate logic, 95, 101\ntree\ninﬁnite, 180, 212\ntruth\ndynamic, 174\nmode, 306, 308\nof knowledge, 326\nstatic, 174\nvalue\nfor predicate logic, 127\nfor propositional logic, 3\ntruth table\nfor conjunction, 37\ntruth tables, 38\ntype, 12, 327\nchecking, 12\ntheory, 170\nunary connective, 307\nundecidability\nof provability, 136\nof satisﬁability, 135\nof validity in predicate logic, 133\nuniversal quantiﬁcation, 268\nuniversal quantiﬁer, 216\nuniversal second-order logic, 140, 156\nuniverse of concrete values, 124\nunreachability, 140\nunsound sequent, 164\nUntil\nin natural language, 182\nnegating, 187\nupdated valuation, 391\nvalid sequent\nof modal logic, 330\npartial correctness, 267\ntotal correctness, 267\nvalidity\nin basic modal logic, 314\nin KT45n, 339\nin propositional logic, 85\nundecidability in predicate logic, 133\nvaluation\nfor propositional logic, 37\nin predicate logic, 123\nin relational mu-calculus, 391\nvalue\ninitial, 206, 268, 269\nVardi, M., 254\nvariable, 94, 260\nboolean, 229, 247, 358\nbound, 103\ncapture, 106\ndummy, 110\nfree, 103\nlocal, 263\nlogical, 268, 290\nvariable ordering",
    "valuation\nfor propositional logic, 37\nin predicate logic, 123\nin relational mu-calculus, 391\nvalue\ninitial, 206, 268, 269\nVardi, M., 254\nvariable, 94, 260\nboolean, 229, 247, 358\nbound, 103\ncapture, 106\ndummy, 110\nfree, 103\nlocal, 263\nlogical, 268, 290\nvariable ordering\ncompatible, 368\nlist, 367\nvariant, 293\nveriﬁcation\nfull, 173\nmethod, 172\nof communication protocols, 175\nof hardware, 175\nof software, 175\nof systems, 256\npost-development, 173, 257\npre-development, 173, 257\nprocess, 271\nprogram, 270\nproperty, 173\nproperty-oriented, 256\nsemi-automatic, 256\ntechniques, 172\nweakest precondition, 276\nwhile-statement, 261, 262\nbody, 273, 282, 286\nnon-termination, 292\nwise-men puzzle, 342\nWolper, P., 254\nword\nempty, 126\nworld\naccessible, 309\npossible, 309, 336\nyear-2000 problem, 258"
]