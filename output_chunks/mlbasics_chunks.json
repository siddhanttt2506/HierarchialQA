[
    "Andreas C. Müller & Sarah Guido\nIntroduction to \nMachine \nLearning  \nwith Python  \nA GUIDE FOR DATA SCIENTISTS\nAndreas C. Müller and Sarah Guido\nIntroduction to Machine Learning\nwith Python\nA Guide for Data Scientists\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\nBoston\nFarnham\nSebastopol\nTokyo\nBeijing\n978-1-449-36941-5\n[LSI]\nIntroduction to Machine Learning with Python\nby Andreas C. Müller and Sarah Guido\nCopyright © 2017 Sarah Guido, Andreas Müller. All rights reserved.\nPrinted in the United States of America.\nPublished by O’Reilly Media, Inc., 1005 Gravenstein Highway North, Sebastopol, CA 95472.\nO’Reilly books may be purchased for educational, business, or sales promotional use. Online editions are\nalso available for most titles (http://safaribooksonline.com). For more information, contact our corporate/\ninstitutional sales department: 800-998-9938 or corporate@oreilly.com.\nEditor: Dawn Schanafelt\nProduction Editor: Kristen Brown\nCopyeditor: Rachel Head\nProofreader: Jasmine Kwityn\nIndexer: Judy McConville\nInterior Designer: David Futato\nCover Designer: Karen Montgomery\nIllustrator: Rebecca Demarest\nOctober 2016:\n First Edition\nRevision History for the First Edition\n2016-09-22: First Release\nSee http://oreilly.com/catalog/errata.csp?isbn=9781449369415 for release details.\nThe O’Reilly logo is a registered trademark of O’Reilly Media, Inc. Introduction to Machine Learning with\nPython, the cover image, and related trade dress are trademarks of O’Reilly Media, Inc.\nWhile the publisher and the authors have used good faith efforts to ensure that the information and\ninstructions contained in this work are accurate, the publisher and the authors disclaim all responsibility\nfor errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own",
    "for errors or omissions, including without limitation responsibility for damages resulting from the use of\nor reliance on this work. Use of the information and instructions contained in this work is at your own\nrisk. If any code samples or other technology this work contains or describes is subject to open source\nlicenses or the intellectual property rights of others, it is your responsibility to ensure that your use\nthereof complies with such licenses and/or rights.\nTable of Contents\nPreface. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  vii\n1. Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  1\nWhy Machine Learning?                                                                                                   1\nProblems Machine Learning Can Solve                                                                      2\nKnowing Your Task and Knowing Your Data                                                            4\nWhy Python?                                                                                                                      5\nscikit-learn                                                                                                                          5\nInstalling scikit-learn                                                                                                     6\nEssential Libraries and Tools                                                                                            7\nJupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7",
    "Jupyter Notebook                                                                                                           7\nNumPy                                                                                                                             7\nSciPy                                                                                                                                 8\nmatplotlib                                                                                                                        9\npandas                                                                                                                            10\nmglearn                                                                                                                          11\nPython 2 Versus Python 3                                                                                               12\nVersions Used in this Book                                                                                             12\nA First Application: Classifying Iris Species                                                                13\nMeet the Data                                                                                                                14\nMeasuring Success: Training and Testing Data                                                        17\nFirst Things First: Look at Your Data                                                                        19\nBuilding Your First Model: k-Nearest Neighbors                                                    20\nMaking Predictions                                                                                                      22\nEvaluating the Model                                                                                                   22\nSummary and Outlook                                                                                                   23\niii",
    "Evaluating the Model                                                                                                   22\nSummary and Outlook                                                                                                   23\niii\n2. Supervised Learning. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  25\nClassification and Regression                                                                                         25\nGeneralization, Overfitting, and Underfitting                                                             26\nRelation of Model Complexity to Dataset Size                                                         29\nSupervised Machine Learning Algorithms                                                                  29\nSome Sample Datasets                                                                                                 30\nk-Nearest Neighbors                                                                                                    35\nLinear Models                                                                                                               45\nNaive Bayes Classifiers                                                                                                 68\nDecision Trees                                                                                                               70\nEnsembles of Decision Trees                                                                                      83\nKernelized Support Vector Machines                                                                        92\nNeural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119",
    "Neural Networks (Deep Learning)                                                                          104\nUncertainty Estimates from Classifiers                                                                      119\nThe Decision Function                                                                                              120\nPredicting Probabilities                                                                                             122\nUncertainty in Multiclass Classification                                                                 124\nSummary and Outlook                                                                                                 127\n3. Unsupervised Learning and Preprocessing. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  131\nTypes of Unsupervised Learning                                                                                 131\nChallenges in Unsupervised Learning                                                                        132\nPreprocessing and Scaling                                                                                            132\nDifferent Kinds of Preprocessing                                                                             133\nApplying Data Transformations                                                                               134\nScaling Training and Test Data the Same Way                                                       136\nThe Effect of Preprocessing on Supervised Learning                                           138\nDimensionality Reduction, Feature Extraction, and Manifold Learning              140\nPrincipal Component Analysis (PCA)                                                                    140\nNon-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163",
    "Non-Negative Matrix Factorization (NMF)                                                           156\nManifold Learning with t-SNE                                                                                 163\nClustering                                                                                                                        168\nk-Means Clustering                                                                                                    168\nAgglomerative Clustering                                                                                         182\nDBSCAN                                                                                                                     187\nComparing and Evaluating Clustering Algorithms                                              191\nSummary of Clustering Methods                                                                             207\nSummary and Outlook                                                                                                 208\n4. Representing Data and Engineering Features. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  211\nCategorical Variables                                                                                                     212\nOne-Hot-Encoding (Dummy Variables)                                                                213\niv \n| \nTable of Contents\nNumbers Can Encode Categoricals                                                                         218\nBinning, Discretization, Linear Models, and Trees                                                   220\nInteractions and Polynomials                                                                                      224\nUnivariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236",
    "Univariate Nonlinear Transformations                                                                      232\nAutomatic Feature Selection                                                                                        236\nUnivariate Statistics                                                                                                    236\nModel-Based Feature Selection                                                                                238\nIterative Feature Selection                                                                                         240\nUtilizing Expert Knowledge                                                                                         242\nSummary and Outlook                                                                                                 250\n5. Model Evaluation and Improvement. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  251\nCross-Validation                                                                                                            252\nCross-Validation in scikit-learn                                                                               253\nBenefits of Cross-Validation                                                                                     254\nStratified k-Fold Cross-Validation and Other Strategies                                      254\nGrid Search                                                                                                                     260\nSimple Grid Search                                                                                                    261\nThe Danger of Overfitting the Parameters and the Validation Set                     261\nGrid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275",
    "Grid Search with Cross-Validation                                                                          263\nEvaluation Metrics and Scoring                                                                                   275\nKeep the End Goal in Mind                                                                                      275\nMetrics for Binary Classification                                                                             276\nMetrics for Multiclass Classification                                                                       296\nRegression Metrics                                                                                                     299\nUsing Evaluation Metrics in Model Selection                                                        300\nSummary and Outlook                                                                                                 302\n6. Algorithm Chains and Pipelines. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  305\nParameter Selection with Preprocessing                                                                    306\nBuilding Pipelines                                                                                                          308\nUsing Pipelines in Grid Searches                                                                                 309\nThe General Pipeline Interface                                                                                    312\nConvenient Pipeline Creation with make_pipeline                                              313\nAccessing Step Attributes                                                                                          314\nAccessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317",
    "Accessing Attributes in a Grid-Searched Pipeline                                                 315\nGrid-Searching Preprocessing Steps and Model Parameters                                  317\nGrid-Searching Which Model To Use                                                                        319\nSummary and Outlook                                                                                                 320\n7. Working with Text Data. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  323\nTypes of Data Represented as Strings                                                                         323\nTable of Contents \n| \nv\nExample Application: Sentiment Analysis of Movie Reviews                                 325\nRepresenting Text Data as a Bag of Words                                                                 327\nApplying Bag-of-Words to a Toy Dataset                                                               329\nBag-of-Words for Movie Reviews                                                                            330\nStopwords                                                                                                                       334\nRescaling the Data with tf–idf                                                                                      336\nInvestigating Model Coefficients                                                                                 338\nBag-of-Words with More Than One Word (n-Grams)                                            339\nAdvanced Tokenization, Stemming, and Lemmatization                                        344\nTopic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348",
    "Topic Modeling and Document Clustering                                                               347\nLatent Dirichlet Allocation                                                                                       348\nSummary and Outlook                                                                                                 355\n8. Wrapping Up. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  357\nApproaching a Machine Learning Problem                                                               357\nHumans in the Loop                                                                                                  358\nFrom Prototype to Production                                                                                    359\nTesting Production Systems                                                                                         359\nBuilding Your Own Estimator                                                                                     360\nWhere to Go from Here                                                                                                361\nTheory                                                                                                                          361\nOther Machine Learning Frameworks and Packages                                           362\nRanking, Recommender Systems, and Other Kinds of Learning                       363\nProbabilistic Modeling, Inference, and Probabilistic Programming                  363\nNeural Networks                                                                                                        364\nScaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365",
    "Scaling to Larger Datasets                                                                                         364\nHoning Your Skills                                                                                                     365\nConclusion                                                                                                                      366\nIndex. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .  367\nvi \n| \nTable of Contents\nPreface\nMachine learning is an integral part of many commercial applications and research\nprojects today, in areas ranging from medical diagnosis and treatment to finding your\nfriends on social networks. Many people think that machine learning can only be\napplied by large companies with extensive research teams. In this book, we want to\nshow you how easy it can be to build machine learning solutions yourself, and how to\nbest go about it. With the knowledge in this book, you can build your own system for\nfinding out how people feel on Twitter, or making predictions about global warming.\nThe applications of machine learning are endless and, with the amount of data avail‐\nable today, mostly limited by your imagination.\nWho Should Read This Book\nThis book is for current and aspiring machine learning practitioners looking to\nimplement solutions to real-world machine learning problems. This is an introduc‐\ntory book requiring no previous knowledge of machine learning or artificial intelli‐\ngence (AI). We focus on using Python and the scikit-learn library, and work\nthrough all the steps to create a successful machine learning application. The meth‐\nods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.",
    "ods we introduce will be helpful for scientists and researchers, as well as data scien‐\ntists working on commercial applications. You will get the most out of the book if you\nare somewhat familiar with Python and the NumPy and matplotlib libraries.\nWe made a conscious effort not to focus too much on the math, but rather on the\npractical aspects of using machine learning algorithms. As mathematics (probability\ntheory, in particular) is the foundation upon which machine learning is built, we\nwon’t go into the analysis of the algorithms in great detail. If you are interested in the\nmathematics of machine learning algorithms, we recommend the book The Elements\nof Statistical Learning (Springer) by Trevor Hastie, Robert Tibshirani, and Jerome\nFriedman, which is available for free at the authors’ website. We will also not describe\nhow to write machine learning algorithms from scratch, and will instead focus on\nvii\nhow to use the large array of models already implemented in scikit-learn and other\nlibraries.\nWhy We Wrote This Book\nThere are many books on machine learning and AI. However, all of them are meant\nfor graduate students or PhD students in computer science, and they’re full of\nadvanced mathematics. This is in stark contrast with how machine learning is being\nused, as a commodity tool in research and commercial applications. Today, applying\nmachine learning does not require a PhD. However, there are few resources out there\nthat fully cover all the important aspects of implementing machine learning in prac‐\ntice, without requiring you to take advanced math courses. We hope this book will\nhelp people who want to apply machine learning without reading up on years’ worth\nof calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.",
    "of calculus, linear algebra, and probability theory.\nNavigating This Book\nThis book is organized roughly as follows:\n• Chapter 1 introduces the fundamental concepts of machine learning and its\napplications, and describes the setup we will be using throughout the book.\n• Chapters 2 and 3 describe the actual machine learning algorithms that are most\nwidely used in practice, and discuss their advantages and shortcomings.\n• Chapter 4 discusses the importance of how we represent data that is processed by\nmachine learning, and what aspects of the data to pay attention to.\n• Chapter 5 covers advanced methods for model evaluation and parameter tuning,\nwith a particular focus on cross-validation and grid search.\n• Chapter 6 explains the concept of pipelines for chaining models and encapsulat‐\ning your workflow.\n• Chapter 7 shows how to apply the methods described in earlier chapters to text\ndata, and introduces some text-specific processing techniques.\n• Chapter 8 offers a high-level overview, and includes references to more advanced\ntopics.\nWhile Chapters 2 and 3 provide the actual algorithms, understanding all of these\nalgorithms might not be necessary for a beginner. If you need to build a machine\nlearning system ASAP, we suggest starting with Chapter 1 and the opening sections of\nChapter 2, which introduce all the core concepts. You can then skip to “Summary and\nOutlook” on page 127 in Chapter 2, which includes a list of all the supervised models\nthat we cover. Choose the model that best fits your needs and flip back to read the\nviii \n| \nPreface\nsection devoted to it for details. Then you can use the techniques in Chapter 5 to eval‐\nuate and tune your model.\nOnline Resources\nWhile studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan",
    "While studying this book, definitely refer to the scikit-learn website for more in-\ndepth documentation of the classes and functions, and many examples. There is also\na video course created by Andreas Müller, “Advanced Machine Learning with scikit-\nlearn,” \nthat \nsupplements \nthis \nbook. \nYou \ncan \nfind \nit \nat \nhttp://bit.ly/\nadvanced_machine_learning_scikit-learn.\nConventions Used in This Book\nThe following typographical conventions are used in this book:\nItalic\nIndicates new terms, URLs, email addresses, filenames, and file extensions.\nConstant width\nUsed for program listings, as well as within paragraphs to refer to program ele‐\nments such as variable or function names, databases, data types, environment\nvariables, statements, and keywords. Also used for commands and module and\npackage names.\nConstant width bold\nShows commands or other text that should be typed literally by the user.\nConstant width italic\nShows text that should be replaced with user-supplied values or by values deter‐\nmined by context.\nThis element signifies a tip or suggestion.\nThis element signifies a general note.\nPreface \n| \nix\nThis icon indicates a warning or caution.\nUsing Code Examples\nSupplemental material (code examples, IPython notebooks, etc.) is available for\ndownload at https://github.com/amueller/introduction_to_ml_with_python.\nThis book is here to help you get your job done. In general, if example code is offered\nwith this book, you may use it in your programs and documentation. You do not\nneed to contact us for permission unless you’re reproducing a significant portion of\nthe code. For example, writing a program that uses several chunks of code from this\nbook does not require permission. Selling or distributing a CD-ROM of examples\nfrom O’Reilly books does require permission. Answering a question by citing this\nbook and quoting example code does not require permission. Incorporating a signifi‐\ncant amount of example code from this book into your product’s documentation does",
    "from O’Reilly books does require permission. Answering a question by citing this\nbook and quoting example code does not require permission. Incorporating a signifi‐\ncant amount of example code from this book into your product’s documentation does\nrequire permission.\nWe appreciate, but do not require, attribution. An attribution usually includes the\ntitle, author, publisher, and ISBN. For example: “An Introduction to Machine Learning\nwith Python by Andreas C. Müller and Sarah Guido (O’Reilly). Copyright 2017 Sarah\nGuido and Andreas Müller, 978-1-449-36941-5.”\nIf you feel your use of code examples falls outside fair use or the permission given\nabove, feel free to contact us at permissions@oreilly.com.\nSafari® Books Online\nSafari Books Online is an on-demand digital library that deliv‐\ners expert content in both book and video form from the\nworld’s leading authors in technology and business.\nTechnology professionals, software developers, web designers, and business and crea‐\ntive professionals use Safari Books Online as their primary resource for research,\nproblem solving, learning, and certification training.\nSafari Books Online offers a range of plans and pricing for enterprise, government,\neducation, and individuals.\nMembers have access to thousands of books, training videos, and prepublication\nmanuscripts in one fully searchable database from publishers like O’Reilly Media,\nPrentice Hall Professional, Addison-Wesley Professional, Microsoft Press, Sams, Que,\nx \n| \nPreface\nPeachpit Press, Focal Press, Cisco Press, John Wiley & Sons, Syngress, Morgan Kauf‐\nmann, IBM Redbooks, Packt, Adobe Press, FT Press, Apress, Manning, New Riders,\nMcGraw-Hill, Jones & Bartlett, Course Technology, and hundreds more. For more\ninformation about Safari Books Online, please visit us online.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472",
    "information about Safari Books Online, please visit us online.\nHow to Contact Us\nPlease address comments and questions concerning this book to the publisher:\nO’Reilly Media, Inc.\n1005 Gravenstein Highway North\nSebastopol, CA 95472\n800-998-9938 (in the United States or Canada)\n707-829-0515 (international or local)\n707-829-0104 (fax)\nWe have a web page for this book, where we list errata, examples, and any additional\ninformation. You can access this page at http://bit.ly/intro-machine-learning-python.\nTo comment or ask technical questions about this book, send email to bookques‐\ntions@oreilly.com.\nFor more information about our books, courses, conferences, and news, see our web‐\nsite at http://www.oreilly.com.\nFind us on Facebook: http://facebook.com/oreilly\nFollow us on Twitter: http://twitter.com/oreillymedia\nWatch us on YouTube: http://www.youtube.com/oreillymedia\nAcknowledgments\nFrom Andreas\nWithout the help and support of a large group of people, this book would never have\nexisted.\nI would like to thank the editors, Meghan Blanchette, Brian MacDonald, and in par‐\nticular Dawn Schanafelt, for helping Sarah and me make this book a reality.\nI want to thank my reviewers, Thomas Caswell, Olivier Grisel, Stefan van der Walt,\nand John Myles White, who took the time to read the early versions of this book and\nprovided me with invaluable feedback—in addition to being some of the corner‐\nstones of the scientific open source ecosystem.\nPreface \n| \nxi\nI am forever thankful for the welcoming open source scientific Python community,\nespecially the contributors to scikit-learn. Without the support and help from this\ncommunity, in particular from Gael Varoquaux, Alex Gramfort, and Olivier Grisel, I\nwould never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.",
    "would never have become a core contributor to scikit-learn or learned to under‐\nstand this package as well as I do now. My thanks also go out to all the other contrib‐\nutors who donate their time to improve and maintain this package.\nI’m also thankful for the discussions with many of my colleagues and peers that hel‐\nped me understand the challenges of machine learning and gave me ideas for struc‐\nturing a textbook. Among the people I talk to about machine learning, I specifically\nwant to thank Brian McFee, Daniela Huttenkoppen, Joel Nothman, Gilles Louppe,\nHugo Bowne-Anderson, Sven Kreis, Alice Zheng, Kyunghyun Cho, Pablo Baberas,\nand Dan Cervone.\nMy thanks also go out to Rachel Rakov, who was an eager beta tester and proofreader\nof an early version of this book, and helped me shape it in many ways.\nOn the personal side, I want to thank my parents, Harald and Margot, and my sister,\nMiriam, for their continuing support and encouragement. I also want to thank the\nmany people in my life whose love and friendship gave me the energy and support to\nundertake such a challenging task.\nFrom Sarah\nI would like to thank Meg Blanchette, without whose help and guidance this project\nwould not have even existed. Thanks to Celia La and Brian Carlson for reading in the\nearly days. Thanks to the O’Reilly folks for their endless patience. And finally, thanks\nto DTS, for your everlasting and endless support.\nxii \n| \nPreface\nCHAPTER 1\nIntroduction\nMachine learning is about extracting knowledge from data. It is a research field at the\nintersection of statistics, artificial intelligence, and computer science and is also\nknown as predictive analytics or statistical learning. The application of machine\nlearning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your",
    "learning methods has in recent years become ubiquitous in everyday life. From auto‐\nmatic recommendations of which movies to watch, to what food to order or which\nproducts to buy, to personalized online radio and recognizing your friends in your\nphotos, many modern websites and devices have machine learning algorithms at their\ncore. When you look at a complex website like Facebook, Amazon, or Netflix, it is\nvery likely that every part of the site contains multiple machine learning models.\nOutside of commercial applications, machine learning has had a tremendous influ‐\nence on the way data-driven research is done today. The tools introduced in this book\nhave been applied to diverse scientific problems such as understanding stars, finding\ndistant planets, discovering new particles, analyzing DNA sequences, and providing\npersonalized cancer treatments.\nYour application doesn’t need to be as large-scale or world-changing as these exam‐\nples in order to benefit from machine learning, though. In this chapter, we will\nexplain why machine learning has become so popular and discuss what kinds of\nproblems can be solved using machine learning. Then, we will show you how to build\nyour first machine learning model, introducing important concepts along the way.\nWhy Machine Learning?\nIn the early days of “intelligent” applications, many systems used handcoded rules of\n“if” and “else” decisions to process data or adjust to user input. Think of a spam filter\nwhose job is to move the appropriate incoming email messages to a spam folder. You\ncould make up a blacklist of words that would result in an email being marked as\n1\nspam. This would be an example of using an expert-designed rule system to design an\n“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:",
    "“intelligent” application. Manually crafting decision rules is feasible for some applica‐\ntions, particularly those in which humans have a good understanding of the process\nto model. However, using handcoded rules to make decisions has two major disad‐\nvantages:\n• The logic required to make a decision is specific to a single domain and task.\nChanging the task even slightly might require a rewrite of the whole system.\n• Designing rules requires a deep understanding of how a decision should be made\nby a human expert.\nOne example of where this handcoded approach will fail is in detecting faces in\nimages. Today, every smartphone can detect a face in an image. However, face detec‐\ntion was an unsolved problem until as recently as 2001. The main problem is that the\nway in which pixels (which make up an image in a computer) are “perceived” by the\ncomputer is very different from how humans perceive a face. This difference in repre‐\nsentation makes it basically impossible for a human to come up with a good set of\nrules to describe what constitutes a face in a digital image.\nUsing machine learning, however, simply presenting a program with a large collec‐\ntion of images of faces is enough for an algorithm to determine what characteristics\nare needed to identify a face.\nProblems Machine Learning Can Solve\nThe most successful kinds of machine learning algorithms are those that automate\ndecision-making processes by generalizing from known examples. In this setting,\nwhich is known as supervised learning, the user provides the algorithm with pairs of\ninputs and desired outputs, and the algorithm finds a way to produce the desired out‐\nput given an input. In particular, the algorithm is able to create an output for an input\nit has never seen before without any help from a human. Going back to our example\nof spam classification, using machine learning, the user provides the algorithm with a\nlarge number of emails (which are the input), together with information about",
    "it has never seen before without any help from a human. Going back to our example\nof spam classification, using machine learning, the user provides the algorithm with a\nlarge number of emails (which are the input), together with information about\nwhether any of these emails are spam (which is the desired output). Given a new\nemail, the algorithm will then produce a prediction as to whether the new email is\nspam.\nMachine learning algorithms that learn from input/output pairs are called supervised\nlearning algorithms because a “teacher” provides supervision to the algorithms in the\nform of the desired outputs for each example that they learn from. While creating a\ndataset of inputs and outputs is often a laborious manual process, supervised learning\nalgorithms are well understood and their performance is easy to measure. If your\napplication can be formulated as a supervised learning problem, and you are able to\n2 \n| \nChapter 1: Introduction\ncreate a dataset that includes the desired outcome, machine learning will likely be\nable to solve your problem.\nExamples of supervised machine learning tasks include:\nIdentifying the zip code from handwritten digits on an envelope\nHere the input is a scan of the handwriting, and the desired output is the actual\ndigits in the zip code. To create a dataset for building a machine learning model,\nyou need to collect many envelopes. Then you can read the zip codes yourself\nand store the digits as your desired outcomes.\nDetermining whether a tumor is benign based on a medical image\nHere the input is the image, and the output is whether the tumor is benign. To\ncreate a dataset for building a model, you need a database of medical images. You\nalso need an expert opinion, so a doctor needs to look at all of the images and\ndecide which tumors are benign and which are not. It might even be necessary to\ndo additional diagnosis beyond the content of the image to determine whether\nthe tumor in the image is cancerous or not.",
    "also need an expert opinion, so a doctor needs to look at all of the images and\ndecide which tumors are benign and which are not. It might even be necessary to\ndo additional diagnosis beyond the content of the image to determine whether\nthe tumor in the image is cancerous or not.\nDetecting fraudulent activity in credit card transactions\nHere the input is a record of the credit card transaction, and the output is\nwhether it is likely to be fraudulent or not. Assuming that you are the entity dis‐\ntributing the credit cards, collecting a dataset means storing all transactions and\nrecording if a user reports any transaction as fraudulent.\nAn interesting thing to note about these examples is that although the inputs and out‐\nputs look fairly straightforward, the data collection process for these three tasks is\nvastly different. While reading envelopes is laborious, it is easy and cheap. Obtaining\nmedical imaging and diagnoses, on the other hand, requires not only expensive\nmachinery but also rare and expensive expert knowledge, not to mention the ethical\nconcerns and privacy issues. In the example of detecting credit card fraud, data col‐\nlection is much simpler. Your customers will provide you with the desired output, as\nthey will report fraud. All you have to do to obtain the input/output pairs of fraudu‐\nlent and nonfraudulent activity is wait.\nUnsupervised algorithms are the other type of algorithm that we will cover in this\nbook. In unsupervised learning, only the input data is known, and no known output\ndata is given to the algorithm. While there are many successful applications of these\nmethods, they are usually harder to understand and evaluate.\nExamples of unsupervised learning include:\nIdentifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.",
    "Identifying topics in a set of blog posts\nIf you have a large collection of text data, you might want to summarize it and\nfind prevalent themes in it. You might not know beforehand what these topics\nare, or how many topics there might be. Therefore, there are no known outputs.\nWhy Machine Learning? \n| \n3\nSegmenting customers into groups with similar preferences\nGiven a set of customer records, you might want to identify which customers are\nsimilar, and whether there are groups of customers with similar preferences. For\na shopping site, these might be “parents,” “bookworms,” or “gamers.” Because you\ndon’t know in advance what these groups might be, or even how many there are,\nyou have no known outputs.\nDetecting abnormal access patterns to a website\nTo identify abuse or bugs, it is often helpful to find access patterns that are differ‐\nent from the norm. Each abnormal pattern might be very different, and you\nmight not have any recorded instances of abnormal behavior. Because in this\nexample you only observe traffic, and you don’t know what constitutes normal\nand abnormal behavior, this is an unsupervised problem.\nFor both supervised and unsupervised learning tasks, it is important to have a repre‐\nsentation of your input data that a computer can understand. Often it is helpful to\nthink of your data as a table. Each data point that you want to reason about (each\nemail, each customer, each transaction) is a row, and each property that describes that\ndata point (say, the age of a customer or the amount or location of a transaction) is a\ncolumn. You might describe users by their age, their gender, when they created an\naccount, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,",
    "account, and how often they have bought from your online shop. You might describe\nthe image of a tumor by the grayscale values of each pixel, or maybe by using the size,\nshape, and color of the tumor.\nEach entity or row here is known as a sample (or data point) in machine learning,\nwhile the columns—the properties that describe these entities—are called features.\nLater in this book we will go into more detail on the topic of building a good repre‐\nsentation of your data, which is called feature extraction or feature engineering. You\nshould keep in mind, however, that no machine learning algorithm will be able to\nmake a prediction on data for which it has no information. For example, if the only\nfeature that you have for a patient is their last name, no algorithm will be able to pre‐\ndict their gender. This information is simply not contained in your data. If you add\nanother feature that contains the patient’s first name, you will have much better luck,\nas it is often possible to tell the gender by a person’s first name.\nKnowing Your Task and Knowing Your Data\nQuite possibly the most important part in the machine learning process is under‐\nstanding the data you are working with and how it relates to the task you want to\nsolve. It will not be effective to randomly choose an algorithm and throw your data at\nit. It is necessary to understand what is going on in your dataset before you begin\nbuilding a model. Each algorithm is different in terms of what kind of data and what\nproblem setting it works best for. While you are building a machine learning solution,\nyou should answer, or at least keep in mind, the following questions:\n4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?",
    "4 \n| \nChapter 1: Introduction\n• What question(s) am I trying to answer? Do I think the data collected can answer\nthat question?\n• What is the best way to phrase my question(s) as a machine learning problem?\n• Have I collected enough data to represent the problem I want to solve?\n• What features of the data did I extract, and will these enable the right\npredictions?\n• How will I measure success in my application?\n• How will the machine learning solution interact with other parts of my research\nor business product?\nIn a larger context, the algorithms and methods in machine learning are only one\npart of a greater process to solve a particular problem, and it is good to keep the big\npicture in mind at all times. Many people spend a lot of time building complex\nmachine learning solutions, only to find out they don’t solve the right problem.\nWhen going deep into the technical aspects of machine learning (as we will in this\nbook), it is easy to lose sight of the ultimate goals. While we will not discuss the ques‐\ntions listed here in detail, we still encourage you to keep in mind all the assumptions\nthat you might be making, explicitly or implicitly, when you start building machine\nlearning models.\nWhy Python?\nPython has become the lingua franca for many data science applications. It combines\nthe power of general-purpose programming languages with the ease of use of\ndomain-specific scripting languages like MATLAB or R. Python has libraries for data\nloading, visualization, statistics, natural language processing, image processing, and\nmore. This vast toolbox provides data scientists with a large array of general- and\nspecial-purpose functionality. One of the main advantages of using Python is the abil‐\nity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for",
    "ity to interact directly with the code, using a terminal or other tools like the Jupyter\nNotebook, which we’ll look at shortly. Machine learning and data analysis are funda‐\nmentally iterative processes, in which the data drives the analysis. It is essential for\nthese processes to have tools that allow quick iteration and easy interaction.\nAs a general-purpose programming language, Python also allows for the creation of\ncomplex graphical user interfaces (GUIs) and web services, and for integration into\nexisting systems.\nscikit-learn\nscikit-learn is an open source project, meaning that it is free to use and distribute,\nand anyone can easily obtain the source code to see what is going on behind the\nWhy Python? \n| \n5\nscenes. The scikit-learn project is constantly being developed and improved, and it\nhas a very active user community. It contains a number of state-of-the-art machine\nlearning algorithms, as well as comprehensive documentation about each algorithm.\nscikit-learn is a very popular tool, and the most prominent Python library for\nmachine learning. It is widely used in industry and academia, and a wealth of tutori‐\nals and code snippets are available online. scikit-learn works well with a number of\nother scientific Python tools, which we will discuss later in this chapter.\nWhile reading this, we recommend that you also browse the scikit-learn user guide \nand API documentation for additional details on and many more options for each\nalgorithm. The online documentation is very thorough, and this book will provide\nyou with all the prerequisites in machine learning to understand it in detail.\nInstalling scikit-learn\nscikit-learn depends on two other Python packages, NumPy and SciPy. For plot‐\nting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda",
    "ting and interactive development, you should also install matplotlib, IPython, and\nthe Jupyter Notebook. We recommend using one of the following prepackaged\nPython distributions, which will provide the necessary packages:\nAnaconda\nA Python distribution made for large-scale data processing, predictive analytics,\nand scientific computing. Anaconda comes with NumPy, SciPy, matplotlib,\npandas, IPython, Jupyter Notebook, and scikit-learn. Available on Mac OS,\nWindows, and Linux, it is a very convenient solution and is the one we suggest\nfor people without an existing installation of the scientific Python packages. Ana‐\nconda now also includes the commercial Intel MKL library for free. Using MKL\n(which is done automatically when Anaconda is installed) can give significant\nspeed improvements for many algorithms in scikit-learn.\nEnthought Canopy\nAnother Python distribution for scientific computing. This comes with NumPy,\nSciPy, matplotlib, pandas, and IPython, but the free version does not come with\nscikit-learn. If you are part of an academic, degree-granting institution, you\ncan request an academic license and get free access to the paid subscription ver‐\nsion of Enthought Canopy. Enthought Canopy is available for Python 2.7.x, and\nworks on Mac OS, Windows, and Linux.\nPython(x,y)\nA free Python distribution for scientific computing, specifically for Windows.\nPython(x,y) comes with NumPy, SciPy, matplotlib, pandas, IPython, and\nscikit-learn.\n6 \n| \nChapter 1: Introduction\n1 If you are unfamiliar with NumPy or matplotlib, we recommend reading the first chapter of the SciPy Lec‐\nture Notes.\nIf you already have a Python installation set up, you can use pip to install all of these\npackages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of",
    "packages:\n$ pip install numpy scipy matplotlib ipython scikit-learn pandas\nEssential Libraries and Tools\nUnderstanding what scikit-learn is and how to use it is important, but there are a\nfew other libraries that will enhance your experience. scikit-learn is built on top of\nthe NumPy and SciPy scientific Python libraries. In addition to NumPy and SciPy, we\nwill be using pandas and matplotlib. We will also introduce the Jupyter Notebook,\nwhich is a browser-based interactive programming environment. Briefly, here is what\nyou should know about these tools in order to get the most out of scikit-learn.1\nJupyter Notebook\nThe Jupyter Notebook is an interactive environment for running code in the browser.\nIt is a great tool for exploratory data analysis and is widely used by data scientists.\nWhile the Jupyter Notebook supports many programming languages, we only need\nthe Python support. The Jupyter Notebook makes it easy to incorporate code, text,\nand images, and all of this book was in fact written as a Jupyter Notebook. All of the\ncode examples we include can be downloaded from GitHub.\nNumPy\nNumPy is one of the fundamental packages for scientific computing in Python. It\ncontains functionality for multidimensional arrays, high-level mathematical func‐\ntions such as linear algebra operations and the Fourier transform, and pseudorandom\nnumber generators.\nIn scikit-learn, the NumPy array is the fundamental data structure. scikit-learn\ntakes in data in the form of NumPy arrays. Any data you’re using will have to be con‐\nverted to a NumPy array. The core functionality of NumPy is the ndarray class, a\nmultidimensional (n-dimensional) array. All elements of the array must be of the\nsame type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy",
    "same type. A NumPy array looks like this:\nIn[2]:\nimport numpy as np\nx = np.array([[1, 2, 3], [4, 5, 6]])\nprint(\"x:\\n{}\".format(x))\nEssential Libraries and Tools \n| \n7\nOut[2]:\nx:\n[[1 2 3]\n [4 5 6]]\nWe will be using NumPy a lot in this book, and we will refer to objects of the NumPy\nndarray class as “NumPy arrays” or just “arrays.”\nSciPy\nSciPy is a collection of functions for scientific computing in Python. It provides,\namong other functionality, advanced linear algebra routines, mathematical function\noptimization, signal processing, special mathematical functions, and statistical distri‐\nbutions. scikit-learn draws from SciPy’s collection of functions for implementing\nits algorithms. The most important part of SciPy for us is scipy.sparse: this provides\nsparse matrices, which are another representation that is used for data in scikit-\nlearn. Sparse matrices are used whenever we want to store a 2D array that contains\nmostly zeros:\nIn[3]:\nfrom scipy import sparse\n# Create a 2D NumPy array with a diagonal of ones, and zeros everywhere else\neye = np.eye(4)\nprint(\"NumPy array:\\n{}\".format(eye))\nOut[3]:\nNumPy array:\n[[ 1.  0.  0.  0.]\n [ 0.  1.  0.  0.]\n [ 0.  0.  1.  0.]\n [ 0.  0.  0.  1.]]\nIn[4]:\n# Convert the NumPy array to a SciPy sparse matrix in CSR format\n# Only the nonzero entries are stored\nsparse_matrix = sparse.csr_matrix(eye)\nprint(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\nOut[4]:\nSciPy sparse CSR matrix:\n  (0, 0)    1.0\n  (1, 1)    1.0\n  (2, 2)    1.0\n  (3, 3)    1.0\n8 \n| \nChapter 1: Introduction\nUsually it is not possible to create dense representations of sparse data (as they would\nnot fit into memory), so we need to create sparse representations directly. Here is a\nway to create the same sparse matrix as before, using the COO format:\nIn[5]:\ndata = np.ones(4)\nrow_indices = np.arange(4)\ncol_indices = np.arange(4)\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"COO representation:\\n{}\".format(eye_coo))\nOut[5]:",
    "way to create the same sparse matrix as before, using the COO format:\nIn[5]:\ndata = np.ones(4)\nrow_indices = np.arange(4)\ncol_indices = np.arange(4)\neye_coo = sparse.coo_matrix((data, (row_indices, col_indices)))\nprint(\"COO representation:\\n{}\".format(eye_coo))\nOut[5]:\nCOO representation:\n  (0, 0)    1.0\n  (1, 1)    1.0\n  (2, 2)    1.0\n  (3, 3)    1.0\nMore details on SciPy sparse matrices can be found in the SciPy Lecture Notes.\nmatplotlib\nmatplotlib is the primary scientific plotting library in Python. It provides functions\nfor making publication-quality visualizations such as line charts, histograms, scatter\nplots, and so on. Visualizing your data and different aspects of your analysis can give\nyou important insights, and we will be using matplotlib for all our visualizations.\nWhen working inside the Jupyter Notebook, you can show figures directly in the\nbrowser by using the %matplotlib notebook and %matplotlib inline commands.\nWe recommend using %matplotlib notebook, which provides an interactive envi‐\nronment (though we are using %matplotlib inline to produce this book). For\nexample, this code produces the plot in Figure 1-1:\nIn[6]:\n%matplotlib inline\nimport matplotlib.pyplot as plt\n# Generate a sequence of numbers from -10 to 10 with 100 steps in between\nx = np.linspace(-10, 10, 100)\n# Create a second array using sine\ny = np.sin(x)\n# The plot function makes a line chart of one array against another\nplt.plot(x, y, marker=\"x\")\nEssential Libraries and Tools \n| \n9\nFigure 1-1. Simple line plot of the sine function using matplotlib\npandas\npandas is a Python library for data wrangling and analysis. It is built around a data\nstructure called the DataFrame that is modeled after the R DataFrame. Simply put, a\npandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\nrange of methods to modify and operate on this table; in particular, it allows SQL-like\nqueries and joins of tables. In contrast to NumPy, which requires that all entries in an",
    "pandas DataFrame is a table, similar to an Excel spreadsheet. pandas provides a great\nrange of methods to modify and operate on this table; in particular, it allows SQL-like\nqueries and joins of tables. In contrast to NumPy, which requires that all entries in an\narray be of the same type, pandas allows each column to have a separate type (for\nexample, integers, dates, floating-point numbers, and strings). Another valuable tool\nprovided by pandas is its ability to ingest from a great variety of file formats and data‐\nbases, like SQL, Excel files, and comma-separated values (CSV) files. Going into\ndetail about the functionality of pandas is out of the scope of this book. However,\nPython for Data Analysis by Wes McKinney (O’Reilly, 2012) provides a great guide.\nHere is a small example of creating a DataFrame using a dictionary:\nIn[7]:\nimport pandas as pd\n# create a simple dataset of people\ndata = {'Name': [\"John\", \"Anna\", \"Peter\", \"Linda\"],\n        'Location' : [\"New York\", \"Paris\", \"Berlin\", \"London\"],\n        'Age' : [24, 13, 53, 33]\n       }\ndata_pandas = pd.DataFrame(data)\n# IPython.display allows \"pretty printing\" of dataframes\n# in the Jupyter notebook\ndisplay(data_pandas)\n10 \n| \nChapter 1: Introduction\nThis produces the following output:\nAge\nLocation\nName\n0\n24\nNew York\nJohn\n1\n13\nParis\nAnna\n2\n53\nBerlin\nPeter\n3\n33\nLondon\nLinda\nThere are several possible ways to query this table. For example:\nIn[8]:\n# Select all rows that have an age column greater than 30\ndisplay(data_pandas[data_pandas.Age > 30])\nThis produces the following result:\nAge\nLocation\nName\n2\n53\nBerlin\nPeter\n3\n33\nLondon\nLinda\nmglearn\nThis book comes with accompanying code, which you can find on GitHub. The\naccompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If",
    "accompanying code includes not only all the examples shown in this book, but also\nthe mglearn library. This is a library of utility functions we wrote for this book, so\nthat we don’t clutter up our code listings with details of plotting and data loading. If\nyou’re interested, you can look up all the functions in the repository, but the details of\nthe mglearn module are not really important to the material in this book. If you see a\ncall to mglearn in the code, it is usually a way to make a pretty picture quickly, or to\nget our hands on some interesting data.\nThroughout the book we make ample use of NumPy, matplotlib\nand pandas. All the code will assume the following imports:\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport mglearn\nWe also assume that you will run the code in a Jupyter Notebook\nwith the %matplotlib notebook or %matplotlib inline magic\nenabled to show plots. If you are not using the notebook or these\nmagic commands, you will have to call plt.show to actually show\nany of the figures.\nEssential Libraries and Tools \n| \n11\n2 The six package can be very handy for that.\nPython 2 Versus Python 3\nThere are two major versions of Python that are widely used at the moment: Python 2\n(more precisely, 2.7) and Python 3 (with the latest release being 3.5 at the time of\nwriting). This sometimes leads to some confusion. Python 2 is no longer actively\ndeveloped, but because Python 3 contains major changes, Python 2 code usually does\nnot run on Python 3. If you are new to Python, or are starting a new project from\nscratch, we highly recommend using the latest version of Python 3 without changes.\nIf you have a large codebase that you rely on that is written for Python 2, you are\nexcused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with",
    "excused from upgrading for now. However, you should try to migrate to Python 3 as\nsoon as possible. When writing any new code, it is for the most part quite easy to\nwrite code that runs under Python 2 and Python 3.2 If you don’t have to interface with\nlegacy software, you should definitely use Python 3. All the code in this book is writ‐\nten in a way that works for both versions. However, the exact output might differ\nslightly under Python 2.\nVersions Used in this Book\nWe are using the following versions of the previously mentioned libraries in this\nbook:\nIn[9]:\nimport sys\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\n12 \n| \nChapter 1: Introduction\nOut[9]:\nPython version: 3.5.2 |Anaconda 4.1.1 (64-bit)| (default, Jul  2 2016, 17:53:06)\n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)]\npandas version: 0.18.1\nmatplotlib version: 1.5.1\nNumPy version: 1.11.1\nSciPy version: 0.17.1\nIPython version: 5.1.0\nscikit-learn version: 0.18\nWhile it is not important to match these versions exactly, you should have a version\nof scikit-learn that is as least as recent as the one we used.\nNow that we have everything set up, let’s dive into our first application of machine\nlearning.\nThis book assumes that you have version 0.18 or later of scikit-\nlearn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create",
    "learn. The model_selection module was added in 0.18, and if you\nuse an earlier version of scikit-learn, you will need to adjust the\nimports from this module.\nA First Application: Classifying Iris Species\nIn this section, we will go through a simple machine learning application and create\nour first model. In the process, we will introduce some core concepts and terms.\nLet’s assume that a hobby botanist is interested in distinguishing the species of some\niris flowers that she has found. She has collected some measurements associated with\neach iris: the length and width of the petals and the length and width of the sepals, all\nmeasured in centimeters (see Figure 1-2).\nShe also has the measurements of some irises that have been previously identified by\nan expert botanist as belonging to the species setosa, versicolor, or virginica. For these\nmeasurements, she can be certain of which species each iris belongs to. Let’s assume\nthat these are the only species our hobby botanist will encounter in the wild.\nOur goal is to build a machine learning model that can learn from the measurements\nof these irises whose species is known, so that we can predict the species for a new\niris.\nA First Application: Classifying Iris Species \n| \n13\nFigure 1-2. Parts of the iris flower\nBecause we have measurements for which we know the correct species of iris, this is a\nsupervised learning problem. In this problem, we want to predict one of several\noptions (the species of iris). This is an example of a classification problem. The possi‐\nble outputs (different species of irises) are called classes. Every iris in the dataset\nbelongs to one of three classes, so this problem is a three-class classification problem.\nThe desired output for a single data point (an iris) is the species of this flower. For a\nparticular data point, the species it belongs to is called its label.\nMeet the Data\nThe data we will use for this example is the Iris dataset, a classical dataset in machine",
    "The desired output for a single data point (an iris) is the species of this flower. For a\nparticular data point, the species it belongs to is called its label.\nMeet the Data\nThe data we will use for this example is the Iris dataset, a classical dataset in machine\nlearning and statistics. It is included in scikit-learn in the datasets module. We\ncan load it by calling the load_iris function:\nIn[10]:\nfrom sklearn.datasets import load_iris\niris_dataset = load_iris()\nThe iris object that is returned by load_iris is a Bunch object, which is very similar\nto a dictionary. It contains keys and values:\n14 \n| \nChapter 1: Introduction\nIn[11]:\nprint(\"Keys of iris_dataset: \\n{}\".format(iris_dataset.keys()))\nOut[11]:\nKeys of iris_dataset:\ndict_keys(['target_names', 'feature_names', 'DESCR', 'data', 'target'])\nThe value of the key DESCR is a short description of the dataset. We show the begin‐\nning of the description here (feel free to look up the rest yourself):\nIn[12]:\nprint(iris_dataset['DESCR'][:193] + \"\\n...\")\nOut[12]:\nIris Plants Database\n====================\nNotes\n----\nData Set Characteristics:\n    :Number of Instances: 150 (50 in each of three classes)\n    :Number of Attributes: 4 numeric, predictive att\n...\n----\nThe value of the key target_names is an array of strings, containing the species of\nflower that we want to predict:\nIn[13]:\nprint(\"Target names: {}\".format(iris_dataset['target_names']))\nOut[13]:\nTarget names: ['setosa' 'versicolor' 'virginica']\nThe value of feature_names is a list of strings, giving the description of each feature:\nIn[14]:\nprint(\"Feature names: \\n{}\".format(iris_dataset['feature_names']))\nOut[14]:\nFeature names:\n['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)',\n 'petal width (cm)']\nThe data itself is contained in the target and data fields. data contains the numeric\nmeasurements of sepal length, sepal width, petal length, and petal width in a NumPy\narray:\nA First Application: Classifying Iris Species \n| \n15\nIn[15]:",
    "'petal width (cm)']\nThe data itself is contained in the target and data fields. data contains the numeric\nmeasurements of sepal length, sepal width, petal length, and petal width in a NumPy\narray:\nA First Application: Classifying Iris Species \n| \n15\nIn[15]:\nprint(\"Type of data: {}\".format(type(iris_dataset['data'])))\nOut[15]:\nType of data: <class 'numpy.ndarray'>\nThe rows in the data array correspond to flowers, while the columns represent the\nfour measurements that were taken for each flower:\nIn[16]:\nprint(\"Shape of data: {}\".format(iris_dataset['data'].shape))\nOut[16]:\nShape of data: (150, 4)\nWe see that the array contains measurements for 150 different flowers. Remember\nthat the individual items are called samples in machine learning, and their properties\nare called features. The shape of the data array is the number of samples multiplied by\nthe number of features. This is a convention in scikit-learn, and your data will\nalways be assumed to be in this shape. Here are the feature values for the first five\nsamples:\nIn[17]:\nprint(\"First five columns of data:\\n{}\".format(iris_dataset['data'][:5]))\nOut[17]:\nFirst five columns of data:\n[[ 5.1  3.5  1.4  0.2]\n [ 4.9  3.   1.4  0.2]\n [ 4.7  3.2  1.3  0.2]\n [ 4.6  3.1  1.5  0.2]\n [ 5.   3.6  1.4  0.2]]\nFrom this data, we can see that all of the first five flowers have a petal width of 0.2 cm\nand that the first flower has the longest sepal, at 5.1 cm.\nThe target array contains the species of each of the flowers that were measured, also\nas a NumPy array:\nIn[18]:\nprint(\"Type of target: {}\".format(type(iris_dataset['target'])))\nOut[18]:\nType of target: <class 'numpy.ndarray'>\ntarget is a one-dimensional array, with one entry per flower:\n16 \n| \nChapter 1: Introduction\nIn[19]:\nprint(\"Shape of target: {}\".format(iris_dataset['target'].shape))\nOut[19]:\nShape of target: (150,)\nThe species are encoded as integers from 0 to 2:\nIn[20]:\nprint(\"Target:\\n{}\".format(iris_dataset['target']))\nOut[20]:\nTarget:",
    "16 \n| \nChapter 1: Introduction\nIn[19]:\nprint(\"Shape of target: {}\".format(iris_dataset['target'].shape))\nOut[19]:\nShape of target: (150,)\nThe species are encoded as integers from 0 to 2:\nIn[20]:\nprint(\"Target:\\n{}\".format(iris_dataset['target']))\nOut[20]:\nTarget:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nThe meanings of the numbers are given by the iris['target_names'] array:\n0 means setosa, 1 means versicolor, and 2 means virginica.\nMeasuring Success: Training and Testing Data\nWe want to build a machine learning model from this data that can predict the spe‐\ncies of iris for a new set of measurements. But before we can apply our model to new\nmeasurements, we need to know whether it actually works—that is, whether we\nshould trust its predictions.\nUnfortunately, we cannot use the data we used to build the model to evaluate it. This\nis because our model can always simply remember the whole training set, and will\ntherefore always predict the correct label for any point in the training set. This\n“remembering” does not indicate to us whether our model will generalize well (in\nother words, whether it will also perform well on new data).\nTo assess the model’s performance, we show it new data (data that it hasn’t seen\nbefore) for which we have labels. This is usually done by splitting the labeled data we\nhave collected (here, our 150 flower measurements) into two parts. One part of the\ndata is used to build our machine learning model, and is called the training data or\ntraining set. The rest of the data will be used to assess how well the model works; this\nis called the test data, test set, or hold-out set.\nscikit-learn contains a function that shuffles the dataset and splits it for you: the",
    "training set. The rest of the data will be used to assess how well the model works; this\nis called the test data, test set, or hold-out set.\nscikit-learn contains a function that shuffles the dataset and splits it for you: the\ntrain_test_split function. This function extracts 75% of the rows in the data as the\ntraining set, together with the corresponding labels for this data. The remaining 25%\nof the data, together with the remaining labels, is declared as the test set. Deciding\nA First Application: Classifying Iris Species \n| \n17\nhow much data you want to put into the training and the test set respectively is some‐\nwhat arbitrary, but using a test set containing 25% of the data is a good rule of thumb.\nIn scikit-learn, data is usually denoted with a capital X, while labels are denoted by\na lowercase y. This is inspired by the standard formulation f(x)=y in mathematics,\nwhere x is the input to a function and y is the output. Following more conventions\nfrom mathematics, we use a capital X because the data is a two-dimensional array (a\nmatrix) and a lowercase y because the target is a one-dimensional array (a vector).\nLet’s call train_test_split on our data and assign the outputs using this nomencla‐\nture:\nIn[21]:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nBefore making the split, the train_test_split function shuffles the dataset using a\npseudorandom number generator. If we just took the last 25% of the data as a test set,\nall the data points would have the label 2, as the data points are sorted by the label\n(see the output for iris['target'] shown earlier). Using a test set containing only\none of the three classes would not tell us much about how well our model generalizes,\nso we shuffle our data to make sure the test data contains data from all classes.\nTo make sure that we will get the same output if we run the same function several",
    "one of the three classes would not tell us much about how well our model generalizes,\nso we shuffle our data to make sure the test data contains data from all classes.\nTo make sure that we will get the same output if we run the same function several\ntimes, we provide the pseudorandom number generator with a fixed seed using the\nrandom_state parameter. This will make the outcome deterministic, so this line will\nalways have the same outcome. We will always fix the random_state in this way when\nusing randomized procedures in this book.\nThe output of the train_test_split function is X_train, X_test, y_train, and\ny_test, which are all NumPy arrays. X_train contains 75% of the rows of the dataset,\nand X_test contains the remaining 25%:\nIn[22]:\nprint(\"X_train shape: {}\".format(X_train.shape))\nprint(\"y_train shape: {}\".format(y_train.shape))\nOut[22]:\nX_train shape: (112, 4)\ny_train shape: (112,)\n18 \n| \nChapter 1: Introduction\nIn[23]:\nprint(\"X_test shape: {}\".format(X_test.shape))\nprint(\"y_test shape: {}\".format(y_test.shape))\nOut[23]:\nX_test shape: (38, 4)\ny_test shape: (38,)\nFirst Things First: Look at Your Data\nBefore building a machine learning model it is often a good idea to inspect the data,\nto see if the task is easily solvable without machine learning, or if the desired infor‐\nmation might not be contained in the data.\nAdditionally, inspecting your data is a good way to find abnormalities and peculiari‐\nties. Maybe some of your irises were measured using inches and not centimeters, for\nexample. In the real world, inconsistencies in the data and unexpected measurements\nare very common.\nOne of the best ways to inspect data is to visualize it. One way to do this is by using a\nscatter plot. A scatter plot of the data puts one feature along the x-axis and another\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer\nscreens have only two dimensions, which allows us to plot only two (or maybe three)",
    "scatter plot. A scatter plot of the data puts one feature along the x-axis and another\nalong the y-axis, and draws a dot for each data point. Unfortunately, computer\nscreens have only two dimensions, which allows us to plot only two (or maybe three)\nfeatures at a time. It is difficult to plot datasets with more than three features this way.\nOne way around this problem is to do a pair plot, which looks at all possible pairs of\nfeatures. If you have a small number of features, such as the four we have here, this is\nquite reasonable. You should keep in mind, however, that a pair plot does not show\nthe interaction of all of features at once, so some interesting aspects of the data may\nnot be revealed when visualizing it this way.\nFigure 1-3 is a pair plot of the features in the training set. The data points are colored\naccording to the species the iris belongs to. To create the plot, we first convert the\nNumPy array into a pandas DataFrame. pandas has a function to create pair plots\ncalled scatter_matrix. The diagonal of this matrix is filled with histograms of each\nfeature:\nIn[24]:\n# create dataframe from data in X_train\n# label the columns using the strings in iris_dataset.feature_names\niris_dataframe = pd.DataFrame(X_train, columns=iris_dataset.feature_names)\n# create a scatter matrix from the dataframe, color by y_train\ngrr = pd.scatter_matrix(iris_dataframe, c=y_train, figsize=(15, 15), marker='o',\n                        hist_kwds={'bins': 20}, s=60, alpha=.8, cmap=mglearn.cm3)\nA First Application: Classifying Iris Species \n| \n19\nFigure 1-3. Pair plot of the Iris dataset, colored by class label\nFrom the plots, we can see that the three classes seem to be relatively well separated\nusing the sepal and petal measurements. This means that a machine learning model\nwill likely be able to learn to separate them.\nBuilding Your First Model: k-Nearest Neighbors\nNow we can start building the actual machine learning model. There are many classi‐",
    "using the sepal and petal measurements. This means that a machine learning model\nwill likely be able to learn to separate them.\nBuilding Your First Model: k-Nearest Neighbors\nNow we can start building the actual machine learning model. There are many classi‐\nfication algorithms in scikit-learn that we could use. Here we will use a k-nearest\nneighbors classifier, which is easy to understand. Building this model only consists of\nstoring the training set. To make a prediction for a new data point, the algorithm\nfinds the point in the training set that is closest to the new point. Then it assigns the\nlabel of this training point to the new data point.\n20 \n| \nChapter 1: Introduction\nThe k in k-nearest neighbors signifies that instead of using only the closest neighbor\nto the new data point, we can consider any fixed number k of neighbors in the train‐\ning (for example, the closest three or five neighbors). Then, we can make a prediction\nusing the majority class among these neighbors. We will go into more detail about\nthis in Chapter 2; for now, we’ll use only a single neighbor.\nAll machine learning models in scikit-learn are implemented in their own classes,\nwhich are called Estimator classes. The k-nearest neighbors classification algorithm\nis implemented in the KNeighborsClassifier class in the neighbors module. Before\nwe can use the model, we need to instantiate the class into an object. This is when we\nwill set any parameters of the model. The most important parameter of KNeighbor\nsClassifier is the number of neighbors, which we will set to 1:\nIn[25]:\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=1)\nThe knn object encapsulates the algorithm that will be used to build the model from\nthe training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.",
    "the training data, as well the algorithm to make predictions on new data points. It will\nalso hold the information that the algorithm has extracted from the training data. In\nthe case of KNeighborsClassifier, it will just store the training set.\nTo build the model on the training set, we call the fit method of the knn object,\nwhich takes as arguments the NumPy array X_train containing the training data and\nthe NumPy array y_train of the corresponding training labels:\nIn[26]:\nknn.fit(X_train, y_train)\nOut[26]:\nKNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n           weights='uniform')\nThe fit method returns the knn object itself (and modifies it in place), so we get a\nstring representation of our classifier. The representation shows us which parameters\nwere used in creating the model. Nearly all of them are the default values, but you can\nalso find n_neighbors=1, which is the parameter that we passed. Most models in\nscikit-learn have many parameters, but the majority of them are either speed opti‐\nmizations or for very special use cases. You don’t have to worry about the other\nparameters shown in this representation. Printing a scikit-learn model can yield\nvery long strings, but don’t be intimidated by these. We will cover all the important\nparameters in Chapter 2. In the remainder of this book, we will not show the output\nof fit because it doesn’t contain any new information.\nA First Application: Classifying Iris Species \n| \n21\nMaking Predictions\nWe can now make predictions using this model on new data for which we might not\nknow the correct labels. Imagine we found an iris in the wild with a sepal length of\n5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\nfeatures (4):",
    "5 cm, a sepal width of 2.9 cm, a petal length of 1 cm, and a petal width of 0.2 cm.\nWhat species of iris would this be? We can put this data into a NumPy array, again by\ncalculating the shape—that is, the number of samples (1) multiplied by the number of\nfeatures (4):\nIn[27]:\nX_new = np.array([[5, 2.9, 1, 0.2]])\nprint(\"X_new.shape: {}\".format(X_new.shape))\nOut[27]:\nX_new.shape: (1, 4)\nNote that we made the measurements of this single flower into a row in a two-\ndimensional NumPy array, as scikit-learn always expects two-dimensional arrays\nfor the data.\nTo make a prediction, we call the predict method of the knn object:\nIn[28]:\nprediction = knn.predict(X_new)\nprint(\"Prediction: {}\".format(prediction))\nprint(\"Predicted target name: {}\".format(\n       iris_dataset['target_names'][prediction]))\nOut[28]:\nPrediction: [0]\nPredicted target name: ['setosa']\nOur model predicts that this new iris belongs to the class 0, meaning its species is\nsetosa. But how do we know whether we can trust our model? We don’t know the cor‐\nrect species of this sample, which is the whole point of building the model!\nEvaluating the Model\nThis is where the test set that we created earlier comes in. This data was not used to\nbuild the model, but we do know what the correct species is for each iris in the test\nset.\nTherefore, we can make a prediction for each iris in the test data and compare it\nagainst its label (the known species). We can measure how well the model works by\ncomputing the accuracy, which is the fraction of flowers for which the right species\nwas predicted:\n22 \n| \nChapter 1: Introduction\nIn[29]:\ny_pred = knn.predict(X_test)\nprint(\"Test set predictions:\\n {}\".format(y_pred))\nOut[29]:\nTest set predictions:\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\nIn[30]:\nprint(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\nOut[30]:\nTest set score: 0.97\nWe can also use the score method of the knn object, which will compute the test set",
    "Out[29]:\nTest set predictions:\n [2 1 0 2 0 2 0 1 1 1 2 1 1 1 1 0 1 1 0 0 2 1 0 0 2 0 0 1 1 0 2 1 0 2 2 1 0 2]\nIn[30]:\nprint(\"Test set score: {:.2f}\".format(np.mean(y_pred == y_test)))\nOut[30]:\nTest set score: 0.97\nWe can also use the score method of the knn object, which will compute the test set\naccuracy for us:\nIn[31]:\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[31]:\nTest set score: 0.97\nFor this model, the test set accuracy is about 0.97, which means we made the right\nprediction for 97% of the irises in the test set. Under some mathematical assump‐\ntions, this means that we can expect our model to be correct 97% of the time for new\nirises. For our hobby botanist application, this high level of accuracy means that our\nmodel may be trustworthy enough to use. In later chapters we will discuss how we\ncan improve performance, and what caveats there are in tuning a model.\nSummary and Outlook\nLet’s summarize what we learned in this chapter. We started with a brief introduction\nto machine learning and its applications, then discussed the distinction between\nsupervised and unsupervised learning and gave an overview of the tools we’ll be\nusing in this book. Then, we formulated the task of predicting which species of iris a\nparticular flower belongs to by using physical measurements of the flower. We used a\ndataset of measurements that was annotated by an expert with the correct species to\nbuild our model, making this a supervised learning task. There were three possible\nspecies, setosa, versicolor, or virginica, which made the task a three-class classification\nproblem. The possible species are called classes in the classification problem, and the\nspecies of a single iris is called its label.\nThe Iris dataset consists of two NumPy arrays: one containing the data, which is\nreferred to as X in scikit-learn, and one containing the correct or desired outputs,\nSummary and Outlook \n| \n23",
    "species of a single iris is called its label.\nThe Iris dataset consists of two NumPy arrays: one containing the data, which is\nreferred to as X in scikit-learn, and one containing the correct or desired outputs,\nSummary and Outlook \n| \n23\nwhich is called y. The array X is a two-dimensional array of features, with one row per\ndata point and one column per feature. The array y is a one-dimensional array, which\nhere contains one class label, an integer ranging from 0 to 2, for each of the samples.\nWe split our dataset into a training set, to build our model, and a test set, to evaluate\nhow well our model will generalize to new, previously unseen data.\nWe chose the k-nearest neighbors classification algorithm, which makes predictions\nfor a new data point by considering its closest neighbor(s) in the training set. This is\nimplemented in the KNeighborsClassifier class, which contains the algorithm that\nbuilds the model as well as the algorithm that makes a prediction using the model.\nWe instantiated the class, setting parameters. Then we built the model by calling the\nfit method, passing the training data (X_train) and training outputs (y_train) as\nparameters. We evaluated the model using the score method, which computes the\naccuracy of the model. We applied the score method to the test set data and the test\nset labels and found that our model is about 97% accurate, meaning it is correct 97%\nof the time on the test set.\nThis gave us the confidence to apply the model to new data (in our example, new\nflower measurements) and trust that the model will be correct about 97% of the time.\nHere is a summary of the code needed for the whole training and evaluation\nprocedure:\nIn[32]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[32]:\nTest set score: 0.97",
    "In[32]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris_dataset['data'], iris_dataset['target'], random_state=0)\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[32]:\nTest set score: 0.97\nThis snippet contains the core code for applying any machine learning algorithm\nusing scikit-learn. The fit, predict, and score methods are the common inter‐\nface to supervised models in scikit-learn, and with the concepts introduced in this\nchapter, you can apply these models to many machine learning tasks. In the next\nchapter, we will go into more depth about the different kinds of supervised models in\nscikit-learn and how to apply them successfully.\n24 \n| \nChapter 1: Introduction\nCHAPTER 2\nSupervised Learning\nAs we mentioned earlier, supervised machine learning is one of the most commonly\nused and successful types of machine learning. In this chapter, we will describe super‐\nvised learning in more detail and explain several popular supervised learning algo‐\nrithms. We already saw an application of supervised machine learning in Chapter 1:\nclassifying iris flowers into several species using physical measurements of the\nflowers.\nRemember that supervised learning is used whenever we want to predict a certain\noutcome from a given input, and we have examples of input/output pairs. We build a\nmachine learning model from these input/output pairs, which comprise our training\nset. Our goal is to make accurate predictions for new, never-before-seen data. Super‐\nvised learning often requires human effort to build the training set, but afterward\nautomates and often speeds up an otherwise laborious or infeasible task.\nClassification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined",
    "Classification and Regression\nThere are two major types of supervised machine learning problems, called classifica‐\ntion and regression.\nIn classification, the goal is to predict a class label, which is a choice from a predefined\nlist of possibilities. In Chapter 1 we used the example of classifying irises into one of\nthree possible species. Classification is sometimes separated into binary classification,\nwhich is the special case of distinguishing between exactly two classes, and multiclass\nclassification, which is classification between more than two classes. You can think of\nbinary classification as trying to answer a yes/no question. Classifying emails as\neither spam or not spam is an example of a binary classification problem. In this\nbinary classification task, the yes/no question being asked would be “Is this email\nspam?”\n25\n1 We ask linguists to excuse the simplified presentation of languages as distinct and fixed entities.\nIn binary classification we often speak of one class being the posi‐\ntive class and the other class being the negative class. Here, positive\ndoesn’t represent having benefit or value, but rather what the object\nof the study is. So, when looking for spam, “positive” could mean\nthe spam class. Which of the two classes is called positive is often a\nsubjective matter, and specific to the domain.\nThe iris example, on the other hand, is an example of a multiclass classification prob‐\nlem. Another example is predicting what language a website is in from the text on the\nwebsite. The classes here would be a pre-defined list of possible languages.\nFor regression tasks, the goal is to predict a continuous number, or a floating-point\nnumber in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an",
    "number in programming terms (or real number in mathematical terms). Predicting a\nperson’s annual income from their education, their age, and where they live is an\nexample of a regression task. When predicting income, the predicted value is an\namount, and can be any number in a given range. Another example of a regression\ntask is predicting the yield of a corn farm given attributes such as previous yields,\nweather, and number of employees working on the farm. The yield again can be an\narbitrary number.\nAn easy way to distinguish between classification and regression tasks is to ask\nwhether there is some kind of continuity in the output. If there is continuity between\npossible outcomes, then the problem is a regression problem. Think about predicting\nannual income. There is a clear continuity in the output. Whether a person makes\n$40,000 or $40,001 a year does not make a tangible difference, even though these are\ndifferent amounts of money; if our algorithm predicts $39,999 or $40,001 when it\nshould have predicted $40,000, we don’t mind that much.\nBy contrast, for the task of recognizing the language of a website (which is a classifi‐\ncation problem), there is no matter of degree. A website is in one language, or it is in\nanother. There is no continuity between languages, and there is no language that is\nbetween English and French.1\nGeneralization, Overfitting, and Underfitting\nIn supervised learning, we want to build a model on the training data and then be\nable to make accurate predictions on new, unseen data that has the same characteris‐\ntics as the training set that we used. If a model is able to make accurate predictions on\nunseen data, we say it is able to generalize from the training set to the test set. We\nwant to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a",
    "want to build a model that is able to generalize as accurately as possible.\n26 \n| \nChapter 2: Supervised Learning\n2 In the real world, this is actually a tricky problem. While we know that the other customers haven’t bought a\nboat from us yet, they might have bought one from someone else, or they may still be saving and plan to buy\none in the future.\nUsually we build a model in such a way that it can make accurate predictions on the\ntraining set. If the training and test sets have enough in common, we expect the\nmodel to also be accurate on the test set. However, there are some cases where this\ncan go wrong. For example, if we allow ourselves to build very complex models, we\ncan always be as accurate as we like on the training set.\nLet’s take a look at a made-up example to illustrate this point. Say a novice data scien‐\ntist wants to predict whether a customer will buy a boat, given records of previous\nboat buyers and customers who we know are not interested in buying a boat.2 The\ngoal is to send out promotional emails to people who are likely to actually make a\npurchase, but not bother those customers who won’t be interested.\nSuppose we have the customer records shown in Table 2-1.\nTable 2-1. Example data about customers\nAge\nNumber of \ncars owned\nOwns house\nNumber of children\nMarital status\nOwns a dog\nBought a boat\n66\n1\nyes\n2\nwidowed\nno\nyes\n52\n2\nyes\n3\nmarried\nno\nyes\n22\n0\nno\n0\nmarried\nyes\nno\n25\n1\nno\n1\nsingle\nno\nno\n44\n0\nno\n2\ndivorced\nyes\nno\n39\n1\nyes\n2\nmarried\nyes\nno\n26\n1\nno\n2\nsingle\nno\nno\n40\n3\nyes\n1\nmarried\nyes\nno\n53\n2\nyes\n2\ndivorced\nno\nyes\n64\n2\nyes\n3\ndivorced\nno\nno\n58\n2\nyes\n2\nmarried\nyes\nyes\n33\n1\nno\n1\nsingle\nno\nno\nAfter looking at the data for a while, our novice data scientist comes up with the fol‐\nlowing rule: “If the customer is older than 45, and has less than 3 children or is not\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is",
    "lowing rule: “If the customer is older than 45, and has less than 3 children or is not\ndivorced, then they want to buy a boat.” When asked how well this rule of his does,\nour data scientist answers, “It’s 100 percent accurate!” And indeed, on the data that is\nin the table, the rule is perfectly accurate. There are many possible rules we could\ncome up with that would explain perfectly if someone in this dataset wants to buy a\nboat. No age appears twice in the data, so we could say people who are 66, 52, 53, or\nGeneralization, Overfitting, and Underfitting \n| \n27\n3 And also provably, with the right math.\n58 years old want to buy a boat, while all others don’t. While we can make up many\nrules that work well on this data, remember that we are not interested in making pre‐\ndictions for this dataset; we already know the answers for these customers. We want\nto know if new customers are likely to buy a boat. We therefore want to find a rule that\nwill work well for new customers, and achieving 100 percent accuracy on the training\nset does not help us there. We might not expect that the rule our data scientist came\nup with will work very well on new customers. It seems too complex, and it is sup‐\nported by very little data. For example, the “or is not divorced” part of the rule hinges\non a single customer.\nThe only measure of whether an algorithm will perform well on new data is the eval‐\nuation on the test set. However, intuitively3 we expect simple models to generalize\nbetter to new data. If the rule was “People older than 50 want to buy a boat,” and this\nwould explain the behavior of all the customers, we would trust it more than the rule\ninvolving children and marital status in addition to age. Therefore, we always want to\nfind the simplest model. Building a model that is too complex for the amount of\ninformation we have, as our novice data scientist did, is called overfitting. Overfitting",
    "involving children and marital status in addition to age. Therefore, we always want to\nfind the simplest model. Building a model that is too complex for the amount of\ninformation we have, as our novice data scientist did, is called overfitting. Overfitting\noccurs when you fit a model too closely to the particularities of the training set and\nobtain a model that works well on the training set but is not able to generalize to new\ndata. On the other hand, if your model is too simple—say, “Everybody who owns a\nhouse buys a boat”—then you might not be able to capture all the aspects of and vari‐\nability in the data, and your model will do badly even on the training set. Choosing\ntoo simple a model is called underfitting.\nThe more complex we allow our model to be, the better we will be able to predict on\nthe training data. However, if our model becomes too complex, we start focusing too\nmuch on each individual data point in our training set, and the model will not gener‐\nalize well to new data.\nThere is a sweet spot in between that will yield the best generalization performance.\nThis is the model we want to find.\nThe trade-off between overfitting and underfitting is illustrated in Figure 2-1.\n28 \n| \nChapter 2: Supervised Learning\nFigure 2-1. Trade-off of model complexity against training and test accuracy\nRelation of Model Complexity to Dataset Size\nIt’s important to note that model complexity is intimately tied to the variation of\ninputs contained in your training dataset: the larger variety of data points your data‐\nset contains, the more complex a model you can use without overfitting. Usually, col‐\nlecting more data points will yield more variety, so larger datasets allow building\nmore complex models. However, simply duplicating the same data points or collect‐\ning very similar data will not help.\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\nand all of them complied with the rule “If the customer is older than 45, and has less",
    "more complex models. However, simply duplicating the same data points or collect‐\ning very similar data will not help.\nGoing back to the boat selling example, if we saw 10,000 more rows of customer data,\nand all of them complied with the rule “If the customer is older than 45, and has less\nthan 3 children or is not divorced, then they want to buy a boat,” we would be much\nmore likely to believe this to be a good rule than when it was developed using only\nthe 12 rows in Table 2-1.\nHaving more data and building appropriately more complex models can often work\nwonders for supervised learning tasks. In this book, we will focus on working with\ndatasets of fixed sizes. In the real world, you often have the ability to decide how\nmuch data to collect, which might be more beneficial than tweaking and tuning your\nmodel. Never underestimate the power of more data.\nSupervised Machine Learning Algorithms\nWe will now review the most popular machine learning algorithms and explain how\nthey learn from data and how they make predictions. We will also discuss how the\nconcept of model complexity plays out for each of these models, and provide an over‐\nSupervised Machine Learning Algorithms \n| \n29\n4 Discussing all of them is beyond the scope of the book, and we refer you to the scikit-learn documentation\nfor more details.\nview of how each algorithm builds a model. We will examine the strengths and weak‐\nnesses of each algorithm, and what kind of data they can best be applied to. We will\nalso explain the meaning of the most important parameters and options.4 Many algo‐\nrithms have a classification and a regression variant, and we will describe both.\nIt is not necessary to read through the descriptions of each algorithm in detail, but\nunderstanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of",
    "understanding the models will give you a better feeling for the different ways\nmachine learning algorithms can work. This chapter can also be used as a reference\nguide, and you can come back to it when you are unsure about the workings of any of\nthe algorithms.\nSome Sample Datasets\nWe will use several datasets to illustrate the different algorithms. Some of the datasets\nwill be small and synthetic (meaning made-up), designed to highlight particular\naspects of the algorithms. Other datasets will be large, real-world examples.\nAn example of a synthetic two-class classification dataset is the forge dataset, which\nhas two features. The following code creates a scatter plot (Figure 2-2) visualizing all\nof the data points in this dataset. The plot has the first feature on the x-axis and the\nsecond feature on the y-axis. As is always the case in scatter plots, each data point is\nrepresented as one dot. The color and shape of the dot indicates its class:\nIn[2]:\n# generate dataset\nX, y = mglearn.datasets.make_forge()\n# plot dataset\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.legend([\"Class 0\", \"Class 1\"], loc=4)\nplt.xlabel(\"First feature\")\nplt.ylabel(\"Second feature\")\nprint(\"X.shape: {}\".format(X.shape))\nOut[2]:\nX.shape: (26, 2)\n30 \n| \nChapter 2: Supervised Learning\nFigure 2-2. Scatter plot of the forge dataset\nAs you can see from X.shape, this dataset consists of 26 data points, with 2 features.\nTo illustrate regression algorithms, we will use the synthetic wave dataset. The wave\ndataset has a single input feature and a continuous target variable (or response) that\nwe want to model. The plot created here (Figure 2-3) shows the single feature on the\nx-axis and the regression target (the output) on the y-axis:\nIn[3]:\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")\nSupervised Machine Learning Algorithms \n| \n31",
    "x-axis and the regression target (the output) on the y-axis:\nIn[3]:\nX, y = mglearn.datasets.make_wave(n_samples=40)\nplt.plot(X, y, 'o')\nplt.ylim(-3, 3)\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Target\")\nSupervised Machine Learning Algorithms \n| \n31\nFigure 2-3. Plot of the wave dataset, with the x-axis showing the feature and the y-axis\nshowing the regression target\nWe are using these very simple, low-dimensional datasets because we can easily visu‐\nalize them—a printed page has two dimensions, so data with more than two features\nis hard to show. Any intuition derived from datasets with few features (also called\nlow-dimensional datasets) might not hold in datasets with many features (high-\ndimensional datasets). As long as you keep that in mind, inspecting algorithms on\nlow-dimensional datasets can be very instructive.\nWe will complement these small synthetic datasets with two real-world datasets that\nare included in scikit-learn. One is the Wisconsin Breast Cancer dataset (cancer,\nfor short), which records clinical measurements of breast cancer tumors. Each tumor\nis labeled as “benign” (for harmless tumors) or “malignant” (for cancerous tumors),\nand the task is to learn to predict whether a tumor is malignant based on the meas‐\nurements of the tissue.\nThe data can be loaded using the load_breast_cancer function from scikit-learn:\nIn[4]:\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nprint(\"cancer.keys(): \\n{}\".format(cancer.keys()))\n32 \n| \nChapter 2: Supervised Learning\nOut[4]:\ncancer.keys():\ndict_keys(['feature_names', 'data', 'DESCR', 'target', 'target_names'])\nDatasets that are included in scikit-learn are usually stored as\nBunch objects, which contain some information about the dataset\nas well as the actual data. All you need to know about Bunch objects\nis that they behave like dictionaries, with the added benefit that you\ncan access values using a dot (as in bunch.key instead of\nbunch['key']).",
    "Bunch objects, which contain some information about the dataset\nas well as the actual data. All you need to know about Bunch objects\nis that they behave like dictionaries, with the added benefit that you\ncan access values using a dot (as in bunch.key instead of\nbunch['key']).\nThe dataset consists of 569 data points, with 30 features each:\nIn[5]:\nprint(\"Shape of cancer data: {}\".format(cancer.data.shape))\nOut[5]:\nShape of cancer data: (569, 30)\nOf these 569 data points, 212 are labeled as malignant and 357 as benign:\nIn[6]:\nprint(\"Sample counts per class:\\n{}\".format(\n      {n: v for n, v in zip(cancer.target_names, np.bincount(cancer.target))}))\nOut[6]:\nSample counts per class:\n{'benign': 357, 'malignant': 212}\nTo get a description of the semantic meaning of each feature, we can have a look at\nthe feature_names attribute:\nIn[7]:\nprint(\"Feature names:\\n{}\".format(cancer.feature_names))\nOut[7]:\nFeature names:\n['mean radius' 'mean texture' 'mean perimeter' 'mean area'\n 'mean smoothness' 'mean compactness' 'mean concavity'\n 'mean concave points' 'mean symmetry' 'mean fractal dimension'\n 'radius error' 'texture error' 'perimeter error' 'area error'\n 'smoothness error' 'compactness error' 'concavity error'\n 'concave points error' 'symmetry error' 'fractal dimension error'\n 'worst radius' 'worst texture' 'worst perimeter' 'worst area'\n 'worst smoothness' 'worst compactness' 'worst concavity'\n 'worst concave points' 'worst symmetry' 'worst fractal dimension']\nSupervised Machine Learning Algorithms \n| \n33\n5 This is called the binomial coefficient, which is the number of combinations of k elements that can be selected\nfrom a set of n elements. Often this is written as n\nk  and spoken as “n choose k”—in this case, “13 choose 2.”\nYou can find out more about the data by reading cancer.DESCR if you are interested.\nWe will also be using a real-world regression dataset, the Boston Housing dataset.",
    "from a set of n elements. Often this is written as n\nk  and spoken as “n choose k”—in this case, “13 choose 2.”\nYou can find out more about the data by reading cancer.DESCR if you are interested.\nWe will also be using a real-world regression dataset, the Boston Housing dataset.\nThe task associated with this dataset is to predict the median value of homes in sev‐\neral Boston neighborhoods in the 1970s, using information such as crime rate, prox‐\nimity to the Charles River, highway accessibility, and so on. The dataset contains 506\ndata points, described by 13 features:\nIn[8]:\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nprint(\"Data shape: {}\".format(boston.data.shape))\nOut[8]:\nData shape: (506, 13)\nAgain, you can get more information about the dataset by reading the DESCR attribute\nof boston. For our purposes here, we will actually expand this dataset by not only\nconsidering these 13 measurements as input features, but also looking at all products\n(also called interactions) between features. In other words, we will not only consider\ncrime rate and highway accessibility as features, but also the product of crime rate\nand highway accessibility. Including derived feature like these is called feature engi‐\nneering, which we will discuss in more detail in Chapter 4. This derived dataset can be\nloaded using the load_extended_boston function:\nIn[9]:\nX, y = mglearn.datasets.load_extended_boston()\nprint(\"X.shape: {}\".format(X.shape))\nOut[9]:\nX.shape: (506, 104)\nThe resulting 104 features are the 13 original features together with the 91 possible\ncombinations of two features within those 13.5\nWe will use these datasets to explain and illustrate the properties of the different\nmachine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors",
    "machine learning algorithms. But for now, let’s get to the algorithms themselves.\nFirst, we will revisit the k-nearest neighbors (k-NN) algorithm that we saw in the pre‐\nvious chapter.\n34 \n| \nChapter 2: Supervised Learning\nk-Nearest Neighbors\nThe k-NN algorithm is arguably the simplest machine learning algorithm. Building\nthe model consists only of storing the training dataset. To make a prediction for a\nnew data point, the algorithm finds the closest data points in the training dataset—its\n“nearest neighbors.”\nk-Neighbors classification\nIn its simplest version, the k-NN algorithm only considers exactly one nearest neigh‐\nbor, which is the closest training data point to the point we want to make a prediction\nfor. The prediction is then simply the known output for this training point. Figure 2-4\nillustrates this for the case of classification on the forge dataset:\nIn[10]:\nmglearn.plots.plot_knn_classification(n_neighbors=1)\nFigure 2-4. Predictions made by the one-nearest-neighbor model on the forge dataset\nHere, we added three new data points, shown as stars. For each of them, we marked\nthe closest point in the training set. The prediction of the one-nearest-neighbor algo‐\nrithm is the label of that point (shown by the color of the cross).\nSupervised Machine Learning Algorithms \n| \n35\nInstead of considering only the closest neighbor, we can also consider an arbitrary\nnumber, k, of neighbors. This is where the name of the k-nearest neighbors algorithm\ncomes from. When considering more than one neighbor, we use voting to assign a\nlabel. This means that for each test point, we count how many neighbors belong to\nclass 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)",
    "class 0 and how many neighbors belong to class 1. We then assign the class that is\nmore frequent: in other words, the majority class among the k-nearest neighbors. The\nfollowing example (Figure 2-5) uses the three closest neighbors:\nIn[11]:\nmglearn.plots.plot_knn_classification(n_neighbors=3)\nFigure 2-5. Predictions made by the three-nearest-neighbors model on the forge dataset\nAgain, the prediction is shown as the color of the cross. You can see that the predic‐\ntion for the new data point at the top left is not the same as the prediction when we\nused only one neighbor.\nWhile this illustration is for a binary classification problem, this method can be\napplied to datasets with any number of classes. For more classes, we count how many\nneighbors belong to each class and again predict the most common class.\nNow let’s look at how we can apply the k-nearest neighbors algorithm using scikit-\nlearn. First, we split our data into a training and a test set so we can evaluate general‐\nization performance, as discussed in Chapter 1:\n36 \n| \nChapter 2: Supervised Learning\nIn[12]:\nfrom sklearn.model_selection import train_test_split\nX, y = mglearn.datasets.make_forge()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nNext, we import and instantiate the class. This is when we can set parameters, like the\nnumber of neighbors to use. Here, we set it to 3:\nIn[13]:\nfrom sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors=3)\nNow, we fit the classifier using the training set. For KNeighborsClassifier this\nmeans storing the dataset, so we can compute neighbors during prediction:\nIn[14]:\nclf.fit(X_train, y_train)\nTo make predictions on the test data, we call the predict method. For each data point\nin the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]",
    "in the test set, this computes its nearest neighbors in the training set and finds the\nmost common class among these:\nIn[15]:\nprint(\"Test set predictions: {}\".format(clf.predict(X_test)))\nOut[15]:\nTest set predictions: [1 0 1 0 1 0 0]\nTo evaluate how well our model generalizes, we can call the score method with the\ntest data together with the test labels:\nIn[16]:\nprint(\"Test set accuracy: {:.2f}\".format(clf.score(X_test, y_test)))\nOut[16]:\nTest set accuracy: 0.86\nWe see that our model is about 86% accurate, meaning the model predicted the class\ncorrectly for 86% of the samples in the test dataset.\nAnalyzing KNeighborsClassifier\nFor two-dimensional datasets, we can also illustrate the prediction for all possible test\npoints in the xy-plane. We color the plane according to the class that would be\nassigned to a point in this region. This lets us view the decision boundary, which is the\ndivide between where the algorithm assigns class 0 versus where it assigns class 1.\nSupervised Machine Learning Algorithms \n| \n37\nThe following code produces the visualizations of the decision boundaries for one,\nthree, and nine neighbors shown in Figure 2-6:\nIn[17]:\nfig, axes = plt.subplots(1, 3, figsize=(10, 3))\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # the fit method returns the object self, so we can instantiate\n    # and fit in one line\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors).fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=True, eps=0.5, ax=ax, alpha=.4)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{} neighbor(s)\".format(n_neighbors))\n    ax.set_xlabel(\"feature 0\")\n    ax.set_ylabel(\"feature 1\")\naxes[0].legend(loc=3)\nFigure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\nues of n_neighbors\nAs you can see on the left in the figure, using a single neighbor results in a decision\nboundary that follows the training data closely. Considering more and more neigh‐",
    "Figure 2-6. Decision boundaries created by the nearest neighbors model for different val‐\nues of n_neighbors\nAs you can see on the left in the figure, using a single neighbor results in a decision\nboundary that follows the training data closely. Considering more and more neigh‐\nbors leads to a smoother decision boundary. A smoother boundary corresponds to a\nsimpler model. In other words, using few neighbors corresponds to high model com‐\nplexity (as shown on the right side of Figure 2-1), and using many neighbors corre‐\nsponds to low model complexity (as shown on the left side of Figure 2-1). If you\nconsider the extreme case where the number of neighbors is the number of all data\npoints in the training set, each test point would have exactly the same neighbors (all\ntraining points) and all predictions would be the same: the class that is most frequent\nin the training set.\nLet’s investigate whether we can confirm the connection between model complexity\nand generalization that we discussed earlier. We will do this on the real-world Breast\nCancer dataset. We begin by splitting the dataset into a training and a test set. Then\n38 \n| \nChapter 2: Supervised Learning\nwe evaluate training and test set performance with different numbers of neighbors.\nThe results are shown in Figure 2-7:\nIn[18]:\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=66)\ntraining_accuracy = []\ntest_accuracy = []\n# try n_neighbors from 1 to 10\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    # build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))",
    "# build the model\n    clf = KNeighborsClassifier(n_neighbors=n_neighbors)\n    clf.fit(X_train, y_train)\n    # record training set accuracy\n    training_accuracy.append(clf.score(X_train, y_train))\n    # record generalization accuracy\n    test_accuracy.append(clf.score(X_test, y_test))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nThe plot shows the training and test set accuracy on the y-axis against the setting of\nn_neighbors on the x-axis. While real-world plots are rarely very smooth, we can still\nrecognize some of the characteristics of overfitting and underfitting (note that\nbecause considering fewer neighbors corresponds to a more complex model, the plot\nis horizontally flipped relative to the illustration in Figure 2-1). Considering a single\nnearest neighbor, the prediction on the training set is perfect. But when more neigh‐\nbors are considered, the model becomes simpler and the training accuracy drops. The\ntest set accuracy for using a single neighbor is lower than when using more neigh‐\nbors, indicating that using the single nearest neighbor leads to a model that is too\ncomplex. On the other hand, when considering 10 neighbors, the model is too simple\nand performance is even worse. The best performance is somewhere in the middle,\nusing around six neighbors. Still, it is good to keep the scale of the plot in mind. The\nworst performance is around 88% accuracy, which might still be acceptable.\nSupervised Machine Learning Algorithms \n| \n39\nFigure 2-7. Comparison of training and test accuracy as a function of n_neighbors\nk-neighbors regression\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\nadded three test data points as green stars on the x-axis. The prediction using a single",
    "k-neighbors regression\nThere is also a regression variant of the k-nearest neighbors algorithm. Again, let’s\nstart by using the single nearest neighbor, this time using the wave dataset. We’ve\nadded three test data points as green stars on the x-axis. The prediction using a single\nneighbor is just the target value of the nearest neighbor. These are shown as blue stars\nin Figure 2-8:\nIn[19]:\nmglearn.plots.plot_knn_regression(n_neighbors=1)\n40 \n| \nChapter 2: Supervised Learning\nFigure 2-8. Predictions made by one-nearest-neighbor regression on the wave dataset\nAgain, we can use more than the single closest neighbor for regression. When using\nmultiple nearest neighbors, the prediction is the average, or mean, of the relevant\nneighbors (Figure 2-9):\nIn[20]:\nmglearn.plots.plot_knn_regression(n_neighbors=3)\nSupervised Machine Learning Algorithms \n| \n41\nFigure 2-9. Predictions made by three-nearest-neighbors regression on the wave dataset\nThe k-nearest neighbors algorithm for regression is implemented in the KNeighbors\nRegressor class in scikit-learn. It’s used similarly to KNeighborsClassifier:\nIn[21]:\nfrom sklearn.neighbors import KNeighborsRegressor\nX, y = mglearn.datasets.make_wave(n_samples=40)\n# split the wave dataset into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate the model and set the number of neighbors to consider to 3\nreg = KNeighborsRegressor(n_neighbors=3)\n# fit the model using the training data and training targets\nreg.fit(X_train, y_train)\nNow we can make predictions on the test set:\nIn[22]:\nprint(\"Test set predictions:\\n{}\".format(reg.predict(X_test)))\n42 \n| \nChapter 2: Supervised Learning\nOut[22]:\nTest set predictions:\n[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\nWe can also evaluate the model using the score method, which for regressors returns\nthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐",
    "Out[22]:\nTest set predictions:\n[-0.054  0.357  1.137 -1.894 -1.139 -1.631  0.357  0.912 -0.447 -1.139]\nWe can also evaluate the model using the score method, which for regressors returns\nthe R2 score. The R2 score, also known as the coefficient of determination, is a meas‐\nure of goodness of a prediction for a regression model, and yields a score between 0\nand 1. A value of 1 corresponds to a perfect prediction, and a value of 0 corresponds\nto a constant model that just predicts the mean of the training set responses, y_train:\nIn[23]:\nprint(\"Test set R^2: {:.2f}\".format(reg.score(X_test, y_test)))\nOut[23]:\nTest set R^2: 0.83\nHere, the score is 0.83, which indicates a relatively good model fit.\nAnalyzing KNeighborsRegressor\nFor our one-dimensional dataset, we can see what the predictions look like for all\npossible feature values (Figure 2-10). To do this, we create a test dataset consisting of\nmany points on the line:\nIn[24]:\nfig, axes = plt.subplots(1, 3, figsize=(15, 4))\n# create 1,000 data points, evenly spaced between -3 and 3\nline = np.linspace(-3, 3, 1000).reshape(-1, 1)\nfor n_neighbors, ax in zip([1, 3, 9], axes):\n    # make predictions using 1, 3, or 9 neighbors\n    reg = KNeighborsRegressor(n_neighbors=n_neighbors)\n    reg.fit(X_train, y_train)\n    ax.plot(line, reg.predict(line))\n    ax.plot(X_train, y_train, '^', c=mglearn.cm2(0), markersize=8)\n    ax.plot(X_test, y_test, 'v', c=mglearn.cm2(1), markersize=8)\n    ax.set_title(\n        \"{} neighbor(s)\\n train score: {:.2f} test score: {:.2f}\".format(\n            n_neighbors, reg.score(X_train, y_train),\n            reg.score(X_test, y_test)))\n    ax.set_xlabel(\"Feature\")\n    ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors",
    "ax.set_ylabel(\"Target\")\naxes[0].legend([\"Model predictions\", \"Training data/target\",\n                \"Test data/target\"], loc=\"best\")\nSupervised Machine Learning Algorithms \n| \n43\nFigure 2-10. Comparing predictions made by nearest neighbors regression for different\nvalues of n_neighbors\nAs we can see from the plot, using only a single neighbor, each point in the training\nset has an obvious influence on the predictions, and the predicted values go through\nall of the data points. This leads to a very unsteady prediction. Considering more\nneighbors leads to smoother predictions, but these do not fit the training data as well.\nStrengths, weaknesses, and parameters\nIn principle, there are two important parameters to the KNeighbors classifier: the\nnumber of neighbors and how you measure distance between data points. In practice,\nusing a small number of neighbors like three or five often works well, but you should\ncertainly adjust this parameter. Choosing the right distance measure is somewhat\nbeyond the scope of this book. By default, Euclidean distance is used, which works\nwell in many settings.\nOne of the strengths of k-NN is that the model is very easy to understand, and often\ngives reasonable performance without a lot of adjustments. Using this algorithm is a\ngood baseline method to try before considering more advanced techniques. Building\nthe nearest neighbors model is usually very fast, but when your training set is very\nlarge (either in number of features or in number of samples) prediction can be slow.\nWhen using the k-NN algorithm, it’s important to preprocess your data (see Chap‐\nter 3). This approach often does not perform well on datasets with many features\n(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used",
    "(hundreds or more), and it does particularly badly with datasets where most features\nare 0 most of the time (so-called sparse datasets).\nSo, while the nearest k-neighbors algorithm is easy to understand, it is not often used\nin practice, due to prediction being slow and its inability to handle many features.\nThe method we discuss next has neither of these drawbacks.\n44 \n| \nChapter 2: Supervised Learning\nLinear Models\nLinear models are a class of models that are widely used in practice and have been\nstudied extensively in the last few decades, with roots going back over a hundred\nyears. Linear models make a prediction using a linear function of the input features,\nwhich we will explain shortly.\nLinear models for regression\nFor regression, the general prediction formula for a linear model looks as follows:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nHere, x[0] to x[p] denotes the features (in this example, the number of features is p)\nof a single data point, w and b are parameters of the model that are learned, and ŷ is\nthe prediction the model makes. For a dataset with a single feature, this is:\nŷ = w[0] * x[0] + b\nwhich you might remember from high school mathematics as the equation for a line.\nHere, w[0] is the slope and b is the y-axis offset. For more features, w contains the\nslopes along each feature axis. Alternatively, you can think of the predicted response\nas being a weighted sum of the input features, with weights (which can be negative)\ngiven by the entries of w.\nTrying to learn the parameters w[0] and b on our one-dimensional wave dataset\nmight lead to the following line (see Figure 2-11):\nIn[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.",
    "In[25]:\nmglearn.plots.plot_linear_regression_wave()\nOut[25]:\nw[0]: 0.393906  b: -0.031804\nSupervised Machine Learning Algorithms \n| \n45\nFigure 2-11. Predictions of a linear model on the wave dataset\nWe added a coordinate cross into the plot to make it easier to understand the line.\nLooking at w[0] we see that the slope should be around 0.4, which we can confirm\nvisually in the plot. The intercept is where the prediction line should cross the y-axis:\nthis is slightly below zero, which you can also confirm in the image.\nLinear models for regression can be characterized as regression models for which the\nprediction is a line for a single feature, a plane when using two features, or a hyper‐\nplane in higher dimensions (that is, when using more features).\nIf you compare the predictions made by the straight line with those made by the\nKNeighborsRegressor in Figure 2-10, using a straight line to make predictions seems\nvery restrictive. It looks like all the fine details of the data are lost. In a sense, this is\ntrue. It is a strong (and somewhat unrealistic) assumption that our target y is a linear\n46 \n| \nChapter 2: Supervised Learning\n6 This is easy to see if you know some linear algebra.\ncombination of the features. But looking at one-dimensional data gives a somewhat\nskewed perspective. For datasets with many features, linear models can be very pow‐\nerful. In particular, if you have more features than training data points, any target y\ncan be perfectly modeled (on the training set) as a linear function.6\nThere are many different linear models for regression. The difference between these\nmodels lies in how the model parameters w and b are learned from the training data,\nand how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐",
    "and how model complexity can be controlled. We will now take a look at the most\npopular linear models for regression.\nLinear regression (aka ordinary least squares)\nLinear regression, or ordinary least squares (OLS), is the simplest and most classic lin‐\near method for regression. Linear regression finds the parameters w and b that mini‐\nmize the mean squared error between predictions and the true regression targets, y,\non the training set. The mean squared error is the sum of the squared differences\nbetween the predictions and the true values. Linear regression has no parameters,\nwhich is a benefit, but it also has no way to control model complexity.\nHere is the code that produces the model you can see in Figure 2-11:\nIn[26]:\nfrom sklearn.linear_model import LinearRegression\nX, y = mglearn.datasets.make_wave(n_samples=60)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nlr = LinearRegression().fit(X_train, y_train)\nThe “slope” parameters (w), also called weights or coefficients, are stored in the coef_\nattribute, while the offset or intercept (b) is stored in the intercept_ attribute:\nIn[27]:\nprint(\"lr.coef_: {}\".format(lr.coef_))\nprint(\"lr.intercept_: {}\".format(lr.intercept_))\nOut[27]:\nlr.coef_: [ 0.394]\nlr.intercept_: -0.031804343026759746\nSupervised Machine Learning Algorithms \n| \n47\nYou might notice the strange-looking trailing underscore at the end\nof coef_ and intercept_. scikit-learn always stores anything\nthat is derived from the training data in attributes that end with a\ntrailing underscore. That is to separate them from parameters that\nare set by the user.\nThe intercept_ attribute is always a single float number, while the coef_ attribute is\na NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))",
    "a NumPy array with one entry per input feature. As we only have a single input fea‐\nture in the wave dataset, lr.coef_ only has a single entry.\nLet’s look at the training set and test set performance:\nIn[28]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\nOut[28]:\nTraining set score: 0.67\nTest set score: 0.66\nAn R2 of around 0.66 is not very good, but we can see that the scores on the training\nand test sets are very close together. This means we are likely underfitting, not over‐\nfitting. For this one-dimensional dataset, there is little danger of overfitting, as the\nmodel is very simple (or restricted). However, with higher-dimensional datasets\n(meaning datasets with a large number of features), linear models become more pow‐\nerful, and there is a higher chance of overfitting. Let’s take a look at how LinearRe\ngression performs on a more complex dataset, like the Boston Housing dataset.\nRemember that this dataset has 506 samples and 105 derived features. First, we load\nthe dataset and split it into a training and a test set. Then we build the linear regres‐\nsion model as before:\nIn[29]:\nX, y = mglearn.datasets.load_extended_boston()\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlr = LinearRegression().fit(X_train, y_train)\nWhen comparing training set and test set scores, we find that we predict very accu‐\nrately on the training set, but the R2 on the test set is much worse:\nIn[30]:\nprint(\"Training set score: {:.2f}\".format(lr.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lr.score(X_test, y_test)))\n48 \n| \nChapter 2: Supervised Learning\n7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\nOut[30]:\nTraining set score: 0.95\nTest set score: 0.61\nThis discrepancy between performance on the training set and the test set is a clear",
    "48 \n| \nChapter 2: Supervised Learning\n7 Mathematically, Ridge penalizes the L2 norm of the coefficients, or the Euclidean length of w.\nOut[30]:\nTraining set score: 0.95\nTest set score: 0.61\nThis discrepancy between performance on the training set and the test set is a clear\nsign of overfitting, and therefore we should try to find a model that allows us to con‐\ntrol complexity. One of the most commonly used alternatives to standard linear\nregression is ridge regression, which we will look into next.\nRidge regression\nRidge regression is also a linear model for regression, so the formula it uses to make\npredictions is the same one used for ordinary least squares. In ridge regression,\nthough, the coefficients (w) are chosen not only so that they predict well on the train‐\ning data, but also to fit an additional constraint. We also want the magnitude of coef‐\nficients to be as small as possible; in other words, all entries of w should be close to\nzero. Intuitively, this means each feature should have as little effect on the outcome as\npossible (which translates to having a small slope), while still predicting well. This\nconstraint is an example of what is called regularization. Regularization means explic‐\nitly restricting a model to avoid overfitting. The particular kind used by ridge regres‐\nsion is known as L2 regularization.7\nRidge regression is implemented in linear_model.Ridge. Let’s see how well it does\non the extended Boston Housing dataset:\nIn[31]:\nfrom sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\nOut[31]:\nTraining set score: 0.89\nTest set score: 0.75\nAs you can see, the training set score of Ridge is lower than for LinearRegression,\nwhile the test set score is higher. This is consistent with our expectation. With linear",
    "print(\"Test set score: {:.2f}\".format(ridge.score(X_test, y_test)))\nOut[31]:\nTraining set score: 0.89\nTest set score: 0.75\nAs you can see, the training set score of Ridge is lower than for LinearRegression,\nwhile the test set score is higher. This is consistent with our expectation. With linear\nregression, we were overfitting our data. Ridge is a more restricted model, so we are\nless likely to overfit. A less complex model means worse performance on the training\nset, but better generalization. As we are only interested in generalization perfor‐\nmance, we should choose the Ridge model over the LinearRegression model.\nSupervised Machine Learning Algorithms \n| \n49\nThe Ridge model makes a trade-off between the simplicity of the model (near-zero\ncoefficients) and its performance on the training set. How much importance the\nmodel places on simplicity versus training set performance can be specified by the\nuser, using the alpha parameter. In the previous example, we used the default param‐\neter alpha=1.0. There is no reason why this will give us the best trade-off, though.\nThe optimum setting of alpha depends on the particular dataset we are using.\nIncreasing alpha forces coefficients to move more toward zero, which decreases\ntraining set performance but might help generalization. For example:\nIn[32]:\nridge10 = Ridge(alpha=10).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))\nOut[32]:\nTraining set score: 0.79\nTest set score: 0.64\nDecreasing alpha allows the coefficients to be less restricted, meaning we move right\nin Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\nand we end up with a model that resembles LinearRegression:\nIn[33]:\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))",
    "in Figure 2-1. For very small values of alpha, coefficients are barely restricted at all,\nand we end up with a model that resembles LinearRegression:\nIn[33]:\nridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\nOut[33]:\nTraining set score: 0.93\nTest set score: 0.77\nHere, alpha=0.1 seems to be working well. We could try decreasing alpha even more\nto improve generalization. For now, notice how the parameter alpha corresponds to\nthe model complexity as shown in Figure 2-1. We will discuss methods to properly\nselect parameters in Chapter 5.\nWe can also get a more qualitative insight into how the alpha parameter changes the\nmodel by inspecting the coef_ attribute of models with different values of alpha. A\nhigher alpha means a more restricted model, so we expect the entries of coef_ to\nhave smaller magnitude for a high value of alpha than for a low value of alpha. This\nis confirmed in the plot in Figure 2-12:\n50 \n| \nChapter 2: Supervised Learning\nIn[34]:\nplt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\nplt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\nplt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\nplt.plot(lr.coef_, 'o', label=\"LinearRegression\")\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.hlines(0, 0, len(lr.coef_))\nplt.ylim(-25, 25)\nplt.legend()\nFigure 2-12. Comparing coefficient magnitudes for ridge regression with different values\nof alpha and linear regression\nHere, the x-axis enumerates the entries of coef_: x=0 shows the coefficient associated\nwith the first feature, x=1 the coefficient associated with the second feature, and so on\nup to x=100. The y-axis shows the numeric values of the corresponding values of the\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are",
    "up to x=100. The y-axis shows the numeric values of the corresponding values of the\ncoefficients. The main takeaway here is that for alpha=10, the coefficients are mostly\nbetween around –3 and 3. The coefficients for the Ridge model with alpha=1 are\nsomewhat larger. The dots corresponding to alpha=0.1 have larger magnitude still,\nand many of the dots corresponding to linear regression without any regularization\n(which would be alpha=0) are so large they are outside of the chart.\nSupervised Machine Learning Algorithms \n| \n51\nAnother way to understand the influence of regularization is to fix a value of alpha\nbut vary the amount of training data available. For Figure 2-13, we subsampled the\nBoston Housing dataset and evaluated LinearRegression and Ridge(alpha=1) on\nsubsets of increasing size (plots that show model performance as a function of dataset\nsize are called learning curves):\nIn[35]:\nmglearn.plots.plot_ridge_n_samples()\nFigure 2-13. Learning curves for ridge regression and linear regression on the Boston\nHousing dataset\nAs one would expect, the training score is higher than the test score for all dataset\nsizes, for both ridge and linear regression. Because ridge is regularized, the training\nscore of ridge is lower than the training score for linear regression across the board.\nHowever, the test score for ridge is better, particularly for small subsets of the data.\nFor less than 400 data points, linear regression is not able to learn anything. As more\nand more data becomes available to the model, both models improve, and linear\nregression catches up with ridge in the end. The lesson here is that with enough train‐\ning data, regularization becomes less important, and given enough data, ridge and\n52 \n| \nChapter 2: Supervised Learning\n8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\nthe coefficients.\nlinear regression will have the same performance (the fact that this happens here",
    "52 \n| \nChapter 2: Supervised Learning\n8 The lasso penalizes the L1 norm of the coefficient vector—or in other words, the sum of the absolute values of\nthe coefficients.\nlinear regression will have the same performance (the fact that this happens here\nwhen using the full dataset is just by chance). Another interesting aspect of\nFigure 2-13 is the decrease in training performance for linear regression. If more data\nis added, it becomes harder for a model to overfit, or memorize the data.\nLasso\nAn alternative to Ridge for regularizing linear regression is Lasso. As with ridge\nregression, using the lasso also restricts coefficients to be close to zero, but in a\nslightly different way, called L1 regularization.8 The consequence of L1 regularization\nis that when using the lasso, some coefficients are exactly zero. This means some fea‐\ntures are entirely ignored by the model. This can be seen as a form of automatic fea‐\nture selection. Having some coefficients be exactly zero often makes a model easier to\ninterpret, and can reveal the most important features of your model.\nLet’s apply the lasso to the extended Boston Housing dataset:\nIn[36]:\nfrom sklearn.linear_model import Lasso\nlasso = Lasso().fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso.coef_ != 0)))\nOut[36]:\nTraining set score: 0.29\nTest set score: 0.21\nNumber of features used: 4\nAs you can see, Lasso does quite badly, both on the training and the test set. This\nindicates that we are underfitting, and we find that it used only 4 of the 105 features.\nSimilarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\nhow strongly coefficients are pushed toward zero. In the previous example, we used\nthe default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we",
    "Similarly to Ridge, the Lasso also has a regularization parameter, alpha, that controls\nhow strongly coefficients are pushed toward zero. In the previous example, we used\nthe default of alpha=1.0. To reduce underfitting, let’s try decreasing alpha. When we\ndo this, we also need to increase the default setting of max_iter (the maximum num‐\nber of iterations to run):\nSupervised Machine Learning Algorithms \n| \n53\nIn[37]:\n# we increase the default setting of \"max_iter\",\n# otherwise the model would warn us that we should increase max_iter.\nlasso001 = Lasso(alpha=0.01, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso001.coef_ != 0)))\nOut[37]:\nTraining set score: 0.90\nTest set score: 0.77\nNumber of features used: 33\nA lower alpha allowed us to fit a more complex model, which worked better on the\ntraining and test data. The performance is slightly better than using Ridge, and we are\nusing only 33 of the 105 features. This makes this model potentially easier to under‐\nstand.\nIf we set alpha too low, however, we again remove the effect of regularization and end\nup overfitting, with a result similar to LinearRegression:\nIn[38]:\nlasso00001 = Lasso(alpha=0.0001, max_iter=100000).fit(X_train, y_train)\nprint(\"Training set score: {:.2f}\".format(lasso00001.score(X_train, y_train)))\nprint(\"Test set score: {:.2f}\".format(lasso00001.score(X_test, y_test)))\nprint(\"Number of features used: {}\".format(np.sum(lasso00001.coef_ != 0)))\nOut[38]:\nTraining set score: 0.95\nTest set score: 0.64\nNumber of features used: 94\nAgain, we can plot the coefficients of the different models, similarly to Figure 2-12.\nThe result is shown in Figure 2-14:\nIn[39]:\nplt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")",
    "Test set score: 0.64\nNumber of features used: 94\nAgain, we can plot the coefficients of the different models, similarly to Figure 2-12.\nThe result is shown in Figure 2-14:\nIn[39]:\nplt.plot(lasso.coef_, 's', label=\"Lasso alpha=1\")\nplt.plot(lasso001.coef_, '^', label=\"Lasso alpha=0.01\")\nplt.plot(lasso00001.coef_, 'v', label=\"Lasso alpha=0.0001\")\nplt.plot(ridge01.coef_, 'o', label=\"Ridge alpha=0.1\")\nplt.legend(ncol=2, loc=(0, 1.05))\nplt.ylim(-25, 25)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\n54 \n| \nChapter 2: Supervised Learning\nFigure 2-14. Comparing coefficient magnitudes for lasso regression with different values\nof alpha and ridge regression\nFor alpha=1, we not only see that most of the coefficients are zero (which we already\nknew), but that the remaining coefficients are also small in magnitude. Decreasing\nalpha to 0.01, we obtain the solution shown as the green dots, which causes most\nfeatures to be exactly zero. Using alpha=0.00001, we get a model that is quite unregu‐\nlarized, with most coefficients nonzero and of large magnitude. For comparison, the\nbest Ridge solution is shown in teal. The Ridge model with alpha=0.1 has similar\npredictive performance as the lasso model with alpha=0.01, but using Ridge, all coef‐\nficients are nonzero.\nIn practice, ridge regression is usually the first choice between these two models.\nHowever, if you have a large amount of features and expect only a few of them to be\nimportant, Lasso might be a better choice. Similarly, if you would like to have a\nmodel that is easy to interpret, Lasso will provide a model that is easier to under‐\nstand, as it will select only a subset of the input features. scikit-learn also provides\nthe ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55",
    "the ElasticNet class, which combines the penalties of Lasso and Ridge. In practice,\nthis combination works best, though at the price of having two parameters to adjust:\none for the L1 regularization, and one for the L2 regularization.\nSupervised Machine Learning Algorithms \n| \n55\nLinear models for classification\nLinear models are also extensively used for classification. Let’s look at binary classifi‐\ncation first. In this case, a prediction is made using the following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nThe formula looks very similar to the one for linear regression, but instead of just\nreturning the weighted sum of the features, we threshold the predicted value at zero.\nIf the function is smaller than zero, we predict the class –1; if it is larger than zero, we\npredict the class +1. This prediction rule is common to all linear models for classifica‐\ntion. Again, there are many different ways to find the coefficients (w) and the inter‐\ncept (b).\nFor linear models for regression, the output, ŷ, is a linear function of the features: a\nline, plane, or hyperplane (in higher dimensions). For linear models for classification,\nthe decision boundary is a linear function of the input. In other words, a (binary) lin‐\near classifier is a classifier that separates two classes using a line, a plane, or a hyper‐\nplane. We will see examples of that in this section.\nThere are many algorithms for learning linear models. These algorithms all differ in\nthe following two ways:\n• The way in which they measure how well a particular combination of coefficients\nand intercept fits the training data\n• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might",
    "• If and what kind of regularization they use\nDifferent algorithms choose different ways to measure what “fitting the training set\nwell” means. For technical mathematical reasons, it is not possible to adjust w and b\nto minimize the number of misclassifications the algorithms produce, as one might\nhope. For our purposes, and many applications, the different choices for item 1 in the\npreceding list (called loss functions) are of little significance.\nThe two most common linear classification algorithms are logistic regression, imple‐\nmented in linear_model.LogisticRegression, and linear support vector machines\n(linear SVMs), implemented in svm.LinearSVC (SVC stands for support vector classi‐\nfier). Despite its name, LogisticRegression is a classification algorithm and not a\nregression algorithm, and it should not be confused with LinearRegression.\nWe can apply the LogisticRegression and LinearSVC models to the forge dataset,\nand visualize the decision boundary as found by the linear models (Figure 2-15):\n56 \n| \nChapter 2: Supervised Learning\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nX, y = mglearn.datasets.make_forge()\nfig, axes = plt.subplots(1, 2, figsize=(10, 3))\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n    clf = model.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n                                    ax=ax, alpha=.7)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\naxes[0].legend()\nFigure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\ndataset with the default parameters\nIn this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\nond feature on the y-axis, as before. We display the decision boundaries found by",
    "Figure 2-15. Decision boundaries of a linear SVM and logistic regression on the forge\ndataset with the default parameters\nIn this figure, we have the first feature of the forge dataset on the x-axis and the sec‐\nond feature on the y-axis, as before. We display the decision boundaries found by\nLinearSVC and LogisticRegression respectively as straight lines, separating the area\nclassified as class 1 on the top from the area classified as class 0 on the bottom. In\nother words, any new data point that lies above the black line will be classified as class\n1 by the respective classifier, while any point that lies below the black line will be clas‐\nsified as class 0.\nThe two models come up with similar decision boundaries. Note that both misclas‐\nsify two of the points. By default, both models apply an L2 regularization, in the same\nway that Ridge does for regression.\nFor LogisticRegression and LinearSVC the trade-off parameter that determines the\nstrength of the regularization is called C, and higher values of C correspond to less\nSupervised Machine Learning Algorithms \n| \n57\nregularization. In other words, when you use a high value for the parameter C, Logis\nticRegression and LinearSVC try to fit the training set as best as possible, while with\nlow values of the parameter C, the models put more emphasis on finding a coefficient\nvector (w) that is close to zero.\nThere is another interesting aspect of how the parameter C acts. Using low values of C\nwill cause the algorithms to try to adjust to the “majority” of data points, while using\na higher value of C stresses the importance that each individual data point be classi‐\nfied correctly. Here is an illustration using LinearSVC (Figure 2-16):\nIn[41]:\nmglearn.plots.plot_linear_svc_regularization()\nFigure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\nvalues of C\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.",
    "In[41]:\nmglearn.plots.plot_linear_svc_regularization()\nFigure 2-16. Decision boundaries of a linear SVM on the forge dataset for different\nvalues of C\nOn the lefthand side, we have a very small C corresponding to a lot of regularization.\nMost of the points in class 0 are at the top, and most of the points in class 1 are at the\nbottom. The strongly regularized model chooses a relatively horizontal line, misclas‐\nsifying two points. In the center plot, C is slightly higher, and the model focuses more\non the two misclassified samples, tilting the decision boundary. Finally, on the right‐\nhand side, the very high value of C in the model tilts the decision boundary a lot, now\ncorrectly classifying all points in class 0. One of the points in class 1 is still misclassi‐\nfied, as it is not possible to correctly classify all points in this dataset using a straight\nline. The model illustrated on the righthand side tries hard to correctly classify all\npoints, but might not capture the overall layout of the classes well. In other words,\nthis model is likely overfitting.\nSimilarly to the case of regression, linear models for classification might seem very\nrestrictive in low-dimensional spaces, only allowing for decision boundaries that are\nstraight lines or planes. Again, in high dimensions, linear models for classification\n58 \n| \nChapter 2: Supervised Learning\nbecome very powerful, and guarding against overfitting becomes increasingly impor‐\ntant when considering more features.\nLet’s analyze LinearLogistic in more detail on the Breast Cancer dataset:\nIn[42]:\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\nOut[42]:",
    "cancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))\nOut[42]:\nTraining set score: 0.953\nTest set score: 0.958\nThe default value of C=1 provides quite good performance, with 95% accuracy on\nboth the training and the test set. But as training and test set performance are very\nclose, it is likely that we are underfitting. Let’s try to increase C to fit a more flexible\nmodel:\nIn[43]:\nlogreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))\nOut[43]:\nTraining set score: 0.972\nTest set score: 0.965\nUsing C=100 results in higher training set accuracy, and also a slightly increased test\nset accuracy, confirming our intuition that a more complex model should perform\nbetter.\nWe can also investigate what happens if we use an even more regularized model than\nthe default of C=1, by setting C=0.01:\nIn[44]:\nlogreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))\nOut[44]:\nTraining set score: 0.934\nTest set score: 0.930\nSupervised Machine Learning Algorithms \n| \n59\nAs expected, when moving more to the left along the scale shown in Figure 2-1 from\nan already underfit model, both training and test set accuracy decrease relative to the\ndefault parameters.\nFinally, let’s look at the coefficients learned by the models with the three different set‐\ntings of the regularization parameter C (Figure 2-17):\nIn[45]:\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")",
    "Finally, let’s look at the coefficients learned by the models with the three different set‐\ntings of the regularization parameter C (Figure 2-17):\nIn[45]:\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.ylim(-5, 5)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.legend()\nAs LogisticRegression applies an L2 regularization by default,\nthe result looks similar to that produced by Ridge in Figure 2-12.\nStronger regularization pushes coefficients more and more toward\nzero, though coefficients never become exactly zero. Inspecting the\nplot more closely, we can also see an interesting effect in the third\ncoefficient, for “mean perimeter.” For C=100 and C=1, the coefficient\nis negative, while for C=0.001, the coefficient is positive, with a\nmagnitude that is even larger than for C=1. Interpreting a model\nlike this, one might think the coefficient tells us which class a fea‐\nture is associated with. For example, one might think that a high\n“texture error” feature is related to a sample being “malignant.”\nHowever, the change of sign in the coefficient for “mean perimeter”\nmeans that depending on which model we look at, a high “mean\nperimeter” could be taken as being either indicative of “benign” or\nindicative of “malignant.” This illustrates that interpretations of\ncoefficients of linear models should always be taken with a grain of\nsalt.\n60 \n| \nChapter 2: Supervised Learning\nFigure 2-17. Coefficients learned by logistic regression on the Breast Cancer dataset for\ndifferent values of C\nSupervised Machine Learning Algorithms \n| \n61\nIf we desire a more interpretable model, using L1 regularization might help, as it lim‐\nits the model to using only a few features. Here is the coefficient plot and classifica‐",
    "different values of C\nSupervised Machine Learning Algorithms \n| \n61\nIf we desire a more interpretable model, using L1 regularization might help, as it lim‐\nits the model to using only a few features. Here is the coefficient plot and classifica‐\ntion accuracies for L1 regularization (Figure 2-18):\nIn[46]:\nfor C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l1 = LogisticRegression(C=C, penalty=\"l1\").fit(X_train, y_train)\n    print(\"Training accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_train, y_train)))\n    print(\"Test accuracy of l1 logreg with C={:.3f}: {:.2f}\".format(\n          C, lr_l1.score(X_test, y_test)))\n    plt.plot(lr_l1.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)\nOut[46]:\nTraining accuracy of l1 logreg with C=0.001: 0.91\nTest accuracy of l1 logreg with C=0.001: 0.92\nTraining accuracy of l1 logreg with C=1.000: 0.96\nTest accuracy of l1 logreg with C=1.000: 0.96\nTraining accuracy of l1 logreg with C=100.000: 0.99\nTest accuracy of l1 logreg with C=100.000: 0.98\nAs you can see, there are many parallels between linear models for binary classifica‐\ntion and linear models for regression. As in regression, the main difference between\nthe models is the penalty parameter, which influences the regularization and\nwhether the model will use all available features or select only a subset.\n62 \n| \nChapter 2: Supervised Learning\nFigure 2-18. Coefficients learned by logistic regression with L1 penalty on the Breast\nCancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common",
    "Cancer dataset for different values of C\nLinear models for multiclass classification\nMany linear classification models are for binary classification only, and don’t extend\nnaturally to the multiclass case (with the exception of logistic regression). A common\ntechnique to extend a binary classification algorithm to a multiclass classification\nalgorithm is the one-vs.-rest approach. In the one-vs.-rest approach, a binary model is\nlearned for each class that tries to separate that class from all of the other classes,\nresulting in as many binary models as there are classes. To make a prediction, all\nbinary classifiers are run on a test point. The classifier that has the highest score on its\nsingle class “wins,” and this class label is returned as the prediction.\nSupervised Machine Learning Algorithms \n| \n63\nHaving one binary classifier per class results in having one vector of coefficients (w)\nand one intercept (b) for each class. The class for which the result of the classification\nconfidence formula given here is highest is the assigned class label:\nw[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nThe mathematics behind multiclass logistic regression differ somewhat from the one-\nvs.-rest approach, but they also result in one coefficient vector and intercept per class,\nand the same method of making a prediction is applied.\nLet’s apply the one-vs.-rest method to a simple three-class classification dataset. We\nuse a two-dimensional dataset, where each class is given by data sampled from a\nGaussian distribution (see Figure 2-19):\nIn[47]:\nfrom sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\nFigure 2-19. Two-dimensional toy dataset containing three classes\n64 \n| \nChapter 2: Supervised Learning\nNow, we train a LinearSVC classifier on the dataset:\nIn[48]:\nlinear_svm = LinearSVC().fit(X, y)",
    "plt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])\nFigure 2-19. Two-dimensional toy dataset containing three classes\n64 \n| \nChapter 2: Supervised Learning\nNow, we train a LinearSVC classifier on the dataset:\nIn[48]:\nlinear_svm = LinearSVC().fit(X, y)\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)\nOut[48]:\nCoefficient shape:  (3, 2)\nIntercept shape:  (3,)\nWe see that the shape of the coef_ is (3, 2), meaning that each row of coef_ con‐\ntains the coefficient vector for one of the three classes and each column holds the\ncoefficient value for a specific feature (there are two in this dataset). The intercept_\nis now a one-dimensional array, storing the intercepts for each class.\nLet’s visualize the lines given by the three binary classifiers (Figure 2-20):\nIn[49]:\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nYou can see that all the points belonging to class 0 in the training data are above the\nline corresponding to class 0, which means they are on the “class 0” side of this binary\nclassifier. The points in class 0 are above the line corresponding to class 2, which\nmeans they are classified as “rest” by the binary classifier for class 2. The points\nbelonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐",
    "belonging to class 0 are to the left of the line corresponding to class 1, which means\nthe binary classifier for class 1 also classifies them as “rest.” Therefore, any point in\nthis area will be classified as class 0 by the final classifier (the result of the classifica‐\ntion confidence formula for classifier 0 is greater than zero, while it is smaller than\nzero for the other two classes).\nBut what about the triangle in the middle of the plot? All three binary classifiers clas‐\nsify points there as “rest.” Which class would a point there be assigned to? The answer\nis the one with the highest value for the classification formula: the class of the closest\nline.\nSupervised Machine Learning Algorithms \n| \n65\nFigure 2-20. Decision boundaries learned by the three one-vs.-rest classifiers\nThe following example (Figure 2-21) shows the predictions for all regions of the 2D\nspace:\nIn[50]:\nmglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,\n                                  ['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) / coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n            'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n66 \n| \nChapter 2: Supervised Learning\nFigure 2-21. Multiclass decision boundaries derived from the three one-vs.-rest classifiers\nStrengths, weaknesses, and parameters\nThe main parameter of linear models is the regularization parameter, called alpha in\nthe regression models and C in LinearSVC and LogisticRegression. Large values for\nalpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to",
    "alpha or small values for C mean simple models. In particular for the regression mod‐\nels, tuning these parameters is quite important. Usually C and alpha are searched for\non a logarithmic scale. The other decision you have to make is whether you want to\nuse L1 regularization or L2 regularization. If you assume that only a few of your fea‐\ntures are actually important, you should use L1. Otherwise, you should default to L2.\nL1 can also be useful if interpretability of the model is important. As L1 will use only\na few features, it is easier to explain which features are important to the model, and\nwhat the effects of these features are.\nLinear models are very fast to train, and also fast to predict. They scale to very large\ndatasets and work well with sparse data. If your data consists of hundreds of thou‐\nsands or millions of samples, you might want to investigate using the solver='sag'\noption in LogisticRegression and Ridge, which can be faster than the default on\nlarge datasets. Other options are the SGDClassifier class and the SGDRegressor\nclass, which implement even more scalable versions of the linear models described\nhere.\nAnother strength of linear models is that they make it relatively easy to understand\nhow a prediction is made, using the formulas we saw earlier for regression and classi‐\nfication. Unfortunately, it is often not entirely clear why coefficients are the way they\nare. This is particularly true if your dataset has highly correlated features; in these\ncases, the coefficients might be hard to interpret.\nSupervised Machine Learning Algorithms \n| \n67\nLinear models often perform well when the number of features is large compared to\nthe number of samples. They are also often used on very large datasets, simply\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\nother models might yield better generalization performance. We will look at some",
    "the number of samples. They are also often used on very large datasets, simply\nbecause it’s not feasible to train other models. However, in lower-dimensional spaces,\nother models might yield better generalization performance. We will look at some\nexamples in which linear models fail in “Kernelized Support Vector Machines” on\npage 92.\nMethod Chaining\nThe fit method of all scikit-learn models returns self. This allows you to write\ncode like the following, which we’ve already used extensively in this chapter:\nIn[51]:\n# instantiate model and fit it in one line\nlogreg = LogisticRegression().fit(X_train, y_train)\nHere, we used the return value of fit (which is self) to assign the trained model to\nthe variable logreg. This concatenation of method calls (here __init__ and then fit)\nis known as method chaining. Another common application of method chaining in\nscikit-learn is to fit and predict in one line:\nIn[52]:\nlogreg = LogisticRegression()\ny_pred = logreg.fit(X_train, y_train).predict(X_test)\nFinally, you can even do model instantiation, fitting, and predicting in one line:\nIn[53]:\ny_pred = LogisticRegression().fit(X_train, y_train).predict(X_test)\nThis very short variant is not ideal, though. A lot is happening in a single line, which\nmight make the code hard to read. Additionally, the fitted logistic regression model\nisn’t stored in any variable, so we can’t inspect it or use it to predict on any other data.\nNaive Bayes Classifiers\nNaive Bayes classifiers are a family of classifiers that are quite similar to the linear\nmodels discussed in the previous section. However, they tend to be even faster in\ntraining. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by",
    "training. The price paid for this efficiency is that naive Bayes models often provide\ngeneralization performance that is slightly worse than that of linear classifiers like\nLogisticRegression and LinearSVC.\nThe reason that naive Bayes models are so efficient is that they learn parameters by\nlooking at each feature individually and collect simple per-class statistics from each\nfeature. There are three kinds of naive Bayes classifiers implemented in scikit-\n68 \n| \nChapter 2: Supervised Learning\nlearn: GaussianNB, BernoulliNB, and MultinomialNB. GaussianNB can be applied to\nany continuous data, while BernoulliNB assumes binary data and MultinomialNB\nassumes count data (that is, that each feature represents an integer count of some‐\nthing, like how often a word appears in a sentence). BernoulliNB and MultinomialNB\nare mostly used in text data classification.\nThe BernoulliNB classifier counts how often every feature of each class is not zero.\nThis is most easily understood with an example:\nIn[54]:\nX = np.array([[0, 1, 0, 1],\n              [1, 0, 1, 1],\n              [0, 0, 0, 1],\n              [1, 0, 1, 0]])\ny = np.array([0, 1, 0, 1])\nHere, we have four data points, with four binary features each. There are two classes,\n0 and 1. For class 0 (the first and third data points), the first feature is zero two times\nand nonzero zero times, the second feature is zero one time and nonzero one time,\nand so on. These same counts are then calculated for the data points in the second\nclass. Counting the nonzero entries per class in essence looks like this:\nIn[55]:\ncounts = {}\nfor label in np.unique(y):\n    # iterate over each class\n    # count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐",
    "# count (sum) entries of 1 per feature\n    counts[label] = X[y == label].sum(axis=0)\nprint(\"Feature counts:\\n{}\".format(counts))\nOut[55]:\nFeature counts:\n{0: array([0, 1, 0, 2]), 1: array([2, 0, 2, 1])}\nThe other two naive Bayes models, MultinomialNB and GaussianNB, are slightly dif‐\nferent in what kinds of statistics they compute. MultinomialNB takes into account the\naverage value of each feature for each class, while GaussianNB stores the average value\nas well as the standard deviation of each feature for each class.\nTo make a prediction, a data point is compared to the statistics for each of the classes,\nand the best matching class is predicted. Interestingly, for both MultinomialNB and\nBernoulliNB, this leads to a prediction formula that is of the same form as in the lin‐\near models (see “Linear models for classification” on page 56). Unfortunately, coef_\nfor the naive Bayes models has a somewhat different meaning than in the linear mod‐\nels, in that coef_ is not the same as w.\nSupervised Machine Learning Algorithms \n| \n69\nStrengths, weaknesses, and parameters\nMultinomialNB and BernoulliNB have a single parameter, alpha, which controls\nmodel complexity. The way alpha works is that the algorithm adds to the data alpha\nmany virtual data points that have positive values for all the features. This results in a\n“smoothing” of the statistics. A large alpha means more smoothing, resulting in less\ncomplex models. The algorithm’s performance is relatively robust to the setting of\nalpha, meaning that setting alpha is not critical for good performance. However,\ntuning it usually improves accuracy somewhat.\nGaussianNB is mostly used on very high-dimensional data, while the other two var‐\niants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).",
    "iants of naive Bayes are widely used for sparse count data such as text. MultinomialNB\nusually performs better than BinaryNB, particularly on datasets with a relatively large\nnumber of nonzero features (i.e., large documents).\nThe naive Bayes models share many of the strengths and weaknesses of the linear\nmodels. They are very fast to train and to predict, and the training procedure is easy\nto understand. The models work very well with high-dimensional sparse data and are\nrelatively robust to the parameters. Naive Bayes models are great baseline models and\nare often used on very large datasets, where training even a linear model might take\ntoo long.\nDecision Trees\nDecision trees are widely used models for classification and regression tasks. Essen‐\ntially, they learn a hierarchy of if/else questions, leading to a decision.\nThese questions are similar to the questions you might ask in a game of 20 Questions.\nImagine you want to distinguish between the following four animals: bears, hawks,\npenguins, and dolphins. Your goal is to get to the right answer by asking as few if/else\nquestions as possible. You might start off by asking whether the animal has feathers, a\nquestion that narrows down your possible animals to just two. If the answer is “yes,”\nyou can ask another question that could help you distinguish between hawks and\npenguins. For example, you could ask whether the animal can fly. If the animal\ndoesn’t have feathers, your possible animal choices are dolphins and bears, and you\nwill need to ask a question to distinguish between these two animals—for example,\nasking whether the animal has fins.\nThis series of questions can be expressed as a decision tree, as shown in Figure 2-22.\nIn[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal",
    "In[56]:\nmglearn.plots.plot_animal_tree()\n70 \n| \nChapter 2: Supervised Learning\nFigure 2-22. A decision tree to distinguish among several animals\nIn this illustration, each node in the tree either represents a question or a terminal\nnode (also called a leaf) that contains the answer. The edges connect the answers to a\nquestion with the next question you would ask.\nIn machine learning parlance, we built a model to distinguish between four classes of\nanimals (hawks, penguins, dolphins, and bears) using the three features “has feath‐\ners,” “can fly,” and “has fins.” Instead of building these models by hand, we can learn\nthem from data using supervised learning.\nBuilding decision trees\nLet’s go through the process of building a decision tree for the 2D classification data‐\nset shown in Figure 2-23. The dataset consists of two half-moon shapes, with each\nclass consisting of 75 data points. We will refer to this dataset as two_moons.\nLearning a decision tree means learning the sequence of if/else questions that gets us\nto the true answer most quickly. In the machine learning setting, these questions are\ncalled tests (not to be confused with the test set, which is the data we use to test to see\nhow generalizable our model is). Usually data does not come in the form of binary\nyes/no features as in the animal example, but is instead represented as continuous\nfeatures such as in the 2D dataset shown in Figure 2-23. The tests that are used on\ncontinuous data are of the form “Is feature i larger than value a?”\nSupervised Machine Learning Algorithms \n| \n71\nFigure 2-23. Two-moons dataset on which the decision tree will be built\nTo build a tree, the algorithm searches over all possible tests and finds the one that is\nmost informative about the target variable. Figure 2-24 shows the first test that is\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\nbest separates the points in class 1 from the points in class 2. The top node, also called",
    "most informative about the target variable. Figure 2-24 shows the first test that is\npicked. Splitting the dataset vertically at x[1]=0.0596 yields the most information; it\nbest separates the points in class 1 from the points in class 2. The top node, also called\nthe root, represents the whole dataset, consisting of 75 points belonging to class 0 and\n75 points belonging to class 1. The split is done by testing whether x[1] <= 0.0596,\nindicated by a black line. If the test is true, a point is assigned to the left node, which\ncontains 2 points belonging to class 0 and 32 points belonging to class 1. Otherwise\nthe point is assigned to the right node, which contains 48 points belonging to class 0\nand 18 points belonging to class 1. These two nodes correspond to the top and bot‐\ntom regions shown in Figure 2-24. Even though the first split did a good job of sepa‐\nrating the two classes, the bottom region still contains points belonging to class 0, and\nthe top region still contains points belonging to class 1. We can build a more accurate\nmodel by repeating the process of looking for the best test in both regions.\nFigure 2-25 shows that the most informative next split for the left and the right region\nis based on x[0].\n72 \n| \nChapter 2: Supervised Learning\nFigure 2-24. Decision boundary of tree with depth 1 (left) and corresponding tree (right)\nFigure 2-25. Decision boundary of tree with depth 2 (left) and corresponding decision\ntree (right)\nThis recursive process yields a binary tree of decisions, with each node containing a\ntest. Alternatively, you can think of each test as splitting the part of the data that is\ncurrently being considered along one axis. This yields a view of the algorithm as\nbuilding a hierarchical partition. As each test concerns only a single feature, the\nregions in the resulting partition always have axis-parallel boundaries.\nThe recursive partitioning of the data is repeated until each region in the partition",
    "building a hierarchical partition. As each test concerns only a single feature, the\nregions in the resulting partition always have axis-parallel boundaries.\nThe recursive partitioning of the data is repeated until each region in the partition\n(each leaf in the decision tree) only contains a single target value (a single class or a\nsingle regression value). A leaf of the tree that contains data points that all share the\nsame target value is called pure. The final partitioning for this dataset is shown in\nFigure 2-26.\nSupervised Machine Learning Algorithms \n| \n73\nFigure 2-26. Decision boundary of tree with depth 9 (left) and part of the corresponding\ntree (right); the full tree is quite large and hard to visualize\nA prediction on a new data point is made by checking which region of the partition\nof the feature space the point lies in, and then predicting the majority target (or the\nsingle target in the case of pure leaves) in that region. The region can be found by\ntraversing the tree from the root and going left or right, depending on whether the\ntest is fulfilled or not.\nIt is also possible to use trees for regression tasks, using exactly the same technique.\nTo make a prediction, we traverse the tree based on the tests in each node and find\nthe leaf the new data point falls into. The output for this data point is the mean target\nof the training points in this leaf.\nControlling complexity of decision trees\nTypically, building a tree as described here and continuing until all leaves are pure\nleads to models that are very complex and highly overfit to the training data. The\npresence of pure leaves mean that a tree is 100% accurate on the training set; each\ndata point in the training set is in a leaf that has the correct majority class. The over‐\nfitting can be seen on the left of Figure 2-26. You can see the regions determined to\nbelong to class 1 in the middle of all the points belonging to class 0. On the other",
    "data point in the training set is in a leaf that has the correct majority class. The over‐\nfitting can be seen on the left of Figure 2-26. You can see the regions determined to\nbelong to class 1 in the middle of all the points belonging to class 0. On the other\nhand, there is a small strip predicted as class 0 around the point belonging to class 0\nto the very right. This is not how one would imagine the decision boundary to look,\nand the decision boundary focuses a lot on single outlier points that are far away\nfrom the other points in that class.\nThere are two common strategies to prevent overfitting: stopping the creation of the\ntree early (also called pre-pruning), or building the tree but then removing or collaps‐\ning nodes that contain little information (also called post-pruning or just pruning).\nPossible criteria for pre-pruning include limiting the maximum depth of the tree,\nlimiting the maximum number of leaves, or requiring a minimum number of points\nin a node to keep splitting it.\n74 \n| \nChapter 2: Supervised Learning\nDecision trees in scikit-learn are implemented in the DecisionTreeRegressor and\nDecisionTreeClassifier classes. scikit-learn only implements pre-pruning, not\npost-pruning.\nLet’s look at the effect of pre-pruning in more detail on the Breast Cancer dataset. As\nalways, we import the dataset and split it into a training and a test part. Then we build\na model using the default setting of fully developing the tree (growing the tree until\nall leaves are pure). We fix the random_state in the tree, which is used for tie-\nbreaking internally:\nIn[58]:\nfrom sklearn.tree import DecisionTreeClassifier\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))",
    "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, stratify=cancer.target, random_state=42)\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\nOut[58]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.937\nAs expected, the accuracy on the training set is 100%—because the leaves are pure,\nthe tree was grown deep enough that it could perfectly memorize all the labels on the\ntraining data. The test set accuracy is slightly worse than for the linear models we\nlooked at previously, which had around 95% accuracy.\nIf we don’t restrict the depth of a decision tree, the tree can become arbitrarily deep\nand complex. Unpruned trees are therefore prone to overfitting and not generalizing\nwell to new data. Now let’s apply pre-pruning to the tree, which will stop developing\nthe tree before we perfectly fit to the training data. One option is to stop building the\ntree after a certain depth has been reached. Here we set max_depth=4, meaning only\nfour consecutive questions can be asked (cf. Figures 2-24 and 2-26). Limiting the\ndepth of the tree decreases overfitting. This leads to a lower accuracy on the training\nset, but an improvement on the test set:\nIn[59]:\ntree = DecisionTreeClassifier(max_depth=4, random_state=0)\ntree.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(tree.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(tree.score(X_test, y_test)))\nSupervised Machine Learning Algorithms \n| \n75\nOut[59]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.951\nAnalyzing decision trees\nWe can visualize the tree using the export_graphviz function from the tree module.\nThis writes a file in the .dot file format, which is a text file format for storing graphs.",
    "| \n75\nOut[59]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.951\nAnalyzing decision trees\nWe can visualize the tree using the export_graphviz function from the tree module.\nThis writes a file in the .dot file format, which is a text file format for storing graphs.\nWe set an option to color the nodes to reflect the majority class in each node and pass\nthe class and features names so the tree can be properly labeled:\nIn[61]:\nfrom sklearn.tree import export_graphviz\nexport_graphviz(tree, out_file=\"tree.dot\", class_names=[\"malignant\", \"benign\"],\n                feature_names=cancer.feature_names, impurity=False, filled=True)\nWe can read this file and visualize it, as seen in Figure 2-27, using the graphviz mod‐\nule (or you can use any program that can read .dot files):\nIn[61]:\nimport graphviz\nwith open(\"tree.dot\") as f:\n    dot_graph = f.read()\ngraphviz.Source(dot_graph)\nFigure 2-27. Visualization of the decision tree built on the Breast Cancer dataset\n76 \n| \nChapter 2: Supervised Learning\nThe visualization of the tree provides a great in-depth view of how the algorithm\nmakes predictions, and is a good example of a machine learning algorithm that is\neasily explained to nonexperts. However, even with a tree of depth four, as seen here,\nthe tree can become a bit overwhelming. Deeper trees (a depth of 10 is not uncom‐\nmon) are even harder to grasp. One method of inspecting the tree that may be helpful\nis to find out which path most of the data actually takes. The n_samples shown in\neach node in Figure 2-27 gives the number of samples in that node, while value pro‐\nvides the number of samples per class. Following the branches to the right, we see\nthat worst radius <= 16.795 creates a node that contains only 8 benign but 134\nmalignant samples. The rest of this side of the tree then uses some finer distinctions\nto split off these 8 remaining benign samples. Of the 142 samples that went to the",
    "that worst radius <= 16.795 creates a node that contains only 8 benign but 134\nmalignant samples. The rest of this side of the tree then uses some finer distinctions\nto split off these 8 remaining benign samples. Of the 142 samples that went to the\nright in the initial split, nearly all of them (132) end up in the leaf to the very right.\nTaking a left at the root, for worst radius > 16.795 we end up with 25 malignant\nand 259 benign samples. Nearly all of the benign samples end up in the second leaf\nfrom the right, with most of the other leaves containing very few samples.\nFeature importance in trees\nInstead of looking at the whole tree, which can be taxing, there are some useful prop‐\nerties that we can derive to summarize the workings of the tree. The most commonly\nused summary is feature importance, which rates how important each feature is for\nthe decision a tree makes. It is a number between 0 and 1 for each feature, where 0\nmeans “not used at all” and 1 means “perfectly predicts the target.” The feature\nimportances always sum to 1:\nIn[62]:\nprint(\"Feature importances:\\n{}\".format(tree.feature_importances_))\nOut[62]:\nFeature importances:\n[ 0.     0.     0.     0.     0.     0.     0.     0.     0.     0.     0.01\n  0.048  0.     0.     0.002  0.     0.     0.     0.     0.     0.727  0.046\n  0.     0.     0.014  0.     0.018  0.122  0.012  0.   ]\nWe can visualize the feature importances in a way that is similar to the way we visual‐\nize the coefficients in the linear model (Figure 2-28):\nIn[63]:\ndef plot_feature_importances_cancer(model):\n    n_features = cancer.data.shape[1]\n    plt.barh(range(n_features), model.feature_importances_, align='center')\n    plt.yticks(np.arange(n_features), cancer.feature_names)\n    plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\nplot_feature_importances_cancer(tree)\nSupervised Machine Learning Algorithms \n| \n77\nFigure 2-28. Feature importances computed from a decision tree learned on the Breast\nCancer dataset",
    "plt.xlabel(\"Feature importance\")\n    plt.ylabel(\"Feature\")\nplot_feature_importances_cancer(tree)\nSupervised Machine Learning Algorithms \n| \n77\nFigure 2-28. Feature importances computed from a decision tree learned on the Breast\nCancer dataset\nHere we see that the feature used in the top split (“worst radius”) is by far the most\nimportant feature. This confirms our observation in analyzing the tree that the first\nlevel already separates the two classes fairly well.\nHowever, if a feature has a low feature_importance, it doesn’t mean that this feature\nis uninformative. It only means that the feature was not picked by the tree, likely\nbecause another feature encodes the same information.\nIn contrast to the coefficients in linear models, feature importances are always posi‐\ntive, and don’t encode which class a feature is indicative of. The feature importances\ntell us that “worst radius” is important, but not whether a high radius is indicative of a\nsample being benign or malignant. In fact, there might not be such a simple relation‐\nship between features and class, as you can see in the following example (Figures 2-29\nand 2-30):\nIn[64]:\ntree = mglearn.plots.plot_tree_not_monotone()\ndisplay(tree)\nOut[64]:\nFeature importances: [ 0.  1.]\n78 \n| \nChapter 2: Supervised Learning\nFigure 2-29. A two-dimensional dataset in which the feature on the y-axis has a nonmo‐\nnotonous relationship with the class label, and the decision boundaries found by a deci‐\nsion tree\nFigure 2-30. Decision tree learned on the data shown in Figure 2-29\nThe plot shows a dataset with two features and two classes. Here, all the information\nis contained in X[1], and X[0] is not used at all. But the relation between X[1] and\nSupervised Machine Learning Algorithms \n| \n79\nthe output class is not monotonous, meaning we cannot say “a high value of X[0]\nmeans class 0, and a low value means class 1” (or vice versa).\nWhile we focused our discussion here on decision trees for classification, all that was",
    "Supervised Machine Learning Algorithms \n| \n79\nthe output class is not monotonous, meaning we cannot say “a high value of X[0]\nmeans class 0, and a low value means class 1” (or vice versa).\nWhile we focused our discussion here on decision trees for classification, all that was\nsaid is similarly true for decision trees for regression, as implemented in Decision\nTreeRegressor. The usage and analysis of regression trees is very similar to that of\nclassification trees. There is one particular property of using tree-based models for\nregression that we want to point out, though. The DecisionTreeRegressor (and all\nother tree-based regression models) is not able to extrapolate, or make predictions\noutside of the range of the training data.\nLet’s look into this in more detail, using a dataset of historical computer memory\n(RAM) prices. Figure 2-31 shows the dataset, with the date on the x-axis and the price\nof one megabyte of RAM in that year on the y-axis:\nIn[65]:\nimport pandas as pd\nram_prices = pd.read_csv(\"data/ram_price.csv\")\nplt.semilogy(ram_prices.date, ram_prices.price)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Price in $/Mbyte\")\nFigure 2-31. Historical development of the price of RAM, plotted on a log scale\n80 \n| \nChapter 2: Supervised Learning\nNote the logarithmic scale of the y-axis. When plotting logarithmically, the relation\nseems to be quite linear and so should be relatively easy to predict, apart from some\nbumps.\nWe will make a forecast for the years after 2000 using the historical data up to that\npoint, with the date as our only feature. We will compare two simple models: a\nDecisionTreeRegressor and LinearRegression. We rescale the prices using a loga‐\nrithm, so that the relationship is relatively linear. This doesn’t make a difference for\nthe DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\nwill discuss this in more depth in Chapter 4). After training the models and making",
    "rithm, so that the relationship is relatively linear. This doesn’t make a difference for\nthe DecisionTreeRegressor, but it makes a big difference for LinearRegression (we\nwill discuss this in more depth in Chapter 4). After training the models and making\npredictions, we apply the exponential map to undo the logarithm transform. We\nmake predictions on the whole dataset for visualization purposes here, but for a\nquantitative evaluation we would only consider the test dataset:\nIn[66]:\nfrom sklearn.tree import DecisionTreeRegressor\n# use historical data to forecast prices after the year 2000\ndata_train = ram_prices[ram_prices.date < 2000]\ndata_test = ram_prices[ram_prices.date >= 2000]\n# predict prices based on date\nX_train = data_train.date[:, np.newaxis]\n# we use a log-transform to get a simpler relationship of data to target\ny_train = np.log(data_train.price)\ntree = DecisionTreeRegressor().fit(X_train, y_train)\nlinear_reg = LinearRegression().fit(X_train, y_train)\n# predict on all data\nX_all = ram_prices.date[:, np.newaxis]\npred_tree = tree.predict(X_all)\npred_lr = linear_reg.predict(X_all)\n# undo log-transform\nprice_tree = np.exp(pred_tree)\nprice_lr = np.exp(pred_lr)\nFigure 2-32, created here, compares the predictions of the decision tree and the linear\nregression model with the ground truth:\nIn[67]:\nplt.semilogy(data_train.date, data_train.price, label=\"Training data\")\nplt.semilogy(data_test.date, data_test.price, label=\"Test data\")\nplt.semilogy(ram_prices.date, price_tree, label=\"Tree prediction\")\nplt.semilogy(ram_prices.date, price_lr, label=\"Linear prediction\")\nplt.legend()\nSupervised Machine Learning Algorithms \n| \n81\n9 It is actually possible to make very good forecasts with tree-based models (for example, when trying to predict\nwhether a price will go up or down). The point of this example was not to show that trees are a bad model for\ntime series, but to illustrate a particular property of how trees make predictions.",
    "whether a price will go up or down). The point of this example was not to show that trees are a bad model for\ntime series, but to illustrate a particular property of how trees make predictions.\nFigure 2-32. Comparison of predictions made by a linear model and predictions made\nby a regression tree on the RAM price data\nThe difference between the models is quite striking. The linear model approximates\nthe data with a line, as we knew it would. This line provides quite a good forecast for\nthe test data (the years after 2000), while glossing over some of the finer variations in\nboth the training and the test data. The tree model, on the other hand, makes perfect\npredictions on the training data; we did not restrict the complexity of the tree, so it\nlearned the whole dataset by heart. However, once we leave the data range for which\nthe model has data, the model simply keeps predicting the last known point. The tree\nhas no ability to generate “new” responses, outside of what was seen in the training\ndata. This shortcoming applies to all models based on trees.9\nStrengths, weaknesses, and parameters\nAs discussed earlier, the parameters that control model complexity in decision trees\nare the pre-pruning parameters that stop the building of the tree before it is fully\ndeveloped. Usually, picking one of the pre-pruning strategies—setting either\n82 \n| \nChapter 2: Supervised Learning\nmax_depth, max_leaf_nodes, or min_samples_leaf—is sufficient to prevent overfit‐\nting.\nDecision trees have two advantages over many of the algorithms we’ve discussed so\nfar: the resulting model can easily be visualized and understood by nonexperts (at\nleast for smaller trees), and the algorithms are completely invariant to scaling of the\ndata. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when",
    "data. As each feature is processed separately, and the possible splits of the data don’t\ndepend on scaling, no preprocessing like normalization or standardization of features\nis needed for decision tree algorithms. In particular, decision trees work well when\nyou have features that are on completely different scales, or a mix of binary and con‐\ntinuous features.\nThe main downside of decision trees is that even with the use of pre-pruning, they\ntend to overfit and provide poor generalization performance. Therefore, in most\napplications, the ensemble methods we discuss next are usually used in place of a sin‐\ngle decision tree.\nEnsembles of Decision Trees\nEnsembles are methods that combine multiple machine learning models to create\nmore powerful models. There are many models in the machine learning literature\nthat belong to this category, but there are two ensemble models that have proven to\nbe effective on a wide range of datasets for classification and regression, both of\nwhich use decision trees as their building blocks: random forests and gradient boos‐\nted decision trees.\nRandom forests\nAs we just observed, a main drawback of decision trees is that they tend to overfit the\ntraining data. Random forests are one way to address this problem. A random forest\nis essentially a collection of decision trees, where each tree is slightly different from\nthe others. The idea behind random forests is that each tree might do a relatively\ngood job of predicting, but will likely overfit on part of the data. If we build many\ntrees, all of which work well and overfit in different ways, we can reduce the amount\nof overfitting by averaging their results. This reduction in overfitting, while retaining\nthe predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other",
    "the predictive power of the trees, can be shown using rigorous mathematics.\nTo implement this strategy, we need to build many decision trees. Each tree should do\nan acceptable job of predicting the target, and should also be different from the other\ntrees. Random forests get their name from injecting randomness into the tree build‐\ning to ensure each tree is different. There are two ways in which the trees in a random\nforest are randomized: by selecting the data points used to build a tree and by select‐\ning the features in each split test. Let’s go into this process in more detail.\nSupervised Machine Learning Algorithms \n| \n83\nBuilding random forests.    To build a random forest model, you need to decide on the\nnumber of trees to build (the n_estimators parameter of RandomForestRegressor or\nRandomForestClassifier). Let’s say we want to build 10 trees. These trees will be\nbuilt completely independently from each other, and the algorithm will make differ‐\nent random choices for each tree to make sure the trees are distinct. To build a tree,\nwe first take what is called a bootstrap sample of our data. That is, from our n_samples\ndata points, we repeatedly draw an example randomly with replacement (meaning the\nsame sample can be picked multiple times), n_samples times. This will create a data‐\nset that is as big as the original dataset, but some data points will be missing from it\n(approximately one third), and some will be repeated.\nTo illustrate, let’s say we want to create a bootstrap sample of the list ['a', 'b',\n'c', 'd']. A possible bootstrap sample would be ['b', 'd', 'd', 'c']. Another\npossible sample would be ['d', 'a', 'd', 'a'].\nNext, a decision tree is built based on this newly created dataset. However, the algo‐\nrithm we described for the decision tree is slightly modified. Instead of looking for\nthe best test for each node, in each node the algorithm randomly selects a subset of",
    "Next, a decision tree is built based on this newly created dataset. However, the algo‐\nrithm we described for the decision tree is slightly modified. Instead of looking for\nthe best test for each node, in each node the algorithm randomly selects a subset of\nthe features, and it looks for the best possible test involving one of these features. The\nnumber of features that are selected is controlled by the max_features parameter.\nThis selection of a subset of features is repeated separately in each node, so that each\nnode in a tree can make a decision using a different subset of the features.\nThe bootstrap sampling leads to each decision tree in the random forest being built\non a slightly different dataset. Because of the selection of features in each node, each\nsplit in each tree operates on a different subset of features. Together, these two mech‐\nanisms ensure that all the trees in the random forest are different.\nA critical parameter in this process is max_features. If we set max_features to n_fea\ntures, that means that each split can look at all features in the dataset, and no ran‐\ndomness will be injected in the feature selection (the randomness due to the\nbootstrapping remains, though). If we set max_features to 1, that means that the\nsplits have no choice at all on which feature to test, and can only search over different\nthresholds for the feature that was selected randomly. Therefore, a high max_fea\ntures means that the trees in the random forest will be quite similar, and they will be\nable to fit the data easily, using the most distinctive features. A low max_features\nmeans that the trees in the random forest will be quite different, and that each tree\nmight need to be very deep in order to fit the data well.\nTo make a prediction using the random forest, the algorithm first makes a prediction\nfor every tree in the forest. For regression, we can average these results to get our final",
    "might need to be very deep in order to fit the data well.\nTo make a prediction using the random forest, the algorithm first makes a prediction\nfor every tree in the forest. For regression, we can average these results to get our final\nprediction. For classification, a “soft voting” strategy is used. This means each algo‐\nrithm makes a “soft” prediction, providing a probability for each possible output\n84 \n| \nChapter 2: Supervised Learning\nlabel. The probabilities predicted by all the trees are averaged, and the class with the\nhighest probability is predicted.\nAnalyzing random forests.    Let’s apply a random forest consisting of five trees to the\ntwo_moons dataset we studied earlier:\nIn[68]:\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)\nforest = RandomForestClassifier(n_estimators=5, random_state=2)\nforest.fit(X_train, y_train)\nThe trees that are built as part of the random forest are stored in the estimator_\nattribute. Let’s visualize the decision boundaries learned by each tree, together with\ntheir aggregate prediction as made by the forest (Figure 2-33):\nIn[69]:\nfig, axes = plt.subplots(2, 3, figsize=(20, 10))\nfor i, (ax, tree) in enumerate(zip(axes.ravel(), forest.estimators_)):\n    ax.set_title(\"Tree {}\".format(i))\n    mglearn.plots.plot_tree_partition(X_train, y_train, tree, ax=ax)\nmglearn.plots.plot_2d_separator(forest, X_train, fill=True, ax=axes[-1, -1],\n                                alpha=.4)\naxes[-1, -1].set_title(\"Random Forest\")\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nYou can clearly see that the decision boundaries learned by the five trees are quite dif‐\nferent. Each of them makes some mistakes, as some of the training points that are",
    "axes[-1, -1].set_title(\"Random Forest\")\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nYou can clearly see that the decision boundaries learned by the five trees are quite dif‐\nferent. Each of them makes some mistakes, as some of the training points that are\nplotted here were not actually included in the training sets of the trees, due to the\nbootstrap sampling.\nThe random forest overfits less than any of the trees individually, and provides a\nmuch more intuitive decision boundary. In any real application, we would use many\nmore trees (often hundreds or thousands), leading to even smoother boundaries.\nSupervised Machine Learning Algorithms \n| \n85\nFigure 2-33. Decision boundaries found by five randomized decision trees and the deci‐\nsion boundary obtained by averaging their predicted probabilities\nAs another example, let’s apply a random forest consisting of 100 trees on the Breast\nCancer dataset:\nIn[70]:\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nforest = RandomForestClassifier(n_estimators=100, random_state=0)\nforest.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(forest.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(forest.score(X_test, y_test)))\nOut[70]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.972\nThe random forest gives us an accuracy of 97%, better than the linear models or a\nsingle decision tree, without tuning any parameters. We could adjust the max_fea\ntures setting, or apply pre-pruning as we did for the single decision tree. However,\noften the default parameters of the random forest already work quite well.\nSimilarly to the decision tree, the random forest provides feature importances, which\nare computed by aggregating the feature importances over the trees in the forest. Typ‐\nically, the feature importances provided by the random forest are more reliable than",
    "Similarly to the decision tree, the random forest provides feature importances, which\nare computed by aggregating the feature importances over the trees in the forest. Typ‐\nically, the feature importances provided by the random forest are more reliable than\nthe ones provided by a single tree. Take a look at Figure 2-34.\n86 \n| \nChapter 2: Supervised Learning\nIn[71]:\nplot_feature_importances_cancer(forest)\nFigure 2-34. Feature importances computed from a random forest that was fit to the\nBreast Cancer dataset\nAs you can see, the random forest gives nonzero importance to many more features\nthan the single tree. Similarly to the single decision tree, the random forest also gives\na lot of importance to the “worst radius” feature, but it actually chooses “worst perim‐\neter” to be the most informative feature overall. The randomness in building the ran‐\ndom forest forces the algorithm to consider many possible explanations, the result\nbeing that the random forest captures a much broader picture of the data than a sin‐\ngle tree.\nStrengths, weaknesses, and parameters.    Random forests for regression and classifica‐\ntion are currently among the most widely used machine learning methods. They are\nvery powerful, often work well without heavy tuning of the parameters, and don’t\nrequire scaling of the data.\nEssentially, random forests share all of the benefits of decision trees, while making up\nfor some of their deficiencies. One reason to still use decision trees is if you need a\ncompact representation of the decision-making process. It is basically impossible to\ninterpret tens or hundreds of trees in detail, and trees in random forests tend to be\ndeeper than decision trees (because of the use of feature subsets). Therefore, if you\nneed to summarize the prediction making in a visual way to nonexperts, a single\ndecision tree might be a better choice. While building random forests on large data‐\nsets might be somewhat time consuming, it can be parallelized across multiple CPU",
    "need to summarize the prediction making in a visual way to nonexperts, a single\ndecision tree might be a better choice. While building random forests on large data‐\nsets might be somewhat time consuming, it can be parallelized across multiple CPU\nSupervised Machine Learning Algorithms \n| \n87\ncores within a computer easily. If you are using a multi-core processor (as nearly all\nmodern computers do), you can use the n_jobs parameter to adjust the number of\ncores to use. Using more CPU cores will result in linear speed-ups (using two cores,\nthe training of the random forest will be twice as fast), but specifying n_jobs larger\nthan the number of cores will not help. You can set n_jobs=-1 to use all the cores in\nyour computer.\nYou should keep in mind that random forests, by their nature, are random, and set‐\nting different random states (or not setting the random_state at all) can drastically\nchange the model that is built. The more trees there are in the forest, the more robust\nit will be against the choice of random state. If you want to have reproducible results,\nit is important to fix the random_state.\nRandom forests don’t tend to perform well on very high dimensional, sparse data,\nsuch as text data. For this kind of data, linear models might be more appropriate.\nRandom forests usually work well even on very large datasets, and training can easily\nbe parallelized over many CPU cores within a powerful computer. However, random\nforests require more memory and are slower to train and to predict than linear mod‐\nels. If time and memory are important in an application, it might make sense to use a\nlinear model instead.\nThe important parameters to adjust are n_estimators, max_features, and possibly\npre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\naging more trees will yield a more robust ensemble by reducing overfitting. However,\nthere are diminishing returns, and more trees need more memory and more time to",
    "pre-pruning options like max_depth. For n_estimators, larger is always better. Aver‐\naging more trees will yield a more robust ensemble by reducing overfitting. However,\nthere are diminishing returns, and more trees need more memory and more time to\ntrain. A common rule of thumb is to build “as many as you have time/memory for.”\nAs described earlier, max_features determines how random each tree is, and a\nsmaller max_features reduces overfitting. In general, it’s a good rule of thumb to use\nthe default values: max_features=sqrt(n_features) for classification and max_fea\ntures=log2(n_features) for regression. Adding max_features or max_leaf_nodes\nmight sometimes improve performance. It can also drastically reduce space and time\nrequirements for training and prediction.\nGradient boosted regression trees (gradient boosting machines)\nThe gradient boosted regression tree is another ensemble method that combines mul‐\ntiple decision trees to create a more powerful model. Despite the “regression” in the\nname, these models can be used for regression and classification. In contrast to the\nrandom forest approach, gradient boosting works by building trees in a serial man‐\nner, where each tree tries to correct the mistakes of the previous one. By default, there\nis no randomization in gradient boosted regression trees; instead, strong pre-pruning\nis used. Gradient boosted trees often use very shallow trees, of depth one to five,\nwhich makes the model smaller in terms of memory and makes predictions faster.\n88 \n| \nChapter 2: Supervised Learning\nThe main idea behind gradient boosting is to combine many simple models (in this\ncontext known as weak learners), like shallow trees. Each tree can only provide good\npredictions on part of the data, and so more and more trees are added to iteratively\nimprove performance.\nGradient boosted trees are frequently the winning entries in machine learning com‐\npetitions, and are widely used in industry. They are generally a bit more sensitive to",
    "predictions on part of the data, and so more and more trees are added to iteratively\nimprove performance.\nGradient boosted trees are frequently the winning entries in machine learning com‐\npetitions, and are widely used in industry. They are generally a bit more sensitive to\nparameter settings than random forests, but can provide better accuracy if the param‐\neters are set correctly.\nApart from the pre-pruning and the number of trees in the ensemble, another impor‐\ntant parameter of gradient boosting is the learning_rate, which controls how\nstrongly each tree tries to correct the mistakes of the previous trees. A higher learning\nrate means each tree can make stronger corrections, allowing for more complex mod‐\nels. Adding more trees to the ensemble, which can be accomplished by increasing\nn_estimators, also increases the model complexity, as the model has more chances\nto correct mistakes on the training set.\nHere is an example of using GradientBoostingClassifier on the Breast Cancer\ndataset. By default, 100 trees of maximum depth 3 and a learning rate of 0.1 are used:\nIn[72]:\nfrom sklearn.ensemble import GradientBoostingClassifier\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[72]:\nAccuracy on training set: 1.000\nAccuracy on test set: 0.958\nAs the training set accuracy is 100%, we are likely to be overfitting. To reduce overfit‐\nting, we could either apply stronger pre-pruning by limiting the maximum depth or\nlower the learning rate:\nSupervised Machine Learning Algorithms \n| \n89\nIn[73]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))",
    "lower the learning rate:\nSupervised Machine Learning Algorithms \n| \n89\nIn[73]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[73]:\nAccuracy on training set: 0.991\nAccuracy on test set: 0.972\nIn[74]:\ngbrt = GradientBoostingClassifier(random_state=0, learning_rate=0.01)\ngbrt.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(gbrt.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(gbrt.score(X_test, y_test)))\nOut[74]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.965\nBoth methods of decreasing the model complexity reduced the training set accuracy,\nas expected. In this case, lowering the maximum depth of the trees provided a signifi‐\ncant improvement of the model, while lowering the learning rate only increased the\ngeneralization performance slightly.\nAs for the other decision tree–based models, we can again visualize the feature\nimportances to get more insight into our model (Figure 2-35). As we used 100 trees, it\nis impractical to inspect them all, even if they are all of depth 1:\nIn[75]:\ngbrt = GradientBoostingClassifier(random_state=0, max_depth=1)\ngbrt.fit(X_train, y_train)\nplot_feature_importances_cancer(gbrt)\n90 \n| \nChapter 2: Supervised Learning\nFigure 2-35. Feature importances computed from a gradient boosting classifier that was\nfit to the Breast Cancer dataset\nWe can see that the feature importances of the gradient boosted trees are somewhat\nsimilar to the feature importances of the random forests, though the gradient boost‐\ning completely ignored some of the features.\nAs both gradient boosting and random forests perform well on similar kinds of data,\na common approach is to first try random forests, which work quite robustly. If ran‐\ndom forests work well but prediction time is at a premium, or it is important to",
    "ing completely ignored some of the features.\nAs both gradient boosting and random forests perform well on similar kinds of data,\na common approach is to first try random forests, which work quite robustly. If ran‐\ndom forests work well but prediction time is at a premium, or it is important to\nsqueeze out the last percentage of accuracy from the machine learning model, mov‐\ning to gradient boosting often helps.\nIf you want to apply gradient boosting to a large-scale problem, it might be worth\nlooking into the xgboost package and its Python interface, which at the time of writ‐\ning is faster (and sometimes easier to tune) than the scikit-learn implementation of\ngradient boosting on many datasets.\nStrengths, weaknesses, and parameters.    Gradient boosted decision trees are among the\nmost powerful and widely used models for supervised learning. Their main drawback\nis that they require careful tuning of the parameters and may take a long time to\ntrain. Similarly to other tree-based models, the algorithm works well without scaling\nand on a mixture of binary and continuous features. As with other tree-based models,\nit also often does not work well on high-dimensional sparse data.\nThe main parameters of gradient boosted tree models are the number of trees, n_esti\nmators, and the learning_rate, which controls the degree to which each tree is\nallowed to correct the mistakes of the previous trees. These two parameters are highly\nSupervised Machine Learning Algorithms \n| \n91\ninterconnected, as a lower learning_rate means that more trees are needed to build\na model of similar complexity. In contrast to random forests, where a higher n_esti\nmators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.",
    "mators value is always better, increasing n_estimators in gradient boosting leads to a\nmore complex model, which may lead to overfitting. A common practice is to fit\nn_estimators depending on the time and memory budget, and then search over dif‐\nferent learning_rates.\nAnother important parameter is max_depth (or alternatively max_leaf_nodes), to\nreduce the complexity of each tree. Usually max_depth is set very low for gradient\nboosted models, often not deeper than five splits.\nKernelized Support Vector Machines\nThe next type of supervised model we will discuss is kernelized support vector\nmachines. We explored the use of linear support vector machines for classification in\n“Linear models for classification” on page 56. Kernelized support vector machines\n(often just referred to as SVMs) are an extension that allows for more complex mod‐\nels that are not defined simply by hyperplanes in the input space. While there are sup‐\nport vector machines for classification and regression, we will restrict ourselves to the\nclassification case, as implemented in SVC. Similar concepts apply to support vector\nregression, as implemented in SVR.\nThe math behind kernelized support vector machines is a bit involved, and is beyond\nthe scope of this book. You can find the details in Chapter 1 of Hastie, Tibshirani, and\nFriedman’s The Elements of Statistical Learning. However, we will try to give you some\nsense of the idea behind the method.\nLinear models and nonlinear features\nAs you saw in Figure 2-15, linear models can be quite limiting in low-dimensional\nspaces, as lines and hyperplanes have limited flexibility. One way to make a linear\nmodel more flexible is by adding more features—for example, by adding interactions\nor polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)",
    "or polynomials of the input features.\nLet’s look at the synthetic dataset we used in “Feature importance in trees” on page 77\n(see Figure 2-29):\nIn[76]:\nX, y = make_blobs(centers=4, random_state=8)\ny = y % 2\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n92 \n| \nChapter 2: Supervised Learning\n10 We picked this particular feature to add for illustration purposes. The choice is not particularly important.\nFigure 2-36. Two-class classification dataset in which classes are not linearly separable\nA linear model for classification can only separate points using a line, and will not be\nable to do a very good job on this dataset (see Figure 2-37):\nIn[77]:\nfrom sklearn.svm import LinearSVC\nlinear_svm = LinearSVC().fit(X, y)\nmglearn.plots.plot_2d_separator(linear_svm, X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nNow let’s expand the set of input features, say by also adding feature1 ** 2, the\nsquare of the second feature, as a new feature. Instead of representing each data point\nas a two-dimensional point, (feature0, feature1), we now represent it as a three-\ndimensional point, (feature0, feature1, feature1 ** 2).10 This new representa‐\ntion is illustrated in Figure 2-38 in a three-dimensional scatter plot:\nSupervised Machine Learning Algorithms \n| \n93\nFigure 2-37. Decision boundary found by a linear SVM\nIn[78]:\n# add the squared first feature\nX_new = np.hstack([X, X[:, 1:] ** 2])\nfrom mpl_toolkits.mplot3d import Axes3D, axes3d\nfigure = plt.figure()\n# visualize in 3D\nax = Axes3D(figure, elev=-152, azim=-26)\n# plot first all the points with y == 0, then all with y == 1\nmask = y == 0\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")",
    "ax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature1 ** 2\")\n94 \n| \nChapter 2: Supervised Learning\nFigure 2-38. Expansion of the dataset shown in Figure 2-37, created by adding a third\nfeature derived from feature1\nIn the new representation of the data, it is now indeed possible to separate the two\nclasses using a linear model, a plane in three dimensions. We can confirm this by fit‐\nting a linear model to the augmented data (see Figure 2-39):\nIn[79]:\nlinear_svm_3d = LinearSVC().fit(X_new, y)\ncoef, intercept = linear_svm_3d.coef_.ravel(), linear_svm_3d.intercept_\n# show linear decision boundary\nfigure = plt.figure()\nax = Axes3D(figure, elev=-152, azim=-26)\nxx = np.linspace(X_new[:, 0].min() - 2, X_new[:, 0].max() + 2, 50)\nyy = np.linspace(X_new[:, 1].min() - 2, X_new[:, 1].max() + 2, 50)\nXX, YY = np.meshgrid(xx, yy)\nZZ = (coef[0] * XX + coef[1] * YY + intercept) / -coef[2]\nax.plot_surface(XX, YY, ZZ, rstride=8, cstride=8, alpha=0.3)\nax.scatter(X_new[mask, 0], X_new[mask, 1], X_new[mask, 2], c='b',\n           cmap=mglearn.cm2, s=60)\nax.scatter(X_new[~mask, 0], X_new[~mask, 1], X_new[~mask, 2], c='r', marker='^',\n           cmap=mglearn.cm2, s=60)\nax.set_xlabel(\"feature0\")\nax.set_ylabel(\"feature1\")\nax.set_zlabel(\"feature0 ** 2\")\nSupervised Machine Learning Algorithms \n| \n95\nFigure 2-39. Decision boundary found by a linear SVM on the expanded three-\ndimensional dataset\nAs a function of the original features, the linear SVM model is not actually linear any‐\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\n(Figure 2-40):\nIn[80]:\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])",
    "As a function of the original features, the linear SVM model is not actually linear any‐\nmore. It is not a line, but more of an ellipse, as you can see from the plot created here\n(Figure 2-40):\nIn[80]:\nZZ = YY ** 2\ndec = linear_svm_3d.decision_function(np.c_[XX.ravel(), YY.ravel(), ZZ.ravel()])\nplt.contourf(XX, YY, dec.reshape(XX.shape), levels=[dec.min(), 0, dec.max()],\n             cmap=mglearn.cm2, alpha=0.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n96 \n| \nChapter 2: Supervised Learning\nFigure 2-40. The decision boundary from Figure 2-39 as a function of the original two\nfeatures\nThe kernel trick\nThe lesson here is that adding nonlinear features to the representation of our data can\nmake linear models much more powerful. However, often we don’t know which fea‐\ntures to add, and adding many features (like all possible interactions in a 100-\ndimensional feature space) might make computation very expensive. Luckily, there is\na clever mathematical trick that allows us to learn a classifier in a higher-dimensional\nspace without actually computing the new, possibly very large representation. This is\nknown as the kernel trick, and it works by directly computing the distance (more pre‐\ncisely, the scalar products) of the data points for the expanded feature representation,\nwithout ever actually computing the expansion.\nThere are two ways to map your data into a higher-dimensional space that are com‐\nmonly used with support vector machines: the polynomial kernel, which computes all\npossible polynomials up to a certain degree of the original features (like feature1 **\n2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\nSupervised Machine Learning Algorithms \n| \n97",
    "2 * feature2 ** 5); and the radial basis function (RBF) kernel, also known as the\nGaussian kernel. The Gaussian kernel is a bit harder to explain, as it corresponds to\nan infinite-dimensional feature space. One way to explain the Gaussian kernel is that\nSupervised Machine Learning Algorithms \n| \n97\n11 This follows from the Taylor expansion of the exponential map.\nit considers all possible polynomials of all degrees, but the importance of the features\ndecreases for higher degrees.11\nIn practice, the mathematical details behind the kernel SVM are not that important,\nthough, and how an SVM with an RBF kernel makes a decision can be summarized\nquite easily—we’ll do so in the next section.\nUnderstanding SVMs\nDuring training, the SVM learns how important each of the training data points is to\nrepresent the decision boundary between the two classes. Typically only a subset of\nthe training points matter for defining the decision boundary: the ones that lie on the\nborder between the classes. These are called support vectors and give the support vec‐\ntor machine its name.\nTo make a prediction for a new point, the distance to each of the support vectors is\nmeasured. A classification decision is made based on the distances to the support vec‐\ntor, and the importance of the support vectors that was learned during training\n(stored in the dual_coef_ attribute of SVC).\nThe distance between data points is measured by the Gaussian kernel:\nkrbf(x1, x2) = exp (ɣǁx1 - x2ǁ2)\nHere, x1 and x2 are data points, ǁ x1 - x2 ǁ denotes Euclidean distance, and ɣ (gamma)\nis a parameter that controls the width of the Gaussian kernel.\nFigure 2-41 shows the result of training a support vector machine on a two-\ndimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC",
    "dimensional two-class dataset. The decision boundary is shown in black, and the sup‐\nport vectors are larger points with the wide outline. The following code creates this\nplot by training an SVM on the forge dataset:\nIn[81]:\nfrom sklearn.svm import SVC\nX, y = mglearn.tools.make_handcrafted_dataset()\nsvm = SVC(kernel='rbf', C=10, gamma=0.1).fit(X, y)\nmglearn.plots.plot_2d_separator(svm, X, eps=.5)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\n# plot support vectors\nsv = svm.support_vectors_\n# class labels of support vectors are given by the sign of the dual coefficients\nsv_labels = svm.dual_coef_.ravel() > 0\nmglearn.discrete_scatter(sv[:, 0], sv[:, 1], sv_labels, s=15, markeredgewidth=3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n98 \n| \nChapter 2: Supervised Learning\nFigure 2-41. Decision boundary and support vectors found by an SVM with RBF kernel\nIn this case, the SVM yields a very smooth and nonlinear (not a straight line) bound‐\nary. We adjusted two parameters here: the C parameter and the gamma parameter,\nwhich we will now discuss in detail.\nTuning SVM parameters\nThe gamma parameter is the one shown in the formula given in the previous section,\nwhich controls the width of the Gaussian kernel. It determines the scale of what it\nmeans for points to be close together. The C parameter is a regularization parameter,\nsimilar to that used in the linear models. It limits the importance of each point (or\nmore precisely, their dual_coef_).\nLet’s have a look at what happens when we vary these parameters (Figure 2-42):\nIn[82]:\nfig, axes = plt.subplots(3, 3, figsize=(15, 10))\nfor ax, C in zip(axes, [-1, 0, 3]):\n    for a, gamma in zip(ax, range(-1, 2)):\n        mglearn.plots.plot_svm(log_C=C, log_gamma=gamma, ax=a)\naxes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n                  ncol=4, loc=(.9, 1.2))\nSupervised Machine Learning Algorithms \n| \n99\nFigure 2-42. Decision boundaries and support vectors for different settings of the param‐",
    "axes[0, 0].legend([\"class 0\", \"class 1\", \"sv class 0\", \"sv class 1\"],\n                  ncol=4, loc=(.9, 1.2))\nSupervised Machine Learning Algorithms \n| \n99\nFigure 2-42. Decision boundaries and support vectors for different settings of the param‐\neters C and gamma\nGoing from left to right, we increase the value of the parameter gamma from 0.1 to 10.\nA small gamma means a large radius for the Gaussian kernel, which means that many\npoints are considered close by. This is reflected in very smooth decision boundaries\non the left, and boundaries that focus more on single points further to the right. A\nlow value of gamma means that the decision boundary will vary slowly, which yields a\nmodel of low complexity, while a high value of gamma yields a more complex model.\nGoing from top to bottom, we increase the C parameter from 0.1 to 1000. As with the\nlinear models, a small C means a very restricted model, where each data point can\nonly have very limited influence. You can see that at the top left the decision bound‐\nary looks nearly linear, with the misclassified points barely having any influence on\nthe line. Increasing C, as shown on the bottom right, allows these points to have a\nstronger influence on the model and makes the decision boundary bend to correctly\nclassify them.\n100 \n| \nChapter 2: Supervised Learning\nLet’s apply the RBF kernel SVM to the Breast Cancer dataset. By default, C=1 and\ngamma=1/n_features:\nIn[83]:\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nsvc = SVC()\nsvc.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(svc.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(svc.score(X_test, y_test)))\nOut[83]:\nAccuracy on training set: 1.00\nAccuracy on test set: 0.63\nThe model overfits quite substantially, with a perfect score on the training set and\nonly 63% accuracy on the test set. While SVMs often perform quite well, they are",
    "Out[83]:\nAccuracy on training set: 1.00\nAccuracy on test set: 0.63\nThe model overfits quite substantially, with a perfect score on the training set and\nonly 63% accuracy on the test set. While SVMs often perform quite well, they are\nvery sensitive to the settings of the parameters and to the scaling of the data. In par‐\nticular, they require all the features to vary on a similar scale. Let’s look at the mini‐\nmum and maximum values for each feature, plotted in log-space (Figure 2-43):\nIn[84]:\nplt.plot(X_train.min(axis=0), 'o', label=\"min\")\nplt.plot(X_train.max(axis=0), '^', label=\"max\")\nplt.legend(loc=4)\nplt.xlabel(\"Feature index\")\nplt.ylabel(\"Feature magnitude\")\nplt.yscale(\"log\")\nFrom this plot we can determine that features in the Breast Cancer dataset are of\ncompletely different orders of magnitude. This can be somewhat of a problem for\nother models (like linear models), but it has devastating effects for the kernel SVM.\nLet’s examine some ways to deal with this issue.\nSupervised Machine Learning Algorithms \n| \n101\nFigure 2-43. Feature ranges for the Breast Cancer dataset (note that the y axis has a log‐\narithmic scale)\nPreprocessing data for SVMs\nOne way to resolve this problem is by rescaling each feature so that they are all\napproximately on the same scale. A common rescaling method for kernel SVMs is to\nscale the data such that all features are between 0 and 1. We will see how to do this\nusing the MinMaxScaler preprocessing method in Chapter 3, where we’ll give more\ndetails. For now, let’s do this “by hand”:\nIn[85]:\n# compute the minimum value per feature on the training set\nmin_on_training = X_train.min(axis=0)\n# compute the range of each feature (max - min) on the training set\nrange_on_training = (X_train - min_on_training).max(axis=0)\n# subtract the min, and divide by range\n# afterward, min=0 and max=1 for each feature\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))",
    "range_on_training = (X_train - min_on_training).max(axis=0)\n# subtract the min, and divide by range\n# afterward, min=0 and max=1 for each feature\nX_train_scaled = (X_train - min_on_training) / range_on_training\nprint(\"Minimum for each feature\\n{}\".format(X_train_scaled.min(axis=0)))\nprint(\"Maximum for each feature\\n {}\".format(X_train_scaled.max(axis=0)))\n102 \n| \nChapter 2: Supervised Learning\nOut[85]:\nMinimum for each feature\n[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\nMaximum for each feature\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\nIn[86]:\n# use THE SAME transformation on the test set,\n# using min and range of the training set (see Chapter 3 for details)\nX_test_scaled = (X_test - min_on_training) / range_on_training\nIn[87]:\nsvc = SVC()\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[87]:\nAccuracy on training set: 0.948\nAccuracy on test set: 0.951\nScaling the data made a huge difference! Now we are actually in an underfitting\nregime, where training and test set performance are quite similar but less close to\n100% accuracy. From here, we can try increasing either C or gamma to fit a more com‐\nplex model. For example:\nIn[88]:\nsvc = SVC(C=1000)\nsvc.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    svc.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(svc.score(X_test_scaled, y_test)))\nOut[88]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\naccuracy.\nSupervised Machine Learning Algorithms \n| \n103\nStrengths, weaknesses, and parameters",
    "Out[88]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nHere, increasing C allows us to improve the model significantly, resulting in 97.2%\naccuracy.\nSupervised Machine Learning Algorithms \n| \n103\nStrengths, weaknesses, and parameters\nKernelized support vector machines are powerful models and perform well on a vari‐\nety of datasets. SVMs allow for complex decision boundaries, even if the data has only\na few features. They work well on low-dimensional and high-dimensional data (i.e.,\nfew and many features), but don’t scale very well with the number of samples. Run‐\nning an SVM on data with up to 10,000 samples might work well, but working with\ndatasets of size 100,000 or more can become challenging in terms of runtime and\nmemory usage.\nAnother downside of SVMs is that they require careful preprocessing of the data and\ntuning of the parameters. This is why, these days, most people instead use tree-based\nmodels such as random forests or gradient boosting (which require little or no pre‐\nprocessing) in many applications. Furthermore, SVM models are hard to inspect; it\ncan be difficult to understand why a particular prediction was made, and it might be\ntricky to explain the model to a nonexpert.\nStill, it might be worth trying SVMs, particularly if all of your features represent\nmeasurements in similar units (e.g., all are pixel intensities) and they are on similar\nscales.\nThe important parameters in kernel SVMs are the regularization parameter C, the\nchoice of the kernel, and the kernel-specific parameters. Although we primarily\nfocused on the RBF kernel, other choices are available in scikit-learn. The RBF\nkernel has only one parameter, gamma, which is the inverse of the width of the Gaus‐\nsian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.",
    "sian kernel. gamma and C both control the complexity of the model, with large values\nin either resulting in a more complex model. Therefore, good settings for the two\nparameters are usually strongly correlated, and C and gamma should be adjusted\ntogether.\nNeural Networks (Deep Learning)\nA family of algorithms known as neural networks has recently seen a revival under\nthe name “deep learning.” While deep learning shows great promise in many machine\nlearning applications, deep learning algorithms are often tailored very carefully to a\nspecific use case. Here, we will only discuss some relatively simple methods, namely\nmultilayer perceptrons for classification and regression, that can serve as a starting\npoint for more involved deep learning methods. Multilayer perceptrons (MLPs) are\nalso known as (vanilla) feed-forward neural networks, or sometimes just neural\nnetworks.\nThe neural network model\nMLPs can be viewed as generalizations of linear models that perform multiple stages\nof processing to come to a decision.\n104 \n| \nChapter 2: Supervised Learning\nRemember that the prediction by a linear regressor is given as:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b\nIn plain English, ŷ is a weighted sum of the input features x[0] to x[p], weighted by\nthe learned coefficients w[0] to w[p]. We could visualize this graphically as shown in\nFigure 2-44:\nIn[89]:\ndisplay(mglearn.plots.plot_logistic_regression_graph())\nFigure 2-44. Visualization of logistic regression, where input features and predictions are\nshown as nodes, and the coefficients are connections between the nodes\nHere, each node on the left represents an input feature, the connecting lines represent\nthe learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are",
    "the learned coefficients, and the node on the right represents the output, which is a\nweighted sum of the inputs.\nIn an MLP this process of computing weighted sums is repeated multiple times, first\ncomputing hidden units that represent an intermediate processing step, which are\nagain combined using weighted sums to yield the final result (Figure 2-45):\nIn[90]:\ndisplay(mglearn.plots.plot_single_hidden_layer_graph())\nSupervised Machine Learning Algorithms \n| \n105\nFigure 2-45. Illustration of a multilayer perceptron with a single hidden layer\nThis model has a lot more coefficients (also called weights) to learn: there is one\nbetween every input and every hidden unit (which make up the hidden layer), and\none between every unit in the hidden layer and the output.\nComputing a series of weighted sums is mathematically the same as computing just\none weighted sum, so to make this model truly more powerful than a linear model,\nwe need one extra trick. After computing a weighted sum for each hidden unit, a\nnonlinear function is applied to the result—usually the rectifying nonlinearity (also\nknown as rectified linear unit or relu) or the tangens hyperbolicus (tanh). The result of\nthis function is then used in the weighted sum that computes the output, ŷ. The two\nfunctions are visualized in Figure 2-46. The relu cuts off values below zero, while tanh\nsaturates to –1 for low input values and +1 for high input values. Either nonlinear\nfunction allows the neural network to learn much more complicated functions than a\nlinear model could:\nIn[91]:\nline = np.linspace(-3, 3, 100)\nplt.plot(line, np.tanh(line), label=\"tanh\")\nplt.plot(line, np.maximum(line, 0), label=\"relu\")\nplt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\n106 \n| \nChapter 2: Supervised Learning\nFigure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\ntion function\nFor the small neural network pictured in Figure 2-45, the full formula for computing",
    "plt.legend(loc=\"best\")\nplt.xlabel(\"x\")\nplt.ylabel(\"relu(x), tanh(x)\")\n106 \n| \nChapter 2: Supervised Learning\nFigure 2-46. The hyperbolic tangent activation function and the rectified linear activa‐\ntion function\nFor the small neural network pictured in Figure 2-45, the full formula for computing\nŷ in the case of regression would be (when using a tanh nonlinearity):\nh[0] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[1] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nh[2] = tanh(w[0, 0] * x[0] + w[1, 0] * x[1] + w[2, 0] * x[2] + w[3, 0] * x[3])\nŷ = v[0] * h[0] + v[1] * h[1] + v[2] * h[2]\nHere, w are the weights between the input x and the hidden layer h, and v are the\nweights between the hidden layer h and the output ŷ. The weights v and w are learned\nfrom data, x are the input features, ŷ is the computed output, and h are intermediate\ncomputations. An important parameter that needs to be set by the user is the number\nof nodes in the hidden layer. This can be as small as 10 for very small or simple data‐\nsets and as big as 10,000 for very complex data. It is also possible to add additional\nhidden layers, as shown in Figure 2-47:\nSupervised Machine Learning Algorithms \n| \n107\nIn[92]:\nmglearn.plots.plot_two_hidden_layer_graph()\nFigure 2-47. A multilayer perceptron with two hidden layers\nHaving large neural networks made up of many of these layers of computation is\nwhat inspired the term “deep learning.”\nTuning neural networks\nLet’s look into the workings of the MLP by applying the MLPClassifier to the\ntwo_moons dataset we used earlier in this chapter. The results are shown in\nFigure 2-48:\nIn[93]:\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)",
    "In[93]:\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=100, noise=0.25, random_state=3)\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                    random_state=42)\nmlp = MLPClassifier(algorithm='l-bfgs', random_state=0).fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n108 \n| \nChapter 2: Supervised Learning\nFigure 2-48. Decision boundary learned by a neural network with 100 hidden units on\nthe two_moons dataset\nAs you can see, the neural network learned a very nonlinear but relatively smooth\ndecision boundary. We used algorithm='l-bfgs', which we will discuss later.\nBy default, the MLP uses 100 hidden nodes, which is quite a lot for this small dataset.\nWe can reduce the number (which reduces the complexity of the model) and still get\na good result (Figure 2-49):\nIn[94]:\nmlp = MLPClassifier(algorithm='l-bfgs', random_state=0, hidden_layer_sizes=[10])\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nSupervised Machine Learning Algorithms \n| \n109\nFigure 2-49. Decision boundary learned by a neural network with 10 hidden units on\nthe two_moons dataset\nWith only 10 hidden units, the decision boundary looks somewhat more ragged. The\ndefault nonlinearity is relu, shown in Figure 2-46. With a single hidden layer, this\nmeans the decision function will be made up of 10 straight line segments. If we want\na smoother decision boundary, we could add more hidden units (as in Figure 2-49),\nadd a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\nIn[95]:\n# using two hidden layers, with 10 units each",
    "a smoother decision boundary, we could add more hidden units (as in Figure 2-49),\nadd a second hidden layer (Figure 2-50), or use the tanh nonlinearity (Figure 2-51):\nIn[95]:\n# using two hidden layers, with 10 units each\nmlp = MLPClassifier(algorithm='l-bfgs', random_state=0,\n                    hidden_layer_sizes=[10, 10])\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\n110 \n| \nChapter 2: Supervised Learning\nIn[96]:\n# using two hidden layers, with 10 units each, now with tanh nonlinearity\nmlp = MLPClassifier(algorithm='l-bfgs', activation='tanh',\n                    random_state=0, hidden_layer_sizes=[10, 10])\nmlp.fit(X_train, y_train)\nmglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3)\nmglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 2-50. Decision boundary learned using 2 hidden layers with 10 hidden units\neach, with rect activation function\nSupervised Machine Learning Algorithms \n| \n111\nFigure 2-51. Decision boundary learned using 2 hidden layers with 10 hidden units\neach, with tanh activation function\nFinally, we can also control the complexity of a neural network by using an l2 penalty\nto shrink the weights toward zero, as we did in ridge regression and the linear classifi‐\ners. The parameter for this in the MLPClassifier is alpha (as in the linear regression\nmodels), and it’s set to a very low value (little regularization) by default. Figure 2-52\nshows the effect of different values of alpha on the two_moons dataset, using two hid‐\nden layers of 10 or 100 units each:\nIn[97]:\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n        mlp = MLPClassifier(algorithm='l-bfgs', random_state=0,",
    "den layers of 10 or 100 units each:\nIn[97]:\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor axx, n_hidden_nodes in zip(axes, [10, 100]):\n    for ax, alpha in zip(axx, [0.0001, 0.01, 0.1, 1]):\n        mlp = MLPClassifier(algorithm='l-bfgs', random_state=0,\n                            hidden_layer_sizes=[n_hidden_nodes, n_hidden_nodes],\n                            alpha=alpha)\n        mlp.fit(X_train, y_train)\n        mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n        mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\n        ax.set_title(\"n_hidden=[{}, {}]\\nalpha={:.4f}\".format(\n                      n_hidden_nodes, n_hidden_nodes, alpha))\n112 \n| \nChapter 2: Supervised Learning\nFigure 2-52. Decision functions for different numbers of hidden units and different set‐\ntings of the alpha parameter\nAs you probably have realized by now, there are many ways to control the complexity\nof a neural network: the number of hidden layers, the number of units in each hidden\nlayer, and the regularization (alpha). There are actually even more, which we won’t\ngo into here.\nAn important property of neural networks is that their weights are set randomly\nbefore learning is started, and this random initialization affects the model that is\nlearned. That means that even when using exactly the same parameters, we can\nobtain very different models when using different random seeds. If the networks are\nlarge, and their complexity is chosen properly, this should not affect accuracy too\nmuch, but it is worth keeping in mind (particularly for smaller networks).\nFigure 2-53 shows plots of several models, all learned with the same settings of the\nparameters:\nIn[98]:\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor i, ax in enumerate(axes.ravel()):\n    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\n                        hidden_layer_sizes=[100, 100])\n    mlp.fit(X_train, y_train)",
    "parameters:\nIn[98]:\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\nfor i, ax in enumerate(axes.ravel()):\n    mlp = MLPClassifier(algorithm='l-bfgs', random_state=i,\n                        hidden_layer_sizes=[100, 100])\n    mlp.fit(X_train, y_train)\n    mglearn.plots.plot_2d_separator(mlp, X_train, fill=True, alpha=.3, ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train, ax=ax)\nSupervised Machine Learning Algorithms \n| \n113\nFigure 2-53. Decision functions learned with the same parameters but different random\ninitializations\nTo get a better understanding of neural networks on real-world data, let’s apply the\nMLPClassifier to the Breast Cancer dataset. We start with the default parameters:\nIn[99]:\nprint(\"Cancer data per-feature maxima:\\n{}\".format(cancer.data.max(axis=0)))\nOut[99]:\nCancer data per-feature maxima:\n[   28.110    39.280   188.500  2501.000     0.163     0.345     0.427\n     0.201     0.304     0.097     2.873     4.885    21.980   542.200\n     0.031     0.135     0.396     0.053     0.079     0.030    36.040\n    49.540   251.200  4254.000     0.223     1.058     1.252     0.291\n     0.664     0.207]\nIn[100]:\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\nmlp = MLPClassifier(random_state=42)\nmlp.fit(X_train, y_train)\nprint(\"Accuracy on training set: {:.2f}\".format(mlp.score(X_train, y_train)))\nprint(\"Accuracy on test set: {:.2f}\".format(mlp.score(X_test, y_test)))\nOut[100]:\nAccuracy on training set: 0.92\nAccuracy on test set: 0.90\nThe accuracy of the MLP is quite good, but not as good as the other models. As in the\nearlier SVC example, this is likely due to scaling of the data. Neural networks also\nexpect all input features to vary in a similar way, and ideally to have a mean of 0, and\n114 \n| \nChapter 2: Supervised Learning\na variance of 1. We must rescale our data so that it fulfills these requirements. Again,",
    "expect all input features to vary in a similar way, and ideally to have a mean of 0, and\n114 \n| \nChapter 2: Supervised Learning\na variance of 1. We must rescale our data so that it fulfills these requirements. Again,\nwe will do this by hand here, but we’ll introduce the StandardScaler to do this auto‐\nmatically in Chapter 3:\nIn[101]:\n# compute the mean value per feature on the training set\nmean_on_train = X_train.mean(axis=0)\n# compute the standard deviation of each feature on the training set\nstd_on_train = X_train.std(axis=0)\n# subtract the mean, and scale by inverse standard deviation\n# afterward, mean=0 and std=1\nX_train_scaled = (X_train - mean_on_train) / std_on_train\n# use THE SAME transformation (using training mean and std) on the test set\nX_test_scaled = (X_test - mean_on_train) / std_on_train\nmlp = MLPClassifier(random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\nOut[101]:\nAccuracy on training set: 0.991\nAccuracy on test set: 0.965\nConvergenceWarning:\n    Stochastic Optimizer: Maximum iterations reached and the optimization\n    hasn't converged yet.\nThe results are much better after scaling, and already quite competitive. We got a\nwarning from the model, though, that tells us that the maximum number of iterations\nhas been reached. This is part of the adam algorithm for learning the model, and tells\nus that we should increase the number of iterations:\nIn[102]:\nmlp = MLPClassifier(max_iter=1000, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\nOut[102]:\nAccuracy on training set: 0.995\nAccuracy on test set: 0.965\nSupervised Machine Learning Algorithms \n| \n115",
    "print(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\nOut[102]:\nAccuracy on training set: 0.995\nAccuracy on test set: 0.965\nSupervised Machine Learning Algorithms \n| \n115\n12 You might have noticed at this point that many of the well-performing models achieved exactly the same\naccuracy of 0.972. This means that all of the models make exactly the same number of mistakes, which is four.\nIf you compare the actual predictions, you can even see that they make exactly the same mistakes! This might\nbe a consequence of the dataset being very small, or it may be because these points are really different from\nthe rest.\nIncreasing the number of iterations only increased the training set performance, not\nthe generalization performance. Still, the model is performing quite well. As there is\nsome gap between the training and the test performance, we might try to decrease the\nmodel’s complexity to get better generalization performance. Here, we choose to\nincrease the alpha parameter (quite aggressively, from 0.0001 to 1) to add stronger\nregularization of the weights:\nIn[103]:\nmlp = MLPClassifier(max_iter=1000, alpha=1, random_state=0)\nmlp.fit(X_train_scaled, y_train)\nprint(\"Accuracy on training set: {:.3f}\".format(\n    mlp.score(X_train_scaled, y_train)))\nprint(\"Accuracy on test set: {:.3f}\".format(mlp.score(X_test_scaled, y_test)))\nOut[103]:\nAccuracy on training set: 0.988\nAccuracy on test set: 0.972\nThis leads to a performance on par with the best models so far.12\nWhile it is possible to analyze what a neural network has learned, this is usually much\ntrickier than analyzing a linear model or a tree-based model. One way to introspect\nwhat was learned is to look at the weights in the model. You can see an example of\nthis in the scikit-learn example gallery. For the Breast Cancer dataset, this might",
    "trickier than analyzing a linear model or a tree-based model. One way to introspect\nwhat was learned is to look at the weights in the model. You can see an example of\nthis in the scikit-learn example gallery. For the Breast Cancer dataset, this might\nbe a bit hard to understand. The following plot (Figure 2-54) shows the weights that\nwere learned connecting the input to the first hidden layer. The rows in this plot cor‐\nrespond to the 30 input features, while the columns correspond to the 100 hidden\nunits. Light colors represent large positive values, while dark colors represent nega‐\ntive values:\nIn[104]:\nplt.figure(figsize=(20, 5))\nplt.imshow(mlp.coefs_[0], interpolation='none', cmap='viridis')\nplt.yticks(range(30), cancer.feature_names)\nplt.xlabel(\"Columns in weight matrix\")\nplt.ylabel(\"Input feature\")\nplt.colorbar()\n116 \n| \nChapter 2: Supervised Learning\nFigure 2-54. Heat map of the first layer weights in a neural network learned on the\nBreast Cancer dataset\nOne possible inference we can make is that features that have very small weights for\nall of the hidden units are “less important” to the model. We can see that “mean\nsmoothness” and “mean compactness,” in addition to the features found between\n“smoothness error” and “fractal dimension error,” have relatively low weights com‐\npared to other features. This could mean that these are less important features or pos‐\nsibly that we didn’t represent them in a way that the neural network could use.\nWe could also visualize the weights connecting the hidden layer to the output layer,\nbut those are even harder to interpret.\nWhile the MLPClassifier and MLPRegressor provide easy-to-use interfaces for the\nmost common neural network architectures, they only capture a small subset of what\nis possible with neural networks. If you are interested in working with more flexible\nor larger models, we encourage you to look beyond scikit-learn into the fantastic",
    "most common neural network architectures, they only capture a small subset of what\nis possible with neural networks. If you are interested in working with more flexible\nor larger models, we encourage you to look beyond scikit-learn into the fantastic\ndeep learning libraries that are out there. For Python users, the most well-established\nare keras, lasagna, and tensor-flow. lasagna builds on the theano library, while\nkeras can use either tensor-flow or theano. These libraries provide a much more\nflexible interface to build neural networks and track the rapid progress in deep learn‐\ning research. All of the popular deep learning libraries also allow the use of high-\nperformance graphics processing units (GPUs), which scikit-learn does not\nsupport. Using GPUs allows us to accelerate computations by factors of 10x to 100x,\nand they are essential for applying deep learning methods to large-scale datasets.\nStrengths, weaknesses, and parameters\nNeural networks have reemerged as state-of-the-art models in many applications of\nmachine learning. One of their main advantages is that they are able to capture infor‐\nmation contained in large amounts of data and build incredibly complex models.\nGiven enough computation time, data, and careful tuning of the parameters, neural\nnetworks often beat other machine learning algorithms (for classification and regres‐\nsion tasks).\nSupervised Machine Learning Algorithms \n| \n117\nThis brings us to the downsides. Neural networks—particularly the large and power‐\nful ones—often take a long time to train. They also require careful preprocessing of\nthe data, as we saw here. Similarly to SVMs, they work best with “homogeneous”\ndata, where all the features have similar meanings. For data that has very different\nkinds of features, tree-based models might work better. Tuning neural network\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐",
    "data, where all the features have similar meanings. For data that has very different\nkinds of features, tree-based models might work better. Tuning neural network\nparameters is also an art unto itself. In our experiments, we barely scratched the sur‐\nface of possible ways to adjust neural network models and how to train them.\nEstimating complexity in neural networks.    The most important parameters are the num‐\nber of layers and the number of hidden units per layer. You should start with one or\ntwo hidden layers, and possibly expand from there. The number of nodes per hidden\nlayer is often similar to the number of input features, but rarely higher than in the low\nto mid-thousands.\nA helpful measure when thinking about the model complexity of a neural network is\nthe number of weights or coefficients that are learned. If you have a binary classifica‐\ntion dataset with 100 features, and you have 100 hidden units, then there are 100 *\n100 = 10,000 weights between the input and the first hidden layer. There are also\n100 * 1 = 100 weights between the hidden layer and the output layer, for a total of\naround 10,100 weights. If you add a second hidden layer with 100 hidden units, there\nwill be another 100 * 100 = 10,000 weights from the first hidden layer to the second\nhidden layer, resulting in a total of 20,100 weights. If instead you use one layer with\n1,000 hidden units, you are learning 100 * 1,000 = 100,000 weights from the input to\nthe hidden layer and 1,000 x 1 weights from the hidden layer to the output layer, for a\ntotal of 101,000. If you add a second hidden layer you add 1,000 * 1,000 = 1,000,000\nweights, for a whopping total of 1,101,000—50 times larger than the model with two\nhidden layers of size 100.\nA common way to adjust parameters in a neural network is to first create a network\nthat is large enough to overfit, making sure that the task can actually be learned by\nthe network. Then, once you know the training data can be learned, either shrink the",
    "hidden layers of size 100.\nA common way to adjust parameters in a neural network is to first create a network\nthat is large enough to overfit, making sure that the task can actually be learned by\nthe network. Then, once you know the training data can be learned, either shrink the\nnetwork or increase alpha to add regularization, which will improve generalization\nperformance.\nIn our experiments, we focused mostly on the definition of the model: the number of\nlayers and nodes per layer, the regularization, and the nonlinearity. These define the\nmodel we want to learn. There is also the question of how to learn the model, or the\nalgorithm that is used for learning the parameters, which is set using the algorithm\nparameter. There are two easy-to-use choices for algorithm. The default is 'adam',\nwhich works well in most situations but is quite sensitive to the scaling of the data (so\nit is important to always scale your data to 0 mean and unit variance). The other one\nis 'l-bfgs', which is quite robust but might take a long time on larger models or\nlarger datasets. There is also the more advanced 'sgd' option, which is what many\ndeep learning researchers use. The 'sgd' option comes with many additional param‐\n118 \n| \nChapter 2: Supervised Learning\neters that need to be tuned for best results. You can find all of these parameters and\ntheir definitions in the user guide. When starting to work with MLPs, we recommend\nsticking to 'adam' and 'l-bfgs'.\nfit Resets a Model\nAn important property of scikit-learn models is that calling fit\nwill always reset everything a model previously learned. So if you\nbuild a model on one dataset, and then call fit again on a different\ndataset, the model will “forget” everything it learned from the first\ndataset. You can call fit as often as you like on a model, and the\noutcome will be the same as calling fit on a “new” model.\nUncertainty Estimates from Classifiers",
    "dataset, the model will “forget” everything it learned from the first\ndataset. You can call fit as often as you like on a model, and the\noutcome will be the same as calling fit on a “new” model.\nUncertainty Estimates from Classifiers\nAnother useful part of the scikit-learn interface that we haven’t talked about yet is\nthe ability of classifiers to provide uncertainty estimates of predictions. Often, you are\nnot only interested in which class a classifier predicts for a certain test point, but also\nhow certain it is that this is the right class. In practice, different kinds of mistakes lead\nto very different outcomes in real-world applications. Imagine a medical application\ntesting for cancer. Making a false positive prediction might lead to a patient undergo‐\ning additional tests, while a false negative prediction might lead to a serious disease\nnot being treated. We will go into this topic in more detail in Chapter 6.\nThere are two different functions in scikit-learn that can be used to obtain uncer‐\ntainty estimates from classifiers: decision_function and predict_proba. Most (but\nnot all) classifiers have at least one of them, and many classifiers have both. Let’s look\nat what these two functions do on a synthetic two-dimensional dataset, when build‐\ning a GradientBoostingClassifier classifier, which has both a decision_function\nand a predict_proba method:\nIn[105]:\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import make_blobs, make_circles\nX, y = make_circles(noise=0.25, factor=0.5, random_state=1)\n# we rename the classes \"blue\" and \"red\" for illustration purposes\ny_named = np.array([\"blue\", \"red\"])[y]\n# we can call train_test_split with arbitrarily many arrays;\n# all will be split in a consistent manner\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n# build the gradient boosting model\ngbrt = GradientBoostingClassifier(random_state=0)",
    "# all will be split in a consistent manner\nX_train, X_test, y_train_named, y_test_named, y_train, y_test = \\\n    train_test_split(X, y_named, y, random_state=0)\n# build the gradient boosting model\ngbrt = GradientBoostingClassifier(random_state=0)\ngbrt.fit(X_train, y_train_named)\nUncertainty Estimates from Classifiers \n| \n119\nThe Decision Function\nIn the binary classification case, the return value of decision_function is of shape\n(n_samples,), and it returns one floating-point number for each sample:\nIn[106]:\nprint(\"X_test.shape: {}\".format(X_test.shape))\nprint(\"Decision function shape: {}\".format(\n    gbrt.decision_function(X_test).shape))\nOut[106]:\nX_test.shape: (25, 2)\nDecision function shape: (25,)\nThis value encodes how strongly the model believes a data point to belong to the\n“positive” class, in this case class 1. Positive values indicate a preference for the posi‐\ntive class, and negative values indicate a preference for the “negative” (other) class:\nIn[107]:\n# show the first few entries of decision_function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6]))\nOut[107]:\nDecision function:\n[ 4.136 -1.683 -3.951 -3.626  4.29   3.662]\nWe can recover the prediction by looking only at the sign of the decision function:\nIn[108]:\nprint(\"Thresholded decision function:\\n{}\".format(\n    gbrt.decision_function(X_test) > 0))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[108]:\nThresholded decision function:\n[ True False False False  True  True False  True  True  True False  True\n  True False  True False False False  True  True  True  True  True False\n  False]\nPredictions:\n['red' 'blue' 'blue' 'blue' 'red' 'red' 'blue' 'red' 'red' 'red' 'blue'\n 'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\nFor binary classification, the “negative” class is always the first entry of the classes_\nattribute, and the “positive” class is the second entry of classes_. So if you want to",
    "'red' 'red' 'blue' 'red' 'blue' 'blue' 'blue' 'red' 'red' 'red' 'red'\n 'red' 'blue' 'blue']\nFor binary classification, the “negative” class is always the first entry of the classes_\nattribute, and the “positive” class is the second entry of classes_. So if you want to\nfully recover the output of predict, you need to make use of the classes_ attribute:\n120 \n| \nChapter 2: Supervised Learning\nIn[109]:\n# make the boolean True/False into 0 and 1\ngreater_zero = (gbrt.decision_function(X_test) > 0).astype(int)\n# use 0 and 1 as indices into classes_\npred = gbrt.classes_[greater_zero]\n# pred is the same as the output of gbrt.predict\nprint(\"pred is equal to predictions: {}\".format(\n    np.all(pred == gbrt.predict(X_test))))\nOut[109]:\npred is equal to predictions: True\nThe range of decision_function can be arbitrary, and depends on the data and the\nmodel parameters:\nIn[110]:\ndecision_function = gbrt.decision_function(X_test)\nprint(\"Decision function minimum: {:.2f} maximum: {:.2f}\".format(\n    np.min(decision_function), np.max(decision_function)))\nOut[110]:\nDecision function minimum: -7.69 maximum: 4.29\nThis arbitrary scaling makes the output of decision_function often hard to\ninterpret.\nIn the following example we plot the decision_function for all points in the 2D\nplane using a color coding, next to a visualization of the decision boundary, as we saw\nearlier. We show training points as circles and test data as triangles (Figure 2-55):\nIn[111]:\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nmglearn.tools.plot_2d_separator(gbrt, X, ax=axes[0], alpha=.4,\n                                fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(gbrt, X, ax=axes[1],\n                                            alpha=.4, cm=mglearn.ReBl)\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,",
    "for ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\nUncertainty Estimates from Classifiers \n| \n121\nFigure 2-55. Decision boundary (left) and decision function (right) for a gradient boost‐\ning model on a two-dimensional toy dataset\nEncoding not only the predicted outcome but also how certain the classifier is pro‐\nvides additional information. However, in this visualization, it is hard to make out the\nboundary between the two classes.\nPredicting Probabilities\nThe output of predict_proba is a probability for each class, and is often more easily\nunderstood than the output of decision_function. It is always of shape (n_samples,\n2) for binary classification:\nIn[112]:\nprint(\"Shape of probabilities: {}\".format(gbrt.predict_proba(X_test).shape))\nOut[112]:\nShape of probabilities: (25, 2)\nThe first entry in each row is the estimated probability of the first class, and the sec‐\nond entry is the estimated probability of the second class. Because it is a probability,\nthe output of predict_proba is always between 0 and 1, and the sum of the entries\nfor both classes is always 1:\nIn[113]:\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(\n    gbrt.predict_proba(X_test[:6])))\n122 \n| \nChapter 2: Supervised Learning\n13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\never, if that happens, the prediction is made at random.\nOut[113]:\nPredicted probabilities:\n[[ 0.016  0.984]\n [ 0.843  0.157]\n [ 0.981  0.019]\n [ 0.974  0.026]",
    "13 Because the probabilities are floating-point numbers, it is unlikely that they will both be exactly 0.500. How‐\never, if that happens, the prediction is made at random.\nOut[113]:\nPredicted probabilities:\n[[ 0.016  0.984]\n [ 0.843  0.157]\n [ 0.981  0.019]\n [ 0.974  0.026]\n [ 0.014  0.986]\n [ 0.025  0.975]]\nBecause the probabilities for the two classes sum to 1, exactly one of the classes will\nbe above 50% certainty. That class is the one that is predicted.13\nYou can see in the previous output that the classifier is relatively certain for most\npoints. How well the uncertainty actually reflects uncertainty in the data depends on\nthe model and the parameters. A model that is more overfitted tends to make more\ncertain predictions, even if they might be wrong. A model with less complexity usu‐\nally has more uncertainty in its predictions. A model is called calibrated if the\nreported uncertainty actually matches how correct it is—in a calibrated model, a pre‐\ndiction made with 70% certainty would be correct 70% of the time.\nIn the following example (Figure 2-56) we again show the decision boundary on the\ndataset, next to the class probabilities for the class 1:\nIn[114]:\nfig, axes = plt.subplots(1, 2, figsize=(13, 5))\nmglearn.tools.plot_2d_separator(\n    gbrt, X, ax=axes[0], alpha=.4, fill=True, cm=mglearn.cm2)\nscores_image = mglearn.tools.plot_2d_scores(\n    gbrt, X, ax=axes[1], alpha=.5, cm=mglearn.ReBl, function='predict_proba')\nfor ax in axes:\n    # plot training and test points\n    mglearn.discrete_scatter(X_test[:, 0], X_test[:, 1], y_test,\n                             markers='^', ax=ax)\n    mglearn.discrete_scatter(X_train[:, 0], X_train[:, 1], y_train,\n                             markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))",
    "markers='o', ax=ax)\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\ncbar = plt.colorbar(scores_image, ax=axes.tolist())\naxes[0].legend([\"Test class 0\", \"Test class 1\", \"Train class 0\",\n                \"Train class 1\"], ncol=4, loc=(.1, 1.1))\nUncertainty Estimates from Classifiers \n| \n123\nFigure 2-56. Decision boundary (left) and predicted probabilities for the gradient boost‐\ning model shown in Figure 2-55\nThe boundaries in this plot are much more well-defined, and the small areas of\nuncertainty are clearly visible.\nThe scikit-learn website has a great comparison of many models and what their\nuncertainty estimates look like. We’ve reproduced this in Figure 2-57, and we encour‐\nage you to go though the example there.\nFigure 2-57. Comparison of several classifiers in scikit-learn on synthetic datasets (image\ncourtesy http://scikit-learn.org)\nUncertainty in Multiclass Classification\nSo far, we’ve only talked about uncertainty estimates in binary classification. But the\ndecision_function and predict_proba methods also work in the multiclass setting.\nLet’s apply them on the Iris dataset, which is a three-class classification dataset:\n124 \n| \nChapter 2: Supervised Learning\nIn[115]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=42)\ngbrt = GradientBoostingClassifier(learning_rate=0.01, random_state=0)\ngbrt.fit(X_train, y_train)\nIn[116]:\nprint(\"Decision function shape: {}\".format(gbrt.decision_function(X_test).shape))\n# plot the first few entries of the decision function\nprint(\"Decision function:\\n{}\".format(gbrt.decision_function(X_test)[:6, :]))\nOut[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,",
    "Out[116]:\nDecision function shape: (38, 3)\nDecision function:\n[[-0.529  1.466 -0.504]\n [ 1.512 -0.496 -0.503]\n [-0.524 -0.468  1.52 ]\n [-0.529  1.466 -0.504]\n [-0.531  1.282  0.215]\n [ 1.512 -0.496 -0.503]]\nIn the multiclass case, the decision_function has the shape (n_samples,\nn_classes) and each column provides a “certainty score” for each class, where a large\nscore means that a class is more likely and a small score means the class is less likely.\nYou can recover the predictions from these scores by finding the maximum entry for\neach data point:\nIn[117]:\nprint(\"Argmax of decision function:\\n{}\".format(\n      np.argmax(gbrt.decision_function(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[117]:\nArgmax of decision function:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nThe output of predict_proba has the same shape, (n_samples, n_classes). Again,\nthe probabilities for the possible classes for each data point sum to 1:\nUncertainty Estimates from Classifiers \n| \n125\nIn[118]:\n# show the first few entries of predict_proba\nprint(\"Predicted probabilities:\\n{}\".format(gbrt.predict_proba(X_test)[:6]))\n# show that sums across rows are one\nprint(\"Sums: {}\".format(gbrt.predict_proba(X_test)[:6].sum(axis=1)))\nOut[118]:\nPredicted probabilities:\n[[ 0.107  0.784  0.109]\n [ 0.789  0.106  0.105]\n [ 0.102  0.108  0.789]\n [ 0.107  0.784  0.109]\n [ 0.108  0.663  0.228]\n [ 0.789  0.106  0.105]]\nSums: [ 1.  1.  1.  1.  1.  1.]\nWe can again recover the predictions by computing the argmax of predict_proba:\nIn[119]:\nprint(\"Argmax of predicted probabilities:\\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[119]:\nArgmax of predicted probabilities:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:",
    "print(\"Argmax of predicted probabilities:\\n{}\".format(\n    np.argmax(gbrt.predict_proba(X_test), axis=1)))\nprint(\"Predictions:\\n{}\".format(gbrt.predict(X_test)))\nOut[119]:\nArgmax of predicted probabilities:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nPredictions:\n[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0 0 0 1 0 0 2 1 0]\nTo summarize, predict_proba and decision_function always have shape (n_sam\nples, n_classes)—apart from decision_function in the special binary case. In the\nbinary case, decision_function only has one column, corresponding to the “posi‐\ntive” class classes_[1]. This is mostly for historical reasons.\nYou can recover the prediction when there are n_classes many columns by comput‐\ning the argmax across columns. Be careful, though, if your classes are strings, or you\nuse integers but they are not consecutive and starting from 0. If you want to compare\nresults obtained with predict to results obtained via decision_function or pre\ndict_proba, make sure to use the classes_ attribute of the classifier to get the actual\nclass names:\n126 \n| \nChapter 2: Supervised Learning\nIn[120]:\nlogreg = LogisticRegression()\n# represent each target by its class name in the iris dataset\nnamed_target = iris.target_names[y_train]\nlogreg.fit(X_train, named_target)\nprint(\"unique classes in training data: {}\".format(logreg.classes_))\nprint(\"predictions: {}\".format(logreg.predict(X_test)[:10]))\nargmax_dec_func = np.argmax(logreg.decision_function(X_test), axis=1)\nprint(\"argmax of decision function: {}\".format(argmax_dec_func[:10]))\nprint(\"argmax combined with classes_: {}\".format(\n        logreg.classes_[argmax_dec_func][:10]))\nOut[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]",
    "Out[120]:\nunique classes in training data: ['setosa' 'versicolor' 'virginica']\npredictions: ['versicolor' 'setosa' 'virginica' 'versicolor' 'versicolor'\n 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nargmax of decision function: [1 0 2 1 1 0 1 2 1 1]\nargmax combined with classes_: ['versicolor' 'setosa' 'virginica' 'versicolor'\n 'versicolor' 'setosa' 'versicolor' 'virginica' 'versicolor' 'versicolor']\nSummary and Outlook\nWe started this chapter with a discussion of model complexity, then discussed gener‐\nalization, or learning a model that is able to perform well on new, previously unseen\ndata. This led us to the concepts of underfitting, which describes a model that cannot\ncapture the variations present in the training data, and overfitting, which describes a\nmodel that focuses too much on the training data and is not able to generalize to new\ndata very well.\nWe then discussed a wide array of machine learning models for classification and\nregression, what their advantages and disadvantages are, and how to control model\ncomplexity for each of them. We saw that for many of the algorithms, setting the right\nparameters is important for good performance. Some of the algorithms are also sensi‐\ntive to how we represent the input data, and in particular to how the features are\nscaled. Therefore, blindly applying an algorithm to a dataset without understanding\nthe assumptions the model makes and the meanings of the parameter settings will\nrarely lead to an accurate model.\nThis chapter contains a lot of information about the algorithms, and it is not neces‐\nsary for you to remember all of these details for the following chapters. However,\nsome knowledge of the models described here—and which to use in a specific situa‐\ntion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models",
    "tion—is important for successfully applying machine learning in practice. Here is a\nquick summary of when to use each model:\nSummary and Outlook \n| \n127\nNearest neighbors\nFor small datasets, good as a baseline, easy to explain.\nLinear models\nGo-to as a first algorithm to try, good for very large datasets, good for very high-\ndimensional data.\nNaive Bayes\nOnly for classification. Even faster than linear models, good for very large data‐\nsets and high-dimensional data. Often less accurate than linear models.\nDecision trees\nVery fast, don’t need scaling of the data, can be visualized and easily explained.\nRandom forests\nNearly always perform better than a single decision tree, very robust and power‐\nful. Don’t need scaling of data. Not good for very high-dimensional sparse data.\nGradient boosted decision trees\nOften slightly more accurate than random forests. Slower to train but faster to\npredict than random forests, and smaller in memory. Need more parameter tun‐\ning than random forests.\nSupport vector machines\nPowerful for medium-sized datasets of features with similar meaning. Require\nscaling of data, sensitive to parameters.\nNeural networks\nCan build very complex models, particularly for large datasets. Sensitive to scal‐\ning of the data and to the choice of parameters. Large models need a long time to\ntrain.\nWhen working with a new dataset, it is in general a good idea to start with a simple\nmodel, such as a linear model or a naive Bayes or nearest neighbors classifier, and see\nhow far you can get. After understanding more about the data, you can consider\nmoving to an algorithm that can build more complex models, such as random forests,\ngradient boosted decision trees, SVMs, or neural networks.\nYou should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐",
    "You should now be in a position where you have some idea of how to apply, tune, and\nanalyze the models we discussed here. In this chapter, we focused on the binary clas‐\nsification case, as this is usually easiest to understand. Most of the algorithms presen‐\nted have classification and regression variants, however, and all of the classification\nalgorithms support both binary and multiclass classification. Try applying any of\nthese algorithms to the built-in datasets in scikit-learn, like the boston_housing or\ndiabetes datasets for regression, or the digits dataset for multiclass classification.\nPlaying around with the algorithms on different datasets will give you a better feel for\n128 \n| \nChapter 2: Supervised Learning\nhow long they need to train, how easy it is to analyze the models, and how sensitive\nthey are to the representation of the data.\nWhile we analyzed the consequences of different parameter settings for the algo‐\nrithms we investigated, building a model that actually generalizes well to new data in\nproduction is a bit trickier than that. We will see how to properly adjust parameters\nand how to find good parameters automatically in Chapter 6.\nFirst, though, we will dive in more detail into unsupervised learning and preprocess‐\ning in the next chapter.\nSummary and Outlook \n| \n129\nCHAPTER 3\nUnsupervised Learning and Preprocessing\nThe second family of machine learning algorithms that we will discuss is unsuper‐\nvised learning algorithms. Unsupervised learning subsumes all kinds of machine\nlearning where there is no known output, no teacher to instruct the learning algo‐\nrithm. In unsupervised learning, the learning algorithm is just shown the input data\nand asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐",
    "and asked to extract knowledge from this data.\nTypes of Unsupervised Learning\nWe will look into two kinds of unsupervised learning in this chapter: transformations\nof the dataset and clustering.\nUnsupervised transformations of a dataset are algorithms that create a new representa‐\ntion of the data which might be easier for humans or other machine learning algo‐\nrithms to understand compared to the original representation of the data. A common\napplication of unsupervised transformations is dimensionality reduction, which takes\na high-dimensional representation of the data, consisting of many features, and finds\na new way to represent this data that summarizes the essential characteristics with\nfewer features. A common application for dimensionality reduction is reduction to\ntwo dimensions for visualization purposes.\nAnother application for unsupervised transformations is finding the parts or compo‐\nnents that “make up” the data. An example of this is topic extraction on collections of\ntext documents. Here, the task is to find the unknown topics that are talked about in\neach document, and to learn what topics appear in each document. This can be useful\nfor tracking the discussion of themes like elections, gun control, or pop stars on social\nmedia.\nClustering algorithms, on the other hand, partition data into distinct groups of similar\nitems. Consider the example of uploading photos to a social media site. To allow you\n131\nto organize your pictures, the site might want to group together pictures that show\nthe same person. However, the site doesn’t know which pictures show whom, and it\ndoesn’t know how many different people appear in your photo collection. A sensible\napproach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning",
    "approach would be to extract all the faces and divide them into groups of faces that\nlook similar. Hopefully, these correspond to the same person, and the images can be\ngrouped together for you.\nChallenges in Unsupervised Learning\nA major challenge in unsupervised learning is evaluating whether the algorithm\nlearned something useful. Unsupervised learning algorithms are usually applied to\ndata that does not contain any label information, so we don’t know what the right\noutput should be. Therefore, it is very hard to say whether a model “did well.” For\nexample, our hypothetical clustering algorithm could have grouped together all the\npictures that show faces in profile and all the full-face pictures. This would certainly\nbe a possible way to divide a collection of pictures of people’s faces, but it’s not the one\nwe were looking for. However, there is no way for us to “tell” the algorithm what we\nare looking for, and often the only way to evaluate the result of an unsupervised algo‐\nrithm is to inspect it manually.\nAs a consequence, unsupervised algorithms are used often in an exploratory setting,\nwhen a data scientist wants to understand the data better, rather than as part of a\nlarger automatic system. Another common application for unsupervised algorithms\nis as a preprocessing step for supervised algorithms. Learning a new representation of\nthe data can sometimes improve the accuracy of supervised algorithms, or can lead to\nreduced memory and time consumption.\nBefore we start with “real” unsupervised algorithms, we will briefly discuss some sim‐\nple preprocessing methods that often come in handy. Even though preprocessing and\nscaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,",
    "scaling are often used in tandem with supervised learning algorithms, scaling meth‐\nods don’t make use of the supervised information, making them unsupervised.\nPreprocessing and Scaling\nIn the previous chapter we saw that some algorithms, like neural networks and SVMs,\nare very sensitive to the scaling of the data. Therefore, a common practice is to adjust\nthe features so that the data representation is more suitable for these algorithms.\nOften, this is a simple per-feature rescaling and shift of the data. The following code\n(Figure 3-1) shows a simple example:\nIn[2]:\nmglearn.plots.plot_scaling()\n132 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n1 The median of a set of numbers is the number x such that half of the numbers are smaller than x and half of\nthe numbers are larger than x. The lower quartile is the number x such that one-fourth of the numbers are\nsmaller than x, and the upper quartile is the number x such that one-fourth of the numbers are larger than x.\nFigure 3-1. Different ways to rescale and preprocess a dataset\nDifferent Kinds of Preprocessing\nThe first plot in Figure 3-1 shows a synthetic two-class classification dataset with two\nfeatures. The first feature (the x-axis value) is between 10 and 15. The second feature\n(the y-axis value) is between around 1 and 9.\nThe following four plots show four different ways to transform the data that yield\nmore standard ranges. The StandardScaler in scikit-learn ensures that for each\nfeature the mean is 0 and the variance is 1, bringing all features to the same magni‐\ntude. However, this scaling does not ensure any particular minimum and maximum\nvalues for the features. The RobustScaler works similarly to the StandardScaler in\nthat it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very",
    "that it ensures statistical properties for each feature that guarantee that they are on the\nsame scale. However, the RobustScaler uses the median and quartiles,1 instead of\nmean and variance. This makes the RobustScaler ignore data points that are very\ndifferent from the rest (like measurement errors). These odd data points are also\ncalled outliers, and can lead to trouble for other scaling techniques.\nThe MinMaxScaler, on the other hand, shifts the data such that all features are exactly\nbetween 0 and 1. For the two-dimensional dataset this means all of the data is con‐\nPreprocessing and Scaling \n| \n133\ntained within the rectangle created by the x-axis between 0 and 1 and the y-axis\nbetween 0 and 1.\nFinally, the Normalizer does a very different kind of rescaling. It scales each data\npoint such that the feature vector has a Euclidean length of 1. In other words, it\nprojects a data point on the circle (or sphere, in the case of higher dimensions) with a\nradius of 1. This means every data point is scaled by a different number (by the\ninverse of its length). This normalization is often used when only the direction (or\nangle) of the data matters, not the length of the feature vector.\nApplying Data Transformations\nNow that we’ve seen what the different kinds of transformations do, let’s apply them\nusing scikit-learn. We will use the cancer dataset that we saw in Chapter 2. Pre‐\nprocessing methods like the scalers are usually applied before applying a supervised\nmachine learning algorithm. As an example, say we want to apply the kernel SVM\n(SVC) to the cancer dataset, and use MinMaxScaler for preprocessing the data. We\nstart by loading our dataset and splitting it into a training set and a test set (we need\nseparate training and test sets to evaluate the supervised model we will build after the\npreprocessing):\nIn[3]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()",
    "separate training and test sets to evaluate the supervised model we will build after the\npreprocessing):\nIn[3]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=1)\nprint(X_train.shape)\nprint(X_test.shape)\nOut[3]:\n(426, 30)\n(143, 30)\nAs a reminder, the dataset contains 569 data points, each represented by 30 measure‐\nments. We split the dataset into 426 samples for the training set and 143 samples for\nthe test set.\nAs with the supervised models we built earlier, we first import the class that imple‐\nments the preprocessing, and then instantiate it:\nIn[4]:\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n134 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nWe then fit the scaler using the fit method, applied to the training data. For the Min\nMaxScaler, the fit method computes the minimum and maximum value of each fea‐\nture on the training set. In contrast to the classifiers and regressors of Chapter 2, the\nscaler is only provided with the data (X_train) when fit is called, and y_train is not\nused:\nIn[5]:\nscaler.fit(X_train)\nOut[5]:\nMinMaxScaler(copy=True, feature_range=(0, 1))\nTo apply the transformation that we just learned—that is, to actually scale the training\ndata—we use the transform method of the scaler. The transform method is used in\nscikit-learn whenever a model returns a new representation of the data:\nIn[6]:\n# transform data\nX_train_scaled = scaler.transform(X_train)\n# print dataset properties before and after scaling\nprint(\"transformed shape: {}\".format(X_train_scaled.shape))\nprint(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\nprint(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\nprint(\"per-feature minimum after scaling:\\n {}\".format(",
    "print(\"transformed shape: {}\".format(X_train_scaled.shape))\nprint(\"per-feature minimum before scaling:\\n {}\".format(X_train.min(axis=0)))\nprint(\"per-feature maximum before scaling:\\n {}\".format(X_train.max(axis=0)))\nprint(\"per-feature minimum after scaling:\\n {}\".format(\n    X_train_scaled.min(axis=0)))\nprint(\"per-feature maximum after scaling:\\n {}\".format(\n    X_train_scaled.max(axis=0)))\nOut[6]:\ntransformed shape: (426, 30)\nper-feature minimum before scaling:\n [   6.98    9.71   43.79  143.50    0.05    0.02    0.      0.      0.11\n     0.05    0.12    0.36    0.76    6.80    0.      0.      0.      0.\n     0.01    0.      7.93   12.02   50.41  185.20    0.07    0.03    0.\n     0.      0.16    0.06]\nper-feature maximum before scaling:\n [   28.11    39.28   188.5   2501.0     0.16     0.29     0.43     0.2\n     0.300    0.100    2.87     4.88    21.98   542.20     0.03     0.14\n     0.400    0.050    0.06     0.03    36.04    49.54   251.20  4254.00\n     0.220    0.940    1.17     0.29     0.58     0.15]\nper-feature minimum after scaling:\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n   0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\nper-feature maximum after scaling:\n [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.\n   1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\nPreprocessing and Scaling \n| \n135\nThe transformed data has the same shape as the original data—the features are simply\nshifted and scaled. You can see that all of the features are now between 0 and 1, as\ndesired.\nTo apply the SVM to the scaled data, we also need to transform the test set. This is\nagain done by calling the transform method, this time on X_test:\nIn[7]:\n# transform test data\nX_test_scaled = scaler.transform(X_test)\n# print test data properties after scaling\nprint(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\nprint(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))\nOut[7]:",
    "In[7]:\n# transform test data\nX_test_scaled = scaler.transform(X_test)\n# print test data properties after scaling\nprint(\"per-feature minimum after scaling:\\n{}\".format(X_test_scaled.min(axis=0)))\nprint(\"per-feature maximum after scaling:\\n{}\".format(X_test_scaled.max(axis=0)))\nOut[7]:\nper-feature minimum after scaling:\n[ 0.034  0.023  0.031  0.011  0.141  0.044  0.     0.     0.154 -0.006\n -0.001  0.006  0.004  0.001  0.039  0.011  0.     0.    -0.032  0.007\n  0.027  0.058  0.02   0.009  0.109  0.026  0.     0.    -0.    -0.002]\nper-feature maximum after scaling:\n[ 0.958  0.815  0.956  0.894  0.811  1.22   0.88   0.933  0.932  1.037\n  0.427  0.498  0.441  0.284  0.487  0.739  0.767  0.629  1.337  0.391\n  0.896  0.793  0.849  0.745  0.915  1.132  1.07   0.924  1.205  1.631]\nMaybe somewhat surprisingly, you can see that for the test set, after scaling, the mini‐\nmum and maximum are not 0 and 1. Some of the features are even outside the 0–1\nrange! The explanation is that the MinMaxScaler (and all the other scalers) always\napplies exactly the same transformation to the training and the test set. This means\nthe transform method always subtracts the training set minimum and divides by the\ntraining set range, which might be different from the minimum and range for the test\nset.\nScaling Training and Test Data the Same Way\nIt is important to apply exactly the same transformation to the training set and the\ntest set for the supervised model to work on the test set. The following example\n(Figure 3-2) illustrates what would happen if we were to use the minimum and range\nof the test set instead:\nIn[8]:\nfrom sklearn.datasets import make_blobs\n# make synthetic data\nX, _ = make_blobs(n_samples=50, centers=5, random_state=4, cluster_std=2)\n# split it into training and test sets\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n# plot the training and test sets\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\n136 \n| \nChapter 3: Unsupervised Learning and Preprocessing",
    "# split it into training and test sets\nX_train, X_test = train_test_split(X, random_state=5, test_size=.1)\n# plot the training and test sets\nfig, axes = plt.subplots(1, 3, figsize=(13, 4))\n136 \n| \nChapter 3: Unsupervised Learning and Preprocessing\naxes[0].scatter(X_train[:, 0], X_train[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[0].scatter(X_test[:, 0], X_test[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[0].legend(loc='upper left')\naxes[0].set_title(\"Original Data\")\n# scale the data using MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# visualize the properly scaled data\naxes[1].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"Training set\", s=60)\naxes[1].scatter(X_test_scaled[:, 0], X_test_scaled[:, 1], marker='^',\n                c=mglearn.cm2(1), label=\"Test set\", s=60)\naxes[1].set_title(\"Scaled Data\")\n# rescale the test set separately\n# so test set min is 0 and test set max is 1\n# DO NOT DO THIS! For illustration purposes only.\ntest_scaler = MinMaxScaler()\ntest_scaler.fit(X_test)\nX_test_scaled_badly = test_scaler.transform(X_test)\n# visualize wrongly scaled data\naxes[2].scatter(X_train_scaled[:, 0], X_train_scaled[:, 1],\n                c=mglearn.cm2(0), label=\"training set\", s=60)\naxes[2].scatter(X_test_scaled_badly[:, 0], X_test_scaled_badly[:, 1],\n                marker='^', c=mglearn.cm2(1), label=\"test set\", s=60)\naxes[2].set_title(\"Improperly Scaled Data\")\nfor ax in axes:\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\nFigure 3-2. Effect of scaling training and test data shown on the left together (center) and\nseparately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but",
    "separately (right)\nPreprocessing and Scaling \n| \n137\nThe first panel is an unscaled two-dimensional dataset, with the training set shown as\ncircles and the test set shown as triangles. The second panel is the same data, but\nscaled using the MinMaxScaler. Here, we called fit on the training set, and then\ncalled transform on the training and test sets. You can see that the dataset in the sec‐\nond panel looks identical to the first; only the ticks on the axes have changed. Now all\nthe features are between 0 and 1. You can also see that the minimum and maximum\nfeature values for the test data (the triangles) are not 0 and 1.\nThe third panel shows what would happen if we scaled the training set and test set\nseparately. In this case, the minimum and maximum feature values for both the train‐\ning and the test set are 0 and 1. But now the dataset looks different. The test points\nmoved incongruously to the training set, as they were scaled differently. We changed\nthe arrangement of the data in an arbitrary way. Clearly this is not what we want to\ndo.\nAs another way to think about this, imagine your test set is a single point. There is no\nway to scale a single point correctly, to fulfill the minimum and maximum require‐\nments of the MinMaxScaler. But the size of your test set should not change your\nprocessing.\nShortcuts and Efficient Alternatives\nOften, you want to fit a model on some dataset, and then transform it. This is a very\ncommon task, which can often be computed more efficiently than by simply calling\nfit and then transform. For this use case, all models that have a transform method\nalso have a fit_transform method. Here is an example using StandardScaler:\nIn[9]:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calling fit and transform in sequence (using method chaining)\nX_scaled = scaler.fit(X).transform(X)\n# same result, but more efficient computation\nX_scaled_d = scaler.fit_transform(X)",
    "In[9]:\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# calling fit and transform in sequence (using method chaining)\nX_scaled = scaler.fit(X).transform(X)\n# same result, but more efficient computation\nX_scaled_d = scaler.fit_transform(X)\nWhile fit_transform is not necessarily more efficient for all models, it is still good\npractice to use this method when trying to transform the training set.\nThe Effect of Preprocessing on Supervised Learning\nNow let’s go back to the cancer dataset and see the effect of using the MinMaxScaler\non learning the SVC (this is a different way of doing the same scaling we did in Chap‐\nter 2). First, let’s fit the SVC on the original data again for comparison:\n138 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[10]:\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target,\n                                                    random_state=0)\nsvm = SVC(C=100)\nsvm.fit(X_train, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(svm.score(X_test, y_test)))\nOut[10]:\nTest set accuracy: 0.63\nNow, let’s scale the data using MinMaxScaler before fitting the SVC:\nIn[11]:\n# preprocessing using 0-1 scaling\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n# learning an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scoring on the scaled test set\nprint(\"Scaled test set accuracy: {:.2f}\".format(\n    svm.score(X_test_scaled, y_test)))\nOut[11]:\nScaled test set accuracy: 0.97\nAs we saw before, the effect of scaling the data is quite significant. Even though scal‐\ning the data doesn’t involve any complicated math, it is good practice to use the scal‐\ning mechanisms provided by scikit-learn instead of reimplementing them yourself,\nas it’s easy to make mistakes even in these simple computations.\nYou can also easily replace one preprocessing algorithm with another by changing the",
    "ing mechanisms provided by scikit-learn instead of reimplementing them yourself,\nas it’s easy to make mistakes even in these simple computations.\nYou can also easily replace one preprocessing algorithm with another by changing the\nclass you use, as all of the preprocessing classes have the same interface, consisting of\nthe fit and transform methods:\nIn[12]:\n# preprocessing using zero mean and unit variance scaling\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nPreprocessing and Scaling \n| \n139\n# learning an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scoring on the scaled test set\nprint(\"SVM test accuracy: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[12]:\nSVM test accuracy: 0.96\nNow that we’ve seen how simple data transformations for preprocessing work, let’s\nmove on to more interesting transformations using unsupervised learning.\nDimensionality Reduction, Feature Extraction, and\nManifold Learning\nAs we discussed earlier, transforming data using unsupervised learning can have\nmany motivations. The most common motivations are visualization, compressing the\ndata, and finding a representation that is more informative for further processing.\nOne of the simplest and most widely used algorithms for all of these is principal com‐\nponent analysis. We’ll also look at two other algorithms: non-negative matrix factori‐\nzation (NMF), which is commonly used for feature extraction, and t-SNE, which is\ncommonly used for visualization using two-dimensional scatter plots.\nPrincipal Component Analysis (PCA)\nPrincipal component analysis is a method that rotates the dataset in a way such that\nthe rotated features are statistically uncorrelated. This rotation is often followed by\nselecting only a subset of the new features, according to how important they are for",
    "Principal Component Analysis (PCA)\nPrincipal component analysis is a method that rotates the dataset in a way such that\nthe rotated features are statistically uncorrelated. This rotation is often followed by\nselecting only a subset of the new features, according to how important they are for\nexplaining the data. The following example (Figure 3-3) illustrates the effect of PCA\non a synthetic two-dimensional dataset:\nIn[13]:\nmglearn.plots.plot_pca_illustration()\nThe first plot (top left) shows the original data points, colored to distinguish among\nthem. The algorithm proceeds by first finding the direction of maximum variance,\nlabeled “Component 1.” This is the direction (or vector) in the data that contains most\nof the information, or in other words, the direction along which the features are most\ncorrelated with each other. Then, the algorithm finds the direction that contains the\nmost information while being orthogonal (at a right angle) to the first direction. In\ntwo dimensions, there is only one possible orientation that is at a right angle, but in\nhigher-dimensional spaces there would be (infinitely) many orthogonal directions.\nAlthough the two components are drawn as arrows, it doesn’t really matter where the\nhead and the tail are; we could have drawn the first component from the center up to\n140 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nthe top left instead of down to the bottom right. The directions found using this pro‐\ncess are called principal components, as they are the main directions of variance in the\ndata. In general, there are as many principal components as original features.\nFigure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so",
    "Figure 3-3. Transformation of data with PCA\nThe second plot (top right) shows the same data, but now rotated so that the first\nprincipal component aligns with the x-axis and the second principal component\naligns with the y-axis. Before the rotation, the mean was subtracted from the data, so\nthat the transformed data is centered around zero. In the rotated representation\nfound by PCA, the two axes are uncorrelated, meaning that the correlation matrix of\nthe data in this representation is zero except for the diagonal.\nWe can use PCA for dimensionality reduction by retaining only some of the principal\ncomponents. In this example, we might keep only the first principal component, as\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n141\nshown in the third panel in Figure 3-3 (bottom left). This reduces the data from a\ntwo-dimensional dataset to a one-dimensional dataset. Note, however, that instead of\nkeeping only one of the original features, we found the most interesting direction\n(top left to bottom right in the first panel) and kept this direction, the first principal\ncomponent.\nFinally, we can undo the rotation and add the mean back to the data. This will result\nin the data shown in the last panel in Figure 3-3. These points are in the original fea‐\nture space, but we kept only the information contained in the first principal compo‐\nnent. This transformation is sometimes used to remove noise effects from the data or\nvisualize what part of the information is retained using the principal components.\nApplying PCA to the cancer dataset for visualization\nOne of the most common applications of PCA is visualizing high-dimensional data‐\nsets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible",
    "sets. As we saw in Chapter 1, it is hard to create scatter plots of data that has more\nthan two features. For the Iris dataset, we were able to create a pair plot (Figure 1-3 in\nChapter 1) that gave us a partial picture of the data by showing us all the possible\ncombinations of two features. But if we want to look at the Breast Cancer dataset,\neven using a pair plot is tricky. This dataset has 30 features, which would result in\n30 * 14 = 420 scatter plots! We’d never be able to look at all these plots in detail, let\nalone try to understand them.\nThere is an even simpler visualization we can use, though—computing histograms of\neach of the features for the two classes, benign and malignant cancer (Figure 3-4):\nIn[14]:\nfig, axes = plt.subplots(15, 2, figsize=(10, 20))\nmalignant = cancer.data[cancer.target == 0]\nbenign = cancer.data[cancer.target == 1]\nax = axes.ravel()\nfor i in range(30):\n    _, bins = np.histogram(cancer.data[:, i], bins=50)\n    ax[i].hist(malignant[:, i], bins=bins, color=mglearn.cm3(0), alpha=.5)\n    ax[i].hist(benign[:, i], bins=bins, color=mglearn.cm3(2), alpha=.5)\n    ax[i].set_title(cancer.feature_names[i])\n    ax[i].set_yticks(())\nax[0].set_xlabel(\"Feature magnitude\")\nax[0].set_ylabel(\"Frequency\")\nax[0].legend([\"malignant\", \"benign\"], loc=\"best\")\nfig.tight_layout()\n142 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-4. Per-class feature histograms on the Breast Cancer dataset\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n143\nHere we create a histogram for each of the features, counting how often a data point\nappears with a feature in a certain range (called a bin). Each plot overlays two histo‐\ngrams, one for all of the points in the benign class (blue) and one for all the points in\nthe malignant class (red). This gives us some idea of how each feature is distributed\nacross the two classes, and allows us to venture a guess as to which features are better",
    "grams, one for all of the points in the benign class (blue) and one for all the points in\nthe malignant class (red). This gives us some idea of how each feature is distributed\nacross the two classes, and allows us to venture a guess as to which features are better\nat distinguishing malignant and benign samples. For example, the feature “smooth‐\nness error” seems quite uninformative, because the two histograms mostly overlap,\nwhile the feature “worst concave points” seems quite informative, because the histo‐\ngrams are quite disjoint.\nHowever, this plot doesn’t show us anything about the interactions between variables\nand how these relate to the classes. Using PCA, we can capture the main interactions\nand get a slightly more complete picture. We can find the first two principal compo‐\nnents, and visualize the data in this new two-dimensional space with a single scatter\nplot.\nBefore we apply PCA, we scale our data so that each feature has unit variance using\nStandardScaler:\nIn[15]:\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nscaler = StandardScaler()\nscaler.fit(cancer.data)\nX_scaled = scaler.transform(cancer.data)\nLearning the PCA transformation and applying it is as simple as applying a prepro‐\ncessing transformation. We instantiate the PCA object, find the principal components\nby calling the fit method, and then apply the rotation and dimensionality reduction\nby calling transform. By default, PCA only rotates (and shifts) the data, but keeps all\nprincipal components. To reduce the dimensionality of the data, we need to specify\nhow many components we want to keep when creating the PCA object:\nIn[16]:\nfrom sklearn.decomposition import PCA\n# keep the first two principal components of the data\npca = PCA(n_components=2)\n# fit PCA model to breast cancer data\npca.fit(X_scaled)\n# transform data onto the first two principal components\nX_pca = pca.transform(X_scaled)\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))",
    "# keep the first two principal components of the data\npca = PCA(n_components=2)\n# fit PCA model to breast cancer data\npca.fit(X_scaled)\n# transform data onto the first two principal components\nX_pca = pca.transform(X_scaled)\nprint(\"Original shape: {}\".format(str(X_scaled.shape)))\nprint(\"Reduced shape: {}\".format(str(X_pca.shape)))\n144 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nOut[16]:\nOriginal shape: (569, 30)\nReduced shape: (569, 2)\nWe can now plot the first two principal components (Figure 3-5):\nIn[17]:\n# plot first vs. second principal component, colored by class\nplt.figure(figsize=(8, 8))\nmglearn.discrete_scatter(X_pca[:, 0], X_pca[:, 1], cancer.target)\nplt.legend(cancer.target_names, loc=\"best\")\nplt.gca().set_aspect(\"equal\")\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nFigure 3-5. Two-dimensional scatter plot of the Breast Cancer dataset using the first two\nprincipal components\nIt is important to note that PCA is an unsupervised method, and does not use any class\ninformation when finding the rotation. It simply looks at the correlations in the data.\nFor the scatter plot shown here, we plotted the first principal component against the\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n145\nsecond principal component, and then used the class information to color the points.\nYou can see that the two classes separate quite well in this two-dimensional space.\nThis leads us to believe that even a linear classifier (that would learn a line in this\nspace) could do a reasonably good job at distinguishing the two classes. We can also\nsee that the malignant (red) points are more spread out than the benign (blue) points\n—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are",
    "—something that we could already see a bit from the histograms in Figure 3-4.\nA downside of PCA is that the two axes in the plot are often not very easy to interpret.\nThe principal components correspond to directions in the original data, so they are\ncombinations of the original features. However, these combinations are usually very\ncomplex, as we’ll see shortly. The principal components themselves are stored in the\ncomponents_ attribute of the PCA object during fitting:\nIn[18]:\nprint(\"PCA component shape: {}\".format(pca.components_.shape))\nOut[18]:\nPCA component shape: (2, 30)\nEach row in components_ corresponds to one principal component, and they are sor‐\nted by their importance (the first principal component comes first, etc.). The columns\ncorrespond to the original features attribute of the PCA in this example, “mean\nradius,” “mean texture,” and so on. Let’s have a look at the content of components_:\nIn[19]:\nprint(\"PCA components:\\n{}\".format(pca.components_))\nOut[19]:\nPCA components:\n[[ 0.219  0.104  0.228  0.221  0.143  0.239  0.258  0.261  0.138  0.064\n   0.206  0.017  0.211  0.203  0.015  0.17   0.154  0.183  0.042  0.103\n   0.228  0.104  0.237  0.225  0.128  0.21   0.229  0.251  0.123  0.132]\n [-0.234 -0.06  -0.215 -0.231  0.186  0.152  0.06  -0.035  0.19   0.367\n  -0.106  0.09  -0.089 -0.152  0.204  0.233  0.197  0.13   0.184  0.28\n  -0.22  -0.045 -0.2   -0.219  0.172  0.144  0.098 -0.008  0.142  0.275]]\nWe can also visualize the coefficients using a heat map (Figure 3-6), which might be\neasier to understand:\nIn[20]:\nplt.matshow(pca.components_, cmap='viridis')\nplt.yticks([0, 1], [\"First component\", \"Second component\"])\nplt.colorbar()\nplt.xticks(range(len(cancer.feature_names)),\n           cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset",
    "cancer.feature_names, rotation=60, ha='left')\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Principal components\")\n146 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-6. Heat map of the first two principal components on the Breast Cancer dataset\nYou can see that in the first component, all features have the same sign (it’s negative,\nbut as we mentioned earlier, it doesn’t matter which direction the arrow points in).\nThat means that there is a general correlation between all features. As one measure‐\nment is high, the others are likely to be high as well. The second component has\nmixed signs, and both of the components involve all of the 30 features. This mixing of\nall features is what makes explaining the axes in Figure 3-6 so tricky.\nEigenfaces for feature extraction\nAnother application of PCA that we mentioned earlier is feature extraction. The idea\nbehind feature extraction is that it is possible to find a representation of your data\nthat is better suited to analysis than the raw representation you were given. A great\nexample of an application where feature extraction is helpful is with images. Images\nare made up of pixels, usually stored as red, green, and blue (RGB) intensities.\nObjects in images are usually made up of thousands of pixels, and only together are\nthey meaningful.\nWe will give a very simple application of feature extraction on images using PCA, by\nworking with face images from the Labeled Faces in the Wild dataset. This dataset\ncontains face images of celebrities downloaded from the Internet, and it includes\nfaces of politicians, singers, actors, and athletes from the early 2000s. We use gray‐\nscale versions of these images, and scale them down for faster processing. You can see\nsome of the images in Figure 3-7:\nIn[21]:\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),",
    "some of the images in Figure 3-7:\nIn[21]:\nfrom sklearn.datasets import fetch_lfw_people\npeople = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\nimage_shape = people.images[0].shape\nfix, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor target, image, ax in zip(people.target, people.images, axes.ravel()):\n    ax.imshow(image)\n    ax.set_title(people.target_names[target])\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n147\nFigure 3-7. Some images from the Labeled Faces in the Wild dataset\nThere are 3,023 images, each 87×65 pixels large, belonging to 62 different people:\nIn[22]:\nprint(\"people.images.shape: {}\".format(people.images.shape))\nprint(\"Number of classes: {}\".format(len(people.target_names)))\nOut[22]:\npeople.images.shape: (3023, 87, 65)\nNumber of classes: 62\nThe dataset is a bit skewed, however, containing a lot of images of George W. Bush\nand Colin Powell, as you can see here:\nIn[23]:\n# count how often each target appears\ncounts = np.bincount(people.target)\n# print counts next to target names\nfor i, (count, name) in enumerate(zip(counts, people.target_names)):\n    print(\"{0:25} {1:3}\".format(name, count), end='   ')\n    if (i + 1) % 3 == 0:\n        print()\n148 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nOut[23]:\nAlejandro Toledo           39   Alvaro Uribe               35\nAmelie Mauresmo            21   Andre Agassi               36\nAngelina Jolie             20   Arnold Schwarzenegger      42\nAtal Bihari Vajpayee       24   Bill Clinton               29\nCarlos Menem               21   Colin Powell              236\nDavid Beckham              31   Donald Rumsfeld           121\nGeorge W Bush             530   George Robertson           22\nGerhard Schroeder         109   Gloria Macapagal Arroyo    44\nGray Davis                 26   Guillermo Coria            30\nHamid Karzai               22   Hans Blix                  39",
    "George W Bush             530   George Robertson           22\nGerhard Schroeder         109   Gloria Macapagal Arroyo    44\nGray Davis                 26   Guillermo Coria            30\nHamid Karzai               22   Hans Blix                  39\nHugo Chavez                71   Igor Ivanov                20\n[...]                           [...]\nLaura Bush                 41   Lindsay Davenport          22\nLleyton Hewitt             41   Luiz Inacio Lula da Silva  48\nMahmoud Abbas              29   Megawati Sukarnoputri      33\nMichael Bloomberg          20   Naomi Watts                22\nNestor Kirchner            37   Paul Bremer                20\nPete Sampras               22   Recep Tayyip Erdogan       30\nRicardo Lagos              27   Roh Moo-hyun               32\nRudolph Giuliani           26   Saddam Hussein             23\nSerena Williams            52   Silvio Berlusconi          33\nTiger Woods                23   Tom Daschle                25\nTom Ridge                  33   Tony Blair                144\nVicente Fox                32   Vladimir Putin             49\nWinona Ryder               24\nTo make the data less skewed, we will only take up to 50 images of each person\n(otherwise, the feature extraction would be overwhelmed by the likelihood of George\nW. Bush):\nIn[24]:\nmask = np.zeros(people.target.shape, dtype=np.bool)\nfor target in np.unique(people.target):\n    mask[np.where(people.target == target)[0][:50]] = 1\nX_people = people.data[mask]\ny_people = people.target[mask]\n# scale the grayscale values to be between 0 and 1\n# instead of 0 and 255 for better numeric stability\nX_people = X_people / 255.\nA common task in face recognition is to ask if a previously unseen face belongs to a\nknown person from a database. This has applications in photo collection, social\nmedia, and security applications. One way to solve this problem would be to build a\nclassifier where each person is a separate class. However, there are usually many dif‐",
    "known person from a database. This has applications in photo collection, social\nmedia, and security applications. One way to solve this problem would be to build a\nclassifier where each person is a separate class. However, there are usually many dif‐\nferent people in face databases, and very few images of the same person (i.e., very few\ntraining examples per class). That makes it hard to train most classifiers. Additionally,\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n149\nyou often want to be able to add new people easily, without needing to retrain a large\nmodel.\nA simple solution is to use a one-nearest-neighbor classifier that looks for the most\nsimilar face image to the face you are classifying. This classifier could in principle\nwork with only a single training example per class. Let’s take a look at how well\nKNeighborsClassifier does here:\nIn[25]:\nfrom sklearn.neighbors import KNeighborsClassifier\n# split the data into training and test sets\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\n# build a KNeighborsClassifier using one neighbor\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train, y_train)\nprint(\"Test set score of 1-nn: {:.2f}\".format(knn.score(X_test, y_test)))\nOut[25]:\nTest set score of 1-nn: 0.27\nWe obtain an accuracy of 26.6%, which is not actually that bad for a 62-class classifi‐\ncation problem (random guessing would give you around 1/62 = 1.5% accuracy), but\nis also not great. We only correctly identify a person every fourth time.\nThis is where PCA comes in. Computing distances in the original pixel space is quite\na bad way to measure similarity between faces. When using a pixel representation to\ncompare two images, we compare the grayscale value of each individual pixel to the\nvalue of the pixel in the corresponding position in the other image. This representa‐\ntion is quite different from how humans would interpret the image of a face, and it is",
    "compare two images, we compare the grayscale value of each individual pixel to the\nvalue of the pixel in the corresponding position in the other image. This representa‐\ntion is quite different from how humans would interpret the image of a face, and it is\nhard to capture the facial features using this raw representation. For example, using\npixel distances means that shifting a face by one pixel to the right corresponds to a\ndrastic change, with a completely different representation. We hope that using distan‐\nces along principal components can improve our accuracy. Here, we enable the\nwhitening option of PCA, which rescales the principal components to have the same\nscale. This is the same as using StandardScaler after the transformation. Reusing the\ndata from Figure 3-3 again, whitening corresponds to not only rotating the data, but\nalso rescaling it so that the center panel is a circle instead of an ellipse (see\nFigure 3-8):\nIn[26]:\nmglearn.plots.plot_pca_whitening()\n150 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-8. Transformation of data with PCA using whitening\nWe fit the PCA object to the training data and extract the first 100 principal compo‐\nnents. Then we transform the training and test data:\nIn[27]:\npca = PCA(n_components=100, whiten=True, random_state=0).fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)\nprint(\"X_train_pca.shape: {}\".format(X_train_pca.shape))\nOut[27]:\nX_train_pca.shape: (1537, 100)\nThe new data has 100 features, the first 100 principal components. Now, we can use\nthe new representation to classify our images using a one-nearest-neighbors classifier:\nIn[28]:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\nOut[28]:\nTest set accuracy: 0.36\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our",
    "In[28]:\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(X_train_pca, y_train)\nprint(\"Test set accuracy: {:.2f}\".format(knn.score(X_test_pca, y_test)))\nOut[28]:\nTest set accuracy: 0.36\nOur accuracy improved quite significantly, from 26.6% to 35.7%, confirming our\nintuition that the principal components might provide a better representation of the\ndata.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n151\nFor image data, we can also easily visualize the principal components that are found.\nRemember that components correspond to directions in the input space. The input\nspace here is 50×37-pixel grayscale images, so directions within this space are also\n50×37-pixel grayscale images.\nLet’s look at the first couple of principal components (Figure 3-9):\nIn[29]:\nprint(\"pca.components_.shape: {}\".format(pca.components_.shape))\nOut[29]:\npca.components_.shape: (100, 5655)\nIn[30]:\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(pca.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape),\n              cmap='viridis')\n    ax.set_title(\"{}. component\".format((i + 1)))\nWhile we certainly cannot understand all aspects of these components, we can guess\nwhich aspects of the face images some of the components are capturing. The first\ncomponent seems to mostly encode the contrast between the face and the back‐\nground, the second component encodes differences in lighting between the right and\nthe left half of the face, and so on. While this representation is slightly more semantic\nthan the raw pixel values, it is still quite far from how a human might perceive a face.\nAs the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not",
    "As the PCA model is based on pixels, the alignment of the face (the position of eyes,\nchin, and nose) and the lighting both have a strong influence on how similar two\nimages are in their pixel representation. But alignment and lighting are probably not\nwhat a human would perceive first. When asking people to rate similarity of faces,\nthey are more likely to use attributes like age, gender, facial expression, and hair style,\nwhich are attributes that are hard to infer from the pixel intensities. It’s important to\nkeep in mind that algorithms often interpret data (particularly visual data, such as\nimages, which humans are very familiar with) quite differently from how a human\nwould.\n152 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-9. Component vectors of the first 15 principal components of the faces dataset\nLet’s come back to the specific case of PCA, though. We introduced the PCA transfor‐\nmation as rotating the data and then dropping the components with low variance.\nAnother useful interpretation is to try to find some numbers (the new feature values\nafter the PCA rotation) so that we can express the test points as a weighted sum of the\nprincipal components (see Figure 3-10).\nFigure 3-10. Schematic view of PCA as decomposing an image into a weighted sum of\ncomponents\nHere, x0, x1, and so on are the coefficients of the principal components for this data\npoint; in other words, they are the representation of the image in the rotated space.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n153\nAnother way we can try to understand what a PCA model is doing is by looking at\nthe reconstructions of the original data using only some components. In Figure 3-3,\nafter dropping the second component and arriving at the third panel, we undid the\nrotation and added the mean back to obtain new points in the original space with the\nsecond component removed, as shown in the last panel. We can do a similar transfor‐",
    "after dropping the second component and arriving at the third panel, we undid the\nrotation and added the mean back to obtain new points in the original space with the\nsecond component removed, as shown in the last panel. We can do a similar transfor‐\nmation for the faces by reducing the data to only some principal components and\nthen rotating back into the original space. This return to the original feature space\ncan be done using the inverse_transform method. Here, we visualize the recon‐\nstruction of some faces using 10, 50, 100, 500, or 2,000 components (Figure 3-11):\nIn[32]:\nmglearn.plots.plot_pca_faces(X_train, X_test, image_shape)\nFigure 3-11. Reconstructing three face images using increasing numbers of principal\ncomponents\nYou can see that when we use only the first 10 principal components, only the essence\nof the picture, like the face orientation and lighting, is captured. By using more and\nmore principal components, more and more details in the image are preserved. This\n154 \n| \nChapter 3: Unsupervised Learning and Preprocessing\ncorresponds to extending the sum in Figure 3-10 to include more and more terms.\nUsing as many components as there are pixels would mean that we would not discard\nany information after the rotation, and we would reconstruct the image perfectly.\nWe can also try to use PCA to visualize all the faces in the dataset in a scatter plot\nusing the first two principal components (Figure 3-12), with classes given by who is\nshown in the image, similarly to what we did for the cancer dataset:\nIn[33]:\nmglearn.discrete_scatter(X_train_pca[:, 0], X_train_pca[:, 1], y_train)\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data",
    "plt.ylabel(\"Second principal component\")\nFigure 3-12. Scatter plot of the faces dataset using the first two principal components (see\nFigure 3-5 for the corresponding image for the cancer dataset)\nAs you can see, when we use only the first two principal components the whole data\nis just a big blob, with no separation of classes visible. This is not very surprising,\ngiven that even with 10 components, as shown earlier in Figure 3-11, PCA only cap‐\ntures very rough characteristics of the faces.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n155\nNon-Negative Matrix Factorization (NMF)\nNon-negative matrix factorization is another unsupervised learning algorithm that\naims to extract useful features. It works similarly to PCA and can also be used for\ndimensionality reduction. As in PCA, we are trying to write each data point as a\nweighted sum of some components, as illustrated in Figure 3-10. But whereas in PCA\nwe wanted components that were orthogonal and that explained as much variance of\nthe data as possible, in NMF, we want the components and the coefficients to be non-\nnegative; that is, we want both the components and the coefficients to be greater than\nor equal to zero. Consequently, this method can only be applied to data where each\nfeature is non-negative, as a non-negative sum of non-negative components cannot\nbecome negative.\nThe process of decomposing data into a non-negative weighted sum is particularly\nhelpful for data that is created as the addition (or overlay) of several independent\nsources, such as an audio track of multiple people speaking, or music with many\ninstruments. In these situations, NMF can identify the original components that\nmake up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and",
    "make up the combined data. Overall, NMF leads to more interpretable components\nthan PCA, as negative components and coefficients can lead to hard-to-interpret can‐\ncellation effects. The eigenfaces in Figure 3-9, for example, contain both positive and\nnegative parts, and as we mentioned in the description of PCA, the sign is actually\narbitrary. Before we apply NMF to the face dataset, let’s briefly revisit the synthetic\ndata.\nApplying NMF to synthetic data\nIn contrast to when using PCA, we need to ensure that our data is positive for NMF\nto be able to operate on the data. This means where the data lies relative to the origin\n(0, 0) actually matters for NMF. Therefore, you can think of the non-negative compo‐\nnents that are extracted as directions from (0, 0) toward the data.\nThe following example (Figure 3-13) shows the results of NMF on the two-\ndimensional toy data:\nIn[34]:\nmglearn.plots.plot_nmf_illustration()\n156 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-13. Components found by non-negative matrix factorization with two compo‐\nnents (left) and one component (right)\nFor NMF with two components, as shown on the left, it is clear that all points in the\ndata can be written as a positive combination of the two components. If there are\nenough components to perfectly reconstruct the data (as many components as there\nare features), the algorithm will choose directions that point toward the extremes of\nthe data.\nIf we only use a single component, NMF creates a component that points toward the\nmean, as pointing there best explains the data. You can see that in contrast with PCA,\nreducing the number of components not only removes some directions, but creates\nan entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on",
    "an entirely different set of components! Components in NMF are also not ordered in\nany specific way, so there is no “first non-negative component”: all components play\nan equal part.\nNMF uses a random initialization, which might lead to different results depending on\nthe random seed. In relatively simple cases such as the synthetic data with two com‐\nponents, where all the data can be explained perfectly, the randomness has little effect\n(though it might change the order or scale of the components). In more complex sit‐\nuations, there might be more drastic changes.\nApplying NMF to face images\nNow, let’s apply NMF to the Labeled Faces in the Wild dataset we used earlier. The\nmain parameter of NMF is how many components we want to extract. Usually this is\nlower than the number of input features (otherwise, the data could be explained by\nmaking each pixel a separate component).\nFirst, let’s inspect how the number of components impacts how well the data can be\nreconstructed using NMF (Figure 3-14):\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n157\nIn[35]:\nmglearn.plots.plot_nmf_faces(X_train, X_test, image_shape)\nFigure 3-14. Reconstructing three face images using increasing numbers of components\nfound by NMF\nThe quality of the back-transformed data is similar to when using PCA, but slightly\nworse. This is expected, as PCA finds the optimum directions in terms of reconstruc‐\ntion. NMF is usually not used for its ability to reconstruct or encode data, but rather\nfor finding interesting patterns within the data.\nAs a first look into the data, let’s try extracting only a few components (say, 15).\nFigure 3-15 shows the result:\n158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),",
    "158 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[36]:\nfrom sklearn.decomposition import NMF\nnmf = NMF(n_components=15, random_state=0)\nnmf.fit(X_train)\nX_train_nmf = nmf.transform(X_train)\nX_test_nmf = nmf.transform(X_test)\nfix, axes = plt.subplots(3, 5, figsize=(15, 12),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (component, ax) in enumerate(zip(nmf.components_, axes.ravel())):\n    ax.imshow(component.reshape(image_shape))\n    ax.set_title(\"{}. component\".format(i))\nFigure 3-15. The components found by NMF on the faces dataset when using 15 compo‐\nnents\nThese components are all positive, and so resemble prototypes of faces much more so\nthan the components shown for PCA in Figure 3-9. For example, one can clearly see\nthat component 3 shows a face rotated somewhat to the right, while component 7\nshows a face somewhat rotated to the left. Let’s look at the images for which these\ncomponents are particularly strong, shown in Figures 3-16 and 3-17:\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n159\nIn[37]:\ncompn = 3\n# sort by 3rd component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\ncompn = 7\n# sort by 7th component, plot first 10 images\ninds = np.argsort(X_train_nmf[:, compn])[::-1]\nfig, axes = plt.subplots(2, 5, figsize=(15, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\nFigure 3-16. Faces that have a large coefficient for component 3\n160 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-17. Faces that have a large coefficient for component 7",
    "for i, (ind, ax) in enumerate(zip(inds, axes.ravel())):\n    ax.imshow(X_train[ind].reshape(image_shape))\nFigure 3-16. Faces that have a large coefficient for component 3\n160 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-17. Faces that have a large coefficient for component 7\nAs expected, faces that have a high coefficient for component 3 are faces looking to\nthe right (Figure 3-16), while faces with a high coefficient for component 7 are look‐\ning to the left (Figure 3-17). As mentioned earlier, extracting patterns like these works\nbest for data with additive structure, including audio, gene expression, and text data.\nLet’s walk through one example on synthetic data to see what this might look like.\nLet’s say we are interested in a signal that is a combination of three different sources\n(Figure 3-18):\nIn[38]:\nS = mglearn.datasets.make_signals()\nplt.figure(figsize=(6, 1))\nplt.plot(S, '-')\nplt.xlabel(\"Time\")\nplt.ylabel(\"Signal\")\nFigure 3-18. Original signal sources\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n161\nUnfortunately we cannot observe the original signals, but only an additive mixture of\nall three of them. We want to recover the decomposition of the mixed signal into the\noriginal components. We assume that we have many different ways to observe the\nmixture (say 100 measurement devices), each of which provides us with a series of\nmeasurements:\nIn[39]:\n# mix data into a 100-dimensional state\nA = np.random.RandomState(0).uniform(size=(100, 3))\nX = np.dot(S, A.T)\nprint(\"Shape of measurements: {}\".format(X.shape))\nOut[39]:\nShape of measurements: (2000, 100)\nWe can use NMF to recover the three signals:\nIn[40]:\nnmf = NMF(n_components=3, random_state=42)\nS_ = nmf.fit_transform(X)\nprint(\"Recovered signal shape: {}\".format(S_.shape))\nOut[40]:\nRecovered signal shape: (2000, 3)\nFor comparison, we also apply PCA:\nIn[41]:\npca = PCA(n_components=3)\nH = pca.fit_transform(X)",
    "In[40]:\nnmf = NMF(n_components=3, random_state=42)\nS_ = nmf.fit_transform(X)\nprint(\"Recovered signal shape: {}\".format(S_.shape))\nOut[40]:\nRecovered signal shape: (2000, 3)\nFor comparison, we also apply PCA:\nIn[41]:\npca = PCA(n_components=3)\nH = pca.fit_transform(X)\nFigure 3-19 shows the signal activity that was discovered by NMF and PCA:\nIn[42]:\nmodels = [X, S, S_, H]\nnames = ['Observations (first three measurements)',\n         'True sources',\n         'NMF recovered signals',\n         'PCA recovered signals']\nfig, axes = plt.subplots(4, figsize=(8, 4), gridspec_kw={'hspace': .5},\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfor model, name, ax in zip(models, names, axes):\n    ax.set_title(name)\n    ax.plot(model[:, :3], '-')\n162 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-19. Recovering mixed sources using NMF and PCA\nThe figure includes 3 of the 100 measurements from X for reference. As you can see,\nNMF did a reasonable job of discovering the original sources, while PCA failed and\nused the first component to explain the majority of the variation in the data. Keep in\nmind that the components produced by NMF have no natural ordering. In this exam‐\nple, the ordering of the NMF components is the same as in the original signal (see the\nshading of the three curves), but this is purely accidental.\nThere are many other algorithms that can be used to decompose each data point into\na weighted sum of a fixed set of components, as PCA and NMF do. Discussing all of\nthem is beyond the scope of this book, and describing the constraints made on the\ncomponents and coefficients often involves probability theory. If you are interested in\nthis kind of pattern extraction, we recommend that you study the sections of the sci\nkit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE",
    "kit_learn user guide on independent component analysis (ICA), factor analysis\n(FA), and sparse coding (dictionary learning), all of which you can find on the page\nabout decomposition methods.\nManifold Learning with t-SNE\nWhile PCA is often a good first approach for transforming your data so that you\nmight be able to visualize it using a scatter plot, the nature of the method (applying a\nrotation and then dropping directions) limits its usefulness, as we saw with the scatter\nplot of the Labeled Faces in the Wild dataset. There is a class of algorithms for visuali‐\nzation called manifold learning algorithms that allow for much more complex map‐\npings, and often provide better visualizations. A particularly useful one is the t-SNE\nalgorithm.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n163\n2 Not to be confused with the much larger MNIST dataset.\nManifold learning algorithms are mainly aimed at visualization, and so are rarely\nused to generate more than two new features. Some of them, including t-SNE, com‐\npute a new representation of the training data, but don’t allow transformations of new\ndata. This means these algorithms cannot be applied to a test set: rather, they can only\ntransform the data they were trained for. Manifold learning can be useful for explora‐\ntory data analysis, but is rarely used if the final goal is supervised learning. The idea\nbehind t-SNE is to find a two-dimensional representation of the data that preserves\nthe distances between points as best as possible. t-SNE starts with a random two-\ndimensional representation for each data point, and then tries to make points that are\nclose in the original feature space closer, and points that are far apart in the original\nfeature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.",
    "feature space farther apart. t-SNE puts more emphasis on points that are close by,\nrather than preserving distances between far-apart points. In other words, it tries to\npreserve the information indicating which points are neighbors to each other.\nWe will apply the t-SNE manifold learning algorithm on a dataset of handwritten dig‐\nits that is included in scikit-learn.2 Each data point in this dataset is an 8×8 gray‐\nscale image of a handwritten digit between 0 and 1. Figure 3-20 shows an example\nimage for each class:\nIn[43]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\nfig, axes = plt.subplots(2, 5, figsize=(10, 5),\n                         subplot_kw={'xticks':(), 'yticks': ()})\nfor ax, img in zip(axes.ravel(), digits.images):\n    ax.imshow(img)\n164 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-20. Example images from the digits dataset\nLet’s use PCA to visualize the data reduced to two dimensions. We plot the first two\nprincipal components, and color each dot by its class (see Figure 3-21):\nIn[44]:\n# build a PCA model\npca = PCA(n_components=2)\npca.fit(digits.data)\n# transform the digits data onto the first two principal components\ndigits_pca = pca.transform(digits.data)\ncolors = [\"#476A2A\", \"#7851B8\", \"#BD3430\", \"#4A2D4E\", \"#875525\",\n          \"#A83683\", \"#4E655E\", \"#853541\", \"#3A3120\", \"#535D8E\"]\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_pca[:, 0].min(), digits_pca[:, 0].max())\nplt.ylim(digits_pca[:, 1].min(), digits_pca[:, 1].max())\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_pca[i, 0], digits_pca[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.",
    "color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"First principal component\")\nplt.ylabel(\"Second principal component\")\nHere, we actually used the true digit classes as glyphs, to show which class is where.\nThe digits zero, six, and four are relatively well separated using the first two principal\ncomponents, though they still overlap. Most of the other digits overlap significantly.\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n165\nFigure 3-21. Scatter plot of the digits dataset using the first two principal components\nLet’s apply t-SNE to the same dataset, and compare the results. As t-SNE does not\nsupport transforming new data, the TSNE class has no transform method. Instead, we\ncan call the fit_transform method, which will build the model and immediately\nreturn the transformed data (see Figure 3-22):\nIn[45]:\nfrom sklearn.manifold import TSNE\ntsne = TSNE(random_state=42)\n# use fit_transform instead of fit, as TSNE has no transform method\ndigits_tsne = tsne.fit_transform(digits.data)\n166 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[46]:\nplt.figure(figsize=(10, 10))\nplt.xlim(digits_tsne[:, 0].min(), digits_tsne[:, 0].max() + 1)\nplt.ylim(digits_tsne[:, 1].min(), digits_tsne[:, 1].max() + 1)\nfor i in range(len(digits.data)):\n    # actually plot the digits as text instead of using scatter\n    plt.text(digits_tsne[i, 0], digits_tsne[i, 1], str(digits.target[i]),\n             color = colors[digits.target[i]],\n             fontdict={'weight': 'bold', 'size': 9})\nplt.xlabel(\"t-SNE feature 0\")\nplt.xlabel(\"t-SNE feature 1\")\nFigure 3-22. Scatter plot of the digits dataset using two components found by t-SNE\nDimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense",
    "Dimensionality Reduction, Feature Extraction, and Manifold Learning \n| \n167\nThe result of t-SNE is quite remarkable. All the classes are quite clearly separated.\nThe ones and nines are somewhat split up, but most of the classes form a single dense\ngroup. Keep in mind that this method has no knowledge of the class labels: it is com‐\npletely unsupervised. Still, it can find a representation of the data in two dimensions\nthat clearly separates the classes, based solely on how close points are in the original\nspace.\nThe t-SNE algorithm has some tuning parameters, though it often works well with\nthe default settings. You can try playing with perplexity and early_exaggeration,\nbut the effects are usually minor.\nClustering\nAs we described earlier, clustering is the task of partitioning the dataset into groups,\ncalled clusters. The goal is to split up the data in such a way that points within a single\ncluster are very similar and points in different clusters are different. Similarly to clas‐\nsification algorithms, clustering algorithms assign (or predict) a number to each data\npoint, indicating which cluster a particular point belongs to.\nk-Means Clustering\nk-means clustering is one of the simplest and most commonly used clustering algo‐\nrithms. It tries to find cluster centers that are representative of certain regions of the\ndata. The algorithm alternates between two steps: assigning each data point to the\nclosest cluster center, and then setting each cluster center as the mean of the data\npoints that are assigned to it. The algorithm is finished when the assignment of\ninstances to clusters no longer changes. The following example (Figure 3-23) illus‐\ntrates the algorithm on a synthetic dataset:\nIn[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors",
    "In[47]:\nmglearn.plots.plot_kmeans_algorithm()\n168 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-23. Input data and three steps of the k-means algorithm\nCluster centers are shown as triangles, while data points are shown as circles. Colors\nindicate cluster membership. We specified that we are looking for three clusters, so\nthe algorithm was initialized by declaring three data points randomly as cluster cen‐\nters (see “Initialization”). Then the iterative algorithm starts. First, each data point is\nassigned to the cluster center it is closest to (see “Assign Points (1)”). Next, the cluster\ncenters are updated to be the mean of the assigned points (see “Recompute Centers\n(1)”). Then the process is repeated two more times. After the third iteration, the\nassignment of points to cluster centers remained unchanged, so the algorithm stops.\nGiven new data points, k-means will assign each to the closest cluster center. The next\nexample (Figure 3-24) shows the boundaries of the cluster centers that were learned\nin Figure 3-23:\nIn[48]:\nmglearn.plots.plot_kmeans_boundaries()\nClustering \n| \n169\n3 If you don’t provide n_clusters, it is set to 8 by default. There is no particular reason why you should use this\nvalue.\nFigure 3-24. Cluster centers and cluster boundaries found by the k-means algorithm\nApplying k-means with scikit-learn is quite straightforward. Here, we apply it to\nthe synthetic data that we used for the preceding plots. We instantiate the KMeans\nclass, and set the number of clusters we are looking for.3 Then we call the fit method\nwith the data:\nIn[49]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\n# generate synthetic two-dimensional data\nX, y = make_blobs(random_state=1)\n# build the clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\nfind these labels in the kmeans.labels_ attribute:\n170 \n|",
    "# generate synthetic two-dimensional data\nX, y = make_blobs(random_state=1)\n# build the clustering model\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\nDuring the algorithm, each training data point in X is assigned a cluster label. You can\nfind these labels in the kmeans.labels_ attribute:\n170 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nIn[50]:\nprint(\"Cluster memberships:\\n{}\".format(kmeans.labels_))\nOut[50]:\nCluster memberships:\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\nAs we asked for three clusters, the clusters are numbered 0 to 2.\nYou can also assign cluster labels to new points, using the predict method. Each new\npoint is assigned to the closest cluster center when predicting, but the existing model\nis not changed. Running predict on the training set returns the same result as\nlabels_:\nIn[51]:\nprint(kmeans.predict(X))\nOut[51]:\n[1 2 2 2 0 0 0 2 1 1 2 2 0 1 0 0 0 1 2 2 0 2 0 1 2 0 0 1 1 0 1 1 0 1 2 0 2\n 2 2 0 0 2 1 2 2 0 1 1 1 1 2 0 0 0 1 0 2 2 1 1 2 0 0 2 2 0 1 0 1 2 2 2 0 1\n 1 2 0 0 1 2 1 2 2 0 1 1 1 1 2 1 0 1 1 2 2 0 0 1 0 1]\nYou can see that clustering is somewhat similar to classification, in that each item gets\na label. However, there is no ground truth, and consequently the labels themselves\nhave no a priori meaning. Let’s go back to the example of clustering face images that\nwe discussed before. It might be that the cluster 3 found by the algorithm contains\nonly faces of your friend Bela. You can only know that after you look at the pictures,\nthough, and the number 3 is arbitrary. The only information the algorithm gives you\nis that all faces labeled as 3 are similar.\nFor the clustering we just computed on the two-dimensional toy dataset, that means\nthat we should not assign any significance to the fact that one group was labeled 0",
    "though, and the number 3 is arbitrary. The only information the algorithm gives you\nis that all faces labeled as 3 are similar.\nFor the clustering we just computed on the two-dimensional toy dataset, that means\nthat we should not assign any significance to the fact that one group was labeled 0\nand another one was labeled 1. Running the algorithm again might result in a differ‐\nent numbering of clusters because of the random nature of the initialization.\nHere is a plot of this data again (Figure 3-25). The cluster centers are stored in the\ncluster_centers_ attribute, and we plot them as triangles:\nIn[52]:\nmglearn.discrete_scatter(X[:, 0], X[:, 1], kmeans.labels_, markers='o')\nmglearn.discrete_scatter(\n    kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], [0, 1, 2],\n    markers='^', markeredgewidth=2)\nClustering \n| \n171\nFigure 3-25. Cluster assignments and cluster centers found by k-means with three\nclusters\nWe can also use more or fewer cluster centers (Figure 3-26):\nIn[53]:\nfig, axes = plt.subplots(1, 2, figsize=(10, 5))\n# using two cluster centers:\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\nassignments = kmeans.labels_\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[0])\n# using five cluster centers:\nkmeans = KMeans(n_clusters=5)\nkmeans.fit(X)\nassignments = kmeans.labels_\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignments, ax=axes[1])\n172 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-26. Cluster assignments found by k-means using two clusters (left) and five\nclusters (right)\nFailure cases of k-means\nEven if you know the “right” number of clusters for a given dataset, k-means might\nnot always be able to recover them. Each cluster is defined solely by its center, which\nmeans that each cluster is a convex shape. As a result of this, k-means can only cap‐\nture relatively simple shapes. k-means also assumes that all clusters have the same",
    "not always be able to recover them. Each cluster is defined solely by its center, which\nmeans that each cluster is a convex shape. As a result of this, k-means can only cap‐\nture relatively simple shapes. k-means also assumes that all clusters have the same\n“diameter” in some sense; it always draws the boundary between clusters to be exactly\nin the middle between the cluster centers. That can sometimes lead to surprising\nresults, as shown in Figure 3-27:\nIn[54]:\nX_varied, y_varied = make_blobs(n_samples=200,\n                                cluster_std=[1.0, 2.5, 0.5],\n                                random_state=170)\ny_pred = KMeans(n_clusters=3, random_state=0).fit_predict(X_varied)\nmglearn.discrete_scatter(X_varied[:, 0], X_varied[:, 1], y_pred)\nplt.legend([\"cluster 0\", \"cluster 1\", \"cluster 2\"], loc='best')\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nClustering \n| \n173\nFigure 3-27. Cluster assignments found by k-means when clusters have different\ndensities\nOne might have expected the dense region in the lower left to be the first cluster, the\ndense region in the upper right to be the second, and the less dense region in the cen‐\nter to be the third. Instead, both cluster 0 and cluster 1 have some points that are far\naway from all the other points in these clusters that “reach” toward the center.\nk-means also assumes that all directions are equally important for each cluster. The\nfollowing plot (Figure 3-28) shows a two-dimensional dataset where there are three\nclearly separated parts in the data. However, these groups are stretched toward the\ndiagonal. As k-means only considers the distance to the nearest cluster center, it can’t\nhandle this kind of data:\nIn[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing",
    "In[55]:\n# generate some random cluster data\nX, y = make_blobs(random_state=170, n_samples=600)\nrng = np.random.RandomState(74)\n# transform the data to be stretched\ntransformation = rng.normal(size=(2, 2))\nX = np.dot(X, transformation)\n174 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n# cluster the data into three clusters\nkmeans = KMeans(n_clusters=3)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm3)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[0, 1, 2], s=100, linewidth=2, cmap=mglearn.cm3)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-28. k-means fails to identify nonspherical clusters\nk-means also performs poorly if the clusters have more complex shapes, like the\ntwo_moons data we encountered in Chapter 2 (see Figure 3-29):\nIn[56]:\n# generate synthetic two_moons data (with less noise this time)\nfrom sklearn.datasets import make_moons\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# cluster the data into two clusters\nkmeans = KMeans(n_clusters=2)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nClustering \n| \n175\n# plot the cluster assignments and cluster centers\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, cmap=mglearn.cm2, s=60)\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1],\n            marker='^', c=[mglearn.cm2(0), mglearn.cm2(1)], s=100, linewidth=2)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nFigure 3-29. k-means fails to identify clusters with complex shapes\nHere, we would hope that the clustering algorithm can discover the two half-moon\nshapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐",
    "shapes. However, this is not possible using the k-means algorithm.\nVector quantization, or seeing k-means as decomposition\nEven though k-means is a clustering algorithm, there are interesting parallels between\nk-means and the decomposition methods like PCA and NMF that we discussed ear‐\nlier. You might remember that PCA tries to find directions of maximum variance in\nthe data, while NMF tries to find additive components, which often correspond to\n“extremes” or “parts” of the data (see Figure 3-13). Both methods tried to express the\ndata points as a sum over some components. k-means, on the other hand, tries to rep‐\nresent each data point using a cluster center. You can think of that as each point being\nrepresented using only a single component, which is given by the cluster center. This\nview of k-means as a decomposition method, where each point is represented using a\nsingle component, is called vector quantization.\n176 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nLet’s do a side-by-side comparison of PCA, NMF, and k-means, showing the compo‐\nnents extracted (Figure 3-30), as well as reconstructions of faces from the test set\nusing 100 components (Figure 3-31). For k-means, the reconstruction is the closest\ncluster center found on the training set:\nIn[57]:\nX_train, X_test, y_train, y_test = train_test_split(\n    X_people, y_people, stratify=y_people, random_state=0)\nnmf = NMF(n_components=100, random_state=0)\nnmf.fit(X_train)\npca = PCA(n_components=100, random_state=0)\npca.fit(X_train)\nkmeans = KMeans(n_clusters=100, random_state=0)\nkmeans.fit(X_train)\nX_reconstructed_pca = pca.inverse_transform(pca.transform(X_test))\nX_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\nIn[58]:\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"Extracted Components\")",
    "X_reconstructed_kmeans = kmeans.cluster_centers_[kmeans.predict(X_test)]\nX_reconstructed_nmf = np.dot(nmf.transform(X_test), nmf.components_)\nIn[58]:\nfig, axes = plt.subplots(3, 5, figsize=(8, 8),\n                         subplot_kw={'xticks': (), 'yticks': ()})\nfig.suptitle(\"Extracted Components\")\nfor ax, comp_kmeans, comp_pca, comp_nmf in zip(\n        axes.T, kmeans.cluster_centers_, pca.components_, nmf.components_):\n    ax[0].imshow(comp_kmeans.reshape(image_shape))\n    ax[1].imshow(comp_pca.reshape(image_shape), cmap='viridis')\n    ax[2].imshow(comp_nmf.reshape(image_shape))\naxes[0, 0].set_ylabel(\"kmeans\")\naxes[1, 0].set_ylabel(\"pca\")\naxes[2, 0].set_ylabel(\"nmf\")\nfig, axes = plt.subplots(4, 5, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(8, 8))\nfig.suptitle(\"Reconstructions\")\nfor ax, orig, rec_kmeans, rec_pca, rec_nmf in zip(\n        axes.T, X_test, X_reconstructed_kmeans, X_reconstructed_pca,\n        X_reconstructed_nmf):\n    ax[0].imshow(orig.reshape(image_shape))\n    ax[1].imshow(rec_kmeans.reshape(image_shape))\n    ax[2].imshow(rec_pca.reshape(image_shape))\n    ax[3].imshow(rec_nmf.reshape(image_shape))\naxes[0, 0].set_ylabel(\"original\")\naxes[1, 0].set_ylabel(\"kmeans\")\naxes[2, 0].set_ylabel(\"pca\")\naxes[3, 0].set_ylabel(\"nmf\")\nClustering \n| \n177\nFigure 3-30. Comparing k-means cluster centers to components found by PCA and NMF\n178 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-31. Comparing image reconstructions using k-means, PCA, and NMF with 100\ncomponents (or cluster centers)—k-means uses only a single cluster center per image\nAn interesting aspect of vector quantization using k-means is that we can use many\nmore clusters than input dimensions to encode our data. Let’s go back to the\ntwo_moons data. Using PCA or NMF, there is nothing much we can do to this data, as\nit lives in only two dimensions. Reducing it to one dimension with PCA or NMF",
    "more clusters than input dimensions to encode our data. Let’s go back to the\ntwo_moons data. Using PCA or NMF, there is nothing much we can do to this data, as\nit lives in only two dimensions. Reducing it to one dimension with PCA or NMF\nwould completely destroy the structure of the data. But we can find a more expressive\nrepresentation with k-means, by using more cluster centers (see Figure 3-32):\nClustering \n| \n179\nIn[59]:\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\nkmeans = KMeans(n_clusters=10, random_state=0)\nkmeans.fit(X)\ny_pred = kmeans.predict(X)\nplt.scatter(X[:, 0], X[:, 1], c=y_pred, s=60, cmap='Paired')\nplt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=60,\n            marker='^', c=range(kmeans.n_clusters), linewidth=2, cmap='Paired')\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nprint(\"Cluster memberships:\\n{}\".format(y_pred))\nOut[59]:\nCluster memberships:\n[9 2 5 4 2 7 9 6 9 6 1 0 2 6 1 9 3 0 3 1 7 6 8 6 8 5 2 7 5 8 9 8 6 5 3 7 0\n 9 4 5 0 1 3 5 2 8 9 1 5 6 1 0 7 4 6 3 3 6 3 8 0 4 2 9 6 4 8 2 8 4 0 4 0 5\n 6 4 5 9 3 0 7 8 0 7 5 8 9 8 0 7 3 9 7 1 7 2 2 0 4 5 6 7 8 9 4 5 4 1 2 3 1\n 8 8 4 9 2 3 7 0 9 9 1 5 8 5 1 9 5 6 7 9 1 4 0 6 2 6 4 7 9 5 5 3 8 1 9 5 6\n 3 5 0 2 9 3 0 8 6 0 3 3 5 6 3 2 0 2 3 0 2 6 3 4 4 1 5 6 7 1 1 3 2 4 7 2 7\n 3 8 6 4 1 4 3 9 9 5 1 7 5 8 2]\nFigure 3-32. Using many k-means clusters to cover the variation in a complex dataset\n180 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n4 In this case, “best” means that the sum of variances of the clusters is small.\nWe used 10 cluster centers, which means each point is now assigned a number\nbetween 0 and 9. We can see this as the data being represented using 10 components\n(that is, we have 10 new features), with all features being 0, apart from the one that\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐",
    "(that is, we have 10 new features), with all features being 0, apart from the one that\nrepresents the cluster center the point is assigned to. Using this 10-dimensional repre‐\nsentation, it would now be possible to separate the two half-moon shapes using a lin‐\near model, which would not have been possible using the original two features. It is\nalso possible to get an even more expressive representation of the data by using the\ndistances to each of the cluster centers as features. This can be accomplished using\nthe transform method of kmeans:\nIn[60]:\ndistance_features = kmeans.transform(X)\nprint(\"Distance feature shape: {}\".format(distance_features.shape))\nprint(\"Distance features:\\n{}\".format(distance_features))\nOut[60]:\nDistance feature shape: (200, 10)\nDistance features:\n[[ 0.922  1.466  1.14  ...,  1.166  1.039  0.233]\n [ 1.142  2.517  0.12  ...,  0.707  2.204  0.983]\n [ 0.788  0.774  1.749 ...,  1.971  0.716  0.944]\n ...,\n [ 0.446  1.106  1.49  ...,  1.791  1.032  0.812]\n [ 1.39   0.798  1.981 ...,  1.978  0.239  1.058]\n [ 1.149  2.454  0.045 ...,  0.572  2.113  0.882]]\nk-means is a very popular algorithm for clustering, not only because it is relatively\neasy to understand and implement, but also because it runs relatively quickly. k-\nmeans scales easily to large datasets, and scikit-learn even includes a more scalable\nvariant in the MiniBatchKMeans class, which can handle very large datasets.\nOne of the drawbacks of k-means is that it relies on a random initialization, which\nmeans the outcome of the algorithm depends on a random seed. By default, scikit-\nlearn runs the algorithm 10 times with 10 different random initializations, and\nreturns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).",
    "returns the best result.4 Further downsides of k-means are the relatively restrictive\nassumptions made on the shape of clusters, and the requirement to specify the num‐\nber of clusters you are looking for (which might not be known in a real-world\napplication).\nNext, we will look at two more clustering algorithms that improve upon these proper‐\nties in some ways.\nClustering \n| \n181\nAgglomerative Clustering\nAgglomerative clustering refers to a collection of clustering algorithms that all build\nupon the same principles: the algorithm starts by declaring each point its own cluster,\nand then merges the two most similar clusters until some stopping criterion is satis‐\nfied. The stopping criterion implemented in scikit-learn is the number of clusters,\nso similar clusters are merged until only the specified number of clusters are left.\nThere are several linkage criteria that specify how exactly the “most similar cluster” is\nmeasured. This measure is always defined between two existing clusters.\nThe following three choices are implemented in scikit-learn:\nward\nThe default choice, ward picks the two clusters to merge such that the variance\nwithin all clusters increases the least. This often leads to clusters that are rela‐\ntively equally sized.\naverage\naverage linkage merges the two clusters that have the smallest average distance\nbetween all their points.\ncomplete\ncomplete linkage (also known as maximum linkage) merges the two clusters that\nhave the smallest maximum distance between their points.\nward works on most datasets, and we will use it in our examples. If the clusters have\nvery dissimilar numbers of members (if one is much bigger than all the others, for\nexample), average or complete might work better.\nThe following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing",
    "The following plot (Figure 3-33) illustrates the progression of agglomerative cluster‐\ning on a two-dimensional dataset, looking for three clusters:\nIn[61]:\nmglearn.plots.plot_agglomerative_algorithm()\n182 \n| \nChapter 3: Unsupervised Learning and Preprocessing\n5 We could also use the labels_ attribute, as we did for k-means.\nFigure 3-33. Agglomerative clustering iteratively joins the two closest clusters\nInitially, each point is its own cluster. Then, in each step, the two clusters that are\nclosest are merged. In the first four steps, two single-point clusters are picked and\nthese are joined into two-point clusters. In step 5, one of the two-point clusters is\nextended to a third point, and so on. In step 9, there are only three clusters remain‐\ning. As we specified that we are looking for three clusters, the algorithm then stops.\nLet’s have a look at how agglomerative clustering performs on the simple three-\ncluster data we used here. Because of the way the algorithm works, agglomerative\nclustering cannot make predictions for new data points. Therefore, Agglomerative\nClustering has no predict method. To build the model and get the cluster member‐\nships on the training set, use the fit_predict method instead.5 The result is shown\nin Figure 3-34:\nIn[62]:\nfrom sklearn.cluster import AgglomerativeClustering\nX, y = make_blobs(random_state=1)\nagg = AgglomerativeClustering(n_clusters=3)\nassignment = agg.fit_predict(X)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], assignment)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nClustering \n| \n183\nFigure 3-34. Cluster assignment using agglomerative clustering with three clusters\nAs expected, the algorithm recovers the clustering perfectly. While the scikit-learn\nimplementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms",
    "implementation of agglomerative clustering requires you to specify the number of\nclusters you want the algorithm to find, agglomerative clustering methods provide\nsome help with choosing the right number, which we will discuss next.\nHierarchical clustering and dendrograms\nAgglomerative clustering produces what is known as a hierarchical clustering. The\nclustering proceeds iteratively, and every point makes a journey from being a single\npoint cluster to belonging to some final cluster. Each intermediate step provides a\nclustering of the data (with a different number of clusters). It is sometimes helpful to\nlook at all possible clusterings jointly. The next example (Figure 3-35) shows an over‐\nlay of all the possible clusterings shown in Figure 3-33, providing some insight into\nhow each cluster breaks up into smaller clusters:\nIn[63]:\nmglearn.plots.plot_agglomerative()\n184 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-35. Hierarchical cluster assignment (shown as lines) generated with agglomera‐\ntive clustering, with numbered data points (cf. Figure 3-36)\nWhile this visualization provides a very detailed view of the hierarchical clustering, it\nrelies on the two-dimensional nature of the data and therefore cannot be used on\ndatasets that have more than two features. There is, however, another tool to visualize\nhierarchical clustering, called a dendrogram, that can handle multidimensional\ndatasets.\nUnfortunately, scikit-learn currently does not have the functionality to draw den‐\ndrograms. However, you can generate them easily using SciPy. The SciPy clustering\nalgorithms have a slightly different interface to the scikit-learn clustering algo‐\nrithms. SciPy provides a function that takes a data array X and computes a linkage\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\nIn[64]:",
    "rithms. SciPy provides a function that takes a data array X and computes a linkage\narray, which encodes hierarchical cluster similarities. We can then feed this linkage\narray into the scipy dendrogram function to plot the dendrogram (Figure 3-36):\nIn[64]:\n# Import the dendrogram function and the ward clustering function from SciPy\nfrom scipy.cluster.hierarchy import dendrogram, ward\nX, y = make_blobs(random_state=0, n_samples=12)\n# Apply the ward clustering to the data array X\n# The SciPy ward function returns an array that specifies the distances\n# bridged when performing agglomerative clustering\nlinkage_array = ward(X)\nClustering \n| \n185\n# Now we plot the dendrogram for the linkage_array containing the distances\n# between clusters\ndendrogram(linkage_array)\n# Mark the cuts in the tree that signify two or three clusters\nax = plt.gca()\nbounds = ax.get_xbound()\nax.plot(bounds, [7.25, 7.25], '--', c='k')\nax.plot(bounds, [4, 4], '--', c='k')\nax.text(bounds[1], 7.25, ' two clusters', va='center', fontdict={'size': 15})\nax.text(bounds[1], 4, ' three clusters', va='center', fontdict={'size': 15})\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-36. Dendrogram of the clustering shown in Figure 3-35 with lines indicating\nsplits into two and three clusters\nThe dendrogram shows data points as points on the bottom (numbered from 0 to\n11). Then, a tree is plotted with these points (representing single-point clusters) as the\nleaves, and a new node parent is added for each two clusters that are joined.\nReading from bottom to top, the data points 1 and 4 are joined first (as you could see\nin Figure 3-33). Next, points 6 and 9 are joined into a cluster, and so on. At the top\nlevel, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing",
    "level, there are two branches, one consisting of points 11, 0, 5, 10, 7, 6, and 9, and the\nother consisting of points 1, 4, 3, 2, and 8. These correspond to the two largest clus‐\nters in the lefthand side of the plot.\n186 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nThe y-axis in the dendrogram doesn’t just specify when in the agglomerative algo‐\nrithm two clusters get merged. The length of each branch also shows how far apart\nthe merged clusters are. The longest branches in this dendrogram are the three lines\nthat are marked by the dashed line labeled “three clusters.” That these are the longest\nbranches indicates that going from three to two clusters meant merging some very\nfar-apart points. We see this again at the top of the chart, where merging the two\nremaining clusters into a single cluster again bridges a relatively large distance.\nUnfortunately, agglomerative clustering still fails at separating complex shapes like\nthe two_moons dataset. But the same is not true for the next algorithm we will look at,\nDBSCAN.\nDBSCAN\nAnother very useful clustering algorithm is DBSCAN (which stands for “density-\nbased spatial clustering of applications with noise”). The main benefits of DBSCAN\nare that it does not require the user to set the number of clusters a priori, it can cap‐\nture clusters of complex shapes, and it can identify points that are not part of any\ncluster. DBSCAN is somewhat slower than agglomerative clustering and k-means, but\nstill scales to relatively large datasets.\nDBSCAN works by identifying points that are in “crowded” regions of the feature\nspace, where many data points are close together. These regions are referred to as\ndense regions in feature space. The idea behind DBSCAN is that clusters form dense\nregions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.",
    "regions of data, separated by regions that are relatively empty.\nPoints that are within a dense region are called core samples (or core points), and they\nare defined as follows. There are two parameters in DBSCAN: min_samples and eps.\nIf there are at least min_samples many data points within a distance of eps to a given\ndata point, that data point is classified as a core sample. Core samples that are closer\nto each other than the distance eps are put into the same cluster by DBSCAN.\nThe algorithm works by picking an arbitrary point to start with. It then finds all\npoints with distance eps or less from that point. If there are less than min_samples\npoints within distance eps of the starting point, this point is labeled as noise, meaning\nthat it doesn’t belong to any cluster. If there are more than min_samples points within\na distance of eps, the point is labeled a core sample and assigned a new cluster label.\nThen, all neighbors (within eps) of the point are visited. If they have not been\nassigned a cluster yet, they are assigned the new cluster label that was just created. If\nthey are core samples, their neighbors are visited in turn, and so on. The cluster\ngrows until there are no more core samples within distance eps of the cluster. Then\nanother point that hasn’t yet been visited is picked, and the same procedure is\nrepeated.\nClustering \n| \n187\nIn the end, there are three kinds of points: core points, points that are within distance\neps of core points (called boundary points), and noise. When the DBSCAN algorithm\nis run on a particular dataset multiple times, the clustering of the core points is always\nthe same, and the same points will always be labeled as noise. However, a boundary\npoint might be neighbor to core samples of more than one cluster. Therefore, the\ncluster membership of boundary points depends on the order in which points are vis‐\nited. Usually there are only few boundary points, and this slight dependence on the\norder of points is not important.",
    "point might be neighbor to core samples of more than one cluster. Therefore, the\ncluster membership of boundary points depends on the order in which points are vis‐\nited. Usually there are only few boundary points, and this slight dependence on the\norder of points is not important.\nLet’s apply DBSCAN on the synthetic dataset we used to demonstrate agglomerative\nclustering. Like agglomerative clustering, DBSCAN does not allow predictions on\nnew test data, so we will use the fit_predict method to perform clustering and\nreturn the cluster labels in one step:\nIn[65]:\nfrom sklearn.cluster import DBSCAN\nX, y = make_blobs(random_state=0, n_samples=12)\ndbscan = DBSCAN()\nclusters = dbscan.fit_predict(X)\nprint(\"Cluster memberships:\\n{}\".format(clusters))\nOut[65]:\nCluster memberships:\n[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nAs you can see, all data points were assigned the label -1, which stands for noise. This\nis a consequence of the default parameter settings for eps and min_samples, which\nare not tuned for small toy datasets. The cluster assignments for different values of\nmin_samples and eps are shown below, and visualized in Figure 3-37:\nIn[66]:\nmglearn.plots.plot_dbscan()\nOut[66]:\nmin_samples: 2 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\nmin_samples: 2 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 2 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 2 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 3 eps: 1.000000  cluster: [-1  0  0 -1  0 -1  1  1  0  1 -1 -1]\nmin_samples: 3 eps: 1.500000  cluster: [0 1 1 1 1 0 2 2 1 2 2 0]\nmin_samples: 3 eps: 2.000000  cluster: [0 1 1 1 1 0 0 0 1 0 0 0]\nmin_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]",
    "min_samples: 3 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\nmin_samples: 5 eps: 1.000000  cluster: [-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1]\nmin_samples: 5 eps: 1.500000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 2.000000  cluster: [-1  0  0  0  0 -1 -1 -1  0 -1 -1 -1]\nmin_samples: 5 eps: 3.000000  cluster: [0 0 0 0 0 0 0 0 0 0 0 0]\n188 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-37. Cluster assignments found by DBSCAN with varying settings for the\nmin_samples and eps parameters\nIn this plot, points that belong to clusters are solid, while the noise points are shown\nin white. Core samples are shown as large markers, while boundary points are dis‐\nplayed as smaller markers. Increasing eps (going from left to right in the figure)\nmeans that more points will be included in a cluster. This makes clusters grow, but\nmight also lead to multiple clusters joining into one. Increasing min_samples (going\nfrom top to bottom in the figure) means that fewer points will be core points, and\nmore points will be labeled as noise.\nThe parameter eps is somewhat more important, as it determines what it means for\npoints to be “close.” Setting eps to be very small will mean that no points are core\nsamples, and may lead to all points being labeled as noise. Setting eps to be very large\nwill result in all points forming a single cluster.\nThe min_samples setting mostly determines whether points in less dense regions will\nbe labeled as outliers or as their own clusters. If you decrease min_samples, anything\nthat would have been a cluster with less than min_samples many samples will now be\nlabeled as noise. min_samples therefore determines the minimum cluster size. You\ncan see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\nClustering \n| \n189\npoints, one of five points, and one of three points. Using min_samples=5, the two",
    "can see this very clearly in Figure 3-37, when going from min_samples=3 to min_sam\nples=5 with eps=1.5. With min_samples=3, there are three clusters: one of four\nClustering \n| \n189\npoints, one of five points, and one of three points. Using min_samples=5, the two\nsmaller clusters (with three and four points) are now labeled as noise, and only the\ncluster with five samples remains.\nWhile DBSCAN doesn’t require setting the number of clusters explicitly, setting eps\nimplicitly controls how many clusters will be found. Finding a good setting for eps is\nsometimes easier after scaling the data using StandardScaler or MinMaxScaler, as\nusing these scaling techniques will ensure that all features have similar ranges.\nFigure 3-38 shows the result of running DBSCAN on the two_moons dataset. The\nalgorithm actually finds the two half-circles and separates them using the default\nsettings:\nIn[67]:\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\ndbscan = DBSCAN()\nclusters = dbscan.fit_predict(X_scaled)\n# plot the cluster assignments\nplt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm2, s=60)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nAs the algorithm produced the desired number of clusters (two), the parameter set‐\ntings seem to work well. If we decrease eps to 0.2 (from the default of 0.5), we will\nget eight clusters, which is clearly too many. Increasing eps to 0.7 results in a single\ncluster.\nWhen using DBSCAN, you need to be careful about handling the returned cluster\nassignments. The use of -1 to indicate noise might result in unexpected effects when\nusing the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms",
    "using the cluster labels to index another array.\n190 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-38. Cluster assignment found by DBSCAN using the default value of eps=0.5\nComparing and Evaluating Clustering Algorithms\nOne of the challenges in applying clustering algorithms is that it is very hard to assess\nhow well an algorithm worked, and to compare outcomes between different algo‐\nrithms. After talking about the algorithms behind k-means, agglomerative clustering,\nand DBSCAN, we will now compare them on some real-world datasets.\nEvaluating clustering with ground truth\nThere are metrics that can be used to assess the outcome of a clustering algorithm\nrelative to a ground truth clustering, the most important ones being the adjusted rand\nindex (ARI) and normalized mutual information (NMI), which both provide a quanti‐\ntative measure between 0 and 1.\nHere, we compare the k-means, agglomerative clustering, and DBSCAN algorithms\nusing ARI. We also include what it looks like when we randomly assign points to two\nclusters for comparison (see Figure 3-39):\nClustering \n| \n191\nIn[68]:\nfrom sklearn.metrics.cluster import adjusted_rand_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# make a list of algorithms to use\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(",
    "random_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n                cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment - ARI: {:.2f}\".format(\n        adjusted_rand_score(y, random_clusters)))\nfor ax, algorithm in zip(axes[1:], algorithms):\n    # plot the cluster assignments and cluster centers\n    clusters = algorithm.fit_predict(X_scaled)\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters,\n               cmap=mglearn.cm3, s=60)\n    ax.set_title(\"{} - ARI: {:.2f}\".format(algorithm.__class__.__name__,\n                                           adjusted_rand_score(y, clusters)))\nFigure 3-39. Comparing random assignment, k-means, agglomerative clustering, and\nDBSCAN on the two_moons dataset using the supervised ARI score\nThe adjusted rand index provides intuitive results, with a random cluster assignment\nhaving a score of 0 and DBSCAN (which recovers the desired clustering perfectly)\nhaving a score of 1.\n192 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nA common mistake when evaluating clustering in this way is to use accuracy_score\ninstead of adjusted_rand_score, normalized_mutual_info_score, or some other\nclustering metric. The problem in using accuracy is that it requires the assigned clus‐\nter labels to exactly match the ground truth. However, the cluster labels themselves\nare meaningless—the only thing that matters is which points are in the same cluster:\nIn[69]:\nfrom sklearn.metrics import accuracy_score\n# these two labelings of points correspond to the same clustering\nclusters1 = [0, 0, 1, 1, 0]\nclusters2 = [1, 1, 0, 0, 1]\n# accuracy is zero, as none of the labels are the same\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# adjusted rand score is 1, as the clustering is exactly the same\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\nOut[69]:\nAccuracy: 0.00\nARI: 1.00",
    "# accuracy is zero, as none of the labels are the same\nprint(\"Accuracy: {:.2f}\".format(accuracy_score(clusters1, clusters2)))\n# adjusted rand score is 1, as the clustering is exactly the same\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(clusters1, clusters2)))\nOut[69]:\nAccuracy: 0.00\nARI: 1.00\nEvaluating clustering without ground truth\nAlthough we have just shown one way to evaluate clustering algorithms, in practice,\nthere is a big problem with using measures like ARI. When applying clustering algo‐\nrithms, there is usually no ground truth to which to compare the results. If we knew\nthe right clustering of the data, we could use this information to build a supervised\nmodel like a classifier. Therefore, using metrics like ARI and NMI usually only helps\nin developing algorithms, not in assessing success in an application.\nThere are scoring metrics for clustering that don’t require ground truth, like the sil‐\nhouette coefficient. However, these often don’t work well in practice. The silhouette\nscore computes the compactness of a cluster, where higher is better, with a perfect\nscore of 1. While compact clusters are good, compactness doesn’t allow for complex\nshapes.\nHere is an example comparing the outcome of k-means, agglomerative clustering,\nand DBSCAN on the two-moons dataset using the silhouette score (Figure 3-40):\nIn[70]:\nfrom sklearn.metrics.cluster import silhouette_score\nX, y = make_moons(n_samples=200, noise=0.05, random_state=0)\n# rescale the data to zero mean and unit variance\nscaler = StandardScaler()\nscaler.fit(X)\nX_scaled = scaler.transform(X)\nClustering \n| \n193\nfig, axes = plt.subplots(1, 4, figsize=(15, 3),\n                         subplot_kw={'xticks': (), 'yticks': ()})\n# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n    cmap=mglearn.cm3, s=60)",
    "# create a random cluster assignment for reference\nrandom_state = np.random.RandomState(seed=0)\nrandom_clusters = random_state.randint(low=0, high=2, size=len(X))\n# plot random assignment\naxes[0].scatter(X_scaled[:, 0], X_scaled[:, 1], c=random_clusters,\n    cmap=mglearn.cm3, s=60)\naxes[0].set_title(\"Random assignment: {:.2f}\".format(\n    silhouette_score(X_scaled, random_clusters)))\nalgorithms = [KMeans(n_clusters=2), AgglomerativeClustering(n_clusters=2),\n              DBSCAN()]\nfor ax, algorithm in zip(axes[1:], algorithms):\n    clusters = algorithm.fit_predict(X_scaled)\n    # plot the cluster assignments and cluster centers\n    ax.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters, cmap=mglearn.cm3,\n               s=60)\n    ax.set_title(\"{} : {:.2f}\".format(algorithm.__class__.__name__,\n                                      silhouette_score(X_scaled, clusters)))\nFigure 3-40. Comparing random assignment, k-means, agglomerative clustering, and\nDBSCAN on the two_moons dataset using the unsupervised silhouette score—the more\nintuitive result of DBSCAN has a lower silhouette score than the assignments found by\nk-means\nAs you can see, k-means gets the highest silhouette score, even though we might pre‐\nfer the result produced by DBSCAN. A slightly better strategy for evaluating clusters\nis using robustness-based clustering metrics. These run an algorithm after adding\nsome noise to the data, or using different parameter settings, and compare the out‐\ncomes. The idea is that if many algorithm parameters and many perturbations of the\ndata return the same result, it is likely to be trustworthy. Unfortunately, this strategy is\nnot implemented in scikit-learn at the time of writing.\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\nknow if there is any semantic meaning in the clustering, or whether the clustering\n194 \n| \nChapter 3: Unsupervised Learning and Preprocessing",
    "not implemented in scikit-learn at the time of writing.\nEven if we get a very robust clustering, or a very high silhouette score, we still don’t\nknow if there is any semantic meaning in the clustering, or whether the clustering\n194 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nreflects an aspect of the data that we are interested in. Let’s go back to the example of\nface images. We hope to find groups of similar faces—say, men and women, or old\npeople and young people, or people with beards and without. Let’s say we cluster the\ndata into two clusters, and all algorithms agree about which points should be clus‐\ntered together. We still don’t know if the clusters that are found correspond in any\nway to the concepts we are interested in. It could be that they found side views versus\nfront views, or pictures taken at night versus pictures taken during the day, or pic‐\ntures taken with iPhones versus pictures taken with Android phones. The only way to\nknow whether the clustering corresponds to anything we are interested in is to ana‐\nlyze the clusters manually.\nComparing algorithms on the faces dataset\nLet’s apply the k-means, DBSCAN, and agglomerative clustering algorithms to the\nLabeled Faces in the Wild dataset, and see if any of them find interesting structure.\nWe will use the eigenface representation of the data, as produced by\nPCA(whiten=True), with 100 components:\nIn[71]:\n# extract eigenfaces from lfw data and transform data\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=100, whiten=True, random_state=0)\npca.fit_transform(X_people)\nX_pca = pca.transform(X_people)\nWe saw earlier that this is a more semantic representation of the face images than the\nraw pixels. It will also make computation faster. A good exercise would be for you to\nrun the following experiments on the original data, without PCA, and see if you find\nsimilar clusters.\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\njust discussed:",
    "raw pixels. It will also make computation faster. A good exercise would be for you to\nrun the following experiments on the original data, without PCA, and see if you find\nsimilar clusters.\nAnalyzing the faces dataset with DBSCAN.    We will start by applying DBSCAN, which we\njust discussed:\nIn[72]:\n# apply DBSCAN with default parameters\ndbscan = DBSCAN()\nlabels = dbscan.fit_predict(X_pca)\nprint(\"Unique labels: {}\".format(np.unique(labels)))\nOut[72]:\nUnique labels: [-1]\nWe see that all the returned labels are –1, so all of the data was labeled as “noise” by\nDBSCAN. There are two things we can change to help this: we can make eps higher,\nto expand the neighborhood of each point, and set min_samples lower, to consider\nsmaller groups of points as clusters. Let’s try changing min_samples first:\nClustering \n| \n195\nIn[73]:\ndbscan = DBSCAN(min_samples=3)\nlabels = dbscan.fit_predict(X_pca)\nprint(\"Unique labels: {}\".format(np.unique(labels)))\nOut[73]:\nUnique labels: [-1]\nEven when considering groups of three points, everything is labeled as noise. So, we\nneed to increase eps:\nIn[74]:\ndbscan = DBSCAN(min_samples=3, eps=15)\nlabels = dbscan.fit_predict(X_pca)\nprint(\"Unique labels: {}\".format(np.unique(labels)))\nOut[74]:\nUnique labels: [-1  0]\nUsing a much larger eps of 15, we get only a single cluster and noise points. We can\nuse this result to find out what the “noise” looks like compared to the rest of the data.\nTo understand better what’s happening, let’s look at how many points are noise, and\nhow many points are inside the cluster:\nIn[75]:\n# Count number of points in all clusters and noise.\n# bincount doesn't allow negative numbers, so we need to add 1.\n# The first number in the result corresponds to noise points.\nprint(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\nOut[75]:\nNumber of points per cluster: [  27 2036]\nThere are very few noise points—only 27—so we can look at all of them (see\nFigure 3-41):\nIn[76]:\nnoise = X_people[labels==-1]",
    "print(\"Number of points per cluster: {}\".format(np.bincount(labels + 1)))\nOut[75]:\nNumber of points per cluster: [  27 2036]\nThere are very few noise points—only 27—so we can look at all of them (see\nFigure 3-41):\nIn[76]:\nnoise = X_people[labels==-1]\nfig, axes = plt.subplots(3, 9, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(12, 4))\nfor image, ax in zip(noise, axes.ravel()):\n    ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n196 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-41. Samples from the faces dataset labeled as noise by DBSCAN\nComparing these images to the random sample of face images from Figure 3-7, we\ncan guess why they were labeled as noise: the fifth image in the first row shows a per‐\nson drinking from a glass, there are images of people wearing hats, and in the last\nimage there’s a hand in front of the person’s face. The other images contain odd angles\nor crops that are too close or too wide.\nThis kind of analysis—trying to find “the odd one out”—is called outlier detection. If\nthis was a real application, we might try to do a better job of cropping images, to get\nmore homogeneous data. There is little we can do about people in photos sometimes\nwearing hats, drinking, or holding something in front of their faces, but it’s good to\nknow that these are issues in the data that any algorithm we might apply needs to\nhandle.\nIf we want to find more interesting clusters than just one large one, we need to set eps\nsmaller, somewhere between 15 and 0.5 (the default). Let’s have a look at what differ‐\nent values of eps result in:\nIn[77]:\nfor eps in [1, 3, 5, 7, 9, 11, 13]:\n    print(\"\\neps={}\".format(eps))\n    dbscan = DBSCAN(eps=eps, min_samples=3)\n    labels = dbscan.fit_predict(X_pca)\n    print(\"Clusters present: {}\".format(np.unique(labels)))\n    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\nOut[78]:\neps=1\nClusters present: [-1]\nCluster sizes: [2063]\neps=3\nClusters present: [-1]",
    "dbscan = DBSCAN(eps=eps, min_samples=3)\n    labels = dbscan.fit_predict(X_pca)\n    print(\"Clusters present: {}\".format(np.unique(labels)))\n    print(\"Cluster sizes: {}\".format(np.bincount(labels + 1)))\nOut[78]:\neps=1\nClusters present: [-1]\nCluster sizes: [2063]\neps=3\nClusters present: [-1]\nCluster sizes: [2063]\nClustering \n| \n197\neps=5\nClusters present: [-1]\nCluster sizes: [2063]\neps=7\nClusters present: [-1  0  1  2  3  4  5  6  7  8  9 10 11 12]\nCluster sizes: [2006  4  6  6  6  9  3  3  4  3  3  3  3  4]\neps=9\nClusters present: [-1  0  1  2]\nCluster sizes: [1269  788    3    3]\neps=11\nClusters present: [-1  0]\nCluster sizes: [ 430 1633]\neps=13\nClusters present: [-1  0]\nCluster sizes: [ 112 1951]\nFor low settings of eps, all points are labeled as noise. For eps=7, we get many noise\npoints and many smaller clusters. For eps=9 we still get many noise points, but we get\none big cluster and some smaller clusters. Starting from eps=11, we get only one large\ncluster and noise.\nWhat is interesting to note is that there is never more than one large cluster. At most,\nthere is one large cluster containing most of the points, and some smaller clusters.\nThis indicates that there are not two or three different kinds of face images in the data\nthat are very distinct, but rather that all images are more or less equally similar to (or\ndissimilar from) the rest.\nThe results for eps=7 look most interesting, with many small clusters. We can investi‐\ngate this clustering in more detail by visualizing all of the points in each of the 13\nsmall clusters (Figure 3-42):\nIn[78]:\ndbscan = DBSCAN(min_samples=3, eps=7)\nlabels = dbscan.fit_predict(X_pca)\nfor cluster in range(max(labels) + 1):\n    mask = labels == cluster\n    n_images =  np.sum(mask)\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\n                             subplot_kw={'xticks': (), 'yticks': ()})\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):",
    "mask = labels == cluster\n    n_images =  np.sum(mask)\n    fig, axes = plt.subplots(1, n_images, figsize=(n_images * 1.5, 4),\n                             subplot_kw={'xticks': (), 'yticks': ()})\n    for image, label, ax in zip(X_people[mask], y_people[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1])\n198 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-42. Clusters found by DBSCAN with eps=7\nSome of the clusters correspond to people with very distinct faces (within this data‐\nset), such as Sharon or Koizumi. Within each cluster, the orientation of the face is also\nClustering \n| \n199\nquite fixed, as well as the facial expression. Some of the clusters contain faces of mul‐\ntiple people, but they share a similar orientation and expression.\nThis concludes our analysis of the DBSCAN algorithm applied to the faces dataset. As\nyou can see, we are doing a manual analysis here, different from the much more auto‐\nmatic search approach we could use for supervised learning based on R2 score or\naccuracy.\nLet’s move on to applying k-means and agglomerative clustering.\nAnalyzing the faces dataset with k-means.    We saw that it was not possible to create\nmore than one big cluster using DBSCAN. Agglomerative clustering and k-means are\nmuch more likely to create clusters of even size, but we do need to set a target num‐\nber of clusters. We could set the number of clusters to the known number of people in\nthe dataset, though it is very unlikely that an unsupervised clustering algorithm will\nrecover them. Instead, we can start with a low number of clusters, like 10, which\nmight allow us to analyze each of the clusters:\nIn[79]:\n# extract clusters with k-means\nkm = KMeans(n_clusters=10, random_state=0)\nlabels_km = km.fit_predict(X_pca)\nprint(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\nOut[79]:\nCluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]",
    "In[79]:\n# extract clusters with k-means\nkm = KMeans(n_clusters=10, random_state=0)\nlabels_km = km.fit_predict(X_pca)\nprint(\"Cluster sizes k-means: {}\".format(np.bincount(labels_km)))\nOut[79]:\nCluster sizes k-means: [269 128 170 186 386 222 237  64 253 148]\nAs you can see, k-means clustering partitioned the data into relatively similarly sized\nclusters from 64 to 386. This is quite different from the result of DBSCAN.\nWe can further analyze the outcome of k-means by visualizing the cluster centers\n(Figure 3-43). As we clustered in the representation produced by PCA, we need to\nrotate the cluster centers back into the original space to visualize them, using\npca.inverse_transform:\nIn[80]:\nfig, axes = plt.subplots(2, 5, subplot_kw={'xticks': (), 'yticks': ()},\n                         figsize=(12, 4))\nfor center, ax in zip(km.cluster_centers_, axes.ravel()):\n    ax.imshow(pca.inverse_transform(center).reshape(image_shape),\n              vmin=0, vmax=1)\n200 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-43. Cluster centers found by k-means when setting the number of clusters to 10\nThe cluster centers found by k-means are very smooth versions of faces. This is not\nvery surprising, given that each center is an average of 64 to 386 face images. Working\nwith a reduced PCA representation adds to the smoothness of the images (compared\nto the faces reconstructed using 100 PCA dimensions in Figure 3-11). The clustering\nseems to pick up on different orientations of the face, different expressions (the third\ncluster center seems to show a smiling face), and the presence of shirt collars (see the\nsecond-to-last cluster center).\nFor a more detailed view, in Figure 3-44 we show for each cluster center the five most\ntypical images in the cluster (the images assigned to the cluster that are closest to the\ncluster center) and the five most atypical images in the cluster (the images assigned to\nthe cluster that are furthest from the cluster center):\nIn[81]:",
    "typical images in the cluster (the images assigned to the cluster that are closest to the\ncluster center) and the five most atypical images in the cluster (the images assigned to\nthe cluster that are furthest from the cluster center):\nIn[81]:\nmglearn.plots.plot_kmeans_faces(km, pca, X_pca, X_people,\n                                y_people, people.target_names)\nClustering \n| \n201\nFigure 3-44. Sample images for each cluster found by k-means—the cluster centers are\non the left, followed by the five closest points to each center and the five points that are\nassigned to the cluster but are furthest away from the center\n202 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-44 confirms our intuition about smiling faces for the third cluster, and also\nthe importance of orientation for the other clusters. The “atypical” points are not very\nsimilar to the cluster centers, though, and their assignment seems somewhat arbi‐\ntrary. This can be attributed to the fact that k-means partitions all the data points and\ndoesn’t have a concept of “noise” points, as DBSCAN does. Using a larger number of\nclusters, the algorithm could find finer distinctions. However, adding more clusters\nmakes manual inspection even harder.\nAnalyzing the faces dataset with agglomerative clustering.    Now, let’s look at the results of\nagglomerative clustering:\nIn[82]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=10)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"Cluster sizes agglomerative clustering: {}\".format(\n    np.bincount(labels_agg)))\nOut[82]:\nCluster sizes agglomerative clustering: [255 623  86 102 122 199 265  26 230 155]\nAgglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.",
    "Agglomerative clustering also produces relatively equally sized clusters, with cluster\nsizes between 26 and 623. These are more uneven than those produced by k-means,\nbut much more even than the ones produced by DBSCAN.\nWe can compute the ARI to measure whether the two partitions of the data given by\nagglomerative clustering and k-means are similar:\nIn[83]:\nprint(\"ARI: {:.2f}\".format(adjusted_rand_score(labels_agg, labels_km)))\nOut[83]:\nARI: 0.13\nAn ARI of only 0.13 means that the two clusterings labels_agg and labels_km have\nlittle in common. This is not very surprising, given the fact that points further away\nfrom the cluster centers seem to have little in common for k-means.\nNext, we might want to plot the dendrogram (Figure 3-45). We’ll limit the depth of\nthe tree in the plot, as branching down to the individual 2,063 data points would\nresult in an unreadably dense plot:\nClustering \n| \n203\nIn[84]:\nlinkage_array = ward(X_pca)\n# now we plot the dendrogram for the linkage_array\n# containing the distances between clusters\nplt.figure(figsize=(20, 5))\ndendrogram(linkage_array, p=7, truncate_mode='level', no_labels=True)\nplt.xlabel(\"Sample index\")\nplt.ylabel(\"Cluster distance\")\nFigure 3-45. Dendrogram of agglomerative clustering on the faces dataset\nCreating 10 clusters, we cut across the tree at the very top, where there are 10 vertical\nlines. In the dendrogram for the toy data shown in Figure 3-36, you could see by the\nlength of the branches that two or three clusters might capture the data appropriately.\nFor the faces data, there doesn’t seem to be a very natural cutoff point. There are\nsome branches that represent more distinct groups, but there doesn’t appear to be a\nparticular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that",
    "particular number of clusters that is a good fit. This is not surprising, given the results\nof DBSCAN, which tried to cluster all points together.\nLet’s visualize the 10 clusters, as we did for k-means earlier (Figure 3-46). Note that\nthere is no notion of cluster center in agglomerative clustering (though we could\ncompute the mean), and we simply show the first couple of points in each cluster. We\nshow the number of points in each cluster to the left of the first image:\nIn[85]:\nn_clusters = 10\nfor cluster in range(n_clusters):\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 10, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    axes[0].set_ylabel(np.sum(mask))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n204 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-46. Random images from the clusters generated by In[82]—each row corre‐\nsponds to one cluster; the number to the left lists the number of images in each cluster\nClustering \n| \n205\nWhile some of the clusters seem to have a semantic theme, many of them are too\nlarge to be actually homogeneous. To get more homogeneous clusters, we can run the\nalgorithm again, this time with 40 clusters, and pick out some of the clusters that are\nparticularly interesting (Figure 3-47):\nIn[86]:\n# extract clusters with ward agglomerative clustering\nagglomerative = AgglomerativeClustering(n_clusters=40)\nlabels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster",
    "labels_agg = agglomerative.fit_predict(X_pca)\nprint(\"cluster sizes agglomerative clustering: {}\".format(np.bincount(labels_agg)))\nn_clusters = 40\nfor cluster in [10, 13, 19, 22, 36]: # hand-picked \"interesting\" clusters\n    mask = labels_agg == cluster\n    fig, axes = plt.subplots(1, 15, subplot_kw={'xticks': (), 'yticks': ()},\n                             figsize=(15, 8))\n    cluster_size = np.sum(mask)\n    axes[0].set_ylabel(\"#{}: {}\".format(cluster, cluster_size))\n    for image, label, asdf, ax in zip(X_people[mask], y_people[mask],\n                                      labels_agg[mask], axes):\n        ax.imshow(image.reshape(image_shape), vmin=0, vmax=1)\n        ax.set_title(people.target_names[label].split()[-1],\n                     fontdict={'fontsize': 9})\n    for i in range(cluster_size, 15):\n        axes[i].set_visible(False)\nOut[86]:\ncluster sizes agglomerative clustering:\n [ 58  80  79  40 222  50  55  78 172  28  26  34  14  11  60  66 152  27\n  47  31  54   5   8  56   3   5   8  18  22  82  37  89  28  24  41  40\n  21  10 113  69]\n206 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nFigure 3-47. Images from selected clusters found by agglomerative clustering when set‐\nting the number of clusters to 40—the text to the left shows the index of the cluster and\nthe total number of points in the cluster\nHere, the clustering seems to have picked up on “dark skinned and smiling,” “collared\nshirt,” “smiling woman,” “Hussein,” and “high forehead.” We could also find these\nhighly similar clusters using the dendrogram, if we did more a detailed analysis.\nSummary of Clustering Methods\nThis section has shown that applying and evaluating clustering is a highly qualitative\nprocedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and",
    "procedure, and often most helpful in the exploratory phase of data analysis. We\nlooked at three clustering algorithms: k-means, DBSCAN, and agglomerative cluster‐\ning. All three have a way of controlling the granularity of clustering. k-means and\nagglomerative clustering allow you to specify the number of desired clusters, while\nDBSCAN lets you define proximity using the eps parameter, which indirectly influ‐\nences cluster size. All three methods can be used on large, real-world datasets, are rel‐\natively easy to understand, and allow for clustering into many clusters.\nEach of the algorithms has somewhat different strengths. k-means allows for a char‐\nacterization of the clusters using the cluster means. It can also be viewed as a decom‐\nposition method, where each data point is represented by its cluster center. DBSCAN\nallows for the detection of “noise points” that are not assigned any cluster, and it can\nhelp automatically determine the number of clusters. In contrast to the other two\nmethods, it allow for complex cluster shapes, as we saw in the two_moons example.\nDBSCAN sometimes produces clusters of very differing size, which can be a strength\nor a weakness. Agglomerative clustering can provide a whole hierarchy of possible\npartitions of the data, which can be easily inspected via dendrograms.\nClustering \n| \n207\nSummary and Outlook\nThis chapter introduced a range of unsupervised learning algorithms that can be\napplied for exploratory data analysis and preprocessing. Having the right representa‐\ntion of the data is often crucial for supervised or unsupervised learning to succeed,\nand preprocessing and decomposition methods play an important part in data prepa‐\nration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory",
    "ration.\nDecomposition, manifold learning, and clustering are essential tools to further your\nunderstanding of your data, and can be the only ways to make sense of your data in\nthe absence of supervision information. Even in a supervised setting, exploratory\ntools are important for a better understanding of the properties of the data. Often it is\nhard to quantify the usefulness of an unsupervised algorithm, though this shouldn’t\ndeter you from using them to gather insights from your data. With these methods\nunder your belt, you are now equipped with all the essential learning algorithms that\nmachine learning practitioners use every day.\nWe encourage you to try clustering and decomposition methods both on two-\ndimensional toy data and on real-world datasets included in scikit-learn, like the\ndigits, iris, and cancer datasets.\n208 \n| \nChapter 3: Unsupervised Learning and Preprocessing\nSummary of the Estimator Interface\nLet’s briefly review the API that we introduced in Chapters 2 and 3. All algorithms in\nscikit-learn, whether preprocessing, supervised learning, or unsupervised learning\nalgorithms, are implemented as classes. These classes are called estimators in scikit-\nlearn. To apply an algorithm, you first have to instantiate an object of the particular\nclass:\nIn[87]:\nfrom sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nThe estimator class contains the algorithm, and also stores the model that is learned\nfrom data using the algorithm.\nYou should set any parameters of the model when constructing the model object.\nThese parameters include regularization, complexity control, number of clusters to\nfind, etc. All estimators have a fit method, which is used to build the model. The fit\nmethod always requires as its first argument the data X, represented as a NumPy array\nor a SciPy sparse matrix, where each row represents a single data point. The data X is\nalways assumed to be a NumPy array or SciPy sparse matrix that has continuous",
    "method always requires as its first argument the data X, represented as a NumPy array\nor a SciPy sparse matrix, where each row represents a single data point. The data X is\nalways assumed to be a NumPy array or SciPy sparse matrix that has continuous\n(floating-point) entries. Supervised algorithms also require a y argument, which is a\none-dimensional NumPy array containing target values for regression or classifica‐\ntion (i.e., the known output labels or responses).\nThere are two main ways to apply a learned model in scikit-learn. To create a pre‐\ndiction in the form of a new output like y, you use the predict method. To create a\nnew representation of the input data X, you use the transform method. Table 3-1\nsummarizes the use cases of the predict and transform methods.\nTable 3-1. scikit-learn API summary\nestimator.fit(x_train, [y_train])\nestimator.predict(X_text)\nestimator.transform(X_test)\nClassification\nPreprocessing\nRegression\nDimensionality reduction\nClustering\nFeature extraction\n \nFeature selection\nAdditionally, all supervised models have a score(X_test, y_test) method that\nallows an evaluation of the model. In Table 3-1, X_train and y_train refer to the\ntraining data and training labels, while X_test and y_test refer to the test data and\ntest labels (if applicable).\nSummary and Outlook \n| \n209\nCHAPTER 4\nRepresenting Data and\nEngineering Features\nSo far, we’ve assumed that our data comes in as a two-dimensional array of floating-\npoint numbers, where each column is a continuous feature that describes the data\npoints. For many applications, this is not how the data is collected. A particularly\ncommon type of feature is the categorical features. Also known as discrete features,\nthese are usually not numeric. The distinction between categorical features and con‐\ntinuous features is analogous to the distinction between classification and regression,\nonly on the input side rather than the output side. Examples of continuous features",
    "these are usually not numeric. The distinction between categorical features and con‐\ntinuous features is analogous to the distinction between classification and regression,\nonly on the input side rather than the output side. Examples of continuous features\nthat we have seen are pixel brightnesses and size measurements of plant flowers.\nExamples of categorical features are the brand of a product, the color of a product, or\nthe department (books, clothing, hardware) it is sold in. These are all properties that\ncan describe a product, but they don’t vary in a continuous way. A product belongs\neither in the clothing department or in the books department. There is no middle\nground between books and clothing, and no natural order for the different categories\n(books is not greater or less than clothing, hardware is not between books and cloth‐\ning, etc.).\nRegardless of the types of features your data consists of, how you represent them can\nhave an enormous effect on the performance of machine learning models. We saw in\nChapters 2 and 3 that scaling of the data is important. In other words, if you don’t\nrescale your data (say, to unit variance), then it makes a difference whether you repre‐\nsent a measurement in centimeters or inches. We also saw in Chapter 2 that it can be\nhelpful to augment your data with additional features, like adding interactions (prod‐\nucts) of features or more general polynomials.\nThe question of how to represent your data best for a particular application is known\nas feature engineering, and it is one of the main tasks of data scientists and machine\n211\nlearning practitioners trying to solve real-world problems. Representing your data in\nthe right way can have a bigger influence on the performance of a supervised model\nthan the exact parameters you choose.\nIn this chapter, we will first go over the important and very common case of categori‐\ncal features, and then give some examples of helpful transformations for specific",
    "the right way can have a bigger influence on the performance of a supervised model\nthan the exact parameters you choose.\nIn this chapter, we will first go over the important and very common case of categori‐\ncal features, and then give some examples of helpful transformations for specific\ncombinations of features and models.\nCategorical Variables\nAs an example, we will use the dataset of adult incomes in the United States, derived\nfrom the 1994 census database. The task of the adult dataset is to predict whether a\nworker has an income of over $50,000 or under $50,000. The features in this dataset\ninclude the workers’ ages, how they are employed (self employed, private industry\nemployee, government employee, etc.), their education, their gender, their working\nhours per week, occupation, and more. Table 4-1 shows the first few entries in the\ndataset.\nTable 4-1. The first few entries in the adult dataset\nage\nworkclass\neducation\ngender hours-per-week occupation\nincome\n0\n39\nState-gov\nBachelors\nMale\n40\nAdm-clerical\n<=50K\n1\n50\nSelf-emp-not-inc\nBachelors\nMale\n13\nExec-managerial\n<=50K\n2\n38\nPrivate\nHS-grad\nMale\n40\nHandlers-cleaners <=50K\n3\n53\nPrivate\n11th\nMale\n40\nHandlers-cleaners <=50K\n4\n28\nPrivate\nBachelors\nFemale\n40\nProf-specialty\n<=50K\n5\n37\nPrivate\nMasters\nFemale\n40\nExec-managerial\n<=50K\n6\n49\nPrivate\n9th\nFemale\n16\nOther-service\n<=50K\n7\n52\nSelf-emp-not-inc\nHS-grad\nMale\n45\nExec-managerial\n>50K\n8\n31\nPrivate\nMasters\nFemale\n50\nProf-specialty\n>50K\n9\n42\nPrivate\nBachelors\nMale\n40\nExec-managerial\n>50K\n10 37\nPrivate\nSome-college Male\n80\nExec-managerial\n>50K\nThe task is phrased as a classification task with the two classes being income <=50k\nand >50k. It would also be possible to predict the exact income, and make this a\nregression task. However, that would be much more difficult, and the 50K division is\ninteresting to understand on its own.\nIn this dataset, age and hours-per-week are continuous features, which we know",
    "and >50k. It would also be possible to predict the exact income, and make this a\nregression task. However, that would be much more difficult, and the 50K division is\ninteresting to understand on its own.\nIn this dataset, age and hours-per-week are continuous features, which we know\nhow to treat. The workclass, education, sex, and occupation features are categori‐\ncal, however. All of them come from a fixed list of possible values, as opposed to a\nrange, and denote a qualitative property, as opposed to a quantity.\n212 \n| \nChapter 4: Representing Data and Engineering Features\nAs a starting point, let’s say we want to learn a logistic regression classifier on this\ndata. We know from Chapter 2 that a logistic regression makes predictions, ŷ, using\nthe following formula:\nŷ = w[0] * x[0] + w[1] * x[1] + ... + w[p] * x[p] + b > 0\nwhere w[i] and b are coefficients learned from the training set and x[i] are the input\nfeatures. This formula makes sense when x[i] are numbers, but not when x[2] is\n\"Masters\" or \"Bachelors\". Clearly we need to represent our data in some different\nway when applying logistic regression. The next section will explain how we can\novercome this problem.\nOne-Hot-Encoding (Dummy Variables)\nBy far the most common way to represent categorical variables is using the one-hot-\nencoding or one-out-of-N encoding, also known as dummy variables. The idea behind\ndummy variables is to replace a categorical variable with one or more new features\nthat can have the values 0 and 1. The values 0 and 1 make sense in the formula for\nlinear binary classification (and for all other models in scikit-learn), and we can\nrepresent any number of categories by introducing one new feature per category, as\ndescribed here.\nLet’s say for the workclass feature we have possible values of \"Government\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\nrated\". To encode these four possible values, we create four new features, called \"Gov",
    "described here.\nLet’s say for the workclass feature we have possible values of \"Government\nEmployee\", \"Private Employee\", \"Self Employed\", and \"Self Employed Incorpo\nrated\". To encode these four possible values, we create four new features, called \"Gov\nernment Employee\", \"Private Employee\", \"Self Employed\", and \"Self Employed\nIncorporated\". A feature is 1 if workclass for this person has the corresponding\nvalue and 0 otherwise, so exactly one of the four new features will be 1 for each data\npoint. This is why this is called one-hot or one-out-of-N encoding.\nThe principle is illustrated in Table 4-2. A single feature is encoded using four new\nfeatures. When using this data in a machine learning algorithm, we would drop the\noriginal workclass feature and only keep the 0–1 features.\nTable 4-2. Encoding the workclass feature using one-hot encoding\nworkclass\nGovernment Employee\nPrivate Employee\nSelf Employed\nSelf Employed Incorporated\nGovernment Employee\n1\n0\n0\n0\nPrivate Employee\n0\n1\n0\n0\nSelf Employed\n0\n0\n1\n0\nSelf Employed Incorporated 0\n0\n0\n1\nCategorical Variables \n| \n213\nThe one-hot encoding we use is quite similar, but not identical, to\nthe dummy encoding used in statistics. For simplicity, we encode\neach category with a different binary feature. In statistics, it is com‐\nmon to encode a categorical feature with k different possible values\ninto k–1 features (the last one is represented as all zeros). This is\ndone to simplify the analysis (more technically, this will avoid mak‐\ning the data matrix rank-deficient).\nThere are two ways to convert your data to a one-hot encoding of categorical vari‐\nables, using either pandas or scikit-learn. At the time of writing, using pandas is\nslightly easier, so let’s go this route. First we load the data using pandas from a\ncomma-separated values (CSV) file:\nIn[2]:\nimport pandas as pd\n# The file has no headers naming the columns, so we pass header=None\n# and provide the column names explicitly in \"names\"\ndata = pd.read_csv(",
    "slightly easier, so let’s go this route. First we load the data using pandas from a\ncomma-separated values (CSV) file:\nIn[2]:\nimport pandas as pd\n# The file has no headers naming the columns, so we pass header=None\n# and provide the column names explicitly in \"names\"\ndata = pd.read_csv(\n    \"/home/andy/datasets/adult.data\", header=None, index_col=False,\n    names=['age', 'workclass', 'fnlwgt', 'education',  'education-num',\n           'marital-status', 'occupation', 'relationship', 'race', 'gender',\n           'capital-gain', 'capital-loss', 'hours-per-week', 'native-country',\n           'income'])\n# For illustration purposes, we only select some of the columns\ndata = data[['age', 'workclass', 'education', 'gender', 'hours-per-week',\n             'occupation', 'income']]\n# IPython.display allows nice output formatting within the Jupyter notebook\ndisplay(data.head())\nTable 4-3 shows the result.\nTable 4-3. The first five rows of the adult dataset\nage\nworkclass\neducation gender hours-per-week occupation\nincome\n0\n39\nState-gov\nBachelors\nMale\n40\nAdm-clerical\n<=50K\n1\n50\nSelf-emp-not-inc\nBachelors\nMale\n13\nExec-managerial\n<=50K\n2\n38\nPrivate\nHS-grad\nMale\n40\nHandlers-cleaners <=50K\n3\n53\nPrivate\n11th\nMale\n40\nHandlers-cleaners <=50K\n4\n28\nPrivate\nBachelors\nFemale\n40\nProf-specialty\n<=50K\nChecking string-encoded categorical data\nAfter reading a dataset like this, it is often good to first check if a column actually\ncontains meaningful categorical data. When working with data that was input by\nhumans (say, users on a website), there might not be a fixed set of categories, and dif‐\nferences in spelling and capitalization might require preprocessing. For example, it\nmight be that some people specified gender as “male” and some as “man,” and we\n214 \n| \nChapter 4: Representing Data and Engineering Features\nmight want to represent these two inputs using the same category. A good way to\ncheck the contents of a column is using the value_counts function of a pandas",
    "might be that some people specified gender as “male” and some as “man,” and we\n214 \n| \nChapter 4: Representing Data and Engineering Features\nmight want to represent these two inputs using the same category. A good way to\ncheck the contents of a column is using the value_counts function of a pandas\nSeries (the type of a single column in a DataFrame), to show us what the unique val‐\nues are and how often they appear:\nIn[3]:\nprint(data.gender.value_counts())\nOut[3]:\n Male      21790\n Female    10771\nName: gender, dtype: int64\nWe can see that there are exactly two values for gender in this dataset, Male and\nFemale, meaning the data is already in a good format to be represented using one-\nhot-encoding. In a real application, you should look at all columns and check their\nvalues. We will skip this here for brevity’s sake.\nThere is a very simple way to encode the data in pandas, using the get_dummies func‐\ntion. The get_dummies function automatically transforms all columns that have\nobject type (like strings) or are categorical (which is a special pandas concept that we\nhaven’t talked about yet):\nIn[4]:\nprint(\"Original features:\\n\", list(data.columns), \"\\n\")\ndata_dummies = pd.get_dummies(data)\nprint(\"Features after get_dummies:\\n\", list(data_dummies.columns))\nOut[4]:\nOriginal features:\n ['age', 'workclass', 'education', 'gender', 'hours-per-week', 'occupation',\n  'income']\nFeatures after get_dummies:\n ['age', 'hours-per-week', 'workclass_ ?', 'workclass_ Federal-gov',\n  'workclass_ Local-gov', 'workclass_ Never-worked', 'workclass_ Private',\n  'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n  'workclass_ State-gov', 'workclass_ Without-pay', 'education_ 10th',\n  'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n   ...\n  'education_ Preschool', 'education_ Prof-school', 'education_ Some-college',\n  'gender_ Female', 'gender_ Male', 'occupation_ ?',\n  'occupation_ Adm-clerical', 'occupation_ Armed-Forces',",
    "'education_ 11th', 'education_ 12th', 'education_ 1st-4th',\n   ...\n  'education_ Preschool', 'education_ Prof-school', 'education_ Some-college',\n  'gender_ Female', 'gender_ Male', 'occupation_ ?',\n  'occupation_ Adm-clerical', 'occupation_ Armed-Forces',\n  'occupation_ Craft-repair', 'occupation_ Exec-managerial',\n  'occupation_ Farming-fishing', 'occupation_ Handlers-cleaners',\n  ...\n  'occupation_ Tech-support', 'occupation_ Transport-moving',\n  'income_ <=50K', 'income_ >50K']\nCategorical Variables \n| \n215\nYou can see that the continuous features age and hours-per-week were not touched,\nwhile the categorical features were expanded into one new feature for each possible\nvalue:\nIn[5]:\ndata_dummies.head()\nOut[5]:\nage\nhours-\nper-\nweek\nworkclass_ ? workclass_\nFederal-\ngov\nworkclass_\nLocal-gov\n…\noccupation_\nTech-\nsupport\noccupation_\nTransport-\nmoving\nincome_\n<=50K\nincome_\n>50K\n0\n39\n40\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n1.0\n0.0\n1\n50\n13\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n1.0\n0.0\n2\n38\n40\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n1.0\n0.0\n3\n53\n40\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n1.0\n0.0\n4\n28\n40\n0.0\n0.0\n0.0\n…\n0.0\n0.0\n1.0\n0.0\n5 rows × 46 columns\nWe can now use the values attribute to convert the data_dummies DataFrame into a\nNumPy array, and then train a machine learning model on it. Be careful to separate\nthe target variable (which is now encoded in two income columns) from the data\nbefore training a model. Including the output variable, or some derived property of\nthe output variable, into the feature representation is a very common mistake in\nbuilding supervised machine learning models.\nBe careful: column indexing in pandas includes the end of the\nrange, so 'age':'occupation_ Transport-moving' is inclusive of\noccupation_ Transport-moving. This is different from slicing a\nNumPy array, where the end of a range is not included: for exam‐\nple, np.arange(11)[0:10] doesn’t include the entry with index 10.\nIn this case, we extract only the columns containing features—that is, all columns",
    "occupation_ Transport-moving. This is different from slicing a\nNumPy array, where the end of a range is not included: for exam‐\nple, np.arange(11)[0:10] doesn’t include the entry with index 10.\nIn this case, we extract only the columns containing features—that is, all columns\nfrom age to occupation_ Transport-moving. This range contains all the features but\nnot the target:\nIn[6]:\nfeatures = data_dummies.ix[:, 'age':'occupation_ Transport-moving']\n# Extract NumPy arrays\nX = features.values\ny = data_dummies['income_ >50K'].values\nprint(\"X.shape: {}  y.shape: {}\".format(X.shape, y.shape))\n216 \n| \nChapter 4: Representing Data and Engineering Features\nOut[6]:\nX.shape: (32561, 44)  y.shape: (32561,)\nNow the data is represented in a way that scikit-learn can work with, and we can\nproceed as usual:\nIn[7]:\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\nprint(\"Test score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[7]:\nTest score: 0.81\nIn this example, we called get_dummies on a DataFrame containing\nboth the training and the test data. This is important to ensure cat‐\negorical values are represented in the same way in the training set\nand the test set.\nImagine we have the training and test sets in two different Data\nFrames. If the \"Private Employee\" value for the workclass feature\ndoes not appear in the test set, pandas will assume there are only\nthree possible values for this feature and will create only three new\ndummy features. Now our training and test sets have different\nnumbers of features, and we can’t apply the model we learned on\nthe training set to the test set anymore. Even worse, imagine the\nworkclass feature has the values \"Government Employee\" and\n\"Private Employee\" in the training set, and \"Self Employed\" and",
    "numbers of features, and we can’t apply the model we learned on\nthe training set to the test set anymore. Even worse, imagine the\nworkclass feature has the values \"Government Employee\" and\n\"Private Employee\" in the training set, and \"Self Employed\" and\n\"Self Employed Incorporated\" in the test set. In both cases,\npandas will create two new dummy features, so the encoded Data\nFrames will have the same number of features. However, the two\ndummy features have entirely different meanings in the training\nand test sets. The column that means \"Government Employee\" for\nthe training set would encode \"Self Employed\" for the test set.\nIf we built a machine learning model on this data it would work\nvery badly, because it would assume the columns mean the same\nthings (because they are in the same position) when in fact they\nmean very different things. To fix this, either call get_dummies on a\nDataFrame that contains both the training and the test data points,\nor make sure that the column names are the same for the training\nand test sets after calling get_dummies, to ensure they have the\nsame semantics.\nCategorical Variables \n| \n217\nNumbers Can Encode Categoricals\nIn the example of the adult dataset, the categorical variables were encoded as strings.\nOn the one hand, that opens up the possibility of spelling errors, but on the other\nhand, it clearly marks a variable as categorical. Often, whether for ease of storage or\nbecause of the way the data is collected, categorical variables are encoded as integers.\nFor example, imagine the census data in the adult dataset was collected using a ques‐\ntionnaire, and the answers for workclass were recorded as 0 (first box ticked), 1 (sec‐\nond box ticked), 2 (third box ticked), and so on. Now the column will contain\nnumbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\nobvious to someone looking at the table representing the dataset whether they should",
    "ond box ticked), 2 (third box ticked), and so on. Now the column will contain\nnumbers from 0 to 8, instead of strings like \"Private\", and it won’t be immediately\nobvious to someone looking at the table representing the dataset whether they should\ntreat this variable as continuous or categorical. Knowing that the numbers indicate\nemployment status, however, it is clear that these are very distinct states and should\nnot be modeled by a single continuous variable.\nCategorical features are often encoded using integers. That they are\nnumbers doesn’t mean that they should necessarily be treated as\ncontinuous features. It is not always clear whether an integer fea‐\nture should be treated as continuous or discrete (and one-hot-\nencoded). If there is no ordering between the semantics that are\nencoded (like in the workclass example), the feature must be\ntreated as discrete. For other cases, like five-star ratings, the better\nencoding depends on the particular task and data and which\nmachine learning algorithm is used.\nThe get_dummies function in pandas treats all numbers as continuous and will not\ncreate dummy variables for them. To get around this, you can either use scikit-\nlearn’s OneHotEncoder, for which you can specify which variables are continuous\nand which are discrete, or convert numeric columns in the DataFrame to strings. To\nillustrate, let’s create a DataFrame object with two columns, one containing strings\nand one containing integers:\nIn[8]:\n# create a DataFrame with an integer feature and a categorical string feature\ndemo_df = pd.DataFrame({'Integer Feature': [0, 1, 2, 1],\n                        'Categorical Feature': ['socks', 'fox', 'socks', 'box']})\ndisplay(demo_df)\nTable 4-4 shows the result.\n218 \n| \nChapter 4: Representing Data and Engineering Features\nTable 4-4. DataFrame containing categorical string features and integer features\nCategorical Feature Integer Feature\n0\nsocks\n0\n1\nfox\n1\n2\nsocks\n2\n3\nbox\n1",
    "display(demo_df)\nTable 4-4 shows the result.\n218 \n| \nChapter 4: Representing Data and Engineering Features\nTable 4-4. DataFrame containing categorical string features and integer features\nCategorical Feature Integer Feature\n0\nsocks\n0\n1\nfox\n1\n2\nsocks\n2\n3\nbox\n1\nUsing get_dummies will only encode the string feature and will not change the integer\nfeature, as you can see in Table 4-5:\nIn[9]:\npd.get_dummies(demo_df)\nTable 4-5. One-hot-encoded version of the data from Table 4-4, leaving the integer feature\nunchanged\nInteger Feature\nCategorical Feature_box\nCategorical Feature_fox\nCategorical Feature_socks\n0\n0\n0.0\n0.0\n1.0\n1\n1\n0.0\n1.0\n0.0\n2\n2\n0.0\n0.0\n1.0\n3\n1\n1.0\n0.0\n0.0\nIf you want dummy variables to be created for the “Integer Feature” column, you can\nexplicitly list the columns you want to encode using the columns parameter. Then,\nboth features will be treated as categorical (see Table 4-6):\nIn[10]:\ndemo_df['Integer Feature'] = demo_df['Integer Feature'].astype(str)\npd.get_dummies(demo_df, columns=['Integer Feature', 'Categorical Feature'])\nTable 4-6. One-hot encoding of the data shown in Table 4-4, encoding the integer and string\nfeatures\nInteger\nFeature_0\nInteger\nFeature_1\nInteger\nFeature_2\nCategorical\nFeature_box\nCategorical\nFeature_fox\nCategorical\nFeature_socks\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n2\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n3\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\nCategorical Variables \n| \n219\nBinning, Discretization, Linear Models, and Trees\nThe best way to represent data depends not only on the semantics of the data, but also\non the kind of model you are using. Linear models and tree-based models (such as\ndecision trees, gradient boosted trees, and random forests), two large and very com‐\nmonly used families, have very different properties when it comes to how they work\nwith different feature representations. Let’s go back to the wave regression dataset that\nwe used in Chapter 2. It has only a single input feature. Here is a comparison of a",
    "monly used families, have very different properties when it comes to how they work\nwith different feature representations. Let’s go back to the wave regression dataset that\nwe used in Chapter 2. It has only a single input feature. Here is a comparison of a\nlinear regression model and a decision tree regressor on this dataset (see Figure 4-1):\nIn[11]:\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nX, y = mglearn.datasets.make_wave(n_samples=100)\nline = np.linspace(-3, 3, 1000, endpoint=False).reshape(-1, 1)\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X, y)\nplt.plot(line, reg.predict(line), label=\"decision tree\")\nreg = LinearRegression().fit(X, y)\nplt.plot(line, reg.predict(line), label=\"linear regression\")\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nAs you know, linear models can only model linear relationships, which are lines in\nthe case of a single feature. The decision tree can build a much more complex model\nof the data. However, this is strongly dependent on the representation of the data.\nOne way to make linear models more powerful on continuous data is to use binning\n(also known as discretization) of the feature to split it up into multiple features, as\ndescribed here.\n220 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-1. Comparing linear regression and a decision tree on the wave dataset\nWe imagine a partition of the input range for the feature (in this case, the numbers\nfrom –3 to 3) into a fixed number of bins—say, 10. A data point will then be repre‐\nsented by which bin it falls into. To determine this, we first have to define the bins. In\nthis case, we’ll define 10 bins equally spaced between –3 and 3. We use the\nnp.linspace function for this, creating 11 entries, which will create 10 bins—they are\nthe spaces in between two consecutive boundaries:\nIn[12]:\nbins = np.linspace(-3, 3, 11)",
    "this case, we’ll define 10 bins equally spaced between –3 and 3. We use the\nnp.linspace function for this, creating 11 entries, which will create 10 bins—they are\nthe spaces in between two consecutive boundaries:\nIn[12]:\nbins = np.linspace(-3, 3, 11)\nprint(\"bins: {}\".format(bins))\nOut[12]:\nbins: [-3.  -2.4 -1.8 -1.2 -0.6  0.   0.6  1.2  1.8  2.4  3. ]\nHere, the first bin contains all data points with feature values –3 to –2.68, the second\nbin contains all points with feature values from –2.68 to –2.37, and so on.\nNext, we record for each data point which bin it falls into. This can be easily compu‐\nted using the np.digitize function:\nBinning, Discretization, Linear Models, and Trees \n| \n221\nIn[13]:\nwhich_bin = np.digitize(X, bins=bins)\nprint(\"\\nData points:\\n\", X[:5])\nprint(\"\\nBin membership for data points:\\n\", which_bin[:5])\nOut[13]:\nData points:\n [[-0.753]\n  [ 2.704]\n  [ 1.392]\n  [ 0.592]\n [-2.064]]\nBin membership for data points:\n [[ 4]\n  [10]\n  [ 8]\n  [ 6]\n [ 2]]\nWhat we did here is transform the single continuous input feature in the wave dataset\ninto a categorical feature that encodes which bin a data point is in. To use a scikit-\nlearn model on this data, we transform this discrete feature to a one-hot encoding\nusing the OneHotEncoder from the preprocessing module. The OneHotEncoder does\nthe same encoding as pandas.get_dummies, though it currently only works on cate‐\ngorical variables that are integers:\nIn[14]:\nfrom sklearn.preprocessing import OneHotEncoder\n# transform using the OneHotEncoder\nencoder = OneHotEncoder(sparse=False)\n# encoder.fit finds the unique values that appear in which_bin\nencoder.fit(which_bin)\n# transform creates the one-hot encoding\nX_binned = encoder.transform(which_bin)\nprint(X_binned[:5])\nOut[14]:\n[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]",
    "X_binned = encoder.transform(which_bin)\nprint(X_binned[:5])\nOut[14]:\n[[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]]\nBecause we specified 10 bins, the transformed dataset X_binned now is made up of 10\nfeatures:\n222 \n| \nChapter 4: Representing Data and Engineering Features\nIn[15]:\nprint(\"X_binned.shape: {}\".format(X_binned.shape))\nOut[15]:\nX_binned.shape: (100, 10)\nNow we build a new linear regression model and a new decision tree model on the\none-hot-encoded data. The result is visualized in Figure 4-2, together with the bin\nboundaries, shown as dotted black lines:\nIn[16]:\nline_binned = encoder.transform(np.digitize(line, bins=bins))\nreg = LinearRegression().fit(X_binned, y)\nplt.plot(line, reg.predict(line_binned), label='linear regression binned')\nreg = DecisionTreeRegressor(min_samples_split=3).fit(X_binned, y)\nplt.plot(line, reg.predict(line_binned), label='decision tree binned')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.vlines(bins, -3, 3, linewidth=1, alpha=.2)\nplt.legend(loc=\"best\")\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nFigure 4-2. Comparing linear regression and decision tree regression on binned features\nBinning, Discretization, Linear Models, and Trees \n| \n223\nThe dashed line and solid line are exactly on top of each other, meaning the linear\nregression model and the decision tree make exactly the same predictions. For each\nbin, they predict a constant value. As features are constant within each bin, any\nmodel must predict the same value for all points within a bin. Comparing what the\nmodels learned before binning the features and after, we see that the linear model\nbecame much more flexible, because it now has a different value for each bin, while\nthe decision tree model got much less flexible. Binning features generally has no ben‐",
    "models learned before binning the features and after, we see that the linear model\nbecame much more flexible, because it now has a different value for each bin, while\nthe decision tree model got much less flexible. Binning features generally has no ben‐\neficial effect for tree-based models, as these models can learn to split up the data any‐\nwhere. In a sense, that means decision trees can learn whatever binning is most useful\nfor predicting on this data. Additionally, decision trees look at multiple features at\nonce, while binning is usually done on a per-feature basis. However, the linear model\nbenefited greatly in expressiveness from the transformation of the data.\nIf there are good reasons to use a linear model for a particular dataset—say, because it\nis very large and high-dimensional, but some features have nonlinear relations with\nthe output—binning can be a great way to increase modeling power.\nInteractions and Polynomials\nAnother way to enrich a feature representation, particularly for linear models, is\nadding interaction features and polynomial features of the original data. This kind of\nfeature engineering is often used in statistical modeling, but it’s also common in many\npractical machine learning applications.\nAs a first example, look again at Figure 4-2. The linear model learned a constant value\nfor each bin in the wave dataset. We know, however, that linear models can learn not\nonly offsets, but also slopes. One way to add a slope to the linear model on the binned\ndata is to add the original feature (the x-axis in the plot) back in. This leads to an 11-\ndimensional dataset, as seen in Figure 4-3:\nIn[17]:\nX_combined = np.hstack([X, X_binned])\nprint(X_combined.shape)\nOut[17]:\n(100, 11)\nIn[18]:\nreg = LinearRegression().fit(X_combined, y)\nline_combined = np.hstack([line, line_binned])\nplt.plot(line, reg.predict(line_combined), label='linear regression combined')\nfor bin in bins:\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\n224 \n|",
    "print(X_combined.shape)\nOut[17]:\n(100, 11)\nIn[18]:\nreg = LinearRegression().fit(X_combined, y)\nline_combined = np.hstack([line, line_binned])\nplt.plot(line, reg.predict(line_combined), label='linear regression combined')\nfor bin in bins:\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\n224 \n| \nChapter 4: Representing Data and Engineering Features\nplt.legend(loc=\"best\")\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.plot(X[:, 0], y, 'o', c='k')\nFigure 4-3. Linear regression using binned features and a single global slope\nIn this example, the model learned an offset for each bin, together with a slope. The\nlearned slope is downward, and shared across all the bins—there is a single x-axis fea‐\nture, which has a single slope. Because the slope is shared across all bins, it doesn’t\nseem to be very helpful. We would rather have a separate slope for each bin! We can\nachieve this by adding an interaction or product feature that indicates which bin a\ndata point is in and where it lies on the x-axis. This feature is a product of the bin\nindicator and the original feature. Let’s create this dataset:\nIn[19]:\nX_product = np.hstack([X_binned, X * X_binned])\nprint(X_product.shape)\nOut[19]:\n(100, 20)\nThe dataset now has 20 features: the indicators for which bin a data point is in, and a\nproduct of the original feature and the bin indicator. You can think of the product\nInteractions and Polynomials \n| \n225\nfeature as a separate copy of the x-axis feature for each bin. It is the original feature\nwithin the bin, and zero everywhere else. Figure 4-4 shows the result of the linear\nmodel on this new representation:\nIn[20]:\nreg = LinearRegression().fit(X_product, y)\nline_product = np.hstack([line_binned, line * line_binned])\nplt.plot(line, reg.predict(line_product), label='linear regression product')\nfor bin in bins:\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")",
    "line_product = np.hstack([line_binned, line * line_binned])\nplt.plot(line, reg.predict(line_product), label='linear regression product')\nfor bin in bins:\n    plt.plot([bin, bin], [-3, 3], ':', c='k')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-4. Linear regression with a separate slope per bin\nAs you can see, now each bin has its own offset and slope in this model.\n226 \n| \nChapter 4: Representing Data and Engineering Features\nUsing binning is one way to expand a continuous feature. Another one is to use poly‐\nnomials of the original features. For a given feature x, we might want to consider\nx ** 2, x ** 3, x ** 4, and so on. This is implemented in PolynomialFeatures in\nthe preprocessing module:\nIn[21]:\nfrom sklearn.preprocessing import PolynomialFeatures\n# include polynomials up to x ** 10:\n# the default \"include_bias=True\" adds a feature that's constantly 1\npoly = PolynomialFeatures(degree=10, include_bias=False)\npoly.fit(X)\nX_poly = poly.transform(X)\nUsing a degree of 10 yields 10 features:\nIn[22]:\nprint(\"X_poly.shape: {}\".format(X_poly.shape))\nOut[22]:\nX_poly.shape: (100, 10)\nLet’s compare the entries of X_poly to those of X:\nIn[23]:\nprint(\"Entries of X:\\n{}\".format(X[:5]))\nprint(\"Entries of X_poly:\\n{}\".format(X_poly[:5]))\nOut[23]:\nEntries of X:\n[[-0.753]\n [ 2.704]\n [ 1.392]\n [ 0.592]\n [-2.064]]\nEntries of X_poly:\n[[    -0.753      0.567     -0.427      0.321     -0.242      0.182\n      -0.137      0.103     -0.078      0.058]\n [     2.704      7.313     19.777     53.482    144.632    391.125\n    1057.714   2860.360   7735.232  20918.278]\n [     1.392      1.938      2.697      3.754      5.226      7.274\n      10.125     14.094     19.618     27.307]\n [     0.592      0.350      0.207      0.123      0.073      0.043\n       0.025      0.015      0.009      0.005]\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n    -159.516    329.222   -679.478   1402.367]]",
    "10.125     14.094     19.618     27.307]\n [     0.592      0.350      0.207      0.123      0.073      0.043\n       0.025      0.015      0.009      0.005]\n [    -2.064      4.260     -8.791     18.144    -37.448     77.289\n    -159.516    329.222   -679.478   1402.367]]\nYou can obtain the semantics of the features by calling the get_feature_names\nmethod, which provides the exponent for each feature:\nInteractions and Polynomials \n| \n227\nIn[24]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[24]:\nPolynomial feature names:\n['x0', 'x0^2', 'x0^3', 'x0^4', 'x0^5', 'x0^6', 'x0^7', 'x0^8', 'x0^9', 'x0^10']\nYou can see that the first column of X_poly corresponds exactly to X, while the other\ncolumns are the powers of the first entry. It’s interesting to see how large some of the\nvalues can get. The second column has entries above 20,000, orders of magnitude dif‐\nferent from the rest.\nUsing polynomial features together with a linear regression model yields the classical\nmodel of polynomial regression (see Figure 4-5):\nIn[26]:\nreg = LinearRegression().fit(X_poly, y)\nline_poly = poly.transform(line)\nplt.plot(line, reg.predict(line_poly), label='polynomial linear regression')\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-5. Linear regression with tenth-degree polynomial features\n228 \n| \nChapter 4: Representing Data and Engineering Features\nAs you can see, polynomial features yield a very smooth fit on this one-dimensional\ndata. However, polynomials of high degree tend to behave in extreme ways on the\nboundaries or in regions with little data.\nAs a comparison, here is a kernel SVM model learned on the original data, without\nany transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')",
    "any transformation (see Figure 4-6):\nIn[26]:\nfrom sklearn.svm import SVR\nfor gamma in [1, 10]:\n    svr = SVR(gamma=gamma).fit(X, y)\n    plt.plot(line, svr.predict(line), label='SVR gamma={}'.format(gamma))\nplt.plot(X[:, 0], y, 'o', c='k')\nplt.ylabel(\"Regression output\")\nplt.xlabel(\"Input feature\")\nplt.legend(loc=\"best\")\nFigure 4-6. Comparison of different gamma parameters for an SVM with RBF kernel\nUsing a more complex model, a kernel SVM, we are able to learn a similarly complex\nprediction to the polynomial regression without an explicit transformation of the\nfeatures.\nInteractions and Polynomials \n| \n229\nAs a more realistic application of interactions and polynomials, let’s look again at the\nBoston Housing dataset. We already used polynomial features on this dataset in\nChapter 2. Now let’s have a look at how these features were constructed, and at how\nmuch the polynomial features help. First we load the data, and rescale it to be\nbetween 0 and 1 using MinMaxScaler:\nIn[27]:\nfrom sklearn.datasets import load_boston\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split\n    (boston.data, boston.target, random_state=0)\n# rescale data\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\nNow, we extract polynomial features and interactions up to a degree of 2:\nIn[28]:\npoly = PolynomialFeatures(degree=2).fit(X_train_scaled)\nX_train_poly = poly.transform(X_train_scaled)\nX_test_poly = poly.transform(X_test_scaled)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different",
    "print(\"X_train_poly.shape: {}\".format(X_train_poly.shape))\nOut[28]:\nX_train.shape: (379, 13)\nX_train_poly.shape: (379, 105)\nThe data originally had 13 features, which were expanded into 105 interaction fea‐\ntures. These new features represent all possible interactions between two different\noriginal features, as well as the square of each original feature. degree=2 here means\nthat we look at all features that are the product of up to two original features. The\nexact correspondence between input and output features can be found using the\nget_feature_names method:\nIn[29]:\nprint(\"Polynomial feature names:\\n{}\".format(poly.get_feature_names()))\nOut[29]:\nPolynomial feature names:\n['1', 'x0', 'x1', 'x2', 'x3', 'x4', 'x5', 'x6', 'x7', 'x8', 'x9', 'x10',\n'x11', 'x12', 'x0^2', 'x0 x1', 'x0 x2', 'x0 x3', 'x0 x4', 'x0 x5', 'x0 x6',\n'x0 x7', 'x0 x8', 'x0 x9', 'x0 x10', 'x0 x11', 'x0 x12', 'x1^2', 'x1 x2',\n230 \n| \nChapter 4: Representing Data and Engineering Features\n'x1 x3', 'x1 x4', 'x1 x5', 'x1 x6', 'x1 x7', 'x1 x8', 'x1 x9', 'x1 x10',\n'x1 x11', 'x1 x12', 'x2^2', 'x2 x3', 'x2 x4', 'x2 x5', 'x2 x6', 'x2 x7',\n'x2 x8', 'x2 x9', 'x2 x10', 'x2 x11', 'x2 x12', 'x3^2', 'x3 x4', 'x3 x5',\n'x3 x6', 'x3 x7', 'x3 x8', 'x3 x9', 'x3 x10', 'x3 x11', 'x3 x12', 'x4^2',\n'x4 x5', 'x4 x6', 'x4 x7', 'x4 x8', 'x4 x9', 'x4 x10', 'x4 x11', 'x4 x12',\n'x5^2', 'x5 x6', 'x5 x7', 'x5 x8', 'x5 x9', 'x5 x10', 'x5 x11', 'x5 x12',\n'x6^2', 'x6 x7', 'x6 x8', 'x6 x9', 'x6 x10', 'x6 x11', 'x6 x12', 'x7^2',\n'x7 x8', 'x7 x9', 'x7 x10', 'x7 x11', 'x7 x12', 'x8^2', 'x8 x9', 'x8 x10',\n'x8 x11', 'x8 x12', 'x9^2', 'x9 x10', 'x9 x11', 'x9 x12', 'x10^2', 'x10 x11',\n'x10 x12', 'x11^2', 'x11 x12', 'x12^2']\nThe first new feature is a constant feature, called \"1\" here. The next 13 features are\nthe original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n(\"x0^2\") and combinations of the first and the other features.\nLet’s compare the performance using Ridge on the data with and without interac‐\ntions:",
    "the original features (called \"x0\" to \"x12\"). Then follows the first feature squared\n(\"x0^2\") and combinations of the first and the other features.\nLet’s compare the performance using Ridge on the data with and without interac‐\ntions:\nIn[30]:\nfrom sklearn.linear_model import Ridge\nridge = Ridge().fit(X_train_scaled, y_train)\nprint(\"Score without interactions: {:.3f}\".format(\n    ridge.score(X_test_scaled, y_test)))\nridge = Ridge().fit(X_train_poly, y_train)\nprint(\"Score with interactions: {:.3f}\".format(\n    ridge.score(X_test_poly, y_test)))\nOut[30]:\nScore without interactions: 0.621\nScore with interactions: 0.753\nClearly, the interactions and polynomial features gave us a good boost in perfor‐\nmance when using Ridge. When using a more complex model like a random forest,\nthe story is a bit different, though:\nIn[31]:\nfrom sklearn.ensemble import RandomForestRegressor\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_scaled, y_train)\nprint(\"Score without interactions: {:.3f}\".format(\n    rf.score(X_test_scaled, y_test)))\nrf = RandomForestRegressor(n_estimators=100).fit(X_train_poly, y_train)\nprint(\"Score with interactions: {:.3f}\".format(rf.score(X_test_poly, y_test)))\nOut[31]:\nScore without interactions: 0.799\nScore with interactions: 0.763\nInteractions and Polynomials \n| \n231\nYou can see that even without additional features, the random forest beats the\nperformance of Ridge. Adding interactions and polynomials actually decreases per‐\nformance slightly.\nUnivariate Nonlinear Transformations\nWe just saw that adding squared or cubed features can help linear models for regres‐\nsion. There are other transformations that often prove useful for transforming certain\nfeatures: in particular, applying mathematical functions like log, exp, or sin. While\ntree-based models only care about the ordering of the features, linear models and\nneural networks are very tied to the scale and distribution of each feature, and if there",
    "features: in particular, applying mathematical functions like log, exp, or sin. While\ntree-based models only care about the ordering of the features, linear models and\nneural networks are very tied to the scale and distribution of each feature, and if there\nis a nonlinear relation between the feature and the target, that becomes hard to model\n—particularly in regression. The functions log and exp can help by adjusting the rel‐\native scales in the data so that they can be captured better by a linear model or neural\nnetwork. We saw an application of that in Chapter 2 with the memory price data. The\nsin and cos functions can come in handy when dealing with data that encodes peri‐\nodic patterns.\nMost models work best when each feature (and in regression also the target) is loosely\nGaussian distributed—that is, a histogram of each feature should have something\nresembling the familiar “bell curve” shape. Using transformations like log and exp is\na hacky but simple and efficient way to achieve this. A particularly common case\nwhen such a transformation can be helpful is when dealing with integer count data.\nBy count data, we mean features like “how often did user A log in?” Counts are never\nnegative, and often follow particular statistical patterns. We are using a synthetic\ndataset of counts here that has properties similar to those you can find in the wild.\nThe features are all integer-valued, while the response is continuous:\nIn[32]:\nrnd = np.random.RandomState(0)\nX_org = rnd.normal(size=(1000, 3))\nw = rnd.normal(size=3)\nX = rnd.poisson(10 * np.exp(X_org))\ny = np.dot(X_org, w)\nLet’s look at the first 10 entries of the first feature. All are integer values and positive,\nbut apart from that it’s hard to make out a particular pattern.\nIf we count the appearance of each value, the distribution of values becomes clearer:\n232 \n| \nChapter 4: Representing Data and Engineering Features\nIn[33]:\nprint(\"Number of feature appearances:\\n{}\".format(np.bincount(X[:, 0])))\nOut[33]:",
    "If we count the appearance of each value, the distribution of values becomes clearer:\n232 \n| \nChapter 4: Representing Data and Engineering Features\nIn[33]:\nprint(\"Number of feature appearances:\\n{}\".format(np.bincount(X[:, 0])))\nOut[33]:\nNumber of feature appearances:\n[28 38 68 48 61 59 45 56 37 40 35 34 36 26 23 26 27 21 23 23 18 21 10  9 17\n  9  7 14 12  7  3  8  4  5  5  3  4  2  4  1  1  3  2  5  3  8  2  5  2  1\n  2  3  3  2  2  3  3  0  1  2  1  0  0  3  1  0  0  0  1  3  0  1  0  2  0\n  1  1  0  0  0  0  1  0  0  2  2  0  1  1  0  0  0  0  1  1  0  0  0  0  0\n  0  0  1  0  0  0  0  0  1  1  0  0  1  0  0  0  0  0  0  0  1  0  0  0  0\n  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  1]\nThe value 2 seems to be the most common, with 62 appearances (bincount always\nstarts at 0), and the counts for higher values fall quickly. However, there are some\nvery high values, like 134 appearing twice. We visualize the counts in Figure 4-7:\nIn[34]:\nbins = np.bincount(X[:, 0])\nplt.bar(range(len(bins)), bins, color='w')\nplt.ylabel(\"Number of appearances\")\nplt.xlabel(\"Value\")\nFigure 4-7. Histogram of feature values for X[0]\nUnivariate Nonlinear Transformations \n| \n233\n1 This is a Poisson distribution, which is quite fundamental to count data.\nFeatures X[:, 1] and X[:, 2] have similar properties. This kind of distribution of\nvalues (many small ones and a few very large ones) is very common in practice.1\nHowever, it is something most linear models can’t handle very well. Let’s try to fit a\nridge regression to this model:\nIn[35]:\nfrom sklearn.linear_model import Ridge\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nscore = Ridge().fit(X_train, y_train).score(X_test, y_test)\nprint(\"Test score: {:.3f}\".format(score))\nOut[35]:\nTest score: 0.622\nAs you can see from the relatively low R2 score, Ridge was not able to really capture\nthe relationship between X and y. Applying a logarithmic transformation can help,",
    "score = Ridge().fit(X_train, y_train).score(X_test, y_test)\nprint(\"Test score: {:.3f}\".format(score))\nOut[35]:\nTest score: 0.622\nAs you can see from the relatively low R2 score, Ridge was not able to really capture\nthe relationship between X and y. Applying a logarithmic transformation can help,\nthough. Because the value 0 appears in the data (and the logarithm is not defined at\n0), we can’t actually just apply log, but we have to compute log(X + 1):\nIn[36]:\nX_train_log = np.log(X_train + 1)\nX_test_log = np.log(X_test + 1)\nAfter the transformation, the distribution of the data is less asymmetrical and doesn’t\nhave very large outliers anymore (see Figure 4-8):\nIn[37]:\nplt.hist(np.log(X_train_log[:, 0] + 1), bins=25, color='gray')\nplt.ylabel(\"Number of appearances\")\nplt.xlabel(\"Value\")\n234 \n| \nChapter 4: Representing Data and Engineering Features\n2 This is a very crude approximation of using Poisson regression, which would be the proper solution from a\nprobabilistic standpoint.\nFigure 4-8. Histogram of feature values for X[0] after logarithmic transformation\nBuilding a ridge model on the new data provides a much better fit:\nIn[38]:\nscore = Ridge().fit(X_train_log, y_train).score(X_test_log, y_test)\nprint(\"Test score: {:.3f}\".format(score))\nOut[38]:\nTest score: 0.875\nFinding the transformation that works best for each combination of dataset and\nmodel is somewhat of an art. In this example, all the features had the same properties.\nThis is rarely the case in practice, and usually only a subset of the features should be\ntransformed, or sometimes each feature needs to be transformed in a different way.\nAs we mentioned earlier, these kinds of transformations are irrelevant for tree-based\nmodels but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2",
    "models but might be essential for linear models. Sometimes it is also a good idea to\ntransform the target variable y in regression. Trying to predict counts (say, number of\norders) is a fairly common task, and using the log(y + 1) transformation often\nhelps.2\nUnivariate Nonlinear Transformations \n| \n235\nAs you saw in the previous examples, binning, polynomials, and interactions can\nhave a huge influence on how models perform on a given dataset. This is particularly\ntrue for less complex models like linear models and naive Bayes models. Tree-based\nmodels, on the other hand, are often able to discover important interactions them‐\nselves, and don’t require transforming the data explicitly most of the time. Other\nmodels, like SVMs, nearest neighbors, and neural networks, might sometimes benefit\nfrom using binning, interactions, or polynomials, but the implications there are usu‐\nally much less clear than in the case of linear models.\nAutomatic Feature Selection\nWith so many ways to create new features, you might get tempted to increase the\ndimensionality of the data way beyond the number of original features. However,\nadding more features makes all models more complex, and so increases the chance of\noverfitting. When adding new features, or with high-dimensional datasets in general,\nit can be a good idea to reduce the number of features to only the most useful ones,\nand discard the rest. This can lead to simpler models that generalize better. But how\ncan you know how good each feature is? There are three basic strategies: univariate\nstatistics, model-based selection, and iterative selection. We will discuss all three of\nthem in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics",
    "them in detail. All of these methods are supervised methods, meaning they need the\ntarget for fitting the model. This means we need to split the data into training and test\nsets, and fit the feature selection only on the training part of the data.\nUnivariate Statistics\nIn univariate statistics, we compute whether there is a statistically significant relation‐\nship between each feature and the target. Then the features that are related with the\nhighest confidence are selected. In the case of classification, this is also known as\nanalysis of variance (ANOVA). A key property of these tests is that they are univari‐\nate, meaning that they only consider each feature individually. Consequently, a fea‐\nture will be discarded if it is only informative when combined with another feature.\nUnivariate tests are often very fast to compute, and don’t require building a model.\nOn the other hand, they are completely independent of the model that you might\nwant to apply after the feature selection.\nTo use univariate feature selection in scikit-learn, you need to choose a test, usu‐\nally either f_classif (the default) for classification or f_regression for regression,\nand a method to discard features based on the p-values determined in the test. All\nmethods for discarding parameters use a threshold to discard all features with too\nhigh a p-value (which means they are unlikely to be related to the target). The meth‐\nods differ in how they compute this threshold, with the simplest ones being SelectKB\nest, which selects a fixed number k of features, and SelectPercentile, which selects\na fixed percentage of features. Let’s apply the feature selection for classification on the\n236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:",
    "236 \n| \nChapter 4: Representing Data and Engineering Features\ncancer dataset. To make the task a bit harder, we’ll add some noninformative noise\nfeatures to the data. We expect the feature selection to be able to identify the features\nthat are noninformative and remove them:\nIn[39]:\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.feature_selection import SelectPercentile\nfrom sklearn.model_selection import train_test_split\ncancer = load_breast_cancer()\n# get deterministic random numbers\nrng = np.random.RandomState(42)\nnoise = rng.normal(size=(len(cancer.data), 50))\n# add noise features to the data\n# the first 30 features are from the dataset, the next 50 are noise\nX_w_noise = np.hstack([cancer.data, noise])\nX_train, X_test, y_train, y_test = train_test_split(\n    X_w_noise, cancer.target, random_state=0, test_size=.5)\n# use f_classif (the default) and SelectPercentile to select 50% of features\nselect = SelectPercentile(percentile=50)\nselect.fit(X_train, y_train)\n# transform training set\nX_train_selected = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_selected.shape: {}\".format(X_train_selected.shape))\nOut[39]:\nX_train.shape: (284, 80)\nX_train_selected.shape: (284, 40)\nAs you can see, the number of features was reduced from 80 to 40 (50 percent of the\noriginal number of features). We can find out which features have been selected using\nthe get_support method, which returns a Boolean mask of the selected features\n(visualized in Figure 4-9):\nIn[40]:\nmask = select.get_support()\nprint(mask)\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237",
    "plt.xlabel(\"Sample index\")\nOut[40]:\n[ True  True  True  True  True  True  True  True  True False  True False\n  True  True  True  True  True  True False False  True  True  True  True\n  True  True  True  True  True  True False False False  True False  True\nAutomatic Feature Selection \n| \n237\n False False  True False False False False  True False False  True False\n False  True False  True False False False False False False  True False\n  True False False False False  True False  True False False False False\n  True  True False  True False False False False]\nFigure 4-9. Features selected by SelectPercentile\nAs you can see from the visualization of the mask, most of the selected features are\nthe original features, and most of the noise features were removed. However, the\nrecovery of the original features is not perfect. Let’s compare the performance of\nlogistic regression on all features against the performance using only the selected\nfeatures:\nIn[41]:\nfrom sklearn.linear_model import LogisticRegression\n# transform test data\nX_test_selected = select.transform(X_test)\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nprint(\"Score with all features: {:.3f}\".format(lr.score(X_test, y_test)))\nlr.fit(X_train_selected, y_train)\nprint(\"Score with only selected features: {:.3f}\".format(\n    lr.score(X_test_selected, y_test)))\nOut[41]:\nScore with all features: 0.930\nScore with only selected features: 0.940\nIn this case, removing the noise features improved performance, even though some\nof the original features were lost. This was a very simple synthetic example, and out‐\ncomes on real data are usually mixed. Univariate feature selection can still be very\nhelpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the",
    "helpful, though, if there is such a large number of features that building a model on\nthem is infeasible, or if you suspect that many features are completely uninformative.\nModel-Based Feature Selection\nModel-based feature selection uses a supervised machine learning model to judge the\nimportance of each feature, and keeps only the most important ones. The supervised\nmodel that is used for feature selection doesn’t need to be the same model that is used\nfor the final supervised modeling. The feature selection model needs to provide some\nmeasure of importance for each feature, so that they can be ranked by this measure.\nDecision trees and decision tree–based models provide a feature_importances_\n238 \n| \nChapter 4: Representing Data and Engineering Features\nattribute, which directly encodes the importance of each feature. Linear models have\ncoefficients, which can also be used to capture feature importances by considering the\nabsolute values. As we saw in Chapter 2, linear models with L1 penalty learn sparse\ncoefficients, which only use a small subset of features. This can be viewed as a form of\nfeature selection for the model itself, but can also be used as a preprocessing step to\nselect features for another model. In contrast to univariate selection, model-based\nselection considers all features at once, and so can capture interactions (if the model\ncan capture them). To use model-based feature selection, we need to use the\nSelectFromModel transformer:\nIn[42]:\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.ensemble import RandomForestClassifier\nselect = SelectFromModel(\n    RandomForestClassifier(n_estimators=100, random_state=42),\n    threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we",
    "threshold=\"median\")\nThe SelectFromModel class selects all features that have an importance measure of\nthe feature (as provided by the supervised model) greater than the provided thresh‐\nold. To get a comparable result to what we got with univariate feature selection, we\nused the median as a threshold, so that half of the features will be selected. We use a\nrandom forest classifier with 100 trees to compute the feature importances. This is a\nquite complex model and much more powerful than using univariate tests. Now let’s\nactually fit the model:\nIn[43]:\nselect.fit(X_train, y_train)\nX_train_l1 = select.transform(X_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\nprint(\"X_train_l1.shape: {}\".format(X_train_l1.shape))\nOut[43]:\nX_train.shape: (284, 80)\nX_train_l1.shape: (284, 40)\nAgain, we can have a look at the features that were selected (Figure 4-10):\nIn[44]:\nmask = select.get_support()\n# visualize the mask -- black is True, white is False\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\nFigure 4-10. Features selected by SelectFromModel using the RandomForestClassifier\nAutomatic Feature Selection \n| \n239\nThis time, all but two of the original features were selected. Because we specified to\nselect 40 features, some of the noise features are also selected. Let’s take a look at the\nperformance:\nIn[45]:\nX_test_l1 = select.transform(X_test)\nscore = LogisticRegression().fit(X_train_l1, y_train).score(X_test_l1, y_test)\nprint(\"Test score: {:.3f}\".format(score))\nOut[45]:\nTest score: 0.951\nWith the better feature selection, we also gained some improvements here.\nIterative Feature Selection\nIn univariate testing we used no model, while in model-based selection we used a sin‐\ngle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or",
    "gle model to select features. In iterative feature selection, a series of models are built,\nwith varying numbers of features. There are two basic methods: starting with no fea‐\ntures and adding features one by one until some stopping criterion is reached, or\nstarting with all features and removing features one by one until some stopping crite‐\nrion is reached. Because a series of models are built, these methods are much more\ncomputationally expensive than the methods we discussed previously. One particular\nmethod of this kind is recursive feature elimination (RFE), which starts with all fea‐\ntures, builds a model, and discards the least important feature according to the\nmodel. Then a new model is built using all but the discarded feature, and so on until\nonly a prespecified number of features are left. For this to work, the model used for\nselection needs to provide some way to determine feature importance, as was the case\nfor the model-based selection. Here, we use the same random forest model that we\nused earlier, and get the results shown in Figure 4-11:\nIn[46]:\nfrom sklearn.feature_selection import RFE\nselect = RFE(RandomForestClassifier(n_estimators=100, random_state=42),\n             n_features_to_select=40)\nselect.fit(X_train, y_train)\n# visualize the selected features:\nmask = select.get_support()\nplt.matshow(mask.reshape(1, -1), cmap='gray_r')\nplt.xlabel(\"Sample index\")\n240 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-11. Features selected by recursive feature elimination with the random forest\nclassifier model\nThe feature selection got better compared to the univariate and model-based selec‐\ntion, but one feature was still missed. Running this code also takes significantly longer\nthan that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:",
    "than that for the model-based selection, because a random forest model is trained 40\ntimes, once for each feature that is dropped. Let’s test the accuracy of the logistic\nregression model when using RFE for feature selection:\nIn[47]:\nX_train_rfe= select.transform(X_train)\nX_test_rfe= select.transform(X_test)\nscore = LogisticRegression().fit(X_train_rfe, y_train).score(X_test_rfe, y_test)\nprint(\"Test score: {:.3f}\".format(score))\nOut[47]:\nTest score: 0.951\nWe can also use the model used inside the RFE to make predictions. This uses only\nthe feature set that was selected:\nIn[48]:\nprint(\"Test score: {:.3f}\".format(select.score(X_test, y_test)))\nOut[48]:\nTest score: 0.951\nHere, the performance of the random forest used inside the RFE is the same as that\nachieved by training a logistic regression model on top of the selected features. In\nother words, once we’ve selected the right features, the linear model performs as well\nas the random forest.\nIf you are unsure when selecting what to use as input to your machine learning algo‐\nrithms, automatic feature selection can be quite helpful. It is also great for reducing\nthe amount of features needed—for example, to speed up prediction or to allow for\nmore interpretable models. In most real-world cases, applying feature selection is\nunlikely to provide large gains in performance. However, it is still a valuable tool in\nthe toolbox of the feature engineer.\nAutomatic Feature Selection \n| \n241\nUtilizing Expert Knowledge\nFeature engineering is often an important place to use expert knowledge for a particu‐\nlar application. While the purpose of machine learning in many cases is to avoid hav‐\ning to create a set of expert-designed rules, that doesn’t mean that prior knowledge of\nthe application or domain should be discarded. Often, domain experts can help in\nidentifying useful features that are much more informative than the initial represen‐\ntation of the data. Imagine you work for a travel agency and want to predict flight",
    "the application or domain should be discarded. Often, domain experts can help in\nidentifying useful features that are much more informative than the initial represen‐\ntation of the data. Imagine you work for a travel agency and want to predict flight\nprices. Let’s say you have a record of prices together with dates, airlines, start loca‐\ntions, and destinations. A machine learning model might be able to build a decent\nmodel from that. Some important factors in flight prices, however, cannot be learned.\nFor example, flights are usually more expensive during peak vacation months and\naround holidays. While the dates of some holidays (like Christmas) are fixed, and\ntheir effect can therefore be learned from the date, others might depend on the phases\nof the moon (like Hanukkah and Easter) or be set by authorities (like school holi‐\ndays). These events cannot be learned from the data if each flight is only recorded\nusing the (Gregorian) date. However, it is easy to add a feature that encodes whether a\nflight was on, preceding, or following a public or school holiday. In this way, prior\nknowledge about the nature of the task can be encoded in the features to aid a\nmachine learning algorithm. Adding a feature does not force a machine learning\nalgorithm to use it, and even if the holiday information turns out to be noninforma‐\ntive for flight prices, augmenting the data with this information doesn’t hurt.\nWe’ll now look at one particular case of using expert knowledge—though in this case\nit might be more rightfully called “common sense.” The task is predicting bicycle rent‐\nals in front of Andreas’s house.\nIn New York, Citi Bike operates a network of bicycle rental stations with a subscrip‐\ntion system. The stations are all over the city and provide a convenient way to get\naround. Bike rental data is made public in an anonymized form and has been ana‐\nlyzed in various ways. The task we want to solve is to predict for a given time and day",
    "tion system. The stations are all over the city and provide a convenient way to get\naround. Bike rental data is made public in an anonymized form and has been ana‐\nlyzed in various ways. The task we want to solve is to predict for a given time and day\nhow many people will rent a bike in front of Andreas’s house—so he knows if any\nbikes will be left for him.\nWe first load the data for August 2015 for this particular station as a pandas Data\nFrame. We resample the data into three-hour intervals to obtain the main trends for\neach day:\nIn[49]:\ncitibike = mglearn.datasets.load_citibike()\n242 \n| \nChapter 4: Representing Data and Engineering Features\nIn[50]:\nprint(\"Citi Bike data:\\n{}\".format(citibike.head()))\nOut[50]:\nCiti Bike data:\nstarttime\n2015-08-01 00:00:00     3.0\n2015-08-01 03:00:00     0.0\n2015-08-01 06:00:00     9.0\n2015-08-01 09:00:00    41.0\n2015-08-01 12:00:00    39.0\nFreq: 3H, Name: one, dtype: float64\nThe following example shows a visualization of the rental frequencies for the whole\nmonth (Figure 4-12):\nIn[51]:\nplt.figure(figsize=(10, 3))\nxticks = pd.date_range(start=citibike.index.min(), end=citibike.index.max(),\n                       freq='D')\nplt.xticks(xticks, xticks.strftime(\"%a %m-%d\"), rotation=90, ha=\"left\")\nplt.plot(citibike, linewidth=1)\nplt.xlabel(\"Date\")\nplt.ylabel(\"Rentals\")\nFigure 4-12. Number of bike rentals over time for a selected Citi Bike station\nLooking at the data, we can clearly distinguish day and night for each 24-hour inter‐\nval. The patterns for weekdays and weekends also seem to be quite different. When\nevaluating a prediction task on a time series like this, we usually want to learn from\nthe past and predict for the future. This means when doing a split into a training and a\ntest set, we want to use all the data up to a certain date as the training set and all the\ndata past that date as the test set. This is how we would usually use time series predic‐",
    "the past and predict for the future. This means when doing a split into a training and a\ntest set, we want to use all the data up to a certain date as the training set and all the\ndata past that date as the test set. This is how we would usually use time series predic‐\ntion: given everything that we know about rentals in the past, what do we think will\nUtilizing Expert Knowledge \n| \n243\nhappen tomorrow? We will use the first 184 data points, corresponding to the first 23\ndays, as our training set, and the remaining 64 data points, corresponding to the\nremaining 8 days, as our test set.\nThe only feature that we are using in our prediction task is the date and time when a\nparticular number of rentals occurred. So, the input feature is the date and time—say,\n2015-08-01 00:00:00—and the output is the number of rentals in the following\nthree hours (three in this case, according to our DataFrame).\nA (surprisingly) common way that dates are stored on computers is using POSIX\ntime, which is the number of seconds since January 1970 00:00:00 (aka the beginning\nof Unix time). As a first try, we can use this single integer feature as our data repre‐\nsentation:\nIn[52]:\n# extract the target values (number of rentals)\ny = citibike.values\n# convert the time to POSIX time using \"%s\"\nX = citibike.index.strftime(\"%s\").astype(\"int\").reshape(-1, 1)\nWe first define a function to split the data into training and test sets, build the model,\nand visualize the result:\nIn[54]:\n# use the first 184 data points for training, and the rest for testing\nn_train = 184\n# function to evaluate and plot a regressor on a given feature set\ndef eval_on_features(features, target, regressor):\n    # split the given features into a training and a test set\n    X_train, X_test = features[:n_train], features[n_train:]\n    # also split the target array\n    y_train, y_test = target[:n_train], target[n_train:]\n    regressor.fit(X_train, y_train)",
    "# split the given features into a training and a test set\n    X_train, X_test = features[:n_train], features[n_train:]\n    # also split the target array\n    y_train, y_test = target[:n_train], target[n_train:]\n    regressor.fit(X_train, y_train)\n    print(\"Test-set R^2: {:.2f}\".format(regressor.score(X_test, y_test)))\n    y_pred = regressor.predict(X_test)\n    y_pred_train = regressor.predict(X_train)\n    plt.figure(figsize=(10, 3))\n    plt.xticks(range(0, len(X), 8), xticks.strftime(\"%a %m-%d\"), rotation=90,\n               ha=\"left\")\n    plt.plot(range(n_train), y_train, label=\"train\")\n    plt.plot(range(n_train, len(y_test) + n_train), y_test, '-', label=\"test\")\n    plt.plot(range(n_train), y_pred_train, '--', label=\"prediction train\")\n    plt.plot(range(n_train, len(y_test) + n_train), y_pred, '--',\n             label=\"prediction test\")\n    plt.legend(loc=(1.01, 0))\n    plt.xlabel(\"Date\")\n    plt.ylabel(\"Rentals\")\n244 \n| \nChapter 4: Representing Data and Engineering Features\nWe saw earlier that random forests require very little preprocessing of the data, which\nmakes this seem like a good model to start with. We use the POSIX time feature X and\npass a random forest regressor to our eval_on_features function. Figure 4-13 shows\nthe result:\nIn[55]:\nfrom sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor(n_estimators=100, random_state=0)\nplt.figure()\neval_on_features(X, y, regressor)\nOut[55]:\nTest-set R^2: -0.04\nFigure 4-13. Predictions made by a random forest using only the POSIX time\nThe predictions on the training set are quite good, as is usual for random forests.\nHowever, for the test set, a constant line is predicted. The R2 is –0.03, which means\nthat we learned nothing. What happened?\nThe problem lies in the combination of our feature and the random forest. The value\nof the POSIX time feature for the test set is outside of the range of the feature values",
    "However, for the test set, a constant line is predicted. The R2 is –0.03, which means\nthat we learned nothing. What happened?\nThe problem lies in the combination of our feature and the random forest. The value\nof the POSIX time feature for the test set is outside of the range of the feature values\nin the training set: the points in the test set have timestamps that are later than all the\npoints in the training set. Trees, and therefore random forests, cannot extrapolate to\nfeature ranges outside the training set. The result is that the model simply predicts the\ntarget value of the closest point in the training set—which is the last time it observed\nany data.\nClearly we can do better than this. This is where our “expert knowledge” comes in.\nFrom looking at the rental figures in the training data, two factors seem to be very\nimportant: the time of day and the day of the week. So, let’s add these two features.\nWe can’t really learn anything from the POSIX time, so we drop that feature. First,\nlet’s use only the hour of the day. As Figure 4-14 shows, now the predictions have the\nsame pattern for each day of the week:\nUtilizing Expert Knowledge \n| \n245\nIn[56]:\nX_hour = citibike.index.hour.reshape(-1, 1)\neval_on_features(X_hour, y, regressor)\nOut[56]:\nTest-set R^2: 0.60\nFigure 4-14. Predictions made by a random forest using only the hour of the day\nThe R2 is already much better, but the predictions clearly miss the weekly pattern.\nNow let’s also add the day of the week (see Figure 4-15):\nIn[57]:\nX_hour_week = np.hstack([citibike.index.dayofweek.reshape(-1, 1),\n                         citibike.index.hour.reshape(-1, 1)])\neval_on_features(X_hour_week, y, regressor)\nOut[57]:\nTest-set R^2: 0.84\nFigure 4-15. Predictions with a random forest using day of week and hour of day\nfeatures\n246 \n| \nChapter 4: Representing Data and Engineering Features\nNow we have a model that captures the periodic behavior by considering the day of",
    "Out[57]:\nTest-set R^2: 0.84\nFigure 4-15. Predictions with a random forest using day of week and hour of day\nfeatures\n246 \n| \nChapter 4: Representing Data and Engineering Features\nNow we have a model that captures the periodic behavior by considering the day of\nweek and time of day. It has an R2 of 0.84, and shows pretty good predictive perfor‐\nmance. What this model likely is learning is the mean number of rentals for each\ncombination of weekday and time of day from the first 23 days of August. This\nactually does not require a complex model like a random forest, so let’s try with a\nsimpler model, LinearRegression (see Figure 4-16):\nIn[58]:\nfrom sklearn.linear_model import LinearRegression\neval_on_features(X_hour_week, y, LinearRegression())\nOut[58]:\nTest-set R^2: 0.13\nFigure 4-16. Predictions made by linear regression using day of week and hour of day as\nfeatures\nLinearRegression works much worse, and the periodic pattern looks odd. The rea‐\nson for this is that we encoded day of week and time of day using integers, which are\ninterpreted as categorical variables. Therefore, the linear model can only learn a lin‐\near function of the time of day—and it learned that later in the day, there are more\nrentals. However, the patterns are much more complex than that. We can capture this\nby interpreting the integers as categorical variables, by transforming them using One\nHotEncoder (see Figure 4-17):\nIn[59]:\nenc = OneHotEncoder()\nX_hour_week_onehot = enc.fit_transform(X_hour_week).toarray()\nUtilizing Expert Knowledge \n| \n247\nIn[60]:\neval_on_features(X_hour_week_onehot, y, Ridge())\nOut[60]:\nTest-set R^2: 0.62\nFigure 4-17. Predictions made by linear regression using a one-hot encoding of hour of\nday and day of week\nThis gives us a much better match than the continuous feature encoding. Now the\nlinear model learns one coefficient for each day of the week, and one coefficient for\neach time of the day. That means that the “time of day” pattern is shared over all days",
    "day and day of week\nThis gives us a much better match than the continuous feature encoding. Now the\nlinear model learns one coefficient for each day of the week, and one coefficient for\neach time of the day. That means that the “time of day” pattern is shared over all days\nof the week, though.\nUsing interaction features, we can allow the model to learn one coefficient for each\ncombination of day and time of day (see Figure 4-18):\nIn[61]:\npoly_transformer = PolynomialFeatures(degree=2, interaction_only=True,\n                                      include_bias=False)\nX_hour_week_onehot_poly = poly_transformer.fit_transform(X_hour_week_onehot)\nlr = Ridge()\neval_on_features(X_hour_week_onehot_poly, y, lr)\nOut[61]:\nTest-set R^2: 0.85\n248 \n| \nChapter 4: Representing Data and Engineering Features\nFigure 4-18. Predictions made by linear regression using a product of the day of week\nand hour of day features\nThis transformation finally yields a model that performs similarly well to the random\nforest. A big benefit of this model is that it is very clear what is learned: one coeffi‐\ncient for each day and time. We can simply plot the coefficients learned by the model,\nsomething that would not be possible for the random forest.\nFirst, we create feature names for the hour and day features:\nIn[62]:\nhour = [\"%02d:00\" % i for i in range(0, 24, 3)]\nday = [\"Mon\", \"Tue\", \"Wed\", \"Thu\", \"Fri\", \"Sat\", \"Sun\"]\nfeatures =  day + hour\nThen we name all the interaction features extracted by PolynomialFeatures, using\nthe get_feature_names method, and keep only the features with nonzero coeffi‐\ncients:\nIn[63]:\nfeatures_poly = poly_transformer.get_feature_names(features)\nfeatures_nonzero = np.array(features_poly)[lr.coef_ != 0]\ncoef_nonzero = lr.coef_[lr.coef_ != 0]\nNow we can visualize the coefficients learned by the linear model, as seen in\nFigure 4-19:\nIn[64]:\nplt.figure(figsize=(15, 2))\nplt.plot(coef_nonzero, 'o')\nplt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)",
    "coef_nonzero = lr.coef_[lr.coef_ != 0]\nNow we can visualize the coefficients learned by the linear model, as seen in\nFigure 4-19:\nIn[64]:\nplt.figure(figsize=(15, 2))\nplt.plot(coef_nonzero, 'o')\nplt.xticks(np.arange(len(coef_nonzero)), features_nonzero, rotation=90)\nplt.xlabel(\"Feature magnitude\")\nplt.ylabel(\"Feature\")\nUtilizing Expert Knowledge \n| \n249\nFigure 4-19. Coefficients of the linear regression model using a product of hour and day\nSummary and Outlook\nIn this chapter, we discussed how to deal with different data types (in particular, with\ncategorical variables). We emphasized the importance of representing data in a way\nthat is suitable for the machine learning algorithm—for example, by one-hot-\nencoding categorical variables. We also discussed the importance of engineering new\nfeatures, and the possibility of utilizing expert knowledge in creating derived features\nfrom your data. In particular, linear models might benefit greatly from generating\nnew features via binning and adding polynomials and interactions, while more com‐\nplex, nonlinear models like random forests and SVMs might be able to learn more\ncomplex tasks without explicitly expanding the feature space. In practice, the features\nthat are used (and the match between features and method) is often the most impor‐\ntant piece in making a machine learning approach work well.\nNow that you have a good idea of how to represent your data in an appropriate way\nand which algorithm to use for which task, the next chapter will focus on evaluating\nthe performance of machine learning models and selecting the right parameter\nsettings.\n250 \n| \nChapter 4: Representing Data and Engineering Features\nCHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.",
    "CHAPTER 5\nModel Evaluation and Improvement\nHaving discussed the fundamentals of supervised and unsupervised learning, and\nhaving explored a variety of machine learning algorithms, we will now dive more\ndeeply into evaluating models and selecting parameters.\nWe will focus on the supervised methods, regression and classification, as evaluating\nand selecting models in unsupervised learning is often a very qualitative process (as\nwe saw in Chapter 3).\nTo evaluate our supervised models, so far we have split our dataset into a training set\nand a test set using the train_test_split function, built a model on the training set\nby calling the fit method, and evaluated it on the test set using the score method,\nwhich for classification computes the fraction of correctly classified samples. Here’s\nan example of that process:\nIn[2]:\nfrom sklearn.datasets import make_blobs\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# create a synthetic dataset\nX, y = make_blobs(random_state=0)\n# split data and labels into a training and a test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n# instantiate a model and fit it to the training set\nlogreg = LogisticRegression().fit(X_train, y_train)\n# evaluate the model on the test set\nprint(\"Test set score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[2]:\nTest set score: 0.88\n251\nRemember, the reason we split our data into training and test sets is that we are inter‐\nested in measuring how well our model generalizes to new, previously unseen data.\nWe are not interested in how well our model fit the training set, but rather in how\nwell it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and",
    "well it can make predictions for data that was not observed during training.\nIn this chapter, we will expand on two aspects of this evaluation. We will first intro‐\nduce cross-validation, a more robust way to assess generalization performance, and\ndiscuss methods to evaluate classification and regression performance that go beyond\nthe default measures of accuracy and R2 provided by the score method.\nWe will also discuss grid search, an effective method for adjusting the parameters in\nsupervised models for the best generalization performance.\nCross-Validation\nCross-validation is a statistical method of evaluating generalization performance that\nis more stable and thorough than using a split into a training and a test set. In cross-\nvalidation, the data is instead split repeatedly and multiple models are trained. The\nmost commonly used version of cross-validation is k-fold cross-validation, where k is\na user-specified number, usually 5 or 10. When performing five-fold cross-validation,\nthe data is first partitioned into five parts of (approximately) equal size, called folds.\nNext, a sequence of models is trained. The first model is trained using the first fold as\nthe test set, and the remaining folds (2–5) are used as the training set. The model is\nbuilt using the data in folds 2–5, and then the accuracy is evaluated on fold 1. Then\nanother model is built, this time using fold 2 as the test set and the data in folds 1, 3,\n4, and 5 as the training set. This process is repeated using folds 3, 4, and 5 as test sets.\nFor each of these five splits of the data into training and test sets, we compute the\naccuracy. In the end, we have collected five accuracy values. The process is illustrated\nin Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement",
    "in Figure 5-1:\nIn[3]:\nmglearn.plots.plot_cross_validation()\nFigure 5-1. Data splitting in five-fold cross-validation\nUsually, the first fifth of the data is the first fold, the second fifth of the data is the\nsecond fold, and so on.\n252 \n| \nChapter 5: Model Evaluation and Improvement\nCross-Validation in scikit-learn\nCross-validation is implemented in scikit-learn using the cross_val_score func‐\ntion from the model_selection module. The parameters of the cross_val_score\nfunction are the model we want to evaluate, the training data, and the ground-truth\nlabels. Let’s evaluate LogisticRegression on the iris dataset:\nIn[4]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.datasets import load_iris\nfrom sklearn.linear_model import LogisticRegression\niris = load_iris()\nlogreg = LogisticRegression()\nscores = cross_val_score(logreg, iris.data, iris.target)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[4]:\nCross-validation scores: [ 0.961  0.922  0.958]\nBy default, cross_val_score performs three-fold cross-validation, returning three\naccuracy values. We can change the number of folds used by changing the cv parame‐\nter:\nIn[5]:\nscores = cross_val_score(logreg, iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[5]:\nCross-validation scores: [ 1.     0.967  0.933  0.9    1.   ]\nA common way to summarize the cross-validation accuracy is to compute the mean:\nIn[6]:\nprint(\"Average cross-validation score: {:.2f}\".format(scores.mean()))\nOut[6]:\nAverage cross-validation score: 0.96\nUsing the mean cross-validation we can conclude that we expect the model to be\naround 96% accurate on average. Looking at all five scores produced by the five-fold\ncross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it",
    "cross-validation, we can also conclude that there is a relatively high variance in the\naccuracy between folds, ranging from 100% accuracy to 90% accuracy. This could\nimply that the model is very dependent on the particular folds used for training, but it\ncould also just be a consequence of the small size of the dataset.\nCross-Validation \n| \n253\nBenefits of Cross-Validation\nThere are several benefits to using cross-validation instead of a single split into a\ntraining and a test set. First, remember that train_test_split performs a random\nsplit of the data. Imagine that we are “lucky” when randomly splitting the data, and\nall examples that are hard to classify end up in the training set. In that case, the test\nset will only contain “easy” examples, and our test set accuracy will be unrealistically\nhigh. Conversely, if we are “unlucky,” we might have randomly put all the hard-to-\nclassify examples in the test set and consequently obtain an unrealistically low score.\nHowever, when using cross-validation, each example will be in the training set exactly\nonce: each example is in one of the folds, and each fold is the test set once. Therefore,\nthe model needs to generalize well to all of the samples in the dataset for all of the\ncross-validation scores (and their mean) to be high.\nHaving multiple splits of the data also provides some information about how sensi‐\ntive our model is to the selection of the training dataset. For the iris dataset, we saw\naccuracies between 90% and 100%. This is quite a range, and it provides us with an\nidea about how the model might perform in the worst case and best case scenarios\nwhen applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold",
    "when applied to new data.\nAnother benefit of cross-validation as compared to using a single split of the data is\nthat we use our data more effectively. When using train_test_split, we usually use\n75% of the data for training and 25% of the data for evaluation. When using five-fold\ncross-validation, in each iteration we can use four-fifths of the data (80%) to fit the\nmodel. When using 10-fold cross-validation, we can use nine-tenths of the data\n(90%) to fit the model. More data will usually result in more accurate models.\nThe main disadvantage of cross-validation is increased computational cost. As we are\nnow training k models instead of a single model, cross-validation will be roughly k\ntimes slower than doing a single split of the data.\nIt is important to keep in mind that cross-validation is not a way to\nbuild a model that can be applied to new data. Cross-validation\ndoes not return a model. When calling cross_val_score, multiple\nmodels are built internally, but the purpose of cross-validation is\nonly to evaluate how well a given algorithm will generalize when\ntrained on a specific dataset.\nStratified k-Fold Cross-Validation and Other Strategies\nSplitting the dataset into k folds by starting with the first one-k-th part of the data, as\ndescribed in the previous section, might not always be a good idea. For example, let’s\nhave a look at the iris dataset:\n254 \n| \nChapter 5: Model Evaluation and Improvement\nIn[7]:\nfrom sklearn.datasets import load_iris\niris = load_iris()\nprint(\"Iris labels:\\n{}\".format(iris.target))\nOut[7]:\nIris labels:\n[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,",
    "1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nAs you can see, the first third of the data is the class 0, the second third is the class 1,\nand the last third is the class 2. Imagine doing three-fold cross-validation on this\ndataset. The first fold would be only class 0, so in the first split of the data, the test set\nwould be only class 0, and the training set would be only classes 1 and 2. As the\nclasses in training and test sets would be different for all three splits, the three-fold\ncross-validation accuracy would be zero on this dataset. That is not very helpful, as\nwe can do much better than 0% accuracy on iris.\nAs the simple k-fold strategy fails here, scikit-learn does not use it for classifica‐\ntion, but rather uses stratified k-fold cross-validation. In stratified cross-validation, we\nsplit the data such that the proportions between classes are the same in each fold as\nthey are in the whole dataset, as illustrated in Figure 5-2:\nIn[8]:\nmglearn.plots.plot_stratified_cross_validation()\nFigure 5-2. Comparison of standard cross-validation and stratified cross-validation\nwhen the data is ordered by class label\nCross-Validation \n| \n255\nFor example, if 90% of your samples belong to class A and 10% of your samples\nbelong to class B, then stratified cross-validation ensures that in each fold, 90% of\nsamples belong to class A and 10% of samples belong to class B.\nIt is usually a good idea to use stratified k-fold cross-validation instead of k-fold\ncross-validation to evaluate a classifier, because it results in more reliable estimates of\ngeneralization performance. In the case of only 10% of samples belonging to class B,\nusing standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.",
    "using standard k-fold cross-validation it might easily happen that one fold only con‐\ntains samples of class A. Using this fold as a test set would not be very informative\nabout the overall performance of the classifier.\nFor regression, scikit-learn uses the standard k-fold cross-validation by default. It\nwould be possible to also try to make each fold representative of the different values\nthe regression target has, but this is not a commonly used strategy and would be sur‐\nprising to most users.\nMore control over cross-validation\nWe saw earlier that we can adjust the number of folds that are used in\ncross_val_score using the cv parameter. However, scikit-learn allows for much\nfiner control over what happens during the splitting of the data by providing a cross-\nvalidation splitter as the cv parameter. For most use cases, the defaults of k-fold cross-\nvalidation for regression and stratified k-fold for classification work well, but there\nare some cases where you might want to use a different strategy. Say, for example, we\nwant to use the standard k-fold cross-validation on a classification dataset to repro‐\nduce someone else’s results. To do this, we first have to import the KFold splitter class\nfrom the model_selection module and instantiate it with the number of folds we\nwant to use:\nIn[9]:\nfrom sklearn.model_selection import KFold\nkfold = KFold(n_splits=5)\nThen, we can pass the kfold splitter object as the cv parameter to cross_val_score:\nIn[10]:\nprint(\"Cross-validation scores:\\n{}\".format(\n      cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[10]:\nCross-validation scores:\n[ 1.     0.933  0.433  0.967  0.433]\nThis way, we can verify that it is indeed a really bad idea to use three-fold (nonstrati‐\nfied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:",
    "fied) cross-validation on the iris dataset:\n256 \n| \nChapter 5: Model Evaluation and Improvement\nIn[11]:\nkfold = KFold(n_splits=3)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[11]:\nCross-validation scores:\n[ 0.  0.  0.]\nRemember: each fold corresponds to one of the classes in the iris dataset, and so\nnothing can be learned. Another way to resolve this problem is to shuffle the data\ninstead of stratifying the folds, to remove the ordering of the samples by label. We can\ndo that by setting the shuffle parameter of KFold to True. If we shuffle the data, we\nalso need to fix the random_state to get a reproducible shuffling. Otherwise, each run\nof cross_val_score would yield a different result, as each time a different split would\nbe used (this might not be a problem, but can be surprising). Shuffling the data before\nsplitting it yields a much better result:\nIn[12]:\nkfold = KFold(n_splits=3, shuffle=True, random_state=0)\nprint(\"Cross-validation scores:\\n{}\".format(\n    cross_val_score(logreg, iris.data, iris.target, cv=kfold)))\nOut[12]:\nCross-validation scores:\n[ 0.9   0.96  0.96]\nLeave-one-out cross-validation\nAnother frequently used cross-validation method is leave-one-out. You can think of\nleave-one-out cross-validation as k-fold cross-validation where each fold is a single\nsample. For each split, you pick a single data point to be the test set. This can be very\ntime consuming, particularly for large datasets, but sometimes provides better esti‐\nmates on small datasets:\nIn[13]:\nfrom sklearn.model_selection import LeaveOneOut\nloo = LeaveOneOut()\nscores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation",
    "scores = cross_val_score(logreg, iris.data, iris.target, cv=loo)\nprint(\"Number of cv iterations: \", len(scores))\nprint(\"Mean accuracy: {:.2f}\".format(scores.mean()))\nOut[13]:\nNumber of cv iterations:  150\nMean accuracy: 0.95\nCross-Validation \n| \n257\nShuffle-split cross-validation\nAnother, very flexible strategy for cross-validation is shuffle-split cross-validation. In\nshuffle-split cross-validation, each split samples train_size many points for the\ntraining set and test_size many (disjoint) point for the test set. This splitting is\nrepeated n_iter times. Figure 5-3 illustrates running four iterations of splitting a\ndataset consisting of 10 points, with a training set of 5 points and test sets of 2 points\neach (you can use integers for train_size and test_size to use absolute sizes for\nthese sets, or floating-point numbers to use fractions of the whole dataset):\nIn[14]:\nmglearn.plots.plot_shuffle_split()\nFigure 5-3. ShuffleSplit with 10 points, train_size=5, test_size=2, and n_iter=4\nThe following code splits the dataset into 50% training set and 50% test set for 10\niterations:\nIn[15]:\nfrom sklearn.model_selection import ShuffleSplit\nshuffle_split = ShuffleSplit(test_size=.5, train_size=.5, n_splits=10)\nscores = cross_val_score(logreg, iris.data, iris.target, cv=shuffle_split)\nprint(\"Cross-validation scores:\\n{}\".format(scores))\nOut[15]:\nCross-validation scores:\n[ 0.96   0.907  0.947  0.96   0.96   0.907  0.893  0.907  0.92   0.973]\nShuffle-split cross-validation allows for control over the number of iterations inde‐\npendently of the training and test sizes, which can sometimes be helpful. It also allows\nfor using only part of the data in each iteration, by providing train_size and\ntest_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.",
    "test_size settings that don’t add up to one. Subsampling the data in this way can be\nuseful for experimenting with large datasets.\nThere is also a stratified variant of ShuffleSplit, aptly named StratifiedShuffleS\nplit, which can provide more reliable results for classification tasks.\n258 \n| \nChapter 5: Model Evaluation and Improvement\nCross-validation with groups\nAnother very common setting for cross-validation is when there are groups in the\ndata that are highly related. Say you want to build a system to recognize emotions\nfrom pictures of faces, and you collect a dataset of pictures of 100 people where each\nperson is captured multiple times, showing various emotions. The goal is to build a\nclassifier that can correctly identify emotions of people not in the dataset. You could\nuse the default stratified cross-validation to measure the performance of a classifier\nhere. However, it is likely that pictures of the same person will be in both the training\nand the test set. It will be much easier for a classifier to detect emotions in a face that\nis part of the training set, compared to a completely new face. To accurately evaluate\nthe generalization to new faces, we must therefore ensure that the training and test\nsets contain images of different people.\nTo achieve this, we can use GroupKFold, which takes an array of groups as argument\nthat we can use to indicate which person is in the image. The groups array here indi‐\ncates groups in the data that should not be split when creating the training and test\nsets, and should not be confused with the class label.\nThis example of groups in the data is common in medical applications, where you\nmight have multiple samples from the same patient, but are interested in generalizing\nto new patients. Similarly, in speech recognition, you might have multiple recordings\nof the same speaker in your dataset, but are interested in recognizing speech of new\nspeakers.",
    "might have multiple samples from the same patient, but are interested in generalizing\nto new patients. Similarly, in speech recognition, you might have multiple recordings\nof the same speaker in your dataset, but are interested in recognizing speech of new\nspeakers.\nThe following is an example of using a synthetic dataset with a grouping given by the\ngroups array. The dataset consists of 12 data points, and for each of the data points,\ngroups specifies which group (think patient) the point belongs to. The groups specify\nthat there are four groups, and the first three samples belong to the first group, the\nnext four samples belong to the second group, and so on:\nIn[17]:\nfrom sklearn.model_selection import GroupKFold\n# create synthetic dataset\nX, y = make_blobs(n_samples=12, random_state=0)\n# assume the first three samples belong to the same group,\n# then the next four, etc.\ngroups = [0, 0, 0, 1, 1, 1, 1, 2, 2, 3, 3, 3]\nscores = cross_val_score(logreg, X, y, groups, cv=GroupKFold(n_splits=3))\nprint(\"Cross-validation scores:\\n{}\".format(scores))\nOut[17]:\nCross-validation scores:\n[ 0.75   0.8    0.667]\nThe samples don’t need to be ordered by group; we just did this for illustration pur‐\nposes. The splits that are calculated based on these labels are visualized in Figure 5-4.\nCross-Validation \n| \n259\nAs you can see, for each split, each group is either entirely in the training set or\nentirely in the test set:\nIn[16]:\nmglearn.plots.plot_label_kfold()\nFigure 5-4. Label-dependent splitting with GroupKFold\nThere are more splitting strategies for cross-validation in scikit-learn, which allow\nfor an even greater variety of use cases (you can find these in the scikit-learn user\nguide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\nthe most commonly used ones.\nGrid Search\nNow that we know how to evaluate how well a model generalizes, we can take the\nnext step and improve the model’s generalization performance by tuning its parame‐",
    "guide). However, the standard KFold, StratifiedKFold, and GroupKFold are by far\nthe most commonly used ones.\nGrid Search\nNow that we know how to evaluate how well a model generalizes, we can take the\nnext step and improve the model’s generalization performance by tuning its parame‐\nters. We discussed the parameter settings of many of the algorithms in scikit-learn\nin Chapters 2 and 3, and it is important to understand what the parameters mean\nbefore trying to adjust them. Finding the values of the important parameters of a\nmodel (the ones that provide the best generalization performance) is a tricky task, but\nnecessary for almost all models and datasets. Because it is such a common task, there\nare standard methods in scikit-learn to help you with it. The most commonly used\nmethod is grid search, which basically means trying all possible combinations of the\nparameters of interest.\nConsider the case of a kernel SVM with an RBF (radial basis function) kernel, as\nimplemented in the SVC class. As we discussed in Chapter 2, there are two important\nparameters: the kernel bandwidth, gamma, and the regularization parameter, C. Say we\nwant to try the values 0.001, 0.01, 0.1, 1, 10, and 100 for the parameter C, and the\nsame for gamma. Because we have six different settings for C and gamma that we want to\ntry, we have 36 combinations of parameters in total. Looking at all possible combina‐\ntions creates a table (or grid) of parameter settings for the SVM, as shown here:\n260 \n| \nChapter 5: Model Evaluation and Improvement\nC = 0.001\nC = 0.01\n… C = 10\ngamma=0.001\nSVC(C=0.001, gamma=0.001)\nSVC(C=0.01, gamma=0.001)\n… SVC(C=10, gamma=0.001)\ngamma=0.01\nSVC(C=0.001, gamma=0.01)\nSVC(C=0.01, gamma=0.01)\n… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:",
    "… SVC(C=10, gamma=0.01)\n…\n…\n…\n… …\ngamma=100\nSVC(C=0.001, gamma=100)\nSVC(C=0.01, gamma=100)\n… SVC(C=10, gamma=100)\nSimple Grid Search\nWe can implement a simple grid search just as for loops over the two parameters,\ntraining and evaluating a classifier for each combination:\nIn[18]:\n# naive grid search implementation\nfrom sklearn.svm import SVC\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\nprint(\"Size of training set: {}   size of test set: {}\".format(\n      X_train.shape[0], X_test.shape[0]))\nbest_score = 0\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters, train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_test, y_test)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\nprint(\"Best score: {:.2f}\".format(best_score))\nprint(\"Best parameters: {}\".format(best_parameters))\nOut[18]:\nSize of training set: 112   size of test set: 38\nBest score: 0.97\nBest parameters: {'C': 100, 'gamma': 0.001}\nThe Danger of Overfitting the Parameters and the Validation Set\nGiven this result, we might be tempted to report that we found a model that performs\nwith 97% accuracy on our dataset. However, this claim could be overly optimistic (or\njust wrong), for the following reason: we tried many different parameters and\nGrid Search \n| \n261\nselected the one with best accuracy on the test set, but this accuracy won’t necessarily\ncarry over to new data. Because we used the test data to adjust the parameters, we can\nno longer use it to assess how good the model is. This is the same reason we needed\nto split the data into training and test sets in the first place; we need an independent",
    "carry over to new data. Because we used the test data to adjust the parameters, we can\nno longer use it to assess how good the model is. This is the same reason we needed\nto split the data into training and test sets in the first place; we need an independent\ndataset to evaluate, one that was not used to create the model.\nOne way to resolve this problem is to split the data again, so we have three sets: the\ntraining set to build the model, the validation (or development) set to select the\nparameters of the model, and the test set to evaluate the performance of the selected\nparameters. Figure 5-5 shows what this looks like:\nIn[19]:\nmglearn.plots.plot_threefold_split()\nFigure 5-5. A threefold split of data into training set, validation set, and test set\nAfter selecting the best parameters using the validation set, we can rebuild a model\nusing the parameter settings we found, but now training on both the training data\nand the validation data. This way, we can use as much data as possible to build our\nmodel. This leads to the following implementation:\nIn[20]:\nfrom sklearn.svm import SVC\n# split data into train+validation set and test set\nX_trainval, X_test, y_trainval, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\n# split train+validation set into training and validation sets\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X_trainval, y_trainval, random_state=1)\nprint(\"Size of training set: {}   size of validation set: {}   size of test set:\"\n      \" {}\\n\".format(X_train.shape[0], X_valid.shape[0], X_test.shape[0]))\nbest_score = 0\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters, train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_valid, y_valid)\n        # if we got a better score, store the score and parameters\n        if score > best_score:",
    "svm = SVC(gamma=gamma, C=C)\n        svm.fit(X_train, y_train)\n        # evaluate the SVC on the test set\n        score = svm.score(X_valid, y_valid)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n262 \n| \nChapter 5: Model Evaluation and Improvement\n# rebuild a model on the combined training and validation set,\n# and evaluate it on the test set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\ntest_score = svm.score(X_test, y_test)\nprint(\"Best score on validation set: {:.2f}\".format(best_score))\nprint(\"Best parameters: \", best_parameters)\nprint(\"Test set score with best parameters: {:.2f}\".format(test_score))\nOut[20]:\nSize of training set: 84   size of validation set: 28   size of test set: 38\nBest score on validation set: 0.96\nBest parameters:  {'C': 10, 'gamma': 0.001}\nTest set score with best parameters: 0.92\nThe best score on the validation set is 96%: slightly lower than before, probably\nbecause we used less data to train the model (X_train is smaller now because we split\nour dataset twice). However, the score on the test set—the score that actually tells us\nhow well we generalize—is even lower, at 92%. So we can only claim to classify new\ndata 92% correctly, not 97% correctly as we thought before!\nThe distinction between the training set, validation set, and test set is fundamentally\nimportant to applying machine learning methods in practice. Any choices made\nbased on the test set accuracy “leak” information from the test set into the model.\nTherefore, it is important to keep a separate test set, which is only used for the final\nevaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐",
    "evaluation. It is good practice to do all exploratory analysis and model selection using\nthe combination of a training and a validation set, and reserve the test set for a final\nevaluation—this is even true for exploratory visualization. Strictly speaking, evaluat‐\ning more than one model on the test set and choosing the better of the two will result\nin an overly optimistic estimate of how accurate the model is.\nGrid Search with Cross-Validation\nWhile the method of splitting the data into a training, a validation, and a test set that\nwe just saw is workable, and relatively commonly used, it is quite sensitive to how\nexactly the data is split. From the output of the previous code snippet we can see that\nGridSearchCV selects 'C': 10, 'gamma': 0.001 as the best parameters, while the\noutput of the code in the previous section selects 'C': 100, 'gamma': 0.001 as the\nbest parameters. For a better estimate of the generalization performance, instead of\nusing a single split into a training and a validation set, we can use cross-validation to\nevaluate the performance of each parameter combination. This method can be coded\nup as follows:\nGrid Search \n| \n263\nIn[21]:\nfor gamma in [0.001, 0.01, 0.1, 1, 10, 100]:\n    for C in [0.001, 0.01, 0.1, 1, 10, 100]:\n        # for each combination of parameters,\n        # train an SVC\n        svm = SVC(gamma=gamma, C=C)\n        # perform cross-validation\n        scores = cross_val_score(svm, X_trainval, y_trainval, cv=5)\n        # compute mean cross-validation accuracy\n        score = np.mean(scores)\n        # if we got a better score, store the score and parameters\n        if score > best_score:\n            best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using",
    "best_score = score\n            best_parameters = {'C': C, 'gamma': gamma}\n# rebuild a model on the combined training and validation set\nsvm = SVC(**best_parameters)\nsvm.fit(X_trainval, y_trainval)\nTo evaluate the accuracy of the SVM using a particular setting of C and gamma using\nfive-fold cross-validation, we need to train 36 * 5 = 180 models. As you can imagine,\nthe main downside of the use of cross-validation is the time it takes to train all these\nmodels.\nThe following visualization (Figure 5-6) illustrates how the best parameter setting is\nselected in the preceding code:\nIn[22]:\nmglearn.plots.plot_cross_val_selection()\nFigure 5-6. Results of grid search with cross-validation\nFor each parameter setting (only a subset is shown), five accuracy values are compu‐\nted, one for each split in the cross-validation. Then the mean validation accuracy is\ncomputed for each parameter setting. The parameters with the highest mean valida‐\ntion accuracy are chosen, marked by the circle.\n264 \n| \nChapter 5: Model Evaluation and Improvement\nAs we said earlier, cross-validation is a way to evaluate a given algo‐\nrithm on a specific dataset. However, it is often used in conjunction\nwith parameter search methods like grid search. For this reason,\nmany people use the term cross-validation colloquially to refer to\ngrid search with cross-validation.\nThe overall process of splitting the data, running the grid search, and evaluating the\nfinal parameters is illustrated in Figure 5-7:\nIn[23]:\nmglearn.plots.plot_grid_search_overview()\nFigure 5-7. Overview of the process of parameter selection and model evaluation with\nGridSearchCV\nBecause grid search with cross-validation is such a commonly used method to adjust\nparameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐",
    "parameters, scikit-learn provides the GridSearchCV class, which implements it in\nthe form of an estimator. To use the GridSearchCV class, you first need to specify the\nparameters you want to search over using a dictionary. GridSearchCV will then per‐\nform all the necessary model fits. The keys of the dictionary are the names of parame‐\nters we want to adjust (as given when constructing the model—in this case, C and\ngamma), and the values are the parameter settings we want to try out. Trying the val‐\nues 0.001, 0.01, 0.1, 1, 10, and 100 for C and gamma translates to the following\ndictionary:\nIn[24]:\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nprint(\"Parameter grid:\\n{}\".format(param_grid))\nOut[24]:\nParameter grid:\n{'C': [0.001, 0.01, 0.1, 1, 10, 100], 'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nGrid Search \n| \n265\n1 A scikit-learn estimator that is created using another estimator is called a meta-estimator. GridSearchCV is\nthe most commonly used meta-estimator, but we will see more later.\nWe can now instantiate the GridSearchCV class with the model (SVC), the parameter\ngrid to search (param_grid), and the cross-validation strategy we want to use (say,\nfive-fold stratified cross-validation):\nIn[25]:\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\nGridSearchCV will use cross-validation in place of the split into a training and valida‐\ntion set that we used before. However, we still need to split the data into a training\nand a test set, to avoid overfitting the parameters:\nIn[26]:\nX_train, X_test, y_train, y_test = train_test_split(\n    iris.data, iris.target, random_state=0)\nThe grid_search object that we created behaves just like a classifier; we can call the\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\nrun cross-validation for each combination of parameters we specified in param_grid:",
    "iris.data, iris.target, random_state=0)\nThe grid_search object that we created behaves just like a classifier; we can call the\nstandard methods fit, predict, and score on it.1 However, when we call fit, it will\nrun cross-validation for each combination of parameters we specified in param_grid:\nIn[27]:\ngrid_search.fit(X_train, y_train)\nFitting the GridSearchCV object not only searches for the best parameters, but also\nautomatically fits a new model on the whole training dataset with the parameters that\nyielded the best cross-validation performance. What happens in fit is therefore\nequivalent to the result of the In[21] code we saw at the beginning of this section. The\nGridSearchCV class provides a very convenient interface to access the retrained\nmodel using the predict and score methods. To evaluate how well the best found\nparameters generalize, we can call score on the test set:\nIn[28]:\nprint(\"Test set score: {:.2f}\".format(grid_search.score(X_test, y_test)))\nOut[28]:\nTest set score: 0.97\nChoosing the parameters using cross-validation, we actually found a model that ach‐\nieves 97% accuracy on the test set. The important thing here is that we did not use the\ntest set to choose the parameters. The parameters that were found are scored in the\n266 \n| \nChapter 5: Model Evaluation and Improvement\nbest_params_ attribute, and the best cross-validation accuracy (the mean accuracy\nover the different splits for this parameter setting) is stored in best_score_:\nIn[29]:\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[29]:\nBest parameters: {'C': 100, 'gamma': 0.01}\nBest cross-validation score: 0.97\nAgain, be careful not to confuse best_score_ with the generaliza‐\ntion performance of the model as computed by the score method\non the test set. Using the score method (or evaluating the output of\nthe predict method) employs a model trained on the whole train‐",
    "Best cross-validation score: 0.97\nAgain, be careful not to confuse best_score_ with the generaliza‐\ntion performance of the model as computed by the score method\non the test set. Using the score method (or evaluating the output of\nthe predict method) employs a model trained on the whole train‐\ning set. The best_score_ attribute stores the mean cross-validation\naccuracy, with cross-validation performed on the training set.\nSometimes it is helpful to have access to the actual model that was found—for exam‐\nple, to look at coefficients or feature importances. You can access the model with the\nbest parameters trained on the whole training set using the best_estimator_\nattribute:\nIn[30]:\nprint(\"Best estimator:\\n{}\".format(grid_search.best_estimator_))\nOut[30]:\nBest estimator:\nSVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n   decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n   max_iter=-1, probability=False, random_state=None, shrinking=True,\n   tol=0.001, verbose=False)\nBecause grid_search itself has predict and score methods, using best_estimator_\nis not needed to make predictions or evaluate the model.\nAnalyzing the result of cross-validation\nIt is often helpful to visualize the results of cross-validation, to understand how the\nmodel generalization depends on the parameters we are searching. As grid searches\nare quite computationally expensive to run, often it is a good idea to start with a rela‐\ntively coarse and small grid. We can then inspect the results of the cross-validated\ngrid search, and possibly expand our search. The results of a grid search can be found\nin the cv_results_ attribute, which is a dictionary storing all aspects of the search. It\nGrid Search \n| \n267\ncontains a lot of details, as you can see in the following output, and is best looked at\nafter converting it to a pandas DataFrame:\nIn[31]:\nimport pandas as pd\n# convert to DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n# show the first 5 rows",
    "Grid Search \n| \n267\ncontains a lot of details, as you can see in the following output, and is best looked at\nafter converting it to a pandas DataFrame:\nIn[31]:\nimport pandas as pd\n# convert to DataFrame\nresults = pd.DataFrame(grid_search.cv_results_)\n# show the first 5 rows\ndisplay(results.head())\nOut[31]:\n    param_C   param_gamma   params                        mean_test_score\n0   0.001     0.001         {'C': 0.001, 'gamma': 0.001}       0.366\n1   0.001      0.01         {'C': 0.001, 'gamma': 0.01}        0.366\n2   0.001       0.1         {'C': 0.001, 'gamma': 0.1}         0.366\n3   0.001         1         {'C': 0.001, 'gamma': 1}           0.366\n4   0.001        10         {'C': 0.001, 'gamma': 10}          0.366\n       rank_test_score  split0_test_score  split1_test_score  split2_test_score\n0               22              0.375           0.347           0.363\n1               22              0.375           0.347           0.363\n2               22              0.375           0.347           0.363\n3               22              0.375           0.347           0.363\n4               22              0.375           0.347           0.363\n       split3_test_score  split4_test_score  std_test_score\n0           0.363              0.380           0.011\n1           0.363              0.380           0.011\n2           0.363              0.380           0.011\n3           0.363              0.380           0.011\n4           0.363              0.380           0.011\nEach row in results corresponds to one particular parameter setting. For each set‐\nting, the results of all cross-validation splits are recorded, as well as the mean and\nstandard deviation over all splits. As we were searching a two-dimensional grid of\nparameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\nspond to C and gamma:\nIn[32]:\nscores = np.array(results.mean_test_score).reshape(6, 6)",
    "parameters (C and gamma), this is best visualized as a heat map (Figure 5-8). First we\nextract the mean validation scores, then we reshape the scores so that the axes corre‐\nspond to C and gamma:\nIn[32]:\nscores = np.array(results.mean_test_score).reshape(6, 6)\n# plot the mean cross-validation scores\nmglearn.tools.heatmap(scores, xlabel='gamma', xticklabels=param_grid['gamma'],\n                      ylabel='C', yticklabels=param_grid['C'], cmap=\"viridis\")\n268 \n| \nChapter 5: Model Evaluation and Improvement\nFigure 5-8. Heat map of mean cross-validation score as a function of C and gamma\nEach point in the heat map corresponds to one run of cross-validation, with a partic‐\nular parameter setting. The color encodes the cross-validation accuracy, with light\ncolors meaning high accuracy and dark colors meaning low accuracy. You can see\nthat SVC is very sensitive to the setting of the parameters. For many of the parameter\nsettings, the accuracy is around 40%, which is quite bad; for other settings the accu‐\nracy is around 96%. We can take away from this plot several things. First, the parame‐\nters we adjusted are very important for obtaining good performance. Both parameters\n(C and gamma) matter a lot, as adjusting them can change the accuracy from 40% to\n96%. Additionally, the ranges we picked for the parameters are ranges in which we\nsee significant changes in the outcome. It’s also important to note that the ranges for\nthe parameters are large enough: the optimum values for each parameter are not on\nthe edges of the plot.\nNow let’s look at some plots (shown in Figure 5-9) where the result is less ideal,\nbecause the search ranges were not chosen properly:\nFigure 5-9. Heat map visualizations of misspecified search grids\nGrid Search \n| \n269\nIn[33]:\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\nparam_grid_linear = {'C': np.linspace(1, 2, 6),\n                     'gamma':  np.linspace(1, 2, 6)}\nparam_grid_one_log = {'C': np.linspace(1, 2, 6),",
    "Figure 5-9. Heat map visualizations of misspecified search grids\nGrid Search \n| \n269\nIn[33]:\nfig, axes = plt.subplots(1, 3, figsize=(13, 5))\nparam_grid_linear = {'C': np.linspace(1, 2, 6),\n                     'gamma':  np.linspace(1, 2, 6)}\nparam_grid_one_log = {'C': np.linspace(1, 2, 6),\n                      'gamma':  np.logspace(-3, 2, 6)}\nparam_grid_range = {'C': np.logspace(-3, 2, 6),\n                    'gamma':  np.logspace(-7, -2, 6)}\nfor param_grid, ax in zip([param_grid_linear, param_grid_one_log,\n                           param_grid_range], axes):\n    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n    grid_search.fit(X_train, y_train)\n    scores = grid_search.cv_results_['mean_test_score'].reshape(6, 6)\n    # plot the mean cross-validation scores\n    scores_image = mglearn.tools.heatmap(\n        scores, xlabel='gamma', ylabel='C', xticklabels=param_grid['gamma'],\n        yticklabels=param_grid['C'], cmap=\"viridis\", ax=ax)\nplt.colorbar(scores_image, ax=axes.tolist())\nThe first panel shows no changes at all, with a constant color over the whole parame‐\nter grid. In this case, this is caused by improper scaling and range of the parameters C\nand gamma. However, if no change in accuracy is visible over the different parameter\nsettings, it could also be that a parameter is just not important at all. It is usually good\nto try very extreme values first, to see if there are any changes in the accuracy as a\nresult of changing a parameter.\nThe second panel shows a vertical stripe pattern. This indicates that only the setting\nof the gamma parameter makes any difference. This could mean that the gamma param‐\neter is searching over interesting values but the C parameter is not—or it could mean\nthe C parameter is not important.\nThe third panel shows changes in both C and gamma. However, we can see that in the\nentire bottom left of the plot, nothing interesting is happening. We can probably",
    "eter is searching over interesting values but the C parameter is not—or it could mean\nthe C parameter is not important.\nThe third panel shows changes in both C and gamma. However, we can see that in the\nentire bottom left of the plot, nothing interesting is happening. We can probably\nexclude the very small values from future grid searches. The optimum parameter set‐\nting is at the top right. As the optimum is in the border of the plot, we can expect that\nthere might be even better values beyond this border, and we might want to change\nour search range to include more parameters in this region.\nTuning the parameter grid based on the cross-validation scores is perfectly fine, and a\ngood way to explore the importance of different parameters. However, you should\nnot test different parameter ranges on the final test set—as we discussed earlier, eval‐\n270 \n| \nChapter 5: Model Evaluation and Improvement\nuation of the test set should happen only once we know exactly what model we want\nto use.\nSearch over spaces that are not grids\nIn some cases, trying all possible combinations of all parameters as GridSearchCV\nusually does, is not a good idea. For example, SVC has a kernel parameter, and\ndepending on which kernel is chosen, other parameters will be relevant. If ker\nnel='linear', the model is linear, and only the C parameter is used. If kernel='rbf',\nboth the C and gamma parameters are used (but not other parameters like degree). In\nthis case, searching over all possible combinations of C, gamma, and kernel wouldn’t\nmake sense: if kernel='linear', gamma is not used, and trying different values for\ngamma would be a waste of time. To deal with these kinds of “conditional” parameters,\nGridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],",
    "GridSearchCV allows the param_grid to be a list of dictionaries. Each dictionary in the\nlist is expanded into an independent grid. A possible grid search involving kernel and\nparameters could look like this:\nIn[34]:\nparam_grid = [{'kernel': ['rbf'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100],\n               'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n              {'kernel': ['linear'],\n               'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nprint(\"List of grids:\\n{}\".format(param_grid))\nOut[34]:\nList of grids:\n[{'kernel': ['rbf'], 'C': [0.001, 0.01, 0.1, 1, 10, 100],\n  'gamma': [0.001, 0.01, 0.1, 1, 10, 100]},\n {'kernel': ['linear'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\nIn the first grid, the kernel parameter is always set to 'rbf' (not that the entry for\nkernel is a list of length one), and both the C and gamma parameters are varied. In the\nsecond grid, the kernel parameter is always set to linear, and only C is varied. Now\nlet’s apply this more complex parameter search:\nIn[35]:\ngrid_search = GridSearchCV(SVC(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nprint(\"Best parameters: {}\".format(grid_search.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid_search.best_score_))\nOut[35]:\nBest parameters: {'C': 100, 'kernel': 'rbf', 'gamma': 0.01}\nBest cross-validation score: 0.97\nGrid Search \n| \n271\nLet’s look at the cv_results_ again. As expected, if kernel is 'linear', then only C is\nvaried:\nIn[36]:\nresults = pd.DataFrame(grid_search.cv_results_)\n# we display the transposed table so that it better fits on the page:\ndisplay(results.T)\nOut[36]:\n0\n1\n2\n3\n… 38\n39\n40\n41\nparam_C\n0.001\n0.001\n0.001\n0.001\n… 0.1\n1\n10\n100\nparam_gamma\n0.001\n0.01\n0.1\n1\n… NaN\nNaN\nNaN\nNaN\nparam_kernel\nrbf\nrbf\nrbf\nrbf\n… linear\nlinear\nlinear\nlinear\nparams\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.001}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.01}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.1}\n{C: 0.001,\nkernel: rbf,\ngamma: 1}\n… {C: 0.1,\nkernel:\nlinear}\n{C: 1,\nkernel:\nlinear}\n{C: 10,\nkernel:\nlinear}",
    "NaN\nNaN\nNaN\nparam_kernel\nrbf\nrbf\nrbf\nrbf\n… linear\nlinear\nlinear\nlinear\nparams\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.001}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.01}\n{C: 0.001,\nkernel: rbf,\ngamma:\n0.1}\n{C: 0.001,\nkernel: rbf,\ngamma: 1}\n… {C: 0.1,\nkernel:\nlinear}\n{C: 1,\nkernel:\nlinear}\n{C: 10,\nkernel:\nlinear}\n{C: 100,\nkernel:\nlinear}\nmean_test_score\n0.37\n0.37\n0.37\n0.37\n… 0.95\n0.97\n0.96\n0.96\nrank_test_score\n27\n27\n27\n27\n… 11\n1\n3\n3\nsplit0_test_score\n0.38\n0.38\n0.38\n0.38\n… 0.96\n1\n0.96\n0.96\nsplit1_test_score\n0.35\n0.35\n0.35\n0.35\n… 0.91\n0.96\n1\n1\nsplit2_test_score\n0.36\n0.36\n0.36\n0.36\n… 1\n1\n1\n1\nsplit3_test_score\n0.36\n0.36\n0.36\n0.36\n… 0.91\n0.95\n0.91\n0.91\nsplit4_test_score\n0.38\n0.38\n0.38\n0.38\n… 0.95\n0.95\n0.95\n0.95\nstd_test_score\n0.011\n0.011\n0.011\n0.011\n… 0.033\n0.022\n0.034\n0.034\n12 rows × 42 columns\nUsing different cross-validation strategies with grid search\nSimilarly to cross_val_score, GridSearchCV uses stratified k-fold cross-validation\nby default for classification, and k-fold cross-validation for regression. However, you\ncan also pass any cross-validation splitter, as described in “More control over cross-\nvalidation” on page 256, as the cv parameter in GridSearchCV. In particular, to get\nonly a single split into a training and a validation set, you can use ShuffleSplit or\nStratifiedShuffleSplit with n_iter=1. This might be helpful for very large data‐\nsets, or very slow models.\nNested cross-validation\nIn the preceding examples, we went from using a single split of the data into training,\nvalidation, and test sets to splitting the data into training and test sets and then per‐\nforming cross-validation on the training set. But when using GridSearchCV as\n272 \n| \nChapter 5: Model Evaluation and Improvement\ndescribed earlier, we still have a single split of the data into training and test sets,\nwhich might make our results unstable and make us depend too much on this single\nsplit of the data. We can go a step further, and instead of splitting the original data",
    "described earlier, we still have a single split of the data into training and test sets,\nwhich might make our results unstable and make us depend too much on this single\nsplit of the data. We can go a step further, and instead of splitting the original data\ninto training and test sets once, use multiple splits of cross-validation. This will result\nin what is called nested cross-validation. In nested cross-validation, there is an outer\nloop over splits of the data into training and test sets. For each of them, a grid search\nis run (which might result in different best parameters for each split in the outer\nloop). Then, for each outer split, the test set score using the best settings is reported.\nThe result of this procedure is a list of scores—not a model, and not a parameter set‐\nting. The scores tell us how well a model generalizes, given the best parameters found\nby the grid. As it doesn’t provide a model that can be used on new data, nested cross-\nvalidation is rarely used when looking for a predictive model to apply to future data.\nHowever, it can be useful for evaluating how well a given model works on a particular\ndataset.\nImplementing nested cross-validation in scikit-learn is straightforward. We call\ncross_val_score with an instance of GridSearchCV as the model:\nIn[34]:\nscores = cross_val_score(GridSearchCV(SVC(), param_grid, cv=5),\n                         iris.data, iris.target, cv=5)\nprint(\"Cross-validation scores: \", scores)\nprint(\"Mean cross-validation score: \", scores.mean())\nOut[34]:\nCross-validation scores:  [ 0.967  1.     0.967  0.967  1.   ]\nMean cross-validation score:  0.98\nThe result of our nested cross-validation can be summarized as “SVC can achieve 98%\nmean cross-validation accuracy on the iris dataset”—nothing more and nothing\nless.\nHere, we used stratified five-fold cross-validation in both the inner and the outer\nloop. As our param_grid contains 36 combinations of parameters, this results in a",
    "mean cross-validation accuracy on the iris dataset”—nothing more and nothing\nless.\nHere, we used stratified five-fold cross-validation in both the inner and the outer\nloop. As our param_grid contains 36 combinations of parameters, this results in a\nwhopping 36 * 5 * 5 = 900 models being built, making nested cross-validation a very\nexpensive procedure. Here, we used the same cross-validation splitter in the inner\nand the outer loop; however, this is not necessary and you can use any combination\nof cross-validation strategies in the inner and outer loops. It can be a bit tricky to\nunderstand what is happening in the single line given above, and it can be helpful to\nvisualize it as for loops, as done in the following simplified implementation:\nGrid Search \n| \n273\nIn[35]:\ndef nested_cv(X, y, inner_cv, outer_cv, Classifier, parameter_grid):\n    outer_scores = []\n    # for each split of the data in the outer cross-validation\n    # (split method returns indices)\n    for training_samples, test_samples in outer_cv.split(X, y):\n        # find best parameter using inner cross-validation\n        best_parms = {}\n        best_score = -np.inf\n        # iterate over parameters\n        for parameters in parameter_grid:\n            # accumulate score over inner splits\n            cv_scores = []\n            # iterate over inner cross-validation\n            for inner_train, inner_test in inner_cv.split(\n                    X[training_samples], y[training_samples]):\n                # build classifier given parameters and training data\n                clf = Classifier(**parameters)\n                clf.fit(X[inner_train], y[inner_train])\n                # evaluate on inner test set\n                score = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters",
    "score = clf.score(X[inner_test], y[inner_test])\n                cv_scores.append(score)\n            # compute mean score over inner folds\n            mean_score = np.mean(cv_scores)\n            if mean_score > best_score:\n                # if better than so far, remember parameters\n                best_score = mean_score\n                best_params = parameters\n        # build classifier on best parameters using outer training set\n        clf = Classifier(**best_params)\n        clf.fit(X[training_samples], y[training_samples])\n        # evaluate\n        outer_scores.append(clf.score(X[test_samples], y[test_samples]))\n    return np.array(outer_scores)\nNow, let’s run this function on the iris dataset:\nIn[36]:\nfrom sklearn.model_selection import ParameterGrid, StratifiedKFold\nscores = nested_cv(iris.data, iris.target, StratifiedKFold(5),\n          StratifiedKFold(5), SVC, ParameterGrid(param_grid))\nprint(\"Cross-validation scores: {}\".format(scores))\nOut[36]:\nCross-validation scores: [ 0.967  1.     0.967  0.967  1.   ]\nParallelizing cross-validation and grid search\nWhile running a grid search over many parameters and on large datasets can be com‐\nputationally challenging, it is also embarrassingly parallel. This means that building a\n274 \n| \nChapter 5: Model Evaluation and Improvement\nmodel using a particular parameter setting on a particular cross-validation split can\nbe done completely independently from the other parameter settings and models.\nThis makes grid search and cross-validation ideal candidates for parallelization over\nmultiple CPU cores or over a cluster. You can make use of multiple cores in Grid\nSearchCV and cross_val_score by setting the n_jobs parameter to the number of\nCPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),",
    "CPU cores you want to use. You can set n_jobs=-1 to use all available cores.\nYou should be aware that scikit-learn does not allow nesting of parallel operations.\nSo, if you are using the n_jobs option on your model (for example, a random forest),\nyou cannot use it in GridSearchCV to search over this model. If your dataset and\nmodel are very large, it might be that using many cores uses up too much memory,\nand you should monitor your memory usage when building large models in parallel.\nIt is also possible to parallelize grid search and cross-validation over multiple\nmachines in a cluster, although at the time of writing this is not supported within\nscikit-learn. It is, however, possible to use the IPython parallel framework for par‐\nallel grid searches, if you don’t mind writing the for loop over parameters as we did\nin “Simple Grid Search” on page 261.\nFor Spark users, there is also the recently developed spark-sklearn package, which\nallows running a grid search over an already established Spark cluster.\nEvaluation Metrics and Scoring\nSo far, we have evaluated classification performance using accuracy (the fraction of\ncorrectly classified samples) and regression performance using R2. However, these are\nonly two of the many possible ways to summarize how well a supervised model per‐\nforms on a given dataset. In practice, these evaluation metrics might not be appropri‐\nate for your application, and it is important to choose the right metric when selecting\nbetween models and adjusting parameters.\nKeep the End Goal in Mind\nWhen selecting a metric, you should always have the end goal of the machine learn‐\ning application in mind. In practice, we are usually interested not just in making\naccurate predictions, but in using these predictions as part of a larger decision-\nmaking process. Before picking a machine learning metric, you should think about\nthe high-level goal of the application, often called the business metric. The conse‐",
    "accurate predictions, but in using these predictions as part of a larger decision-\nmaking process. Before picking a machine learning metric, you should think about\nthe high-level goal of the application, often called the business metric. The conse‐\nquences of choosing a particular algorithm for a machine learning application are\nEvaluation Metrics and Scoring \n| \n275\n2 We ask scientifically minded readers to excuse the commercial language in this section. Not losing track of the\nend goal is equally important in science, though the authors are not aware of a similar phrase to “business\nimpact” being used in that realm.\ncalled the business impact.2 Maybe the high-level goal is avoiding traffic accidents, or\ndecreasing the number of hospital admissions. It could also be getting more users for\nyour website, or having users spend more money in your shop. When choosing a\nmodel or adjusting parameters, you should pick the model or parameter values that\nhave the most positive influence on the business metric. Often this is hard, as assess‐\ning the business impact of a particular model might require putting it in production\nin a real-life system.\nIn the early stages of development, and for adjusting parameters, it is often infeasible\nto put models into production just for testing purposes, because of the high business\nor personal risks that can be involved. Imagine evaluating the pedestrian avoidance\ncapabilities of a self-driving car by just letting it drive around, without verifying it\nfirst; if your model is bad, pedestrians will be in trouble! Therefore we often need to\nfind some surrogate evaluation procedure, using an evaluation metric that is easier to\ncompute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐",
    "compute. For example, we could test classifying images of pedestrians against non-\npedestrians and measure accuracy. Keep in mind that this is only a surrogate, and it\npays off to find the closest metric to the original business goal that is feasible to evalu‐\nate. This closest metric should be used whenever possible for model evaluation and\nselection. The result of this evaluation might not be a single number—the conse‐\nquence of your algorithm could be that you have 10% more customers, but each cus‐\ntomer will spend 15% less—but it should capture the expected business impact of\nchoosing one model over another.\nIn this section, we will first discuss metrics for the important special case of binary\nclassification, then turn to multiclass classification and finally regression.\nMetrics for Binary Classification\nBinary classification is arguably the most common and conceptually simple applica‐\ntion of machine learning in practice. However, there are still a number of caveats in\nevaluating even this simple task. Before we dive into alternative metrics, let’s have a\nlook at the ways in which measuring accuracy might be misleading. Remember that\nfor binary classification, we often speak of a positive class and a negative class, with\nthe understanding that the positive class is the one we are looking for.\nKinds of errors\nOften, accuracy is not a good measure of predictive performance, as the number of\nmistakes we make does not contain all the information we are interested in. Imagine\nan application to screen for the early detection of cancer using an automated test. If\n276 \n| \nChapter 5: Model Evaluation and Improvement\nthe test is negative, the patient will be assumed healthy, while if the test is positive, the\npatient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any",
    "patient will undergo additional screening. Here, we would call a positive test (an indi‐\ncation of cancer) the positive class, and a negative test the negative class. We can’t\nassume that our model will always work perfectly, and it will make mistakes. For any\napplication, we need to ask ourselves what the consequences of these mistakes might\nbe in the real world.\nOne possible mistake is that a healthy patient will be classified as positive, leading to\nadditional testing. This leads to some costs and an inconvenience for the patient (and\npossibly some mental distress). An incorrect positive prediction is called a false posi‐\ntive. The other possible mistake is that a sick patient will be classified as negative, and\nwill not receive further tests and treatment. The undiagnosed cancer might lead to\nserious health issues, and could even be fatal. A mistake of this kind—an incorrect\nnegative prediction—is called a false negative. In statistics, a false positive is also\nknown as type I error, and a false negative as type II error. We will stick to “false nega‐\ntive” and “false positive,” as they are more explicit and easier to remember. In the can‐\ncer diagnosis example, it is clear that we want to avoid false negatives as much as\npossible, while false positives can be viewed as more of a minor nuisance.\nWhile this is a particularly drastic example, the consequence of false positives and\nfalse negatives are rarely the same. In commercial applications, it might be possible to\nassign dollar values to both kinds of mistakes, which would allow measuring the error\nof a particular prediction in dollars, instead of accuracy. This might be much more\nmeaningful for making business decisions on which model to use.\nImbalanced datasets\nTypes of errors play an important role when one of two classes is much more frequent\nthan the other one. This is very common in practice; a good example is click-through",
    "meaningful for making business decisions on which model to use.\nImbalanced datasets\nTypes of errors play an important role when one of two classes is much more frequent\nthan the other one. This is very common in practice; a good example is click-through\nprediction, where each data point represents an “impression,” an item that was shown\nto a user. This item might be an ad, or a related story, or a related person to follow on\na social media site. The goal is to predict whether, if shown a particular item, a user\nwill click on it (indicating they are interested). Most things users are shown on the\nInternet (in particular, ads) will not result in a click. You might need to show a user\n100 ads or articles before they find something interesting enough to click on. This\nresults in a dataset where for each 99 “no click” data points, there is 1 “clicked” data\npoint; in other words, 99% of the samples belong to the “no click” class. Datasets in\nwhich one class is much more frequent than the other are often called imbalanced\ndatasets, or datasets with imbalanced classes. In reality, imbalanced data is the norm,\nand it is rare that the events of interest have equal or even similar frequency in the\ndata.\nNow let’s say you build a classifier that is 99% accurate on the click prediction task.\nWhat does that tell you? 99% accuracy sounds impressive, but this doesn’t take the\nEvaluation Metrics and Scoring \n| \n277\nclass imbalance into account. You can achieve 99% accuracy without building a\nmachine learning model, by always predicting “no click.” On the other hand, even\nwith imbalanced data, a 99% accurate model could in fact be quite good. However,\naccuracy doesn’t allow us to distinguish the constant “no click” model from a poten‐\ntially good model.\nTo illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\ning the digit 9 against the nine other classes:\nIn[37]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\ny = digits.target == 9",
    "tially good model.\nTo illustrate, we’ll create a 9:1 imbalanced dataset from the digits dataset, by classify‐\ning the digit 9 against the nine other classes:\nIn[37]:\nfrom sklearn.datasets import load_digits\ndigits = load_digits()\ny = digits.target == 9\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\nWe can use the DummyClassifier to always predict the majority class (here\n“not nine”) to see how uninformative accuracy can be:\nIn[38]:\nfrom sklearn.dummy import DummyClassifier\ndummy_majority = DummyClassifier(strategy='most_frequent').fit(X_train, y_train)\npred_most_frequent = dummy_majority.predict(X_test)\nprint(\"Unique predicted labels: {}\".format(np.unique(pred_most_frequent)))\nprint(\"Test score: {:.2f}\".format(dummy_majority.score(X_test, y_test)))\nOut[38]:\nUnique predicted labels: [False]\nTest score: 0.90\nWe obtained close to 90% accuracy without learning anything. This might seem strik‐\ning, but think about it for a minute. Imagine someone telling you their model is 90%\naccurate. You might think they did a very good job. But depending on the problem,\nthat might be possible by just predicting one class! Let’s compare this against using an\nactual classifier:\nIn[39]:\nfrom sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier(max_depth=2).fit(X_train, y_train)\npred_tree = tree.predict(X_test)\nprint(\"Test score: {:.2f}\".format(tree.score(X_test, y_test)))\nOut[39]:\nTest score: 0.92\n278 \n| \nChapter 5: Model Evaluation and Improvement\nAccording to accuracy, the DecisionTreeClassifier is only slightly better than the\nconstant predictor. This could indicate either that something is wrong with how we\nused DecisionTreeClassifier, or that accuracy is in fact not a good measure here.\nFor comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:",
    "For comparison purposes, let’s evaluate two more classifiers, LogisticRegression\nand the default DummyClassifier, which makes random predictions but produces\nclasses with the same proportions as in the training set:\nIn[40]:\nfrom sklearn.linear_model import LogisticRegression\ndummy = DummyClassifier().fit(X_train, y_train)\npred_dummy = dummy.predict(X_test)\nprint(\"dummy score: {:.2f}\".format(dummy.score(X_test, y_test)))\nlogreg = LogisticRegression(C=0.1).fit(X_train, y_train)\npred_logreg = logreg.predict(X_test)\nprint(\"logreg score: {:.2f}\".format(logreg.score(X_test, y_test)))\nOut[40]:\ndummy score: 0.80\nlogreg score: 0.98\nThe dummy classifier that produces random output is clearly the worst of the lot\n(according to accuracy), while LogisticRegression produces very good results.\nHowever, even the random classifier yields over 80% accuracy. This makes it very\nhard to judge which of these results is actually helpful. The problem here is that accu‐\nracy is an inadequate measure for quantifying predictive performance in this imbal‐\nanced setting. For the rest of this chapter, we will explore alternative metrics that\nprovide better guidance in selecting models. In particular, we would like to have met‐\nrics that tell us how much better a model is than making “most frequent” predictions\nor random predictions, as they are computed in pred_most_frequent and\npred_dummy. If we use a metric to assess our models, it should definitely be able to\nweed out these nonsense predictions.\nConfusion matrices\nOne of the most comprehensive ways to represent the result of evaluating binary clas‐\nsification is using confusion matrices. Let’s inspect the predictions of LogisticRegres\nsion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)",
    "sion from the previous section using the confusion_matrix function. We already\nstored the predictions on the test set in pred_logreg:\nEvaluation Metrics and Scoring \n| \n279\nIn[41]:\nfrom sklearn.metrics import confusion_matrix\nconfusion = confusion_matrix(y_test, pred_logreg)\nprint(\"Confusion matrix:\\n{}\".format(confusion))\nOut[41]:\nConfusion matrix:\n[[401   2]\n [  8  39]]\nThe output of confusion_matrix is a two-by-two array, where the rows correspond\nto the true classes and the columns correspond to the predicted classes. Each entry\ncounts how often a sample that belongs to the class corresponding to the row (here,\n“not nine” and “nine”) was classified as the class corresponding to the column. The\nfollowing plot (Figure 5-10) illustrates this meaning:\nIn[42]:\nmglearn.plots.plot_confusion_matrix_illustration()\nFigure 5-10. Confusion matrix of the “nine vs. rest” classification task\n280 \n| \nChapter 5: Model Evaluation and Improvement\n3 The main diagonal of a two-dimensional array or matrix A is A[i, i].\nEntries on the main diagonal3 of the confusion matrix correspond to correct classifi‐\ncations, while other entries tell us how many samples of one class got mistakenly clas‐\nsified as another class.\nIf we declare “a nine” the positive class, we can relate the entries of the confusion\nmatrix with the terms false positive and false negative that we introduced earlier. To\ncomplete the picture, we call correctly classified samples belonging to the positive\nclass true positives and correctly classified samples belonging to the negative class true\nnegatives. These terms are usually abbreviated FP, FN, TP, and TN and lead to the fol‐\nlowing interpretation for the confusion matrix (Figure 5-11):\nIn[43]:\nmglearn.plots.plot_binary_confusion_matrix()\nFigure 5-11. Confusion matrix for binary classification\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two\ndummy models, the decision tree, and the logistic regression):\nIn[44]:",
    "In[43]:\nmglearn.plots.plot_binary_confusion_matrix()\nFigure 5-11. Confusion matrix for binary classification\nNow let’s use the confusion matrix to compare the models we fitted earlier (the two\ndummy models, the decision tree, and the logistic regression):\nIn[44]:\nprint(\"Most frequent class:\")\nprint(confusion_matrix(y_test, pred_most_frequent))\nprint(\"\\nDummy model:\")\nprint(confusion_matrix(y_test, pred_dummy))\nprint(\"\\nDecision tree:\")\nprint(confusion_matrix(y_test, pred_tree))\nprint(\"\\nLogistic Regression\")\nprint(confusion_matrix(y_test, pred_logreg))\nEvaluation Metrics and Scoring \n| \n281\nOut[44]:\nMost frequent class:\n[[403   0]\n [ 47   0]]\nDummy model:\n[[361  42]\n [ 43   4]]\nDecision tree:\n[[390  13]\n [ 24  23]]\nLogistic Regression\n[[401   2]\n [  8  39]]\nLooking at the confusion matrix, it is quite clear that something is wrong with\npred_most_frequent, because it always predicts the same class. pred_dummy, on the\nother hand, has a very small number of true positives (4), particularly compared to\nthe number of false negatives and false positives—there are many more false positives\nthan true positives! The predictions made by the decision tree make much more\nsense than the dummy predictions, even though the accuracy was nearly the same.\nFinally, we can see that logistic regression does better than pred_tree in all aspects: it\nhas more true positives and true negatives while having fewer false positives and false\nnegatives. From this comparison, it is clear that only the decision tree and the logistic\nregression give reasonable results, and that the logistic regression works better than\nthe tree on all accounts. However, inspecting the full confusion matrix is a bit cum‐\nbersome, and while we gained a lot of insight from looking at all aspects of the\nmatrix, the process was very manual and qualitative. There are several ways to sum‐\nmarize the information in the confusion matrix, which we will discuss next.",
    "bersome, and while we gained a lot of insight from looking at all aspects of the\nmatrix, the process was very manual and qualitative. There are several ways to sum‐\nmarize the information in the confusion matrix, which we will discuss next.\nRelation to accuracy.    We already saw one way to summarize the result in the confu‐\nsion matrix—by computing accuracy, which can be expressed as:\nAccuracy =\nTP+TN\nTP+TN + FP + FN\nIn other words, accuracy is the number of correct predictions (TP and TN) divided\nby the number of all samples (all entries of the confusion matrix summed up).\nPrecision, recall, and f-score.    There are several other ways to summarize the confusion\nmatrix, with the most common ones being precision and recall. Precision measures\nhow many of the samples predicted as positive are actually positive:\n282 \n| \nChapter 5: Model Evaluation and Improvement\nPrecision =\nTP\nTP+FP\nPrecision is used as a performance metric when the goal is to limit the number of\nfalse positives. As an example, imagine a model for predicting whether a new drug\nwill be effective in treating a disease in clinical trials. Clinical trials are notoriously\nexpensive, and a pharmaceutical company will only want to run an experiment if it is\nvery sure that the drug will actually work. Therefore, it is important that the model\ndoes not produce many false positives—in other words, that it has a high precision.\nPrecision is also known as positive predictive value (PPV).\nRecall, on the other hand, measures how many of the positive samples are captured\nby the positive predictions:\nRecall =\nTP\nTP+FN\nRecall is used as performance metric when we need to identify all positive samples;\nthat is, when it is important to avoid false negatives. The cancer diagnosis example\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐\nple that are sick, possibly including healthy patients in the prediction. Other names",
    "that is, when it is important to avoid false negatives. The cancer diagnosis example\nfrom earlier in this chapter is a good example for this: it is important to find all peo‐\nple that are sick, possibly including healthy patients in the prediction. Other names\nfor recall are sensitivity, hit rate, or true positive rate (TPR).\nThere is a trade-off between optimizing recall and optimizing precision. You can triv‐\nially obtain a perfect recall if you predict all samples to belong to the positive class—\nthere will be no false negatives, and no true negatives either. However, predicting all\nsamples as positive will result in many false positives, and therefore the precision will\nbe very low. On the other hand, if you find a model that predicts only the single data\npoint it is most sure about as positive and the rest as negative, then precision will be\nperfect (assuming this data point is in fact positive), but recall will be very bad.\nPrecision and recall are only two of many classification measures\nderived from TP, FP, TN, and FN. You can find a great summary of\nall the measures on Wikipedia. In the machine learning commu‐\nnity, precision and recall are arguably the most commonly used\nmeasures for binary classification, but other communities might\nuse other related metrics.\nSo, while precision and recall are very important measures, looking at only one of\nthem will not provide you with the full picture. One way to summarize them is the\nf-score or f-measure, which is with the harmonic mean of precision and recall:\nF = 2 · precision·recall\nprecision+recall\nEvaluation Metrics and Scoring \n| \n283\nThis particular variant is also known as the f1-score. As it takes precision and recall\ninto account, it can be a better measure than accuracy on imbalanced binary classifi‐\ncation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\ncomputed earlier. Here, we will assume that the “nine” class is the positive class (it is",
    "into account, it can be a better measure than accuracy on imbalanced binary classifi‐\ncation datasets. Let’s run it on the predictions for the “nine vs. rest” dataset that we\ncomputed earlier. Here, we will assume that the “nine” class is the positive class (it is\nlabeled as True while the rest is labeled as False), so the positive class is the minority\nclass:\nIn[45]:\nfrom sklearn.metrics import f1_score\nprint(\"f1 score most frequent: {:.2f}\".format(\n        f1_score(y_test, pred_most_frequent)))\nprint(\"f1 score dummy: {:.2f}\".format(f1_score(y_test, pred_dummy)))\nprint(\"f1 score tree: {:.2f}\".format(f1_score(y_test, pred_tree)))\nprint(\"f1 score logistic regression: {:.2f}\".format(\n        f1_score(y_test, pred_logreg)))\nOut[45]:\nf1 score most frequent: 0.00\nf1 score dummy: 0.10\nf1 score tree: 0.55\nf1 score logistic regression: 0.89\nWe can note two things here. First, we get an error message for the most_frequent\nprediction, as there were no predictions of the positive class (which makes the\ndenominator in the f-score zero). Also, we can see a pretty strong distinction between\nthe dummy predictions and the tree predictions, which wasn’t clear when looking at\naccuracy alone. Using the f-score for evaluation, we summarized the predictive per‐\nformance again in one number. However, the f-score seems to capture our intuition\nof what makes a good model much better than accuracy did. A disadvantage of the\nf-score, however, is that it is harder to interpret and explain than accuracy.\nIf we want a more comprehensive summary of precision, recall, and f1-score, we can\nuse the classification_report convenience function to compute all three at once,\nand print them in a nice format:\nIn[46]:\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_most_frequent,\n                            target_names=[\"not nine\", \"nine\"]))\n284 \n| \nChapter 5: Model Evaluation and Improvement\nOut[46]:\n             precision    recall  f1-score   support",
    "In[46]:\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, pred_most_frequent,\n                            target_names=[\"not nine\", \"nine\"]))\n284 \n| \nChapter 5: Model Evaluation and Improvement\nOut[46]:\n             precision    recall  f1-score   support\n   not nine       0.90      1.00      0.94       403\n       nine       0.00      0.00      0.00        47\navg / total       0.80      0.90      0.85       450\nThe classification_report function produces one line per class (here, True and\nFalse) and reports precision, recall, and f-score with this class as the positive class.\nBefore, we assumed the minority “nine” class was the positive class. If we change the\npositive class to “not nine,” we can see from the output of classification_report\nthat we obtain an f-score of 0.94 with the most_frequent model. Furthermore, for the\n“not nine” class we have a recall of 1, as we classified all samples as “not nine.” The\nlast column next to the f-score provides the support of each class, which simply means\nthe number of samples in this class according to the ground truth.\nThe last row in the classification report shows a weighted (by the number of samples\nin the class) average of the numbers for each class. Here are two more reports, one for\nthe dummy classifier and one for the logistic regression:\nIn[47]:\nprint(classification_report(y_test, pred_dummy,\n                            target_names=[\"not nine\", \"nine\"]))\nOut[47]:\n             precision    recall  f1-score   support\n   not nine       0.90      0.92      0.91       403\n       nine       0.11      0.09      0.10        47\navg / total       0.81      0.83      0.82       450\nIn[48]:\nprint(classification_report(y_test, pred_logreg,\n                            target_names=[\"not nine\", \"nine\"]))\nOut[48]:\n             precision    recall  f1-score   support\n   not nine       0.98      1.00      0.99       403\n       nine       0.95      0.83      0.89        47",
    "In[48]:\nprint(classification_report(y_test, pred_logreg,\n                            target_names=[\"not nine\", \"nine\"]))\nOut[48]:\n             precision    recall  f1-score   support\n   not nine       0.98      1.00      0.99       403\n       nine       0.95      0.83      0.89        47\navg / total       0.98      0.98      0.98       450\nEvaluation Metrics and Scoring \n| \n285\nAs you may notice when looking at the reports, the differences between the dummy\nmodels and a very good model are not as clear any more. Picking which class is\ndeclared the positive class has a big impact on the metrics. While the f-score for the\ndummy classification is 0.13 (vs. 0.89 for the logistic regression) on the “nine” class,\nfor the “not nine” class it is 0.90 vs. 0.99, which both seem like reasonable results.\nLooking at all the numbers together paints a pretty accurate picture, though, and we\ncan clearly see the superiority of the logistic regression model.\nTaking uncertainty into account\nThe confusion matrix and the classification report provide a very detailed analysis of\na particular set of predictions. However, the predictions themselves already threw\naway a lot of information that is contained in the model. As we discussed in Chap‐\nter 2, most classifiers provide a decision_function or a predict_proba method to\nassess degrees of certainty about predictions. Making predictions can be seen as\nthresholding the output of decision_function or predict_proba at a certain fixed\npoint—in binary classification we use 0 for the decision function and 0.5 for\npredict_proba.\nThe following is an example of an imbalanced binary classification task, with 400\npoints in the negative class classified against 50 points in the positive class. The train‐\ning data is shown on the left in Figure 5-12. We train a kernel SVM model on this\ndata, and the plots to the right of the training data illustrate the values of the decision",
    "points in the negative class classified against 50 points in the positive class. The train‐\ning data is shown on the left in Figure 5-12. We train a kernel SVM model on this\ndata, and the plots to the right of the training data illustrate the values of the decision\nfunction as a heat map. You can see a black circle in the plot in the top center, which\ndenotes the threshold of the decision_function being exactly zero. Points inside this\ncircle will be classified as the positive class, and points outside as the negative class:\nIn[49]:\nfrom mglearn.datasets import make_blobs\nX, y = make_blobs(n_samples=(400, 50), centers=2, cluster_std=[7.0, 2],\n                  random_state=22)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nsvc = SVC(gamma=.05).fit(X_train, y_train)\nIn[50]:\nmglearn.plots.plot_decision_threshold()\n286 \n| \nChapter 5: Model Evaluation and Improvement\nFigure 5-12. Heatmap of the decision function and the impact of changing the decision\nthreshold\nWe can use the classification_report function to evaluate precision and recall for\nboth classes:\nIn[51]:\nprint(classification_report(y_test, svc.predict(X_test)))\nOut[51]:\n             precision    recall  f1-score   support\n          0       0.97      0.89      0.93       104\n          1       0.35      0.67      0.46         9\navg / total       0.92      0.88      0.89       113\nFor class 1, we get a fairly small recall, and precision is mixed. Because class 0 is so\nmuch larger, the classifier focuses on getting class 0 right, and not the smaller class 1.\nLet’s assume in our application it is more important to have a high recall for class 1, as\nin the cancer screening example earlier. This means we are willing to risk more false\npositives (false class 1) in exchange for more true positives (which will increase the\nrecall). The predictions generated by svc.predict really do not fulfill this require‐\nment, but we can adjust the predictions to focus on a higher recall of class 1 by",
    "positives (false class 1) in exchange for more true positives (which will increase the\nrecall). The predictions generated by svc.predict really do not fulfill this require‐\nment, but we can adjust the predictions to focus on a higher recall of class 1 by\nchanging the decision threshold away from 0. By default, points with a deci\nsion_function value greater than 0 will be classified as class 1. We want more points\nto be classified as class 1, so we need to decrease the threshold:\nEvaluation Metrics and Scoring \n| \n287\nIn[52]:\ny_pred_lower_threshold = svc.decision_function(X_test) > -.8\nLet’s look at the classification report for this prediction:\nIn[53]:\nprint(classification_report(y_test, y_pred_lower_threshold))\nOut[53]:\n             precision    recall  f1-score   support\n          0       1.00      0.82      0.90       104\n          1       0.32      1.00      0.49         9\navg / total       0.95      0.83      0.87       113\nAs expected, the recall of class 1 went up, and the precision went down. We are now\nclassifying a larger region of space as class 1, as illustrated in the top-right panel of\nFigure 5-12. If you value precision over recall or the other way around, or your data is\nheavily imbalanced, changing the decision threshold is the easiest way to obtain bet‐\nter results. As the decision_function can have arbitrary ranges, it is hard to provide\na rule of thumb regarding how to pick a threshold.\nIf you do set a threshold, you need to be careful not to do so using\nthe test set. As with any other parameter, setting a decision thresh‐\nold on the test set is likely to yield overly optimistic results. Use a\nvalidation set or cross-validation instead.\nPicking a threshold for models that implement the predict_proba method can be\neasier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\nities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”",
    "validation set or cross-validation instead.\nPicking a threshold for models that implement the predict_proba method can be\neasier, as the output of predict_proba is on a fixed 0 to 1 scale, and models probabil‐\nities. By default, the threshold of 0.5 means that if the model is more than 50% “sure”\nthat a point is of the positive class, it will be classified as such. Increasing the thresh‐\nold means that the model needs to be more confident to make a positive decision\n(and less confident to make a negative decision). While working with probabilities\nmay be more intuitive than working with arbitrary thresholds, not all models provide\nrealistic models of uncertainty (a DecisionTree that is grown to its full depth is\nalways 100% sure of its decisions, even though it might often be wrong). This relates\nto the concept of calibration: a calibrated model is a model that provides an accurate\nmeasure of its uncertainty. Discussing calibration in detail is beyond the scope of this\nbook, but you can find more details in the paper “Predicting Good Probabilities with\nSupervised Learning” by Alexandru Niculescu-Mizil and Rich Caruana.\n288 \n| \nChapter 5: Model Evaluation and Improvement\nPrecision-recall curves and ROC curves\nAs we just discussed, changing the threshold that is used to make a classification deci‐\nsion in a model is a way to adjust the trade-off of precision and recall for a given clas‐\nsifier. Maybe you want to miss less than 10% of positive samples, meaning a desired\nrecall of 90%. This decision depends on the application, and it should be driven by\nbusiness goals. Once a particular goal is set—say, a particular recall or precision value\nfor a class—a threshold can be set appropriately. It is always possible to set a thresh‐\nold to fulfill a particular target, like 90% recall. The hard part is to develop a model\nthat still has reasonable precision with this threshold—if you classify everything as\npositive, you will have 100% recall, but your model will be useless.",
    "old to fulfill a particular target, like 90% recall. The hard part is to develop a model\nthat still has reasonable precision with this threshold—if you classify everything as\npositive, you will have 100% recall, but your model will be useless.\nSetting a requirement on a classifier like 90% recall is often called setting the operat‐\ning point. Fixing an operating point is often helpful in business settings to make per‐\nformance guarantees to customers or other groups inside your organization.\nOften, when developing a new model, it is not entirely clear what the operating point\nwill be. For this reason, and to understand a modeling problem better, it is instructive\nto look at all possible thresholds, or all possible trade-offs of precision and recalls at\nonce. This is possible using a tool called the precision-recall curve. You can find the\nfunction to compute the precision-recall curve in the sklearn.metrics module. It\nneeds the ground truth labeling and predicted uncertainties, created via either\ndecision_function or predict_proba:\nIn[54]:\nfrom sklearn.metrics import precision_recall_curve\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, svc.decision_function(X_test))\nThe precision_recall_curve function returns a list of precision and recall values\nfor all possible thresholds (all values that appear in the decision function) in sorted\norder, so we can plot a curve, as seen in Figure 5-13:\nIn[55]:\n# Use more data points for a smoother curve\nX, y = make_blobs(n_samples=(4000, 500), centers=2, cluster_std=[7.0, 2],\n                  random_state=22)\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\nsvc = SVC(gamma=.05).fit(X_train, y_train)\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, svc.decision_function(X_test))\n# find threshold closest to zero\nclose_zero = np.argmin(np.abs(thresholds))\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,",
    "svc = SVC(gamma=.05).fit(X_train, y_train)\nprecision, recall, thresholds = precision_recall_curve(\n    y_test, svc.decision_function(X_test))\n# find threshold closest to zero\nclose_zero = np.argmin(np.abs(thresholds))\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\nplt.plot(precision, recall, label=\"precision recall curve\")\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nEvaluation Metrics and Scoring \n| \n289\nFigure 5-13. Precision recall curve for SVC(gamma=0.05)\nEach point along the curve in Figure 5-13 corresponds to a possible threshold of the\ndecision_function. We can see, for example, that we can achieve a recall of 0.4 at a\nprecision of about 0.75. The black circle marks the point that corresponds to a thresh‐\nold of 0, the default threshold for decision_function. This point is the trade-off that\nis chosen when calling the predict method.\nThe closer a curve stays to the upper-right corner, the better the classifier. A point at\nthe upper right means high precision and high recall for the same threshold. The\ncurve starts at the top-left corner, corresponding to a very low threshold, classifying\neverything as the positive class. Raising the threshold moves the curve toward higher\nprecision, but also lower recall. Raising the threshold more and more, we get to a sit‐\nuation where most of the points classified as being positive are true positives, leading\nto a very high precision but lower recall. The more the model keeps recall high as\nprecision goes up, the better.\nLooking at this particular curve a bit more, we can see that with this model it is possi‐\nble to get a precision of up to around 0.5 with very high recall. If we want a much\nhigher precision, we have to sacrifice a lot of recall. In other words, on the left the\ncurve is relatively flat, meaning that recall does not go down a lot when we require",
    "ble to get a precision of up to around 0.5 with very high recall. If we want a much\nhigher precision, we have to sacrifice a lot of recall. In other words, on the left the\ncurve is relatively flat, meaning that recall does not go down a lot when we require\nincreased precision. For precision greater than 0.5, each gain in precision costs us a\nlot of recall.\nDifferent classifiers can work well in different parts of the curve—that is, at different\noperating points. Let’s compare the SVM we trained to a random forest trained on the\nsame dataset. The RandomForestClassifier doesn’t have a decision_function, only\npredict_proba. The precision_recall_curve function expects as its second argu‐\nment a certainty measure for the positive class (class 1), so we pass the probability of\na sample being class 1—that is, rf.predict_proba(X_test)[:, 1]. The default\nthreshold for predict_proba in binary classification is 0.5, so this is the point we\nmarked on the curve (see Figure 5-14):\n290 \n| \nChapter 5: Model Evaluation and Improvement\nIn[56]:\nfrom sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=100, random_state=0, max_features=2)\nrf.fit(X_train, y_train)\n# RandomForestClassifier has predict_proba, but not decision_function\nprecision_rf, recall_rf, thresholds_rf = precision_recall_curve(\n    y_test, rf.predict_proba(X_test)[:, 1])\nplt.plot(precision, recall, label=\"svc\")\nplt.plot(precision[close_zero], recall[close_zero], 'o', markersize=10,\n         label=\"threshold zero svc\", fillstyle=\"none\", c='k', mew=2)\nplt.plot(precision_rf, recall_rf, label=\"rf\")\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k',\n         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.legend(loc=\"best\")\nFigure 5-14. Comparing precision recall curves of SVM and random forest",
    "plt.plot(precision_rf[close_default_rf], recall_rf[close_default_rf], '^', c='k',\n         markersize=10, label=\"threshold 0.5 rf\", fillstyle=\"none\", mew=2)\nplt.xlabel(\"Precision\")\nplt.ylabel(\"Recall\")\nplt.legend(loc=\"best\")\nFigure 5-14. Comparing precision recall curves of SVM and random forest\nFrom the comparison plot we can see that the random forest performs better at the\nextremes, for very high recall or very high precision requirements. Around the mid‐\ndle (approximately precision=0.7), the SVM performs better. If we only looked at the\nf1-score to compare overall performance, we would have missed these subtleties. The\nf1-score only captures one point on the precision-recall curve, the one given by the\ndefault threshold:\nEvaluation Metrics and Scoring \n| \n291\n4 There are some minor technical differences between the area under the precision-recall curve and average\nprecision. However, this explanation conveys the general idea.\nIn[57]:\nprint(\"f1_score of random forest: {:.3f}\".format(\n    f1_score(y_test, rf.predict(X_test))))\nprint(\"f1_score of svc: {:.3f}\".format(f1_score(y_test, svc.predict(X_test))))\nOut[57]:\nf1_score of random forest: 0.610\nf1_score of svc: 0.656\nComparing two precision-recall curves provides a lot of detailed insight, but is a fairly\nmanual process. For automatic model comparison, we might want to summarize the\ninformation contained in the curve, without limiting ourselves to a particular thresh‐\nold or operating point. One particular way to summarize the precision-recall curve is\nby computing the integral or area under the curve of the precision-recall curve, also\nknown as the average precision.4 You can use the average_precision_score function\nto compute the average precision. Because we need to compute the ROC curve and\nconsider multiple thresholds, the result of decision_function or predict_proba\nneeds to be passed to average_precision_score, not the result of predict:\nIn[58]:\nfrom sklearn.metrics import average_precision_score",
    "to compute the average precision. Because we need to compute the ROC curve and\nconsider multiple thresholds, the result of decision_function or predict_proba\nneeds to be passed to average_precision_score, not the result of predict:\nIn[58]:\nfrom sklearn.metrics import average_precision_score\nap_rf = average_precision_score(y_test, rf.predict_proba(X_test)[:, 1])\nap_svc = average_precision_score(y_test, svc.decision_function(X_test))\nprint(\"Average precision of random forest: {:.3f}\".format(ap_rf))\nprint(\"Average precision of svc: {:.3f}\".format(ap_svc))\nOut[58]:\nAverage precision of random forest: 0.666\nAverage precision of svc: 0.663\nWhen averaging over all possible thresholds, we see that the random forest and SVC\nperform similarly well, with the random forest even slightly ahead. This is quite dif‐\nferent from the result we got from f1_score earlier. Because average precision is the\narea under a curve that goes from 0 to 1, average precision always returns a value\nbetween 0 (worst) and 1 (best). The average precision of a classifier that assigns\ndecision_function at random is the fraction of positive samples in the dataset.\nReceiver operating characteristics (ROC) and AUC\nThere is another tool that is commonly used to analyze the behavior of classifiers at\ndifferent thresholds: the receiver operating characteristics curve, or ROC curve for\nshort. Similar to the precision-recall curve, the ROC curve considers all possible\n292 \n| \nChapter 5: Model Evaluation and Improvement\nthresholds for a given classifier, but instead of reporting precision and recall, it shows\nthe false positive rate (FPR) against the true positive rate (TPR). Recall that the true\npositive rate is simply another name for recall, while the false positive rate is the frac‐\ntion of false positives out of all negative samples:\nFPR =\nFP\nFP+TN\nThe ROC curve can be computed using the roc_curve function (see Figure 5-15):\nIn[59]:\nfrom sklearn.metrics import roc_curve",
    "positive rate is simply another name for recall, while the false positive rate is the frac‐\ntion of false positives out of all negative samples:\nFPR =\nFP\nFP+TN\nThe ROC curve can be computed using the roc_curve function (see Figure 5-15):\nIn[59]:\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, svc.decision_function(X_test))\nplt.plot(fpr, tpr, label=\"ROC Curve\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (recall)\")\n# find threshold closest to zero\nclose_zero = np.argmin(np.abs(thresholds))\nplt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n         label=\"threshold zero\", fillstyle=\"none\", c='k', mew=2)\nplt.legend(loc=4)\nFigure 5-15. ROC curve for SVM\nFor the ROC curve, the ideal curve is close to the top left: you want a classifier that\nproduces a high recall while keeping a low false positive rate. Compared to the default\nthreshold of 0, the curve shows that we can achieve a significantly higher recall\n(around 0.9) while only increasing the FPR slightly. The point closest to the top left\nmight be a better operating point than the one chosen by default. Again, be aware that\nchoosing a threshold should not be done on the test set, but on a separate validation\nset.\nEvaluation Metrics and Scoring \n| \n293\nYou can find a comparison of the random forest and the SVM using ROC curves in\nFigure 5-16:\nIn[60]:\nfrom sklearn.metrics import roc_curve\nfpr_rf, tpr_rf, thresholds_rf = roc_curve(y_test, rf.predict_proba(X_test)[:, 1])\nplt.plot(fpr, tpr, label=\"ROC Curve SVC\")\nplt.plot(fpr_rf, tpr_rf, label=\"ROC Curve RF\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR (recall)\")\nplt.plot(fpr[close_zero], tpr[close_zero], 'o', markersize=10,\n         label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\nplt.legend(loc=4)",
    "label=\"threshold zero SVC\", fillstyle=\"none\", c='k', mew=2)\nclose_default_rf = np.argmin(np.abs(thresholds_rf - 0.5))\nplt.plot(fpr_rf[close_default_rf], tpr[close_default_rf], '^', markersize=10,\n         label=\"threshold 0.5 RF\", fillstyle=\"none\", c='k', mew=2)\nplt.legend(loc=4)\nFigure 5-16. Comparing ROC curves for SVM and random forest\nAs for the precision-recall curve, we often want to summarize the ROC curve using a\nsingle number, the area under the curve (this is commonly just referred to as the\nAUC, and it is understood that the curve in question is the ROC curve). We can com‐\npute the area under the ROC curve using the roc_auc_score function:\n294 \n| \nChapter 5: Model Evaluation and Improvement\nIn[61]:\nfrom sklearn.metrics import roc_auc_score\nrf_auc = roc_auc_score(y_test, rf.predict_proba(X_test)[:, 1])\nsvc_auc = roc_auc_score(y_test, svc.decision_function(X_test))\nprint(\"AUC for Random Forest: {:.3f}\".format(rf_auc))\nprint(\"AUC for SVC: {:.3f}\".format(svc_auc))\nOut[61]:\nAUC for Random Forest: 0.937\nAUC for SVC: 0.916\nComparing the random forest and SVM using the AUC score, we find that the ran‐\ndom forest performs quite a bit better than the SVM. Recall that because average pre‐\ncision is the area under a curve that goes from 0 to 1, average precision always returns\na value between 0 (worst) and 1 (best). Predicting randomly always produces an AUC\nof 0.5, no matter how imbalanced the classes in a dataset are. This makes AUC a\nmuch better metric for imbalanced classification problems than accuracy. The AUC\ncan be interpreted as evaluating the ranking of positive samples. It’s equivalent to the\nprobability that a randomly picked point of the positive class will have a higher score\naccording to the classifier than a randomly picked point from the negative class. So, a\nperfect AUC of 1 means that all positive points have a higher score than all negative\npoints. For classification problems with imbalanced classes, using AUC for model",
    "according to the classifier than a randomly picked point from the negative class. So, a\nperfect AUC of 1 means that all positive points have a higher score than all negative\npoints. For classification problems with imbalanced classes, using AUC for model\nselection is often much more meaningful than using accuracy.\nLet’s go back to the problem we studied earlier of classifying all nines in the digits\ndataset versus all other digits. We will classify the dataset with an SVM with three dif‐\nferent settings of the kernel bandwidth, gamma (see Figure 5-17):\nIn[62]:\ny = digits.target == 9\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, y, random_state=0)\nplt.figure()\nfor gamma in [1, 0.05, 0.01]:\n    svc = SVC(gamma=gamma).fit(X_train, y_train)\n    accuracy = svc.score(X_test, y_test)\n    auc = roc_auc_score(y_test, svc.decision_function(X_test))\n    fpr, tpr, _ = roc_curve(y_test , svc.decision_function(X_test))\n    print(\"gamma = {:.2f}  accuracy = {:.2f}  AUC = {:.2f}\".format(\n    gamma, accuracy, auc))\n    plt.plot(fpr, tpr, label=\"gamma={:.3f}\".format(gamma))\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.xlim(-0.01, 1)\nplt.ylim(0, 1.02)\nplt.legend(loc=\"best\")\nEvaluation Metrics and Scoring \n| \n295\n5 Looking at the curve for gamma=0.01 in detail, you can see a small kink close to the top left. That means that at\nleast one point was not ranked correctly. The AUC of 1.0 is a consequence of rounding to the second decimal\npoint.\nOut[62]:\ngamma = 1.00  accuracy = 0.90  AUC = 0.50\ngamma = 0.05  accuracy = 0.90  AUC = 0.90\ngamma = 0.01  accuracy = 0.90  AUC = 1.00\nFigure 5-17. Comparing ROC curves of SVMs with different settings of gamma\nThe accuracy of all three settings of gamma is the same, 90%. This might be the same\nas chance performance, or it might not. Looking at the AUC and the corresponding\ncurve, however, we see a clear distinction between the three models. With gamma=1.0,",
    "The accuracy of all three settings of gamma is the same, 90%. This might be the same\nas chance performance, or it might not. Looking at the AUC and the corresponding\ncurve, however, we see a clear distinction between the three models. With gamma=1.0,\nthe AUC is actually at chance level, meaning that the output of the decision_func\ntion is as good as random. With gamma=0.05, performance drastically improves to an\nAUC of 0.5. Finally, with gamma=0.01, we get a perfect AUC of 1.0. That means that\nall positive points are ranked higher than all negative points according to the decision\nfunction. In other words, with the right threshold, this model can classify the data\nperfectly!5 Knowing this, we can adjust the threshold on this model and obtain great\npredictions. If we had only used accuracy, we would never have discovered this.\nFor this reason, we highly recommend using AUC when evaluating models on imbal‐\nanced data. Keep in mind that AUC does not make use of the default threshold,\nthough, so adjusting the decision threshold might be necessary to obtain useful classi‐\nfication results from a model with a high AUC.\nMetrics for Multiclass Classification\nNow that we have discussed evaluation of binary classification tasks in depth, let’s\nmove on to metrics to evaluate multiclass classification. Basically, all metrics for\nmulticlass classification are derived from binary classification metrics, but averaged\n296 \n| \nChapter 5: Model Evaluation and Improvement\nover all classes. Accuracy for multiclass classification is again defined as the fraction\nof correctly classified examples. And again, when classes are imbalanced, accuracy is\nnot a great evaluation measure. Imagine a three-class classification problem with 85%\nof points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from",
    "of points belonging to class A, 10% belonging to class B, and 5% belonging to class C.\nWhat does being 85% accurate mean on this dataset? In general, multiclass classifica‐\ntion results are harder to understand than binary classification results. Apart from\naccuracy, common tools are the confusion matrix and the classification report we saw\nin the binary case in the previous section. Let’s apply these two detailed evaluation\nmethods on the task of classifying the 10 different handwritten digits in the digits\ndataset:\nIn[63]:\nfrom sklearn.metrics import accuracy_score\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target, random_state=0)\nlr = LogisticRegression().fit(X_train, y_train)\npred = lr.predict(X_test)\nprint(\"Accuracy: {:.3f}\".format(accuracy_score(y_test, pred)))\nprint(\"Confusion matrix:\\n{}\".format(confusion_matrix(y_test, pred)))\nOut[63]:\nAccuracy: 0.953\nConfusion matrix:\n[[37  0  0  0  0  0  0  0  0  0]\n [ 0 39  0  0  0  0  2  0  2  0]\n [ 0  0 41  3  0  0  0  0  0  0]\n [ 0  0  1 43  0  0  0  0  0  1]\n [ 0  0  0  0 38  0  0  0  0  0]\n [ 0  1  0  0  0 47  0  0  0  0]\n [ 0  0  0  0  0  0 52  0  0  0]\n [ 0  1  0  1  1  0  0 45  0  0]\n [ 0  3  1  0  0  0  0  0 43  1]\n [ 0  0  0  1  0  1  0  0  1 44]]\nThe model has an accuracy of 95.3%, which already tells us that we are doing pretty\nwell. The confusion matrix provides us with some more detail. As for the binary case,\neach row corresponds to a true label, and each column corresponds to a predicted\nlabel. You can find a visually more appealing plot in Figure 5-18:\nIn[64]:\nscores_image = mglearn.tools.heatmap(\n    confusion_matrix(y_test, pred), xlabel='Predicted label',\n    ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task",
    "ylabel='True label', xticklabels=digits.target_names,\n    yticklabels=digits.target_names, cmap=plt.cm.gray_r, fmt=\"%d\")\nplt.title(\"Confusion matrix\")\nplt.gca().invert_yaxis()\nEvaluation Metrics and Scoring \n| \n297\nFigure 5-18. Confusion matrix for the 10-digit classification task\nFor the first class, the digit 0, there are 37 samples in the class, and all of these sam‐\nples were classified as class 0 (there are no false negatives for class 0). We can see that\nbecause all other entries in the first row of the confusion matrix are 0. We can also see\nthat no other digits were mistakenly classified as 0, because all other entries in the\nfirst column of the confusion matrix are 0 (there are no false positives for class 0).\nSome digits were confused with others, though—for example, the digit 2 (third row),\nthree of which were classified as the digit 3 (fourth column). There was also one digit\n3 that was classified as 2 (third column, fourth row) and one digit 8 that was classified\nas 2 (thrid column, fourth row).\nWith the classification_report function, we can compute the precision, recall,\nand f-score for each class:\nIn[65]:\nprint(classification_report(y_test, pred))\nOut[65]:\n             precision    recall  f1-score   support\n          0       1.00      1.00      1.00        37\n          1       0.89      0.91      0.90        43\n          2       0.95      0.93      0.94        44\n          3       0.90      0.96      0.92        45\n          4       0.97      1.00      0.99        38\n          5       0.98      0.98      0.98        48\n          6       0.96      1.00      0.98        52\n          7       1.00      0.94      0.97        48\n          8       0.93      0.90      0.91        48\n          9       0.96      0.94      0.95        47\navg / total       0.95      0.95      0.95       450\n298 \n| \nChapter 5: Model Evaluation and Improvement\nUnsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐",
    "8       0.93      0.90      0.91        48\n          9       0.96      0.94      0.95        47\navg / total       0.95      0.95      0.95       450\n298 \n| \nChapter 5: Model Evaluation and Improvement\nUnsurprisingly, precision and recall are a perfect 1 for class 0, as there are no confu‐\nsions with this class. For class 7, on the other hand, precision is 1 because no other\nclass was mistakenly classified as 7, while for class 6, there are no false negatives, so\nthe recall is 1. We can also see that the model has particular difficulties with classes 8\nand 3.\nThe most commonly used metric for imbalanced datasets in the multiclass setting is\nthe multiclass version of the f-score. The idea behind the multiclass f-score is to com‐\npute one binary f-score per class, with that class being the positive class and the other\nclasses making up the negative classes. Then, these per-class f-scores are averaged\nusing one of the following strategies:\n• \"macro\" averaging computes the unweighted per-class f-scores. This gives equal\nweight to all classes, no matter what their size is.\n• \"weighted\" averaging computes the mean of the per-class f-scores, weighted by\ntheir support. This is what is reported in the classification report.\n• \"micro\" averaging computes the total number of false positives, false negatives,\nand true positives over all classes, and then computes precision, recall, and f-\nscore using these counts.\nIf you care about each sample equally much, it is recommended to use the \"micro\"\naverage f1-score; if you care about each class equally much, it is recommended to use\nthe \"macro\" average f1-score:\nIn[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics",
    "In[66]:\nprint(\"Micro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"micro\")))\nprint(\"Macro average f1 score: {:.3f}\".format\n       (f1_score(y_test, pred, average=\"macro\")))\nOut[66]:\nMicro average f1 score: 0.953\nMacro average f1 score: 0.954\nRegression Metrics\nEvaluation for regression can be done in similar detail as we did for classification—\nfor example, by analyzing overpredicting the target versus underpredicting the target.\nHowever, in most applications we’ve seen, using the default R2 used in the score\nmethod of all regressors is enough. Sometimes business decisions are made on the\nbasis of mean squared error or mean absolute error, which might give incentive to\ntune models using these metrics. In general, though, we have found R2 to be a more\nintuitive metric to evaluate regression models.\nEvaluation Metrics and Scoring \n| \n299\nUsing Evaluation Metrics in Model Selection\nWe have discussed many evaluation methods in detail, and how to apply them given\nthe ground truth and a model. However, we often want to use metrics like AUC in\nmodel selection using GridSearchCV or cross_val_score. Luckily scikit-learn\nprovides a very simple way to achieve this, via the scoring argument that can be used\nin both GridSearchCV and cross_val_score. You can simply provide a string\ndescribing the evaluation metric you want to use. Say, for example, we want to evalu‐\nate the SVM classifier on the “nine vs. rest” task on the digits dataset, using the AUC\nscore. Changing the score from the default (accuracy) to AUC can be done by provid‐\ning \"roc_auc\" as the scoring parameter:\nIn[67]:\n# default scoring for classification is accuracy\nprint(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")",
    "print(\"Default scoring: {}\".format(\n    cross_val_score(SVC(), digits.data, digits.target == 9)))\n# providing scoring=\"accuracy\" doesn't change the results\nexplicit_accuracy =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                                     scoring=\"accuracy\")\nprint(\"Explicit accuracy scoring: {}\".format(explicit_accuracy))\nroc_auc =  cross_val_score(SVC(), digits.data, digits.target == 9,\n                           scoring=\"roc_auc\")\nprint(\"AUC scoring: {}\".format(roc_auc))\nOut[67]:\nDefault scoring: [ 0.9  0.9  0.9]\nExplicit accuracy scoring: [ 0.9  0.9  0.9]\nAUC scoring: [ 0.994  0.99   0.996]\nSimilarly, we can change the metric used to pick the best parameters in Grid\nSearchCV:\nIn[68]:\nX_train, X_test, y_train, y_test = train_test_split(\n    digits.data, digits.target == 9, random_state=0)\n# we provide a somewhat bad grid to illustrate the point:\nparam_grid = {'gamma': [0.0001, 0.01, 0.1, 1, 10]}\n# using the default scoring of accuracy:\ngrid = GridSearchCV(SVC(), param_grid=param_grid)\ngrid.fit(X_train, y_train)\nprint(\"Grid-Search with accuracy\")\nprint(\"Best parameters:\", grid.best_params_)\nprint(\"Best cross-validation score (accuracy)): {:.3f}\".format(grid.best_score_))\nprint(\"Test set AUC: {:.3f}\".format(\n    roc_auc_score(y_test, grid.decision_function(X_test))))\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\n300 \n| \nChapter 5: Model Evaluation and Improvement\n6 Finding a higher-accuracy solution using AUC is likely a consequence of accuracy being a bad measure of\nmodel performance on imbalanced data.\nOut[68]:\nGrid-Search with accuracy\nBest parameters: {'gamma': 0.0001}\nBest cross-validation score (accuracy)): 0.970\nTest set AUC: 0.992\nTest set accuracy: 0.973\nIn[69]:\n# using AUC scoring instead:\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\ngrid.fit(X_train, y_train)\nprint(\"\\nGrid-Search with AUC\")\nprint(\"Best parameters:\", grid.best_params_)",
    "Test set AUC: 0.992\nTest set accuracy: 0.973\nIn[69]:\n# using AUC scoring instead:\ngrid = GridSearchCV(SVC(), param_grid=param_grid, scoring=\"roc_auc\")\ngrid.fit(X_train, y_train)\nprint(\"\\nGrid-Search with AUC\")\nprint(\"Best parameters:\", grid.best_params_)\nprint(\"Best cross-validation score (AUC): {:.3f}\".format(grid.best_score_))\nprint(\"Test set AUC: {:.3f}\".format(\n    roc_auc_score(y_test, grid.decision_function(X_test))))\nprint(\"Test set accuracy: {:.3f}\".format(grid.score(X_test, y_test)))\nOut[69]:\nGrid-Search with AUC\nBest parameters: {'gamma': 0.01}\nBest cross-validation score (AUC): 0.997\nTest set AUC: 1.000\nTest set accuracy: 1.000\nWhen using accuracy, the parameter gamma=0.0001 is selected, while gamma=0.01 is\nselected when using AUC. The cross-validation accuracy is consistent with the test set\naccuracy in both cases. However, using AUC found a better parameter setting in\nterms of AUC and even in terms of accuracy.6\nThe most important values for the scoring parameter for classification are accuracy\n(the default); roc_auc for the area under the ROC curve; average_precision for the\narea under the precision-recall curve; f1, f1_macro, f1_micro, and f1_weighted for\nthe binary f1-score and the different weighted variants. For regression, the most com‐\nmonly used values are r2 for the R2 score, mean_squared_error for mean squared\nerror, and mean_absolute_error for mean absolute error. You can find a full list of\nsupported arguments in the documentation or by looking at the SCORER dictionary\ndefined in the metrics.scorer module:\nEvaluation Metrics and Scoring \n| \n301\n7 We highly recommend Foster Provost and Tom Fawcett’s book Data Science for Business (O’Reilly) for more\ninformation on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',",
    "information on this topic.\nIn[70]:\nfrom sklearn.metrics.scorer import SCORERS\nprint(\"Available scorers:\\n{}\".format(sorted(SCORERS.keys())))\nOut[70]:\nAvailable scorers:\n['accuracy', 'adjusted_rand_score', 'average_precision', 'f1', 'f1_macro',\n 'f1_micro', 'f1_samples', 'f1_weighted', 'log_loss', 'mean_absolute_error',\n 'mean_squared_error', 'median_absolute_error', 'precision', 'precision_macro',\n 'precision_micro', 'precision_samples', 'precision_weighted', 'r2', 'recall',\n 'recall_macro', 'recall_micro', 'recall_samples', 'recall_weighted', 'roc_auc']\nSummary and Outlook\nIn this chapter we discussed cross-validation, grid search, and evaluation metrics, the\ncornerstones of evaluating and improving machine learning algorithms. The tools\ndescribed in this chapter, together with the algorithms described in Chapters 2 and 3,\nare the bread and butter of every machine learning practitioner.\nThere are two particular points that we made in this chapter that warrant repeating,\nbecause they are often overlooked by new practitioners. The first has to do with\ncross-validation. Cross-validation or the use of a test set allow us to evaluate a\nmachine learning model as it will perform in the future. However, if we use the test\nset or cross-validation to select a model or select model parameters, we “use up” the\ntest data, and using the same data to evaluate how well our model will do in the future\nwill lead to overly optimistic estimates. We therefore need to resort to a split into\ntraining data for model building, validation data for model and parameter selection,\nand test data for model evaluation. Instead of a simple split, we can replace each of\nthese splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring",
    "these splits with cross-validation. The most commonly used form (as described ear‐\nlier) is a training/test split for evaluation, and using cross-validation on the training\nset for model and parameter selection.\nThe second point has to do with the importance of the evaluation metric or scoring\nfunction used for model selection and model evaluation. The theory of how to make\nbusiness decisions from the predictions of a machine learning model is somewhat\nbeyond the scope of this book.7 However, it is rarely the case that the end goal of a\nmachine learning task is building a model with a high accuracy. Make sure that the\nmetric you choose to evaluate and select a model for is a good stand-in for what the\nmodel will actually be used for. In reality, classification problems rarely have balanced\nclasses, and often false positives and false negatives have very different consequences.\n302 \n| \nChapter 5: Model Evaluation and Improvement\nMake sure you understand what these consequences are, and pick an evaluation met‐\nric accordingly.\nThe model evaluation and selection techniques we have described so far are the most\nimportant tools in a data scientist’s toolbox. Grid search and cross-validation as we’ve\ndescribed them in this chapter can only be applied to a single supervised model. We\nhave seen before, however, that many models require preprocessing, and that in some\napplications, like the face recognition example in Chapter 3, extracting a different\nrepresentation of the data can be useful. In the next chapter, we will introduce the\nPipeline class, which allows us to use grid search and cross-validation on these com‐\nplex chains of algorithms.\nSummary and Outlook \n| \n303\nCHAPTER 6\nAlgorithm Chains and Pipelines\nFor many machine learning algorithms, the particular representation of the data that\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\nthe data and combining features by hand and goes all the way to learning features",
    "303\nCHAPTER 6\nAlgorithm Chains and Pipelines\nFor many machine learning algorithms, the particular representation of the data that\nyou provide is very important, as we discussed in Chapter 4. This starts with scaling\nthe data and combining features by hand and goes all the way to learning features\nusing unsupervised machine learning, as we saw in Chapter 3. Consequently, most\nmachine learning applications require not only the application of a single algorithm,\nbut the chaining together of many different processing steps and machine learning\nmodels. In this chapter, we will cover how to use the Pipeline class to simplify the\nprocess of building chains of transformations and models. In particular, we will see\nhow we can combine Pipeline and GridSearchCV to search over parameters for all\nprocessing steps at once.\nAs an example of the importance of chaining models, we noticed that we can greatly\nimprove the performance of a kernel SVM on the cancer dataset by using the Min\nMaxScaler for preprocessing. Here’s code for splitting the data, computing the mini‐\nmum and maximum, scaling the data, and training the SVM:\nIn[1]:\nfrom sklearn.svm import SVC\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n# load and split the data\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\n# compute minimum and maximum on the training data\nscaler = MinMaxScaler().fit(X_train)\n305\nIn[2]:\n# rescale the training data\nX_train_scaled = scaler.transform(X_train)\nsvm = SVC()\n# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing",
    "# learn an SVM on the scaled training data\nsvm.fit(X_train_scaled, y_train)\n# scale the test data and score the scaled data\nX_test_scaled = scaler.transform(X_test)\nprint(\"Test score: {:.2f}\".format(svm.score(X_test_scaled, y_test)))\nOut[2]:\nTest score: 0.95\nParameter Selection with Preprocessing\nNow let’s say we want to find better parameters for SVC using GridSearchCV, as dis‐\ncussed in Chapter 5. How should we go about doing this? A naive approach might\nlook like this:\nIn[3]:\nfrom sklearn.model_selection import GridSearchCV\n# for illustration purposes only, don't use this code!\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\ngrid = GridSearchCV(SVC(), param_grid=param_grid, cv=5)\ngrid.fit(X_train_scaled, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Best set score: {:.2f}\".format(grid.score(X_test_scaled, y_test)))\nprint(\"Best parameters: \", grid.best_params_)\nOut[3]:\nBest cross-validation accuracy: 0.98\nBest set score: 0.97\nBest parameters:  {'gamma': 1, 'C': 1}\nHere, we ran the grid search over the parameters of SVC using the scaled data. How‐\never, there is a subtle catch in what we just did. When scaling the data, we used all the\ndata in the training set to find out how to train it. We then use the scaled training data\nto run our grid search using cross-validation. For each split in the cross-validation,\nsome part of the original training set will be declared the training part of the split,\nand some the test part of the split. The test part is used to measure what new data will\nlook like to a model trained on the training part. However, we already used the infor‐\nmation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n|",
    "mation contained in the test part of the split, when scaling the data. Remember that\nthe test part in each split in the cross-validation is part of the training set, and we\nused the information from the entire training set to find the right scaling of the data.\n306 \n| \nChapter 6: Algorithm Chains and Pipelines\nThis is fundamentally different from how new data looks to the model. If we observe\nnew data (say, in form of our test set), this data will not have been used to scale the\ntraining data, and it might have a different minimum and maximum than the train‐\ning data. The following example (Figure 6-1) shows how the data processing during\ncross-validation and the final evaluation differ:\nIn[4]:\nmglearn.plots.plot_improper_processing()\nFigure 6-1. Data usage when preprocessing outside the cross-validation loop\nSo, the splits in the cross-validation no longer correctly mirror how new data will\nlook to the modeling process. We already leaked information from these parts of the\ndata into our modeling process. This will lead to overly optimistic results during\ncross-validation, and possibly the selection of suboptimal parameters.\nTo get around this problem, the splitting of the dataset during cross-validation should\nbe done before doing any preprocessing. Any process that extracts knowledge from the\ndataset should only ever be applied to the training portion of the dataset, so any\ncross-validation should be the “outermost loop” in your processing.\nTo achieve this in scikit-learn with the cross_val_score function and the Grid\nSearchCV function, we can use the Pipeline class. The Pipeline class is a class that\nallows “gluing” together multiple processing steps into a single scikit-learn estima‐\nParameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline",
    "Parameter Selection with Preprocessing \n| \n307\n1 With one exception: the name can’t contain a double underscore, __.\ntor. The Pipeline class itself has fit, predict, and score methods and behaves just\nlike any other model in scikit-learn. The most common use case of the Pipeline\nclass is in chaining preprocessing steps (like scaling of the data) together with a\nsupervised model like a classifier.\nBuilding Pipelines\nLet’s look at how we can use the Pipeline class to express the workflow for training\nan SVM after scaling the data with MinMaxScaler (for now without the grid search).\nFirst, we build a pipeline object by providing it with a list of steps. Each step is a tuple\ncontaining a name (any string of your choosing1) and an instance of an estimator:\nIn[5]:\nfrom sklearn.pipeline import Pipeline\npipe = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC())])\nHere, we created two steps: the first, called \"scaler\", is an instance of MinMaxScaler,\nand the second, called \"svm\", is an instance of SVC. Now, we can fit the pipeline, like\nany other scikit-learn estimator:\nIn[6]:\npipe.fit(X_train, y_train)\nHere, pipe.fit first calls fit on the first step (the scaler), then transforms the train‐\ning data using the scaler, and finally fits the SVM with the scaled data. To evaluate on\nthe test data, we simply call pipe.score:\nIn[7]:\nprint(\"Test score: {:.2f}\".format(pipe.score(X_test, y_test)))\nOut[7]:\nTest score: 0.95\nCalling the score method on the pipeline first transforms the test data using the\nscaler, and then calls the score method on the SVM using the scaled test data. As you\ncan see, the result is identical to the one we got from the code at the beginning of the\nchapter, when doing the transformations by hand. Using the pipeline, we reduced the\ncode needed for our “preprocessing + classification” process. The main benefit of\nusing the pipeline, however, is that we can now use this single estimator in\ncross_val_score or GridSearchCV.\n308 \n|",
    "chapter, when doing the transformations by hand. Using the pipeline, we reduced the\ncode needed for our “preprocessing + classification” process. The main benefit of\nusing the pipeline, however, is that we can now use this single estimator in\ncross_val_score or GridSearchCV.\n308 \n| \nChapter 6: Algorithm Chains and Pipelines\nUsing Pipelines in Grid Searches\nUsing a pipeline in a grid search works the same way as using any other estimator. We\ndefine a parameter grid to search over, and construct a GridSearchCV from the pipe‐\nline and the parameter grid. When specifying the parameter grid, there is a slight\nchange, though. We need to specify for each parameter which step of the pipeline it\nbelongs to. Both parameters that we want to adjust, C and gamma, are parameters of\nSVC, the second step. We gave this step the name \"svm\". The syntax to define a param‐\neter grid for a pipeline is to specify for each parameter the step name, followed by __\n(a double underscore), followed by the parameter name. To search over the C param‐\neter of SVC we therefore have to use \"svm__C\" as the key in the parameter grid dictio‐\nnary, and similarly for gamma:\nIn[8]:\nparam_grid = {'svm__C': [0.001, 0.01, 0.1, 1, 10, 100],\n              'svm__gamma': [0.001, 0.01, 0.1, 1, 10, 100]}\nWith this parameter grid we can use GridSearchCV as usual:\nIn[9]:\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\nprint(\"Test set score: {:.2f}\".format(grid.score(X_test, y_test)))\nprint(\"Best parameters: {}\".format(grid.best_params_))\nOut[9]:\nBest cross-validation accuracy: 0.98\nTest set score: 0.97\nBest parameters: {'svm__C': 1, 'svm__gamma': 1}\nIn contrast to the grid search we did before, now for each split in the cross-validation,\nthe MinMaxScaler is refit with only the training splits and no information is leaked\nfrom the test split into the parameter search. Compare this (Figure 6-2) with",
    "In contrast to the grid search we did before, now for each split in the cross-validation,\nthe MinMaxScaler is refit with only the training splits and no information is leaked\nfrom the test split into the parameter search. Compare this (Figure 6-2) with\nFigure 6-1 earlier in this chapter:\nIn[10]:\nmglearn.plots.plot_proper_processing()\nUsing Pipelines in Grid Searches \n| \n309\nFigure 6-2. Data usage when preprocessing inside the cross-validation loop with a\npipeline\nThe impact of leaking information in the cross-validation varies depending on the\nnature of the preprocessing step. Estimating the scale of the data using the test fold\nusually doesn’t have a terrible impact, while using the test fold in feature extraction\nand feature selection can lead to substantial differences in outcomes.\nIllustrating Information Leakage\nA great example of leaking information in cross-validation is given in Hastie, Tibshir‐\nani, and Friedman’s book The Elements of Statistical Learning, and we reproduce an\nadapted version here. Let’s consider a synthetic regression task with 100 samples and\n1,000 features that are sampled independently from a Gaussian distribution. We also\nsample the response from a Gaussian distribution:\nIn[11]:\nrnd = np.random.RandomState(seed=0)\nX = rnd.normal(size=(100, 10000))\ny = rnd.normal(size=(100,))\nGiven the way we created the dataset, there is no relation between the data, X, and the\ntarget, y (they are independent), so it should not be possible to learn anything from\nthis dataset. We will now do the following. First, select the most informative of the 10\nfeatures using SelectPercentile feature selection, and then we evaluate a Ridge\nregressor using cross-validation:\n310 \n| \nChapter 6: Algorithm Chains and Pipelines\nIn[12]:\nfrom sklearn.feature_selection import SelectPercentile, f_regression\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\nX_selected = select.transform(X)\nprint(\"X_selected.shape: {}\".format(X_selected.shape))",
    "310 \n| \nChapter 6: Algorithm Chains and Pipelines\nIn[12]:\nfrom sklearn.feature_selection import SelectPercentile, f_regression\nselect = SelectPercentile(score_func=f_regression, percentile=5).fit(X, y)\nX_selected = select.transform(X)\nprint(\"X_selected.shape: {}\".format(X_selected.shape))\nOut[12]:\nX_selected.shape: (100, 500)\nIn[13]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import Ridge\nprint(\"Cross-validation accuracy (cv only on ridge): {:.2f}\".format(\n      np.mean(cross_val_score(Ridge(), X_selected, y, cv=5))))\nOut[13]:\nCross-validation accuracy (cv only on ridge): 0.91\nThe mean R2 computed by cross-validation is 0.91, indicating a very good model.\nThis clearly cannot be right, as our data is entirely random. What happened here is\nthat our feature selection picked out some features among the 10,000 random features\nthat are (by chance) very well correlated with the target. Because we fit the feature\nselection outside of the cross-validation, it could find features that are correlated both\non the training and the test folds. The information we leaked from the test folds was\nvery informative, leading to highly unrealistic results. Let’s compare this to a proper\ncross-validation using a pipeline:\nIn[14]:\npipe = Pipeline([(\"select\", SelectPercentile(score_func=f_regression,\n                                             percentile=5)),\n                 (\"ridge\", Ridge())])\nprint(\"Cross-validation accuracy (pipeline): {:.2f}\".format(\n      np.mean(cross_val_score(pipe, X, y, cv=5))))\nOut[14]:\nCross-validation accuracy (pipeline): -0.25\nThis time, we get a negative R2 score, indicating a very poor model. Using the pipe‐\nline, the feature selection is now inside the cross-validation loop. This means features\ncan only be selected using the training folds of the data, not the test fold. The feature\nselection finds features that are correlated with the target on the training set, but",
    "line, the feature selection is now inside the cross-validation loop. This means features\ncan only be selected using the training folds of the data, not the test fold. The feature\nselection finds features that are correlated with the target on the training set, but\nbecause the data is entirely random, these features are not correlated with the target\non the test set. In this example, rectifying the data leakage issue in the feature selec‐\ntion makes the difference between concluding that a model works very well and con‐\ncluding that a model works not at all.\nUsing Pipelines in Grid Searches \n| \n311\n2 Or just fit_transform.\nThe General Pipeline Interface\nThe Pipeline class is not restricted to preprocessing and classification, but can in\nfact join any number of estimators together. For example, you could build a pipeline\ncontaining feature extraction, feature selection, scaling, and classification, for a total\nof four steps. Similarly, the last step could be regression or clustering instead of classi‐\nfication.\nThe only requirement for estimators in a pipeline is that all but the last step need to\nhave a transform method, so they can produce a new representation of the data that\ncan be used in the next step.\nInternally, during the call to Pipeline.fit, the pipeline calls fit and then transform\non each step in turn,2 with the input given by the output of the transform method of\nthe previous step. For the last step in the pipeline, just fit is called.\nBrushing over some finer details, this is implemented as follows. Remember that pipe\nline.steps is a list of tuples, so pipeline.steps[0][1] is the first estimator, pipe\nline.steps[1][1] is the second estimator, and so on:\nIn[15]:\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # iterate over all but the final step\n        # fit and transform the data\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # fit the last step",
    "In[15]:\ndef fit(self, X, y):\n    X_transformed = X\n    for name, estimator in self.steps[:-1]:\n        # iterate over all but the final step\n        # fit and transform the data\n        X_transformed = estimator.fit_transform(X_transformed, y)\n    # fit the last step\n    self.steps[-1][1].fit(X_transformed, y)\n    return self\nWhen predicting using Pipeline, we similarly transform the data using all but the\nlast step, and then call predict on the last step:\nIn[16]:\ndef predict(self, X):\n    X_transformed = X\n    for step in self.steps[:-1]:\n        # iterate over all but the final step\n        # transform the data\n        X_transformed = step[1].transform(X_transformed)\n    # fit the last step\n    return self.steps[-1][1].predict(X_transformed)\n312 \n| \nChapter 6: Algorithm Chains and Pipelines\nThe process is illustrated in Figure 6-3 for two transformers, T1 and T2, and a\nclassifier (called Classifier).\nFigure 6-3. Overview of the pipeline training and prediction process\nThe pipeline is actually even more general than this. There is no requirement for the\nlast step in a pipeline to have a predict function, and we could create a pipeline just\ncontaining, for example, a scaler and PCA. Then, because the last step (PCA) has a\ntransform method, we could call transform on the pipeline to get the output of\nPCA.transform applied to the data that was processed by the previous step. The last\nstep of a pipeline is only required to have a fit method.\nConvenient Pipeline Creation with make_pipeline\nCreating a pipeline using the syntax described earlier is sometimes a bit cumbersome,\nand we often don’t need user-specified names for each step. There is a convenience\nfunction, make_pipeline, that will create a pipeline for us and automatically name\neach step based on its class. The syntax for make_pipeline is as follows:\nIn[17]:\nfrom sklearn.pipeline import make_pipeline\n# standard syntax\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# abbreviated syntax",
    "each step based on its class. The syntax for make_pipeline is as follows:\nIn[17]:\nfrom sklearn.pipeline import make_pipeline\n# standard syntax\npipe_long = Pipeline([(\"scaler\", MinMaxScaler()), (\"svm\", SVC(C=100))])\n# abbreviated syntax\npipe_short = make_pipeline(MinMaxScaler(), SVC(C=100))\nThe General Pipeline Interface \n| \n313\nThe pipeline objects pipe_long and pipe_short do exactly the same thing, but\npipe_short has steps that were automatically named. We can see the names of the\nsteps by looking at the steps attribute:\nIn[18]:\nprint(\"Pipeline steps:\\n{}\".format(pipe_short.steps))\nOut[18]:\nPipeline steps:\n[('minmaxscaler', MinMaxScaler(copy=True, feature_range=(0, 1))),\n ('svc', SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n \n     decision_function_shape=None, degree=3, gamma='auto',\n             kernel='rbf', max_iter=-1, probability=False,\n             random_state=None, shrinking=True, tol=0.001,\n             verbose=False))]\nThe steps are named minmaxscaler and svc. In general, the step names are just low‐\nercase versions of the class names. If multiple steps have the same class, a number is\nappended:\nIn[19]:\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\npipe = make_pipeline(StandardScaler(), PCA(n_components=2), StandardScaler())\nprint(\"Pipeline steps:\\n{}\".format(pipe.steps))\nOut[19]:\nPipeline steps:\n[('standardscaler-1', StandardScaler(copy=True, with_mean=True, with_std=True)),\n ('pca', PCA(copy=True, iterated_power=4, n_components=2, random_state=None,\n             svd_solver='auto', tol=0.0, whiten=False)),\n ('standardscaler-2', StandardScaler(copy=True, with_mean=True, with_std=True))]\nAs you can see, the first StandardScaler step was named standardscaler-1 and the\nsecond standardscaler-2. However, in such settings it might be better to use the\nPipeline construction with explicit names, to give more semantic names to each\nstep.\nAccessing Step Attributes",
    "As you can see, the first StandardScaler step was named standardscaler-1 and the\nsecond standardscaler-2. However, in such settings it might be better to use the\nPipeline construction with explicit names, to give more semantic names to each\nstep.\nAccessing Step Attributes\nOften you will want to inspect attributes of one of the steps of the pipeline—say, the\ncoefficients of a linear model or the components extracted by PCA. The easiest way to\naccess the steps in a pipeline is via the named_steps attribute, which is a dictionary\nfrom the step names to the estimators:\n314 \n| \nChapter 6: Algorithm Chains and Pipelines\nIn[20]:\n# fit the pipeline defined before to the cancer dataset\npipe.fit(cancer.data)\n# extract the first two principal components from the \"pca\" step\ncomponents = pipe.named_steps[\"pca\"].components_\nprint(\"components.shape: {}\".format(components.shape))\nOut[20]:\ncomponents.shape: (2, 30)\nAccessing Attributes in a Grid-Searched Pipeline\nAs we discussed earlier in this chapter, one of the main reasons to use pipelines is for\ndoing grid searches. A common task is to access some of the steps of a pipeline inside\na grid search. Let’s grid search a LogisticRegression classifier on the cancer dataset,\nusing Pipeline and StandardScaler to scale the data before passing it to the Logisti\ncRegression classifier. First we create a pipeline using the make_pipeline function:\nIn[21]:\nfrom sklearn.linear_model import LogisticRegression\npipe = make_pipeline(StandardScaler(), LogisticRegression())\nNext, we create a parameter grid. As explained in Chapter 2, the regularization\nparameter to tune for LogisticRegression is the parameter C. We use a logarithmic\ngrid for this parameter, searching between 0.01 and 100. Because we used the\nmake_pipeline function, the name of the LogisticRegression step in the pipeline is\nthe lowercased class name, logisticregression. To tune the parameter C, we there‐\nfore have to specify a parameter grid for logisticregression__C:\nIn[22]:",
    "make_pipeline function, the name of the LogisticRegression step in the pipeline is\nthe lowercased class name, logisticregression. To tune the parameter C, we there‐\nfore have to specify a parameter grid for logisticregression__C:\nIn[22]:\nparam_grid = {'logisticregression__C': [0.01, 0.1, 1, 10, 100]}\nAs usual, we split the cancer dataset into training and test sets, and fit a grid search:\nIn[23]:\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=4)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nSo how do we access the coefficients of the best LogisticRegression model that was\nfound by GridSearchCV? From Chapter 5 we know that the best model found by\nGridSearchCV, trained on all the training data, is stored in grid.best_estimator_:\nThe General Pipeline Interface \n| \n315\nIn[24]:\nprint(\"Best estimator:\\n{}\".format(grid.best_estimator_))\nOut[24]:\nBest estimator:\nPipeline(steps=[\n    ('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)),\n    ('logisticregression', LogisticRegression(C=0.1, class_weight=None,\n    dual=False, fit_intercept=True, intercept_scaling=1, max_iter=100,\n    multi_class='ovr', n_jobs=1, penalty='l2', random_state=None,\n    solver='liblinear', tol=0.0001, verbose=0, warm_start=False))])\nThis best_estimator_ in our case is a pipeline with two steps, standardscaler and\nlogisticregression. To access the logisticregression step, we can use the\nnamed_steps attribute of the pipeline, as explained earlier:\nIn[25]:\nprint(\"Logistic regression step:\\n{}\".format(\n      grid.best_estimator_.named_steps[\"logisticregression\"]))\nOut[25]:\nLogistic regression step:\nLogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n                  penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n                  verbose=0, warm_start=False)",
    "LogisticRegression(C=0.1, class_weight=None, dual=False, fit_intercept=True,\n                  intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n                  penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n                  verbose=0, warm_start=False)\nNow that we have the trained LogisticRegression instance, we can access the coeffi‐\ncients (weights) associated with each input feature:\nIn[26]:\nprint(\"Logistic regression coefficients:\\n{}\".format(\n      grid.best_estimator_.named_steps[\"logisticregression\"].coef_))\nOut[26]:\nLogistic regression coefficients:\n[[-0.389 -0.375 -0.376 -0.396 -0.115  0.017 -0.355 -0.39  -0.058  0.209\n  -0.495 -0.004 -0.371 -0.383 -0.045  0.198  0.004 -0.049  0.21   0.224\n  -0.547 -0.525 -0.499 -0.515 -0.393 -0.123 -0.388 -0.417 -0.325 -0.139]]\nThis might be a somewhat lengthy expression, but often it comes in handy in under‐\nstanding your models.\n316 \n| \nChapter 6: Algorithm Chains and Pipelines\nGrid-Searching Preprocessing Steps and Model\nParameters\nUsing pipelines, we can encapsulate all the processing steps in our machine learning\nworkflow in a single scikit-learn estimator. Another benefit of doing this is that we\ncan now adjust the parameters of the preprocessing using the outcome of a supervised\ntask like regression or classification. In previous chapters, we used polynomial fea‐\ntures on the boston dataset before applying the ridge regressor. Let’s model that using\na pipeline instead. The pipeline contains three steps—scaling the data, computing\npolynomial features, and ridge regression:\nIn[27]:\nfrom sklearn.datasets import load_boston\nboston = load_boston()\nX_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    random_state=0)\nfrom sklearn.preprocessing import PolynomialFeatures\npipe = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    Ridge())",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target,\n                                                    random_state=0)\nfrom sklearn.preprocessing import PolynomialFeatures\npipe = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    Ridge())\nHow do we know which degrees of polynomials to choose, or whether to choose any\npolynomials or interactions at all? Ideally we want to select the degree parameter\nbased on the outcome of the classification. Using our pipeline, we can search over the\ndegree parameter together with the parameter alpha of Ridge. To do this, we define a\nparam_grid that contains both, appropriately prefixed by the step names:\nIn[28]:\nparam_grid = {'polynomialfeatures__degree': [1, 2, 3],\n              'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\nNow we can run our grid search again:\nIn[29]:\ngrid = GridSearchCV(pipe, param_grid=param_grid, cv=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\nWe can visualize the outcome of the cross-validation using a heat map (Figure 6-4), as\nwe did in Chapter 5:\nIn[30]:\nplt.matshow(grid.cv_results_['mean_test_score'].reshape(3, -1),\n            vmin=0, cmap=\"viridis\")\nplt.xlabel(\"ridge__alpha\")\nplt.ylabel(\"polynomialfeatures__degree\")\nGrid-Searching Preprocessing Steps and Model Parameters \n| \n317\nplt.xticks(range(len(param_grid['ridge__alpha'])), param_grid['ridge__alpha'])\nplt.yticks(range(len(param_grid['polynomialfeatures__degree'])),\n           param_grid['polynomialfeatures__degree'])\nplt.colorbar()\nFigure 6-4. Heat map of mean cross-validation score as a function of the degree of the\npolynomial features and alpha parameter of Ridge\nLooking at the results produced by the cross-validation, we can see that using polyno‐\nmials of degree two helps, but that degree-three polynomials are much worse than\neither degree one or two. This is reflected in the best parameters that were found:\nIn[31]:\nprint(\"Best parameters: {}\".format(grid.best_params_))\nOut[31]:",
    "mials of degree two helps, but that degree-three polynomials are much worse than\neither degree one or two. This is reflected in the best parameters that were found:\nIn[31]:\nprint(\"Best parameters: {}\".format(grid.best_params_))\nOut[31]:\nBest parameters: {'polynomialfeatures__degree': 2, 'ridge__alpha': 10}\nWhich lead to the following score:\nIn[32]:\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\nOut[32]:\nTest-set score: 0.77\nLet’s run a grid search without polynomial features for comparison:\nIn[33]:\nparam_grid = {'ridge__alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\npipe = make_pipeline(StandardScaler(), Ridge())\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Score without poly features: {:.2f}\".format(grid.score(X_test, y_test)))\n318 \n| \nChapter 6: Algorithm Chains and Pipelines\nOut[33]:\nScore without poly features: 0.63\nAs we would expect looking at the grid search results visualized in Figure 6-4, using\nno polynomial features leads to decidedly worse results.\nSearching over preprocessing parameters together with model parameters is a very\npowerful strategy. However, keep in mind that GridSearchCV tries all possible combi‐\nnations of the specified parameters. Therefore, adding more parameters to your grid\nexponentially increases the number of models that need to be built.\nGrid-Searching Which Model To Use\nYou can even go further in combining GridSearchCV and Pipeline: it is also possible\nto search over the actual steps being performed in the pipeline (say whether to use\nStandardScaler or MinMaxScaler). This leads to an even bigger search space and\nshould be considered carefully. Trying all possible solutions is usually not a viable\nmachine learning strategy. However, here is an example comparing a RandomForest\nClassifier and an SVC on the iris dataset. We know that the SVC might need the\ndata to be scaled, so we also search over whether to use StandardScaler or no pre‐",
    "machine learning strategy. However, here is an example comparing a RandomForest\nClassifier and an SVC on the iris dataset. We know that the SVC might need the\ndata to be scaled, so we also search over whether to use StandardScaler or no pre‐\nprocessing. For the RandomForestClassifier, we know that no preprocessing is nec‐\nessary. We start by defining the pipeline. Here, we explicitly name the steps. We want\ntwo steps, one for the preprocessing and then a classifier. We can instantiate this\nusing SVC and StandardScaler:\nIn[34]:\npipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', SVC())])\nNow we can define the parameter_grid to search over. We want the classifier to\nbe either RandomForestClassifier or SVC. Because they have different parameters to\ntune, and need different preprocessing, we can make use of the list of search grids we\ndiscussed in “Search over spaces that are not grids” on page 271. To assign an estima‐\ntor to a step, we use the name of the step as the parameter name. When we wanted to\nskip a step in the pipeline (for example, because we don’t need preprocessing for the\nRandomForest), we can set that step to None:\nIn[35]:\nfrom sklearn.ensemble import RandomForestClassifier\nparam_grid = [\n    {'classifier': [SVC()], 'preprocessing': [StandardScaler(), None],\n     'classifier__gamma': [0.001, 0.01, 0.1, 1, 10, 100],\n     'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100]},\n    {'classifier': [RandomForestClassifier(n_estimators=100)],\n     'preprocessing': [None], 'classifier__max_features': [1, 2, 3]}]\nGrid-Searching Which Model To Use \n| \n319\nNow we can instantiate and run the grid search as usual, here on the cancer dataset:\nIn[36]:\nX_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best params:\\n{}\\n\".format(grid.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))",
    "X_train, X_test, y_train, y_test = train_test_split(\n    cancer.data, cancer.target, random_state=0)\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best params:\\n{}\\n\".format(grid.best_params_))\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Test-set score: {:.2f}\".format(grid.score(X_test, y_test)))\nOut[36]:\nBest params:\n{'classifier':\n SVC(C=10, cache_size=200, class_weight=None, coef0=0.0,\n     decision_function_shape=None, degree=3, gamma=0.01, kernel='rbf',\n     max_iter=-1, probability=False, random_state=None, shrinking=True,\n     tol=0.001, verbose=False),\n 'preprocessing':\n StandardScaler(copy=True, with_mean=True, with_std=True),\n 'classifier__C': 10, 'classifier__gamma': 0.01}\nBest cross-validation score: 0.99\nTest-set score: 0.98\nThe outcome of the grid search is that SVC with StandardScaler preprocessing, C=10,\nand gamma=0.01 gave the best result.\nSummary and Outlook\nIn this chapter we introduced the Pipeline class, a general-purpose tool to chain\ntogether multiple processing steps in a machine learning workflow. Real-world appli‐\ncations of machine learning rarely involve an isolated use of a model, and instead are\na sequence of processing steps. Using pipelines allows us to encapsulate multiple steps\ninto a single Python object that adheres to the familiar scikit-learn interface of fit,\npredict, and transform. In particular when doing model evaluation using cross-\nvalidation and parameter selection using grid search, using the Pipeline class to cap‐\nture all the processing steps is essential for proper evaluation. The Pipeline class also\nallows writing more succinct code, and reduces the likelihood of mistakes that can\nhappen when building processing chains without the pipeline class (like forgetting\nto apply all transformers on the test set, or not applying them in the right order).\nChoosing the right combination of feature extraction, preprocessing, and models is",
    "happen when building processing chains without the pipeline class (like forgetting\nto apply all transformers on the test set, or not applying them in the right order).\nChoosing the right combination of feature extraction, preprocessing, and models is\nsomewhat of an art, and often requires some trial and error. However, using pipe‐\nlines, this “trying out” of many different processing steps is quite simple. When\n320 \n| \nChapter 6: Algorithm Chains and Pipelines\nexperimenting, be careful not to overcomplicate your processes, and make sure to\nevaluate whether every component you are including in your model is necessary.\nWith this chapter, we have completed our survey of general-purpose tools and algo‐\nrithms provided by scikit-learn. You now possess all the required skills and know\nthe necessary mechanisms to apply machine learning in practice. In the next chapter,\nwe will dive in more detail into one particular type of data that is commonly seen in\npractice, and that requires some special expertise to handle correctly: text data.\nSummary and Outlook \n| \n321\nCHAPTER 7\nWorking with Text Data\nIn Chapter 4, we talked about two kinds of features that can represent properties of\nthe data: continuous features that describe a quantity, and categorical features that are\nitems from a fixed list. There is a third kind of feature that can be found in many\napplications, which is text. For example, if we want to classify an email message as\neither a legitimate email or spam, the content of the email will certainly contain\nimportant information for this classification task. Or maybe we want to learn about\nthe opinion of a politician on the topic of immigration. Here, that individual’s\nspeeches or tweets might provide useful information. In customer service, we often\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\nand content of a message to automatically determine the customer’s intent, which",
    "speeches or tweets might provide useful information. In customer service, we often\nwant to find out if a message is a complaint or an inquiry. We can use the subject line\nand content of a message to automatically determine the customer’s intent, which\nallows us to send the message to the appropriate department, or even send a fully\nautomatic reply.\nText data is usually represented as strings, made up of characters. In any of the exam‐\nples just given, the length of the text data will vary. This feature is clearly very differ‐\nent from the numeric features that we’ve discussed so far, and we will need to process\nthe data before we can apply our machine learning algorithms to it.\nTypes of Data Represented as Strings\nBefore we dive into the processing steps that go into representing text data for\nmachine learning, we want to briefly discuss different kinds of text data that you\nmight encounter. Text is usually just a string in your dataset, but not all string features\nshould be treated as text. A string feature can sometimes represent categorical vari‐\nables, as we discussed in Chapter 5. There is no way to know how to treat a string\nfeature before looking at the data.\n323\nThere are four kinds of string data you might see:\n• Categorical data\n• Free strings that can be semantically mapped to categories\n• Structured string data\n• Text data\nCategorical data is data that comes from a fixed list. Say you collect data via a survey\nwhere you ask people their favorite color, with a drop-down menu that allows them\nto select from “red,” “green,” “blue,” “yellow,” “black,” “white,” “purple,” and “pink.”\nThis will result in a dataset with exactly eight different possible values, which clearly\nencode a categorical variable. You can check whether this is the case for your data by\neyeballing it (if you see very many different strings it is unlikely that this is a categori‐\ncal variable) and confirm it by computing the unique values over the dataset, and",
    "encode a categorical variable. You can check whether this is the case for your data by\neyeballing it (if you see very many different strings it is unlikely that this is a categori‐\ncal variable) and confirm it by computing the unique values over the dataset, and\npossibly a histogram over how often each appears. You also might want to check\nwhether each variable actually corresponds to a category that makes sense for your\napplication. Maybe halfway through the existence of your survey, someone found that\n“black” was misspelled as “blak” and subsequently fixed the survey. As a result, your\ndataset contains both “blak” and “black,” which correspond to the same semantic\nmeaning and should be consolidated.\nNow imagine instead of providing a drop-down menu, you provide a text field for the\nusers to provide their own favorite colors. Many people might respond with a color\nname like “black” or “blue.” Others might make typographical errors, use different\nspellings like “gray” and “grey,” or use more evocative and specific names like “mid‐\nnight blue.” You will also have some very strange entries. Some good examples come\nfrom the xkcd Color Survey, where people had to name colors and came up with\nnames like “velociraptor cloaka” and “my dentist’s office orange. I still remember his\ndandruff slowly wafting into my gaping yaw,” which are hard to map to colors auto‐\nmatically (or at all). The responses you can obtain from a text field belong to the sec‐\nond category in the list, free strings that can be semantically mapped to categories. It\nwill probably be best to encode this data as a categorical variable, where you can\nselect the categories either by using the most common entries, or by defining cate‐\ngories that will capture responses in a way that makes sense for your application. You\nmight then have some categories for standard colors, maybe a category “multicol‐\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐",
    "gories that will capture responses in a way that makes sense for your application. You\nmight then have some categories for standard colors, maybe a category “multicol‐\nored” for people that gave answers like “green and red stripes,” and an “other” cate‐\ngory for things that cannot be encoded otherwise. This kind of preprocessing of\nstrings can take a lot of manual effort and is not easily automated. If you are in a posi‐\ntion where you can influence data collection, we highly recommend avoiding man‐\nually entered values for concepts that are better captured using categorical variables.\nOften, manually entered values do not correspond to fixed categories, but still have\nsome underlying structure, like addresses, names of places or people, dates, telephone\n324 \n| \nChapter 7: Working with Text Data\n1 Arguably, the content of websites linked to in tweets contains more information than the text of the tweets\nthemselves.\n2 Most of what we will talk about in the rest of the chapter also applies to other languages that use the Roman\nalphabet, and partially to other languages with word boundary delimiters. Chinese, for example, does not\ndelimit word boundaries, and has other challenges that make applying the techniques in this chapter difficult.\n3 The dataset is available at http://ai.stanford.edu/~amaas/data/sentiment/.\nnumbers, or other identifiers. These kinds of strings are often very hard to parse, and\ntheir treatment is highly dependent on context and domain. A systematic treatment\nof these cases is beyond the scope of this book.\nThe final category of string data is freeform text data that consists of phrases or sen‐\ntences. Examples include tweets, chat logs, and hotel reviews, as well as the collected\nworks of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one",
    "works of Shakespeare, the content of Wikipedia, or the Project Gutenberg collection\nof 50,000 ebooks. All of these collections contain information mostly as sentences\ncomposed of words.1 For simplicity’s sake, let’s assume all our documents are in one\nlanguage, English.2 In the context of text analysis, the dataset is often called the cor‐\npus, and each data point, represented as a single text, is called a document. These\nterms come from the information retrieval (IR) and natural language processing (NLP)\ncommunity, which both deal mostly in text data.\nExample Application: Sentiment Analysis of Movie\nReviews\nAs a running example in this chapter, we will use a dataset of movie reviews from the\nIMDb (Internet Movie Database) website collected by Stanford researcher Andrew\nMaas.3 This dataset contains the text of the reviews, together with a label that indi‐\ncates whether a review is “positive” or “negative.” The IMDb website itself contains\nratings from 1 to 10. To simplify the modeling, this annotation is summarized as a\ntwo-class classification dataset where reviews with a score of 6 or higher are labeled as\npositive, and the rest as negative. We will leave the question of whether this is a good\nrepresentation of the data open, and simply use the data as provided by Andrew\nMaas.\nAfter unpacking the data, the dataset is provided as text files in two separate folders,\none for the training data and one for the test data. Each of these in turn has two sub‐\nfolders, one called pos and one called neg:\nExample Application: Sentiment Analysis of Movie Reviews \n| \n325\nIn[2]:\n!tree -L 2 data/aclImdb\nOut[2]:\ndata/aclImdb\n├── test\n│   ├── neg\n│   └── pos\n└── train\n    ├── neg\n    └── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called",
    "└── pos\n6 directories, 0 files\nThe pos folder contains all the positive reviews, each as a separate text file, and simi‐\nlarly for the neg folder. There is a helper function in scikit-learn to load files stored\nin such a folder structure, where each subfolder corresponds to a label, called\nload_files. We apply the load_files function first to the training data:\nIn[3]:\nfrom sklearn.datasets import load_files\nreviews_train = load_files(\"data/aclImdb/train/\")\n# load_files returns a bunch, containing training texts and training labels\ntext_train, y_train = reviews_train.data, reviews_train.target\nprint(\"type of text_train: {}\".format(type(text_train)))\nprint(\"length of text_train: {}\".format(len(text_train)))\nprint(\"text_train[1]:\\n{}\".format(text_train[1]))\nOut[3]:\ntype of text_train:  <class 'list'>\nlength of text_train:  25000\ntext_train[1]:\nb'Words can\\'t describe how bad this movie is. I can\\'t explain it by writing\n  only. You have too see it for yourself to get at grip of how horrible a movie\n  really can be. Not that I recommend you to do that. There are so many\n  clich\\xc3\\xa9s, mistakes (and all other negative things you can imagine) here\n  that will just make you cry. To start with the technical first, there are a\n  LOT of mistakes regarding the airplane. I won\\'t list them here, but just\n  mention the coloring of the plane. They didn\\'t even manage to show an\n  airliner in the colors of a fictional airline, but instead used a 747\n  painted in the original Boeing livery. Very bad. The plot is stupid and has\n  been done many times before, only much, much better. There are so many\n  ridiculous moments here that i lost count of it really early. Also, I was on\n  the bad guys\\' side all the time in the movie, because the good guys were so\n  stupid. \"Executive Decision\" should without a doubt be you\\'re choice over\n  this one, even the \"Turbulence\"-movies are better. In fact, every other\n  movie in the world is better than this one.'",
    "the bad guys\\' side all the time in the movie, because the good guys were so\n  stupid. \"Executive Decision\" should without a doubt be you\\'re choice over\n  this one, even the \"Turbulence\"-movies are better. In fact, every other\n  movie in the world is better than this one.'\nYou can see that text_train is a list of length 25,000, where each entry is a string\ncontaining a review. We printed the review with index 1. You can also see that the\nreview contains some HTML line breaks (<br />). While these are unlikely to have a\n326 \n| \nChapter 7: Working with Text Data\nlarge impact on our machine learning models, it is better to clean the data and\nremove this formatting before we proceed:\nIn[4]:\ntext_train = [doc.replace(b\"<br />\", b\" \") for doc in text_train]\nThe type of the entries of text_train will depend on your Python version. In Python\n3, they will be of type bytes which represents a binary encoding of the string data. In\nPython 2, text_train contains strings. We won’t go into the details of the different\nstring types in Python here, but we recommend that you read the Python 2 and/or\nPython 3 documentation regarding strings and Unicode.\nThe dataset was collected such that the positive class and the negative class balanced,\nso that there are as many positive as negative strings:\nIn[5]:\nprint(\"Samples per class (training): {}\".format(np.bincount(y_train)))\nOut[5]:\nSamples per class (training): [12500 12500]\nWe load the test dataset in the same manner:\nIn[6]:\nreviews_test = load_files(\"data/aclImdb/test/\")\ntext_test, y_test = reviews_test.data, reviews_test.target\nprint(\"Number of documents in test data: {}\".format(len(text_test)))\nprint(\"Samples per class (test): {}\".format(np.bincount(y_test)))\ntext_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label",
    "text_test = [doc.replace(b\"<br />\", b\" \") for doc in text_test]\nOut[6]:\nNumber of documents in test data: 25000\nSamples per class (test): [12500 12500]\nThe task we want to solve is as follows: given a review, we want to assign the label\n“positive” or “negative” based on the text content of the review. This is a standard\nbinary classification task. However, the text data is not in a format that a machine\nlearning model can handle. We need to convert the string representation of the text\ninto a numeric representation that we can apply our machine learning algorithms to.\nRepresenting Text Data as a Bag of Words\nOne of the most simple but effective and commonly used ways to represent text for\nmachine learning is using the bag-of-words representation. When using this represen‐\ntation, we discard most of the structure of the input text, like chapters, paragraphs,\nsentences, and formatting, and only count how often each word appears in each text in\nRepresenting Text Data as a Bag of Words \n| \n327\nthe corpus. Discarding the structure and counting only word occurrences leads to the\nmental image of representing text as a “bag.”\nComputing the bag-of-words representation for a corpus of documents consists of\nthe following three steps:\n1. Tokenization. Split each document into the words that appear in it (called tokens),\nfor example by splitting them on whitespace and punctuation.\n2. Vocabulary building. Collect a vocabulary of all words that appear in any of the\ndocuments, and number them (say, in alphabetical order).\n3. Encoding. For each document, count how often each of the words in the vocabu‐\nlary appear in this document.\nThere are some subtleties involved in step 1 and step 2, which we will discuss in more\ndetail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐",
    "detail later in this chapter. For now, let’s look at how we can apply the bag-of-words\nprocessing using scikit-learn. Figure 7-1 illustrates the process on the string \"This\nis how you get ants.\". The output is one vector of word counts for each docu‐\nment. For each word in the vocabulary, we have a count of how often it appears in\neach document. That means our numeric representation has one feature for each\nunique word in the whole dataset. Note how the order of the words in the original\nstring is completely irrelevant to the bag-of-words feature representation.\nFigure 7-1. Bag-of-words processing\n328 \n| \nChapter 7: Working with Text Data\nApplying Bag-of-Words to a Toy Dataset\nThe bag-of-words representation is implemented in CountVectorizer, which is a\ntransformer. Let’s first apply it to a toy dataset, consisting of two samples, to see it\nworking:\nIn[7]:\nbards_words =[\"The fool doth think he is wise,\",\n              \"but the wise man knows himself to be a fool\"]\nWe import and instantiate the CountVectorizer and fit it to our toy data as follows:\nIn[8]:\nfrom sklearn.feature_extraction.text import CountVectorizer\nvect = CountVectorizer()\nvect.fit(bards_words)\nFitting the CountVectorizer consists of the tokenization of the training data and\nbuilding of the vocabulary, which we can access as the vocabulary_ attribute:\nIn[9]:\nprint(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\nprint(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\nOut[9]:\nVocabulary size: 13\nVocabulary content:\n {'the': 9, 'himself': 5, 'wise': 12, 'he': 4, 'doth': 2, 'to': 11, 'knows': 7,\n  'man': 8, 'fool': 3, 'is': 6, 'be': 0, 'think': 10, 'but': 1}\nThe vocabulary consists of 13 words, from \"be\" to \"wise\".\nTo create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'",
    "To create the bag-of-words representation for the training data, we call the transform\nmethod:\nIn[10]:\nbag_of_words = vect.transform(bards_words)\nprint(\"bag_of_words: {}\".format(repr(bag_of_words)))\nOut[10]:\nbag_of_words: <2x13 sparse matrix of type '<class 'numpy.int64'>'\n    with 16 stored elements in Compressed Sparse Row format>\nThe bag-of-words representation is stored in a SciPy sparse matrix that only stores\nthe entries that are nonzero (see Chapter 1). The matrix is of shape 2×13, with one\nrow for each of the two data points and one feature for each of the words in the\nvocabulary. A sparse matrix is used as most documents only contain a small subset of\nthe words in the vocabulary, meaning most entries in the feature array are 0. Think\nRepresenting Text Data as a Bag of Words \n| \n329\n4 This is possible because we are using a small toy dataset that contains only 13 words. For any real dataset, this\nwould result in a MemoryError.\nabout how many different words might appear in a movie review compared to all the\nwords in the English language (which is what the vocabulary models). Storing all\nthose zeros would be prohibitive, and a waste of memory. To look at the actual con‐\ntent of the sparse matrix, we can convert it to a “dense” NumPy array (that also stores\nall the 0 entries) using the toarray method:4\nIn[11]:\nprint(\"Dense representation of bag_of_words:\\n{}\".format(\n    bag_of_words.toarray()))\nOut[11]:\nDense representation of bag_of_words:\n[[0 0 1 1 1 0 1 0 0 1 1 0 1]\n [1 1 0 1 0 1 0 1 1 1 0 1 1]]\nWe can see that the word counts for each word are either 0 or 1; neither of the two\nstrings in bards_words contains a word twice. Let’s take a look at how to read these\nfeature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐",
    "feature vectors. The first string (\"The fool doth think he is wise,\") is repre‐\nsented as the first row in, and it contains the first word in the vocabulary, \"be\", zero\ntimes. It also contains the second word in the vocabulary, \"but\", zero times. It con‐\ntains the third word, \"doth\", once, and so on. Looking at both rows, we can see that\nthe fourth word, \"fool\", the tenth word, \"the\", and the thirteenth word, \"wise\",\nappear in both strings.\nBag-of-Words for Movie Reviews\nNow that we’ve gone through the bag-of-words process in detail, let’s apply it to our\ntask of sentiment analysis for movie reviews. Earlier, we loaded our training and test\ndata from the IMDb reviews into lists of strings (text_train and text_test), which\nwe will now process:\nIn[12]:\nvect = CountVectorizer().fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train:\\n{}\".format(repr(X_train)))\nOut[12]:\nX_train:\n<25000x74849 sparse matrix of type '<class 'numpy.int64'>'\n    with 3431196 stored elements in Compressed Sparse Row format>\n330 \n| \nChapter 7: Working with Text Data\n5 A quick analysis of the data confirms that this is indeed the case. Try confirming it yourself.\nThe shape of X_train, the bag-of-words representation of the training data, is\n25,000×74,849, indicating that the vocabulary contains 74,849 entries. Again, the data\nis stored as a SciPy sparse matrix. Let’s look at the vocabulary in a bit more detail.\nAnother way to access the vocabulary is using the get_feature_name method of the\nvectorizer, which returns a convenient list where each entry corresponds to one fea‐\nture:\nIn[13]:\nfeature_names = vect.get_feature_names()\nprint(\"Number of features: {}\".format(len(feature_names)))\nprint(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:",
    "print(\"First 20 features:\\n{}\".format(feature_names[:20]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 2000th feature:\\n{}\".format(feature_names[::2000]))\nOut[13]:\nNumber of features: 74849\nFirst 20 features:\n['00', '000', '0000000000001', '00001', '00015', '000s', '001', '003830',\n '006', '007', '0079', '0080', '0083', '0093638', '00am', '00pm', '00s',\n '01', '01pm', '02']\nFeatures 20010 to 20030:\n['dratted', 'draub', 'draught', 'draughts', 'draughtswoman', 'draw', 'drawback',\n 'drawbacks', 'drawer', 'drawers', 'drawing', 'drawings', 'drawl',\n 'drawled', 'drawling', 'drawn', 'draws', 'draza', 'dre', 'drea']\nEvery 2000th feature:\n['00', 'aesir', 'aquarian', 'barking', 'blustering', 'bête', 'chicanery',\n 'condensing', 'cunning', 'detox', 'draper', 'enshrined', 'favorit', 'freezer',\n 'goldman', 'hasan', 'huitieme', 'intelligible', 'kantrowitz', 'lawful',\n 'maars', 'megalunged', 'mostey', 'norrland', 'padilla', 'pincher',\n 'promisingly', 'receptionist', 'rivals', 'schnaas', 'shunning', 'sparse',\n 'subset', 'temptations', 'treatises', 'unproven', 'walkman', 'xylophonist']\nAs you can see, possibly a bit surprisingly, the first 10 entries in the vocabulary are all\nnumbers. All these numbers appear somewhere in the reviews, and are therefore\nextracted as words. Most of these numbers don’t have any immediate semantic mean‐\ning—apart from \"007\", which in the particular context of movies is likely to refer to\nthe James Bond character.5 Weeding out the meaningful from the nonmeaningful\n“words” is sometimes tricky. Looking further along in the vocabulary, we find a col‐\nlection of English words starting with “dra”. You might notice that for \"draught\",\n\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\nvocabulary as distinct words. These words have very closely related semantic mean‐\nings, and counting them as different words, corresponding to different features,\nmight not be ideal.",
    "\"drawback\", and \"drawer\" both the singular and plural forms are contained in the\nvocabulary as distinct words. These words have very closely related semantic mean‐\nings, and counting them as different words, corresponding to different features,\nmight not be ideal.\nRepresenting Text Data as a Bag of Words \n| \n331\n6 The attentive reader might notice that we violate our lesson from Chapter 6 on cross-validation with prepro‐\ncessing here. Using the default settings of CountVectorizer, it actually does not collect any statistics, so our\nresults are valid. Using Pipeline from the start would be a better choice for applications, but we defer it for\nease of exposure.\nBefore we try to improve our feature extraction, let’s obtain a quantitative measure of\nperformance by actually building a classifier. We have the training labels stored in\ny_train and the bag-of-words representation of the training data in X_train, so we\ncan train a classifier on this data. For high-dimensional, sparse data like this, linear\nmodels like LogisticRegression often work best.\nLet’s start by evaluating LogisticRegresssion using cross-validation:6\nIn[14]:\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nscores = cross_val_score(LogisticRegression(), X_train, y_train, cv=5)\nprint(\"Mean cross-validation accuracy: {:.2f}\".format(np.mean(scores)))\nOut[14]:\nMean cross-validation accuracy: 0.88\nWe obtain a mean cross-validation score of 88%, which indicates reasonable perfor‐\nmance for a balanced binary classification task. We know that LogisticRegression\nhas a regularization parameter, C, which we can tune via cross-validation:\nIn[15]:\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters: \", grid.best_params_)\nOut[15]:",
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters: \", grid.best_params_)\nOut[15]:\nBest cross-validation score: 0.89\nBest parameters:  {'C': 0.1}\nWe obtain a cross-validation score of 89% using C=0.1. We can now assess the gener‐\nalization performance of this parameter setting on the test set:\nIn[16]:\nX_test = vect.transform(text_test)\nprint(\"{:.2f}\".format(grid.score(X_test, y_test)))\nOut[16]:\n0.88\n332 \n| \nChapter 7: Working with Text Data\nNow, let’s see if we can improve the extraction of words. The CountVectorizer\nextracts tokens using a regular expression. By default, the regular expression that is\nused is \"\\b\\w\\w+\\b\". If you are not familiar with regular expressions, this means it\nfinds all sequences of characters that consist of at least two letters or numbers (\\w)\nand that are separated by word boundaries (\\b). It does not find single-letter words,\nand it splits up contractions like “doesn’t” or “bit.ly”, but it matches “h8ter” as a single\nword. The CountVectorizer then converts all words to lowercase characters, so that\n“soon”, “Soon”, and “sOon” all correspond to the same token (and therefore feature).\nThis simple mechanism works quite well in practice, but as we saw earlier, we get\nmany uninformative features (like the numbers). One way to cut back on these is to\nonly use tokens that appear in at least two documents (or at least five documents, and\nso on). A token that appears only in a single document is unlikely to appear in the test\nset and is therefore not helpful. We can set the minimum number of documents a\ntoken needs to appear in with the min_df parameter:\nIn[17]:\nvect = CountVectorizer(min_df=5).fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with min_df: {}\".format(repr(X_train)))\nOut[17]:",
    "set and is therefore not helpful. We can set the minimum number of documents a\ntoken needs to appear in with the min_df parameter:\nIn[17]:\nvect = CountVectorizer(min_df=5).fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with min_df: {}\".format(repr(X_train)))\nOut[17]:\nX_train with min_df: <25000x27271 sparse matrix of type '<class 'numpy.int64'>'\n    with 3354014 stored elements in Compressed Sparse Row format>\nBy requiring at least five appearances of each token, we can bring down the number\nof features to 27,271, as seen in the preceding output—only about a third of the origi‐\nnal features. Let’s look at some tokens again:\nIn[18]:\nfeature_names = vect.get_feature_names()\nprint(\"First 50 features:\\n{}\".format(feature_names[:50]))\nprint(\"Features 20010 to 20030:\\n{}\".format(feature_names[20010:20030]))\nprint(\"Every 700th feature:\\n{}\".format(feature_names[::700]))\nOut[18]:\nFirst 50 features:\n['00', '000', '007', '00s', '01', '02', '03', '04', '05', '06', '07', '08',\n '09', '10', '100', '1000', '100th', '101', '102', '103', '104', '105', '107',\n '108', '10s', '10th', '11', '110', '112', '116', '117', '11th', '12', '120',\n '12th', '13', '135', '13th', '14', '140', '14th', '15', '150', '15th', '16',\n '160', '1600', '16mm', '16s', '16th']\nFeatures 20010 to 20030:\n['repentance', 'repercussions', 'repertoire', 'repetition', 'repetitions',\n 'repetitious', 'repetitive', 'rephrase', 'replace', 'replaced', 'replacement',\n 'replaces', 'replacing', 'replay', 'replayable', 'replayed', 'replaying',\n 'replays', 'replete', 'replica']\nRepresenting Text Data as a Bag of Words \n| \n333\nEvery 700th feature:\n['00', 'affections', 'appropriately', 'barbra', 'blurbs', 'butchered',\n 'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds',\n 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate',\n 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking',",
    "'cheese', 'commitment', 'courts', 'deconstructed', 'disgraceful', 'dvds',\n 'eschews', 'fell', 'freezer', 'goriest', 'hauser', 'hungary', 'insinuate',\n 'juggle', 'leering', 'maelstrom', 'messiah', 'music', 'occasional', 'parking',\n 'pleasantville', 'pronunciation', 'recipient', 'reviews', 'sas', 'shea',\n 'sneers', 'steiger', 'swastika', 'thrusting', 'tvs', 'vampyre', 'westerns']\nThere are clearly many fewer numbers, and some of the more obscure words or mis‐\nspellings seem to have vanished. Let’s see how well our model performs by doing a\ngrid search again:\nIn[19]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[19]:\nBest cross-validation score: 0.89\nThe best validation accuracy of the grid search is still 89%, unchanged from before.\nWe didn’t improve our model, but having fewer features to deal with speeds up pro‐\ncessing and throwing away useless features might make the model more interpretable.\nIf the transform method of CountVectorizer is called on a docu‐\nment that contains words that were not contained in the training\ndata, these words will be ignored as they are not part of the dictio‐\nnary. This is not really an issue for classification, as it’s not possible\nto learn anything about words that are not in the training data. For\nsome applications, like spam detection, it might be helpful to add a\nfeature that encodes how many so-called “out of vocabulary” words\nthere are in a particular document, though. For this to work, you\nneed to set min_df; otherwise, this feature will never be active dur‐\ning training.\nStopwords\nAnother way that we can get rid of uninformative words is by discarding words that\nare too frequent to be informative. There are two main approaches: using a language-\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\nlearn has a built-in list of English stopwords in the feature_extraction.text\nmodule:",
    "are too frequent to be informative. There are two main approaches: using a language-\nspecific list of stopwords, or discarding words that appear too frequently. scikit-\nlearn has a built-in list of English stopwords in the feature_extraction.text\nmodule:\n334 \n| \nChapter 7: Working with Text Data\nIn[20]:\nfrom sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\nprint(\"Number of stop words: {}\".format(len(ENGLISH_STOP_WORDS)))\nprint(\"Every 10th stopword:\\n{}\".format(list(ENGLISH_STOP_WORDS)[::10]))\nOut[20]:\nNumber of stop words: 318\nEvery 10th stopword:\n['above', 'elsewhere', 'into', 'well', 'rather', 'fifteen', 'had', 'enough',\n 'herein', 'should', 'third', 'although', 'more', 'this', 'none', 'seemed',\n 'nobody', 'seems', 'he', 'also', 'fill', 'anyone', 'anything', 'me', 'the',\n 'yet', 'go', 'seeming', 'front', 'beforehand', 'forty', 'i']\nClearly, removing the stopwords in the list can only decrease the number of features\nby the length of the list—here, 318—but it might lead to an improvement in perfor‐\nmance. Let’s give it a try:\nIn[21]:\n# Specifying stop_words=\"english\" uses the built-in list.\n# We could also augment it and pass our own.\nvect = CountVectorizer(min_df=5, stop_words=\"english\").fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train with stop words:\\n{}\".format(repr(X_train)))\nOut[21]:\nX_train with stop words:\n<25000x26966 sparse matrix of type '<class 'numpy.int64'>'\n    with 2149958 stored elements in Compressed Sparse Row format>\nThere are now 305 (27,271–26,966) fewer features in the dataset, which means that\nmost, but not all, of the stopwords appeared. Let’s run the grid search again:\nIn[22]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[22]:\nBest cross-validation score: 0.88\nThe grid search performance decreased slightly using the stopwords—not enough to",
    "In[22]:\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[22]:\nBest cross-validation score: 0.88\nThe grid search performance decreased slightly using the stopwords—not enough to\nworry about, but given that excluding 305 features out of over 27,000 is unlikely to\nchange performance or interpretability a lot, it doesn’t seem worth using this list.\nFixed lists are mostly helpful for small datasets, which might not contain enough\ninformation for the model to determine which words are stopwords from the data\nitself. As an exercise, you can try out the other approach, discarding frequently\nStopwords \n| \n335\n7 We provide this formula here mostly for completeness; you don’t need to remember it to use the tf–idf\nencoding.\nappearing words, by setting the max_df option of CountVectorizer and see how it\ninfluences the number of features and the performance.\nRescaling the Data with tf–idf\nInstead of dropping features that are deemed unimportant, another approach is to\nrescale features by how informative we expect them to be. One of the most common\nways to do this is using the term frequency–inverse document frequency (tf–idf)\nmethod. The intuition of this method is to give high weight to any term that appears\noften in a particular document, but not in many documents in the corpus. If a word\nappears often in a particular document, but not in very many documents, it is likely\nto be very descriptive of the content of that document. scikit-learn implements the\ntf–idf method in two classes: TfidfTransformer, which takes in the sparse matrix\noutput produced by CountVectorizer and transforms it, and TfidfVectorizer,\nwhich takes in the text data and does both the bag-of-words feature extraction and\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\nwhich you can read about on Wikipedia. The tf–idf score for word w in document d",
    "which takes in the text data and does both the bag-of-words feature extraction and\nthe tf–idf transformation. There are several variants of the tf–idf rescaling scheme,\nwhich you can read about on Wikipedia. The tf–idf score for word w in document d\nas implemented in both the TfidfTransformer and TfidfVectorizer classes is given\nby:7\ntfidf w, d = tf log\nN + 1\nNw + 1 + 1\nwhere N is the number of documents in the training set, Nw is the number of docu‐\nments in the training set that the word w appears in, and tf (the term frequency) is the\nnumber of times that the word w appears in the query document d (the document\nyou want to transform or encode). Both classes also apply L2 normalization after\ncomputing the tf–idf representation; in other words, they rescale the representation\nof each document to have Euclidean norm 1. Rescaling in this way means that the\nlength of a document (the number of words) does not change the vectorized repre‐\nsentation.\nBecause tf–idf actually makes use of the statistical properties of the training data, we\nwill use a pipeline, as described in Chapter 6, to ensure the results of our grid search\nare valid. This leads to the following code:\n336 \n| \nChapter 7: Working with Text Data\nIn[23]:\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\npipe = make_pipeline(TfidfVectorizer(min_df=5, norm=None),\n                     LogisticRegression())\nparam_grid = {'logisticregression__C': [0.001, 0.01, 0.1, 1, 10]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nOut[23]:\nBest cross-validation score: 0.89\nAs you can see, there is some improvement when using tf–idf instead of just word\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a",
    "Out[23]:\nBest cross-validation score: 0.89\nAs you can see, there is some improvement when using tf–idf instead of just word\ncounts. We can also inspect which words tf–idf found most important. Keep in mind\nthat the tf–idf scaling is meant to find words that distinguish documents, but it is a\npurely unsupervised technique. So, “important” here does not necessarily relate to the\n“positive review” and “negative review” labels we are interested in. First, we extract\nthe TfidfVectorizer from the pipeline:\nIn[24]:\nvectorizer = grid.best_estimator_.named_steps[\"tfidfvectorizer\"]\n# transform the training dataset\nX_train = vectorizer.transform(text_train)\n# find maximum value for each of the features over the dataset\nmax_value = X_train.max(axis=0).toarray().ravel()\nsorted_by_tfidf = max_value.argsort()\n# get feature names\nfeature_names = np.array(vectorizer.get_feature_names())\nprint(\"Features with lowest tfidf:\\n{}\".format(\n    feature_names[sorted_by_tfidf[:20]]))\nprint(\"Features with highest tfidf: \\n{}\".format(\n    feature_names[sorted_by_tfidf[-20:]]))\nOut[24]:\nFeatures with lowest tfidf:\n['poignant' 'disagree' 'instantly' 'importantly' 'lacked' 'occurred'\n 'currently' 'altogether' 'nearby' 'undoubtedly' 'directs' 'fond' 'stinker'\n 'avoided' 'emphasis' 'commented' 'disappoint' 'realizing' 'downhill'\n 'inane']\nFeatures with highest tfidf:\n['coop' 'homer' 'dillinger' 'hackenstein' 'gadget' 'taker' 'macarthur'\n 'vargas' 'jesse' 'basket' 'dominick' 'the' 'victor' 'bridget' 'victoria'\n 'khouri' 'zizek' 'rob' 'timon' 'titanic']\nRescaling the Data with tf–idf \n| \n337\nFeatures with low tf–idf are those that either are very commonly used across docu‐\nments or are only used sparingly, and only in very long documents. Interestingly,\nmany of the high-tf–idf features actually identify certain shows or movies. These\nterms only appear in reviews for this particular show or franchise, but tend to appear",
    "ments or are only used sparingly, and only in very long documents. Interestingly,\nmany of the high-tf–idf features actually identify certain shows or movies. These\nterms only appear in reviews for this particular show or franchise, but tend to appear\nvery often in these particular reviews. This is very clear, for example, for \"pokemon\",\n\"smallville\", and \"doodlebops\", but \"scanners\" here actually also refers to a\nmovie title. These words are unlikely to help us in our sentiment classification task\n(unless maybe some franchises are universally reviewed positively or negatively) but\ncertainly contain a lot of specific information about the reviews.\nWe can also find the words that have low inverse document frequency—that is, those\nthat appear frequently and are therefore deemed less important. The inverse docu‐\nment frequency values found on the training set are stored in the idf_ attribute:\nIn[25]:\nsorted_by_idf = np.argsort(vectorizer.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n    feature_names[sorted_by_idf[:100]]))\nOut[25]:\nFeatures with lowest idf:\n['the' 'and' 'of' 'to' 'this' 'is' 'it' 'in' 'that' 'but' 'for' 'with'\n 'was' 'as' 'on' 'movie' 'not' 'have' 'one' 'be' 'film' 'are' 'you' 'all'\n 'at' 'an' 'by' 'so' 'from' 'like' 'who' 'they' 'there' 'if' 'his' 'out'\n 'just' 'about' 'he' 'or' 'has' 'what' 'some' 'good' 'can' 'more' 'when'\n 'time' 'up' 'very' 'even' 'only' 'no' 'would' 'my' 'see' 'really' 'story'\n 'which' 'well' 'had' 'me' 'than' 'much' 'their' 'get' 'were' 'other'\n 'been' 'do' 'most' 'don' 'her' 'also' 'into' 'first' 'made' 'how' 'great'\n 'because' 'will' 'people' 'make' 'way' 'could' 'we' 'bad' 'after' 'any'\n 'too' 'then' 'them' 'she' 'watch' 'think' 'acting' 'movies' 'seen' 'its'\n 'him']\nAs expected, these are mostly English stopwords like \"the\" and \"no\". But some are\nclearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\nand so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most fre‐",
    "'him']\nAs expected, these are mostly English stopwords like \"the\" and \"no\". But some are\nclearly domain-specific to the movie reviews, like \"movie\", \"film\", \"time\", \"story\",\nand so on. Interestingly, \"good\", \"great\", and \"bad\" are also among the most fre‐\nquent and therefore “least relevant” words according to the tf–idf measure, even\nthough we might expect these to be very important for our sentiment analysis task.\nInvestigating Model Coefficients\nFinally, let’s look in a bit more detail into what our logistic regression model actually\nlearned from the data. Because there are so many features—27,271 after removing the\ninfrequent ones—we clearly cannot look at all of the coefficients at the same time.\nHowever, we can look at the largest coefficients, and see which words these corre‐\nspond to. We will use the last model that we trained, based on the tf–idf features.\nThe following bar chart (Figure 7-2) shows the 25 largest and 25 smallest coefficients\nof the logistic regression model, with the bars showing the size of each coefficient:\n338 \n| \nChapter 7: Working with Text Data\nIn[26]:\nmglearn.tools.visualize_coefficients(\n    grid.best_estimator_.named_steps[\"logisticregression\"].coef_,\n    feature_names, n_top_features=40)\nFigure 7-2. Largest and smallest coefficients of logistic regression trained on tf-idf fea‐\ntures\nThe negative coefficients on the left belong to words that according to the model are\nindicative of negative reviews, while the positive coefficients on the right belong to\nwords that according to the model indicate positive reviews. Most of the terms are\nquite intuitive, like \"worst\", \"waste\", \"disappointment\", and \"laughable\" indicat‐\ning bad movie reviews, while \"excellent\", \"wonderful\", \"enjoyable\", and\n\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)",
    "\"refreshing\" indicate positive movie reviews. Some words are slightly less clear, like\n\"bit\", \"job\", and \"today\", but these might be part of phrases like “good job” or “best\ntoday.”\nBag-of-Words with More Than One Word (n-Grams)\nOne of the main disadvantages of using a bag-of-words representation is that word\norder is completely discarded. Therefore, the two strings “it’s bad, not good at all” and\n“it’s good, not bad at all” have exactly the same representation, even though the mean‐\nings are inverted. Putting “not” in front of a word is only one example (if an extreme\none) of how context matters. Fortunately, there is a way of capturing context when\nusing a bag-of-words representation, by not only considering the counts of single\ntokens, but also the counts of pairs or triplets of tokens that appear next to each other.\nPairs of tokens are known as bigrams, triplets of tokens are known as trigrams, and\nmore generally sequences of tokens are known as n-grams. We can change the range\nof tokens that are considered as features by changing the ngram_range parameter of\nCountVectorizer or TfidfVectorizer. The ngram_range parameter is a tuple, con‐\nBag-of-Words with More Than One Word (n-Grams) \n| \n339\nsisting of the minimum length and the maximum length of the sequences of tokens\nthat are considered. Here is an example on the toy data we used earlier:\nIn[27]:\nprint(\"bards_words:\\n{}\".format(bards_words))\nOut[27]:\nbards_words:\n['The fool doth think he is wise,',\n 'but the wise man knows himself to be a fool']\nThe default is to create one feature per sequence of tokens that is at least one token\nlong and at most one token long, or in other words exactly one token long (single\ntokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:",
    "tokens are also called unigrams):\nIn[28]:\ncv = CountVectorizer(ngram_range=(1, 1)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[28]:\nVocabulary size: 13\nVocabulary:\n['be', 'but', 'doth', 'fool', 'he', 'himself', 'is', 'knows', 'man', 'the',\n 'think', 'to', 'wise']\nTo look only at bigrams—that is, only at sequences of two tokens following each\nother—we can set ngram_range to (2, 2):\nIn[29]:\ncv = CountVectorizer(ngram_range=(2, 2)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[29]:\nVocabulary size: 14\nVocabulary:\n['be fool', 'but the', 'doth think', 'fool doth', 'he is', 'himself to',\n 'is wise', 'knows himself', 'man knows', 'the fool', 'the wise',\n 'think he', 'to be', 'wise man']\nUsing longer sequences of tokens usually results in many more features, and in more\nspecific features. There is no common bigram between the two phrases in\nbard_words:\n340 \n| \nChapter 7: Working with Text Data\nIn[30]:\nprint(\"Transformed data (dense):\\n{}\".format(cv.transform(bards_words).toarray()))\nOut[30]:\nTransformed data (dense):\n[[0 0 1 1 1 0 1 0 0 1 0 1 0 0]\n [1 1 0 0 0 1 0 1 1 0 1 0 1 1]]\nFor most applications, the minimum number of tokens should be one, as single\nwords often capture a lot of meaning. Adding bigrams helps in most cases. Adding\nlonger sequences—up to 5-grams—might help too, but this will lead to an explosion\nof the number of features and might lead to overfitting, as there will be many very\nspecific features. In principle, the number of bigrams could be the number of\nunigrams squared and the number of trigrams could be the number of unigrams to\nthe power of three, leading to very large feature spaces. In practice, the number of\nhigher n-grams that actually appear in the data is much smaller, because of the struc‐\nture of the (English) language, though it is still large.",
    "the power of three, leading to very large feature spaces. In practice, the number of\nhigher n-grams that actually appear in the data is much smaller, because of the struc‐\nture of the (English) language, though it is still large.\nHere is what using unigrams, bigrams, and trigrams on bards_words looks like:\nIn[31]:\ncv = CountVectorizer(ngram_range=(1, 3)).fit(bards_words)\nprint(\"Vocabulary size: {}\".format(len(cv.vocabulary_)))\nprint(\"Vocabulary:\\n{}\".format(cv.get_feature_names()))\nOut[31]:\nVocabulary size: 39\nVocabulary:\n['be', 'be fool', 'but', 'but the', 'but the wise', 'doth', 'doth think',\n 'doth think he', 'fool', 'fool doth', 'fool doth think', 'he', 'he is',\n 'he is wise', 'himself', 'himself to', 'himself to be', 'is', 'is wise',\n 'knows', 'knows himself', 'knows himself to', 'man', 'man knows',\n 'man knows himself', 'the', 'the fool', 'the fool doth', 'the wise',\n 'the wise man', 'think', 'think he', 'think he is', 'to', 'to be',\n 'to be fool', 'wise', 'wise man', 'wise man knows']\nLet’s try out the TfidfVectorizer on the IMDb movie review data and find the best\nsetting of n-gram range using a grid search:\nIn[32]:\npipe = make_pipeline(TfidfVectorizer(min_df=5), LogisticRegression())\n# running the grid search takes a long time because of the\n# relatively large grid and the inclusion of trigrams\nparam_grid = {\"logisticregression__C\": [0.001, 0.01, 0.1, 1, 10, 100],\n              \"tfidfvectorizer__ngram_range\": [(1, 1), (1, 2), (1, 3)]}\ngrid = GridSearchCV(pipe, param_grid, cv=5)\ngrid.fit(text_train, y_train)\nprint(\"Best cross-validation score: {:.2f}\".format(grid.best_score_))\nprint(\"Best parameters:\\n{}\".format(grid.best_params_))\nBag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐",
    "Bag-of-Words with More Than One Word (n-Grams) \n| \n341\nOut[32]:\nBest cross-validation score: 0.91\nBest parameters:\n{'tfidfvectorizer__ngram_range': (1, 3), 'logisticregression__C': 100}\nAs you can see from the results, we improved performance by a bit more than a per‐\ncent by adding bigram and trigram features. We can visualize the cross-validation\naccuracy as a function of the ngram_range and C parameter as a heat map, as we did\nin Chapter 5 (see Figure 7-3):\nIn[33]:\n# extract scores from grid_search\nscores = grid.cv_results_['mean_test_score'].reshape(-1, 3).T\n# visualize heat map\nheatmap = mglearn.tools.heatmap(\n    scores, xlabel=\"C\", ylabel=\"ngram_range\", cmap=\"viridis\", fmt=\"%.3f\",\n    xticklabels=param_grid['logisticregression__C'],\n    yticklabels=param_grid['tfidfvectorizer__ngram_range'])\nplt.colorbar(heatmap)\nFigure 7-3. Heat map visualization of mean cross-validation accuracy as a function of\nthe parameters ngram_range and C\nFrom the heat map we can see that using bigrams increases performance quite a bit,\nwhile adding trigrams only provides a very small benefit in terms of accuracy. To\nunderstand better how the model improved, we can visualize the important coeffi‐\n342 \n| \nChapter 7: Working with Text Data\ncient for the best model, which includes unigrams, bigrams, and trigrams (see\nFigure 7-4):\nIn[34]:\n# extract feature names and coefficients\nvect = grid.best_estimator_.named_steps['tfidfvectorizer']\nfeature_names = np.array(vect.get_feature_names())\ncoef = grid.best_estimator_.named_steps['logisticregression'].coef_\nmglearn.tools.visualize_coefficients(coef, feature_names, n_top_features=40)\nFigure 7-4. Most important features when using unigrams, bigrams, and trigrams with\ntf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a",
    "tf-idf rescaling\nThere are particularly interesting features containing the word “worth” that were not\npresent in the unigram model: \"not worth\" is indicative of a negative review, while\n\"definitely worth\" and \"well worth\" are indicative of a positive review. This is a\nprime example of context influencing the meaning of the word “worth.”\nNext, we’ll visualize only trigrams, to provide further insight into why these features\nare helpful. Many of the useful bigrams and trigrams consist of common words that\nwould not be informative on their own, as in the phrases \"none of the\", \"the only\ngood\", \"on and on\", \"this is one\", \"of the most\", and so on. However, the\nimpact of these features is quite limited compared to the importance of the unigram\nfeatures, as you can see in Figure 7-5:\nIn[35]:\n# find 3-gram features\nmask = np.array([len(feature.split(\" \")) for feature in feature_names]) == 3\n# visualize only 3-gram features\nmglearn.tools.visualize_coefficients(coef.ravel()[mask],\n                                     feature_names[mask], n_top_features=40)\nBag-of-Words with More Than One Word (n-Grams) \n| \n343\nFigure 7-5. Visualization of only the important trigram features of the model\nAdvanced Tokenization, Stemming, and Lemmatization\nAs mentioned previously, the feature extraction in the CountVectorizer and Tfidf\nVectorizer is relatively simple, and much more elaborate methods are possible. One\nparticular step that is often improved in more sophisticated text-processing applica‐\ntions is the first step in the bag-of-words model: tokenization. This step defines what\nconstitutes a word for the purpose of feature extraction.\nWe saw earlier that the vocabulary often contains singular and plural versions of\nsome words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only",
    "some words, as in \"drawback\" and \"drawbacks\", \"drawer\" and \"drawers\", and\n\"drawing\" and \"drawings\". For the purposes of a bag-of-words model, the semantics\nof \"drawback\" and \"drawbacks\" are so close that distinguishing them will only\nincrease overfitting, and not allow the model to fully exploit the training data. Simi‐\nlarly, we found the vocabulary includes words like \"replace\", \"replaced\", \"replace\nment\", \"replaces\", and \"replacing\", which are different verb forms and a noun\nrelating to the verb “to replace.” Similarly to having singular and plural forms of a\nnoun, treating different verb forms and related words as distinct tokens is disadvanta‐\ngeous for building a model that generalizes well.\nThis problem can be overcome by representing each word using its word stem, which\ninvolves identifying (or conflating) all the words that have the same word stem. If this\nis done by using a rule-based heuristic, like dropping common suffixes, it is usually\nreferred to as stemming. If instead a dictionary of known word forms is used (an\nexplicit and human-verified system), and the role of the word in the sentence is taken\ninto account, the process is referred to as lemmatization and the standardized form of\nthe word is referred to as the lemma. Both processing methods, lemmatization and\nstemming, are forms of normalization that try to extract some normal form of a\nword. Another interesting case of normalization is spelling correction, which can be\nhelpful in practice but is outside of the scope of this book.\n344 \n| \nChapter 7: Working with Text Data\n8 For details of the interface, consult the nltk and spacy documentation. We are more interested in the general\nprinciples here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk",
    "principles here.\nTo get a better understanding of normalization, let’s compare a method for stemming\n—the Porter stemmer, a widely used collection of heuristics (here imported from the\nnltk package)—to lemmatization as implemented in the spacy package:8\nIn[36]:\nimport spacy\nimport nltk\n# load spacy's English-language models\nen_nlp = spacy.load('en')\n# instantiate nltk's Porter stemmer\nstemmer = nltk.stem.PorterStemmer()\n# define function to compare lemmatization in spacy with stemming in nltk\ndef compare_normalization(doc):\n    # tokenize document in spacy\n    doc_spacy = en_nlp(doc)\n    # print lemmas found by spacy\n    print(\"Lemmatization:\")\n    print([token.lemma_ for token in doc_spacy])\n    # print tokens found by Porter stemmer\n    print(\"Stemming:\")\n    print([stemmer.stem(token.norm_.lower()) for token in doc_spacy])\nWe will compare lemmatization and the Porter stemmer on a sentence designed to\nshow some of the differences:\nIn[37]:\ncompare_normalization(u\"Our meeting today was worse than yesterday, \"\n                       \"I'm scared of meeting the clients tomorrow.\")\nOut[37]:\nLemmatization:\n['our', 'meeting', 'today', 'be', 'bad', 'than', 'yesterday', ',', 'i', 'be',\n 'scared', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming:\n['our', 'meet', 'today', 'wa', 'wors', 'than', 'yesterday', ',', 'i', \"'m\",\n 'scare', 'of', 'meet', 'the', 'client', 'tomorrow', '.']\nStemming is always restricted to trimming the word to a stem, so \"was\" becomes\n\"wa\", while lemmatization can retrieve the correct base verb form, \"be\". Similarly,\nlemmatization can normalize \"worse\" to \"bad\", while stemming produces \"wors\".\nAnother major difference is that stemming reduces both occurrences of \"meeting\" to\n\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced",
    "\"meet\". Using lemmatization, the first occurrence of \"meeting\" is recognized as a\nAdvanced Tokenization, Stemming, and Lemmatization \n| \n345\nnoun and left as is, while the second occurrence is recognized as a verb and reduced\nto \"meet\". In general, lemmatization is a much more involved process than stem‐\nming, but it usually produces better results than stemming when used for normaliz‐\ning tokens for machine learning.\nWhile scikit-learn implements neither form of normalization, CountVectorizer\nallows specifying your own tokenizer to convert each document into a list of tokens\nusing the tokenizer parameter. We can use the lemmatization from spacy to create a\ncallable that will take a string and produce a list of lemmas:\nIn[38]:\n# Technicality: we want to use the regexp-based tokenizer\n# that is used by CountVectorizer and only use the lemmatization\n# from spacy. To this end, we replace en_nlp.tokenizer (the spacy tokenizer)\n# with the regexp-based tokenization.\nimport re\n# regexp used in CountVectorizer\nregexp = re.compile('(?u)\\\\b\\\\w\\\\w+\\\\b')\n# load spacy language model and save old tokenizer\nen_nlp = spacy.load('en')\nold_tokenizer = en_nlp.tokenizer\n# replace the tokenizer with the preceding regexp\nen_nlp.tokenizer = lambda string: old_tokenizer.tokens_from_list(\n    regexp.findall(string))\n# create a custom tokenizer using the spacy document processing pipeline\n# (now using our own tokenizer)\ndef custom_tokenizer(document):\n    doc_spacy = en_nlp(document, entity=False, parse=False)\n    return [token.lemma_ for token in doc_spacy]\n# define a count vectorizer with the custom tokenizer\nlemma_vect = CountVectorizer(tokenizer=custom_tokenizer, min_df=5)\nLet’s transform the data and inspect the vocabulary size:\nIn[39]:\n# transform text_train using CountVectorizer with lemmatization\nX_train_lemma = lemma_vect.fit_transform(text_train)\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n# standard CountVectorizer for reference",
    "Let’s transform the data and inspect the vocabulary size:\nIn[39]:\n# transform text_train using CountVectorizer with lemmatization\nX_train_lemma = lemma_vect.fit_transform(text_train)\nprint(\"X_train_lemma.shape: {}\".format(X_train_lemma.shape))\n# standard CountVectorizer for reference\nvect = CountVectorizer(min_df=5).fit(text_train)\nX_train = vect.transform(text_train)\nprint(\"X_train.shape: {}\".format(X_train.shape))\n346 \n| \nChapter 7: Working with Text Data\nOut[39]:\nX_train_lemma.shape:  (25000, 21596)\nX_train.shape:  (25000, 27271)\nAs you can see from the output, lemmatization reduced the number of features from\n27,271 (with the standard CountVectorizer processing) to 21,596. Lemmatization\ncan be seen as a kind of regularization, as it conflates certain features. Therefore, we\nexpect lemmatization to improve performance most when the dataset is small. To\nillustrate how lemmatization can help, we will use StratifiedShuffleSplit for\ncross-validation, using only 1% of the data as training data and the rest as test data:\nIn[40]:\n# build a grid search using only 1% of the data as the training set\nfrom sklearn.model_selection import StratifiedShuffleSplit\nparam_grid = {'C': [0.001, 0.01, 0.1, 1, 10]}\ncv = StratifiedShuffleSplit(n_iter=5, test_size=0.99,\n                            train_size=0.01, random_state=0)\ngrid = GridSearchCV(LogisticRegression(), param_grid, cv=cv)\n# perform grid search with standard CountVectorizer\ngrid.fit(X_train, y_train)\nprint(\"Best cross-validation score \"\n      \"(standard CountVectorizer): {:.3f}\".format(grid.best_score_))\n# perform grid search with lemmatization\ngrid.fit(X_train_lemma, y_train)\nprint(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with",
    "print(\"Best cross-validation score \"\n      \"(lemmatization): {:.3f}\".format(grid.best_score_))\nOut[40]:\nBest cross-validation score (standard CountVectorizer): 0.721\nBest cross-validation score (lemmatization): 0.731\nIn this case, lemmatization provided a modest improvement in performance. As with\nmany of the different feature extraction techniques, the result varies depending on\nthe dataset. Lemmatization and stemming can sometimes help in building better (or\nat least more compact) models, so we suggest you give these techniques a try when\ntrying to squeeze out the last bit of performance on a particular task.\nTopic Modeling and Document Clustering\nOne particular technique that is often applied to text data is topic modeling, which is\nan umbrella term describing the task of assigning each document to one or multiple\ntopics, usually without supervision. A good example for this is news data, which\nmight be categorized into topics like “politics,” “sports,” “finance,” and so on. If each\ndocument is assigned a single topic, this is the task of clustering the documents, as\ndiscussed in Chapter 3. If each document can have more than one topic, the task\nTopic Modeling and Document Clustering \n| \n347\n9 There is another machine learning model that is also often abbreviated LDA: Linear Discriminant Analysis, a\nlinear classification model. This leads to quite some confusion. In this book, LDA refers to Latent Dirichlet\nAllocation.\nrelates to the decomposition methods from Chapter 3. Each of the components we\nlearn then corresponds to one topic, and the coefficients of the components in the\nrepresentation of a document tell us how strongly related that document is to a par‐\nticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear",
    "ticular topic. Often, when people talk about topic modeling, they refer to one particu‐\nlar decomposition method called Latent Dirichlet Allocation (often LDA for short).9\nLatent Dirichlet Allocation\nIntuitively, the LDA model tries to find groups of words (the topics) that appear\ntogether frequently. LDA also requires that each document can be understood as a\n“mixture” of a subset of the topics. It is important to understand that for the machine\nlearning model a “topic” might not be what we would normally call a topic in every‐\nday speech, but that it resembles more the components extracted by PCA or NMF\n(which we discussed in Chapter 3), which might or might not have a semantic mean‐\ning. Even if there is a semantic meaning for an LDA “topic”, it might not be some‐\nthing we’d usually call a topic. Going back to the example of news articles, we might\nhave a collection of articles about sports, politics, and finance, written by two specific\nauthors. In a politics article, we might expect to see words like “governor,” “vote,”\n“party,” etc., while in a sports article we might expect words like “team,” “score,” and\n“season.” Words in each of these groups will likely appear together, while it’s less likely\nthat, for example, “team” and “governor” will appear together. However, these are not\nthe only groups of words we might expect to appear together. The two reporters\nmight prefer different phrases or different choices of words. Maybe one of them likes\nto use the word “demarcate” and one likes the word “polarize.” Other “topics” would\nthen be “words often used by reporter A” and “words often used by reporter B,”\nthough these are not topics in the usual sense of the word.\nLet’s apply LDA to our movie review dataset to see how it works in practice. For\nunsupervised text document models, it is often good to remove very common words,\nas they might otherwise dominate the analysis. We’ll remove words that appear in at",
    "Let’s apply LDA to our movie review dataset to see how it works in practice. For\nunsupervised text document models, it is often good to remove very common words,\nas they might otherwise dominate the analysis. We’ll remove words that appear in at\nleast 20 percent of the documents, and we’ll limit the bag-of-words model to the\n10,000 words that are most common after removing the top 20 percent:\nIn[41]:\nvect = CountVectorizer(max_features=10000, max_df=.15)\nX = vect.fit_transform(text_train)\n348 \n| \nChapter 7: Working with Text Data\n10 In fact, NMF and LDA solve quite related problems, and we could also use NMF to extract topics.\nWe will learn a topic model with 10 topics, which is few enough that we can look at all\nof them. Similarly to the components in NMF, topics don’t have an inherent ordering,\nand changing the number of topics will change all of the topics.10 We’ll use the\n\"batch\" learning method, which is somewhat slower than the default (\"online\") but\nusually provides better results, and increase \"max_iter\", which can also lead to better\nmodels:\nIn[42]:\nfrom sklearn.decomposition import LatentDirichletAllocation\nlda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\",\n                                max_iter=25, random_state=0)\n# We build the model and transform the data in one step\n# Computing transform takes some time,\n# and we can save time by doing both at once\ndocument_topics = lda.fit_transform(X)\nLike the decomposition methods we saw in Chapter 3, LatentDirichletAllocation\nhas a components_ attribute that stores how important each word is for each topic.\nThe size of components_ is (n_topics, n_words):\nIn[43]:\nlda.components_.shape\nOut[43]:\n(10, 10000)\nTo understand better what the different topics mean, we will look at the most impor‐\ntant words for each of the topics. The print_topics function provides a nice format‐\nting for these features:\nIn[44]:\n# For each topic (a row in the components_), sort the features (ascending)",
    "Out[43]:\n(10, 10000)\nTo understand better what the different topics mean, we will look at the most impor‐\ntant words for each of the topics. The print_topics function provides a nice format‐\nting for these features:\nIn[44]:\n# For each topic (a row in the components_), sort the features (ascending)\n# Invert rows with [:, ::-1] to make sorting descending\nsorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n# Get the feature names from the vectorizer\nfeature_names = np.array(vect.get_feature_names())\nIn[45]:\n# Print out the 10 topics:\nmglearn.tools.print_topics(topics=range(10), feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=5, n_words=10)\nTopic Modeling and Document Clustering \n| \n349\nOut[45]:\ntopic 0       topic 1       topic 2       topic 3       topic 4\n--------      --------      --------      --------      --------\nbetween       war           funny         show          didn\nyoung         world         worst         series        saw\nfamily        us            comedy        episode       am\nreal          our           thing         tv            thought\nperformance   american      guy           episodes      years\nbeautiful     documentary   re            shows         book\nwork          history       stupid        season        watched\neach          new           actually      new           now\nboth          own           nothing       television    dvd\ndirector      point         want          years         got\ntopic 5       topic 6       topic 7       topic 8       topic 9\n--------      --------      --------      --------      --------\nhorror        kids          cast          performance   house\naction        action        role          role          woman\neffects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife",
    "effects       animation     john          john          gets\nbudget        game          version       actor         killer\nnothing       fun           novel         oscar         girl\noriginal      disney        both          cast          wife\ndirector      children      director      plays         horror\nminutes       10            played        jack          young\npretty        kid           performance   joe           goes\ndoesn         old           mr            performances  around\nJudging from the important words, topic 1 seems to be about historical and war mov‐\nies, topic 2 might be about bad comedies, topic 3 might be about TV series. Topic 4\nseems to capture some very common words, while topic 6 appears to be about child‐\nren’s movies and topic 8 seems to capture award-related reviews. Using only 10 topics,\neach of the topics needs to be very broad, so that they can together cover all the dif‐\nferent kinds of reviews in our dataset.\nNext, we will learn another model, this time with 100 topics. Using more topics\nmakes the analysis much harder, but makes it more likely that topics can specialize to\ninteresting subsets of the data:\nIn[46]:\nlda100 = LatentDirichletAllocation(n_topics=100, learning_method=\"batch\",\n                                   max_iter=25, random_state=0)\ndocument_topics100 = lda100.fit_transform(X)\nLooking at all 100 topics would be a bit overwhelming, so we selected some interest‐\ning and representative topics:\n350 \n| \nChapter 7: Working with Text Data\nIn[47]:\ntopics = np.array([7, 16, 24, 25, 28, 36, 37, 45, 51, 53, 54, 63, 89, 97])\nsorting = np.argsort(lda100.components_, axis=1)[:, ::-1]\nfeature_names = np.array(vect.get_feature_names())\nmglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\nOut[48]:\ntopic 7       topic 16      topic 24      topic 25      topic 28\n--------      --------      --------      --------      --------",
    "mglearn.tools.print_topics(topics=topics, feature_names=feature_names,\n                           sorting=sorting, topics_per_chunk=7, n_words=20)\nOut[48]:\ntopic 7       topic 16      topic 24      topic 25      topic 28\n--------      --------      --------      --------      --------\nthriller      worst         german        car           beautiful\nsuspense      awful         hitler        gets          young\nhorror        boring        nazi          guy           old\natmosphere    horrible      midnight      around        romantic\nmystery       stupid        joe           down          between\nhouse         thing         germany       kill          romance\ndirector      terrible      years         goes          wonderful\nquite         script        history       killed        heart\nbit           nothing       new           going         feel\nde            worse         modesty       house         year\nperformances  waste         cowboy        away          each\ndark          pretty        jewish        head          french\ntwist         minutes       past          take          sweet\nhitchcock     didn          kirk          another       boy\ntension       actors        young         getting       loved\ninteresting   actually      spanish       doesn         girl\nmysterious    re            enterprise    now           relationship\nmurder        supposed      von           night         saw\nending        mean          nazis         right         both\ncreepy        want          spock         woman         simple\ntopic 36      topic 37      topic 41      topic 45      topic 51\n--------      --------      --------      --------      --------\nperformance   excellent     war           music         earth\nrole          highly        american      song          space\nactor         amazing       world         songs         planet\ncast          wonderful     soldiers      rock          superman\nplay          truly         military      band          alien",
    "role          highly        american      song          space\nactor         amazing       world         songs         planet\ncast          wonderful     soldiers      rock          superman\nplay          truly         military      band          alien\nactors        superb        army          soundtrack    world\nperformances  actors        tarzan        singing       evil\nplayed        brilliant     soldier       voice         humans\nsupporting    recommend     america       singer        aliens\ndirector      quite         country       sing          human\noscar         performance   americans     musical       creatures\nroles         performances  during        roll          miike\nactress       perfect       men           fan           monsters\nexcellent     drama         us            metal         apes\nscreen        without       government    concert       clark\nplays         beautiful     jungle        playing       burton\naward         human         vietnam       hear          tim\nwork          moving        ii            fans          outer\nplaying       world         political     prince        men\ngives         recommended   against       especially    moon\nTopic Modeling and Document Clustering \n| \n351\ntopic 53      topic 54      topic 63      topic 89      topic 97\n--------      --------      --------      --------      --------\nscott         money         funny         dead          didn\ngary          budget        comedy        zombie        thought\nstreisand     actors        laugh         gore          wasn\nstar          low           jokes         zombies       ending\nhart          worst         humor         blood         minutes\nlundgren      waste         hilarious     horror        got\ndolph         10            laughs        flesh         felt\ncareer        give          fun           minutes       part\nsabrina       want          re            body          going\nrole          nothing       funniest      living        seemed",
    "dolph         10            laughs        flesh         felt\ncareer        give          fun           minutes       part\nsabrina       want          re            body          going\nrole          nothing       funniest      living        seemed\ntemple        terrible      laughing      eating        bit\nphantom       crap          joke          flick         found\njudy          must          few           budget        though\nmelissa       reviews       moments       head          nothing\nzorro         imdb          guy           gory          lot\ngets          director      unfunny       evil          saw\nbarbra        thing         times         shot          long\ncast          believe       laughed       low           interesting\nshort         am            comedies      fulci         few\nserial        actually      isn           re            half\nThe topics we extracted this time seem to be more specific, though many are hard to\ninterpret. Topic 7 seems to be about horror movies and thrillers; topics 16 and 54\nseem to capture bad reviews, while topic 63 mostly seems to be capturing positive\nreviews of comedies. If we want to make further inferences using the topics that were\ndiscovered, we should confirm the intuition we gained from looking at the highest-\nranking words for each topic by looking at the documents that are assigned to these\ntopics. For example, topic 45 seems to be about music. Let’s check which kinds of\nreviews are assigned to this topic:\nIn[49]:\n# sort by weight of \"music\" topic 45\nmusic = np.argsort(document_topics100[:, 45])[::-1]\n# print the five documents where the topic is most important\nfor i in music[:10]:\n    # pshow first two sentences\n    print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\nOut[49]:\nb'I love this movie and never get tired of watching. The music in it is great.\\n'\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n  band from the 70's decide to give it another try.\\n\"",
    "print(b\".\".join(text_train[i].split(b\".\")[:2]) + b\".\\n\")\nOut[49]:\nb'I love this movie and never get tired of watching. The music in it is great.\\n'\nb\"I enjoyed Still Crazy more than any film I have seen in years. A successful\n  band from the 70's decide to give it another try.\\n\"\nb'Hollywood Hotel was the last movie musical that Busby Berkeley directed for\n  Warner Bros. His directing style had changed or evolved to the point that\n  this film does not contain his signature overhead shots or huge production\n  numbers with thousands of extras.\\n'\nb\"What happens to washed up rock-n-roll stars in the late 1990's?\n  They launch a comeback / reunion tour. At least, that's what the members of\n  Strange Fruit, a (fictional) 70's stadium rock group do.\\n\"\n352 \n| \nChapter 7: Working with Text Data\nb'As a big-time Prince fan of the last three to four years, I really can\\'t\n  believe I\\'ve only just got round to watching \"Purple Rain\". The brand new\n  2-disc anniversary Special Edition led me to buy it.\\n'\nb\"This film is worth seeing alone for Jared Harris' outstanding portrayal\n  of John Lennon. It doesn't matter that Harris doesn't exactly resemble\n  Lennon; his mannerisms, expressions, posture, accent and attitude are\n  pure Lennon.\\n\"\nb\"The funky, yet strictly second-tier British glam-rock band Strange Fruit\n  breaks up at the end of the wild'n'wacky excess-ridden 70's. The individual\n  band members go their separate ways and uncomfortably settle into lackluster\n  middle age in the dull and uneventful 90's: morose keyboardist Stephen Rea\n  winds up penniless and down on his luck, vain, neurotic, pretentious lead\n  singer Bill Nighy tries (and fails) to pursue a floundering solo career,\n  paranoid drummer Timothy Spall resides in obscurity on a remote farm so he\n  can avoid paying a hefty back taxes debt, and surly bass player Jimmy Nail\n  installs roofs for a living.\\n\"\nb\"I just finished reading a book on Anita Loos' work and the photo in TCM",
    "paranoid drummer Timothy Spall resides in obscurity on a remote farm so he\n  can avoid paying a hefty back taxes debt, and surly bass player Jimmy Nail\n  installs roofs for a living.\\n\"\nb\"I just finished reading a book on Anita Loos' work and the photo in TCM\n  Magazine of MacDonald in her angel costume looked great (impressive wings),\n  so I thought I'd watch this movie. I'd never heard of the film before, so I\n  had no preconceived notions about it whatsoever.\\n\"\nb'I love this movie!!! Purple Rain came out the year I was born and it has had\n  my heart since I can remember. Prince is so tight in this movie.\\n'\nb\"This movie is sort of a Carrie meets Heavy Metal. It's about a highschool\n  guy who gets picked on alot and he totally gets revenge with the help of a\n  Heavy Metal ghost.\\n\"\nAs we can see, this topic covers a wide variety of music-centered reviews, from musi‐\ncals, to biographical movies, to some hard-to-specify genre in the last review. Another\ninteresting way to inspect the topics is to see how much weight each topic gets over‐\nall, by summing the document_topics over all reviews. We name each topic by the\ntwo most common words. Figure 7-6 shows the topic weights learned:\nIn[50]:\nfig, ax = plt.subplots(1, 2, figsize=(10, 10))\ntopic_names = [\"{:>2} \".format(i) + \" \".join(words)\n               for i, words in enumerate(feature_names[sorting[:, :2]])]\n# two column bar chart:\nfor col in [0, 1]:\n    start = col * 50\n    end = (col + 1) * 50\n    ax[col].barh(np.arange(50), np.sum(document_topics100, axis=0)[start:end])\n    ax[col].set_yticks(np.arange(50))\n    ax[col].set_yticklabels(topic_names[start:end], ha=\"left\", va=\"top\")\n    ax[col].invert_yaxis()\n    ax[col].set_xlim(0, 2000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()\nTopic Modeling and Document Clustering \n| \n353\nFigure 7-6. Topic weights learned by LDA\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐",
    "ax[col].set_xlim(0, 2000)\n    yax = ax[col].get_yaxis()\n    yax.set_tick_params(pad=130)\nplt.tight_layout()\nTopic Modeling and Document Clustering \n| \n353\nFigure 7-6. Topic weights learned by LDA\nThe most important topics are 97, which seems to consist mostly of stopwords, possi‐\nbly with a slight negative direction; topic 16, which is clearly about bad reviews; fol‐\nlowed by some genre-specific topics and 36 and 37, both of which seem to contain\nlaudatory words.\nIt seems like LDA mostly discovered two kind of topics, genre-specific and rating-\nspecific, in addition to several more unspecific topics. This is an interesting discovery,\nas most reviews are made up of some movie-specific comments and some comments\nthat justify or emphasize the rating.\nTopic models like LDA are interesting methods to understand large text corpora in\nthe absence of labels—or, as here, even if labels are available. The LDA algorithm is\nrandomized, though, and changing the random_state parameter can lead to quite\n354 \n| \nChapter 7: Working with Text Data\ndifferent outcomes. While identifying topics can be helpful, any conclusions you\ndraw from an unsupervised model should be taken with a grain of salt, and we rec‐\nommend verifying your intuition by looking at the documents in a specific topic. The\ntopics produced by the LDA.transform method can also sometimes be used as a com‐\npact representation for supervised learning. This is particularly helpful when few\ntraining examples are available.\nSummary and Outlook\nIn this chapter we talked about the basics of processing text, also known as natural\nlanguage processing (NLP), with an example application classifying movie reviews.\nThe tools discussed here should serve as a great starting point when trying to process\ntext data. In particular for text classification tasks such as spam and fraud detection\nor sentiment analysis, bag-of-words representations provide a simple and powerful",
    "The tools discussed here should serve as a great starting point when trying to process\ntext data. In particular for text classification tasks such as spam and fraud detection\nor sentiment analysis, bag-of-words representations provide a simple and powerful\nsolution. As is often the case in machine learning, the representation of the data is key\nin NLP applications, and inspecting the tokens and n-grams that are extracted can\ngive powerful insights into the modeling process. In text-processing applications, it is\noften possible to introspect models in a meaningful way, as we saw in this chapter, for\nboth supervised and unsupervised tasks. You should take full advantage of this ability\nwhen using NLP-based methods in practice.\nNatural language and text processing is a large research field, and discussing the\ndetails of advanced methods is far beyond the scope of this book. If you want to learn\nmore, we recommend the O’Reilly book Natural Language Processing with Python by\nSteven Bird, Ewan Klein, and Edward Loper, which provides an overview of NLP\ntogether with an introduction to the nltk Python package for NLP. Another great and\nmore conceptual book is the standard reference Introduction to Information Retrieval\nby Christopher Manning, Prabhakar Raghavan, and Hinrich Schütze, which describes\nfundamental algorithms in information retrieval, NLP, and machine learning. Both\nbooks have online versions that can be accessed free of charge. As we discussed ear‐\nlier, the classes CountVectorizer and TfidfVectorizer only implement relatively\nsimple text-processing methods. For more advanced text-processing methods, we\nrecommend the Python packages spacy (a relatively new but very efficient and well-\ndesigned package), nltk (a very well-established and complete but somewhat dated\nlibrary), and gensim (an NLP package with an emphasis on topic modeling).\nThere have been several very exciting new developments in text processing in recent",
    "designed package), nltk (a very well-established and complete but somewhat dated\nlibrary), and gensim (an NLP package with an emphasis on topic modeling).\nThere have been several very exciting new developments in text processing in recent\nyears, which are outside of the scope of this book and relate to neural networks. The\nfirst is the use of continuous vector representations, also known as word vectors or\ndistributed word representations, as implemented in the word2vec library. The origi‐\nnal paper “Distributed Representations of Words and Phrases and Their Composi‐\ntionality” by Thomas Mikolov et al. is a great introduction to the subject. Both spacy\nSummary and Outlook \n| \n355\nand gensim provide functionality for the techniques discussed in this paper and its\nfollow-ups.\nAnother direction in NLP that has picked up momentum in recent years is the use of\nrecurrent neural networks (RNNs) for text processing. RNNs are a particularly power‐\nful type of neural network that can produce output that is again text, in contrast to\nclassification models that can only assign class labels. The ability to produce text as\noutput makes RNNs well suited for automatic translation and summarization. An\nintroduction to the topic can be found in the relatively technical paper “Sequence to\nSequence Learning with Neural Networks” by Ilya Suskever, Oriol Vinyals, and Quoc\nLe. A more practical tutorial using the tensorflow framework can be found on the\nTensorFlow website.\n356 \n| \nChapter 7: Working with Text Data\nCHAPTER 8\nWrapping Up\nYou now know how to apply the important machine learning algorithms for super‐\nvised and unsupervised learning, which allow you to solve a wide variety of machine\nlearning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.",
    "learning problems. Before we leave you to explore all the possibilities that machine\nlearning offers, we want to give you some final words of advice, point you toward\nsome additional resources, and give you suggestions on how you can further improve\nyour machine learning and data science skills.\nApproaching a Machine Learning Problem\nWith all the great methods that we introduced in this book now at your fingertips, it\nmay be tempting to jump in and start solving your data-related problem by just run‐\nning your favorite algorithm. However, this is not usually a good way to begin your\nanalysis. The machine learning algorithm is usually only a small part of a larger data\nanalysis and decision-making process. To make effective use of machine learning, we\nneed to take a step back and consider the problem at large. First, you should think\nabout what kind of question you want to answer. Do you want to do exploratory anal‐\nysis and just see if you find something interesting in the data? Or do you already have\na particular goal in mind? Often you will start with a goal, like detecting fraudulent\nuser transactions, making movie recommendations, or finding unknown planets. If\nyou have such a goal, before building a system to achieve it, you should first think\nabout how to define and measure success, and what the impact of a successful solu‐\ntion would be to your overall business or research goals. Let’s say your goal is fraud\ndetection.\n357\nThen the following questions open up:\n• How do I measure if my fraud prediction is actually working?\n• Do I have the right data to evaluate an algorithm?\n• If I am successful, what will be the business impact of my solution?\nAs we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I",
    "As we discussed in Chapter 5, it is best if you can measure the performance of your\nalgorithm directly using a business metric, like increased profit or decreased losses.\nThis is often hard to do, though. A question that can be easier to answer is “What if I\nbuilt the perfect model?” If perfectly detecting any fraud will save your company $100\na month, these possible savings will probably not be enough to warrant the effort of\nyou even starting to develop an algorithm. On the other hand, if the model might\nsave your company tens of thousands of dollars every month, the problem might be\nworth exploring.\nSay you’ve defined the problem to solve, you know a solution might have a significant\nimpact for your project, and you’ve ensured that you have the right information to\nevaluate success. The next steps are usually acquiring the data and building a working\nprototype. In this book we have talked about many models you can employ, and how\nto properly evaluate and tune these models. While trying out models, though, keep in\nmind that this is only a small part of a larger data science workflow, and model build‐\ning is often part of a feedback circle of collecting new data, cleaning data, building\nmodels, and analyzing the models. Analyzing the mistakes a model makes can often\nbe informative about what is missing in the data, what additional data could be col‐\nlected, or how the task could be reformulated to make machine learning more effec‐\ntive. Collecting more or different data or changing the task formulation slightly might\nprovide a much higher payoff than running endless grid searches to tune parameters.\nHumans in the Loop\nYou should also consider if and how you should have humans in the loop. Some pro‐\ncesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need",
    "cesses (like pedestrian detection in a self-driving car) need to make immediate deci‐\nsions. Others might not need immediate responses, and so it can be possible to have\nhumans confirm uncertain decisions. Medical applications, for example, might need\nvery high levels of precision that possibly cannot be achieved by a machine learning\nalgorithm alone. But if an algorithm can make 90 percent, 50 percent, or maybe even\njust 10 percent of decisions automatically, that might already increase response time\nor reduce cost. Many applications are dominated by “simple cases,” for which an algo‐\nrithm can make a decision, with relatively few “complicated cases,” which can be\nrerouted to a human.\n358 \n| \nChapter 8: Wrapping Up\nFrom Prototype to Production\nThe tools we’ve discussed in this book are great for many machine learning applica‐\ntions, and allow very quick analysis and prototyping. Python and scikit-learn are\nalso used in production systems in many organizations—even very large ones like\ninternational banks and global social media companies. However, many companies\nhave complex infrastructure, and it is not always easy to include Python in these sys‐\ntems. That is not necessarily a problem. In many companies, the data analytics teams\nwork with languages like Python and R that allow the quick testing of ideas, while\nproduction teams work with languages like Go, Scala, C++, and Java to build robust,\nscalable systems. Data analysis has different requirements from building live services,\nand so using different languages for these tasks makes sense. A relatively common\nsolution is to reimplement the solution that was found by the analytics team inside\nthe larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is",
    "the larger framework, using a high-performance language. This can be easier than\nembedding a whole library or programming language and converting from and to the\ndifferent data formats.\nRegardless of whether you can use scikit-learn in a production system or not, it is\nimportant to keep in mind that production systems have different requirements from\none-off analysis scripts. If an algorithm is deployed into a larger system, software\nengineering aspects like reliability, predictability, runtime, and memory requirements\ngain relevance. Simplicity is key in providing machine learning systems that perform\nwell in these areas. Critically inspect each part of your data processing and prediction\npipeline and ask yourself how much complexity each step creates, how robust each\ncomponent is to changes in the data or compute infrastructure, and if the benefit of\neach component warrants the complexity. If you are building involved machine learn‐\ning systems, we highly recommend reading the paper “Machine Learning: The High\nInterest Credit Card of Technical Debt”, published by researchers in Google’s\nmachine learning team. The paper highlights the trade-off in creating and maintain‐\ning machine learning software in production at a large scale. While the issue of tech‐\nnical debt is particularly pressing in large-scale and long-term projects, the lessons\nlearned can help us build better software even for short-lived and smaller systems.\nTesting Production Systems\nIn this book, we covered how to evaluate algorithmic predictions based on a test set\nthat we collected beforehand. This is known as offline evaluation. If your machine\nlearning system is user-facing, this is only the first step in evaluating an algorithm,\nthough. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change",
    "though. The next step is usually online testing or live testing, where the consequences\nof employing the algorithm in the overall system are evaluated. Changing the recom‐\nmendations or search results users are shown by a website can drastically change\ntheir behavior and lead to unexpected consequences. To protect against these sur‐\nprises, most user-facing services employ A/B testing, a form of blind user study. In\nFrom Prototype to Production \n| \n359\nA/B testing, without their knowledge a selected portion of users will be provided with\na website or service using algorithm A, while the rest of the users will be provided\nwith algorithm B. For both groups, relevant success metrics will be recorded for a set\nperiod of time. Then, the metrics of algorithm A and algorithm B will be compared,\nand a selection between the two approaches will be made according to these metrics.\nUsing A/B testing enables us to evaluate the algorithms “in the wild,” which might\nhelp us to discover unexpected consequences when users are interacting with our\nmodel. Often A is a new model, while B is the established system. There are more\nelaborate mechanisms for online testing that go beyond A/B testing, such as bandit\nalgorithms. A great introduction to this subject can be found in the book Bandit Algo‐\nrithms for Website Optimization by John Myles White (O’Reilly). \nBuilding Your Own Estimator\nThis book has covered a variety of tools and algorithms implemented in scikit-\nlearn that can be used on a wide range of tasks. However, often there will be some\nparticular processing you need to do for your data that is not implemented in\nscikit-learn. It may be enough to just preprocess your data before passing it to your\nscikit-learn model or pipeline. However, if your preprocessing is data dependent,\nand you want to apply a grid search or cross-validation, things become trickier.\nIn Chapter 6 we discussed the importance of putting all data-dependent processing",
    "scikit-learn model or pipeline. However, if your preprocessing is data dependent,\nand you want to apply a grid search or cross-validation, things become trickier.\nIn Chapter 6 we discussed the importance of putting all data-dependent processing\ninside the cross-validation loop. So how can you use your own processing together\nwith the scikit-learn tools? There is a simple solution: build your own estimator!\nImplementing an estimator that is compatible with the scikit-learn interface, so\nthat it can be used with Pipeline, GridSearchCV, and cross_val_score, is quite easy.\nYou can find detailed instructions in the scikit-learn documentation, but here is\nthe gist. The simplest way to implement a transformer class is by inheriting from\nBaseEstimator and TransformerMixin, and then implementing the __init__, fit,\nand predict functions like this:\n360 \n| \nChapter 8: Wrapping Up\nIn[1]:\nfrom sklearn.base import BaseEstimator, TransformerMixin\nclass MyTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, first_parameter=1, second_parameter=2):\n        # All parameters must be specified in the __init__ function\n        self.first_parameter = 1\n        self.second_parameter = 2\n    def fit(self, X, y=None):\n        # fit should only take X and y as parameters\n        # Even if your model is unsupervised, you need to accept a y argument!\n        # Model fitting code goes here\n        print(\"fitting the model right here\")\n        # fit returns self\n        return self\n    def transform(self, X):\n        # transform takes as parameter only X\n        # Apply some transformation to X\n        X_transformed = X + 1\n        return X_transformed\nImplementing a classifier or regressor works similarly, only instead of Transformer\nMixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\nof implementing transform, you would implement predict.\nAs you can see from the example given here, implementing your own estimator",
    "Implementing a classifier or regressor works similarly, only instead of Transformer\nMixin you need to inherit from ClassifierMixin or RegressorMixin. Also, instead\nof implementing transform, you would implement predict.\nAs you can see from the example given here, implementing your own estimator\nrequires very little code, and most scikit-learn users build up a collection of cus‐\ntom models over time.\nWhere to Go from Here\nThis book provides an introduction to machine learning and will make you an effec‐\ntive practitioner. However, if you want to further your machine learning skills, here\nare some suggestions of books and more specialized resources to investigate to dive\ndeeper.\nTheory\nIn this book, we tried to provide an intuition of how the most common machine\nlearning algorithms work, without requiring a strong foundation in mathematics or\ncomputer science. However, many of the models we discussed use principles from\nprobability theory, linear algebra, and optimization. While it is not necessary to\nunderstand all the details of how these algorithms are implemented, we think that\nWhere to Go from Here \n| \n361\n1 Andreas might not be entirely objective in this matter.\nknowing some of the theory behind the algorithms will make you a better data scien‐\ntist. There have been many good books written about the theory of machine learning,\nand if we were able to excite you about the possibilities that machine learning opens\nup, we suggest you pick up at least one of them and dig deeper. We already men‐\ntioned Hastie, Tibshirani, and Friedman’s book The Elements of Statistical Learning in\nthe Preface, but it is worth repeating this recommendation here. Another quite acces‐\nsible book, with accompanying Python code, is Machine Learning: An Algorithmic\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop",
    "sible book, with accompanying Python code, is Machine Learning: An Algorithmic\nPerspective by Stephen Marsland (Chapman and Hall/CRC). Two other highly recom‐\nmended classics are Pattern Recognition and Machine Learning by Christopher Bishop\n(Springer), a book that emphasizes a probabilistic framework, and Machine Learning:\nA Probabilistic Perspective by Kevin Murphy (MIT Press), a comprehensive (read:\n1,000+ pages) dissertation on machine learning methods featuring in-depth discus‐\nsions of state-of-the-art approaches, far beyond what we could cover in this book.\nOther Machine Learning Frameworks and Packages\nWhile scikit-learn is our favorite package for machine learning1 and Python is our\nfavorite language for machine learning, there are many other options out there.\nDepending on your needs, Python and scikit-learn might not be the best fit for\nyour particular situation. Often using Python is great for trying out and evaluating\nmodels, but larger web services and applications are more commonly written in Java\nor C++, and integrating into these systems might be necessary for your model to be\ndeployed. Another reason you might want to look beyond scikit-learn is if you are\nmore interested in statistical modeling and inference than prediction. In this case,\nyou should consider the statsmodel package for Python, which implements several\nlinear models with a more statistically minded interface. If you are not married to\nPython, you might also consider using R, another lingua franca of data scientists. R is\na language designed specifically for statistical analysis and is famous for its excellent\nvisualization capabilities and the availability of many (often highly specialized) statis‐\ntical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and",
    "tical modeling packages.\nAnother popular machine learning package is vowpal wabbit (often called vw to\navoid possible tongue twisting), a highly optimized machine learning package written\nin C++ with a command-line interface. vw is particularly useful for large datasets and\nfor streaming data. For running machine learning algorithms distributed on a cluster,\none of the most popular solutions at the time of writing is mllib, a Scala library built\non top of the spark distributed computing environment.\n362 \n| \nChapter 8: Wrapping Up\nRanking, Recommender Systems, and Other Kinds of Learning\nBecause this is an introductory book, we focused on the most common machine\nlearning tasks: classification and regression in supervised learning, and clustering and\nsignal decomposition in unsupervised learning. There are many more kinds of\nmachine learning out there, with many important applications. There are two partic‐\nularly important topics that we did not cover in this book. The first is ranking, in\nwhich we want to retrieve answers to a particular query, ordered by their relevance.\nYou’ve probably already used a ranking system today; this is how search engines\noperate. You input a search query and obtain a sorted list of answers, ranked by how\nrelevant they are. A great introduction to ranking is provided in Manning, Raghavan,\nand Schütze’s book Introduction to Information Retrieval. The second topic is recom‐\nmender systems, which provide suggestions to users based on their preferences.\nYou’ve probably encountered recommender systems under headings like “People You\nMay Know,” “Customers Who Bought This Item Also Bought,” or “Top Picks for\nYou.” There is plenty of literature on the topic, and if you want to dive right in you\nmight be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another",
    "might be interested in the now classic “Netflix prize challenge”, in which the Netflix\nvideo streaming site released a large dataset of movie preferences and offered a prize\nof $1 million to the team that could provide the best recommendations. Another\ncommon application is prediction of time series (like stock prices), which also has a\nwhole body of literature devoted to it. There are many more machine learning tasks\nout there—much more than we can list here—and we encourage you to seek out\ninformation from books, research papers, and online communities to find the para‐\ndigms that best apply to your situation.\nProbabilistic Modeling, Inference, and Probabilistic Programming\nMost machine learning packages provide predefined machine learning models that\napply one particular algorithm. However, many real-world problems have a particular\nstructure that, when properly incorporated into the model, can yield much better-\nperforming predictions. Often, the structure of a particular problem can be expressed\nusing the language of probability theory. Such structure commonly arises from hav‐\ning a mathematical model of the situation for which you want to predict. To under‐\nstand what we mean by a structured problem, consider the following example.\nLet’s say you want to build a mobile application that provides a very detailed position\nestimate in an outdoor space, to help users navigate a historical site. A mobile phone\nprovides many sensors to help you get precise location measurements, like the GPS,\naccelerometer, and compass. You also have an exact map of the area. This problem is\nhighly structured. You know where the paths and points of interest are from your\nmap. You also have rough positions from the GPS, and the accelerometer and com‐\npass in the user’s device provide you with very precise relative measurements. But\nthrowing these all together into a black-box machine learning system to predict posi‐",
    "map. You also have rough positions from the GPS, and the accelerometer and com‐\npass in the user’s device provide you with very precise relative measurements. But\nthrowing these all together into a black-box machine learning system to predict posi‐\ntions might not be the best idea. This would throw away all the information you\nWhere to Go from Here \n| \n363\n2 A preprint of Deep Learning can be viewed at http://www.deeplearningbook.org/.\nalready know about how the real world works. If the compass and accelerometer tell\nyou a user is going north, and the GPS is telling you the user is going south, you\nprobably can’t trust the GPS. If your position estimate tells you the user just walked\nthrough a wall, you should also be highly skeptical. It’s possible to express this situa‐\ntion using a probabilistic model, and then use machine learning or probabilistic\ninference to find out how much you should trust each measurement, and to reason\nabout what the best guess for the location of a user is.\nOnce you’ve expressed the situation and your model of how the different factors work\ntogether in the right way, there are methods to compute the predictions using these\ncustom models directly. The most general of these methods are called probabilistic\nprogramming languages, and they provide a very elegant and compact way to express\na learning problem. Examples of popular probabilistic programming languages are\nPyMC (which can be used in Python) and Stan (a framework that can be used from\nseveral languages, including Python). While these packages require some under‐\nstanding of probability theory, they simplify the creation of new models significantly.\nNeural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and",
    "Neural Networks\nWhile we touched on the subject of neural networks briefly in Chapters 2 and 7, this\nis a rapidly evolving area of machine learning, with innovations and new applications\nbeing announced on a weekly basis. Recent breakthroughs in machine learning and\nartificial intelligence, such as the victory of the Alpha Go program against human\nchampions in the game of Go, the constantly improving performance of speech\nunderstanding, and the availability of near-instantaneous speech translation, have all\nbeen driven by these advances. While the progress in this field is so fast-paced that\nany current reference to the state of the art will soon be outdated, the recent book\nDeep Learning by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press)\nis a comprehensive introduction into the subject.2\nScaling to Larger Datasets\nIn this book, we always assumed that the data we were working with could be stored\nin a NumPy array or SciPy sparse matrix in memory (RAM). Even though modern\nservers often have hundreds of gigabytes (GB) of RAM, this is a fundamental restric‐\ntion on the size of data you can work with. Not everybody can afford to buy such a\nlarge machine, or even to rent one from a cloud provider. In most applications, the\ndata that is used to build a machine learning system is relatively small, though, and\nfew machine learning datasets consist of hundreds of gigabites of data or more. This\nmakes expanding your RAM or renting a machine from a cloud provider a viable sol‐\nution in many cases. If you need to work with terabytes of data, however, or you need\n364 \n| \nChapter 8: Wrapping Up\nto process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single",
    "to process large amounts of data on a budget, there are two basic strategies: out-of-\ncore learning and parallelization over a cluster.\nOut-of-core learning describes learning from data that cannot be stored in main\nmemory, but where the learning takes place on a single computer (or even a single\nprocessor within a computer). The data is read from a source like the hard disk or the\nnetwork either one sample at a time or in chunks of multiple samples, so that each\nchunk fits into RAM. This subset of the data is then processed and the model is upda‐\nted to reflect what was learned from the data. Then, this chunk of the data is dis‐\ncarded and the next bit of data is read. Out-of-core learning is implemented for some\nof the models in scikit-learn, and you can find details on it in the online user\nguide. Because out-of-core learning requires all of the data to be processed by a single\ncomputer, this can lead to long runtimes on very large datasets. Also, not all machine\nlearning algorithms can be implemented in this way.\nThe other strategy for scaling is distributing the data over multiple machines in a\ncompute cluster, and letting each computer process part of the data. This can be\nmuch faster for some models, and the size of the data that can be processed is only\nlimited by the size of the cluster. However, such computations often require relatively\ncomplex infrastructure. One of the most popular distributed computing platforms at\nthe moment is the spark platform built on top of Hadoop. spark includes some\nmachine learning functionality within the MLLib package. If your data is already on a\nHadoop filesystem, or you are already using spark to preprocess your data, this might\nbe the easiest option. If you don’t already have such infrastructure in place, establish‐\ning and integrating a spark cluster might be too large an effort, however. The vw\npackage mentioned earlier provides some distributed features and might be a better\nsolution in this case.\nHoning Your Skills",
    "be the easiest option. If you don’t already have such infrastructure in place, establish‐\ning and integrating a spark cluster might be too large an effort, however. The vw\npackage mentioned earlier provides some distributed features and might be a better\nsolution in this case.\nHoning Your Skills\nAs with many things in life, only practice will allow you to become an expert in the\ntopics we covered in this book. Feature extraction, preprocessing, visualization, and\nmodel building can vary widely between different tasks and different datasets. Maybe\nyou are lucky enough to already have access to a variety of datasets and tasks. If you\ndon’t already have a task in mind, a good place to start is machine learning competi‐\ntions, in which a dataset with a given task is published, and teams compete in creating\nthe best possible predictions. Many companies, nonprofit organizations, and univer‐\nsities host these competitions. One of the most popular places to find them is Kaggle,\na website that regularly holds data science competitions, some of which have substan‐\ntial prize money attached.\nThe Kaggle forums are also a good source of information about the latest tools and\ntricks in machine learning, and a wide range of datasets are available on the site. Even\nmore datasets with associated tasks can be found on the OpenML platform, which\nWhere to Go from Here \n| \n365\nhosts over 20,000 datasets with over 50,000 associated machine learning tasks. Work‐\ning with these datasets can provide a great opportunity to practice your machine\nlearning skills. A disadvantage of competitions is that they already provide a particu‐\nlar metric to optimize, and usually a fixed, preprocessed dataset. Keep in mind that\ndefining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion",
    "defining the problem and collecting the data are also important aspects of real-world\nproblems, and that representing the problem in the right way might be much more\nimportant than squeezing the last percent of accuracy out of a classifier.\nConclusion\nWe hope we have convinced you of the usefulness of machine learning in a wide vari‐\nety of applications, and how easily machine learning can be implemented in practice.\nKeep digging into the data, and don’t lose sight of the larger picture.\n366 \n| \nChapter 8: Wrapping Up\nIndex\nA\nA/B testing, 359\naccuracy, 22, 282\nacknowledgments, xi\nadjusted rand index (ARI), 191\nagglomerative clustering\nevaluating and comparing, 191\nexample of, 183\nhierarchical clustering, 184\nlinkage choices, 182\nprinciple of, 182\nalgorithm chains and pipelines, 305-321\nbuilding pipelines, 308\nbuilding pipelines with make_pipeline,\n313-316\ngrid search preprocessing steps, 317\ngrid-searching for model selection, 319\nimportance of, 305\noverview of, 320\nparameter selection with preprocessing, 306\npipeline interface, 312\nusing pipelines in grid searches, 309-311\nalgorithm parameter, 118\nalgorithms (see also models; problem solving)\nevaluating, 28\nminimal code to apply to algorithm, 24\nsample datasets, 30-34\nscaling\nMinMaxScaler, 102, 135-139, 190, 230,\n308, 319\nNormalizer, 134\nRobustScaler, 133\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nsupervised, classification\ndecision trees, 70-83\ngradient boosting, 88-91, 119, 124\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear SVMs, 56\nlogistic regression, 56\nnaive Bayes, 68-70\nneural networks, 104-119\nrandom forests, 84-88\nsupervised, regression\ndecision trees, 70-83\ngradient boosting, 88-91\nk-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181",
    "k-nearest neighbors, 40\nLasso, 53-55\nlinear regression (OLS), 47, 220-229\nneural networks, 104-119\nrandom forests, 84-88\nRidge, 49-55, 67, 112, 231, 234, 310,\n317-319\nunsupervised, clustering\nagglomerative clustering, 182-187,\n191-195, 203-207\nDBSCAN, 187-190\nk-means, 168-181\nunsupervised, manifold learning\nt-SNE, 163-168\nunsupervised, signal decomposition\nnon-negative matrix factorization,\n156-163\nprincipal component analysis, 140-155\nalpha parameter in linear models, 50\nAnaconda, 6\n367\nanalysis of variance (ANOVA), 236\narea under the curve (AUC), 294-296\nattributions, x\naverage precision, 292\nB\nbag-of-words representation\napplying to movie reviews, 330-334\napplying to toy dataset, 329\nmore than one word (n-grams), 339-344\nsteps in computing, 327\nBernoulliNB, 68\nbigrams, 339\nbinary classification, 25, 56, 276-296\nbinning, 144, 220-224\nbootstrap samples, 84\nBoston Housing dataset, 34\nboundary points, 188\nBunch objects, 33\nbusiness metric, 275, 358\nC\nC parameter in SVC, 99\ncalibration, 288\ncancer dataset, 32\ncategorical features\ncategorical data, defined, 324\ndefined, 211\nencoded as numbers, 218\nexample of, 212\nrepresentation in training and test sets, 217\nrepresenting using one-hot-encoding, 213\ncategorical variables (see categorical features)\nchaining (see algorithm chains and pipelines)\nclass labels, 25\nclassification problems\nbinary vs. multiclass, 25\nexamples of, 26\ngoals for, 25\niris classification example, 14\nk-nearest neighbors, 35\nlinear models, 56\nnaive Bayes classifiers, 68\nvs. regression problems, 26\nclassifiers\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLinearSVC, 56-59, 65, 67, 68\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131",
    "LogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nMLPClassifier, 107-119\nnaive Bayes, 68-70\nSVC, 56, 100, 134, 139, 260, 269-272, 273,\n305-309, 313-320\nuncertainty estimates from, 119-127\ncluster centers, 168\nclustering algorithms\nagglomerative clustering, 182-187\napplications for, 131\ncomparing on faces dataset, 195-207\nDBSCAN, 187-190\nevaluating with ground truth, 191-193\nevaluating without ground truth, 193-195\ngoals of, 168\nk-means clustering, 168-181\nsummary of, 207\ncode examples\ndownloading, x\npermission for use, x\ncoef_ attribute, 47, 50\ncomments and questions, xi\ncompetitions, 365\nconflation, 344\nconfusion matrices, 279-286\ncontext, 343\ncontinuous features, 211, 218\ncore samples/core points, 187\ncorpus, 325\ncos function, 232\nCountVectorizer, 334\ncross-validation\nanalyzing results of, 267-271\nbenefits of, 254\ncross-validation splitters, 256\ngrid search and, 263-275\nin scikit-learn, 253\nleave-one-out cross-validation, 257\nnested, 272\nparallelizing with grid search, 274\nprinciple of, 252\npurpose of, 254\nshuffle-split cross-validation, 258\nstratified k-fold, 254-256\nwith groups, 259\ncross_val_score function, 254, 307\n368 \n| \nIndex\nD\ndata points, defined, 4\ndata representation, 211-250 (see also feature\nextraction/feature engineering; text data)\nautomatic feature selection, 236-241\nbinning and, 220-224\ncategorical features, 212-220\neffect on model performance, 211\ninteger features, 218\nmodel complexity vs. dataset size, 29\noverview of, 250\ntable analogy, 4\nin training vs. test sets, 217\nunderstanding your data, 4\nunivariate nonlinear transformations,\n232-236\ndata transformations, 134\n(see also preprocessing)\ndata-driven research, 1\nDBSCAN\nevaluating and comparing, 191-207\nparameters, 189\nprinciple of, 187\nreturned cluster assignments, 190\nstrengths and weaknesses, 187\ndecision boundaries, 37, 56\ndecision function, 120\ndecision trees\nanalyzing, 76\nbuilding, 71\ncontrolling complexity of, 74\ndata representation and, 220-224\nfeature importance in, 77",
    "parameters, 189\nprinciple of, 187\nreturned cluster assignments, 190\nstrengths and weaknesses, 187\ndecision boundaries, 37, 56\ndecision function, 120\ndecision trees\nanalyzing, 76\nbuilding, 71\ncontrolling complexity of, 74\ndata representation and, 220-224\nfeature importance in, 77\nif/else structure of, 70\nparameters, 82\nvs. random forests, 83\nstrengths and weaknesses, 83\ndecision_function, 286\ndeep learning (see neural networks)\ndendrograms, 184\ndense regions, 187\ndimensionality reduction, 141, 156\ndiscrete features, 211\ndiscretization, 220-224\ndistributed computing, 362\ndocument clustering, 347\ndocuments, defined, 325\ndual_coef_ attribute, 98\nE\neigenfaces, 147\nembarrassingly parallel, 274\nencoding, 328\nensembles\ndefined, 83\ngradient boosted regression trees, 88-92\nrandom forests, 83-88\nEnthought Canopy, 6\nestimators, 21, 360\nestimator_ attribute of RFECV, 85\nevaluation metrics and scoring\nfor binary classification, 276-296\nfor multiclass classification, 296-299\nmetric selection, 275\nmodel selection and, 300\nregression metrics, 299\ntesting production systems, 359\nexp function, 232\nexpert knowledge, 242-250\nF\nf(x)=y formula, 18\nfacial recognition, 147, 157\nfactor analysis (FA), 163\nfalse positive rate (FPR), 292\nfalse positive/false negative errors, 277\nfeature extraction/feature engineering, 211-250\n(see also data representation; text data)\naugmenting data with, 211\nautomatic feature selection, 236-241\ncategorical features, 212-220\ncontinuous vs. discrete features, 211\ndefined, 4, 34, 211\ninteraction features, 224-232\nwith non-negative matrix factorization, 156\noverview of, 250\npolynomial features, 224-232\nwith principal component analysis, 147\nunivariate nonlinear transformations,\n232-236\nusing expert knowledge, 242-250\nfeature importance, 77\nfeatures, defined, 4\nfeature_names attribute, 33\nfeed-forward neural networks, 104\nfit method, 21, 68, 119, 135\nfit_transform method, 138\nfloating-point numbers, 26\nIndex \n| \n369\nfolds, 252\nforge dataset, 30\nframeworks, 362",
    "232-236\nusing expert knowledge, 242-250\nfeature importance, 77\nfeatures, defined, 4\nfeature_names attribute, 33\nfeed-forward neural networks, 104\nfit method, 21, 68, 119, 135\nfit_transform method, 138\nfloating-point numbers, 26\nIndex \n| \n369\nfolds, 252\nforge dataset, 30\nframeworks, 362\nfree string data, 324\nfreeform text data, 325\nG\ngamma parameter, 100\nGaussian kernels of SVC, 97, 100\nGaussianNB, 68\ngeneralization\nbuilding models for, 26\ndefined, 17\nexamples of, 27\nget_dummies function, 218\nget_support method of feature selection, 237\ngradient boosted regression trees\nfor feature selection, 220-224\nlearning_rate parameter, 89\nparameters, 91\nvs. random forests, 88\nstrengths and weaknesses, 91\ntraining set accuracy, 90\ngraphviz module, 76\ngrid search\naccessing pipeline attributes, 315\nalternate strategies for, 272\navoiding overfitting, 261\nmodel selection with, 319\nnested cross-validation, 272\nparallelizing with cross-validation, 274\npipeline preprocessing, 317\nsearching non-grid spaces, 271\nsimple example of, 261\ntuning parameters with, 260\nusing pipelines in, 309-311\nwith cross-validation, 263-275\nGridSearchCV\nbest_estimator_ attribute, 267\nbest_params_ attribute, 266\nbest_score_ attribute, 266\nH\nhandcoded rules, disadvantages of, 1\nheat maps, 146\nhidden layers, 106\nhidden units, 105\nhierarchical clustering, 184\nhigh recall, 293\nhigh-dimensional datasets, 32\nhistograms, 144\nhit rate, 283\nhold-out sets, 17\nhuman involvement/oversight, 358\nI\nimbalanced datasets, 277\nindependent component analysis (ICA), 163\ninference, 363\ninformation leakage, 310\ninformation retrieval (IR), 325\ninteger features, 218\n\"intelligent\" applications, 1\ninteractions, 34, 224-232\nintercept_ attribute, 47\niris classification application\ndata inspection, 19\ndataset for, 14\ngoals for, 13\nk-nearest neighbors, 20\nmaking predictions, 22\nmodel evaluation, 22\nmulticlass problem, 26\noverview of, 23\ntraining and testing data, 17\niterative feature selection, 240\nJ\nJupyter Notebook, 7\nK",
    "iris classification application\ndata inspection, 19\ndataset for, 14\ngoals for, 13\nk-nearest neighbors, 20\nmaking predictions, 22\nmodel evaluation, 22\nmulticlass problem, 26\noverview of, 23\ntraining and testing data, 17\niterative feature selection, 240\nJ\nJupyter Notebook, 7\nK\nk-fold cross-validation, 252\nk-means clustering\napplying with scikit-learn, 170\nvs. classification, 171\ncluster centers, 169\ncomplex datasets, 179\nevaluating and comparing, 191\nexample of, 168\nfailures of, 173\nstrengths and weaknesses, 181\nvector quantization with, 176\nk-nearest neighbors (k-NN)\nanalyzing KNeighborsClassifier, 37\nanalyzing KNeighborsRegressor, 43\nbuilding, 20\nclassification, 35-37\n370 \n| \nIndex\nvs. linear models, 46\nparameters, 44\npredictions with, 35\nregression, 40\nstrengths and weaknesses, 44\nKaggle, 365\nkernelized support vector machines (SVMs)\nkernel trick, 97\nlinear models and nonlinear features, 92\nvs. linear support vector machines, 92\nmathematics of, 92\nparameters, 104\npredictions with, 98\npreprocessing data for, 102\nstrengths and weaknesses, 104\ntuning SVM parameters, 99\nunderstanding, 98\nknn object, 21\nL\nL1 regularization, 53\nL2 regularization, 49, 60, 67\nLasso model, 53\nLatent Dirichlet Allocation (LDA), 348-355\nleafs, 71\nleakage, 310\nlearn from the past approach, 243\nlearning_rate parameter, 89\nleave-one-out cross-validation, 257\nlemmatization, 344-347\nlinear functions, 56\nlinear models\nclassification, 56\ndata representation and, 220-224\nvs. k-nearest neighbors, 46\nLasso, 53\nlinear SVMs, 56\nlogistic regression, 56\nmulticlass classification, 63\nordinary least squares, 47\nparameters, 67\npredictions with, 45\nregression, 45\nridge regression, 49\nstrengths and weaknesses, 67\nlinear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366",
    "linear regression, 47, 224-232\nlinear support vector machines (SVMs), 56\nlinkage arrays, 185\nlive testing, 359\nlog function, 232\nloss functions, 56\nlow-dimensional datasets, 32\nM\nmachine learning\nalgorithm chains and pipelines, 305-321\napplications for, 1-5\napproach to problem solving, 357-366\nbenefits of Python for, 5\nbuilding your own systems, vii\ndata representation, 211-250\nexamples of, 1, 13-23\nmathematics of, vii\nmodel evaluation and improvement,\n251-303\npreprocessing and scaling, 132-140\nprerequisites to learning, vii\nresources, ix, 361-366\nscikit-learn and, 5-13\nsupervised learning, 25-129\nunderstanding your data, 4\nunsupervised learning, 131-209\nworking with text data, 323-356\nmake_pipeline function\naccessing step attributes, 314\ndisplaying steps attribute, 314\ngrid-searched pipelines and, 315\nsyntax for, 313\nmanifold learning algorithms\napplications for, 164\nexample of, 164\nresults of, 168\nvisualizations with, 163\nmathematical functions for feature transforma‐\ntions, 232\nmatplotlib, 9\nmax_features parameter, 84\nmeta-estimators for trees and forests, 266\nmethod chaining, 68\nmetrics (see evaluation metrics and scoring)\nmglearn, 11\nmllib, 362\nmodel-based feature selection, 238\nmodels (see also algorithms)\ncalibrated, 288\ncapable of generalization, 26\ncoefficients with text data, 338-347\ncomplexity vs. dataset size, 29\nIndex \n| \n371\ncross-validation of, 252-260\neffect of data representation choices on, 211\nevaluation and improvement, 251-252\nevaluation metrics and scoring, 275-302\niris classification application, 13-23\noverfitting vs. underfitting, 28\npipeline preprocessing and, 317\nselecting, 300\nselecting with grid search, 319\ntheory behind, 361\ntuning parameters with grid search, 260-275\nmovie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers",
    "movie reviews, 325\nmulticlass classification\nvs. binary classification, 25\nevaluation metrics and scoring for, 296-299\nlinear models for, 63\nuncertainty estimates, 124\nmultilayer perceptrons (MLPs), 104\nMultinomialNB, 68\nN\nn-grams, 339\nnaive Bayes classifiers\nkinds in scikit-learn, 68\nparameters, 70\nstrengths and weaknesses, 70\nnatural language processing (NLP), 325, 355\nnegative class, 26\nnested cross-validation, 272\nNetflix prize challenge, 363\nneural networks (deep learning)\naccuracy of, 114\nestimating complexity in, 118\npredictions with, 104\nrandomization in, 113\nrecent breakthroughs in, 364\nstrengths and weaknesses, 117\ntuning, 108\nnon-negative matrix factorization (NMF)\napplications for, 156\napplying to face images, 157\napplying to synthetic data, 156\nnormalization, 344\nnormalized mutual information (NMI), 191\nNumPy (Numeric Python) library, 7\nO\noffline evaluation, 359\none-hot-encoding, 213-217\none-out-of-N encoding, 213-217\none-vs.-rest approach, 63\nonline resources, ix\nonline testing, 359\nOpenML platform, 365\noperating points, 289\nordinary least squares (OLS), 47\nout-of-core learning, 364\noutlier detection, 197\noverfitting, 28, 261\nP\npair plots, 19\npandas\nbenefits of, 10\nchecking string-encoded data, 214\ncolumn indexing in, 216\nconverting data to one-hot-encoding, 214\nget_dummies function, 218\nparallelization over a cluster, 364\npermissions, x\npipelines (see algorithm chains and pipelines)\npolynomial features, 224-232\npolynomial kernels, 97\npolynomial regression, 228\npositive class, 26\nPOSIX time, 244\npre- and post-pruning, 74\nprecision, 282, 358\nprecision-recall curves, 289-292\npredict for the future approach, 243\npredict method, 22, 37, 68, 267\npredict_proba function, 122, 286\npreprocessing, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\nkinds of, 133\nparameter selection with, 306\npipelines and, 317\npurpose of, 132\nscaling training and test data, 136\nprincipal component analysis (PCA)\ndrawbacks of, 146\nexample of, 140",
    "preprocessing, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\nkinds of, 133\nparameter selection with, 306\npipelines and, 317\npurpose of, 132\nscaling training and test data, 136\nprincipal component analysis (PCA)\ndrawbacks of, 146\nexample of, 140\nfeature extraction with, 147\nunsupervised nature of, 145\nvisualizations with, 142\nwhitening option, 150\nprobabilistic modeling, 363\n372 \n| \nIndex\nprobabilistic programming, 363\nproblem solving\nbuilding your own estimators, 360\nbusiness metrics and, 358\ninitial approach to, 357\nresources, 361-366\nsimple vs. complicated cases, 358\nsteps of, 358\ntesting your system, 359\ntool choice, 359\nproduction systems\ntesting, 359\ntool choice, 359\npruning for decision trees, 74\npseudorandom number generators, 18\npure leafs, 73\nPyMC language, 364\nPython\nbenefits of, 5\nprepackaged distributions, 6\nPython 2 vs. Python 3, 12\nPython(x,y), 6\nstatsmodel package, 362\nR\nR language, 362\nradial basis function (RBF) kernel, 97\nrandom forests\nanalyzing, 85\nbuilding, 84\ndata representation and, 220-224\nvs. decision trees, 83\nvs. gradient boosted regression trees, 88\nparameters, 88\npredictions with, 84\nrandomization in, 83\nstrengths and weaknesses, 87\nrandom_state parameter, 18\nranking, 363\nreal numbers, 26\nrecall, 282\nreceiver operating characteristics (ROC)\ncurves, 292-296\nrecommender systems, 363\nrectified linear unit (relu), 106\nrectifying nonlinearity, 106\nrecurrent neural networks (RNNs), 356\nrecursive feature elimination (RFE), 240\nregression\nf_regression, 236, 310\nLinearRegression, 47-56, 81, 247\nregression problems\nBoston Housing dataset, 34\nvs. classification problems, 26\nevaluation metrics and scoring, 299\nexamples of, 26\ngoals for, 26\nk-nearest neighbors, 40\nLasso, 53\nlinear models, 45\nridge regression, 49\nwave dataset illustration, 31\nregularization\nL1 regularization, 53\nL2 regularization, 49, 60\nrescaling\nexample of, 132-140\nkernel SVMs, 102\nresources, ix\nridge regression, 49\nrobustness-based clustering, 194",
    "goals for, 26\nk-nearest neighbors, 40\nLasso, 53\nlinear models, 45\nridge regression, 49\nwave dataset illustration, 31\nregularization\nL1 regularization, 53\nL2 regularization, 49, 60\nrescaling\nexample of, 132-140\nkernel SVMs, 102\nresources, ix\nridge regression, 49\nrobustness-based clustering, 194\nroots, 72\nS\nSafari Books Online, x\nsamples, defined, 4\nscaling, 132-140\ndata transformation application, 134\neffect on supervised learning, 138\ninto larger datasets, 364\nkinds of, 133\npurpose of, 132\ntraining and test data, 136\nscatter plots, 19\nscikit-learn\nalternate frameworks, 362\nbenefits of, 5\nBunch objects, 33\ncancer dataset, 32\ncore code for, 24\ndata and labels in, 18\ndocumentation, 6\nfeature_names attribute, 33\nfit method, 21, 68, 119, 135\nfit_transform method, 138\ninstalling, 6\nknn object, 21\nlibraries and tools, 7-11\nIndex \n| \n373\npredict method, 22, 37, 68\nPython 2 vs. Python 3, 12\nrandom_state parameter, 18\nscaling mechanisms in, 139\nscore method, 23, 37, 43\ntransform method, 135\nuser guide, 6\nversions used, 12\nscikit-learn classes and functions\naccuracy_score, 193\nadjusted_rand_score, 191\nAgglomerativeClustering, 182, 191, 203-207\naverage_precision_score, 292\nBaseEstimator, 360\nclassification_report, 284-288, 298\nconfusion_matrix, 279-299\nCountVectorizer, 329-355\ncross_val_score, 253, 256, 300, 307, 360\nDBSCAN, 187-190\nDecisionTreeClassifier, 75, 278\nDecisionTreeRegressor, 75, 80\nDummyClassifier, 278\nElasticNet class, 55\nENGLISH_STOP_WORDS, 334\nEstimator, 21\nexport_graphviz, 76\nf1_score, 284, 291\nfetch_lfw_people, 147\nf_regression, 236, 310\nGradientBoostingClassifier, 88-91, 119, 124\nGridSearchCV, 263-275, 300-301, 305-309,\n315-320, 360\nGroupKFold, 259\nKFold, 256, 260\nKMeans, 174-181\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLasso, 53-55\nLatentDirichletAllocation, 348\nLeaveOneOut, 257\nLinearRegression, 47-56, 81, 247\nLinearSVC, 56-59, 65, 67, 68\nload_boston, 34, 230, 317\nload_breast_cancer, 32, 38, 59, 75, 134, 144,\n236, 305",
    "KFold, 256, 260\nKMeans, 174-181\nKNeighborsClassifier, 21-24, 37-43\nKNeighborsRegressor, 42-47\nLasso, 53-55\nLatentDirichletAllocation, 348\nLeaveOneOut, 257\nLinearRegression, 47-56, 81, 247\nLinearSVC, 56-59, 65, 67, 68\nload_boston, 34, 230, 317\nload_breast_cancer, 32, 38, 59, 75, 134, 144,\n236, 305\nload_digits, 164, 278\nload_files, 326\nload_iris, 14, 124, 253\nLogisticRegression, 56-62, 67, 209, 253, 279,\n315, 332-347\nmake_blobs, 92, 119, 136, 173-183, 188, 286\nmake_circles, 119\nmake_moons, 85, 108, 175, 190-195\nmake_pipeline, 313-319\nMinMaxScaler, 102, 133, 135-139, 190, 230,\n308, 309, 319\nMLPClassifier, 107-119\nNMF, 140, 159-163, 179-182, 348\nNormalizer, 134\nOneHotEncoder, 218, 247\nParameterGrid, 274\nPCA, 140-166, 179, 195-206, 313-314, 348\nPipeline, 305-319, 320\nPolynomialFeatures, 227-230, 248, 317\nprecision_recall_curve, 289-292\nRandomForestClassifier, 84-86, 238, 290,\n319\nRandomForestRegressor, 84, 231, 240\nRFE, 240-241\nRidge, 49, 67, 112, 231, 234, 310, 317-319\nRobustScaler, 133\nroc_auc_score, 294-301\nroc_curve, 293-296\nSCORERS, 301\nSelectFromModel, 238\nSelectPercentile, 236, 310\nShuffleSplit, 258, 258\nsilhouette_score, 193\nStandardScaler, 114, 133, 138, 144, 150,\n190-195, 314-320\nStratifiedKFold, 260, 274\nStratifiedShuffleSplit, 258, 347\nSVC, 56, 100, 134, 139, 260-267, 269-272,\n305-309, 313-320\nSVR, 92, 229\nTfidfVectorizer, 336-356\ntrain_test_split, 17-19, 251, 286, 289\nTransformerMixin, 360\nTSNE, 166\nSciPy, 8\nscore method, 23, 37, 43, 267, 308\nsensitivity, 283\nsentiment analysis example, 325\nshapes, defined, 16\nshuffle-split cross-validation, 258\nsin function, 232\nsoft voting strategy, 84\n374 \n| \nIndex\nspark computing environment, 362\nsparse coding (dictionary learning), 163\nsparse datasets, 44\nsplits, 252\nStan language, 364\nstatsmodel package, 362\nstemming, 344-347\nstopwords, 334\nstratified k-fold cross-validation, 254-256\nstring-encoded categorical data, 214\nsupervised learning, 25-129 (see also classifica‐\ntion problems; regression problems)",
    "sparse datasets, 44\nsplits, 252\nStan language, 364\nstatsmodel package, 362\nstemming, 344-347\nstopwords, 334\nstratified k-fold cross-validation, 254-256\nstring-encoded categorical data, 214\nsupervised learning, 25-129 (see also classifica‐\ntion problems; regression problems)\nalgorithms for\ndecision trees, 70-83\nensembles of decision trees, 83-92\nk-nearest neighbors, 35-44\nkernelized support vector machines,\n92-104\nlinear models, 45-68\nnaive Bayes classifiers, 68\nneural networks (deep learning),\n104-119\noverview of, 2\ndata representation, 4\nexamples of, 3\ngeneralization, 26\ngoals for, 25\nmodel complexity vs. dataset size, 29\noverfitting vs. underfitting, 28\noverview of, 127\nsample datasets, 30-34\nuncertainty estimates, 119-127\nsupport vectors, 98\nsynthetic datasets, 30\nT\nt-SNE algorithm (see manifold learning algo‐\nrithms)\ntangens hyperbolicus (tanh), 106\nterm frequency–inverse document frequency\n(tf–idf), 336-347\nterminal nodes, 71\ntest data/test sets\nBoston Housing dataset, 34\ndefined, 17\nforge dataset, 30\nwave dataset, 31\nWisconsin Breast Cancer dataset, 32\ntext data, 323-356\nbag-of-words representation, 327-334\nexamples of, 323\nmodel coefficients, 338\noverview of, 355\nrescaling data with tf-idf, 336-338\nsentiment analysis example, 325\nstopwords, 334\ntopic modeling and document clustering,\n347-355\ntypes of, 323-325\ntime series predictions, 363\ntokenization, 328, 344-347\ntop nodes, 72\ntopic modeling, with LDA, 347-355\ntraining data, 17\ntrain_test_split function, 254\ntransform method, 135, 312, 334\ntransformations\nselecting, 235\nunivariate nonlinear, 232-236\nunsupervised, 131\ntree module, 76\ntrigrams, 339\ntrue positive rate (TPR), 283, 292\ntrue positives/true negatives, 281\ntypographical conventions, ix\nU\nuncertainty estimates\napplications for, 119\ndecision function, 120\nin binary classification evaluation, 286-288\nmulticlass classification, 124\npredicting probabilities, 122\nunderfitting, 28\nunigrams, 340\nunivariate nonlinear transformations, 232-236",
    "typographical conventions, ix\nU\nuncertainty estimates\napplications for, 119\ndecision function, 120\nin binary classification evaluation, 286-288\nmulticlass classification, 124\npredicting probabilities, 122\nunderfitting, 28\nunigrams, 340\nunivariate nonlinear transformations, 232-236\nunivariate statistics, 236\nunsupervised learning, 131-209\nalgorithms for\nagglomerative clustering, 182-187\nclustering, 168-207\nDBSCAN, 187-190\nk-means clustering, 168-181\nmanifold learning with t-SNE, 163-168\nnon-negative matrix factorization,\n156-163\noverview of, 3\nprincipal component analysis, 140-155\nIndex \n| \n375\nchallenges of, 132\ndata representation, 4\nexamples of, 3\noverview of, 208\nscaling and preprocessing for, 132-140\ntypes of, 131\nunsupervised transformations, 131\nV\nvalue_counts function, 214\nvector quantization, 176\nvocabulary building, 328\nvoting, 36\nvowpal wabbit, 362\nW\nwave dataset, 31\nweak learners, 88\nweights, 47, 106\nwhitening option, 150\nWisconsin Breast Cancer dataset, 32\nword stems, 344\nX\nxgboost package, 91\nxkcd Color Survey, 324\n376 \n| \nIndex\nAbout the Authors\nAndreas Müller received his PhD in machine learning from the University of Bonn.\nAfter working as a machine learning researcher on computer vision applications at\nAmazon for a year, he joined the Center for Data Science at New York University. For\nthe last four years, he has been a maintainer of and one of the core contributors to\nscikit-learn, a machine learning toolkit widely used in industry and academia, and\nhas authored and contributed to several other widely used machine learning pack‐\nages. His mission is to create open tools to lower the barrier of entry for machine\nlearning applications, promote reproducible science, and democratize the access to\nhigh-quality machine learning algorithms.\nSarah Guido is a data scientist who has spent a lot of time working in start-ups. She\nloves Python, machine learning, large quantities of data, and the tech world. An",
    "learning applications, promote reproducible science, and democratize the access to\nhigh-quality machine learning algorithms.\nSarah Guido is a data scientist who has spent a lot of time working in start-ups. She\nloves Python, machine learning, large quantities of data, and the tech world. An\naccomplished conference speaker, Sarah attended the University of Michigan for grad\nschool and currently resides in New York City.\nColophon\nThe animal on the cover of Introduction to Machine Learning with Python is a hell‐\nbender salamander (Cryptobranchus alleganiensis), an amphibian native to the eastern\nUnited States (ranging from New York to Georgia). It has many colorful nicknames,\nincluding “Allegheny alligator,” “snot otter,” and “mud-devil.” The origin of the name\n“hellbender” is unclear: one theory is that early settlers found the salamander’s\nappearance unsettling and supposed it to be a demonic creature trying to return to\nhell.\nThe hellbender salamander is a member of the giant salamander family, and can grow\nas large as 29 inches long. This is the third-largest aquatic salamander species in the\nworld. Their bodies are rather flat, with thick folds of skin along their sides. While\nthey do have a single gill on each side of the neck, hellbenders largely rely on their\nskin folds to breathe: gas flows in and out through capillaries near the surface of the\nskin.\nBecause of this, their ideal habitat is in clear, fast-moving, shallow streams, which\nprovide plenty of oxygen. The hellbender shelters under rocks and hunts primarily by\nsense of smell, though it is also able to detect vibrations in the water. Its diet is made\nup of crayfish, small fish, and occasionally the eggs of its own species. The hellbender\nis also a key member of its ecosystem as prey: predators include various fish, snakes,\nand turtles.\nHellbender salamander populations have decreased significantly in the last few deca‐\ndes. Water quality is the largest issue, as their respiratory system makes them very",
    "is also a key member of its ecosystem as prey: predators include various fish, snakes,\nand turtles.\nHellbender salamander populations have decreased significantly in the last few deca‐\ndes. Water quality is the largest issue, as their respiratory system makes them very\nsensitive to polluted or murky water. An increase in agriculture and other human\nactivity near their habitat means greater amounts of sediment and chemicals in the\nwater. In an effort to save this endangered species, biologists have begun to raise the\namphibians in captivity and release them when they reach a less vulnerable age.\nMany of the animals on O’Reilly covers are endangered; all of them are important to\nthe world. To learn more about how you can help, go to animals.oreilly.com.\nThe cover image is from Wood’s Animate Creation. The cover fonts are URW Type‐\nwriter and Guardian Sans. The text font is Adobe Minion Pro; the heading font is\nAdobe Myriad Condensed; and the code font is Dalton Maag’s Ubuntu Mono."
]