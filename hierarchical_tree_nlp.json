{
    "id": "root",
    "title": "Textbook",
    "content": null,
    "children": [
        {
            "id": "chapter-1",
            "title": "The Information Age",
            "content": null,
            "children": [
                {
                    "id": "chapter-1-section-1",
                    "title": "The Information Age",
                    "content": null,
                    "children": []
                },
                {
                    "id": "chapter-1-section-2",
                    "title": "Technology for Accessing Info",
                    "content": null,
                    "children": []
                },
                {
                    "id": "chapter-1-section-3",
                    "title": "Question Answering Systems",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-3-subsection-1",
                            "title": "ELIZA - The Rogerian Therapist",
                            "content": "the problem and the range of ideas that have been tried out.\nWe begin with a very simple yet highly successful system\nthat was however never intended to be an NLP system nor\nwas it designed to answer any questions at all.\nYet this\nsystem has a simple and neat design and knowing this helps\nus to understand other NLP systems better.\n1.3.1\nELIZA - The Rogerian Therapist\nThere are many approaches to improving mental health.\nHere we are talking about a particular approach called Roge-\nrian Therapy and a system called ELIZA developed with\nthis approach in mind. Empathy is the foundation of Carl\nRogers’ client-centered therapy (also known as Rogerian ther-\napy). He asserted that empathy alone is healing. A client\ncentered therapist strives to provide an environment of em-\npathy, unconditional positive regard, and acceptance. Ther-\napists are trained to accept the client where they are at the\nmoment. Client-centered therapists consider diagnosis and\ntreatment planning to be much less important than being\nsupportive to the client. Instead they act as an understand-\ning listener, helping the client by providing advice and al-\nternate interpretations to past events only when asked.\nRogerian therapy involves the therapist’s entry into the\n10\nCHAPTER 1. THE INFORMATION AGE\nclient’s unique phenomenological world. In mirroring this\nworld, the therapist does not disagree or point out contra-\ndictions. Neither does he/she attempt to delve into the un-\nconscious. The focus is on immediate conscious experience.\nRogerian therapy is a process of freeing a person and re-\nmoving obstacles so that normal growth and development\ncan proceed and the client can become independent and self-\ndirected. During the course of therapy the client moves from\nrigidly of self-perception to ﬂuidity.\nThe ELIZA system was developed by Joseph Weizen-\nbaum in 1966. ELIZA is a system whose sole purpose is to\nmaintain a conversation with the client. It is not really re-\ndirected. During the course of therapy the client moves from\nrigidly of self-perception to ﬂuidity.\nThe ELIZA system was developed by Joseph Weizen-\nbaum in 1966. ELIZA is a system whose sole purpose is to\nmaintain a conversation with the client. It is not really re-\nquired to understand what the user says nor is it expected\nto answer his questions. Such a system is very useful for\nrecording and later analyzing the mind of patients with a\nmental problem. When allowed to talk in a free and con-\nducive environment, such patients tend to come close to the\nsource of their problem irrespective of what topic the con-\nversation starts from. Psychiatrists are trained to analyze\nthe conversations and guess the source of the problems and\nhence plan further investigations. Patients are always con-\nscious of what the other person may think of whatever they\nspeak. It can be expected that patients tend to be more\nfree and forthcoming when they are talking to a dumb, face-\nless machine than with doctors or other people. Hence this\nsystem. Here is a transcript of a typical conversation with\nELIZA:\nMen are all alike.\nIN WHAT WAY\nThey’re always bugging us about something\nor the other.\n1.3. QUESTION ANSWERING SYSTEMS\n11\nCAN YOU THINK OF A SPECIFIC EXAMPLE\nWell, my boyfriend made me come here\nYOUR BOYFRIEND MADE YOU COME HERE\nHe says I’m depressed much of the time.\nI AM SORRY TO HEAR YOU ARE DEPRESSED\nIt’s true. I am unhappy.\nDO YOU THINK COMING HERE WILL HELP YOU\nNOT TO BE UNHAPPY\nI need some help, that much seems certain.\nWHAT WOULD IT MEAN TO YOU IF YOU GOT\nSOME HELP\nPerhaps I could learn to get along with my mother\nTELL ME MORE ABOUT YOUR FAMILY\nFIG 1.1 Sample Transcripts from ELIZA. ELIZA’s\noutput is in all-caps.\nQuite impressive, right? ELIZA was so successful that\neven its developers were said to have started confessing to\nELIZA! There are many implementations that are freely\navailable and the readers may ﬁnd it interesting to play\nELIZA’s response could be diﬀerent. Many people have re-\nimplemented ELIZA and the more sophisticated versions can\ndetect if you are too curt and reserved or if you try to abuse\nthe system. Some of them try to pull you back into what\nyou were saying earlier to avoid digressing too far.\nThe\ntable below shows a sample of the kind of keyword based\npattern matching system that ELIZA uses. Letters preﬁxed\nwith a question mark are variables that can match parts of\nsentences.\n1.3. QUESTION ANSWERING SYSTEMS\n13\nKey\nPriority\nPattern\nPossible Outputs\nalike\n10\n?X\nIn what way?\nWhat resemblance\ndo you see?\nare\n3\n?X are you ?Y\nWould you prefer it\nif I weren’t ?Y?\n3\n?X are ?Y\nWhat if they were not\n?Y?\nalways\n5\n?X\nCan you think of a\nspeciﬁc example?\nWhen?\nReally, always?\nwhat\n2\n?X\nWhy do you ask?\nDoes that interest you?\nTABLE 1.1 PATTERN MATCHING SYSTEM IN ELIZA\nELIZA’s power is in its simplicity and generality - it can say\nsomething for any input whatever and what it says is usually\nreasonably well connected with the given input. This strength is\nalso its weakness - its shallow, superﬁcial analysis often leads it\nastray. The system is not scalable or expendable to applications\nthat demand deeper and better understanding.\n1.3.2\nEarly NLP Systems\nA large number of NLP systems were developed in the 1960s and\n1970s.\nMany of them were question answering systems.\nThey\nwere all toy systems by today’s standards but it is instructive\nto take a peep at some of them. For the ﬁrst time, researchers\nwere trying to integrate linguistic analysis at various levels ranging\nfrom dictionaries and morphological analysis through rudiments of\nsyntax to discourse analysis and build complete working systems.\nHere we take a quick look at a few of them.\nSTUDENT\nThe STUDENT system was developed by Daniel G Bobrow in\nthe year 1964. This system was designed to solve algebra story\n14\nCHAPTER 1. THE INFORMATION AGE\nproblems from school text books. These problems are expressed in\nFIG 1.1 Sample Transcripts from ELIZA. ELIZA’s\noutput is in all-caps.\nQuite impressive, right? ELIZA was so successful that\neven its developers were said to have started confessing to\nELIZA! There are many implementations that are freely\navailable and the readers may ﬁnd it interesting to play\nwith versions of ELIZA. The gnu-emacs editor comes with\na built-in version too. Source codes are also available and\nthose interested in building similar toy NLP systems may\nﬁnd it useful to go through the code.\n12\nCHAPTER 1. THE INFORMATION AGE\nELIZA is, however, an extremely simple system.\nThe\nsole purpose of ELIZA is to maintain a conversation. Since\nELIZA does not have to really understand what the user is\nsaying, it does not have to linguistically analyze the input\nsentences in any great depth or detail. In fact ELIZA does\nnot perform any of the usual NLP tasks such as dictionary\nlook-up, morphological analysis or syntactic analysis.\nIn\nfact a useful by-product of this simplistic approach is that\nELIZA does not complain if you make spelling mistakes or\nyour sentences are grammatically incorrect or incomplete.\nAll that ELIZA does is to look for keywords. ELIZA has\na set of prioritized keywords and for each keyword a set of\ninput-patterns to match the input sentences against and a\nset of output patterns which it uses to generate responses.\nIf many keywords are found, it simply uses the one with\nhighest priority. If no keywords are found, ELIZA has a set\nof default outputs such as “please continue”, “I see”, “tell me\nmore” and so on. To avoid giving a sense of monotony and\nrepetition, it rotates among a set of alternative responses\nfor a given keyword. Thus if you say the same thing again,\nELIZA’s response could be diﬀerent. Many people have re-\nimplemented ELIZA and the more sophisticated versions can\ndetect if you are too curt and reserved or if you try to abuse\nthe system. Some of them try to pull you back into what\nyou were saying earlier to avoid digressing too far.\nThe\nworth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was\nRichard’s friend, BORIS would need to know what colleges are\nand what exactly college room-mate means. It is easier to answer\nquestions on what happened to whom, where, when etc. literally\nbut assessing mental states and reactions of people in diﬀerent\nsituations is a diﬀerent matter. That BORIS could understand\nthat Richard wanted to know how Paul was is surely a human-\nlike response. BORIS knows that if X lends money to Y, that is\nthe same as Y borrowing money from X.\nThe point is not whether the answers are correct or not. Of-\nten there are no correct or wrong answers, only more appropriate\nand less appropriate ones.\nIf somebody asks you where is Taj\nMahal what would you say? If you are in Europe you would per-\nhaps say that Taj Mahal is in India but if you are in some part\nof India you would probably say that it is in Agra. If you happen\nto be already in Agra then you are perhaps expected to say how\nexactly to reach Taj Mahal from wherever you are. An precise\ndescription of the location in terms of latitudes and longitudes\nmay be mathematically more accurate but it may not serve the\npurpose. Appropriateness of answers depends upon the situation\nand understanding situational context is as much a part of natural\nlanguage understanding as all the linguistic processing. Speakers\nhave mental models of listeners and vice versa. To answer some-\nbody you ﬁrst need to understand what exactly he or she wants\nto know. Literal interpretation of given texts does not constitute\nin-depth understanding.\nWith many more examples of this kind, Dyer in his thesis tries\nto convince the reader that in-depth understanding and human-\nlike question answering behaviour is very much possible for com-\n26",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-2",
                            "title": "Early NLP Systems",
                            "content": "translation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP\nsystems came to focus. During the seventies and eighties, much\nmore sophisticated systems were built. Linguistic data resources\nhad expanded and tools and technologies had also matured to\nsome extent. Slowly signs of saturation started showing up. There\nwere barriers that were diﬃcult to cross. Performance of systems\ncould not be improved even with great eﬀort. It took great eﬀort\nand time to develop and improve linguistic knowledge but the\nincremental improvements in performance obtained got smaller\nand smaller.\nThe early nineties saw a major paradigm shift. The limita-\ntions of the knowledge based approach had been well understood.\nIt had been shown that natural language understanding and gen-\neration could be done if all the required knowledge could be made\n94\nCHAPTER 2. FOUNDATIONS OF NLP\navailable but knowledge acquisition became the real bottleneck.\nExperts were hard to ﬁnd, hand-crafting knowledge structures was\nextremely tedious, time consuming and costly and after all that,\nsaturation points would be reached beyond which it is diﬃcult to\nprogress. Focus shifted to machine learning techniques to auto-\nmatically ‘learn’ to solve a problem by looking at a large collection\nof examples. Computing power and memory had increased mani-\nfolds and machine costs had drastically come down. What could\nonce be tried out only on big machines could now be attempted on\na desk top PC. Focus shifted to development and statistical anal-\nysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\nplines and specializations for the sake of convenience but now they\nseem to have become major barriers for scientiﬁc exploration. Sci-\nence today is highly inter-disciplinary. This is perhaps the most\nimportant lesson we have to learn from these early NLP systems.\nPSP2\nThe PSP2 system was developed by the author in the eighties.\nPSP2 is a simple natural language question answering system in\nthe domain of ﬁnancial investments. It is a keyword based pattern\nmatching system like most other early NLP systems, only the\npattern matching is a bit more sophisticated. We ﬁrst give sample\ntranscripts and then summarize the salient features.\nOnly the\nqueries given by the user and the system’s interpretation are given.\nAfter taking user’s concurrence, the system gives out pre-stored\nresponses. The spelling errors are all intentional.\nUser:\nPSP?\nSystem:\nWHAT IS PSP2 INVESTMENT PLAN ALL ABOUT?\nUser:\nHow muhc should I ivnest?\nSystem:\nWHAT IS THE INITIAL AMOUNT OF INVESTMENT\nIN THE PSP2 PLAN?\nUser:\nIs that the lest?\nSystem:\nWHAT IS THE MINIMUM AMOUNT OF INVESTMENT?\n18\nCHAPTER 1. THE INFORMATION AGE\nUser:\nWhy?\nSystem:\nWHY IS THERE A MINIMUM AMOUNT FOR INVESTMENT\nIN PSP2?\nUser:\nHow many yearrs\nSystem:\nWHAT IS THE PERIOD OF INVESTMENT?\nUser:\nis that the least?\nSystem:\nWHAT IS THE MINIMUM PERIOD OF INVESTMENT?\nUser:\nHow much iterest do I get?\nSystem:\nWHAT IS THE RATE OF INTEREST IN PSP2 PLAN?\nUser:\nCan I take back my money whenver I want?\nSystem:\nIS THERE A FACILITY FOR PREMATURE\nWITHDRAWALS?\nUser:\nWhere should I sign on the check?\nSystem:\nTELL ME MORE ABOUT PSP2 CHEQUES.\nFIG 1.4 Sample Transcripts from PSP2\nPSP2 accepts free format natural language queries and pro-\nduces natural language responses using a bank of pre-stored fre-\nquently asked questions and corresponding answers. These stored\nquestions and answers are provided by human experts and can\ntherefore be taken as accurate and reliable.\nThe only issue is\nhow to understand the users’ queries and map them to the closest\nerate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-\nman languages. Speech perception, understanding and synthesis\nwere also major areas of focus. However, early work in NLP was\nmostly limited to handling written texts and was thus largely dis-\njoint from speech technology research. Of late we can see a closer\nintegration of NLP and speech technologies. There is also a trend\ntowards multi-modal communications involving speech, language\nand gestures.\nWe begin this chapter with elements of linguistic analysis es-\nsential for NLP and take up corpus based statistical approaches in\nthe second half. The treatment is introductory. Interested readers\nwill ﬁnd pointers to more advanced and detailed material in the\nbibliography.\n2.1.3\nNLP: An AI Perspective\nThere are three major concerns in NLP:\n• Natural Language Understanding (NLU): How do we un-\nderstand what we read or hear?\n• Natural Language Generation (NLG): How do we synthesize\nNatural Language utterances to convey whatever we have\nin mind?\n• Natural Language Acquisition (NLA): How do we acquire\nor learn a language?\n2.1. INTRODUCTION\n89\nThe Producer-Comprehender Model\nWe use language to communicate information as also our ideas,\nfeelings, emotions and attitudes.\nThis communication through\nlanguage can only happen between active, cognitive processors\nsuch as human beings and, hopefully, computers. We don’t speak\nto walls, right? There can be no proper communication if one or\nboth the parties are sleeping or otherwise inattentive. We shall\nuse the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive\nin the transcripts above are intended, observe them carefully.\nThe power of NLP systems such as PSP2 comes from their\nsimplicity. This simplicity is also their main weakness. Programs\nsuch as PSP2, STUDENT and SIR are simplistic, superﬁcial, lim-\nited and ad-hoc. They cannot be scaled to real life, wide coverage,\n1.3. QUESTION ANSWERING SYSTEMS\n21\nrobust applications nor can they be easily adapted or retrained\nfor diﬀerent application domains. These systems surely generated\nenough initial interest and curiosity at their times. In fact such\ntoy systems are being developed and used even today, albeit in\na more sophisticated form with animated human faces and voice\noutput. The only purpose they serve is to generate some curios-\nity. The big question remains - can machines really understand\nlanguage at all?\n1.3.3\nFoundations of Story Understanding\nDuring the seventies, Roger Schank and his team at Yale Uni-\nversity were focussing on the foundations of story understanding.\nConceptual Dependency (CD) theory, Scripts, Plans and Goals\nwere some of the ideas that came out of this eﬀort.\nConceptual Dependency is a system designed to represent the\nmeanings of sentences by decomposing them into a small number\nof primitive “acts”. In CD theory, sentences with identical mean-\ning should have the same underlying conceptual representation,\nregardless of the diﬀerences in grammatical form or the language\nused. In this sense, CD was a kind of an inter-lingua. A basic\npremise of the Cd theory is that meaning arises from a combi-\nnation of memory search, planning and inference. Only a small\nfraction of meaning is actually conveyed directly by the lexical\nitems which explicitly appear in a given sentence. For example,\nif we read the sentence “John bought a television” we understand\nmany things - that John probably bought the TV in a store, there\nwas another person in the store, John gave him money, that per-\nson gave John the TV, the person who sold the TV no longer\nPeople have world knowledge and commonsense and this forms a\ntacit assumption when dealing with people. The goal of NLP, on\nthe other hand, is to make the computer, the idiot box that has no\ncommonsense, world knowledge or human-like reasoning power, to\nperform language processing tasks with human-like performance.\nA few examples are not enough. Exhaustive sets of data, rules,\nguidelines and principles need to be provided to handle special,\nunusual situations as also the run-of-the-mill cases. Explaining in\nsimple language is not suﬃcient, we will have to write detailed\nand precise program code.\nLet us take a speciﬁc example. One of the primary goals of\nmodern generative linguistics lead by Noam Chomsky is to under-\nstand how children, exposed to such small and imperfect samples\n96\nCHAPTER 2. FOUNDATIONS OF NLP\nof language use, can learn the structure and properties of their\nnative language, whatever that may be, so easily within just a\ncouple of years. From a computational point of view, an accept-\nable solution to this problem would be a computational model of\nchild language acquisition which can be experimentally tested. We\nshould be able to give controlled input and observe the outputs\nas well as the changes that take place in the internal structure of\nthe model to see what exactly constitutes learning and how the\nchild is actually learning with each input sample. We cannot open\nup somebody’s brain and see before and after changes as a sen-\ntence is being processed by the brain. But with a computational\nmodel such a detailed probing would be feasible. For example, if\nwe build a neural network model, we can see the changes taking\nplace in the various synaptic weights with each iteration.\nThe\nindividual numbers may not make much sense but we can surely\nget a global picture of whether the system is learning, at what\nrate it is learning, has a saturation point been reached etc. De-\nspite the great deal of eﬀort that has gone into building linguistic",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-3",
                            "title": "Foundations of Story Understanding",
                            "content": "try to develop everything from scratch on their own rather than\nteam up with others. This may be partly due to lack of mutual\ntrust and understanding. As a country, we have the potential to\nbecome world leaders in language technologies. But we need to\nlearn many basic lessons ﬁrst.\n2.5. CONCLUSIONS\n325\n2.5\nConclusions\nBy now you will have realized that NLP is a vast and highly\ninter-disciplinary subject. It is not possible to cover everything in\ndetail in a single book. The purpose of this book is only to give\nyou an initial idea about the nature of problems and issues, kinds\nof approaches that can be and have been taken, some idea about\nthe current status and challenges yet to be overcome and some\ngeneral idea about the shape of things to come. Each of the topics\ncovered here is a vast subject in itself and the readers interested\nin pursuing any of the areas in more depth will ﬁnd many good\nbooks and research articles. We have seen that although a great\ndeal of progress has been made over the last ﬁve decades, many of\nthe core problems remain unsolved. There are many interesting\nand useful applications and there is a lot that needs to be done. If\nthis book helps beginners to get interested and to kick start serious\nexplorations and research, it would have served its purpose.\nWe have been looking at NLP both as an independent ﬁeld of\nstudy and in the context of Information retrieval and other closely\nrelated applications. We will get back to IR in the next chapter\nand look at some of the advances that are being made in that\nﬁeld.\n326\nCHAPTER 2. FOUNDATIONS OF NLP\nChapter 3\nAdvances in\nInformation Retrieval\nInformation Retrieval is all about eﬃcient storage and retrieval\nof documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nin the transcripts above are intended, observe them carefully.\nThe power of NLP systems such as PSP2 comes from their\nsimplicity. This simplicity is also their main weakness. Programs\nsuch as PSP2, STUDENT and SIR are simplistic, superﬁcial, lim-\nited and ad-hoc. They cannot be scaled to real life, wide coverage,\n1.3. QUESTION ANSWERING SYSTEMS\n21\nrobust applications nor can they be easily adapted or retrained\nfor diﬀerent application domains. These systems surely generated\nenough initial interest and curiosity at their times. In fact such\ntoy systems are being developed and used even today, albeit in\na more sophisticated form with animated human faces and voice\noutput. The only purpose they serve is to generate some curios-\nity. The big question remains - can machines really understand\nlanguage at all?\n1.3.3\nFoundations of Story Understanding\nDuring the seventies, Roger Schank and his team at Yale Uni-\nversity were focussing on the foundations of story understanding.\nConceptual Dependency (CD) theory, Scripts, Plans and Goals\nwere some of the ideas that came out of this eﬀort.\nConceptual Dependency is a system designed to represent the\nmeanings of sentences by decomposing them into a small number\nof primitive “acts”. In CD theory, sentences with identical mean-\ning should have the same underlying conceptual representation,\nregardless of the diﬀerences in grammatical form or the language\nused. In this sense, CD was a kind of an inter-lingua. A basic\npremise of the Cd theory is that meaning arises from a combi-\nnation of memory search, planning and inference. Only a small\nfraction of meaning is actually conveyed directly by the lexical\nitems which explicitly appear in a given sentence. For example,\nif we read the sentence “John bought a television” we understand\nmany things - that John probably bought the TV in a store, there\nwas another person in the store, John gave him money, that per-\nson gave John the TV, the person who sold the TV no longer\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We\noften scan news headlines before or instead of reading the full\nnews. Teachers produce outlines of notes and course materials to\nstudents. Minutes of a meeting summarize what all transpired in\nthe meeting. Previews of movies give a quick advance view of the\nshape of things to come in the movie. Synopses of TV serials are\ntelecast to let the users get a feel for the serials. Book reviews\nare summaries as retold by the reviewer. Newspapers and maga-\nzines publish Radio and TV guides for the coming week or month.\nResumes and Obituaries are biographical summaries. Novels are\noften speciﬁcally abridged for, say, children to read. Weather fore-\ncasts and Market report bulletins are summaries too. Sound Bites\nconsolidate on going debate on a particular issue. Chronologies\nand gists of history are a kind of summary.\nSummarization is\nitself not new. Automatic techniques for summarization are.\nAlta-Vista Discovery uses Inxight’s summarizer for ﬁltering\nweb based IR. Orcale’s Context (data mining of text databases),\nMicrosoft Word’s AutoSummarize, British Telecom’s ProSum are\n1.6. AUTOMATIC SUMMARIZATION\n41\nsome of the commercial implementations.\n1.6.2\nApproaches to Automatic Summarization\nSummarization can be viewed as a reductive transformation of\nsource text to summary text through content reduction by selec-\ntion and / or generalization of what is important in the source.\nText Summarization has been deﬁned as the process of distilling\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\nject, you may ASK, INFORM-REASON, BARGAIN, THREATEN,\nOVERPOWER or STEAL. Plans have preconditions. You have\nto be in physical proximity to ask or you must a communication\ndevice such as a phone. In order to steal some object, you must\ngo near that object (not the owner!)\nDetailed planning is however not always required. There are\nstereotypical situations where a lot of details are generally known.\nTaking a train involves going to the station, buying the ticket,\nreaching the platform, waiting for the train, entraining, looking\nfor a seat, detraining at the destination station, exiting the station\netc. Buying groceries, going for a movie, going to a restaurant or a\nmarriage party can all be visualized this way. Scripts capture the\nfull details of such stereotypical scenarios. Nobody writes down\nin minute detail all that happened and if at all one tries, that\nwould be the most boring story. Writers write only the interest-\ning and signiﬁcant aspects and readers ﬁll in the rest by their\ncommonsense knowledge. Scripts can be used to do this ﬁlling in.\nThe Yale school made signiﬁcant contributions along these\nlines and built many systems. But the big questions remained.\nHow do we make computers understand language in a way we\nhumans can do?\n1.3.4\nIn-Depth Understanding\nAll the systems we have seen above are toy systems. They can-\nnot be easily scaled up to real systems.\nNone of them answer\nserious questions about the feasibility of developing real question-\nanswering systems. Can we, even in principle, build systems that\ncan answer questions like human beings can? Can we build sys-\ntems that can read, understand and then answer questions? In\nearly 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\nunfortunate that many have given too much of attention to the\ndetails of the code itself rather than to the mechanisms of produc-\ning and interpreting the codes. For example, syntacticians have\n2.1. INTRODUCTION\n93\noften spent all their life analyzing the intricacies of structure of\nsentences rather than worry about what those sentences really\nmean and how particular syntactic structures encode given mean-\nings and intentions. The central element of language is meaning\nand intention. It is unfortunate that phonology, morphology and\nsyntax have come to be considered the “core” of linguistics rather\nthan semantics. A lot of time, eﬀort and energy has been spent on\nsuperﬁcial details rather than the real, core problems and issues.\nThe available body of knowledge in linguistics and NLP today is\nthus heavily skewed and inevitably, this books reﬂects the same.\nWe will be raising some critical issues with semantics but we may\nnot be able to give satisfactory solutions in all cases.\n2.1.4\nNLP Over the Decades\nWe have seen the origins of NLP within Artiﬁcial Intelligence\nduring the mid ﬁfties. The primary aims were a) computational\nmodelling of human language production and comprehension and\nb) building intelligent systems with natural language capabilities.\nMany NLP systems were developed mainly to demonstrate ideas,\nproblems and possible solutions. They were all toy systems by to-\nday’s standards. One of the major application area was machine\ntranslation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-4",
                            "title": "In-Depth Understanding",
                            "content": "to know. Literal interpretation of given texts does not constitute\nin-depth understanding.\nWith many more examples of this kind, Dyer in his thesis tries\nto convince the reader that in-depth understanding and human-\nlike question answering behaviour is very much possible for com-\n26\nCHAPTER 1. THE INFORMATION AGE\nputers. Then why is it that we still do not have computers that\nwe can communicate with in natural language? Why are we still\nstuck with complex programming languages with so little expres-\nsive power? Why don’t we still have “intelligent” computers as-\nsisting us in all walks of life?\nIn-depth understanding requires great amounts of world knowl-\nedge. It is all much more to do with commonsense reasoning than\nwith merely linguistic analysis or straight forward mathematical\nlogic.\nDyer’s purpose was to show that given all the required\nknowledge, represented in suitable structures, computers could be\nexpected to give human-like performance in natural language un-\nderstanding. If every piece of knowledge required was available,\ncomputers could perhaps think like us. But this “if” is a very big\n“if”. Dyer could, over a period of several years, carefully hand-\ncode all the required knowledge for demonstrating in-depth under-\nstanding in a limited set of stories. We cannot possibly hand-code\nsuch vast and complex knowledge structures for real applications\nspanning across a wide range of topics and situations. Nor can\nmachines learn such knowledge structures automatically. Com-\nmonsense is what we have learnt automatically by mere contact\nwith the real world. Commonsense is what cannot be explicitly\ntaught or learnt. We still do not know how exactly we acquire\ncommonsense knowledge. We still do not know how to make com-\nputers acquire commonsense knowledge.\nBORIS demonstrated that in-depth understanding was possi-\nble provided all relevant knowledge could be obtained and prop-\nerly represented in the computer. However, the big question that\nearly 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-\nsible for computers to demonstrate in-depth understanding and\nanswer complex questions in a human-like manner. Let us look at\nsamples of actual transcripts from BORIS:\nRichard hadn’t heard from his college roommate Paul\nfor years.\nRichard\nhad borrowed\nmoney from\nPaul,\nwhich was\nnever paid back,\nbut now he had no idea\nwhere to find his old friend. When a letter finally\narrived from San Francisco,\nRichard was anxious to\nfind out how Paul was.\nQ What happened to Richard at home?\nA Richard got a letter from Paul\nQ Who is Paul?\nA Richard’s friend\nQ Did Richard want to see Paul?\nA Yes. Richard wanted to know how Paul was\nQ Had Paul lent money to Richard?\nA Yes. Paul had lent money to Richard\nQ Why didn’t Richard pay Paul back?\nA Richard did not know where Paul was\nQ How did Richard feel when the letter appeared?\nA Richard felt glad because Paul and he were friends\nFIG 1.5 Sample Transcripts from BORIS\n1.3. QUESTION ANSWERING SYSTEMS\n25\nThis sample transcript vividly brings out the depth of under-\nstanding by BORIS. It is not straight forward to answer questions\nsuch as what happened to somebody somewhere. Many things\nmight have happened, some of them may be explicitly given to\nus in the story while some we can infer from our world knowl-\nedge of what “typically” happens in given kinds of situations.\nBut everything that happened or could have happened would not\nbe interesting. Something unusual, something special, something\nworth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was\nComputer Science, Mathematics, Statistics, Physics and En-\ngineering.\nNo individual can claim expertise in so many\nareas. Yet we need to have some understanding and appre-\nciation of the concerns, aims, terminology, deﬁnitions and\nmethodologies of these various disciplines. While good spe-\ncialized books exist, introductory books that give a balanced\nview from diﬀerent perspectives are rare. There is an acute\nshortage of trained manpower in this fascinating and impor-\ntant subject. Perhaps one of reasons for this situation is the\nnon availability of suitable text books. It is hoped that this\nbook will be useful for beginners from varied disciplines.\nThis book grew out of a series of lectures and you will\nﬁnd the same informal, almost conversational style of pre-\nsentation.\nThis is not so much of a formal text book or\na research monograph as it is a beginners’ manual.\nNo\nbackground is assumed. Anybody should be able to read\nthe book without much diﬃculty. The aim has been to en-\nsure technical correctness and soundness while still sound-\ning simple and easy. This is especially diﬃcult in a highly\nmulti-disciplinary area, each area having several competing\ntheories and view points. At times simplicity and under-\nstandability by un-initiated beginners coming from diverse\nbackgrounds had to be given priority over being technically\nthe most precise as per particular theories or models of lan-\nguage.\nThe primary goal of the book is not so much to describe\nstate-of-the-art technologies and advanced research results\nas it is to provide a broad background to help prepare stu-\ndents and researchers coming from diﬀerent backgrounds in-\ncluding computer science, library and information science,\nartiﬁcial intelligence and linguistics. The book aims to pro-\niv\nvide a balanced treatment of linguistic, statistical and tech-\nnological material with an Indian language focus. Treatment\nis kept simple and mathematical jugglery is kept to a mini-\ncommonsense knowledge. We still do not know how to make com-\nputers acquire commonsense knowledge.\nBORIS demonstrated that in-depth understanding was possi-\nble provided all relevant knowledge could be obtained and prop-\nerly represented in the computer. However, the big question that\nremains is how to make the computer obtain all relevant knowl-\nedge. Thus fully automated in-depth understanding has remained\na distant dream even today.\n1.3. QUESTION ANSWERING SYSTEMS\n27\n1.3.5\nTuring Test\nINTERROGATOR\nHUMAN BEING\nMACHINE\nSWITCH\nQUESTION\nANSWER\nANSWER\nANSWER\nFIG 1.6 Turing Test\nIn 1950 Alan Turing devised a test for the thinking ability of\nmachines. An interrogator asks questions and gets answers from\neither a human being or a machine, sitting in diﬀerent rooms, and\ncommunicating only through typed messages. If the interrogator\nis unable to tell the man from the machine purely based on the an-\nswers he gets for his questions, we may conclude that the machine\nis at least as intelligent as the human being. The interrogator is\nfree to ask any question and the machine is allowed to do whatever\nit wants to prove its case. For example, given a simple arithmetic\nproblem the machine may take a long time and answer wrongly.\nHere is a hypothetical conversation given by Turing:\nINTERROGATOR: In the first line of your sonnet which\nreads\n‘‘Shall I compare\nthee\nto a summer’s\nday’’,\nwould not ‘‘a spring day’’ do as well or better?\nA: It woudn’t scan.\nINTERROGATOR:\nHow\nabout\n‘‘a winter’s day’’?\nThat\nwould scan alright.\nA:\nYes,\nbut\nnobody\nwants\nto\nbe\ncompared\nto a\nwinter’s day.\nINTERROGATOR:\nWould\nyou say\nMr. Pickwick\nreminded\nyou of Christmas?\n28\nCHAPTER 1. THE INFORMATION AGE\nA: In a way.\nINTERROGATOR: Yet\nChristmas\nis a winter’s\nday, and\nI do not think Mr. Pickwick would mind the comparison.\nA: I don’t think\nyou are serious.\nBy a winter’s day\none\nmeans a\ntypical\nwinter’s\nday,\nrather\nthan a\nspecial one like Christmas.\nFIG 1.7 A Hypothetical Conversation\nQuestion-answering is a very powerful and ﬂexible means\nfor testing the level of understanding. We are free to select\nthe number, nature and order of questions. We may probe\ndeeper by asking follow-up questions. We may give a set of\noptions to chose from. We may ask the candidate to ﬁll gaps,\nmatch alternatives, deﬁne, describe or explain something, we\nmay ask him to give examples and counter-examples. You\nmight have seen a variety of questions in school examination\nquestion papers, entrance tests and personal interviews. We\nmay prompt the candidate with clues and hints. We may\nask diﬃcult questions just to see how he reacts, rather than\nworry about what actually he says. Applied carefully, one\ncan get a fairly clear idea of somebody’s level of understand-\ning through question-answering.\nOne may also ask a candidate to perform some task that\npresumably requires understanding and guage the level of\nunderstanding by his proﬁciency in performing the task. For\n1.3. QUESTION ANSWERING SYSTEMS\n9\nexample, we may ask the candidate to translate the given\ntext into some other language. Such task based evaluations\nare diﬃcult because the overall proﬁciency in performing\nthe task depends not only on proper understanding of the\ngiven text but also on other factors such as proﬁciency in the\ntarget language for translation. Question answering remains\nthe best bet.\nPresumably, one has to understand the meaning of a text\nbefore he or she can answer questions relating to the given\ntext. Naturally there was a great deal of interest in building\nquestion-answering systems in the early days of NLP. Here\nwe sample a few of them just to get a taste of the nature of\nthe problem and the range of ideas that have been tried out.\nWe begin with a very simple yet highly successful system\nthat was however never intended to be an NLP system nor\nwas it designed to answer any questions at all.\nYet this\nsystem has a simple and neat design and knowing this helps",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-3-subsection-5",
                            "title": "Turing Test",
                            "content": "commonsense knowledge. We still do not know how to make com-\nputers acquire commonsense knowledge.\nBORIS demonstrated that in-depth understanding was possi-\nble provided all relevant knowledge could be obtained and prop-\nerly represented in the computer. However, the big question that\nremains is how to make the computer obtain all relevant knowl-\nedge. Thus fully automated in-depth understanding has remained\na distant dream even today.\n1.3. QUESTION ANSWERING SYSTEMS\n27\n1.3.5\nTuring Test\nINTERROGATOR\nHUMAN BEING\nMACHINE\nSWITCH\nQUESTION\nANSWER\nANSWER\nANSWER\nFIG 1.6 Turing Test\nIn 1950 Alan Turing devised a test for the thinking ability of\nmachines. An interrogator asks questions and gets answers from\neither a human being or a machine, sitting in diﬀerent rooms, and\ncommunicating only through typed messages. If the interrogator\nis unable to tell the man from the machine purely based on the an-\nswers he gets for his questions, we may conclude that the machine\nis at least as intelligent as the human being. The interrogator is\nfree to ask any question and the machine is allowed to do whatever\nit wants to prove its case. For example, given a simple arithmetic\nproblem the machine may take a long time and answer wrongly.\nHere is a hypothetical conversation given by Turing:\nINTERROGATOR: In the first line of your sonnet which\nreads\n‘‘Shall I compare\nthee\nto a summer’s\nday’’,\nwould not ‘‘a spring day’’ do as well or better?\nA: It woudn’t scan.\nINTERROGATOR:\nHow\nabout\n‘‘a winter’s day’’?\nThat\nwould scan alright.\nA:\nYes,\nbut\nnobody\nwants\nto\nbe\ncompared\nto a\nwinter’s day.\nINTERROGATOR:\nWould\nyou say\nMr. Pickwick\nreminded\nyou of Christmas?\n28\nCHAPTER 1. THE INFORMATION AGE\nA: In a way.\nINTERROGATOR: Yet\nChristmas\nis a winter’s\nday, and\nI do not think Mr. Pickwick would mind the comparison.\nA: I don’t think\nyou are serious.\nBy a winter’s day\none\nmeans a\ntypical\nwinter’s\nday,\nrather\nthan a\nspecial one like Christmas.\nFIG 1.7 A Hypothetical Conversation\nA: In a way.\nINTERROGATOR: Yet\nChristmas\nis a winter’s\nday, and\nI do not think Mr. Pickwick would mind the comparison.\nA: I don’t think\nyou are serious.\nBy a winter’s day\none\nmeans a\ntypical\nwinter’s\nday,\nrather\nthan a\nspecial one like Christmas.\nFIG 1.7 A Hypothetical Conversation\nIf the “A” in the above conversation were actually a machine\nrather than a human being, perhaps we can accept that the ma-\nchine was intelligent and capable of thinking and understanding.\nOf course there is no machine today that can pass the Turing\nTest and nobody knows if we will ever be able to develop one.\nIt is interesting to note that question answering based on natural\nlanguage understanding and generation has been used as a test\nof human-like intelligence. Indeed language is at the very core of\nhuman intelligence.\nNLP started oﬀwith simple ideas and toy systems. Over the\nyears, researchers have discovered how complex natural languages\nare. Focus has shifted from building toy systems to developing\nlarge scale linguistic data resources, wide coverage grammars and\nparsers etc. Statistical analysis and machine learning techniques\nare being combined with linguistic approaches. Scalability and\nadaptability or trainability have now become very important is-\nsues. Many useful applications have emerged. Yet we are far from\nthe ultimate goals of NLP - human-like understanding, genera-\ntion and language learning by machines. There are no question-\nanswering systems today that we can reliably use for accessing\nwhatever kind of information we may need.\n1.4\nInformation Retrieval\nIn this section we sketch the rudiments of modern IR systems.\nThe ideas we describe here will form the background for more\nadvanced techniques we will be looking at in Chapter Three.\n1.4. INFORMATION RETRIEVAL\n29\n1.4.1\nIR Deﬁned\nInformation Retrieval is a vast ﬁeld concerned with the storage\nand retrieval of documents. Documents may include texts, im-\nages, video, speech, music and web pages in various combinations.\nPeople have world knowledge and commonsense and this forms a\ntacit assumption when dealing with people. The goal of NLP, on\nthe other hand, is to make the computer, the idiot box that has no\ncommonsense, world knowledge or human-like reasoning power, to\nperform language processing tasks with human-like performance.\nA few examples are not enough. Exhaustive sets of data, rules,\nguidelines and principles need to be provided to handle special,\nunusual situations as also the run-of-the-mill cases. Explaining in\nsimple language is not suﬃcient, we will have to write detailed\nand precise program code.\nLet us take a speciﬁc example. One of the primary goals of\nmodern generative linguistics lead by Noam Chomsky is to under-\nstand how children, exposed to such small and imperfect samples\n96\nCHAPTER 2. FOUNDATIONS OF NLP\nof language use, can learn the structure and properties of their\nnative language, whatever that may be, so easily within just a\ncouple of years. From a computational point of view, an accept-\nable solution to this problem would be a computational model of\nchild language acquisition which can be experimentally tested. We\nshould be able to give controlled input and observe the outputs\nas well as the changes that take place in the internal structure of\nthe model to see what exactly constitutes learning and how the\nchild is actually learning with each input sample. We cannot open\nup somebody’s brain and see before and after changes as a sen-\ntence is being processed by the brain. But with a computational\nmodel such a detailed probing would be feasible. For example, if\nwe build a neural network model, we can see the changes taking\nplace in the various synaptic weights with each iteration.\nThe\nindividual numbers may not make much sense but we can surely\nget a global picture of whether the system is learning, at what\nrate it is learning, has a saturation point been reached etc. De-\nspite the great deal of eﬀort that has gone into building linguistic\n16\nCHAPTER 1. THE INFORMATION AGE\nEvery keypunch operator is a girl.\nI UNDERSTAND\nAny girl is an example of a person.\nI UNDERSTAND\nIs a keypuch operator a person?\nYES\nIs a person a person?\nYES\nIs a person a girl?\nSOMETIMES\nIs a monkey a keypunch operator?\nINSUFFICIENT INFORMATION\nFIG 1.3 Sample Transcripts from SIR\nThese transcripts clearly bring out the diﬀerences between hu-\nman beings and machines. We human beings have a great deal of\nworld knowledge and we are capable of very sophisticated com-\nmon sense reasoning. Machines lack both world knowledge and\ncommon sense reasoning. We do not need to be told that a girl is\na person - we know it. We do not need to be given a rule which\nsays an X is a X - we know it. But computers need to be told\neven such “obvious” things. This is why it has not been possible\nto build machines that show human like intelligent behaviour even\ntill date.\nLanguage is not a completely independent and autonomous\nfaculty of the mind - knowledge representation, reasoning, learn-\n1.3. QUESTION ANSWERING SYSTEMS\n17\ning are all inseparably tied up with the language faculty. There\nwas a time when somebody could become a good painter if he had\nsome imagination and skill in handling the brush. To become a\nsuccessful painter today, one must know the physics of light, the\nchemistry of the paint, the biology of the eye, the psychology of\ncolour and many other things. Language must be explored not\nonly from the point of view of linguistics but also from the per-\nspectives of psychology, cognitive science, artiﬁcial intelligence,\nstatistics, mathematics, physics, computer science and engineer-\ning. We have created artiﬁcial boundaries of departments, disci-\nplines and specializations for the sake of convenience but now they\nseem to have become major barriers for scientiﬁc exploration. Sci-\nence today is highly inter-disciplinary. This is perhaps the most\nimportant lesson we have to learn from these early NLP systems.\nPSP2\nchines can do better than us.\nWe can write a C compiler but\nthe machine does the compilation better. Computers can execute\nprograms with great speed and accuracy, we cannot. We can de-\nvelop new programming languages but we are in fact not very\ngood at writing programs ourselves. Computer programming is\ndiﬃcult because it requires you to think like a dumb machine and\nit is not easy for intelligent human beings to think like a machine.\nEven experts can rarely write a program correctly the ﬁrst time.\nThis is not because they do not understand clearly what is it\nthat they want to do but it is because writing computer programs\nis an inherently diﬃcult task. The human brain is designed for\ncommunicating with natural languages, not with computer pro-\ngramming languages. It is easy to explain what a program should\ndo in English but is not so easy to express the same thing in a\nprogramming language. It is easy for us to draw an approximate\ncircle but diﬃcult to draw an exact circle. On the other hand it\nis straightforward to write a program to make the machine draw\na perfect circle. It is much more diﬃcult to make it draw an ap-\nproximate circle. The machine does not know what you mean by\n“approximate circle” - it expects you to deﬁne it precisely! The\ncomputer has been of great help to me in writing this book - I\nhave written, erased, placed it back, re-arranged, re-written many\nmany times. I could not have done all that with equal convenience\nand speed sitting with paper and pencil. But the machine could\nnot have written this book by itself. The critical factor, therefore,\nis to understand what kinds of things machines are good at and\nexploit them for our beneﬁt. Machines can indeed do many more\nthings than we expect of them.\nIn terms of speed, automated systems are far superior to man-\nual methods in most tasks. Computers can process thousands,\nlakhs or even millions of documents in the time it takes us to\ngo through just a few pages. In terms of accuracy, automated",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-4",
                    "title": "Information Retrieval",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-4-subsection-1",
                            "title": "IR Defined",
                            "content": "exclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models\nAn IR model must specify four things:\n• Document Representation\n• Query Representation\n• Deﬁnition of Relevance\n• Matching/Retrieval Function\nThe simplest way one can think of matching documents and\nquery is to perform direct keyword search - search for the given\nquery string verbatim in the documents and return the documents\nthat contain the given query string. This is too rigid. Computer\nwill not match Computers or even computer if the search is case\nsensitive. So some kind of soft match may be introduced. Even\nwith that extension the search is too rigid.\nSuppose you want\ndocuments containing the words Sun, Moon, orbit, eclipse but not\n3.2. BASIC IR MODELS\n331\nnecessarily all at one place and not necessarily exactly in the order\ngiven. It would then be helpful to consider documents and queries\nas unordered collections of words. This is called that bag-of-words\nrepresentation. A bag is an unordered collection of items just like\na set, only multiple occurrences are allowed. Some early systems\nused pre-speciﬁed sets of index terms. Current systems tend to\nprefer full text indexing. It is also possible to exploit structure\nof documents and meta-data ( such as URL, title, anchor string,\nauthor and other meta tags, font sizes, capitalization and position\nin the document). Thus the HTML/XML structure in hypertext\ndocuments could be made use of.\nThe Boolean Retrieval Model\nThe Boolean model is based on set theoretic principles. Here doc-\numents are treated as sets of keywords.\nNote that a set is an\nunordered collection of items. Frequency of occurrence is not rel-\nevant. In a set an item is either present or absent. Items cannot\nrepeat. Queries are treated as Boolean expressions of keywords,\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nthe correct sense of the word can we be sure that we are looking\nat the right document. We may try to take into account the order\nof words in the query. A full syntactic analysis may be carried\n3.3. TOWARDS INTELLIGENT IR\n339\nout. We may try to adapt to the user based on direct or indirect\nfeedback. We may try to take into account the authority of the\nsource.\nWe have already seen that many of the NLP tasks are inher-\nently diﬃcult in themselves and performance of current systems\nis limited. Thus full syntactic analysis is not only time consuming\nbut also limited in performance - current grammars and parsers\nfail on a signiﬁcant percentage of cases. Unrestricted WSD sys-\ntems are still at a research level. The real challenge, therefore, is\nto integrate the best of available NLP technologies with IR models\nwithout sacriﬁcing the simplicity and eﬃciency of IR models. We\nwill explore below some the ideas and techniques that have been\nused to build better IR systems.\n3.3.1\nImproving User Queries - Relevance Feed-\nback\nA big question is how do we assess the relevance of a retrieved\ndocument for a given query? Relevance is a subjective judgment\nand may include being on the proper subject, being timely (re-\ncent information), being authoritative (from a trusted source),\nsatisfying the goals of the user and his/her intended use of the\ninformation (information need), etc. Relevance is not an yes/no\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\nprovides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers\nand industrial partners. It is based on the Resource Description\nFramework (RDF), which integrates a variety of applications us-\ning XML for syntax and URIs for naming. See Scientiﬁc American\nMay 2001 issue for an interesting article by Tim Berners-Lee.\n3.3.7\nInformation Retrieval is Diﬃcult\nIR is an inherently diﬃcult task. Can you search millions of doc-\numents and accurately suggest the most relevant documents for a\ngiven query in a fraction of a second? In IR we are asking com-\nputers to do what we human beings cannot do. And we want IR\nto be fully automatic - there is no scope for human intervention.\nWhat makes IR diﬃcult? There are three major issues:\n• Understanding user needs: Understanding exactly what the\nuser is looking for is not easy. Key words do not tell us\nwhat is the purpose of the current search, what all the user\nalready knows, what all he has already searched or what\nlevel of abstraction would suit his level of knowledge and\nexpertise. Social and cultural contexts are important. It\nlooks strange that we set forth on a grand searching and\nretrieval operation without understanding exactly what we\nare looking for!\n• Understanding the Documents: Unless you know exactly\nwhat the documents contain, what they pertain to and what\n3.3. TOWARDS INTELLIGENT IR\n351\nall things they include, for whom it is written, what back-\nground is assumed etc., how can we say which documents\nwill suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient\nto go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant\nto the query. As we have already seen, the most common approach\nis to view documents and queries are bags of words and represent\nthem as feature vectors where each feature corresponds to one\nword. Similarity between feature vectors is quantiﬁed in terms\nof the orientations of these vectors. Performance is measured in\nterms of Precision and Recall.\nIntelligent IR, on the other hand, requires that we consider the\nsyntax as well as semantics of documents and queries, we adapt\nto users based on direct or indirect feedback and learning, and we\ntake care of authority and dependability of documents. An ideal\ninformation retrieval system is one that can perform like a human\nassistant. This is really the software grand challenge. Obviously,\nwe are far from such an ideal.\nLet us now see what kinds of\nimprovements and enhancements can be or have been made in\nthe ﬁeld of information retrieval.\nMere presence or absence of keywords is clearly too naive a\nview of a document. Can we say I have a bad head ache and Now\nI am free from head ache mean the same thing and both match\nthe query head ache equally well? Is India beat Australia same as\nAustralia beat India? Can we equate I like Govinda’s movies and\nI like Govinda’s movies as much as I would like a burning stove if\nI were sitting on it? Most current IR systems continue to use the\nbag-of-words representation while examples like this clearly show\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-2",
                            "title": "Documents as Bags-of-Words",
                            "content": "traditional library setting, a document may be a book while on the\nInternet a document may be just a single web page. A Collection\nis a set of such documents. A Query refers to a short formulation\nof a user request in a suitable format.\nSEARCH ENGINE\nCRAWLER\nINDEX\nDOCUMENT\nREPOSITORY\nQUERY\nRESPONSE\nUSER\nFIG 1.8 Information Retrieval\n30\nCHAPTER 1. THE INFORMATION AGE\n1.4.2\nDocuments as Bags-of-Words\nThe meaning of a text is indicated by the meanings of the words\nused as also by the syntactic, semantic and pragmatic structures\ninto which these words are placed in a coherent framework. Nev-\nertheless, it is very common in IR systems to take the extreme\nview that texts are merely unordered sets of words, without any\nregard to syntactic, semantic or discourse structure. Thus ’I eat,\ntherefore I am’ and ’I am, therefore I eat’ will be treated as equiv-\nalent! This representation is called the bag-of-words representa-\ntion and has been widely used, despite its obvious limitations. In\nthe bag-of-words representation the document texts as well as the\nuser queries are represented as unordered collections of words and\nword-like features such as phrases, together known as terms.\nQueries may be simple or they may be structured query ex-\npressions involving Boolean operators such as AND, OR and NOT.\nYou may ask, for example, for documents containing the terms\n“Data” AND “Mining” but NOT “Coal” OR “Gold”. If exact\nmatch is expected, there are chances of getting too few or too\nmany matching documents, especially when the document collec-\ntions are large and heterogeneous. The queries may be simply too\ngeneral or too very speciﬁc. Recent research has therefore focused\non probabilistic models where documents are ranked according to\ntheir estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.\nof documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nIn this chapter we will look at some the recent advances in\nthe ﬁeld of Information Retrieval. We begin with a brief historical\nsketch and then give the basic IR model. Then we look at various\ndirections of development of the IR ﬁeld. The idea of Intelligent\nInformation Retrieval will be introduced and the need for deeper\nlinguistic analysis will be highlighted.\n3.1\nA Brief History of Information Re-\ntrieval\nInitial explorations of text retrieval systems from small collections\nof scientiﬁc abstracts, legal and business documents etc. began\nin the nineteen sixties and seventies. Foundations of Boolean and\nvector-space models were developed. During these early days, doc-\numents were studied and brief descriptors or lists of index terms\nwere manually prepared for each document.\n327\n328\nCHAPTER 3. ADVANCES IN IR\nDuring the eighties, large scale document databases started\nappearing.\nLexis-Nexis, Dialog and MEDLINE are noteworthy\nexamples of such databases. The need for eﬃcient retrieval from\nlarge collections was increasingly felt. This gave a big push to IR\nresearch and development.\nDuring the nineties, focus shifted to searching FTP-able doc-\numents on Internet (example: Archie, WAIS) and searching the\nWorld Wide Web (example: Lycos, Yahoo, Altavista). Organized\ncompetitions such as NIST TREC were held. Recommender sys-\ntems (example: Ringo, Amazon, NetPerceptions) appeared. Au-\ntomated text categorization and clustering systems were devel-\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\n2.2. COMPUTATIONAL LINGUISTICS\n109\npragmatic knowledge is hazy.\nFrom a computational point of view, completeness and consis-\ntency are desirable. We cannot include “November” and exclude\n“December”.\nThe list of function words and other closed-class\nwords must be complete. Redundancy should be minimized. Cir-\ncular deﬁnitions must be avoided. A detailed analysis by computer\npoints to many places where such requirements have not been met\nin many good printed dictionaries. For example, in one dictionary\n’body’ is deﬁned as ’the whole of a person’ and used in a diﬀerent\nsense in deﬁning ’parliament’ as ’a law-making body’. In another\ndictionary a ’box’ is deﬁned as ’a container’ and then a ’container’\nas ’a very large, usually metal box’. Circular deﬁnitions of higher\npath lengths are very diﬃcult to detect.\nLay users as well as students of NLP often have a very sim-\nplistic view of dictionaries and lexicography. Developing lexical\nresources is considered a lower job, more akin to typing work, and\nnobody wants to work on these areas. There is always a fancy\nfor big tasks such as machine translation or speech recognition. If\nsigniﬁcant progress has to be made in any of the applications of\nNLP, lexical resources must be given great attention. One of the\npurposes of this section has been to show that there is more to it\nthan meets the eye when it comes to dictionaries.\nStructure and Organization of a Dictionary\nThe most common organization of dictionary entries is a alpha-\nbetically sorted list of entries. Such a dictionary arranged in al-\nphabetical order makes it simple and eﬃcient to look up a given\nword. Eﬃcient data structures and algorithms exist for searching\nin sorted lists of items. Data structures such as height balanced\nbinary search trees (also called AVL Trees), height balanced m-\nary trees (such as B-Trees, B+ Trees and B* trees), splay trees,\nTRIE etc. have been used. Readers may refer to any good text",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-3",
                            "title": "The Vector Space Model",
                            "content": "rant” will not match with “cafe”. “PRC” will not match with\n“China”.\nBut “Bat” as in cricket may match with the mam-\nmal called bat. “Apple” will treat the apple fruit and the Apple\ncomputer without distinction. Vector space models lack the tight\ncontrol that Boolean models permit. Given a query “A B”, vector\nspace models may prefer a document containing A frequently but\nnot B, or B frequently but not A, over documents that contain\nboth A and B but less frequently. Is this what we really want in\nall situations?\nIn spite of the fact that there are still many open questions,\nvector space models have worked fairly well in practice, especially\nfor large collections such as the web. They are simple, based on\nprinciples of mathematics and statistics and amenable for eﬃcient\nimplementations. They are more ﬂexible than Boolean Models -\npartial matching is allowed and the retrieved documents can be\nranked and ordered. The vector space model based on the bag-\nof-words representation is the mainstream approach in IR today.\n3.2.2\nTerm Weighting: tf-idf\nIt may be argued that the number of times a term occurs in a\ndocument is a stronger indicator of its semantic content as com-\npared to mere presence or absence of terms. Thus feature values\ncan be taken as Term Frequency (TF) rather than as Boolean.\nAn inverted index can also be maintained, giving for each word,\nthe frequencies and the positions of the word for each document.\nThe inverted index makes it easy to locate documents containing\nparticular words or phrases. Thus if the terms Batting, Bowling,\nFielding, LBW, Run-Out, Boundary, Catch, Run etc. occur fre-\nquently, we may think that that the document must be related to\nCricket.\nOne cannot be so sure that the document is about football\nif the word goal appears frequently because the word goal can\nappear in many diﬀerent kinds of documents with diﬀerent mean-\nings. Similarly, if some words, say, important, however, also, re-\n334\nCHAPTER 3. ADVANCES IN IR\nentity) is a possible feature. Features are given numerical values.\nThus feature vectors can be geometrically visualized as points in\nn-dimensional space or as vectors connecting the origin to these\npoints. The spatial similarity between such vectors is used as a\nmetaphor to deﬁne the similarity between documents and queries.\nRetrieved documents are ranked based on the degree of similarity.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 3.1 The Vector Space Model in Two Dimensions\nThere are several issues in the design of a vector space model.\nHow do we determine which terms in a given document are impor-\ntant? What about word sense? Should we take the surface form\nof words or the root forms? What deﬁnition of word do we use?\nHow do we take care of multi-token words, compounds, idioms\nand phrases? What is the relationship between the importance of\na given term for a given document in relation to its importance\nfor other documents in the collection or for the whole collection?\nHow exactly do we compute the similarity between a query and\na document? In the case of a large hyperlinked and highly dy-\nnamic collection such as the world wide web, what exactly is the\ncollection and what are the eﬀects of the hyperlinks, formatting\ninformation and meta data available? There are no perfect an-\nswers to all the questions but we will be able to give some ideas\nas we proceed.\n3.2. BASIC IR MODELS\n333\nVector space models based on the bag-of-words representa-\ntion ignores syntax (word order, phrase structure, proximity, and\nmore) and semantics (word senses, scope of quantiﬁers and nega-\ntion, anaphoric references, synonymy and other kinds of relation-\nships between words and between words and concepts). “Restau-\nrant” will not match with “cafe”. “PRC” will not match with\n“China”.\nBut “Bat” as in cricket may match with the mam-\nmal called bat. “Apple” will treat the apple fruit and the Apple\ncomputer without distinction. Vector space models lack the tight\ntures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.\nEach word in a text is a potential feature. In the domain of\ntext categorization, words and word-like features (such as phrases)\n1.7. AUTOMATIC TEXT CATEGORIZATION\n51\nare called terms. Documents are treated as bags of terms. Fea-\nture dimensions are thus often very large, running into tens of\nthousands.\nThe curse of dimensionality is that the number of\ntraining data samples required grows exponentially with the num-\nber of features. Choice of the right subset of potential features\nis a major concern. A variety of dimensionality reduction tech-\nniques are used in pattern recognition. The discriminating power\nof features is evaluated and the least discriminating features can\nbe discarded without much loss. Concepts such as Mutual Infor-\nmation and Information Gain have been applied to evaluate the\ndiscriminating power of features. Principal Component Analysis\nis a standard technique for identifying the feature dimensions with\nmaximal variance. Similarly, Singular Value Decomposition is a\ntechnique that rotates the feature space so as to align the most\ndiscriminating features along the axes of the rotated feature space.\nInterested readers are directed to books on Pattern Classiﬁcation\nor Machine Learning for more details.\nCommonly used pre-processing steps include\n• Stop word removal - eliminating function words and other\nvery frequently occurring, less discriminative terms\n• Morphology or Stemming - replacing fully inﬂected words\nwith their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting\ntheir estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.\nIn the bag-of-words approach, features corre-\nspond to the terms - each term (word or phrase) is a potential\nfeature.\nFeatures are given numerical values.\nIn the simplest\ncase, the feature values are Boolean - that is, a value is 1 if the\ncorresponding term occurs in the document or the query as the\ncase may be, and 0, otherwise. Alternatively, the numerical value\nof a feature can be simply the number of times it occurs in the\ndocument. Each vector is thus simply a list of numbers.\nA vector of n such features can be geometrically viewed as\na point in n-dimensional space. The geometric spatial proximity\nbetween two vectors is used as a metaphor for the semantic prox-\n1.4. INFORMATION RETRIEVAL\n31\nimity between the corresponding documents and/or query strings.\nThis model is thus conceptually simple and appealing. The most\nrelevant documents are the ones that include the most terms from\na given query and thus spatially closest to the query vector. The\nexample below illustrates these ideas in just two dimensions. If\nthere were three terms, we need to consider vectors in three dimen-\nsional space and, by extension of this idea, if there are n terms, the\nvectors will be in n-dimensional space - diﬃcult to get a mental\npicture but mathematically a simple extension all the same.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 1.9 Vector-Space Model\nThe direction of the vectors is a good indicator of the semantic\ncontent of the corresponding documents, not the length of the\nvectors. Hence it is usual to normalize the lengths of all vectors\nto unity and consider only the directions.\n1.4.4\nPerformance Evaluation\nPerformance of IR systems is measured in terms of Recall and\nare semantically similar according to the co-occurrence analysis.\nLSI can thus be viewed as a similarity metric and an alternative\nto word overlap measures such as tf.idf. The latent semantic space\nhas fewer dimensions and thus LSI can also be looked upon as a\ndimensionality reduction technique.\nLSI chooses an optimal mapping among various possible map-\n346\nCHAPTER 3. ADVANCES IN IR\npings to a lower dimension. This is achieved by using a mathe-\nmatical technique known as Singular Value Decomposition (SVD).\nLSI is basically an application of SVD to the word-by-document\nmatrix. The dimensions of the reduced space correspond to the\naxes of greatest variation.\nThe idea is to capture as much of\nthe variation in the data as possible. SVD can be viewed as a\nmethod to rotate the axes of on an n-dimensional space so that\nthe ﬁrst axis runs along the direction of largest variation, the sec-\nond axis runs along the direction of the second largest variation,\nand so on. Thus SVD is similar to Principle Component Analysis\n(PCA). PCA is applicable only to square matrices whereas SVD\ncan be applied to any matrix. SVD is a least square method. SVD\ntakes a matrix A and represents it as ˆA in a lower dimensional\nspace such that the “distance” between the two as measured by\nthe 2-norm is minimized:\n△=∥A −ˆA ∥2\nThe 2-norm is for matrices what Euclidean distance is for\nvectors.\nSVD, and therefore LSI, assumes normal distribution which\nis seldom true for term distributions. Also SVD can be slower.\nHence the use of LSI is justiﬁed only when the performance im-\nprovements are very signiﬁcant.\n3.3.5\nMeta Search Engines\nWith millions of pages being added to the World Wide Web every\nyear it was realized that no search engine could possibly index all\nthe information on the web and be reasonably eﬃcient at the same\ntime. The web is extremely large. To get an idea, consider these\n1996 ﬁgures for the number of pages indexed by various search",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-4",
                            "title": "Performance Evaluation",
                            "content": "things than we expect of them.\nIn terms of speed, automated systems are far superior to man-\nual methods in most tasks. Computers can process thousands,\nlakhs or even millions of documents in the time it takes us to\ngo through just a few pages. In terms of accuracy, automated\nmethods give comparable or superior performance today in cer-\ntain areas such as Information Retrieval, and Text Categorizations\nwhen the data is very large. On the other hand, people are often\n1.11. SHAPE OF THINGS TO COME\n81\nmuch better compared to machines when it comes to tasks such\nas translation or question answering. But we must remember that\neven when automated systems give good performance, their scope\nis often limited and they may be brittle - even a small change\nin the conditions may bring down the performance drastically.\nFor example, today’s speech recognition systems are not robust\nagainst such simple looking variations as change of microphone\nor even moving the head a little away from the microphone while\nspeaking. Human beings are generally more robust than machines\nin most cases.\nThe question therefore is not whether people are better or\nmachines are better. Each of them have their own strengths and\nweaknesses. The challenge is to properly understand the strengths\nand weaknesses of the two and build man-machine synergies that\ncan produce the best results with minimal eﬀort.\n1.11\nShape of Things to Come\nSo far we have seen several interesting and useful applications of\nhuman language technologies. Our treatment has been introduc-\ntory and informal. The focus has been on what kinds of things\ncan be and have been done. In the process we have got some feel\nfor the need for thorough linguistic and statistical analysis of lan-\nguage, techniques for representing and reasoning with knowledge,\ncomputational methodologies for making the machines learn from\ndata, and large scale linguistic data resources that permit eﬀective\nlearning and generalization by machines. We have made progress\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\nLanguage Understanding. However, some researchers in the IR\nﬁeld have traditionally considered it neither absolutely essential\nnor always highly beneﬁcial to carry out in depth linguistic anal-\nysis of documents or user queries. The challenge they have set for\nthemselves is to achieve high levels of retrieval performance with-\n34\nCHAPTER 1. THE INFORMATION AGE\nout recourse to Natural Language Processing (NLP) in any great\nmeasure. Empirical studies of the use of advanced NLP techniques\nhave also given mixed results - in some cases there was some im-\nprovement in performance while in other cases either there was no\nsigniﬁcant improvement or there was actually a small reduction in\nperformance. This could be for various reasons including possibly\nthe kind of linguistic analysis that was carried out and the speciﬁc\nIR tasks and evaluation methods employed.\nPart of the reason for diﬀerences of opinion on the role of\nNLP is the criteria for success. What is it that we want in the\nend? Is the performance measured in terms of Precision, Recall\nor whatever the only criterion for success? Or are we looking for\nintelligent IR systems, intelligent IE systems and so on? In the\nlong run, speciﬁc applications such as IR, IE, Categorization and\nSummarization must be viewed in the context of intelligent pro-\ncessing of human languages by machines. If we can move towards\nmachines that are capable of human-like understanding, generat-\ning and learning natural languages, we can not only get better\nIR systems but also many other applications that have not taken\noﬀyet. Automatic Programming has long remained unsuccess-\nful. If one day we could tell computers instead of program them,\nthe whole world of software engineering will change dramatically.\nResearch should not be constrained too much by forces of pro-\nfessionalism. Asking the right questions is more important than\nbeing successful all the times.\nThe bag of words representation is too crude. Documents are\neven incorporate a semantics component in any serious measure.\nThere is even a claim that a lot can be done without going for\nany in-depth linguistic analysis. And practical experience does\nshow that in some tasks high performance can be achieved by\nonly a very superﬁcial analysis provided large scale training data\nis available and the right methods are used for learning from this\ndata. For example, text categorization systems achieve 95% plus\nperformance by using raw words as features, without need for any\ndictionary or morphological analysis, let alone syntax or seman-\ntics. Human beings are also far from perfect and in some tasks\nit is possible to achieve performance comparable to or even some\nwhat better than human performance without need for in-depth\nlinguistic analysis.\nIt would be wrong, however, to conclude that machines have\nreached or are approaching human levels of intelligence. Given\nsuitable, data, sets of features to use, a method to compute and\nweigh the features and a classiﬁcation method, automatic tech-\nniques exist for performing classiﬁcation in an optimal way as\ndeﬁned by some given criteria. Human beings can also do this,\nalthough they may take more time than computers or commit\nmistakes while computing, but they can also do much more. Ma-\nchine learning today is largely restricted to generalization from\nexamples. But we human beings can ’learn’ even new methods of\nmachine learning. We learn to identify discriminative features, we\nlearn to weigh them and we learn the optimization criteria and\nmethods. There is no comparison between machines and human\nbeings. High performance is achievable only in some restricted\ntasks under suitable assumptions. This does not mean that ma-\nchines can understand the meaning of texts nor does it mean that\nunderstanding meanings is not necessary.\nIn the section on syntax, we said form follows function and\nhence knowing the structure greatly facilitates understanding the\nValues of the decision variable for the two classes are chosen sym-\nmetrically around zero and the parameters are estimated from the\ntraining data. A test sample can then be classiﬁed as belonging to\nclass C1 or C2 depending upon whether the value of the decision\nvariable is positive or negative. It is possible to reject a point if\nthe value of the decision variable is too close to zero, say, closer\nthan a speciﬁed threshold.\nClassiﬁcation performance can be speciﬁed in terms of Pre-\ncision and Recall, or using some combined measure such as the\nF-Measure:\nRecall =\nOk\nTotal ∗100\n(2.6)\nPrecision =\nOk\nTotal −Unknown ∗100\n(2.7)\nF = 2PR\nP + R\n(2.8)\nwhere Ok is the number of test samples that are correctly classi-\nﬁed, Unknown is the number of test samples that are not classiﬁed\nand Total is the total size of the test data. There is usually a trade\noﬀbetween Precision and Recall and a single combined measure is\ntherefore useful for comparison. F-Measure is one such measure.\nThe deﬁnition shown here gives equal weightage for Precision and\nRecall.\nWe have outlined a general method for supervised two-class\nclassiﬁcation using Multiple Linear Regression.\nThe method is\nconceptually simple and based on sound theoretical foundations.\nThe method is symmetric in the features. Although matrix inver-\nsion is required for estimating the values of the parameters, once\n2.3. STATISTICAL APPROACHES\n249\nthe model is built classifying objects is very eﬃcient - only com-\nputation of the linear regression equation and checking the sign\nof the decision variable are required. The technique is thus highly\nsuitable for two-class classiﬁcation problems with a reasonably\nsmall number of features.\nTechniques also exist for validating the adequacy of the model\nfor a given problem and for evaluating the relative signiﬁcance of\nthe various features (which can be used for feature selection).\nWe will illustrate the use of regression as a classiﬁcation tool",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-5",
                            "title": "Measuring Relevance",
                            "content": "unchangeable. Meaning depends upon the reader’s interest, back-\nground, purpose, attitudes etc. Relevance is not a simple yes-no\nquestion. We can therefore think of degree of relevance. No sin-\ngle document may be highly relevant but a combination of two or\nmore may be.\nClearly, such a strong view of relevance is subjective and im-\npracticable for automatic evaluation. Hence in practice a weaker\ndeﬁnition involving only the topical relevance is used as an indica-\ntor of potentially useful documents. Topical relevance is necessary\nbut not suﬃcient. It is easier to deal with and often a major con-\ntributor to total relevance. Topical relevance is more closely tied\nup with the document itself and not so much to idiosyncrasies\nof individual users. The relevance of documents is measured by\nsimply counting the proportion of terms in the query which are\nfound in the documents retrieved. The retrieved documents are\nranked accordingly.\nWhat we have seen in this section is the bare-bones descrip-\ntion of a modern IR system. It is deﬁcient in many ways and a\nlarge number of ideas have been proposed and used to go beyond\nthis primitive design. Chapter Three of the book is devoted to ad-\ndressing these concerns in some detail and Chapter Two provides\nthe required background in Natural Language Processing.\n1.4. INFORMATION RETRIEVAL\n33\n1.4.6\nChallenges in Information Retrieval\nIn his Turing Award lecture, Jim Grey deﬁned the Software Grand\nChallenge as a software that could answer questions as eﬀectively\nas an educated person. Answering questions, or even just retriev-\ning relevant documents from which we can hopefully ﬁnd answers\nto our speciﬁc questions, is not easy. There are three steps in the\nprocess and each one is a challenge - 1) understand exactly what\nthe user wants 2) understand the contents of the documents so\nyou know which document is relevant for what, and 3) develop\nautomatic methods for matching the user requirements with the\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\nassumptions in practice. For example, we may use the simplest\nnotion of relevance that the query string appears verbatim in the\ndocument. A slightly less strict notion could be that the words in\nthe query appear frequently in the document, in any order (bag\nof words).\nInstead of looking for improving the performance of retrieval\nfor given queries, we may invert the problem and see how the\nquery itself may be improved so that performance is maximized.\nThis makes sense because it is not always very easy for a user\nto specify exactly what he wants as a query. An ideal query is\none that expresses the user’s requirements precisely in relation to\nthe documents in the collection.\nBut ﬁnding an ideal query is\ndiﬃcult unless we already know exactly what the documents in\n340\nCHAPTER 3. ADVANCES IN IR\nthe collection contain. We may therefore start with a good guess\nand hope to improve after we see some results. We make the as-\nsumption that documents which are relevant to a given query are\nsimilar. Therefore a query can be improved by making it closer\nto relevant documents retrieved and farther from the irrelevant\ndocuments retrieved. Thus by looking at the returned results and\njudging them as relevant or otherwise, we may obtain an improved\nquery. More terms from the relevant documents retrieved can be\nadded and weights for the terms can also be adjusted based on\nthe frequency of occurrence of those terms in the relevant and ir-\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nInstead of interactively improving the query, one may assume\nthat the top few (say, 20) hits are all actually relevant and au-\ntomatically improve the query without asking users to judge any\nretrieved documents. This technique is termed Pseudo-relevance\nFeedback.\n3.3.2\nPage Ranking\nRetrieving the documents that best match the given query does\nnot necessarily give us the best overall performance. There are a\nvery large number of documents on the Internet and too many of\nthem may match equally well. Simply listing a large number of\nrelevant documents is not very good. How do we help the users\nthen? We should use not only the terms in the documents and\nqueries for matching and ranking the documents, but also some\ngeneral measure of goodness of various documents. It would be\nnice if we could somehow compute authenticity or dependability\nof documents but there is no simple way to do that. What search\nengines such as Google do is to instead use the popularity of the\ndocuments as a measure of goodness. If many people are look-\ning at a document perhaps there is something important or useful\nabout it. But how do we ﬁnd out who is looking at which docu-\nment and for what purpose? One thing that is clearly computable\nfrom the web of documents on the Internet is the hyperlink struc-\nture that links up various documents. We could look at the links\n3.3. TOWARDS INTELLIGENT IR\n341\ngoing out from a given document but perhaps the links coming\ninto a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nsearch where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the\nviolation of this assumption is when there are duplicates in the\ncollection. If an ambiguous word such as capital is included in the\nquery, an optimal system may be expected to retrieve and present\nthe documents so that the user sees this ambiguity but the PRP\nprinciple would give documents that are maximally relevant for\neither of the two senses of the ambiguous word. Further, relevance\nis diﬃcult to quantify and measure accurately. At best we make\ngood estimates. It may be worth looking at the variance of these\nestimates and prefer those decisions with lower variance.\n3.2.5\nPerformance Evaluation\nAs we have already seen, Precision is the percentage of relevant\ndocuments in the returned set and Recall is the percentage of\nall relevant documents in the collection that is in the returned\nset. However, most IR systems produce a ranked list of returned\n3.2. BASIC IR MODELS\n337\ndocuments and a case where the last three of the ten documents\nreturned are relevant cannot be equated with the case where the\nﬁrst three out of ten are relevant. Most users scan the returned\ndocuments from top to bottom and would like to see many relevant\ndocuments right at the top. Thus by measuring the precision at\nseveral initial segments of the ranked list, one may obtain a good\nimpression of how well the system ranks relevant documents. We\nmay therefore measure the Precision at speciﬁed cutoﬀlevels such\nas 5, 10, 20 or 100 from the top. By considering all the documents\nabove a relevant document and computing Precision and then by\naveraging all such Precision values for each of the relevant docu-\nment retrieved, one gets an uninterpolated average Precision. This\naverage Precision will be 1 if all the relevant documents are at the\ntheir estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.\nIn the bag-of-words approach, features corre-\nspond to the terms - each term (word or phrase) is a potential\nfeature.\nFeatures are given numerical values.\nIn the simplest\ncase, the feature values are Boolean - that is, a value is 1 if the\ncorresponding term occurs in the document or the query as the\ncase may be, and 0, otherwise. Alternatively, the numerical value\nof a feature can be simply the number of times it occurs in the\ndocument. Each vector is thus simply a list of numbers.\nA vector of n such features can be geometrically viewed as\na point in n-dimensional space. The geometric spatial proximity\nbetween two vectors is used as a metaphor for the semantic prox-\n1.4. INFORMATION RETRIEVAL\n31\nimity between the corresponding documents and/or query strings.\nThis model is thus conceptually simple and appealing. The most\nrelevant documents are the ones that include the most terms from\na given query and thus spatially closest to the query vector. The\nexample below illustrates these ideas in just two dimensions. If\nthere were three terms, we need to consider vectors in three dimen-\nsional space and, by extension of this idea, if there are n terms, the\nvectors will be in n-dimensional space - diﬃcult to get a mental\npicture but mathematically a simple extension all the same.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 1.9 Vector-Space Model\nThe direction of the vectors is a good indicator of the semantic\ncontent of the corresponding documents, not the length of the\nvectors. Hence it is usual to normalize the lengths of all vectors\nto unity and consider only the directions.\n1.4.4\nPerformance Evaluation\nPerformance of IR systems is measured in terms of Recall and",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-4-subsection-6",
                            "title": "Challenges in Information Retrieval",
                            "content": "of documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nIn this chapter we will look at some the recent advances in\nthe ﬁeld of Information Retrieval. We begin with a brief historical\nsketch and then give the basic IR model. Then we look at various\ndirections of development of the IR ﬁeld. The idea of Intelligent\nInformation Retrieval will be introduced and the need for deeper\nlinguistic analysis will be highlighted.\n3.1\nA Brief History of Information Re-\ntrieval\nInitial explorations of text retrieval systems from small collections\nof scientiﬁc abstracts, legal and business documents etc. began\nin the nineteen sixties and seventies. Foundations of Boolean and\nvector-space models were developed. During these early days, doc-\numents were studied and brief descriptors or lists of index terms\nwere manually prepared for each document.\n327\n328\nCHAPTER 3. ADVANCES IN IR\nDuring the eighties, large scale document databases started\nappearing.\nLexis-Nexis, Dialog and MEDLINE are noteworthy\nexamples of such databases. The need for eﬃcient retrieval from\nlarge collections was increasingly felt. This gave a big push to IR\nresearch and development.\nDuring the nineties, focus shifted to searching FTP-able doc-\numents on Internet (example: Archie, WAIS) and searching the\nWorld Wide Web (example: Lycos, Yahoo, Altavista). Organized\ncompetitions such as NIST TREC were held. Recommender sys-\ntems (example: Ringo, Amazon, NetPerceptions) appeared. Au-\ntomated text categorization and clustering systems were devel-\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nto go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant\nto the query. As we have already seen, the most common approach\nis to view documents and queries are bags of words and represent\nthem as feature vectors where each feature corresponds to one\nword. Similarity between feature vectors is quantiﬁed in terms\nof the orientations of these vectors. Performance is measured in\nterms of Precision and Recall.\nIntelligent IR, on the other hand, requires that we consider the\nsyntax as well as semantics of documents and queries, we adapt\nto users based on direct or indirect feedback and learning, and we\ntake care of authority and dependability of documents. An ideal\ninformation retrieval system is one that can perform like a human\nassistant. This is really the software grand challenge. Obviously,\nwe are far from such an ideal.\nLet us now see what kinds of\nimprovements and enhancements can be or have been made in\nthe ﬁeld of information retrieval.\nMere presence or absence of keywords is clearly too naive a\nview of a document. Can we say I have a bad head ache and Now\nI am free from head ache mean the same thing and both match\nthe query head ache equally well? Is India beat Australia same as\nAustralia beat India? Can we equate I like Govinda’s movies and\nI like Govinda’s movies as much as I would like a burning stove if\nI were sitting on it? Most current IR systems continue to use the\nbag-of-words representation while examples like this clearly show\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\nassumptions in practice. For example, we may use the simplest\nnotion of relevance that the query string appears verbatim in the\ndocument. A slightly less strict notion could be that the words in\nthe query appear frequently in the document, in any order (bag\nof words).\nInstead of looking for improving the performance of retrieval\nfor given queries, we may invert the problem and see how the\nquery itself may be improved so that performance is maximized.\nThis makes sense because it is not always very easy for a user\nto specify exactly what he wants as a query. An ideal query is\none that expresses the user’s requirements precisely in relation to\nthe documents in the collection.\nBut ﬁnding an ideal query is\ndiﬃcult unless we already know exactly what the documents in\n340\nCHAPTER 3. ADVANCES IN IR\nthe collection contain. We may therefore start with a good guess\nand hope to improve after we see some results. We make the as-\nsumption that documents which are relevant to a given query are\nsimilar. Therefore a query can be improved by making it closer\nto relevant documents retrieved and farther from the irrelevant\ndocuments retrieved. Thus by looking at the returned results and\njudging them as relevant or otherwise, we may obtain an improved\nquery. More terms from the relevant documents retrieved can be\nadded and weights for the terms can also be adjusted based on\nthe frequency of occurrence of those terms in the relevant and ir-\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\nprovides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers\nand industrial partners. It is based on the Resource Description\nFramework (RDF), which integrates a variety of applications us-\ning XML for syntax and URIs for naming. See Scientiﬁc American\nMay 2001 issue for an interesting article by Tim Berners-Lee.\n3.3.7\nInformation Retrieval is Diﬃcult\nIR is an inherently diﬃcult task. Can you search millions of doc-\numents and accurately suggest the most relevant documents for a\ngiven query in a fraction of a second? In IR we are asking com-\nputers to do what we human beings cannot do. And we want IR\nto be fully automatic - there is no scope for human intervention.\nWhat makes IR diﬃcult? There are three major issues:\n• Understanding user needs: Understanding exactly what the\nuser is looking for is not easy. Key words do not tell us\nwhat is the purpose of the current search, what all the user\nalready knows, what all he has already searched or what\nlevel of abstraction would suit his level of knowledge and\nexpertise. Social and cultural contexts are important. It\nlooks strange that we set forth on a grand searching and\nretrieval operation without understanding exactly what we\nare looking for!\n• Understanding the Documents: Unless you know exactly\nwhat the documents contain, what they pertain to and what\n3.3. TOWARDS INTELLIGENT IR\n351\nall things they include, for whom it is written, what back-\nground is assumed etc., how can we say which documents\nwill suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-5",
                    "title": "Information Extraction",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-5-subsection-1",
                            "title": "What is Information Extraction?",
                            "content": "den structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nnatural language text, and extraction of the relevant arguments of\nthose events and relationships. IE systems produce a structured\nrepresentation such pieces of information extracted from the given\ntexts. These structured databases can in turn be used for anal-\nysis using conventional database query methods and data-mining\ntechniques, for generating natural language summaries, question-\nanswering, intelligent information retrieval, text indexing and so\non.\n1.5.2\nInformation Extraction Tasks\nIn the mid 1980’s a number of research sites in the United States\nwere working on IE from naval messages, in projects sponsored\nby the Defense Advanced Research Projects Agency (DARPA). In\norder to understand and compare the behavior of such systems,\na number of these message understanding (MU) projects decided\nto work on a set of common messages and then convene to see\nhow their systems perform for some new, unseen messages. This\ngathering constituted an ongoing series of extremely productive\nmessage understanding conferences (MUCs), which have served\nas key events in driving the ﬁeld of IE forward. Because of the\ncomplexity in information extraction task, MUCs divided IE into\ndiﬀerent tasks and then evaluated the performance of IE tasks\nseparately. The IE tasks deﬁned in MUC-7 are:\n1.\nNamed Entity Task: The “NE task” involves ﬁnding and\ncategorizing certain classes of proper names that appear in\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\nCoreference Task: The “Co task” involves ﬁnding and\nlinking together all references of the same object, set or\nactivity. This task is important, because it helps in proper\nacquisition of attributes and relations between the IE task\nentities.\n3.\nTemplate Element Task: The “TE task” builds on NE\nand Co tasks. The goal of this task is to ﬁnd entities and\nidentify certain features of entities.\n4.\nTemplate Relation Task: The Template Relation (TR)\ntask marks relationships between template elements.\n5.\nScenario Template Task:\nScenario templates are the\nprototypical outputs of IE systems. They tie together TE\nentities into event and relation descriptions. This task re-\nquires identifying instances of a task-speciﬁc event, identify-\ning event attributes, and construction of an object-oriented\nstructure recording the entities and the relationships.\nOne of the advantages of this task orientation is that inputs\nand outputs of an information extraction system can be deﬁned\nprecisely, which facilitates the evaluation of diﬀerent systems and\napproaches. Two important metrics for assessing the performance\nof an IE system are Recall and Precision. Recall measures the\namount of relevant information that the NLP system correctly\nextracts from the test collection. Precision refers to the reliability\nof the information so extracted.\nRecall = correct slot fillers in output templates\nslot fillers in answer keys\nPrecision = correct slot fillers in output templates\nslot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\nbe performed on summaries of documents rather than the original\ndocuments, hopefully giving faster and better results. Informa-\ntion Extraction can be performed on summaries rather than the\noriginal documents. Text categorization can be done on the sum-\nmaries. Summaries can be used to ﬁlter out unwanted mails and to\n40\nCHAPTER 1. THE INFORMATION AGE\nroute the rest of the mails automatically to relevant departments.\nDocuments retrieved from the web may be summarized and the\nsummaries translated into other languages. Hopefully, translating\nthe summaries would be easier and quality of translations would\nbe better. A doctor may want to see case histories related to a\nparticular case retrieved, summarized, the treatments compared\nand a report generated, all automatically.\nImagine instructing\nyour home computer to watch the TV news while you are away\nand give you a summary of what all is happening when you come\nback home. Imagine instructing your computer to search the web\nfor relevant documents in various languages and produce a sum-\nmary of what the Russians and the Japanese are saying about a\nparticular move by the United States Government. Think of a\nsystem that can read aloud a summary of a given text for a blind\nperson.\nSummarization can improve the performance of other appli-\ncations. Summarization involves size reduction and this in turn\ncan give speed beneﬁts to other applications. Hopefully, the data\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-5-subsection-2",
                            "title": "Information Extraction Tasks",
                            "content": "den structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nnatural language text, and extraction of the relevant arguments of\nthose events and relationships. IE systems produce a structured\nrepresentation such pieces of information extracted from the given\ntexts. These structured databases can in turn be used for anal-\nysis using conventional database query methods and data-mining\ntechniques, for generating natural language summaries, question-\nanswering, intelligent information retrieval, text indexing and so\non.\n1.5.2\nInformation Extraction Tasks\nIn the mid 1980’s a number of research sites in the United States\nwere working on IE from naval messages, in projects sponsored\nby the Defense Advanced Research Projects Agency (DARPA). In\norder to understand and compare the behavior of such systems,\na number of these message understanding (MU) projects decided\nto work on a set of common messages and then convene to see\nhow their systems perform for some new, unseen messages. This\ngathering constituted an ongoing series of extremely productive\nmessage understanding conferences (MUCs), which have served\nas key events in driving the ﬁeld of IE forward. Because of the\ncomplexity in information extraction task, MUCs divided IE into\ndiﬀerent tasks and then evaluated the performance of IE tasks\nseparately. The IE tasks deﬁned in MUC-7 are:\n1.\nNamed Entity Task: The “NE task” involves ﬁnding and\ncategorizing certain classes of proper names that appear in\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\nCoreference Task: The “Co task” involves ﬁnding and\nlinking together all references of the same object, set or\nactivity. This task is important, because it helps in proper\nacquisition of attributes and relations between the IE task\nentities.\n3.\nTemplate Element Task: The “TE task” builds on NE\nand Co tasks. The goal of this task is to ﬁnd entities and\nidentify certain features of entities.\n4.\nTemplate Relation Task: The Template Relation (TR)\ntask marks relationships between template elements.\n5.\nScenario Template Task:\nScenario templates are the\nprototypical outputs of IE systems. They tie together TE\nentities into event and relation descriptions. This task re-\nquires identifying instances of a task-speciﬁc event, identify-\ning event attributes, and construction of an object-oriented\nstructure recording the entities and the relationships.\nOne of the advantages of this task orientation is that inputs\nand outputs of an information extraction system can be deﬁned\nprecisely, which facilitates the evaluation of diﬀerent systems and\napproaches. Two important metrics for assessing the performance\nof an IE system are Recall and Precision. Recall measures the\namount of relevant information that the NLP system correctly\nextracts from the test collection. Precision refers to the reliability\nof the information so extracted.\nRecall = correct slot fillers in output templates\nslot fillers in answer keys\nPrecision = correct slot fillers in output templates\nslot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nretrieval. There is an increasing cross-fertilization and integration\nof related technologies including speech.\nCurrently indexing is performed automatically on full texts.\nManual processing is slow, costly and may be inconsistent. On the\nother hand, while automatic processing can be very fast, it lacks\nthe commonsense and human judgement of manual methods and\ncan therefore be somewhat inferior in quality. The challenge today\nis to keep the superior speed factor and yet achieve near-human\nperformance through automatic methods.\nIt is clear that IR today is closely related to many other dis-\nciplines.\nIt interfaces with Database Management systems, Li-\nbrary and Information sciences, Artiﬁcial Intelligence, Natural\nLanguage Processing and Machine Learning. Database Manage-\nment systems focus on structured data stored in tables and eﬃ-\ncient processing of precisely deﬁned queries expressed in a formal\nlanguage such as SQL. The syntax and semantics of the data as\nwell as the query are clear. Recent trends towards semi-structured\ndata such as XML brings it closer to IR. Library and Information\nScience has focused on the human user aspects such as human-\ncomputer interaction, user interfaces and visualization. Eﬀective\ncategorization of human knowledge is a primary goal. Citation\nanalysis and bibliometrics are focus areas. Recent work in digital\nlibraries is bringing library science closer to computer science and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-5-subsection-3",
                            "title": "Architecture of an IE System",
                            "content": "slot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE\nwhere P is precision, R is recall, and β is a parameter en-\ncoding the relative importance of recall and precision. When we\ngive equal weightage to precision and recall, F-measure will be\n2PR/(P + R).\n1.5.3\nArchitecture of an IE System\nHobbs proposed a generic architecture for an IE system.\nThe\nHobbs system consists of the following ten modules:\n• Text Zoner - turns a text into a set of segments.\n• Preprocessor - turns a text or text segment into a se-\nquence of sentences, each of which is a sequence of lexical\nitems.\n• A Filter - turns a sequence of sentences into a smaller set\nof sentences by ﬁltering out irrelevant ones.\n• A Preparser - takes a sequence of lexical items and tries\nto identify reliably determinable small-scale structures.\n• A Parser - takes a set of lexical items (words and phrases)\nand outputs a set of parse-tree fragments, which may or\nmay not be complete.\n• Fragment Combiner - attempts to combine parse-tree or\nlogical-form fragments into a structure of the same type for\nthe whole sentence.\n• A Semantic Interpreter - generates semantic structures\nor logical forms from parse-tree fragments.\n• A Lexical Disambiguator - reduces the ambiguity of the\npredicates in the logical form fragments.\n• A Coreference Resolver - identiﬁes diﬀerent descriptions\nof the same entity in diﬀerent parts of a text.\n• A Template Generator - ﬁlls the IE templates from the\nsemantic structures.\nThe SIFT system developed by BBN, the LOLITA system de-\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nnatural language text, and extraction of the relevant arguments of\nthose events and relationships. IE systems produce a structured\nrepresentation such pieces of information extracted from the given\ntexts. These structured databases can in turn be used for anal-\nysis using conventional database query methods and data-mining\ntechniques, for generating natural language summaries, question-\nanswering, intelligent information retrieval, text indexing and so\non.\n1.5.2\nInformation Extraction Tasks\nIn the mid 1980’s a number of research sites in the United States\nwere working on IE from naval messages, in projects sponsored\nby the Defense Advanced Research Projects Agency (DARPA). In\norder to understand and compare the behavior of such systems,\na number of these message understanding (MU) projects decided\nto work on a set of common messages and then convene to see\nhow their systems perform for some new, unseen messages. This\ngathering constituted an ongoing series of extremely productive\nmessage understanding conferences (MUCs), which have served\nas key events in driving the ﬁeld of IE forward. Because of the\ncomplexity in information extraction task, MUCs divided IE into\ndiﬀerent tasks and then evaluated the performance of IE tasks\nseparately. The IE tasks deﬁned in MUC-7 are:\n1.\nNamed Entity Task: The “NE task” involves ﬁnding and\ncategorizing certain classes of proper names that appear in\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.\nquestions requires a good deal of intelligence.\n• Browsers allow us to interactively navigate through a\nweb of inter-connected documents, physically located\nin diﬀerent computers spread across the globe. Here\nthe role of the computer is limited to locating and\ntaking you to the documents you ask for. You start\nby specifying a URL (Uniform Resource Locator) - an\naddress of a website or a speciﬁc document you are\nlooking for. You decide which links to explore further\nand which documents to read or download and save\nfor future use.\n• Search Engines and Information Retrieval (IR)\nSystems accept a query from the user and attempt to\n1.2. TECHNOLOGY FOR ACCESSING INFO\n5\nretrieve those documents in the collection that seem to\nmatch the user’s needs as expressed in his/her query.\nThe role of the machine is limited to drawing the user’s\nattention to documents that are potentially relevant\nto his/her needs. The challenge is to understand the\nuser’s speciﬁc requirements and locate documents that\nare, hopefully, the most relevant. Search engines have\nbecome an integral part of our everyday use of com-\nputers.\n• There are many search engines but no single search\nengine is good enough in all situations. Meta Search\nEngines combine the best of several Search Engines.\nThey provide a common user interface, format queries\nas required for various search engines, ﬁre the search\nengines serially or in parallel either as a foreground\nor as a background process, collect and collate results,\nstore results in a local database for reuse, personalize\nand adapt to individual user’s needs etc. Search en-\ngines are general purpose solutions while meta search\nengines can reside on your computer and they can be\npersonalized to suit your needs. The challenge is to\nbuild user models as well as to decide which search\nengines to ﬁre for what kind of queries. Collating re-\ntrieved results, removing duplicates etc. also require\nsubstantial amounts of intelligence.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\ntion from anywhere in the world. We are no longer conﬁned\nto one oﬃce or one library. Today we live in the information\nage.\nWe have billions of pages of material on the Internet to-\nday. With the Digital Library initiatives whole libraries are\ngetting converted to electronic form. Trillions of bytes of\nelectronic data are getting generated everyday. But merely\nhaving some data or information somewhere is of no use.\nWhat is really required is an easy way to access relevant,\ntimely, useful, and authentic information in a well presented\nmanner. How do we know which documents are relevant,\nauthentic and dependable?\nHow do we ﬁsh out what we\nare looking for in this vast ocean of web pages? How do we\ncategorize, classify, index and structure these largely unor-\nganized collections of electronic documents on the Internet\nso that they become more easily accessible and hence more\nuseful?\nTechnology enables us to create, store and process large\nvolumes of information at great speed.\nBut the speed at\nwhich we human beings can read and understand documents\nremains the same, irrespective of technological advances.\nWe still take minutes to browse a page, possibly hours to\nread carefully and understand the content and may be days,\nweeks or even months to chew and fully digest the purport of\na serious piece of writing. Thus we are in a situation today\nwhere we have more information than we can handle. Tech-\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-6",
                    "title": "Automatic Summarization",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-6-subsection-1",
                            "title": "Why Summarization?",
                            "content": "that you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We\noften scan news headlines before or instead of reading the full\nnews. Teachers produce outlines of notes and course materials to\nstudents. Minutes of a meeting summarize what all transpired in\nthe meeting. Previews of movies give a quick advance view of the\nshape of things to come in the movie. Synopses of TV serials are\ntelecast to let the users get a feel for the serials. Book reviews\nare summaries as retold by the reviewer. Newspapers and maga-\nzines publish Radio and TV guides for the coming week or month.\nResumes and Obituaries are biographical summaries. Novels are\noften speciﬁcally abridged for, say, children to read. Weather fore-\ncasts and Market report bulletins are summaries too. Sound Bites\nconsolidate on going debate on a particular issue. Chronologies\nand gists of history are a kind of summary.\nSummarization is\nitself not new. Automatic techniques for summarization are.\nAlta-Vista Discovery uses Inxight’s summarizer for ﬁltering\nweb based IR. Orcale’s Context (data mining of text databases),\nMicrosoft Word’s AutoSummarize, British Telecom’s ProSum are\n1.6. AUTOMATIC SUMMARIZATION\n41\nsome of the commercial implementations.\n1.6.2\nApproaches to Automatic Summarization\nSummarization can be viewed as a reductive transformation of\nsource text to summary text through content reduction by selec-\ntion and / or generalization of what is important in the source.\nText Summarization has been deﬁned as the process of distilling\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nimportant content, where content is a matter of both information\nand its expression, and importance is a matter of what is essential\nas well as what is salient.\nAutomatically generated summaries cannot be expected to\nbe as good as human generated summaries. However, automatic\nsummarizers work much faster and thereby enable us to consider\nmore documents in a given amount of time.\n1.6.6\nSummarization in the Context of Indian\nTradition\nIt is interesting to relate current interest in technology for auto-\nmatic summarization with the Indian scene, especially with re-\n46\nCHAPTER 1. THE INFORMATION AGE\ngard to our ancient tradition. Brevity was considered an essential\nquality and every eﬀort was taken to write brieﬂy and precisely.\nMany of the greatest works, which are widely read, discussed and\ndebated for thousands of years now, are very small in size. The\nbhagavadgiita has only 700 verses. baadaraayaNa’s brahmasuu-\ntras, which purports to explain all of the upanishadic thought\nin a coherent way, is just 555 aphorisms. pataMjali’s yoogasuu-\ntras consist of 195 aphorisms. maaMDuukyoopanishat, considered\nto be the essence of all upanishadic thought, is just 12 mantras.\ns’aMkara’s aatma SaTkam is six verses. His saadhana paMcakam\nis just ﬁve verses. People could remember entire texts by heart.\nThere was in fact not much need for writing.\nSummarization\nmakes no sense.\nIn fact the need was the opposite. Some of the works were so\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-2",
                            "title": "Approaches to Automatic Summarization",
                            "content": "the most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nimportant content, where content is a matter of both information\nand its expression, and importance is a matter of what is essential\nas well as what is salient.\nAutomatically generated summaries cannot be expected to\nbe as good as human generated summaries. However, automatic\nsummarizers work much faster and thereby enable us to consider\nmore documents in a given amount of time.\n1.6.6\nSummarization in the Context of Indian\nTradition\nIt is interesting to relate current interest in technology for auto-\nmatic summarization with the Indian scene, especially with re-\n46\nCHAPTER 1. THE INFORMATION AGE\ngard to our ancient tradition. Brevity was considered an essential\nquality and every eﬀort was taken to write brieﬂy and precisely.\nMany of the greatest works, which are widely read, discussed and\ndebated for thousands of years now, are very small in size. The\nbhagavadgiita has only 700 verses. baadaraayaNa’s brahmasuu-\ntras, which purports to explain all of the upanishadic thought\nin a coherent way, is just 555 aphorisms. pataMjali’s yoogasuu-\ntras consist of 195 aphorisms. maaMDuukyoopanishat, considered\nto be the essence of all upanishadic thought, is just 12 mantras.\ns’aMkara’s aatma SaTkam is six verses. His saadhana paMcakam\nis just ﬁve verses. People could remember entire texts by heart.\nThere was in fact not much need for writing.\nSummarization\nmakes no sense.\nIn fact the need was the opposite. Some of the works were so\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-3",
                            "title": "Summarization in Relation to Information Extraction",
                            "content": "that you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\nthe text and its relation to communicative goals are identi-\nﬁed. Such structures include\n(a) Format - Hypertext Markup, Document Outline\n44\nCHAPTER 1. THE INFORMATION AGE\n(b) Discourse Segments and Themes\n(c) Rhetorical Structure - argumentation, proof, narra-\ntive, etc.\nA summary may be an extract or an abstract. In the former\ncase, parts of the original document, say important sentences, are\nselected and presented as a summary. For example, in any coher-\nent writing, it is usually possible to identify one or two sentences\nin each paragraph which contain the essence of the whole para-\ngraph. Replacing the paragraphs with these extracts will result\nin a condensed form of the original text that, hopefully, retains\nthe essential information.\nExtraction is therefore identiﬁcation\nand “lifting” of important parts of the source text. The output\nis therefore linguistically close to the original source and follows\nthe same general order. An abstract, on the other hand, is gener-\nated from salient pieces of information identiﬁed from the original\ntext. Extracts are usually easier to obtain than abstracts, as can\nbe expected. Generation of natural language sentences is itself a\nvery complex task.\n1.6.3\nSummarization in Relation to Information\nExtraction\nText summarization is related to Information Extraction. Text\nsummarization is an open approach to extracting information in\nthat there is no prior assumption or expectation of what kinds\nof information are important or what kinds of things to look for.\nWhat is important for a source text is marked by the text itself\naccording to some general, linguistically-based importance crite-\nria. In contrast, Information Extraction is a closed approach in\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nnatural language text, and extraction of the relevant arguments of\nthose events and relationships. IE systems produce a structured\nrepresentation such pieces of information extracted from the given\ntexts. These structured databases can in turn be used for anal-\nysis using conventional database query methods and data-mining\ntechniques, for generating natural language summaries, question-\nanswering, intelligent information retrieval, text indexing and so\non.\n1.5.2\nInformation Extraction Tasks\nIn the mid 1980’s a number of research sites in the United States\nwere working on IE from naval messages, in projects sponsored\nby the Defense Advanced Research Projects Agency (DARPA). In\norder to understand and compare the behavior of such systems,\na number of these message understanding (MU) projects decided\nto work on a set of common messages and then convene to see\nhow their systems perform for some new, unseen messages. This\ngathering constituted an ongoing series of extremely productive\nmessage understanding conferences (MUCs), which have served\nas key events in driving the ﬁeld of IE forward. Because of the\ncomplexity in information extraction task, MUCs divided IE into\ndiﬀerent tasks and then evaluated the performance of IE tasks\nseparately. The IE tasks deﬁned in MUC-7 are:\n1.\nNamed Entity Task: The “NE task” involves ﬁnding and\ncategorizing certain classes of proper names that appear in\n1.5. INFORMATION EXTRACTION\n37\nthe text. A named entity task consists of three subtasks\n(entity names, temporal expressions, number expressions).\nSome examples are: Names of organizations, persons, loca-\ntions, mentions of dates, times, currency and percentage.\n2.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-4",
                            "title": "Summarization in Relation to Other Technologies",
                            "content": "the most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\nbe performed on summaries of documents rather than the original\ndocuments, hopefully giving faster and better results. Informa-\ntion Extraction can be performed on summaries rather than the\noriginal documents. Text categorization can be done on the sum-\nmaries. Summaries can be used to ﬁlter out unwanted mails and to\n40\nCHAPTER 1. THE INFORMATION AGE\nroute the rest of the mails automatically to relevant departments.\nDocuments retrieved from the web may be summarized and the\nsummaries translated into other languages. Hopefully, translating\nthe summaries would be easier and quality of translations would\nbe better. A doctor may want to see case histories related to a\nparticular case retrieved, summarized, the treatments compared\nand a report generated, all automatically.\nImagine instructing\nyour home computer to watch the TV news while you are away\nand give you a summary of what all is happening when you come\nback home. Imagine instructing your computer to search the web\nfor relevant documents in various languages and produce a sum-\nmary of what the Russians and the Japanese are saying about a\nparticular move by the United States Government. Think of a\nsystem that can read aloud a summary of a given text for a blind\nperson.\nSummarization can improve the performance of other appli-\ncations. Summarization involves size reduction and this in turn\ncan give speed beneﬁts to other applications. Hopefully, the data\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-5",
                            "title": "Evaluation of Summarization Systems",
                            "content": "the most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\noften task speciﬁc and depends on the speciﬁc needs of the users.\nWhat is appropriate in a given situation may not be the best for\nanother situation.\nHuman beings have an incredible capacity to produce excel-\nlent summaries. This is possible because we are intelligent, we\nhave common sense and world knowledge, we have superb reason-\ning power and we understand the meaning and intention of the\ndocument before we try to summarize. Computers are deﬁcient in\nthese respects and hence building automatic summarization sys-\ntems is a challenge.\nThe input to a summarization system may be a single docu-\nment or several. Texts can be in one language or several. Tra-\nditionally, summarization is performed on text documents but\nmulti-media documents containing images, audio, video etc. may\nalso be considered. Output produced may be a stand-alone docu-\nment or it may be linked and presented in the context of the source\nsuch as by highlighting the selected parts. In some cases coher-\nent, connected sentences may be generated as summary while in\nothers it may be fragmentary, say just a list of phrases. Generic\nsummaries can be produced for wide readership or a user’s spe-\nciﬁc need as expressed in a query, area of interest or topic may\nbe used to generate user-speciﬁc summaries. A summary can be\nindicative of the topic of the original document, informative in\nthe sense of summarizing the essential information content of the\ngiven source document, or a critical evaluation of the source. The\nrequired compression ratio (ratio of the size of the summary to\nthe size of the original text) can be speciﬁed by the user. Typical\ncompression ratios range from 1% to 30%.\n42\nCHAPTER 1. THE INFORMATION AGE\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.\nThe overall performance of IE systems remains very low till\ndate.\nIE is an inherently complex task and more detailed and\nthorough analysis of the texts is essential to move further. Re-\ncent research in IE has again shown very clearly the need for\ndeeper syntactic and semantic analysis of texts and resolution of\nanaphoric references.\n1.6\nAutomatic Summarization\n1.6.1\nWhy Summarization?\nThere is an ever increasing amount of electronic texts available\ntoday. Millions of pages of text are available for free reading from\nthe World Wide Web and other on-line Resources. Developments\nin Information Technology have made it possible to produce such\nlarge volumes of information within a short period of time. The\nspeed at which we can read and understand texts is, however,\nconstant as always. How then do we make best use of all these\navailable resources?\nHow then do we take decisions as we are\ncalled upon to take in a reasonable amount of time based on as\nmuch of available information as we can?\nOne solution lies in\ndeveloping technologies for automatically summarizing available\ntexts. Gists and summaries are shorter by deﬁnition and hopefully\neasier to get a hang of too. Tools which can quickly digest large\nquantities of information will enable us to consider a wider and\npotentially richer set of information sources to support decision\nmaking. Hence this relatively new ﬁeld of Automatic Summariza-\ntion.\nText summarization can be used as an application by itself\nor integrated in various interesting combinations with other tech-\nnologies. Summaries help us to get a quick idea about the contents\nof a large book or report. We may use that to decide whether to\nread the report in more detail or not. Information Retrieval can\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nimportant content, where content is a matter of both information\nand its expression, and importance is a matter of what is essential\nas well as what is salient.\nAutomatically generated summaries cannot be expected to\nbe as good as human generated summaries. However, automatic\nsummarizers work much faster and thereby enable us to consider\nmore documents in a given amount of time.\n1.6.6\nSummarization in the Context of Indian\nTradition\nIt is interesting to relate current interest in technology for auto-\nmatic summarization with the Indian scene, especially with re-\n46\nCHAPTER 1. THE INFORMATION AGE\ngard to our ancient tradition. Brevity was considered an essential\nquality and every eﬀort was taken to write brieﬂy and precisely.\nMany of the greatest works, which are widely read, discussed and\ndebated for thousands of years now, are very small in size. The\nbhagavadgiita has only 700 verses. baadaraayaNa’s brahmasuu-\ntras, which purports to explain all of the upanishadic thought\nin a coherent way, is just 555 aphorisms. pataMjali’s yoogasuu-\ntras consist of 195 aphorisms. maaMDuukyoopanishat, considered\nto be the essence of all upanishadic thought, is just 12 mantras.\ns’aMkara’s aatma SaTkam is six verses. His saadhana paMcakam\nis just ﬁve verses. People could remember entire texts by heart.\nThere was in fact not much need for writing.\nSummarization\nmakes no sense.\nIn fact the need was the opposite. Some of the works were so\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-6-subsection-6",
                            "title": "Summarization in the Context of Indian Tradition",
                            "content": "instead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its\nimportant content, where content is a matter of both information\nand its expression, and importance is a matter of what is essential\nas well as what is salient.\nAutomatically generated summaries cannot be expected to\nbe as good as human generated summaries. However, automatic\nsummarizers work much faster and thereby enable us to consider\nmore documents in a given amount of time.\n1.6.6\nSummarization in the Context of Indian\nTradition\nIt is interesting to relate current interest in technology for auto-\nmatic summarization with the Indian scene, especially with re-\n46\nCHAPTER 1. THE INFORMATION AGE\ngard to our ancient tradition. Brevity was considered an essential\nquality and every eﬀort was taken to write brieﬂy and precisely.\nMany of the greatest works, which are widely read, discussed and\ndebated for thousands of years now, are very small in size. The\nbhagavadgiita has only 700 verses. baadaraayaNa’s brahmasuu-\ntras, which purports to explain all of the upanishadic thought\nin a coherent way, is just 555 aphorisms. pataMjali’s yoogasuu-\ntras consist of 195 aphorisms. maaMDuukyoopanishat, considered\nto be the essence of all upanishadic thought, is just 12 mantras.\ns’aMkara’s aatma SaTkam is six verses. His saadhana paMcakam\nis just ﬁve verses. People could remember entire texts by heart.\nThere was in fact not much need for writing.\nSummarization\nmakes no sense.\nIn fact the need was the opposite. Some of the works were so\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and\nwork well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages\nnot in, but on Indian languages and Indian tradition and culture.\nMulti-lingual and multi-media extensions will become increasingly\nimportant.\nBibliography\n1. Akshar Bharati, Vineet Chaitanya and Rajeev Sangal, “Nat-\nural Language Processing: A Paninian Perspective”, Prentice-\nHall of India, 1995\n2. Andrian Akmajian, Richard A Demers and Robert M Har-\nnish, “Linguistics: An Introduction to Language and Com-\nmunication”, The MIT Press, Second Edition, 1984\n3. Baker C L, “English Syntax”, MIT Press, 1989\n4. Ben Gold and Nelson Morgan, “Speech an Audio Signal\nProcessing”,John Wiley and Sons Inc., 2002\n5. Charniak E, “Statistical Language Learning”, MIT Press,\n1993\n6. Christopher D Manning and Hinrich Shutze, “Foundations\nof Statistical Natural Language Processing”, The MIT Press,\n2000\n7. Daniel Jurafsky and James H Martin, “Speech and Lan-\nguage Processing”, Pearson Education, 2002\n8. Douglas O’Shaughnessy, “Speech Communications - Hu-\nman and Machine”, Second Edition, Universities Press, 2001\n9. D C Montgomery, E A Peck and G G Vining, “Introduction\nto Linear Regression Analysis”, John Wiley and Sons, INC.,\n2001\n10. George W Smith, “Computers and Human Language”, Ox-\nford University Press, 1991\n353\n11. Hopcroft J.E. and Ullman J.D., “Introduction to Automata\nTheory, Languages, and Computation”, Addison-Wesley,\n1979\n12. Inderjeet Mani and Mark T Maybury (Eds), “Advances in\nAutomatic Text Summarization”, the MIT Press, 1999\n13. James Allen,”Natural Language Understanding”, Second\nEdition, Pearson Education, 2003\n14. Lawrence Rabiner and Biing-Hwang Juang, “Fundamentals\nof Speech Recognition”, Pearson Education, 2003\n15. Michael G Dyer, “In-Depth Understanding”, MIT Press,1983\n16. Nigel Fabb, “Sentence Structure”, Routledge\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We\noften scan news headlines before or instead of reading the full\nnews. Teachers produce outlines of notes and course materials to\nstudents. Minutes of a meeting summarize what all transpired in\nthe meeting. Previews of movies give a quick advance view of the\nshape of things to come in the movie. Synopses of TV serials are\ntelecast to let the users get a feel for the serials. Book reviews\nare summaries as retold by the reviewer. Newspapers and maga-\nzines publish Radio and TV guides for the coming week or month.\nResumes and Obituaries are biographical summaries. Novels are\noften speciﬁcally abridged for, say, children to read. Weather fore-\ncasts and Market report bulletins are summaries too. Sound Bites\nconsolidate on going debate on a particular issue. Chronologies\nand gists of history are a kind of summary.\nSummarization is\nitself not new. Automatic techniques for summarization are.\nAlta-Vista Discovery uses Inxight’s summarizer for ﬁltering\nweb based IR. Orcale’s Context (data mining of text databases),\nMicrosoft Word’s AutoSummarize, British Telecom’s ProSum are\n1.6. AUTOMATIC SUMMARIZATION\n41\nsome of the commercial implementations.\n1.6.2\nApproaches to Automatic Summarization\nSummarization can be viewed as a reductive transformation of\nsource text to summary text through content reduction by selec-\ntion and / or generalization of what is important in the source.\nText Summarization has been deﬁned as the process of distilling\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the\nimportant information emerge naturally, as appropriate for each\ncase while information extraction is intended to ﬁnd individual\nmanifestations of speciﬁed important notions, regardless of source\nstatus. In information extraction one view of what is important is\nassumed and imposed regardless of what the original intentions of\n1.6. AUTOMATIC SUMMARIZATION\n45\nthe author were or what the readers of the document would nat-\nurally ﬁnd important. Summarization is “text extraction” while\ninformation extraction is “fact extraction”. Summarization has\nthe advantage of generality but delivers relatively low-quality out-\nput because the weak and indirect methods used may not be very\neﬀective in identifying important material and presenting as a co-\nherent, well organized text. The information extraction approach\ncan deliver better quality output in substance and presentation as\nfar as the selected material is concerned, but with the disadvan-\ntages that the required type of information has to be explicitly\nand eﬀortfully speciﬁed and may not be important for the source\ntext itself. Thus the two approaches are complementary and both\nviews can be applied for summarization as well.\n1.6.4\nSummarization in Relation to Other Tech-\nnologies\nText summarization is also related to other areas of language\nengineering. Information retrieval, information extraction, text\ncategorization translation etc. may be performed on summaries\ninstead of original texts.\nOr summaries can be produced after\nretrieving relevant documents or after translation.\n1.6.5\nEvaluation of Summarization Systems\nAutomatic summarization is an inherently hard task. We have\nto characterize a source text as a whole, we have to capture its",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-7",
                    "title": "Automatic Text Categorization",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-7-subsection-1",
                            "title": "Why Text Categorization?",
                            "content": "ﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nlarger collection of training documents. One may have to resort\nto multi-level or hierarchical classiﬁcation.\nWe have seen that practical text categorization systems treat\ndocuments as unordered collection of words, use these words as\nfeatures and count frequency of occurrence to weigh these features.\nLinguists will be shocked to know that today’s text categorization\nsystems treat “India beat Australia” and “Australia beat India”\nas identical - both have the same three words. There is hardly\nany linguistic analysis of the texts concerned. Speed is not the\nonly criteria in all situations. More intelligent text classiﬁcation\nsystems will surely require a deeper linguistic analysis of the doc-\nument texts. Also, the feature dimensions are extremely large and\ndimensionality reduction is an important issue.\nIt is interesting to note that NLP started with big goals like\nnatural language understanding and generation but only toy sys-\ntems could be built then.\nNowadays much larger applications\nthat give reasonably good performance in real life situations are\nbeing built but with only the most superﬁcial and rudimentary\nlinguistic analysis of the language. Future may perhaps lie in an\nintelligent combination of deep linguistic analysis and statistical\nmethods based on large scale training data.\n1.8\nMachine Translation\nMachine Translation (MT), also known as automatic translation\nthrows open great opportunities. While human costs are going\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\n50\nCHAPTER 1. THE INFORMATION AGE\ngory must be identiﬁed. This distinction is more pragmatic\nthan conceptual. Thus if all the documents are not avail-\nable to start with, document pivoted categorization may\nbe more appropriate while category pivoted categorization\nmay be the preferred choice if new categories get added and\nalready classiﬁed documents need to be re-classiﬁed.\n• In Hard categorization, the classiﬁer is required to ﬁrmly\nassign categories to documents (or the other way around)\nwhereas in Ranking Categorization, the system ranks the\nvarious possible assignments and the ﬁnal decision about\nclass assignments is left to the user. This leads us to the\npossibility of semi-automatic or interactive classiﬁers where\nhuman users take the ﬁnal decisions to ensure highest levels\nof accuracy.\n• Constraints may be imposed on the number of categories\nthat may be assigned to each document - exactly k, at least\nk, at most k, and so on. In the single label case, k = 1 and\na single category is to be assigned to each document. If k\nis more than 1, we have the multi-label categorization.\n• The text categorization problem can be reduced to a set of\nbinary classiﬁcation problems one for each category - where\neach document is categorized as either belonging to a given\ncategory or not.\n• If only unlabeled training data is available we may have to\nuse unsupervised learning techniques to perform Text Clus-\ntering instead of classiﬁcation into known classes. Here the\naim is to determine the similarities and diﬀerences among\nthe various documents and ﬁnd out a natural way of group-\ning them so that similar documents are grouped together.\n1.7.3\nText Representation\nClassiﬁcation systems represent documents in terms of sets of fea-\ntures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.\nlize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.\nAutomatic text categorization has many applications. Infor-\nmation Retrieval systems and Search Engines will greatly ben-\neﬁt if the documents in the collection are already categorized.\nIt would then be possible to limit the search to relevant classes\nof documents, thereby enhancing both the speed and the perfor-\nmance.\nText Categorization system can be used for automatic\nﬁltering and routing of documents. For example, junk mails can\nbe detected and removed and mails can be routed to diﬀerent de-\npartments based on their topic. There are tools that collect news\nfrom various newspapers and other sources on the web, perform\nnews aggregation and suitably re-organize the results. Automatic\ntext categorization can be of great value to such systems. Iden-\ntiﬁcation of topic also helps in many other areas such as word\nsense disambiguation by narrowing the space of possibilities and\nbringing things into sharper focus.\n1.7.2\nApproaches to Automatic Text Catego-\nrization\nBefore the 1990s, the predominant approach to text classiﬁcation\nwas the knowledge based approach. Rules and heuristics based on\nexperience were used to place documents in appropriate classes.\nWhen performed on a small scale, as in the case of individual\nlibraries, classiﬁcation experts manually read the title, front mat-\nter, cover page material, table of contents etc. and decided where\nexactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines\n38\n1.6\nAutomatic Summarization . . . . . . . . . . .\n39\nxi\nxii\nCONTENTS\n1.6.1\nWhy Summarization?\n. . . . . . . . .\n39\n1.6.2\nApproaches to Automatic Summariza-\ntion\n. . . . . . . . . . . . . . . . . . .\n41\n1.6.3\nSummarization in Relation to Infor-\nmation Extraction . . . . . . . . . . .\n44\n1.6.4\nSummarization in Relation to Other\nTechnologies\n. . . . . . . . . . . . . .\n45\n1.6.5\nEvaluation of Summarization Systems\n45\n1.6.6\nSummarization in the Context of In-\ndian Tradition\n. . . . . . . . . . . . .\n45\n1.7\nAutomatic Text Categorization . . . . . . . .\n47\n1.7.1\nWhy Text Categorization? . . . . . . .\n47\n1.7.2\nApproaches to Automatic Text Cate-\ngorization . . . . . . . . . . . . . . . .\n47\n1.7.3\nText Representation . . . . . . . . . .\n50\n1.7.4\nFeature Weighting . . . . . . . . . . .\n51\n1.7.5\nText Classiﬁcation and Clustering\n. .\n53\n1.8\nMachine Translation . . . . . . . . . . . . . .\n54\n1.8.1\nMachine Translation is Hard\n. . . . .\n55\n1.8.2\nDeploying Machine Translation . . . .\n60\n1.8.3\nApproaches to Machine Translation\n.\n62\n1.8.4\nChallenges in Machine Translation . .\n64\n1.8.5\nMachine Translation in India . . . . .\n64\n1.9\nSpeech Technologies\n. . . . . . . . . . . . . .\n71\n1.9.1\nAutomatic Speech Recognition . . . .\n72\n1.9.2\nSpeech Synthesis . . . . . . . . . . . .\n74\n1.9.3\nOther Speech Technologies\n. . . . . .\n76\n1.10 Human and Machine Intelligence . . . . . . .\n77\n1.11 Shape of Things to Come\n. . . . . . . . . . .\n81\n2\nFoundations of NLP\n85\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . .\n85\n2.1.1\nLanguage, Communication, Technology 85\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\n. . . . . . . . .\n87\nCONTENTS\nxiii\n2.1.3\nNLP: An AI Perspective . . . . . . . .\n88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3\nassumed to be uniform and hence bracketed out. This assumption\nof uniform priors is questionable and has led to criticism of the\nBayesian approaches.\nBayesian method requires the estimation of joint probabilities\nof all the features for each category.\nIn order to simplify this,\nindependence is often assumed. That is, the conditional probabil-\nity of a feature given a category is assumed to be independent of\nthe conditional probabilities of other features given that category.\nA Bayesian classiﬁer that makes this independence assumption is\ntermed a Naive Bayes Classiﬁer. The Independence assumption\nis rarely valid in real world. Yet the method works quite well and\nis used in practice.\nProbabilities are numbers less than one and multiplying prob-\nabilities makes the numbers smaller and smaller. Form an imple-\nmentation point of view, therefore, it is useful to take logarithms.\nLogarithms also convert multiplications to additions.\nIn order to understand how Bayesian learning actually works,\nlet us apply this to the Text Categorization problem. The basic\nidea is to use the joint probabilities of document terms and cate-\ngories to estimate the probabilities of categories given a document.\nTo categorize a test document dj as belonging to a category Ci,\nthe maximum likelihood is ﬁrst estimated over all categories:\nP(dj|Ci) =\nX\nw∈dj\nlog(P(w|Ci))\n(2.9)\nHere w is a word or a term occurring in a given document.\nThe basic idea is that words occur with varying frequencies in\ndocuments of diﬀerent categories. No single word may be good\nenough to make a decision but the combined eﬀect of all the words\nin the document collection may be good enough to give correct\nclassiﬁcation.\nThe prior probabilities of each category Prior(Ci) are evalu-\nated as the ratios of the number of documents in category Ci to\nthe number of documents in the total collection.\n2.3. STATISTICAL APPROACHES\n253\nFinally, the posterior probabilities of each category are calcu-",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-2",
                            "title": "Approaches to Automatic Text Categorization",
                            "content": "lize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.\nAutomatic text categorization has many applications. Infor-\nmation Retrieval systems and Search Engines will greatly ben-\neﬁt if the documents in the collection are already categorized.\nIt would then be possible to limit the search to relevant classes\nof documents, thereby enhancing both the speed and the perfor-\nmance.\nText Categorization system can be used for automatic\nﬁltering and routing of documents. For example, junk mails can\nbe detected and removed and mails can be routed to diﬀerent de-\npartments based on their topic. There are tools that collect news\nfrom various newspapers and other sources on the web, perform\nnews aggregation and suitably re-organize the results. Automatic\ntext categorization can be of great value to such systems. Iden-\ntiﬁcation of topic also helps in many other areas such as word\nsense disambiguation by narrowing the space of possibilities and\nbringing things into sharper focus.\n1.7.2\nApproaches to Automatic Text Catego-\nrization\nBefore the 1990s, the predominant approach to text classiﬁcation\nwas the knowledge based approach. Rules and heuristics based on\nexperience were used to place documents in appropriate classes.\nWhen performed on a small scale, as in the case of individual\nlibraries, classiﬁcation experts manually read the title, front mat-\nter, cover page material, table of contents etc. and decided where\nexactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines\ngiven the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-\numents, typically based on the subject matter or topic, without\nany manual eﬀort. A text categorization program can automati-\ncally categorize thousands of documents in a few minutes. There\nis no way manual classiﬁcation can match that speed. In terms of\naccuracy of classiﬁcation, automatic systems can achieve accura-\ncies of 95% or even higher, depending upon the speciﬁcities of the\ntask. Manual classiﬁcation can in principle be fully correct but in\npractice one must make allowance for some errors.\nAutomated text categorization can be deﬁned as assigning\npre-deﬁned category labels to new documents based on the likeli-\nhood suggested by a training set of labeled documents. Given a\n1.7. AUTOMATIC TEXT CATEGORIZATION\n49\nset of documents with the associated category labels, the system\nlooks for discriminating features that help to set the various cate-\ngories apart. This process is called training. The system learns, so\nto say, how exactly documents within a class are similar and doc-\numents in diﬀerent classes are diﬀerent from each another. Once\nthe system has been trained, it can look at new documents unseen\nbefore and classify them into one of the set categories.\nThe similarities and diﬀerences between documents of various\nclasses are expressed in terms of features. In the simplest case,\nfeatures are the words in these documents. The assumption is that\nthere are words that occur frequently in certain classes but not\nin the others. Words like election, mandate, constituency, party,\nlegislature, parliament are more likely to occur in the political\narena than in sports. Of course there can be politics in sports,\nthere can be elections and parties in sports domain too. That is\nvectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives\na brief description of some of these techniques.\nIn particular,\nthere we will show how the Bayesian Learning approach can be\napplied to the task of automatic text categorization. Models are\nbuilt from labelled training data and then applied to classify new\nunseen documents. When there is no labeled training data set\navailable, it is also possible to automatically cluster or group to-\ngether similar documents. A suitable measure of similarity must\nbe deﬁned. Often the term distance is used as a measure of dis-\nsimilarity. Clustering techniques work by attempting to reduce\nthe intra-cluster distances while maximizing the inter-cluster dis-\ntances. See section 2.3.3 for more on this.\nA number of automatic text categorization systems have been\ndeveloped and put to use for English and other major languages of\nthe world. Work on Indian languages has started only recently. A\ncategorization system developed recently by the author could clas-\nsify News Articles in Telugu into broad categories such as Sports,\nPolitics, Economics and Business and Cinema with nearly 95% ac-\ncuracy. The system was trained on a preclassiﬁed set of about 600\n54\nCHAPTER 1. THE INFORMATION AGE\ndocuments and tested on about 200 previously unseen documents.\nDeveloping automatic text categorization systems requires a\nlarge amounts of pre-classiﬁed training data.\nAlso, the perfor-\nmance of the system may deteriorate if the classes considered are\nﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nlarger collection of training documents. One may have to resort\nto multi-level or hierarchical classiﬁcation.\nWe have seen that practical text categorization systems treat\ndocuments as unordered collection of words, use these words as\nfeatures and count frequency of occurrence to weigh these features.\nLinguists will be shocked to know that today’s text categorization\nsystems treat “India beat Australia” and “Australia beat India”\nas identical - both have the same three words. There is hardly\nany linguistic analysis of the texts concerned. Speed is not the\nonly criteria in all situations. More intelligent text classiﬁcation\nsystems will surely require a deeper linguistic analysis of the doc-\nument texts. Also, the feature dimensions are extremely large and\ndimensionality reduction is an important issue.\nIt is interesting to note that NLP started with big goals like\nnatural language understanding and generation but only toy sys-\ntems could be built then.\nNowadays much larger applications\nthat give reasonably good performance in real life situations are\nbeing built but with only the most superﬁcial and rudimentary\nlinguistic analysis of the language. Future may perhaps lie in an\nintelligent combination of deep linguistic analysis and statistical\nmethods based on large scale training data.\n1.8\nMachine Translation\nMachine Translation (MT), also known as automatic translation\nthrows open great opportunities. While human costs are going\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\n50\nCHAPTER 1. THE INFORMATION AGE\ngory must be identiﬁed. This distinction is more pragmatic\nthan conceptual. Thus if all the documents are not avail-\nable to start with, document pivoted categorization may\nbe more appropriate while category pivoted categorization\nmay be the preferred choice if new categories get added and\nalready classiﬁed documents need to be re-classiﬁed.\n• In Hard categorization, the classiﬁer is required to ﬁrmly\nassign categories to documents (or the other way around)\nwhereas in Ranking Categorization, the system ranks the\nvarious possible assignments and the ﬁnal decision about\nclass assignments is left to the user. This leads us to the\npossibility of semi-automatic or interactive classiﬁers where\nhuman users take the ﬁnal decisions to ensure highest levels\nof accuracy.\n• Constraints may be imposed on the number of categories\nthat may be assigned to each document - exactly k, at least\nk, at most k, and so on. In the single label case, k = 1 and\na single category is to be assigned to each document. If k\nis more than 1, we have the multi-label categorization.\n• The text categorization problem can be reduced to a set of\nbinary classiﬁcation problems one for each category - where\neach document is categorized as either belonging to a given\ncategory or not.\n• If only unlabeled training data is available we may have to\nuse unsupervised learning techniques to perform Text Clus-\ntering instead of classiﬁcation into known classes. Here the\naim is to determine the similarities and diﬀerences among\nthe various documents and ﬁnd out a natural way of group-\ning them so that similar documents are grouped together.\n1.7.3\nText Representation\nClassiﬁcation systems represent documents in terms of sets of fea-\ntures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-3",
                            "title": "Text Representation",
                            "content": "38\n1.6\nAutomatic Summarization . . . . . . . . . . .\n39\nxi\nxii\nCONTENTS\n1.6.1\nWhy Summarization?\n. . . . . . . . .\n39\n1.6.2\nApproaches to Automatic Summariza-\ntion\n. . . . . . . . . . . . . . . . . . .\n41\n1.6.3\nSummarization in Relation to Infor-\nmation Extraction . . . . . . . . . . .\n44\n1.6.4\nSummarization in Relation to Other\nTechnologies\n. . . . . . . . . . . . . .\n45\n1.6.5\nEvaluation of Summarization Systems\n45\n1.6.6\nSummarization in the Context of In-\ndian Tradition\n. . . . . . . . . . . . .\n45\n1.7\nAutomatic Text Categorization . . . . . . . .\n47\n1.7.1\nWhy Text Categorization? . . . . . . .\n47\n1.7.2\nApproaches to Automatic Text Cate-\ngorization . . . . . . . . . . . . . . . .\n47\n1.7.3\nText Representation . . . . . . . . . .\n50\n1.7.4\nFeature Weighting . . . . . . . . . . .\n51\n1.7.5\nText Classiﬁcation and Clustering\n. .\n53\n1.8\nMachine Translation . . . . . . . . . . . . . .\n54\n1.8.1\nMachine Translation is Hard\n. . . . .\n55\n1.8.2\nDeploying Machine Translation . . . .\n60\n1.8.3\nApproaches to Machine Translation\n.\n62\n1.8.4\nChallenges in Machine Translation . .\n64\n1.8.5\nMachine Translation in India . . . . .\n64\n1.9\nSpeech Technologies\n. . . . . . . . . . . . . .\n71\n1.9.1\nAutomatic Speech Recognition . . . .\n72\n1.9.2\nSpeech Synthesis . . . . . . . . . . . .\n74\n1.9.3\nOther Speech Technologies\n. . . . . .\n76\n1.10 Human and Machine Intelligence . . . . . . .\n77\n1.11 Shape of Things to Come\n. . . . . . . . . . .\n81\n2\nFoundations of NLP\n85\n2.1\nIntroduction . . . . . . . . . . . . . . . . . . .\n85\n2.1.1\nLanguage, Communication, Technology 85\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\n. . . . . . . . .\n87\nCONTENTS\nxiii\n2.1.3\nNLP: An AI Perspective . . . . . . . .\n88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy\nUniversity of Hyderabad\nPublished By\nEss Ess Publications\nFor\nSarada Ranganathan Endowment for Library\nScience\nBangalore, INDIA\n2006\ni\nc\n⃝Kavi Narayana Murthy and Sarada Ranganathan\nEndowment for Library Science (2005)\nAll rights reserved. No part of this publication may be\nreproduced, stored in a retrieval system or transmitted, in\nany form or by any means, electronic, mechanical, photo-\ncopying, recording or otherwise without the prior written\npermission of the publisher.\nThis book has been printed from the camera-ready\ncopy prepared by the author using LATEX\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy, University of Hyderabad\nSarada Ranganathan Endowment Lecture, 24(2004)\nFirst Published 2006\nISBN 81-7000-485-3\nPrice: Rs. 850/-\nPublished by\nEss Ess Publications\n4837/24, Ansari Road, Darya Ganj, New Delhi-110 002\nTel: 001-23260807 Fax: 001-23274173\nE-mail: essess@del3.vsnl.net.in url:\nhttp//www.essessreference.com\nFor\nSarada Ranganathan Endowment for Library\nScience\n702, ‘Upstairs’, 42nd Cross, III Block, Rajajinagar,\nBangalore 560 010\nE-mail: srels@vsnl.com Tel: 080-23305109\nPrinted in India at: Printline, New Delhi 111 002\nii\nPreface\nThe contributions of Dr. S R Ranganathan to the ﬁeld of\nlibrary and information sciences is well known. Sarada Ran-\nganathan Endowment for Library Science (SRELS), founded\nby Dr Ranganathan in 1961 has been carrying out com-\nmendable work in promoting library and information sci-\nences. SRELS has been working towards improvement of\nlibrary and information services in India, training personnel\nin library and information sciences and applying research\nresults in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on\ntext documents. While there is an increasing trend towards pro-\ncessing of multi-media documents, text processing continues to be\n2.1. INTRODUCTION\n87\nthe major focus. We shall limit ourselves mainly to processing of\ntext documents in this book.\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\nThis chapter deals with the fundamentals of language processing\nthat are essential for realizing Intelligent Information Retrieval\nas well as other applications of language and speech technologies.\nNatural Language Processing (NLP) forms the backbone of every\nhuman language technology application. NLP is concerned with\nnatural or human languages - languages which we human beings\nuse for day to day communications, as against artiﬁcial languages\nsuch as computer and robot programming languages.\nThe terms Natural Language Processing and Computational\nLinguistics have been used interchangeably and we shall do the\nsame here. There is no real diﬀerence between the two except per-\nhaps that linguists sometimes tend to prefer the latter term - in\nComputational Linguistics, Linguistics is the head of the phrase\nand Computational is only an adjectival modiﬁer. Computers, like\nmathematics, are generic and powerful tools and mere use of com-\nputers to carry out linguistic studies will not qualify as a distinct\nbranch of study in itself. Computational Linguistics or NLP is\ndiﬀerent from Linguistics in that the primary aim is to build com-\nputational models of various aspects of human language faculty.\nSuch models need to be simple, elegant and eﬃcient, yet very pre-\ncise, detailed and exhaustive. The models built must stand the\ntest of large scale real life data - language as people use it. NLP\nthe text and its relation to communicative goals are identi-\nﬁed. Such structures include\n(a) Format - Hypertext Markup, Document Outline\n44\nCHAPTER 1. THE INFORMATION AGE\n(b) Discourse Segments and Themes\n(c) Rhetorical Structure - argumentation, proof, narra-\ntive, etc.\nA summary may be an extract or an abstract. In the former\ncase, parts of the original document, say important sentences, are\nselected and presented as a summary. For example, in any coher-\nent writing, it is usually possible to identify one or two sentences\nin each paragraph which contain the essence of the whole para-\ngraph. Replacing the paragraphs with these extracts will result\nin a condensed form of the original text that, hopefully, retains\nthe essential information.\nExtraction is therefore identiﬁcation\nand “lifting” of important parts of the source text. The output\nis therefore linguistically close to the original source and follows\nthe same general order. An abstract, on the other hand, is gener-\nated from salient pieces of information identiﬁed from the original\ntext. Extracts are usually easier to obtain than abstracts, as can\nbe expected. Generation of natural language sentences is itself a\nvery complex task.\n1.6.3\nSummarization in Relation to Information\nExtraction\nText summarization is related to Information Extraction. Text\nsummarization is an open approach to extracting information in\nthat there is no prior assumption or expectation of what kinds\nof information are important or what kinds of things to look for.\nWhat is important for a source text is marked by the text itself\naccording to some general, linguistically-based importance crite-\nria. In contrast, Information Extraction is a closed approach in\nthat you already know what is important and you look for just\nthose pieces of information in the source text. Thus the source\ntext is viewed as a speciﬁc instantiation of some previously estab-\nlished generic content. Thus summarization is intended to let the",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-4",
                            "title": "Feature Weighting",
                            "content": "with their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting\nNumerical weights need to be computed for the index terms before\nmachine learning techniques can be applied. Here are some of the\nbasic ideas for term weighting:\n• Term Attributes:\nAttributes of the terms such as their\nsyntactic categories can be used to weight the terms.\n• Text attributes:\nThe number of terms in a text, the length\nof the text etc. can be used.\n52\nCHAPTER 1. THE INFORMATION AGE\n• Relation between the term and the text:\nRelative frequency\nof the term in the text, location of the term in the text,\nrelationship with other terms in the text etc.\n• Relation to corpus:\nRelation between the term and the\ndocument corpus or some other reference corpus can also\nbe used.\n• Expert Knowledge:\nExpert knowledge is a potential source\nbut is rarely used.\nThe most common approach is to consider the frequency of\noccurrence of terms in a given document in relation to their fre-\nquencies of occurrence in other documents in the collection. This\nscheme is known as the tf-idf scheme. Here is how tf-idf weights\ncan be computed for given terms:\n• Term Frequency:\nWords that occur more frequently in a\ngiven category are likely to be more signiﬁcant to the spec-\niﬁed category and are thus given higher weightage. Since\nthe occurrence of a rare term in a short text is more sig-\nniﬁcant than its occurrence in a long text, log of the term\nfrequency is used to reduce the importance of raw term\nfrequencies in those collections that have a wide range of\ntext lengths.\nAnaphoric references and synonyms reduce\nthe true term frequency. In morphologically rich languages,\npoor morphological analysis or stemming also adds to this\neﬀect.\n• Inverse Document Frequency: Terms that occur in almost\ntures. Feature sets form a compact and eﬀective representation\nof the whole data. Typically a vector space model is used - each\ndata item can them be visualized as a point in the D-Dimensional\nfeature space where D is the number of features.\nEach word in a text is a potential feature. In the domain of\ntext categorization, words and word-like features (such as phrases)\n1.7. AUTOMATIC TEXT CATEGORIZATION\n51\nare called terms. Documents are treated as bags of terms. Fea-\nture dimensions are thus often very large, running into tens of\nthousands.\nThe curse of dimensionality is that the number of\ntraining data samples required grows exponentially with the num-\nber of features. Choice of the right subset of potential features\nis a major concern. A variety of dimensionality reduction tech-\nniques are used in pattern recognition. The discriminating power\nof features is evaluated and the least discriminating features can\nbe discarded without much loss. Concepts such as Mutual Infor-\nmation and Information Gain have been applied to evaluate the\ndiscriminating power of features. Principal Component Analysis\nis a standard technique for identifying the feature dimensions with\nmaximal variance. Similarly, Singular Value Decomposition is a\ntechnique that rotates the feature space so as to align the most\ndiscriminating features along the axes of the rotated feature space.\nInterested readers are directed to books on Pattern Classiﬁcation\nor Machine Learning for more details.\nCommonly used pre-processing steps include\n• Stop word removal - eliminating function words and other\nvery frequently occurring, less discriminative terms\n• Morphology or Stemming - replacing fully inﬂected words\nwith their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting\nsmall number of features.\nTechniques also exist for validating the adequacy of the model\nfor a given problem and for evaluating the relative signiﬁcance of\nthe various features (which can be used for feature selection).\nWe will illustrate the use of regression as a classiﬁcation tool\nfor identifying language from small text samples in section ???\nbelow.\nNearest Neighbour Classiﬁers\nOne of the conceptually simple and practically quite eﬀective clas-\nsiﬁcation methods is based on the notion of nearest neighbours.\nOnce a collection of objects have been placed in a space of possible\nfeature values, objects of the same kind tend to group together\nprovided the features used are appropriate.\nBirds of the same\nfeather ﬂock together. If you can identify the type of the sur-\nrounding birds, you can guess the type of the bird on hand quite\naccurately. This is the basic idea behind nearest neighbour clas-\nsiﬁcation.\nThe feature values are computed and the k nearest neighbours\nare determined using a suitable distance measure. The value of\nk can be either simply assumed to be some small number, say\n5, or the best value of k can be experimentally determined after\ntrying out several values. The categories of each of nearest neigh-\nbours is checked and the object on hand is assigned the same cat-\negory as the category of the majority of the k-nearest neighbours.\nNote that all the k-nearest neighbours may not be equally distant\nfrom the candidate object. To take the distances of the neigh-\nbours into account, distance-weighted decision rules have been\nproposed. Also, the k-neighbours may or may not show a clear\nmajority. Several advanced ideas have been proposed to improve\nthe classiﬁcation performance.\nNote that each time we need to classify a given object, the k\nnearest neighbours will have to be computed. This involves com-\nputing the distances to all the objects in the collection. Therefore\nthis technique is an instance based classiﬁcation technique.\n250\ntheir estimated relevance to the user query instead of looking for\nexact matches.\n1.4.3\nThe Vector Space Model\nOne of the most widely used models for IR is the vector space\nmodel. Here documents as well as queries are represented as vec-\ntors of features.\nIn the bag-of-words approach, features corre-\nspond to the terms - each term (word or phrase) is a potential\nfeature.\nFeatures are given numerical values.\nIn the simplest\ncase, the feature values are Boolean - that is, a value is 1 if the\ncorresponding term occurs in the document or the query as the\ncase may be, and 0, otherwise. Alternatively, the numerical value\nof a feature can be simply the number of times it occurs in the\ndocument. Each vector is thus simply a list of numbers.\nA vector of n such features can be geometrically viewed as\na point in n-dimensional space. The geometric spatial proximity\nbetween two vectors is used as a metaphor for the semantic prox-\n1.4. INFORMATION RETRIEVAL\n31\nimity between the corresponding documents and/or query strings.\nThis model is thus conceptually simple and appealing. The most\nrelevant documents are the ones that include the most terms from\na given query and thus spatially closest to the query vector. The\nexample below illustrates these ideas in just two dimensions. If\nthere were three terms, we need to consider vectors in three dimen-\nsional space and, by extension of this idea, if there are n terms, the\nvectors will be in n-dimensional space - diﬃcult to get a mental\npicture but mathematically a simple extension all the same.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 1.9 Vector-Space Model\nThe direction of the vectors is a good indicator of the semantic\ncontent of the corresponding documents, not the length of the\nvectors. Hence it is usual to normalize the lengths of all vectors\nto unity and consider only the directions.\n1.4.4\nPerformance Evaluation\nPerformance of IR systems is measured in terms of Recall and\nfrequently in L1 but not in L2. Thus the feature values are all\nintegers.\nNote that the feature values are not computed directly and\nsolely from the training samples. Instead, they are expressed in\nterms of the prior knowledge obtained from corpora as encapsu-\n320\nCHAPTER 2. FOUNDATIONS OF NLP\nlated in the tables obtained in the ﬁrst phase. Since all we need is\nplain text corpora and small corpora are suﬃcient, this two-stage\nfeature extraction is feasible and practicable. The features so ex-\ntracted can be expected to be more robust and more reliable than\nfeatures extracted directly from small training samples.\nIf we are using the MLR technique, the parameters of the\nregression equation are estimated from the training samples ran-\ndomly extracted from the corpora. Then testing can be carried\nout on test data, also extracted randomly from the rest of the cor-\npus. Each sample is analyzed into aksharas and the diﬀerential\nfeature values are obtained. The value of the decision variable\nis computed and the sample classiﬁed accordingly. The perfor-\nmance is measured in terms of Precision, Recall and F-Measure.\nExperiments may be conducted to ascertain the eﬀect of number\nof training samples, size of training samples, size of test samples,\nrelative signiﬁcance of features in various combinations etc. Ex-\nperiments can also be repeated for purposes of cross-validation.\nExperiments can also be performed with diﬀerent values for\nthe threshold in order to explore the trade-oﬀbetween Precision\nand Recall. When encountered with the task of identifying the\nlanguage of a small piece of text, it is possible to initially look for\na high-precision, low-recall solution and reduce the threshold value\niteratively in case identiﬁcation fails until a solution is obtained.\nThe diﬀerences between the within-language-family and across-\nlanguage-family cases may be explored. We can see the degree of\n“closeness” between various language pairs. Hindi and Punjabi",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-7-subsection-5",
                            "title": "Text Classification and Clustering",
                            "content": "vectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives\na brief description of some of these techniques.\nIn particular,\nthere we will show how the Bayesian Learning approach can be\napplied to the task of automatic text categorization. Models are\nbuilt from labelled training data and then applied to classify new\nunseen documents. When there is no labeled training data set\navailable, it is also possible to automatically cluster or group to-\ngether similar documents. A suitable measure of similarity must\nbe deﬁned. Often the term distance is used as a measure of dis-\nsimilarity. Clustering techniques work by attempting to reduce\nthe intra-cluster distances while maximizing the inter-cluster dis-\ntances. See section 2.3.3 for more on this.\nA number of automatic text categorization systems have been\ndeveloped and put to use for English and other major languages of\nthe world. Work on Indian languages has started only recently. A\ncategorization system developed recently by the author could clas-\nsify News Articles in Telugu into broad categories such as Sports,\nPolitics, Economics and Business and Cinema with nearly 95% ac-\ncuracy. The system was trained on a preclassiﬁed set of about 600\n54\nCHAPTER 1. THE INFORMATION AGE\ndocuments and tested on about 200 previously unseen documents.\nDeveloping automatic text categorization systems requires a\nlarge amounts of pre-classiﬁed training data.\nAlso, the perfor-\nmance of the system may deteriorate if the classes considered are\nﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nlize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.\nAutomatic text categorization has many applications. Infor-\nmation Retrieval systems and Search Engines will greatly ben-\neﬁt if the documents in the collection are already categorized.\nIt would then be possible to limit the search to relevant classes\nof documents, thereby enhancing both the speed and the perfor-\nmance.\nText Categorization system can be used for automatic\nﬁltering and routing of documents. For example, junk mails can\nbe detected and removed and mails can be routed to diﬀerent de-\npartments based on their topic. There are tools that collect news\nfrom various newspapers and other sources on the web, perform\nnews aggregation and suitably re-organize the results. Automatic\ntext categorization can be of great value to such systems. Iden-\ntiﬁcation of topic also helps in many other areas such as word\nsense disambiguation by narrowing the space of possibilities and\nbringing things into sharper focus.\n1.7.2\nApproaches to Automatic Text Catego-\nrization\nBefore the 1990s, the predominant approach to text classiﬁcation\nwas the knowledge based approach. Rules and heuristics based on\nexperience were used to place documents in appropriate classes.\nWhen performed on a small scale, as in the case of individual\nlibraries, classiﬁcation experts manually read the title, front mat-\nter, cover page material, table of contents etc. and decided where\nexactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines\nthat the string “abnidella” is unlikely to be a valid English word\nwithout consulting a dictionary.\nThus machine learning is a purely data driven approach. The\ngreatest merit of this approach therefore is its generality and\nadaptability. All we need to migrate to a new language or a new\napplication is to provide appropriate training data in that lan-\nguage or for that application. The machine unlearns and relearns\n246\nCHAPTER 2. FOUNDATIONS OF NLP\nto automatically to adapt to the new situation. Migrating from\none language to another using a linguistic approach, on the other\nhand, would necessitate extensive manual exploration of the new\nlanguage structures and properties.\nMachine learning can be supervised or unsupervised. In su-\npervised learning, a set of labeled training data is given and the\nmachine learns a general decision rule which can be used for clas-\nsiﬁcation of new data items. In unsupervised learning, a set of\nunlabeled training data is given and the machine learns to group\nsimilar data items into clusters so that new data items can be\nplaced into the right clusters. The number of clusters may or may\nnot be known beforehand.\nWe describe here a selection of machine learning techniques\nbrieﬂy. The purpose is to give an exposure to the basic ideas.\nInterested readers may consult books on machine learning and\npattern classiﬁcation for an in-depth exposition.\nRegression as Classiﬁcation\nRegression analysis is a statistical technique for investigating and\nmodeling the relationship between variables in a system. When\nthere are more than two variables in the system, the term multiple\nregression is employed. Regression is often used as a modeling\ntechnique where the value of one of the selected variables, called\nthe response variable, is determined by the values of the other\nindependent variables, also called the regressors. The modeling\nprocess basically involves determining parameters of the model,\nthe classiﬁcation performance.\nNote that each time we need to classify a given object, the k\nnearest neighbours will have to be computed. This involves com-\nputing the distances to all the objects in the collection. Therefore\nthis technique is an instance based classiﬁcation technique.\n250\nCHAPTER 2. FOUNDATIONS OF NLP\nNearest neighbour classiﬁcation techniques work quite well in\nmany application areas and are widely used.\nBayesian Learning Theory\nBayesian Learning is a probabilistic approach to inference based\non the assumption that the quantities of interest are governed by\nprobability distributions and the optimal decision can be made by\nreasoning about these probabilities together with observed data.\nIn corpus based approaches, probabilities are estimated by\ncounting the frequencies of favourable cases and all possible cases\nand taking the ratio of the two.\nFor example, the number of\ndocuments belong to a given category divided by the total number\nof documents would give the probability that a document belongs\nto the speciﬁed category. Thus the probabilities we are talking\nhere are empirical probabilities.\nP(X) denotes the probability of some event X. P(X, Y ) de-\nnotes the joint probability of both the events X and Y occurring\ntogether. P(X|Y ) denotes the probability of X given that Y has\nalready occurred. It is thus a conditional probability. P(X|Y )\ncan be computed as P(X,Y )\nP(Y ) . Similarly, P(Y |X) is P(Y,X)\nP(X) .\nSince P(X, Y ) and P(Y, X) are the same, we can combine these\nlast two equations to get\nP(X|Y ) = P(Y |X)∗P(X)\nP(Y )\nThis is known as Bayes Theorem. To understand Bayes The-\norem let us replace X by “disease” and Y by “symptoms”. Then\nwe get:\nP(disease|symptoms) = P(symptoms|disease)∗P(disease)\nP(symptoms)\nWe are interested in computing the probability that a patient\nhas a particular disease given the symptoms he has. For exam-\nple, we may wish to compute the probability that a patient has\ngiven the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-\numents, typically based on the subject matter or topic, without\nany manual eﬀort. A text categorization program can automati-\ncally categorize thousands of documents in a few minutes. There\nis no way manual classiﬁcation can match that speed. In terms of\naccuracy of classiﬁcation, automatic systems can achieve accura-\ncies of 95% or even higher, depending upon the speciﬁcities of the\ntask. Manual classiﬁcation can in principle be fully correct but in\npractice one must make allowance for some errors.\nAutomated text categorization can be deﬁned as assigning\npre-deﬁned category labels to new documents based on the likeli-\nhood suggested by a training set of labeled documents. Given a\n1.7. AUTOMATIC TEXT CATEGORIZATION\n49\nset of documents with the associated category labels, the system\nlooks for discriminating features that help to set the various cate-\ngories apart. This process is called training. The system learns, so\nto say, how exactly documents within a class are similar and doc-\numents in diﬀerent classes are diﬀerent from each another. Once\nthe system has been trained, it can look at new documents unseen\nbefore and classify them into one of the set categories.\nThe similarities and diﬀerences between documents of various\nclasses are expressed in terms of features. In the simplest case,\nfeatures are the words in these documents. The assumption is that\nthere are words that occur frequently in certain classes but not\nin the others. Words like election, mandate, constituency, party,\nlegislature, parliament are more likely to occur in the political\narena than in sports. Of course there can be politics in sports,\nthere can be elections and parties in sports domain too. That is",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-8",
                    "title": "Machine Translation",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-8-subsection-1",
                            "title": "Machine Translation is Hard",
                            "content": "is no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical\nargument but in practice it does not take us anywhere. The only\nknown examples of good translators are the human beings and we\nhuman beings read, understand and only then translate. There\nare no working systems to prove mechanical transformation ideas\nand no one has any clue how to go about building such a system.\n1.8.4\nChallenges in Machine Translation\nGreat progress has been made in the area of machine translation\nand some systems have even be deployed for regular use in lim-\nited domains. Also, crude, ﬁrst-cut translations are available and\nare being used by Search Engines. Nonetheless, do not expect\nhigh quality automatic translations to become available in the\nnear future. Wherever quality is important (and quality is almost\nalways important) you can only expect semi-automatic, human\naided translation systems.\nSome of the major areas of focus in automatic translation to-\nday are 1) detailed analysis of sentence structure 2) identifying\nthe correct sense of words in context, 3) combining human judge-\nment and commonsense with statistical learning techniques and\n4) building high quality lexical resources in large scale.\n1.8.5\nMachine Translation in India\nThere has been a great interest in India in Machine Translation\nand a lot of work has been done over the last 15 years. In fact\n1.8. MACHINE TRANSLATION\n65\nMT has been given so much of attention that many equate NLP to\nMT. Some groups have concentrated on translating from one In-\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncomes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never\nintended for literary translations. Translation of literary works is\nbest performed by human experts. They can do it, they enjoy\ndoing it and there is no need to replace that with any kind of\nautomatic device. Contrast this situation with, say, Information\nRetrieval from a large collection of documents, a task that human\nbeings just cannot do, nor will they enjoy doing. Machine trans-\nlation is a very diﬀerent kind of an application compared to many\nother areas of language engineering.\nA layman can get direct beneﬁt from an IR system or a sum-\nmarization system. All of us can use these applications in our\ndaily life. But who wants machine translation? Common people\nhave no daily requirement for translation at all. If you are think-\ning in terms of an application meant for direct use by ordinary\npeople, machine translation is a non-existent, imaginary task.\nAutomatic translation makes more sense in those areas where\nit is routine, tedious and boring for people and people really wish\nto avoid doing it manually. Such situations include translation of\nmanuals for scientiﬁc instruments, machinery and consumer prod-\nucts, routine paper work in government oﬃces etc. However, in\nsituations such as these, the people who need translations are not\nexpert translators themselves. Their proﬁciency in language may\nnot be very high. They would therefore be expecting a fully au-\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-2",
                            "title": "Deploying Machine Translation",
                            "content": "up, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\nis no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical\nargument but in practice it does not take us anywhere. The only\nknown examples of good translators are the human beings and we\nhuman beings read, understand and only then translate. There\nare no working systems to prove mechanical transformation ideas\nand no one has any clue how to go about building such a system.\n1.8.4\nChallenges in Machine Translation\nGreat progress has been made in the area of machine translation\nand some systems have even be deployed for regular use in lim-\nited domains. Also, crude, ﬁrst-cut translations are available and\nare being used by Search Engines. Nonetheless, do not expect\nhigh quality automatic translations to become available in the\nnear future. Wherever quality is important (and quality is almost\nalways important) you can only expect semi-automatic, human\naided translation systems.\nSome of the major areas of focus in automatic translation to-\nday are 1) detailed analysis of sentence structure 2) identifying\nthe correct sense of words in context, 3) combining human judge-\nment and commonsense with statistical learning techniques and\n4) building high quality lexical resources in large scale.\n1.8.5\nMachine Translation in India\nThere has been a great interest in India in Machine Translation\nand a lot of work has been done over the last 15 years. In fact\n1.8. MACHINE TRANSLATION\n65\nMT has been given so much of attention that many equate NLP to\nMT. Some groups have concentrated on translating from one In-\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\ncomes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never\nintended for literary translations. Translation of literary works is\nbest performed by human experts. They can do it, they enjoy\ndoing it and there is no need to replace that with any kind of\nautomatic device. Contrast this situation with, say, Information\nRetrieval from a large collection of documents, a task that human\nbeings just cannot do, nor will they enjoy doing. Machine trans-\nlation is a very diﬀerent kind of an application compared to many\nother areas of language engineering.\nA layman can get direct beneﬁt from an IR system or a sum-\nmarization system. All of us can use these applications in our\ndaily life. But who wants machine translation? Common people\nhave no daily requirement for translation at all. If you are think-\ning in terms of an application meant for direct use by ordinary\npeople, machine translation is a non-existent, imaginary task.\nAutomatic translation makes more sense in those areas where\nit is routine, tedious and boring for people and people really wish\nto avoid doing it manually. Such situations include translation of\nmanuals for scientiﬁc instruments, machinery and consumer prod-\nucts, routine paper work in government oﬃces etc. However, in\nsituations such as these, the people who need translations are not\nexpert translators themselves. Their proﬁciency in language may\nnot be very high. They would therefore be expecting a fully au-\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-3",
                            "title": "Approaches to Machine Translation",
                            "content": "diﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua\nis no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical\nargument but in practice it does not take us anywhere. The only\nknown examples of good translators are the human beings and we\nhuman beings read, understand and only then translate. There\nare no working systems to prove mechanical transformation ideas\nand no one has any clue how to go about building such a system.\n1.8.4\nChallenges in Machine Translation\nGreat progress has been made in the area of machine translation\nand some systems have even be deployed for regular use in lim-\nited domains. Also, crude, ﬁrst-cut translations are available and\nare being used by Search Engines. Nonetheless, do not expect\nhigh quality automatic translations to become available in the\nnear future. Wherever quality is important (and quality is almost\nalways important) you can only expect semi-automatic, human\naided translation systems.\nSome of the major areas of focus in automatic translation to-\nday are 1) detailed analysis of sentence structure 2) identifying\nthe correct sense of words in context, 3) combining human judge-\nment and commonsense with statistical learning techniques and\n4) building high quality lexical resources in large scale.\n1.8.5\nMachine Translation in India\nThere has been a great interest in India in Machine Translation\nand a lot of work has been done over the last 15 years. In fact\n1.8. MACHINE TRANSLATION\n65\nMT has been given so much of attention that many equate NLP to\nMT. Some groups have concentrated on translating from one In-\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncomes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never\nintended for literary translations. Translation of literary works is\nbest performed by human experts. They can do it, they enjoy\ndoing it and there is no need to replace that with any kind of\nautomatic device. Contrast this situation with, say, Information\nRetrieval from a large collection of documents, a task that human\nbeings just cannot do, nor will they enjoy doing. Machine trans-\nlation is a very diﬀerent kind of an application compared to many\nother areas of language engineering.\nA layman can get direct beneﬁt from an IR system or a sum-\nmarization system. All of us can use these applications in our\ndaily life. But who wants machine translation? Common people\nhave no daily requirement for translation at all. If you are think-\ning in terms of an application meant for direct use by ordinary\npeople, machine translation is a non-existent, imaginary task.\nAutomatic translation makes more sense in those areas where\nit is routine, tedious and boring for people and people really wish\nto avoid doing it manually. Such situations include translation of\nmanuals for scientiﬁc instruments, machinery and consumer prod-\nucts, routine paper work in government oﬃces etc. However, in\nsituations such as these, the people who need translations are not\nexpert translators themselves. Their proﬁciency in language may\nnot be very high. They would therefore be expecting a fully au-\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-4",
                            "title": "Challenges in Machine Translation",
                            "content": "diﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\nis no need to understand the meanings at all. Given two sets of\nsymbols and a set of rules to operate upon strings of symbols from\nthe two sets, anybody should be able to do translation from one\nlanguage to another. This may sound like a good philosophical\nargument but in practice it does not take us anywhere. The only\nknown examples of good translators are the human beings and we\nhuman beings read, understand and only then translate. There\nare no working systems to prove mechanical transformation ideas\nand no one has any clue how to go about building such a system.\n1.8.4\nChallenges in Machine Translation\nGreat progress has been made in the area of machine translation\nand some systems have even be deployed for regular use in lim-\nited domains. Also, crude, ﬁrst-cut translations are available and\nare being used by Search Engines. Nonetheless, do not expect\nhigh quality automatic translations to become available in the\nnear future. Wherever quality is important (and quality is almost\nalways important) you can only expect semi-automatic, human\naided translation systems.\nSome of the major areas of focus in automatic translation to-\nday are 1) detailed analysis of sentence structure 2) identifying\nthe correct sense of words in context, 3) combining human judge-\nment and commonsense with statistical learning techniques and\n4) building high quality lexical resources in large scale.\n1.8.5\nMachine Translation in India\nThere has been a great interest in India in Machine Translation\nand a lot of work has been done over the last 15 years. In fact\n1.8. MACHINE TRANSLATION\n65\nMT has been given so much of attention that many equate NLP to\nMT. Some groups have concentrated on translating from one In-\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\nIn some cases substitution of appropriate pronouns and def-\ninite noun phrases in the target language may carry forward\nidentical interpretations from source language to target lan-\nguage.\n• Discourse Structure It is not suﬃcient to work with one\nsentence at a time. Sentence by sentence translation does\nnot always work.\nDiscourse level analysis is essential to\nensure good translation.\nThus machine translation requires many steps and the errors\naccumulate and compound.\nThe performance demands on the\nindividual modules will be very high in order to obtain reasonable\nperformance for MT as a whole. MT remains a tough problem.\nThe best known event in the history of machine translation\nis without doubt the publication in November 1966 of the re-\nport by the Automatic Language Processing Advisory Committee\n(ALPAC 1966). Its eﬀect was to bring to an end the substantial\nfunding for MT research in the United States for some twenty\nyears. More signiﬁcantly, perhaps, was the clear message to the\ngeneral public and the rest of the scientiﬁc community that MT\nwas hopeless. For years afterwards, an interest in MT was some-\nthing to keep quiet about; it was almost shameful. To this day,\nthe “failure” of MT is still repeated by many as an indisputable\nfact.\nThe report said (page 16): “There is no emergency in the ﬁeld\nof translation. The problem is not to meet some nonexistent need\nthrough nonexistent machine translation.\nThere are, however,\nseveral crucial problems of translation. These are quality, speed,\nand cost.” Quality, speed and cost remain the most important\nyardsticks even today.\n1.8.2\nDeploying Machine Translation\nWhenever we talk of machine translation, the ﬁrst thing that\ncomes to the mind is translation of literary works. However, lit-\n1.8. MACHINE TRANSLATION\n61\nerary works exploit higher levels of human cognition to bring out\nthe eﬀect and subtle emotions and superﬁcial analysis of sentences\nwill not be suﬃcient to produce good translations. MT was never",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-8-subsection-5",
                            "title": "Machine Translation in India",
                            "content": "Recently, there is also an increased emphasis on translation be-\ntween English and Indian languages. While demonstration level\nsystems have been built, there is a long way to go before machine\ntranslation can be applied in real life situations.\nThere is some scattered work on Information Retrieval, Infor-\nmation Extraction, Text summarization, Text and Web Mining,\netc. Strong, focussed, long term eﬀorts are rarely seen.\nThere is an acute paucity of lexical resources including plain\nand annotated corpora, parallel corpora, dictionaries, thesauri,\nWordNets etc. Greater emphasis is being given to development of\nlexical resources and hopefully things would be much better soon.\nIn spite of being a country with many languages, high degree\nof illiteracy and basically an oral tradition, speech has taken a\nback seat. Only a couple of centres have been active in speech\nfor a long time. There is now a realization that the future lies in\nspeech. There is even some thought on speech based cross-lingual\ninformation access and speech-to-speech translation. Closer inte-\ngration of NLP and speech technologies is the need of the hour.\n2.4.11\nNLP and Sanskrit\nThere is a common misconception that Sanskrit is the best lan-\nguage for NLP. What does this mean?\nShould we stop using\nall other languages and start using only Sanskrit for everything?\nThat would not make much sense.\nMany people would surely\nwant to learn Sanskrit but not for the sake of giving up their own\nmother tongue, not because computers would start demanding\nSanskrit.\n322\nCHAPTER 2. FOUNDATIONS OF NLP\nIt is also not true that Sanskrit is a lot more amenable for\nautomatic processing by the computer. True, Sanskrit has an ex-\ntremely systematic and scientiﬁc way of dealing with all aspects\nof language starting from the alphabet through words, phonetics,\nphonology, morphology and syntax to semantics and proper in-\nterpretation of meanings. Yet all this is designed for intelligent\ndian language to another, exploiting as they do the commonalities\nin linguistic structure and interpretation based on common social\ncontext. Others have started working on translation between En-\nglish and Indian languages. Demonstration level systems are now\nbecoming available but there are still no real life applications.\nThe MAT System\nHere we brieﬂy describe MAT - a small machine assisted trans-\nlation system that was developed by the author for the Govern-\nment of Karnataka, a southern state in India. MAT is used for\ntranslating English texts into Kannada, the oﬃcial language of\nKarnataka. MAT is a parser based translation aid, suitable for\ntranslating between positional languages like English and Indian\nlanguages, which are characterized by a relatively free word or-\nder and a very rich system of morphology. MAT is based on the\nUCSG (Universal Clause Structure Grammar) theory of syntactic\nanalysis developed by the author. The primary goals of the MAT\nproject were to explore the feasibility of parser based translation\nbetween English and Indian languages and to develop a proto-\ntype machine aided translation system for translating the budget\nspeech texts of the Government of Karnataka with the primary\ngoal of saving time and eﬀort. Quality of translation was ulti-\nmately the responsibility of the human translator.\nIt is now fairly clear that fully automatic high quality transla-\ntion is diﬃcult to realize in practice. Either we lose out on quality\nor we will have to involve the human translator in the process\nsomewhere. MAT is a machine assisted translation system that\nprovides for a full spectrum of possibilities - from fully automatic\ngeneration of raw translations suitable for manual post editing,\nthrough semi-automatic translation to almost fully manual trans-\nlation using the facilities provided by the system. The basic idea\nis to make the best of both the human and machine capabili-\nties to achieve good translations with minimum time and eﬀort.\ntomatic, high quality translation which is very diﬃcult to achieve\neven under restricted conditions.\nWith today’s technology MT makes sense only when viewed\nas a combined man-machine eﬀort. Human translators who need\nto routinely translate large volumes of textual material may ﬁnd\nsome of the MT tools useful and time saving. The ﬁnal respon-\nsibility for quality of translation must lie with the human expert,\nnot with the machine. The time has not yet come for stand-alone\nMT.\nOne way MT systems can ﬁnd real uses today is in applica-\ntions in which MT becomes an embedded component. Users do\nnot get to see the quality of translation directly. Only the over-\nall performance of the integrated system is perceived by the user.\nMT is thus getting embedded into Information Retrieval systems,\nleading to cross-lingual IR. Performance of such systems is cur-\n62\nCHAPTER 1. THE INFORMATION AGE\nrently still very low.\n1.8.3\nApproaches to Machine Translation\nSOURCE\nLANGUAGE\nSENTENCE\nTARGET\nLANGUAGE\nSENTENCE\nANALYZER\nGENERATOR\nTRANSFER\nFIG 1.10 Machine Translation\nFully automatic high quality translation has not been possi-\nble so far between any pair of languages of the world in general.\nLanguages are rich and varied - there are a large number of words,\nwords take on various morphological forms, there is a lot more to\nsentence structure than merely a sequence of words, words have\nmany meanings and meanings of sentences are not always simple\ncombinations of meanings of words. It is not possible to obtain\ngood translations without suﬃciently detailed and thorough anal-\nysis of the given texts. Superﬁcial analysis may be suﬃcient to\ngive good enough performance in other areas but translation is a\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\nup, machine costs are coming down. Human translators are not\nalways available where and when needed. You can make multiple\ncopies of an MT system and use it simultaneously at many places\nwhereas one human expert can only be at one place and can only\ndo one thing at a time. The machine does not get bored or tired,\n1.8. MACHINE TRANSLATION\n55\nand it does not complain if asked to work 24 hours a day. With\nincreasing globalization and need to communicate with people in\ndiﬀerent languages, translation loads are increasing every day and\nwe will never be able to meet the demands only through human\ntranslators. MT can, in principle, save a lot of time, eﬀort and\nmoney. The crucial question is can we build MT systems with\nadequate performance to meet these demands.\nAutomatic Translation was perhaps the ﬁrst and the most\nprominent application of NLP. Interest in Machine Translation\nis almost as old as modern digital computers.\nComputers can\nstore and process large collections of textual data eﬃciently and\nso they should be able to translate texts from one language to\nanother without much diﬃculty. After all, a text is simply a se-\nquence of words and if we are able to choose the right words and\nplace them in the right order, we should be able to perform trans-\nlation automatically. That would save a great deal of time, eﬀort\nand money. Such was the optimism with which work on Machine\nTranslation started right in the nineteen ﬁfties and sixties.\n1.8.1\nMachine Translation is Hard\nIt was soon discovered that translation is an inherently complex\ntask and a great deal of fundamental research and development\nwork was essential before we can start building useable systems.\nLet us see why machine translation is a hard problem:\n• Lexical Ambiguities: Every language has many words that\ncan mean more than one thing.\nWords may also belong\nto more than one grammatical category. The English word\n“like” can be a verb, an adjective and a preposition. We use\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-9",
                    "title": "Speech Technologies",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-1-section-9-subsection-1",
                            "title": "Automatic Speech Recognition",
                            "content": "sounds are in the frequency components and the way they change\nwith time.\nFeatures based on such observations are extracted\nfrom a large training data collected from many speakers. Inher-\nent variability is handled by building probabilistic models. Since\nboundaries between linguistic units such as words and sentences\nare almost impossible to recognize accurately, language models are\nbuilt for sub-word units and then extended for whole sequences\nof words based on the statistical analysis of large text corpora.\nTools to perform all these complex tasks are now freely and pub-\nlicly available. There are also commercial products ﬁne tuned for\nspeciﬁc applications such as dictation machines to replace typing.\nIn future we may see more and more of speech based interfaces\nto computers.\nInformation retrieval via speech may become a\nreality soon. Speech recognition is a complex multi-disciplinary\narea and it is beyond the scope of this book to provide a detailed\naccount. Good books are available for the interested reader.\n1.9.2\nSpeech Synthesis\nMachines that read out given texts, called Text-to-Speech (TTS)\nsystems, are in some sense easier to develop since several dimen-\nsions of variability have already been removed in expressing the\nmessage in text form. Speaker’s voice qualities or external noise\nare no longer relevant when text is given as input. Sentence and\nword boundaries are usually quite clear. However, there are is-\nsues that need to be taken care of. There can be words which are\nwritten with the same spelling but pronounced diﬀerently and the\nother way around - project (n) versus project (v) and their versus,\nthere. 1999 may have to be read as one thousand nine hundred\n1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-\nmicrophone with respect to the speaker’s mouth and the recording\nroom conditions can be critical factors. While several centres are\nworking on speech technologies in India, publicly useable speech\ncorpora are not yet available for any of our languages.\nSpeech corpora meant for speech recognition are generally\nrecorded under conditions similar to the conditions where the\nrecognition system may ﬁnally be used, say in an ordinary of-\nﬁce room or at a railway station. Speaker independent systems\nrequire speech data from a large number of speakers of diﬀer-\nent age, sex, speaking rates and styles etc. Spontaneous speech,\nnatural, continuous but carefully articulated speech of coopera-\ntive speakers, or just read speech may be recorded depending on\nneed. For speech synthesis purposes, data from a single speaker\nor a few selected speakers (one male, one female, for example) are\nrecorded under noise-free conditions as in a studio. The future lies\nin speech. we will surely see more and more of speech technologies\nin future.\n2.3.2\nStatistical Approaches to Language\nWe may have diﬃculties in accepting a statistical orientation for\nhuman languages. We feel languages are rule governed and things\nare not a matter of chance or probability. How can simply some\nnumbers characterize something so complex yet highly structured\n242\nCHAPTER 2. FOUNDATIONS OF NLP\nobject as language?\nHow can we interpret numbers and make\nsense out of them? Language manifests regularities and follows\nrules, either it is OK or it is not OK, where is the question of\ngradations? Everything must be either right or wrong, either true\nor false, either zero or one. Where does probability come from?\nLinguistics is all about possibilities. The principles and rules\nof linguistics rule out the impossible combinations and allow valid\ncombinations. The aim is to build theories and models so that all\nand only valid structures can be accepted.\nWe must immediately realize that the notion of probability\nsimple and easy tasks.\nIn fact these are extra-ordinarily com-\nplex tasks. Unlike in written text, there are no clear cut word\nboundaries or even sentence boundaries. Speech signals show sub-\nstantial variability across speakers. Even the same word spoken\nby the same speaker can be very diﬀerent in terms of the signal\ncharacteristics. External noise can cause serious problems. While\nsigniﬁcant progress has been made in automatic speech recogni-\ntion over the last ﬁfty years, we still have a long way to go to\nachieve human like capabilities.\nSpeech understanding requires all aspects of NLP including\nsyntax, semantics and pragmatics and in addition requires the\ncapability to decode information encoded in speech signals.\nA\nspeech signal is a composite of the linguistic message being con-\nveyed, the language and dialect used, style, speakers identiﬁcation\nincluding parameters such as sex, age, health and emotional sta-\ntus, as also background noise which may include speech sounds\nof other people speaking nearby.\nAutomatically extracting the\nlinguistic message while ﬁltering out all other irrelevant aspects\nfrom such a composite signal is not an easy task. While the aim in\nautomatic speech understanding is to understand the meaning and\nintentions conveyed by a given speech utterance, automatic speech\nrecognition tries to look at a supposedly simpler task of extract-\ning the message content and representing it in text form, without\nnecessarily having to understand the meanings or intentions.\nRecognized Message\nEXTRACTION\nFEATURE\nPREPROCESSING\nMODELS\nRECOGNITION\nText Corpora\n(Text)\nSpeech\nSignal\nTraining\nTesting\nFIG 1.12 Speech Recognition\nPeople do not seem to recognize speech to produce a text rep-\nresentation and then process that text in their brains to under-\nstand speech. Understanding seems to be a direct process without\n1.9. SPEECH TECHNOLOGIES\n73\nneed for an intermediate text representation. In fact the very act\nPeople do not seem to recognize speech to produce a text rep-\nresentation and then process that text in their brains to under-\nstand speech. Understanding seems to be a direct process without\n1.9. SPEECH TECHNOLOGIES\n73\nneed for an intermediate text representation. In fact the very act\nof trying to represent speech as text incurs both loss and distortion\nof information. Thus there is no guarantee that speech recogni-\ntion is actually simpler than speech understanding. By saying we\ndo not need to understanding meanings, we are also losing all the\nconstraints that come from semantics. Any way, the decision to\ngo for speech recognition rather than speech understanding is a\npurely practical, engineering decision. Let us now see in broad\nterms how this can be done.\nSpeech sounds are produced by the vibration of the vocal cords\ncaused by the air ﬂow pumped by the lungs, and subsequent ma-\nnipulation of the sounds by the articulatory movements of the\ntongue, lips etc. in the vocal tract to produce diﬀerent kinds of\nsounds. Electronic, mathematical or computational analogues of\nthis source-ﬁlter system have been developed to ﬁnd out how we\ncan distinguish diﬀerent sounds from one another. Finding an ap-\npropriate set of features to characterize and distinguish between\ndiﬀerent speech sounds while accounting for the large inherent\nvariability in natural speech across speakers and contexts is still a\nlargely unsolved problem. It has not been possible so far to build\nspeech recognition systems that handle unlimited vocabulary and\nspontaneous speech of any arbitrary speaker in open domains.\nNevertheless, under restricted conditions, it is possible to develop\nautomatic speech recognition systems that perform fairly well.\nInitial attempts therefore restricted one or more dimensions\nof variability. Isolated word recognition systems did not have to\nworry about identifying word boundaries. Small vocabulary sys-\ntems exploited reduced possibilities of confusing one word with\nand maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE\nand intelligently select the most appropriate units.\nTo make the sounds produced by TTS systems more natural,\nit is essential to include prosodic features such as stress, duration\nand intonation. Systems without good prosody sound very dull\nand machine like. Developing high quality, natural sounding TTS\nsystems is in fact quite diﬃcult. Interested readers may refer to\nany of the good books available on the subject.\n1.9.3\nOther Speech Technologies\nWhile speech recognition attempts to extract the linguistic mes-\nsage in the speech signal without regard to who spoke it, speaker\nrecognition focuses on recognition or identiﬁcation of the speaker.\nHere recognizing what exactly he or she said is secondary. It is be-\nlieved that voice of individuals is almost as unique as ﬁnger prints.\nThus signatures, voice prints, ﬁnger prints, iris prints and palm\nprints can be used in conjunction with one another to build very\nrobust biometric systems to identify individuals. A speaker may\nbe asked to speak out pre-speciﬁed passwords or the system may\ndynamically prompt the user each time with a diﬀerent password.\nWe have just have to accept or reject a candidate or we may have\nto ﬁnd the correct identity of the individual. Speaker recognition\nand identiﬁcation have applications in various forms of security\nand access control.\nWe may tend to think that written texts are more authentic\nsince speech sounds themselves are ephemeral.\nHowever, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-9-subsection-2",
                            "title": "Speech Synthesis",
                            "content": "1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-\npropriate pronunciations is an important task. Indian languages\nuse a phonetic writing system and hence there are really no such\nthings as spellings.\nOnly a few relatively simple rules of map-\nping are suﬃcient to take care of deviations from true phonetic\nrepresentation.\nPARAMETERS\nG2P\nDATABASE\nCONCATENATIVE\nSYNTHESIS\nPROSODY\nWAVEFORM\nSYNTHESIS\nNORMALIZATION\nTEXT\nSPEECH OUTPUT\nSPEECH OUTPUT\nOR\nTEXT\nFIG 1.13 Text to Speech Synthesis\nThere are basically two diﬀerent ways speech sounds can be\nactually generated. In waveform generation approaches, a speech\nsignal waveform is actually constructed from features correspond-\ning to the sounds to be produced. Diﬀerent kinds of resonances\n(called formants) are set up in the vocal cavity when we speak\ndiﬀerent sounds. Formant synthesizers produce diﬀerent speech\nsounds by simulating these resonances. Articulatory synthesizers\nattempt to model the speech production processes and synthesize\nspeech using those models.\nAlternatively, one may record bits\nand pieces of sounds and concatenate them to produce whatever\nsequences of sounds need to be generated. Choice of the right\nlevel of recorded units and smoothing at the junctures are two\nimportant considerations. It is found that splitting in the middle\nof linguistic units can be better than splitting at the boundaries\nthemselves, since the middle portions are generally more stable\nand maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE\nsounds are in the frequency components and the way they change\nwith time.\nFeatures based on such observations are extracted\nfrom a large training data collected from many speakers. Inher-\nent variability is handled by building probabilistic models. Since\nboundaries between linguistic units such as words and sentences\nare almost impossible to recognize accurately, language models are\nbuilt for sub-word units and then extended for whole sequences\nof words based on the statistical analysis of large text corpora.\nTools to perform all these complex tasks are now freely and pub-\nlicly available. There are also commercial products ﬁne tuned for\nspeciﬁc applications such as dictation machines to replace typing.\nIn future we may see more and more of speech based interfaces\nto computers.\nInformation retrieval via speech may become a\nreality soon. Speech recognition is a complex multi-disciplinary\narea and it is beyond the scope of this book to provide a detailed\naccount. Good books are available for the interested reader.\n1.9.2\nSpeech Synthesis\nMachines that read out given texts, called Text-to-Speech (TTS)\nsystems, are in some sense easier to develop since several dimen-\nsions of variability have already been removed in expressing the\nmessage in text form. Speaker’s voice qualities or external noise\nare no longer relevant when text is given as input. Sentence and\nword boundaries are usually quite clear. However, there are is-\nsues that need to be taken care of. There can be words which are\nwritten with the same spelling but pronounced diﬀerently and the\nother way around - project (n) versus project (v) and their versus,\nthere. 1999 may have to be read as one thousand nine hundred\n1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-\nmicrophone with respect to the speaker’s mouth and the recording\nroom conditions can be critical factors. While several centres are\nworking on speech technologies in India, publicly useable speech\ncorpora are not yet available for any of our languages.\nSpeech corpora meant for speech recognition are generally\nrecorded under conditions similar to the conditions where the\nrecognition system may ﬁnally be used, say in an ordinary of-\nﬁce room or at a railway station. Speaker independent systems\nrequire speech data from a large number of speakers of diﬀer-\nent age, sex, speaking rates and styles etc. Spontaneous speech,\nnatural, continuous but carefully articulated speech of coopera-\ntive speakers, or just read speech may be recorded depending on\nneed. For speech synthesis purposes, data from a single speaker\nor a few selected speakers (one male, one female, for example) are\nrecorded under noise-free conditions as in a studio. The future lies\nin speech. we will surely see more and more of speech technologies\nin future.\n2.3.2\nStatistical Approaches to Language\nWe may have diﬃculties in accepting a statistical orientation for\nhuman languages. We feel languages are rule governed and things\nare not a matter of chance or probability. How can simply some\nnumbers characterize something so complex yet highly structured\n242\nCHAPTER 2. FOUNDATIONS OF NLP\nobject as language?\nHow can we interpret numbers and make\nsense out of them? Language manifests regularities and follows\nrules, either it is OK or it is not OK, where is the question of\ngradations? Everything must be either right or wrong, either true\nor false, either zero or one. Where does probability come from?\nLinguistics is all about possibilities. The principles and rules\nof linguistics rule out the impossible combinations and allow valid\ncombinations. The aim is to build theories and models so that all\nand only valid structures can be accepted.\nWe must immediately realize that the notion of probability\nerate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-\nman languages. Speech perception, understanding and synthesis\nwere also major areas of focus. However, early work in NLP was\nmostly limited to handling written texts and was thus largely dis-\njoint from speech technology research. Of late we can see a closer\nintegration of NLP and speech technologies. There is also a trend\ntowards multi-modal communications involving speech, language\nand gestures.\nWe begin this chapter with elements of linguistic analysis es-\nsential for NLP and take up corpus based statistical approaches in\nthe second half. The treatment is introductory. Interested readers\nwill ﬁnd pointers to more advanced and detailed material in the\nbibliography.\n2.1.3\nNLP: An AI Perspective\nThere are three major concerns in NLP:\n• Natural Language Understanding (NLU): How do we un-\nderstand what we read or hear?\n• Natural Language Generation (NLG): How do we synthesize\nNatural Language utterances to convey whatever we have\nin mind?\n• Natural Language Acquisition (NLA): How do we acquire\nor learn a language?\n2.1. INTRODUCTION\n89\nThe Producer-Comprehender Model\nWe use language to communicate information as also our ideas,\nfeelings, emotions and attitudes.\nThis communication through\nlanguage can only happen between active, cognitive processors\nsuch as human beings and, hopefully, computers. We don’t speak\nto walls, right? There can be no proper communication if one or\nboth the parties are sleeping or otherwise inattentive. We shall\nuse the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive\nand maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE\nand intelligently select the most appropriate units.\nTo make the sounds produced by TTS systems more natural,\nit is essential to include prosodic features such as stress, duration\nand intonation. Systems without good prosody sound very dull\nand machine like. Developing high quality, natural sounding TTS\nsystems is in fact quite diﬃcult. Interested readers may refer to\nany of the good books available on the subject.\n1.9.3\nOther Speech Technologies\nWhile speech recognition attempts to extract the linguistic mes-\nsage in the speech signal without regard to who spoke it, speaker\nrecognition focuses on recognition or identiﬁcation of the speaker.\nHere recognizing what exactly he or she said is secondary. It is be-\nlieved that voice of individuals is almost as unique as ﬁnger prints.\nThus signatures, voice prints, ﬁnger prints, iris prints and palm\nprints can be used in conjunction with one another to build very\nrobust biometric systems to identify individuals. A speaker may\nbe asked to speak out pre-speciﬁed passwords or the system may\ndynamically prompt the user each time with a diﬀerent password.\nWe have just have to accept or reject a candidate or we may have\nto ﬁnd the correct identity of the individual. Speaker recognition\nand identiﬁcation have applications in various forms of security\nand access control.\nWe may tend to think that written texts are more authentic\nsince speech sounds themselves are ephemeral.\nHowever, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve",
                            "children": []
                        },
                        {
                            "id": "chapter-1-section-9-subsection-3",
                            "title": "Other Speech Technologies",
                            "content": "sounds are in the frequency components and the way they change\nwith time.\nFeatures based on such observations are extracted\nfrom a large training data collected from many speakers. Inher-\nent variability is handled by building probabilistic models. Since\nboundaries between linguistic units such as words and sentences\nare almost impossible to recognize accurately, language models are\nbuilt for sub-word units and then extended for whole sequences\nof words based on the statistical analysis of large text corpora.\nTools to perform all these complex tasks are now freely and pub-\nlicly available. There are also commercial products ﬁne tuned for\nspeciﬁc applications such as dictation machines to replace typing.\nIn future we may see more and more of speech based interfaces\nto computers.\nInformation retrieval via speech may become a\nreality soon. Speech recognition is a complex multi-disciplinary\narea and it is beyond the scope of this book to provide a detailed\naccount. Good books are available for the interested reader.\n1.9.2\nSpeech Synthesis\nMachines that read out given texts, called Text-to-Speech (TTS)\nsystems, are in some sense easier to develop since several dimen-\nsions of variability have already been removed in expressing the\nmessage in text form. Speaker’s voice qualities or external noise\nare no longer relevant when text is given as input. Sentence and\nword boundaries are usually quite clear. However, there are is-\nsues that need to be taken care of. There can be words which are\nwritten with the same spelling but pronounced diﬀerently and the\nother way around - project (n) versus project (v) and their versus,\nthere. 1999 may have to be read as one thousand nine hundred\n1.9. SPEECH TECHNOLOGIES\n75\nand ninety nine in some situations and as nineteen ninety nine\nin others. Thus Text Normalization forms the very ﬁrst step in\nTTS. Some languages such a English use an alphabetic, spelling\noriented writing system and mapping from written spellings to ap-\nand maximum variations and co-articulation eﬀects occur at the\nboundaries.\nFor example, diphones, consisting of latter half of\nprevious phone and initial half of current phone, have been found\nto be useful. Some systems work with several levels of sound units\n76\nCHAPTER 1. THE INFORMATION AGE\nand intelligently select the most appropriate units.\nTo make the sounds produced by TTS systems more natural,\nit is essential to include prosodic features such as stress, duration\nand intonation. Systems without good prosody sound very dull\nand machine like. Developing high quality, natural sounding TTS\nsystems is in fact quite diﬃcult. Interested readers may refer to\nany of the good books available on the subject.\n1.9.3\nOther Speech Technologies\nWhile speech recognition attempts to extract the linguistic mes-\nsage in the speech signal without regard to who spoke it, speaker\nrecognition focuses on recognition or identiﬁcation of the speaker.\nHere recognizing what exactly he or she said is secondary. It is be-\nlieved that voice of individuals is almost as unique as ﬁnger prints.\nThus signatures, voice prints, ﬁnger prints, iris prints and palm\nprints can be used in conjunction with one another to build very\nrobust biometric systems to identify individuals. A speaker may\nbe asked to speak out pre-speciﬁed passwords or the system may\ndynamically prompt the user each time with a diﬀerent password.\nWe have just have to accept or reject a candidate or we may have\nto ﬁnd the correct identity of the individual. Speaker recognition\nand identiﬁcation have applications in various forms of security\nand access control.\nWe may tend to think that written texts are more authentic\nsince speech sounds themselves are ephemeral.\nHowever, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve\nHowever, it is\neasier to tamper with written texts (or even replace them with\nfake ones) than manipulate speech ﬁles. Thus recorded voice may\nin future be accepted as a better and more authentic proof than\nwritten documents. Technologies to record and carefully preserve\nsuch proofs will be of immense value.\nIf speech can be compressed to read faster, it will save space\nand give a quick view. Similarly if speech can be slowed down,\nwe can carefully listen and use that for, say, transcription. Secre-\ntaries need not learn short hand. Note that a speech signal cannot\nbe simply compressed or expanded linearly in time - that would\nchange the speech quality or even render it unintelligible.\nSeparating the speech of individual speakers in a multi-party\nconversation has many uses. You can then store, index and re-\ntrieve diﬀerent speakers’ speech separately.\n1.10. HUMAN AND MACHINE INTELLIGENCE\n77\nIn multi-lingual environments, one may have to ﬁrst recognize\nthe language even before speech recognition is attempted. This is\nvery important in countries like India where people speak many\ndiﬀerent languages.\nLanguage identiﬁcation from short speech\nsamples is an interesting and challenging problem in itself.\nThere are interesting applications of speech technologies to\nmusic recognition, understanding and synthesis. Machines that\ncompose and sing can be built. Machines that teach music can\nbe conceived. Even models of creativity in music can be explored.\nInformation retrieval in the music domain is a fascinating ﬁeld by\nitself.\nSpeech is a technologically complex area requiring insights\nfrom diverse disciplines.\nAcoustics, Signal Processing, Pattern\nClassiﬁcation, Machine Learning, Linguistics, Computer Science\nare all involved. It is beyond the scope of this book to go into the\ndetails of speech technology.\nInterested readers will ﬁnd many\ngood books and articles for further exploration.\n1.10\nHuman and Machine Intelligence\nany further processing or analysis. In most cases, the electronic\nforms of the documents are never archieved.\nSpeech technologies are especially important for a country like\nIndia with many languages and high levels of illiteracy. There are\nagain certain characteristics of Indian languages which are quite\ndistinct from English. For example, stress is relatively less impor-\ntant and other prosodic features such as duration are more signiﬁ-\ncant. Aspiration is a contrastive feature. A deeper understanding\nof characteristics of our languages is essential and technology de-\nveloped for other languages cannot be simply borrowed. There is\na lot of quantitative work that needs to be done. Very little has\nbeen done so far. We do not even have large speech corpora.\nHowever, things are changing fast. There is a much higher de-\ngree of understanding and appreciation of the language technology\nissues at all levels. There is deﬁnite trend towards standardiza-\ntion. Several serious large scale eﬀorts have been initiated. There\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n267\nis a corpus of about 35 Million words for Telugu today. Auto-\nmatic text categorization systems have been developed. Language\nidentiﬁcation across Indian languages is now possible. OCR sys-\ntems have started appearing for Indian scripts. Major initiatives\nhave been taken in machine translation and speech technologies.\nSearch engines specialized for Indian languages have been devel-\noped. There is progress on Information retrieval and extraction\nsystems as well. There is hope that Indian language technologies\nwill develop very fast over the next few years.\nHowever, we should add a word of caution to our note of op-\ntimism. Whatever has been done so far in terms of technology\ndevelopment is largely ad-hoc, hoch-poch, untested, unﬁnished\nand essentially unusable.\nSpeciﬁcations are not written down.\nSystems are not designed carefully. Instead we jump to imple-\nmentation right away. Bench mark standards and standard test\nday when you could just talk to your computer and your com-\nputer could understand what you said and answer back in speech\nor written language.\nLet us stretch our imagination further and think of possible\ntechnologies of the future. But beware, today’s science is only yes-\nterday’s science ﬁction. Some of things we are talking about here\nhave already been attempted and you may even ﬁnd limited suc-\ncesses here and there. But by and large they are largely unsolved\nproblems as yet.\nWhat if we could do simultaneous translation into many lan-\nguages in real time? You have seen human experts doing this in\nimportant international meetings. What if we could do speech\nunderstanding rather than speech recognition?\nComputers are\ngetting embedded into all kinds of devices and you can think of\ntalking not only to your computer but also to your refrigerator,\nwashing machine, TV, car or your home robot. What if we could\ncombine speech recognition and speech synthesis with automatic\ntranslation to build speech-to-speech translators? You speak in\nTamil over phone and I would hear it in Hindi. I speak back in\nHindi but you hear the Tamil version. Language barriers would\nmelt away.\nA vast majority of our Indian population who are\ncurrently deprived of the direct beneﬁts of information technol-\nogy would be able to access information through speech in their\nown language. Knowledge of English would no longer be essen-\ntial. You could get information from your phone, there is no need\nto have access to a computer or know how to use it eﬀectively.\nWhat if could combine intelligent information retrieval systems\nwith automatic summarization systems or Information Extraction\nsystems? What if we could integrate these with Expert Systems\nand Decision Support systems? What if we could build intelligent\n1.11. SHAPE OF THINGS TO COME\n83\nassistants and robots for use at oﬃce, home or factory? What if\nyou can instruct your TV, in spoken language, to watch all the",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-1-section-10",
                    "title": "Human and Machine Intelligence",
                    "content": null,
                    "children": []
                },
                {
                    "id": "chapter-1-section-11",
                    "title": "Shape of Things to Come",
                    "content": null,
                    "children": []
                }
            ]
        },
        {
            "id": "chapter-2",
            "title": "Foundations of NLP",
            "content": null,
            "children": [
                {
                    "id": "chapter-2-section-1",
                    "title": "Introduction",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-2-section-1-subsection-1",
                            "title": "Language, Communication, Technology",
                            "content": "slow decay in the use of our languages. Your grand parents were\nwell read scholars, your parents studied your language up to high\nschool and were able to read and write, you can speak but you\ncannot read and write, and your children will perhaps ﬁnd it dif-\nﬁcult even to speak your own language ﬂuently. Language is a\nmirror of human life. Languages are also treasuries of human civ-\nilization. Everything that we know today is encoded in language\nsomewhere. Everything we have learnt over the last thousands\nof years is hidden somewhere in our languages. As the languages\nslowly go into oblivion, so do these vast treasures of knowledge -\nknowledge of medicine, knowledge of astronomy, of mathematics,\nof mythology, of human values, of traditions and customs. Tech-\nnology development for Indian languages has the potential to slow\ndown and perhaps even reverse this trend, at least to some extent.\nAlmost all of the commercial and business transactions are\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n263\ndone in English not only for interacting with people abroad but\neven within the country. A large part of government and legal\ntransactions are also performed in English.\nAlmost all of the\nhigher education is in the medium of English. Thus centres of\npower and money do not often see any great urgency for Indian\nlanguage technologies. Private companies in Information technol-\nogy sector recruit only those who know English. Companies need\npeople who know English to interact with their foreign counter-\nparts and it is easy to ﬁnd people who are good in the relevant\nskills as also in English. Since everybody in the companies knows\nEnglish, even the thought of using other local languages never\narises. Groups trying to promote Indian language computing com-\nplain of lack of market. It is only recently that the importance\nof local language computing is getting realized more and more\nand Government of India has started promoting Indian language\ntechnologies in a big way.\nany further processing or analysis. In most cases, the electronic\nforms of the documents are never archieved.\nSpeech technologies are especially important for a country like\nIndia with many languages and high levels of illiteracy. There are\nagain certain characteristics of Indian languages which are quite\ndistinct from English. For example, stress is relatively less impor-\ntant and other prosodic features such as duration are more signiﬁ-\ncant. Aspiration is a contrastive feature. A deeper understanding\nof characteristics of our languages is essential and technology de-\nveloped for other languages cannot be simply borrowed. There is\na lot of quantitative work that needs to be done. Very little has\nbeen done so far. We do not even have large speech corpora.\nHowever, things are changing fast. There is a much higher de-\ngree of understanding and appreciation of the language technology\nissues at all levels. There is deﬁnite trend towards standardiza-\ntion. Several serious large scale eﬀorts have been initiated. There\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n267\nis a corpus of about 35 Million words for Telugu today. Auto-\nmatic text categorization systems have been developed. Language\nidentiﬁcation across Indian languages is now possible. OCR sys-\ntems have started appearing for Indian scripts. Major initiatives\nhave been taken in machine translation and speech technologies.\nSearch engines specialized for Indian languages have been devel-\noped. There is progress on Information retrieval and extraction\nsystems as well. There is hope that Indian language technologies\nwill develop very fast over the next few years.\nHowever, we should add a word of caution to our note of op-\ntimism. Whatever has been done so far in terms of technology\ndevelopment is largely ad-hoc, hoch-poch, untested, unﬁnished\nand essentially unusable.\nSpeciﬁcations are not written down.\nSystems are not designed carefully. Instead we jump to imple-\nmentation right away. Bench mark standards and standard test\nFoundations of Natural\nLanguage Processing\n2.1\nIntroduction\n2.1.1\nLanguage, Communication, Technology\nIf there is one thing that sets human beings apart from all other\nliving creatures it is Language. Language and thought are closely\nrelated. Language enables us to articulate our thoughts and com-\nmunicate with fellow human beings. Man is a social animal. It is\ncommunication through language that binds this society together.\nIt is the language faculty that makes us what we are today. If it\nwere not for language, perhaps the human society could not have\ndeveloped to this extent. To understand the importance of lan-\nguage, try spending just one day without using language in any\nway. Asking somebody to shut up is a big punishment.\nLanguage manifests itself most naturally in the form of speech.\nSpeech is a natural, eﬃcient and direct means of communication\nbetween human beings. Technology such as the Tape Recorder,\nRadio, the Television, the Telephone make it possible for people\nto communicate with one another across space and time - you can\nspeak at one place and at one time and be heard at another place\nor at a later time.\nLanguage can also be codiﬁed in other ways. Speech, by its\nvery nature is transient. Writing, that is creating graphical shapes\n85\n86\nCHAPTER 2. FOUNDATIONS OF NLP\non a two dimensional surface, is more permanent. Scribing, en-\ngraving or embossing on any kind of a surface including stone,\npalm leaves, parchment, metal, cloth or paper qualiﬁes as written\nform of language. Speech is an all-in-one representation, encoding\nas it does the message being conveyed, the language, the speaker’s\nidentity, his or her emotional status, environmental conditions and\nso on. Written text is limited - a lot of useful information is lost\nwhen we write down things. Yet writing has played a major role\nin shaping the destiny of mankind. In fact our knowledge of our\nhistory owes in a large measure to the development of writing.\nso on. Written text is limited - a lot of useful information is lost\nwhen we write down things. Yet writing has played a major role\nin shaping the destiny of mankind. In fact our knowledge of our\nhistory owes in a large measure to the development of writing.\nWriting gives us an opportunity to carefully plan, organize and\nstructure our thoughts and thus communicate more eﬀectively.\nSpeech is direct and real-time. The written form of communica-\ntion, on the other hand, is not limited by the real-time processing\nconstraints. Hence writing can be more detailed, more elaborate.\nWritten texts can be preserved for long periods of time, repro-\nduced in large quantities and read by many people at the same\ntime at diﬀerent places.\nPrinting technology enabled eﬀective dissemination of knowl-\nedge and gave a big push to human development. Now the mod-\nern computer and communication technology is making it ever\nso much more easier to create, store, modify, reproduce and dis-\nseminate documents in electronic form. A single CD can store\nthe equivalent of several hundred full length books. Modern tech-\nnology has revolutionized the way we communicate. The impact\ntechnology will have on society at large is diﬃcult to guage.\nTechnology now makes it possible to scan text documents and\ncreate images of these texts.\nThis technology goes a long way\nin preserving ancient manuscripts.\nIt is easier to make several\ncopies of electronic documents and store them in many diﬀerent\nmedia and at many diﬀerent places for ensuring safe custody. Pre-\nserving palm leaves etc. is more diﬃcult. Technology, known as\nOptical Character Recognition also exists for recognizing the writ-\nten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on\nconcerned with social aspects of language. Language is related\nto social status, power and politics. Historical linguistics is con-\ncerned with linguistic genetics and language change over time.\n2.3. STATISTICAL APPROACHES\n233\nLanguage policy is important as it has direct implications for the\npeople. Should the primary education be in the mother tongue\nor in some other language? The three language formula in our\ncountry is a example of language policy implementation. Teach-\ning language is another aspect that is closely related other areas\nof language and linguistics. Language teaching requires method-\nologies and techniques diﬀerent from those required for teaching,\nsay, mathematics or science. There are even language games that\ncan promote eﬀective language learning. To know the importance\nof language try to spend one day, just one day, without speaking,\nlistening, reading or writing! That would be extremely diﬃcult.\n2.3\nCorpus Based and Statistical Ap-\nproaches\nAt one point of time, experts used to specify what is right or ac-\nceptable and what is not. Grammars used to be prescriptive in\nnature and students could be punished for not following the rules.\nThis strictness was considered essential for maintaining the purity\nand standards. After all language is for communication and we\ncannot eﬀectively communicate if we all do not follow a set of com-\nmonly accepted protocols and standards. Looseness and lightness\nof thought about language is really more dangerous than careless\nuse of language. We often hear people say that grammar is not\nimportant and as long as they can communicate with others, that\nis good enough. But one must remember that we cannot commu-\nnicate eﬀectively unless we take language a bit more seriously. A\nlarge number of day to day problems at home or oﬃce can ac-\ntually be traced to communication problems arising from laxity\nin the use of language. Carelessness about language is harmful.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-2",
                            "title": "Natural Language Processing and Computational Linguistics",
                            "content": "ten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on\ntext documents. While there is an increasing trend towards pro-\ncessing of multi-media documents, text processing continues to be\n2.1. INTRODUCTION\n87\nthe major focus. We shall limit ourselves mainly to processing of\ntext documents in this book.\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\nThis chapter deals with the fundamentals of language processing\nthat are essential for realizing Intelligent Information Retrieval\nas well as other applications of language and speech technologies.\nNatural Language Processing (NLP) forms the backbone of every\nhuman language technology application. NLP is concerned with\nnatural or human languages - languages which we human beings\nuse for day to day communications, as against artiﬁcial languages\nsuch as computer and robot programming languages.\nThe terms Natural Language Processing and Computational\nLinguistics have been used interchangeably and we shall do the\nsame here. There is no real diﬀerence between the two except per-\nhaps that linguists sometimes tend to prefer the latter term - in\nComputational Linguistics, Linguistics is the head of the phrase\nand Computational is only an adjectival modiﬁer. Computers, like\nmathematics, are generic and powerful tools and mere use of com-\nputers to carry out linguistic studies will not qualify as a distinct\nbranch of study in itself. Computational Linguistics or NLP is\ndiﬀerent from Linguistics in that the primary aim is to build com-\nputational models of various aspects of human language faculty.\nSuch models need to be simple, elegant and eﬃcient, yet very pre-\ncise, detailed and exhaustive. The models built must stand the\ntest of large scale real life data - language as people use it. NLP\nnot been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.\nSemantics has taken a back seat\neven in linguistics. Not everything can be done by simply count-\ning words. Perhaps the time has come to rethink and re-plan the\nNLP agenda.\n2.1.5\nLinguistics versus NLP\nMany still think that computational linguistics or NLP can be\ndone by putting together a linguist and a programmer. Just as\nChina plus Philosophy does not yield Chinese Philosophy, Compu-\ntational Linguistics or NLP is not merely computers plus linguis-\ntics. Computers are powerful tools, like mathematics, and have\napplications and uses in almost all branches of study. Computa-\ntional, linguistic, statistical and engineering concepts need to be\nclosely integrated in order to synthesize a new ﬁeld of study called\ncomputational linguistics, NLP or language engineering. The pur-\npose of this book is to give a ﬂavor of such an integrated multi-\ndisciplinary ﬁeld. Understanding or at least appreciation of the\nconcerns, aims, assumptions, jargon, deﬁnitions and methodolo-\ngies of various disciplines concerned is important. People who are\ntrained to think and work like that are in short supply. If this\nbook helps in any way to develop such trained manpower, that\nwould be considered a success.\nComputational linguistics or NLP provides a computational\nview point for linguistics. There are many things that are obvious\nfor human beings and the focus will naturally be only on the un-\nusual, special situations when all this is for human consumption.\nPeople have world knowledge and commonsense and this forms a\ntacit assumption when dealing with people. The goal of NLP, on\nthe other hand, is to make the computer, the idiot box that has no\ncommonsense, world knowledge or human-like reasoning power, to\nEdition, Pearson Education, 2003\n14. Lawrence Rabiner and Biing-Hwang Juang, “Fundamentals\nof Speech Recognition”, Pearson Education, 2003\n15. Michael G Dyer, “In-Depth Understanding”, MIT Press,1983\n16. Nigel Fabb, “Sentence Structure”, Routledge\n17. Ralph Grishman, “Computational Linguistics: An Intro-\nduction”, Cambridge University Press, 1999\n18. Richard Coates, “Word Structure”, Routledge, 1999\n19. Richard Hudson, “Word Meaning”, Routledge\n20. Richard O Duda, Peter E Hart and David G Stork, “Pattern\nClassiﬁcation”, John Wiley and Sons Inc., 2001\n21. Richard Sproat, “Morphology and Computation”, MIT Press,\n1992\n22. Roche E.and Schabes Y. (Eds), “Finite State Language Pro-\ncessing”, MIT Press, 1997\n23. Tom M Mitchell, “Machine Learning”, The McGraw-Hill\nCompanies Inc., 1997\n24. The Unicode Standard Version 4.0, Addison-Wesley, 2004\n354\nAppendix 1: C5 Tag\nSet\n1. AJ0 adjective (unmarked) (e.g. GOOD, OLD)\n2. AJC comparative adjective (e.g. BETTER, OLDER)\n3. AJS superlative adjective (e.g. BEST, OLDEST)\n4. AT0 article (e.g. THE, A, AN)\n5. AV0 adverb (unmarked) (e.g. OFTEN, WELL, LONGER,\nFURTHEST)\n6. AVP adverb particle (e.g. UP, OFF, OUT)\n7. AVQ wh-adverb (e.g. WHEN, HOW, WHY)\n8. CJC coordinating conjunction (e.g. AND, OR)\n9. CJS subordinating conjunction (e.g. ALTHOUGH, WHEN)\n10. CJT the conjunction THAT\n11. CRD cardinal numeral (e.g. 3, FIFTY-FIVE, 6609)\n12. DPS possessive determiner form (e.g. YOUR, THEIR)\n13. DT0 general determiner (e.g. THESE, SOME)\n14. DTQ wh-determiner (e.g. WHOSE, WHICH)\n15. EX0 existential THERE\n16. ITJ interjection or other isolate (e.g. OH, YES, MHM)\n17. NN0 noun (neutral for number) (e.g. AIRCRAFT, DATA)\n355\n18. NN1 singular noun (e.g. PENCIL, GOOSE)\n19. NN2 plural noun (e.g. PENCILS, GEESE)\n20. NP0 proper noun (e.g. LONDON, MICHAEL, MARS)\n21. ORD ordinal (e.g. SIXTH, 77TH, LAST)\n22. PNI indeﬁnite pronoun (e.g. NONE, EVERYTHING)\n23. PNP personal pronoun (e.g. YOU, THEM, OURS)\n24. PNQ wh-pronoun (e.g. WHO, WHOEVER)\nguage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from\nlearning language by merely interacting with the community of\nspeakers of that language. Natural language acquisition deals with\nall forms of language learning.\nThe aim in all cases is to build computational models - models\nthat are amenable for detailed inspection, models that can be\ntested on real life data. The models need to be both exhaustive\nand very detailed and precise. Samples are not suﬃcient.\nAlthough understanding, generation and acquisition have all\nbeen the primary goals of NLP, we ﬁnd that over the last 50 years\nor so maximum attention has been given to natural language un-\nderstanding, generation has received relatively less attention and\nlanguage learning has not received much attention at all. In fact\nan analysis view point is presumed in many cases by default. Thus\nwhile talking about syntax, NLP researchers tend to take the syn-\ntactic analysis view point rather than that of synthesis or gener-\nation. Linguists, on the other hand, are more used to thinking\nof grammars as abstract characterizations of the constraints of a\nlanguage for producing all and only valid structures.\nNLP involves a deep understanding of human cognition and\nintelligence.\nFor the sake of convenience, the language faculty\nis often studied independently of other cognitive tasks but we\nmust remember that this is only a restricted view taken for conve-\nnience and not the complete and accurate picture in itself. NLP is\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\na model of human language processing than an ineﬃcient one.\nSince computationally less powerful grammars will in general lead\nto more eﬃcient parsing, the search in NLP is for the least pow-\nerful grammar that is suﬃcient to deal eﬀectively with natural\nlanguages. We should not use a bull dozer if that could be done\n2.2. COMPUTATIONAL LINGUISTICS\n179\nwith a hand tool. We should not use context free grammars if that\ncould be done with regular grammars. Some of the early grammar\nformalisms including the transformational grammar proposed by\nNoam Chomsky and the Augmented Transition Network (ATN)\ngrammars proposed by computer scientists were computationally\nmore much more complex than needed. Many of the major gram-\nmar formalisms proposed by linguists continue to be that way even\ntoday. Positing an unnecessarily complex mechanism is unwise\nand wasteful. Computer science provides many powerful tools for\nanalyzing the formal complexity of grammars and the correspond-\ning parsing systems.\nAcademicians and researchers have a tendency to ﬁrst look for\nwhat is most interesting, rather than what is most useful. Intel-\nlectuals are motivated by love of complexity. We take pride and\nderive personal satisfaction in doing complex things.\nWe even\nhave a tendency to do simple things in a complex way. The Red\nQueen in Lewis Carrol’s Alice in Wonderland said, “I could have\ndone it in a much more complicated way”, with immense pride. A\nlot of time and eﬀort is often spent on a few rare, odd exceptional\nexamples rather than lay primary emphasis on simple, commonly\nused structures. Priorities get distorted.\nii) Universality:\nIt has been well recognized that despite superﬁcial diﬀerences,\nhuman languages share certain common underlying principles.\nLinguists are on the look out for a universal grammar - a grammar\nthat uncovers these underlying universal features. Linguists view\nuniversal grammar as an in-born biological endowment of the hu-",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-3",
                            "title": "NLP: An AI Perspective",
                            "content": "erate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-\nman languages. Speech perception, understanding and synthesis\nwere also major areas of focus. However, early work in NLP was\nmostly limited to handling written texts and was thus largely dis-\njoint from speech technology research. Of late we can see a closer\nintegration of NLP and speech technologies. There is also a trend\ntowards multi-modal communications involving speech, language\nand gestures.\nWe begin this chapter with elements of linguistic analysis es-\nsential for NLP and take up corpus based statistical approaches in\nthe second half. The treatment is introductory. Interested readers\nwill ﬁnd pointers to more advanced and detailed material in the\nbibliography.\n2.1.3\nNLP: An AI Perspective\nThere are three major concerns in NLP:\n• Natural Language Understanding (NLU): How do we un-\nderstand what we read or hear?\n• Natural Language Generation (NLG): How do we synthesize\nNatural Language utterances to convey whatever we have\nin mind?\n• Natural Language Acquisition (NLA): How do we acquire\nor learn a language?\n2.1. INTRODUCTION\n89\nThe Producer-Comprehender Model\nWe use language to communicate information as also our ideas,\nfeelings, emotions and attitudes.\nThis communication through\nlanguage can only happen between active, cognitive processors\nsuch as human beings and, hopefully, computers. We don’t speak\nto walls, right? There can be no proper communication if one or\nboth the parties are sleeping or otherwise inattentive. We shall\nuse the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nthe correct sense of the word can we be sure that we are looking\nat the right document. We may try to take into account the order\nof words in the query. A full syntactic analysis may be carried\n3.3. TOWARDS INTELLIGENT IR\n339\nout. We may try to adapt to the user based on direct or indirect\nfeedback. We may try to take into account the authority of the\nsource.\nWe have already seen that many of the NLP tasks are inher-\nently diﬃcult in themselves and performance of current systems\nis limited. Thus full syntactic analysis is not only time consuming\nbut also limited in performance - current grammars and parsers\nfail on a signiﬁcant percentage of cases. Unrestricted WSD sys-\ntems are still at a research level. The real challenge, therefore, is\nto integrate the best of available NLP technologies with IR models\nwithout sacriﬁcing the simplicity and eﬃciency of IR models. We\nwill explore below some the ideas and techniques that have been\nused to build better IR systems.\n3.3.1\nImproving User Queries - Relevance Feed-\nback\nA big question is how do we assess the relevance of a retrieved\ndocument for a given query? Relevance is a subjective judgment\nand may include being on the proper subject, being timely (re-\ncent information), being authoritative (from a trusted source),\nsatisfying the goals of the user and his/her intended use of the\ninformation (information need), etc. Relevance is not an yes/no\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\ndependability of documents. Developing an information retrieval\nsystem that can perform like a human assistant has been dubbed\nas the software grand challenge. It is natural, therefore, that we\nhave talked more of problems and broad ideas rather than concrete\nsolutions.\n352\nCHAPTER 3. ADVANCES IN IR\nOn the one hand we feel the need for deeper linguistic analysis\nand NLP. On the other hand we want great speed and robustness.\nThe challenge is to use deeper linguistic analysis without losing\nthe advantages of speed, ﬂexibility and robustness.\nLinguistic\nanalysis is usually quite language speciﬁc whereas statistical tech-\nniques and machine learning algorithms can be adapted to a wide\nvariety of languages without much manual eﬀort.Linguists have\nbeen talking of universal grammars for a long time now but there\ndo not seem to be any such universal grammars that can be used\nright away.\nDevelopments in IR must be understood in the context of de-\nvelopment in related ﬁelds such as Information Extraction, Text\nCategorization and Automatic Summarization as also NLP tech-\nnologies such as WordNets, stemming algorithms, partial parsing\nsystems and WSD. With the rapid developments in each of these\nﬁelds and increasing synergy between the various areas, we can ex-\npect IR systems to become much more sophisticated in the years\nto come.\nDocument in Indian languages have slowly started growing.\nThere are still some teething problems with regard to non-standard\nencoding schemes. We can hope that the Indian language content\nwill grow fast in the near future. The need for indexing and search-\ning pages in Indian languages will grow. The tools and techniques\nwhich have been been ﬁne tuned for English do not necessarily\nwork well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages\nguage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from\nlearning language by merely interacting with the community of\nspeakers of that language. Natural language acquisition deals with\nall forms of language learning.\nThe aim in all cases is to build computational models - models\nthat are amenable for detailed inspection, models that can be\ntested on real life data. The models need to be both exhaustive\nand very detailed and precise. Samples are not suﬃcient.\nAlthough understanding, generation and acquisition have all\nbeen the primary goals of NLP, we ﬁnd that over the last 50 years\nor so maximum attention has been given to natural language un-\nderstanding, generation has received relatively less attention and\nlanguage learning has not received much attention at all. In fact\nan analysis view point is presumed in many cases by default. Thus\nwhile talking about syntax, NLP researchers tend to take the syn-\ntactic analysis view point rather than that of synthesis or gener-\nation. Linguists, on the other hand, are more used to thinking\nof grammars as abstract characterizations of the constraints of a\nlanguage for producing all and only valid structures.\nNLP involves a deep understanding of human cognition and\nintelligence.\nFor the sake of convenience, the language faculty\nis often studied independently of other cognitive tasks but we\nmust remember that this is only a restricted view taken for conve-\nnience and not the complete and accurate picture in itself. NLP is\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy\nUniversity of Hyderabad\nPublished By\nEss Ess Publications\nFor\nSarada Ranganathan Endowment for Library\nScience\nBangalore, INDIA\n2006\ni\nc\n⃝Kavi Narayana Murthy and Sarada Ranganathan\nEndowment for Library Science (2005)\nAll rights reserved. No part of this publication may be\nreproduced, stored in a retrieval system or transmitted, in\nany form or by any means, electronic, mechanical, photo-\ncopying, recording or otherwise without the prior written\npermission of the publisher.\nThis book has been printed from the camera-ready\ncopy prepared by the author using LATEX\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy, University of Hyderabad\nSarada Ranganathan Endowment Lecture, 24(2004)\nFirst Published 2006\nISBN 81-7000-485-3\nPrice: Rs. 850/-\nPublished by\nEss Ess Publications\n4837/24, Ansari Road, Darya Ganj, New Delhi-110 002\nTel: 001-23260807 Fax: 001-23274173\nE-mail: essess@del3.vsnl.net.in url:\nhttp//www.essessreference.com\nFor\nSarada Ranganathan Endowment for Library\nScience\n702, ‘Upstairs’, 42nd Cross, III Block, Rajajinagar,\nBangalore 560 010\nE-mail: srels@vsnl.com Tel: 080-23305109\nPrinted in India at: Printline, New Delhi 111 002\nii\nPreface\nThe contributions of Dr. S R Ranganathan to the ﬁeld of\nlibrary and information sciences is well known. Sarada Ran-\nganathan Endowment for Library Science (SRELS), founded\nby Dr Ranganathan in 1961 has been carrying out com-\nmendable work in promoting library and information sci-\nences. SRELS has been working towards improvement of\nlibrary and information services in India, training personnel\nin library and information sciences and applying research\nresults in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-4",
                            "title": "NLP Over the Decades",
                            "content": "try to develop everything from scratch on their own rather than\nteam up with others. This may be partly due to lack of mutual\ntrust and understanding. As a country, we have the potential to\nbecome world leaders in language technologies. But we need to\nlearn many basic lessons ﬁrst.\n2.5. CONCLUSIONS\n325\n2.5\nConclusions\nBy now you will have realized that NLP is a vast and highly\ninter-disciplinary subject. It is not possible to cover everything in\ndetail in a single book. The purpose of this book is only to give\nyou an initial idea about the nature of problems and issues, kinds\nof approaches that can be and have been taken, some idea about\nthe current status and challenges yet to be overcome and some\ngeneral idea about the shape of things to come. Each of the topics\ncovered here is a vast subject in itself and the readers interested\nin pursuing any of the areas in more depth will ﬁnd many good\nbooks and research articles. We have seen that although a great\ndeal of progress has been made over the last ﬁve decades, many of\nthe core problems remain unsolved. There are many interesting\nand useful applications and there is a lot that needs to be done. If\nthis book helps beginners to get interested and to kick start serious\nexplorations and research, it would have served its purpose.\nWe have been looking at NLP both as an independent ﬁeld of\nstudy and in the context of Information retrieval and other closely\nrelated applications. We will get back to IR in the next chapter\nand look at some of the advances that are being made in that\nﬁeld.\n326\nCHAPTER 2. FOUNDATIONS OF NLP\nChapter 3\nAdvances in\nInformation Retrieval\nInformation Retrieval is all about eﬃcient storage and retrieval\nof documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nguage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from\nlearning language by merely interacting with the community of\nspeakers of that language. Natural language acquisition deals with\nall forms of language learning.\nThe aim in all cases is to build computational models - models\nthat are amenable for detailed inspection, models that can be\ntested on real life data. The models need to be both exhaustive\nand very detailed and precise. Samples are not suﬃcient.\nAlthough understanding, generation and acquisition have all\nbeen the primary goals of NLP, we ﬁnd that over the last 50 years\nor so maximum attention has been given to natural language un-\nderstanding, generation has received relatively less attention and\nlanguage learning has not received much attention at all. In fact\nan analysis view point is presumed in many cases by default. Thus\nwhile talking about syntax, NLP researchers tend to take the syn-\ntactic analysis view point rather than that of synthesis or gener-\nation. Linguists, on the other hand, are more used to thinking\nof grammars as abstract characterizations of the constraints of a\nlanguage for producing all and only valid structures.\nNLP involves a deep understanding of human cognition and\nintelligence.\nFor the sake of convenience, the language faculty\nis often studied independently of other cognitive tasks but we\nmust remember that this is only a restricted view taken for conve-\nnience and not the complete and accurate picture in itself. NLP is\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nthe correct sense of the word can we be sure that we are looking\nat the right document. We may try to take into account the order\nof words in the query. A full syntactic analysis may be carried\n3.3. TOWARDS INTELLIGENT IR\n339\nout. We may try to adapt to the user based on direct or indirect\nfeedback. We may try to take into account the authority of the\nsource.\nWe have already seen that many of the NLP tasks are inher-\nently diﬃcult in themselves and performance of current systems\nis limited. Thus full syntactic analysis is not only time consuming\nbut also limited in performance - current grammars and parsers\nfail on a signiﬁcant percentage of cases. Unrestricted WSD sys-\ntems are still at a research level. The real challenge, therefore, is\nto integrate the best of available NLP technologies with IR models\nwithout sacriﬁcing the simplicity and eﬃciency of IR models. We\nwill explore below some the ideas and techniques that have been\nused to build better IR systems.\n3.3.1\nImproving User Queries - Relevance Feed-\nback\nA big question is how do we assess the relevance of a retrieved\ndocument for a given query? Relevance is a subjective judgment\nand may include being on the proper subject, being timely (re-\ncent information), being authoritative (from a trusted source),\nsatisfying the goals of the user and his/her intended use of the\ninformation (information need), etc. Relevance is not an yes/no\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\ntranslation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP\nsystems came to focus. During the seventies and eighties, much\nmore sophisticated systems were built. Linguistic data resources\nhad expanded and tools and technologies had also matured to\nsome extent. Slowly signs of saturation started showing up. There\nwere barriers that were diﬃcult to cross. Performance of systems\ncould not be improved even with great eﬀort. It took great eﬀort\nand time to develop and improve linguistic knowledge but the\nincremental improvements in performance obtained got smaller\nand smaller.\nThe early nineties saw a major paradigm shift. The limita-\ntions of the knowledge based approach had been well understood.\nIt had been shown that natural language understanding and gen-\neration could be done if all the required knowledge could be made\n94\nCHAPTER 2. FOUNDATIONS OF NLP\navailable but knowledge acquisition became the real bottleneck.\nExperts were hard to ﬁnd, hand-crafting knowledge structures was\nextremely tedious, time consuming and costly and after all that,\nsaturation points would be reached beyond which it is diﬃcult to\nprogress. Focus shifted to machine learning techniques to auto-\nmatically ‘learn’ to solve a problem by looking at a large collection\nof examples. Computing power and memory had increased mani-\nfolds and machine costs had drastically come down. What could\nonce be tried out only on big machines could now be attempted on\na desk top PC. Focus shifted to development and statistical anal-\nysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\ndependability of documents. Developing an information retrieval\nsystem that can perform like a human assistant has been dubbed\nas the software grand challenge. It is natural, therefore, that we\nhave talked more of problems and broad ideas rather than concrete\nsolutions.\n352\nCHAPTER 3. ADVANCES IN IR\nOn the one hand we feel the need for deeper linguistic analysis\nand NLP. On the other hand we want great speed and robustness.\nThe challenge is to use deeper linguistic analysis without losing\nthe advantages of speed, ﬂexibility and robustness.\nLinguistic\nanalysis is usually quite language speciﬁc whereas statistical tech-\nniques and machine learning algorithms can be adapted to a wide\nvariety of languages without much manual eﬀort.Linguists have\nbeen talking of universal grammars for a long time now but there\ndo not seem to be any such universal grammars that can be used\nright away.\nDevelopments in IR must be understood in the context of de-\nvelopment in related ﬁelds such as Information Extraction, Text\nCategorization and Automatic Summarization as also NLP tech-\nnologies such as WordNets, stemming algorithms, partial parsing\nsystems and WSD. With the rapid developments in each of these\nﬁelds and increasing synergy between the various areas, we can ex-\npect IR systems to become much more sophisticated in the years\nto come.\nDocument in Indian languages have slowly started growing.\nThere are still some teething problems with regard to non-standard\nencoding schemes. We can hope that the Indian language content\nwill grow fast in the near future. The need for indexing and search-\ning pages in Indian languages will grow. The tools and techniques\nwhich have been been ﬁne tuned for English do not necessarily\nwork well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-1-subsection-5",
                            "title": "Linguistics versus NLP",
                            "content": "97\nhome the problem by taking a down to earth example here, the\nnature of diﬀerence in the orientation of linguistics and NLP is\nthe same at all levels of abstraction.\nNLP forces linguistic theories, models, ideas and hypotheses\nto be tested on large scale, real world data. This helps to validate\nthe theories, ﬁnd places where they fail, pick up counter examples\nand reﬁne the theory or model accordingly. The NLP approach\nalso makes available large scale data which can be used to build\ntheories and models. Manual methods are tedious, time consum-\ning and hence limited to small data sets. They are also prone to\nhuman biases and misinterpretations. Testing on large scale real\nworld data often throws out new insights.\nAny aspect of human language processing can be modeled and\ntested in NLP as long as the data and rules to handle all cases\nare covered exhaustively in detail and in precise enough terms for\na computer to follow. Priorities for development may be dictated\nby concerns of practical need etc. but there is really no area of\nlinguistics that NLP cannot touch.\nNLP attaches great importance to computational eﬃciency in\nterms of memory space and computation time.\nLinguistics at-\ntaches great importance to simplicity, elegance and economy of\nexpression but eﬃciency in terms of computational space and time\nis usually not a major concern except in certain areas such as psy-\ncholinguistics. The fundamental assumption in AI is that human\nbeings are intelligent and doing things in a simpler, more elegant,\nfaster and more eﬃcient way is an important element of human\nintelligence. In this sense, if an NLP approach shows a much more\neﬃcient method of solving a particular problem, linguists cannot\naﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It\nguages express more of syntax in morphology than other\nlanguages, e.g., verb arguments are incorporated into the\nverb.\nThis classiﬁcation is quite artiﬁcial.\nReal languages rarely\nfall cleanly into one of the above classes, e.g., even Mandarin\n2.2. COMPUTATIONAL LINGUISTICS\n137\nhas a few suﬃxes. Dravidian languages are inﬂectional as well\nas agglutinative. Moreover, this classiﬁcation mixes the aspect of\nwhat is expressed morphologically and the means for expressing\nit.\nThere is a lot more to it than meets the eye:\nBy now you must have understood that even a simple looking\ntask such as breaking sentences into words is really a very complex\ntask. It is easy to build demonstration systems to give a false\nimpression of progress and success. You might have heard people\nclaim that they have solved all problems. Researchers must learn\nto see through the hype and propaganda and get straight to the\nground realities. NLP is not easy. NLP should not be compared\nwith telephones or cars. NLP requires working with meanings,\nNLP requires working with complex issues of human cognition.\nNLP can only be compared with other similar tasks that are also\nlinked to human cognition. Have we been able to understand how\nwe learn? Have we been able to understand how we understand\nspeech? Have we been able to understand how we can recognize\na person by his/her voice, hand-writing or a momentary glance\nat his/her face? May be but in a very limited way. NLP is no\ndiﬀerent. Fantastic results should not be expected. Do not fall\nfor false claims.\nIt is also not a very good idea for every young researcher\nto jump into machine translation, information extraction, speech\nrecognition or other such big tasks. We need to do a lot of ground\nwork before we can see real successes in such large tasks. The\ncomputational dictionaries and morphological analyzers we have\ntoday for Indian languages are all far from perfect, far from ade-\nquate. It takes many years of real hard work to prepare a good\nguage itself can be acquired, reﬁned and perfected as we keep\nanalyzing natural language utterances. There are signiﬁcant dif-\nferences between ﬁrst language acquisition and second language\nacquisition. Learning language by being taught is diﬀerent from\nlearning language by merely interacting with the community of\nspeakers of that language. Natural language acquisition deals with\nall forms of language learning.\nThe aim in all cases is to build computational models - models\nthat are amenable for detailed inspection, models that can be\ntested on real life data. The models need to be both exhaustive\nand very detailed and precise. Samples are not suﬃcient.\nAlthough understanding, generation and acquisition have all\nbeen the primary goals of NLP, we ﬁnd that over the last 50 years\nor so maximum attention has been given to natural language un-\nderstanding, generation has received relatively less attention and\nlanguage learning has not received much attention at all. In fact\nan analysis view point is presumed in many cases by default. Thus\nwhile talking about syntax, NLP researchers tend to take the syn-\ntactic analysis view point rather than that of synthesis or gener-\nation. Linguists, on the other hand, are more used to thinking\nof grammars as abstract characterizations of the constraints of a\nlanguage for producing all and only valid structures.\nNLP involves a deep understanding of human cognition and\nintelligence.\nFor the sake of convenience, the language faculty\nis often studied independently of other cognitive tasks but we\nmust remember that this is only a restricted view taken for conve-\nnience and not the complete and accurate picture in itself. NLP is\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\n88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3\nMorphology . . . . . . . . . . . . . . . 120\n2.2.4\nPOS Tagging . . . . . . . . . . . . . . 162\n2.2.5\nSyntax: Grammars and Parsers . . . . 166\n2.2.6\nSemantics . . . . . . . . . . . . . . . . 215\n2.2.7\nPragmatics . . . . . . . . . . . . . . . 232\n2.2.8\nOther Areas of Linguistics . . . . . . . 232\n2.3\nStatistical Approaches . . . . . . . . . . . . . 233\n2.3.1\nCorpora . . . . . . . . . . . . . . . . . 235\n2.3.2\nStatistical Approaches to Language . . 241\n2.3.3\nMachine Learning\n. . . . . . . . . . . 245\n2.4\nIndian Language Technologies . . . . . . . . . 261\n2.4.1\nThe Text Processing Environment: . . 269\n2.4.2\nThe Alphabet . . . . . . . . . . . . . . 271\n2.4.3\nThe Script Grammar . . . . . . . . . . 273\n2.4.4\nFonts, Glyphs and Encoding Standards 277\n2.4.5\nCharacter Encoding Standards . . . . 281\n2.4.6\nRomanization . . . . . . . . . . . . . . 301\n2.4.7\nSpell Checkers\n. . . . . . . . . . . . . 304\n2.4.8\nOptical Character Recognition\n. . . . 311\n2.4.9\nLanguage Identiﬁcation\n. . . . . . . . 316\n2.4.10 Others Technologies for Indian Lan-\nguages . . . . . . . . . . . . . . . . . . 321\n2.4.11 NLP and Sanskrit\n. . . . . . . . . . . 321\n2.4.12 Epilogue . . . . . . . . . . . . . . . . . 324\n2.5\nConclusions . . . . . . . . . . . . . . . . . . . 325\n3\nAdvances in IR\n327\n3.1\nHistory of IR . . . . . . . . . . . . . . . . . . 327\n3.1.1\nFrom The Library to the Internet . . . 329\nxiv\nCONTENTS\n3.2\nBasic IR Models\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nof collection. A striking example of this is the WordNet - a col-\nlection of English words grouped based on semantic similarities\nand interconnected in various semantic dimensions of relation-\nships. WordNet was initially developed with psychology in mind\nbut it has been used very extensively in a variety of NLP tasks\nand applications. It is therefore important to be careful and be\n236\nCHAPTER 2. FOUNDATIONS OF NLP\nas general and open as possible and include as many dimensions\nof variability as practically possible, while developing a corpus.\nCorpus development is a time consuming and costly process and\nevery care should be taken to ensure that what is done once can\nbe used for many diﬀerent applications.\nLet us get back to the question of how large is a large cor-\npus. We have seen the growth rate curves for types against to-\nkens for several major Indian languages - refer to Figure 2.5 above.\nThe distinction between Dravidian languages and Indo-Aryan lan-\nguages is striking in this ﬁgure - there are many more word forms\n(types) in Dravidian languages than in the other Indian languages.\nWhile 150,000 to 200,000 word types should be giving a very good\ncoverage for northern languages, Dravidian languages such as Tel-\nugu spoken mainly in the southern parts of India require a much\nlarger number of word forms. And, more importantly, the avail-\nable corpus is not suﬃcient even to get a clear idea of how many\nwords are there in the language. The morphology of these lan-\nguages is so rich, no one so far has an exact idea how many dif-\nferent word forms are there in the language. We have mentioned\nearlier that there can be as many as 1,80,000 diﬀerent types arising\nfrom a single verb root Telugu.\nWhat this shows is that techniques based on these corpora\nwhich work well for Indo-Aryan languages may not be applica-\nble to Dravidian languages. For example, it would be possible to\nsimply list all forms of all words and use this for dictionary based",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-2",
                    "title": "Computational Linguistics",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-2-section-2-subsection-1",
                            "title": "Dictionaries",
                            "content": "mation and therefore, they have a key role to play in NLP. Also,\ntheoretical linguistics has come to assign an increasingly central\n100\nCHAPTER 2. FOUNDATIONS OF NLP\nrole to the lexicon.\nLongman’s dictionary of contemporary English (1987) deﬁnes\na dictionary as ‘a book that gives a list of words in alphabetical\norder with their meanings in the same or another language and\nusually with their pronunciations’. According to Random House\nDictionary of the English Language (College Edition), a dictio-\nnary is ‘a book containing a selection of the words of a language\nusually arranged alphabetically, giving information about their\nmeanings, pronunciations, etymologies etc.; a lexicon’. These def-\ninitions suggest that a dictionary is essentially a list of words of\na language, typically sorted alphabetically, giving the associated\nmeanings. Pronunciation, part-of-speech and other grammatical\nfeatures, etymology, usage, examples, pictures and so on may also\nbe optionally provided.\nDictionaries can be built for speciﬁc purposes and the con-\ntents and organization would vary accordingly. Thus we can talk\nof a dictionary meant for second language learners, a dictionary\nof technical terms in a speciﬁed domain, or a dictionary for peda-\ngogical purposes. We can have a dictionary of phrasal verbs and\na dictionary of idioms. A children’s dictionary may contain only\nthe most frequent and basic words. The term ’lexicon’, on the\nother hand, is used in a technical sense in linguistics. A lexicon\ncontains a complete inventory of all the words in a language along\nwith associated information conforming to the speciﬁcations of a\ngiven theoretical framework. This information would be organized\nas required by the theory. The term lexicon is also used in NLP to\nstand for a dictionary considered to be one of the components of\nan NLP system. Although a lexicon is diﬀerent from a dictionary,\nthese two terms are sometimes used interchangeably.\nElectronic Dictionaries\nWeb enabled dictionaries require server side and/or client side\nscripts to interact with the users, search the dictionary, format\nthe search results for display etc. Eﬃcient algorithms exist for\nfast searching.\nThere are tools for veriﬁcation and validation.\nThese tools make it possible to develop, maintain and use large\ndictionaries and other lexical resources eﬀectively and eﬃciently.\nWe have seen printed dictionaries that include line drawing\nand maps but electronic dictionaries may be enriched in many\nways that printed dictionaries do not allow. We can include mono-\ntone or colour photographs as also animations to illustrate some\naction ‘by doing and showing it’. We can incorporate speech out-\nput to illustrate correct pronunciations. We can include maps that\nyou can zoom into.\nOur focus here will be electronic dictionaries designed for NLP\napplications. There are three main issues in the design of dictio-\nnaries:\n• What kinds of words are listed in the dictionary?\n• What information should the entries provide?\n• How should that information be organized and structured?\nLet us explore each of these in turn.\n104\nCHAPTER 2. FOUNDATIONS OF NLP\nThe Contents of a Dictionary\nA dictionary consists of lexical entries and the associated infor-\nmation. What kinds of lexical entries should we include in a dic-\ntionary? What kind of grammatical, semantic or other kinds of\ninformation should be include for these lexical entries?\nLet us\nlook at these issues one by one.\nThe content of a dictionary and its organization critically de-\npend on the ways in which NLP applications use the dictionary.\nSpell checkers often use merely a list of words. For a syntactic\nparsing program, the grammatical category and other grammati-\ncal features of words may be suﬃcient and the dictionary may not\neven contain information on pronunciation, etymology or mean-\ning descriptions. However, for a full-ﬂedged natural language un-\nderstanding system, meanings and even usage may be extremely\nalmost every activity in computational linguistics and NLP - word\nprocessing and text critiquing systems, spelling error detection\nand correction, grammar checking, oﬃce automation, morpholog-\nical analysis and synthesis, parsing and generation, machine trans-\nlation, question answering systems, story understanding, natural\nlanguage interfaces to databases, computer aided instruction, in-\nformation retrieval, speech synthesis, speech recognition, auto-\nmatic indexing and abstracting, concordance and other statistical\nanalyses, vocabulary studies, stylistics, psycholinguistic studies,\ntaxonomical studies etc.\nWe have seen that electronic dictionaries can be very useful\nfor people as also for NLP applications. Yet another dimension\nof the relationship between dictionaries and computers is the use\nof computers in developing dictionaries. Dictionary development\nis a huge and complex task requiring great skill and expertise on\nthe part of the lexicographer. Classical dictionaries have taken\ndecades to develop. At one point of time words used to be writ-\nten down on cards, cards sorted manually, then word frequencies\ncounted manually and duplicate cards removed and so on. Today\ncomputers provide assistance at practically every stage of dictio-\nnary development. To give an idea of this, we may start with large\nand representative collections of electronic texts, extract words\nand perform a type-token analysis (each distinct word form is a\ntype and each occurrence of a type is a token), perform morpho-\nlogical analysis to extract root words, select words, identify parts\nof speech (POS) using POS tagged corpora or through morpholog-\nical analysis, use a KWIC (Key Word in Context) Concordance\nprogram to extract sentences containing a given key word and\ndecide meanings etc.\nfrom these sentences, select examples of\nusage, and format the entries in the dictionary as required. All\nthese steps can be done semi-automatically.\nThe machine per-\nas required by the theory. The term lexicon is also used in NLP to\nstand for a dictionary considered to be one of the components of\nan NLP system. Although a lexicon is diﬀerent from a dictionary,\nthese two terms are sometimes used interchangeably.\nElectronic Dictionaries\nDictionaries in the form of printed books are not of much direct\nuse in NLP. Only dictionaries in electronic form can be used by\ncomputer programs. Computer programs can be written to search\nand automatically process such electronic dictionaries. Programs\nalso exist for assisting the very development of electronic dic-\ntionaries. Electronic dictionaries greatly enhance the ﬂexibility,\nconvenience and speed of access by human beings too. Under-\nstandably, therefore, the development of large computerized lex-\n2.2. COMPUTATIONAL LINGUISTICS\n101\nical knowledge bases has emerged as probably the most urgent,\nexpensive, and time consuming task facing linguistics and NLP\ntoday. And this is all the more relevant and exigent in the Indian\ncontext where, what is already available is very little compared to\nwhat is needed.\nOne might think of electronic dictionaries as simply electronic\ncounterparts of printed dictionaries.\nThe same information is\nstored in magnetic medium in computer processable forms and\nused in a similar way. People ’look up’ the dictionary by making\nsuitable requests to a computer program and in turn get the pro-\ngram to display on the screen or print onto paper the information\nrequested by the user. The computer can hold huge amounts of\ninformation and can turn the pages for you very fast. While all\nthis is true, electronic dictionaries are in fact much more powerful.\nThe following paragraphs show how.\nWe go to a dictionary to look up the meaning of a word or\nits correct spelling or pronunciation. Sometimes we may be inter-\nested in knowing more about the grammatical properties, usage,\netymology etc.\nWe can use electronic dictionaries for all these\nThe following paragraphs show how.\nWe go to a dictionary to look up the meaning of a word or\nits correct spelling or pronunciation. Sometimes we may be inter-\nested in knowing more about the grammatical properties, usage,\netymology etc.\nWe can use electronic dictionaries for all these\npurposes. We can also do many more things that would be diﬃ-\ncult with printed dictionaries. How do you extract all words that\nare both nouns and verbs? How do you make a list of 5 letter\nwords? Or bisyllabic words? Or words with a Sanskrit origin?\nWith printed dictionaries one will have no option than to scan\nthrough the whole dictionary manually, a task that can be ex-\ntremely tedious and time consuming. With an electronic lexical\ndatabase, one could script a few lines of code and get all these\ndone in seconds.\nA researcher interested in morphology may like to extract and\nor reorder the words in the dictionary in the right to left (re-\nversed) alphabetical order so that for example, all ’-ing’ ending\nwords would get grouped together and words ending with ’-ed’\nwould form another cluster.\nThis can be done very easily and\nvery fast on a computer. Electronic lexical databases, properly\ndesigned and organized, can also be easily ’reconﬁgured’ to suit\ndiﬀerent needs. For example, one can create graded dictionaries\nfor children, with vocabularies limited to what can be consid-\nered adequate for various age groups. Clearly, large scale lexical\ndatabases are extremely important and useful linguistic resources\nfor any language.\n102\nCHAPTER 2. FOUNDATIONS OF NLP\nElectronic dictionaries are also directly usable by computer\nprograms. Electronic dictionaries form an integral component of\nalmost every activity in computational linguistics and NLP - word\nprocessing and text critiquing systems, spelling error detection\nand correction, grammar checking, oﬃce automation, morpholog-\nical analysis and synthesis, parsing and generation, machine trans-",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-2",
                            "title": "Thesauri and WordNets",
                            "content": "glish words. This was a natural choice since the starting point\nwas an English-Kannada dictionary.\nClearly, the words we get from this thesaurus are not always\nexactly synonymous. The whole idea of a thesaurus is to provide a\ntool to the user to explore the semantic space of words by oﬀering\nterms that are related in some way to the given word. Users are\noften not looking for exact synonyms, they are in fact looking for\nterms that that ﬁt the particular usage on hand.\nWordNet\nWordNet is an on-line lexical reference system whose design is\ninspired by psycholinguistic theories of human lexical memory.\nEnglish nouns, verbs, adjectives and adverbs are organized into\nsynonym sets, each representing one underlying lexical concept.\nDiﬀerent relations link the synonym sets.\nWordNet was devel-\noped by the Cognitive Science Laboratory at Princeton Univer-\nsity. Similar eﬀorts are now going on for many other languages of\nthe world.\nWordNet groups together words into synonym sets called syn-\nsets. Various types of relationships between synsets are depicted.\nSome of these relationships are:\n• Hypernym: The generic term used to designate a whole class\nof speciﬁc instances. Y is a hypernym of X if X is a (kind\nof) Y. A rose is a kind of a ﬂower.\n• Hyponym: The speciﬁc term used to designate a member of\na class. X is a hyponym of Y if X is a (kind of) Y.\n• Meronym: The name of a constituent part of, the substance\nof, or a member of something. X is a meronym of Y if X is\na part of Y. A ﬁnger is a part of a hand.\n• Holonym: The name of the whole of which the meronym\nnames a part. Y is a holonym of X if X is a part of Y.\n• Troponym: A verb expressing a speciﬁc manner elaboration\nof another verb. X is a troponym of Y if X is to Y in some\nmanner. Limping is a kind of walking.\nHere is the search result for the word “table” from WordNet:\n2.2. COMPUTATIONAL LINGUISTICS\n119\nThe noun ”table” has 6 senses in WordNet.\n1. table, tabular array - (a set of data arranged in rows\ntion and display of the annotation results.\nFrameNet is in its name obviously modeled after WordNet,\nand the intention has always been for FrameNet to be both a\ndictionary and a thesaurus, much in the sense that WordNet is.\nA major diﬀerence is that the FrameNet database is founded on\ncorpus attestations, and that it includes examples, taken from\nthe corpus, of each sense and each grammatical variant of each\n120\nCHAPTER 2. FOUNDATIONS OF NLP\nword in each sense, going far beyond the ‘Someone\ns some-\nthing’ patterns found in WordNet. It also diﬀers from WordNet\nin recognizing relationships among words in a single frame that\nare of diﬀerent parts of speech.\nOntology\nThe term ontology is used in diﬀerent disciplines with somewhat\ndiﬀerent connotations. Philosophers have been using this term for\na long time, and AI researchers have borrowed and used the term\nin a somewhat diﬀerent sense. Ontologies, as used in NLP and\nrelated disciplines is simply an organization of concepts and their\ninter-relationships. The crucial point is that an ontology is deﬁned\nin terms of concepts, not words of a particular language. Concepts\nare generally independent of language and hence a given ontology\nis expected to be useful for various languages.\nOftentimes the\nconcepts are represented using words of a particular language but\nthey are concepts, not words nonetheless.\nThe organization of\nwords in a hierarchical structure in the Roget’s thesaurus could\nbe considered an ontology.\nOne may think of a general ontology of the whole world or\nsmaller, specialized ontologies that are relevant for particular do-\nmains of applications. There is no single perfect way of describing\nthe world, or a sub-world for that matter. There is thus always a\nquestion of which way of structuring the concepts is the best? For\nNLP applications, the criteria would naturally be in terms of the\napplicability and suitability for speciﬁc applications. A structure\nperformance can be obtained if suﬃcient training data is avail-\nable. Sense tagged data, however, is not readily available. Thus\nautomatic generation of sense tagged data has become a major\nquestion.\nResolution of Anaphora\nLook the following examples (from Graeme Hirst):\n1a. Bill Thought that John would laugh at him\n2.2. COMPUTATIONAL LINGUISTICS\n229\n1b. Bill Thought that John would laugh at himself\n2a. When Sue went to Nadia’s house for dinner,\nshe served sukiyaki augratin\n2b. When Sue went to Nadia’s house for dinner,\nshe ate sukiyaki augratin\n3. Give the bananas to the monkeys\nalthough they are not ripe\nbecause they are hungry\n4. Smoking gives one cancer\nWho is “him” in 1a above and who is “himself” in 1b? Who\nserved sukiyaki augratin and who ate? What are the two “they”s\nin 3 above? Who is this “one” in the last example? Words like\n“him, himself, they, one” refer to some body or something men-\ntioned before.\nThey are references to objects found elsewhere\nin the text. The objects referred to are called referents or an-\ntecedents. Resolving references, that is identifying what refers to\nwhat, is an important problem in linguistics and NLP.\nBecause we ﬁnd it too monotonous and boring to say some-\nthing like “cows give milk. Cows have four legs and two horns.\nCows eat grass. Cows are domestic animals. Cows ...” we say\n“cow” once or twice and then onwards we start referring to this\ncow as “it”. Pronouns have the main purpose of standing in place\nof nouns and referring to them. Pronouns are abbreviated forms of\nnouns, abbreviated in terms of their information content, that is.\nThus “he” refers to any single male human being. This pronoun\ncontains this much of information but the last piece identifying\nthe exact person is missing. In fact any situation where there is\nan abbreviation of information, even ellipses, can be considered\nas a case of reference.\nModern linguistic theories provide a set of principles that ap-\npronunciation, meanings, usage, etymology and other such pieces\nof information associated with that word. A thesaurus, on the\nother hand, is organized in terms of an ontology - a hierarchy of\nconcepts, and the words are structured into groups that convey\na speciﬁc meaning.\nThe diﬀerence between a dictionary and a\nthesaurus, therefore, is more of structure and organization rather\nthan that of content. In other words, dictionaries are typically\nsemasiological in direction, i.e., name to notion or form to content,\nwhereas thesauri are typically onomasiological i.e., notion to name\nor content to form.\nBoth the dictionary and the thesaurus contain words of a given\nlanguage and the meanings. Given this, it makes a lot of sense\nto consider a dictionary and a thesaurus as simply two diﬀerent\nviews of the same data, rather than as two entirely diﬀerent enti-\nties. Why should we store the same words, once structured as a\ndictionary, and then again structured in a diﬀerent way as a the-\nsaurus? Especially when we are talking of electronic dictionaries\nand thesauri, it appears to be a good idea to store the words only\nonce and provide two diﬀerent indexing mechanisms, one to use\nthe words as a dictionary, and another to use the same words as\na thesaurus. We have seen that amarakoos’a, the Sanskrit dictio-\nnary, is actually organized in terms of semantic groups. All you\nneed is an eﬃcient indexing mechanism so that you can also look\nup words based on spellings rather than meaning classes.\nConstructing a thesaurus is not an easy job. It requires, like\nother lexicographic works, an in-depth and thorough understand-\ning of words, their meanings and behavior. It also requires a great\ndeal of time and eﬀort even with all the modern information tech-\nnology tools we have today. Developing a suitable ontology is itself\na huge task. Whether we see the world through the language we\nuse or we look at the language in terms of our real world experi-\ngrammatical categories or diﬀerent senses of a given word. The\nalgorithm can be implemented eﬃciently using suitable data struc-\ntures and hashing techniques. It takes only a few minutes to gen-\nerate the complete thesaurus on a desktop personal computer.\nThe thesaurus so generated may then be enhanced, enriched and\nreﬁned further.\nWe show below examples from a thesaurus for Kannada gen-\nerated automatically from an available English-Kannada dictio-\nnary.\nThe dictionary itself was developed for the purposes of\nmachine aided translation and as such gave more or less substi-\ntutable equivalents. Several possible equivalents were listed in the\ndictionary to give maximum choice for the users of the machine\ntranslation system. These semantically related words have been\ngrouped together to construct the thesaurus. No in-depth analy-\nsis has been performed, only simple re-grouping of words.\nnooDu :\nSynset\nCategory\nSense\nparis’iilisu\nv\nLOOK\ndiTTisu\nv\nLOOK\nkaaNu\nv\nLOOK\ntooru\nv\nLOOK\n2.2. COMPUTATIONAL LINGUISTICS\n117\nmane :\nSynset\nCategory\nSense\nkaTTaDa\nn\nBUILDING\nsadana\nn\nHOUSE\ngRha\nn\nHOUSE\nnivaasa\nn\nRESIDENCE\nvaasasthaana\nn\nRESIDENCE\nvaasa\nn\nHABITATION\niruvu\nn\nHABITATION\nbiiDu\nn\nHABITATION\nvasati\nn\nHABITATION\ncikka :\nSynset\nCategory\nSense\nsvalpa\na\nLITTLE\nkoMca\na\nLITTLE\ntusa\na\nLITTLE\ngiDDa\na\nSHORT\nkuLLa\na\nSHORT\nmooTu\na\nSHORT\nsaNNa\na\nSMALL\npuTTa\na\nSMALL\nkiriya\na\nSMALL\nkSudra\na\nSMALL\npuTaaNi\na\nTINY\nkaDime\na\nLESS\neLeya\na\nYOUNG\nhareyada\na\nYOUNG\nyauvanaavastheya\na\nYOUNG\nyuvakanaada\na\nYOUNG\nFIG 2.2 Examples from a Machine Constructed Kannada\nThesaurus\n118\nCHAPTER 2. FOUNDATIONS OF NLP\nHere the diﬀerent meanings and usages are indicated by En-\nglish words. This was a natural choice since the starting point\nwas an English-Kannada dictionary.\nClearly, the words we get from this thesaurus are not always\nexactly synonymous. The whole idea of a thesaurus is to provide a\ntool to the user to explore the semantic space of words by oﬀering",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-3",
                            "title": "Morphology",
                            "content": "that there exist simple known rules to handle all kinds of words\nthat we come across in various applications. A waiter in a hotel\nis not just any person who waits (customers wait for food etc.\ntoo). Cooker is not one who cooks but the vessel we use for cook-\ning. Analysis of words is not always straight forward. Synthesis is\nalso very complex. The noun form of create is creation but noun\nform of state is statement, not station. Computational models\nof morphology aim to produce exhaustive yet simple, elegant and\ncomputationally eﬃcient solutions to morphological analysis and\ngeneration. Computational models do not guarantee correct inter-\npretations of meanings. Instead the aim is to produce appropriate\n2.2. COMPUTATIONAL LINGUISTICS\n153\nanalyses for all words of a given language.\nFrom one point of view, morphology is just a question of stor-\ning all word forms versus deriving some from a smaller, more ba-\nsic set of stored words. Computationally, this translates into the\nusual trade-oﬀbetween time and space. It takes space to store\nall forms of all words and it takes time to search for a given word\nin a large list. It also takes some space to store the rules of mor-\nphology and more importantly, it takes time to select and apply\nthe rules. A practical solution has to ﬁnd a good compromise.\nThe productivity and uniformity of rules is the key factor.\nIn\nthe case of English, where there about 300 aﬃxes for derivation,\none widely used approach is to simply store all derived forms and\nhandle only the few cases of inﬂection through rules. There are\nmany idiosyncrasies in the rules of derivational morphology and\nit is simpler to simply store all the derived forms. Thus friend,\nfriendly and friendliness are all directly stored in the dictionary\nbut friends is not stored.\nSeveral computational models have also been proposed for\nmorphological analysis and generation.\nIn the next section we\nshall brieﬂy sketch one such model.\nMorphology of Indian Languages\nthe world, or a sub-world for that matter. There is thus always a\nquestion of which way of structuring the concepts is the best? For\nNLP applications, the criteria would naturally be in terms of the\napplicability and suitability for speciﬁc applications. A structure\nrestricts and limits the focus of attention and guides our search.\nIf the structure is well designed, it takes us to the right places\nwith minimal eﬀort and thus useful. If the structure is not good,\nit may lead us away from the goal, confuse us or prevent us from\nreaching the goal quickly. Thus design of ontologies is a complex\nmix of science and art.\n2.2.3\nMorphology\nMorphology is the study of structure, properties and formation\nof words. We are all familiar with the notion of a word - apple,\neat, good are English words.\nWe can similarly name words in\nother languages. Words form one of the most fundamental units\n2.2. COMPUTATIONAL LINGUISTICS\n121\nof linguistic structure. As children we learnt to speak single words\nand as we grew up picked up thousands of words and a variety of\ninformation associated with those words. The list of all the words\nof a given language is referred to as its lexicon.\nWhen we hear somebody speak our language, we can recog-\nnize the words as they speak. When we read printed text, we\ncan see the words neatly laid out on paper separated by spaces.\nExistence of words seems obvious. Yet, if you listen to a language\nthat you do not know, you will hear only a continuous blur of\nsounds and you ﬁnd it hard to recognize individual words. You\ntend to think the speaker is speaking too fast. Recognizing indi-\nvidual words from a continuous stream of sounds when somebody\nspeaks is an extremely complex task.\nThe ability to recognize\nwords constitutes a major part of language comprehension. When\nyou “know” a language, you have mastered the art of recogniz-\ning words without eﬀort. You will have in fact understood many\nproperties associated with words albeit unconsciously.\nWhat is a word?\npectual auxiliaries, negation, causation etc. Exhaustive studies\nhave not yet been made for many of our languages. Some experts\nhave estimated that there can be as many as 1,80,000 diﬀerent\nword forms that can be obtained from a single verb root in Tel-\nugu!\nMeaning of Complex Words\nThe main justiﬁcation for analyzing the structure of words in\nterms of the constituent morphemes comes from the assumption\nthat the meaning of the word as a whole can be understood in\nterms of the meaning of its parts. Let us examine to what ex-\ntent this assumption of compositionality of meaning is actually\nvalid. Curable means able to be cured and inﬂatable means able\nto be inﬂated. Things are not always so simple. If a theory is\nquestionable, it is not just that questions can be asked about that\ntheory. Questions can anyway be asked about any theory. If you\nsay a theory is questionable, you mean it is dubious and suspect.\nIf a bill is payable by so and so date, it means you better pay it\nbefore the speciﬁed data. If a book is readable, it is well writ-\nten. Morphology can only point to the general nature of semantic\n2.2. COMPUTATIONAL LINGUISTICS\n151\nrelationships between a complex word and its constituent parts.\nMorphology is only a guide, not a complete solution. We cannot\nexpect to write down a set of rules that can help us to correctly\nanalyze the meaning of any given word. It is not easy to write\ncomputer programs to analyze the meanings of words. It is not\neasy to automate language understanding.\nNew Meanings for Old Words\nNative Speakers often take an existing word and extend its mean-\ning in a recognizable way. Although no new word is added, the\nlanguage is enriched just the same. For example, many terms used\nin ocean navigation have been extended to the realm of space ex-\nploration - ship, docking, navigation, ﬂoating, captain, crew, deck,\netc. Although the situations are drastically diﬀerent, people see\nenough similarities to make analogical or metaphorical extension.\nanalysis is produced and a possible root is hypothesized. Hence\nthe process component needs parallel procedures for making and\nbreaking saMdhi. On the other hand, the network component is\ninherently bidirectional.\nLemmatization and Stemming\nWe have seen that morphology of Indian languages is quite com-\nplex. It has not been possible so far to develop high performance\nmorphological analyzers and generators for many of our languages.\nSystems developed so far are far from adequate. Even simple ap-\nplications such as spelling error detection and correction require\nthe full power of morphological analysis and generation. Is there\na way out?\nFor many applications, it is the root that contains the maxi-\nmum useful semantic content and the rest of morphemes indicate\ngrammatical features etc. which are relatively less relevant. For\nexample, an Information Retrieval system would be substantially\nbeneﬁtted if all words could be replaced with their roots. This\nwould reduce the sparseness introduced by the variations due to\nmorphology. The crucial part is the meaning of the root.\nLemmatization is the process of extracting the root of a given\nword, without necessarily performing full morphological analysis.\nLemmatization involves the reduction of corpus words to their re-\nspective headwords (lemmas). For example, the inﬂected forms\n“speaks” and “speaking” resulting from a combination of a single\nroot with two diﬀerent suﬃxes (-s and -ing) are brought back to\nthe lemma “speak”. Lemmatization is a process wherein the in-\nﬂectional and variant forms of a word are reduced to their lemma\n- their base form, or dictionary look-up form. When one lemma-\ntizes a text, one replaces each individual word in that text with its\nlemma. A text in English which has been lemmatized, then, would\ncontain all forms of a verb represented by its inﬁnitive, all forms\n2.2. COMPUTATIONAL LINGUISTICS\n161\nof a noun by its nominative singular, and so forth. In languages\nT Maybury.\nParts of the section on morphology dealing\nwith English morphology have been inﬂuenced by the book\nby Andrian Akmajian, Richard A Demers and Robert M\nHarnish. Acknowledgements and credits have been included\nat appropriate places in the text.\nMy wife Nalini and my daughter Arabhi have put up\nwith many hardships and extended their fullest cooperation\nwhile I was away from home writing the book. I do not have\nx\nwords to thank my father Kavi Krishna Murthy who initi-\nated me into Sanskrit and taught me the joy of working with\nlanguages. Nor do I have words to thank my mother who\nhas in a sense taught me everything I know, not by teaching\nor preaching but by making me think.\nKavi Narayana Murthy\nContents\nPreface\nii\nForeword\nvi\n1\nThe Information Age\n1\n1.1\nThe Information Age . . . . . . . . . . . . . .\n1\n1.2\nTechnology for Accessing Info . . . . . . . . .\n3\n1.3\nQuestion Answering Systems\n. . . . . . . . .\n7\n1.3.1\nELIZA - The Rogerian Therapist . . .\n9\n1.3.2\nEarly NLP Systems\n. . . . . . . . . .\n13\n1.3.3\nFoundations of Story Understanding .\n21\n1.3.4\nIn-Depth Understanding . . . . . . . .\n23\n1.3.5\nTuring Test . . . . . . . . . . . . . . .\n27\n1.4\nInformation Retrieval\n. . . . . . . . . . . . .\n28\n1.4.1\nIR Deﬁned\n. . . . . . . . . . . . . . .\n29\n1.4.2\nDocuments as Bags-of-Words . . . . .\n30\n1.4.3\nThe Vector Space Model . . . . . . . .\n30\n1.4.4\nPerformance Evaluation . . . . . . . .\n31\n1.4.5\nMeasuring Relevance . . . . . . . . . .\n32\n1.4.6\nChallenges in Information Retrieval\n.\n33\n1.5\nInformation Extraction . . . . . . . . . . . . .\n35\n1.5.1\nWhat is Information Extraction? . . .\n35\n1.5.2\nInformation Extraction Tasks . . . . .\n36\n1.5.3\nArchitecture of an IE System . . . . .\n38\n1.6\nAutomatic Summarization . . . . . . . . . . .\n39\nxi\nxii\nCONTENTS\n1.6.1\nWhy Summarization?\n. . . . . . . . .\n39\n1.6.2\nApproaches to Automatic Summariza-\ntion\n. . . . . . . . . . . . . . . . . . .\n41\n1.6.3\nSummarization in Relation to Infor-\nmation Extraction . . . . . . . . . . .\n44\n1.6.4",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-4",
                            "title": "POS Tagging",
                            "content": "POS tagging is the process of taking plain text as input and\nautomatically marking or tagging each word with the grammati-\ncal category most appropriate to that word in the given context.\nThe following is an example of a piece of BNC (British National\nCorpus) text with c5 part-of-speech markers (taken from Captain\nPugwash and the Huge Reward):\n<s c=\"0000002 002\" n=00001>\nWhen<AVQ-CJS> Captain<NP0> Pugwash<NP0> retires<VVZ>\nfrom<PRP>\nactive<AJ0>\npiracy<NN1>\nhe<PNP> is<VBZ>\namazed<AJ0-VVN> and<CJC>\ndelighted<AJ0-VVN> to<TO0>\nbe<VBI>\noffered<VVN>\na<AT0> Huge<AJ0>\nReward<NN1>\nfor<PRP> what<DTQ> seems<VVZ> to<TO0> be<VBI> a<AT0>\nsimple<AJ0> task<NN1>.<PUN>\n<s c=\"0000005 022\" n=00002>\nLittle<DT0> does<VDZ> he<PNP> realise<VVI> what<DTQ>\nvillainy<NN1>\nand<CJC> treachery<NN1> lurk<NN1-VVB>\nin<PRP>\nthe<AT0>\nlittle<AJ0>\ntown<NN1>\nof<PRF>\nSinkport<NN1-NP0>,<PUN>\nor<CJC>\nwhat<DTQ>\na<AT0>\nhideous<AJ0> fate<NN1>\nmay<VM0> await<VVI> him<PNP>\nthere<AV0>.<PUN>\nPOS taggers are far from perfect. The Claws tagger used to\ntag the BNC corpus gives two-tag combinations such as AVQ-\nCJS when it is not quite sure which of these two is the correct\none. All it says is that the ﬁrst tag in a two-tag combination is\ngenerally more likely to be correct than the second. If the tagger\nis unable to do even that, it issues an UNC (unknown) tag. POS\ntaggers may not be able to distinctions between homonymy and\npolysemy. POS taggers generally may have diﬃculties in handling\ncompounds, phrases etc. Taggers are not perfect and there may\nbe tagging errors. Hence automatically tagged corpora must be\nused with care.\nDictionaries give us allowed sets of tags for words. Choos-\ning one of the possible tags has to be done based on context. In\n164\nCHAPTER 2. FOUNDATIONS OF NLP\nEnglish words that follow the word “the” may be expected to be\nnouns or adjectives. Context imposes restrictions and we must ex-\nploit these restrictions to tag each word with the most likely tag.\nsequences with respect to given models and for determining opti-\nmal state sequences. See section 2.3.3 for a brief introduction to\nHMMs.\nIn the HMM formulation for POS tagging, the sequence of\nwords in a given text would be considered as the observable states\nof the HMM and the sequence of associated POS tags would be\nconsidered to be the hidden states.\nThere is an algorithm to\nﬁnd the optimal state sequence called Viterbi algorithm.\nThis\nalgorithm gives us the best possible tag sequence for a given word\nsequence. Note that whole sentences are tagged in one go, we do\nnot tag one word at a time.\nHMM based POS-taggers have been used quite successfully for\nEnglish and other positional languages. Practical taggers use more\nsophisticated models. For example, tri-tag based models would\n2.2. COMPUTATIONAL LINGUISTICS\n165\nconsider three-tag sequences as states and consider sequences of\nsuch states.\nA HMM model is built from training data.\nTraining data\nhere would consist of dependable POS-tagged text corpora. How\nwould one get a POS-tagged corpus in the ﬁrst place? We may\nhave to tag a small corpus manually. Using this small manually\ntagged corpus as a training corpus, a HMM based POS-tagger can\nbe built. This tagger is run on a larger corpus to produce a larger\nPOS-tagged corpus. Since the tagger was built from small data,\nits performance will perhaps be not very good. Manual checking\nmay be required to obtain accurately tagged corpus that can then\nbe used as training data to build even better taggers. This sort\nof boot-strapping is common to many statistical approaches.\nSuppose we tag every word blindly with the most frequent tag\nfor that word irrespective of the context in which it has occurred.\nFor English you will ﬁnd that the tagging so generated would be\nabout 85% correct! It appears that POS tagging is an easy job.\nThis is just the nature of the language and the task and this level\nof performance can be taken as a base line. Performance of taggers\nFor English you will ﬁnd that the tagging so generated would be\nabout 85% correct! It appears that POS tagging is an easy job.\nThis is just the nature of the language and the task and this level\nof performance can be taken as a base line. Performance of taggers\nwould be judged relative to this base-line.\nWe have noted earlier that dictionaries usually simply list pos-\nsible grammatical categories without worrying about which cate-\ngory is correct for a given situation. It must also be noted that\ndictionaries usually list only the root forms of words, not all the\ninﬂected forms. Irregular forms may be listed but all the regular\ninﬂections are rarely listed. For languages such as English that\nshow extremely simple inﬂectional morphology and fairly strict\nword order, HMM models are naturally well suited.\nIndian languages, especially the Dravidian languages are char-\nacterized by a very rich system of inﬂectional morphology that is\nalso closely tied up with rich derivational morphology.\nWords\ncarry a great deal of grammatical information within themselves\nand word order is not the main channel for expressing syntac-\ntic constraints. Sanskrit takes this to the extreme - almost any\npermutation of a given sentence is grammatically valid and all per-\nmutations convey exactly the same primary meaning. Sentences\nare actually unordered sets of words, not really sequences. This\ngives tremendous amount of ﬂexibility to the composer in Sanskrit\nand poetry ﬂows out so ﬂuently and eﬀortlessly. In fact it is easier\nto write in verse form than prose in Sanskrit. A large portion of\n166\nCHAPTER 2. FOUNDATIONS OF NLP\nall works in Sanskrit is in verse form. Modern Indian language lie\nin between Sanskrit and English in this regard but perhaps much\ncloser to Sanskrit than to English. Any system that heavily de-\npends on word order would be inappropriate. Morphology should\nbe the major criterion. HMMs are perhaps not the best models\nfor Indian languages.\nin between Sanskrit and English in this regard but perhaps much\ncloser to Sanskrit than to English. Any system that heavily de-\npends on word order would be inappropriate. Morphology should\nbe the major criterion. HMMs are perhaps not the best models\nfor Indian languages.\nHMM models may not be very suitable for Indian languages.\nMorphology of Indian languages has not yet been worked out fully.\nPreliminary tag-sets have been deﬁned and used to build taggers\nfor some Indian languages but there is still a long way to go.\nThe grain size of the tag set is itself a major issue. If we include\nonly the major categories that would be too coarse and although\ntaggers may be built easily, tagged corpus so generated will be of\nvery limited use. If, on the other hand, we attempt to capture all\nthe ﬁne variations depicted in the morphology of the words, the\ntag set would become very large and POS tagging will essentially\nboil down to morphological analysis. Should we have a hundred\nthousand tags? If one were to postulate morphological analysis at\nrun time for any given application, generating and storing POS-\ntagged corpora would no longer serve any serious purpose.\nPerhaps a hierarchical design where tags are not simple atomic\nsymbols but composites indicating hierarchical reﬁnements would\nbe most appropriate for Indian languages. This would also enable\ndevelopment of more and more reﬁned tag sets and POS taggers\nin a phased manner. An an example of this concept, the tag v\nfor verb could be revised to include v-i and v-t for v-intransitive\nand v-transitive and this could further be reﬁned to show more\ndetailed sub-categorizations and so on.\n2.2.5\nSyntax: Grammars and Parsers\nLanguages exhibit complex structures and a detailed and system-\natic analysis of the structure of natural language sentences is in-\nvaluable in determining the meaning of the sentences. Form fol-\nlows function. Knowing the structure will, hopefully, help us in\nnot much is available in those genres. You will ﬁnd any amount\nof material on literature but hardly any on scientiﬁc and technical\ndomains. Thus there is a tendency to put together whatever is\n240\nCHAPTER 2. FOUNDATIONS OF NLP\navailable and call it a corpus. Such corpora may not be balanced\nand must be used with great care.\nIndian languages are characterized by rich morphology and\nrelatively free word order. Hence sequence based techniques such\nas HMMs may not be the most suitable for POS tagging of Indian\nlanguage corpora. Also, since there are so many aspects that go\ninto individual word tokens, the design of the tag-sets is itself a\nvery complex task. Too gross a classiﬁcation (such as into the\nbasic grammatical categories) may not be good enough and too\nﬁne grained a classiﬁcation would make POS tagging heavily de-\npendent on morphology. If there are 1,80,000 word forms derived\nfrom a single verb root in Telugu, should we have so many tags\nthen? That would not make much sense. Perhaps a hierarchical\ntagging scheme where we can work at various grain sizes in stages\nmay be a good idea. POS tagged corpora are not yet available in\na big measure in Indian languages today.\nThere are no computational grammars and hence no syntactic\nparsers for any of the Indian languages. The question of a parsed\ncorpus therefore does not arise. Similarly, we do not have a clear\nidea about the number and nature of word sense distinctions that\nmay have to be made. There are no dependable sources for word\nsenses and dictionaries vary quite widely. There is no sense tagged\ncorpora. We have a long way to go.\nWhen we say so and so thing does not exist, we mean large\nscale, properly designed, thoroughly tested, proven, publicly avail-\nable solutions do not exist.\nThere may be some half-hearted,\nshort-sighted, ad-hoc, hoch-poch implementations and student\nprojects here and there. What is the use of such toy, demo sys-\ntems?\nCorpus based approaches require a number of tools for devel-",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-5",
                            "title": "Syntax: Grammars and Parsers",
                            "content": "give some idea about the broad nature of issues involved in de-\nveloping and using computational grammars.\nA set of sample\nsentences is given in the Appendix and the readers are strongly\nadvised to try out each of these sentences on the grammar given\nhere. In the latter part, we shall brieﬂy summarize the salient\nfeatures of several other grammar formalisms including the Lex-\nical Functional Grammar (LFG), Generalized Phrase Structure\nGrammar (GPSG) and the Tree Adjoining Grammar (TAG). Our\naim here is not to provide an exhaustive and in-depth coverage of\ngrammar formalisms. We will be selective and very brief. Inter-\nested readers will ﬁnd a good deal of literature on all aspects of\nsyntax.\nAugmented Transition Network Grammar\nAugmented Transition Networks were developed in the early 1970s\naround the same time that linguistics was also taking a new shape\nwith Noam Chomsky leading the generative linguistics programme\nwith his Transformational Grammar (TG). While the concepts be-\nhind ATN existed for some time before, it was Woods who gave a\nformal shape to ATN. As is clear from his work, ATN came up as\nan alternative to, and improvement over, Chomsky’s TG. While\nclaiming equivalence to TG in power, Woods argued that ATN\noﬀered a number of advantages including perspicuity, suﬃcient\ngenerative power, eﬃciency of representation, ability to capture\nregularities, eﬃciency of operation and ﬂexibility for experimen-\n198\nCHAPTER 2. FOUNDATIONS OF NLP\ntation. Surprisingly, linguists are hardly aware of ATN grammars.\nWe have seen that Context Free grammars are more power-\nful than Regular Grammars. Regular grammars are equivalent\nto Simple Transition Networks (more commonly known as Finite\nState Machines or Finite State Automata).\nCFGs are in fact\nequivalent to Recursive Transition Networks, state transition net-\nworks which can call one another recursively.\nSome of the arc\nlabels can be labels of other networks.\nATNs are obtained by\ncomplex sentences. Every elementary subtree, that is, a subtree\nthat includes just one node and its children, corresponds to one\napplication of a phrase structure rule. Thus trees are closer to\nphrase structure rules than to the structure of the sentences. We\nneed structural descriptions which separate out and vividly show\nlinear, hierarchical as well as functional structure inherent in the\ngiven sentence.\nTrees are not the most suitable structures for\ndepicting the structure of natural language sentences, although\nthey are widely used.\nWe will now brieﬂy sketch the merits and demerits of exist-\ning grammar formalisms. Linguistic grammar formalisms can be\nviewed as extensions of basic phrase structure grammars, more\nspeciﬁcally, Context Free Grammars (CFG). Let us ﬁrst under-\nstand the strengths and weaknesses of Context Free Grammars.\nThis would help us in understanding other grammar formalisms\nclearly.\nContext Free Grammars and Natural Languages\nIn the 1950s, there were major developments taking place in the\nﬁeld of computer science, including the development of high level\nprogramming languages. Grammars and parsers needed for com-\npiling these programs received a great deal of importance. In his\npapers in 1956 and 1959, Noam Chomsky laid out the foundations\nof formal properties of grammars. Chomsky classiﬁed grammars\nbased on string rewriting rules into four classes of formal com-\nplexity called Type 0 or Unrestricted Phrase Structure Grammar,\nType 1 or Context Sensitive Grammar, Type 2 or Context Free\nGrammar and Type 3 or Regular Grammar. Context Free Gram-\n2.2. COMPUTATIONAL LINGUISTICS\n183\nmars (CFG), the second least powerful in the Chomsky Hierarchy,\nwere found to be ideal for dealing with programming languages\neﬃciently. Since then a great deal of formal studies have been\nmade on CFGs within computer science and very eﬃcient pars-\ning algorithms have been developed. Linguists often fail to make\nup parsing strategies are combined for practical advantage. What-\never the case, parsing in CFGs has an asymptotic worst case time\ncomplexity of O(n3) in general. Deterministic subclasses of CFGs\nexist in some cases for which the time complexity will be linear.\nSuch special subclasses have been exploited for building grammars\nfor programming languages but they cannot be extended easily to\nnatural languages.\nPhrase structure grammars are string re-writing grammars.\nWe start with a string and repeatedly rewrite the string until we\nderive the string we are interested in. Each application of a rule\nto rewrite a string is called a step of derivation. The parse tree\nshows the end result of this process of derivation. The parse tree\nfor the sentence a big black elephant is shown in ﬁgure 2.8 above.\nNote that the root of a parse tree always corresponds to the start\nsymbol, the intermediate nodes correspond to non-terminal sym-\nbols, the leaf nodes correspond to the terminal symbols and any\nnode with its child nodes constitutes one application of a rule.\nThe node corresponds to the left hand side of the rule and the\nchildren, read left to right, correspond to the right hand side of\nthe rule.\nStrengths and Limitations of CFGs:\nThe merits and demerits of CFGs for dealing with human lan-\nguages are now fairly well understood and it is generally accepted\nthat CFGs do not form a good model for natural languages. Yet\nthe study of CFGs is signiﬁcant in NLP for four reasons. Firstly,\nsigniﬁcant portions of natural languages are context free although\nthe whole may not be. Since our aim is to develop a grammar\nformalism of the least powerful type, it is important that we\nunderstand the strengths and weaknesses of CFGs. We should\n186\nCHAPTER 2. FOUNDATIONS OF NLP\nnot employ more powerful grammars where CFGs suﬃce and we\nshould clearly understand the reasons for using more powerful\ngrammars where really required. Secondly, most of the linguistic\nutterances using only a bare minimum of semantic and pragmatic\ninformation.\nGrammars\nLanguages manifest as linear sequence of symbols in text or speech\nform. Not all sequences of these symbols are meaningful. We need\nto restrict the set of all possible sequences of symbols and include\nall the valid sequences and only the valid ones. In other words, we\nneed to impose constraints on the possible sequences of symbols.\nSuch a set of constraints, expressed as rules, or principles or what-\never, is called a grammar. A grammar of a language is a formal\nspeciﬁcation of the valid structures in that language. Thus we may\nthink of grammars at various levels - morphology is the grammar\nat word level, syntax is usually concerned with grammars at the\nsentence level, discourse grammars deﬁne valid discourse elements\nand so on. In syntax, we are naturally concerned with grammars\nat the sentence level.\nWe have deﬁned syntactic analysis as a process of mapping a\ngiven sentence to a description of its structure. The inputs to the\nsyntactic analysis process, also called the parsing process, include\nthe given sentence and the knowledge of language, in particu-\nlar, the lexicon, morphology and the syntactic grammar of the\nlanguage. The goal of syntax in linguistics is to characterize a\ngrammar that accepts all valid sentences and only the valid sen-\n2.2. COMPUTATIONAL LINGUISTICS\n175\ntences. The search is for such a grammar that is simple, elegant\nand psychologically plausible.\nComputational grammars, that is grammars meant for auto-\nmatic processing in NLP applications, need to be detailed, precise\nand exhaustive. Syntacticians often give only samples and then\nget into theoretical discussions on the abstract characteristics of\ngrammars. What we need are descriptive grammars that are very\ndetailed and precise so that computer programs can correctly in-\nterpret and apply them. Developing wide coverage grammars has\nproved to be a very diﬃcult task. The best available grammars,\nparsers for Indian languages. Even such basic issues as sentence\nboundary identiﬁcation have not been addressed in any serious\nmeasure. The sentence level forms a very important barrier and\nif we do not make an all out eﬀort to pass through this stage at\nthe earliest, there is a real danger that we will be left behind and\nwe will not be able to compete with others in the world. Com-\nputational syntax has somehow not received the due attention it\ndeserves in India. Given this, our treatment of syntax here will be\na bit more elaborate and detailed. Young researchers are well ad-\nvised to take computational syntax of Indian languages seriously.\nPeople do not communicate using isolated sentences but with\ncoherent pieces of discourse.\nMost syntactic theories, however,\nlimit themselves to analyzing one sentence at a time, isolating\ninter-sentence interactions for a separate detailed study. Firstly,\nthis is possible because sentences have their own internal syntactic\nstructure that is for the most part independent of how and where\nthe sentence is used. Secondly discourse level grammars have not\nmatured to a stage where one can employ discourse grammars\ndirectly in practice for syntactic analysis of whole discourses in\none stretch. We therefore generally presume that the input to the\nsyntactic analyzer is one isolated sentence.\nIt is not easy to provide a precise deﬁnition of what a sentence\nis. Sentences are generally considered to be sequences of words\n168\nCHAPTER 2. FOUNDATIONS OF NLP\nthat give complete meaning. Firstly this presupposes that there\nare identiﬁable things called words. Secondly it is not very clear\nwhat exactly we mean by complete meaning. If you remove some\nwords from a sentence, it will usually appear incomplete but it is\npossible to remove some words such as optional modiﬁers, with-\nout giving a sense of incompleteness. Also, a single sentence taken\nfrom a paragraph surely does not convey the complete meaning",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-6",
                            "title": "Semantics",
                            "content": "before the basic question of its utility for semantics is answered.\nMeanings of words apart, our interpretation and understand-\ning is what ﬁnally matters. In that sense meanings are in our\nminds. Somebody is a freedom-ﬁghter or a terrorist depending\nupon how you look at it and which side you take. The same thing\ncan mean diﬀerent things to diﬀerent people in diﬀerent contexts.\nThe same thing also changes meaning as you get better and deeper\nunderstanding. There is a limit for purely symbolic and objective\nconsideration of meanings.\nThere is a lot that has been done in semantics but there is\nstill a lot more that needs to be done. It is beyond the scope of\nthis book to get into the depths of semantics. We will only look\nat selected linguistic phenomena with examples.\n220\nCHAPTER 2. FOUNDATIONS OF NLP\nAttachment\nThere are some areas which lie at the boundary between syntax\nand semantics. Attachment of prepositional phrases and subor-\ndinate clauses is one example. Resolution of anaphoric references\nis another. To a small extent these problems can be tacked from\nthe point of syntax but by and large they belong the realm of\nsemantics. Syntax cannot take us too far. Consider the following\nwell known example:\nI saw a man on the hill with a telescope\nThe prepositional phrase on the hill may modify the verb,\nor the object. Accordingly we get two diﬀerent meanings - the\nseeing action took place on the hill (compare: I ate an apple on\nthe hill) or the man was actually on the hill when he was sighted.\nSimilarly, the prepositional phrase with a telescope can modify\neither the verb or the man or the hill.\nPerhaps the telescope\nwas used as an instrument for seeing or the man was carrying\na portable telescope in his hand or a big telescope was mounted\non the hill on which you saw the man. In any given sentence,\none of these possibilities or the other may look more natural and\nsome possibilities may even look odd.\nLook at many diﬀerent\nand perfected by subsequent semantic and pragmatic components.\nThese structural descriptions produced by syntax only aim to cap-\nture the raw structural information content in the sentence. The\nstructure of a sentence is analyzed in terms of meaningful parts\nand subparts and the relationships among these parts. The small-\n2.2. COMPUTATIONAL LINGUISTICS\n171\nest parts are the words which can be mapped on to grammatical\ncategories.\nAlthough we normally deal with grammatical cate-\ngories like noun and verb as uninterpreted symbols within a syn-\ntactic framework, it should be emphasized that these categories\nare also essentially semantic in nature - nouns represent things,\nverbs represent actions and states, etc. When we say that ‘book’,\n‘beauty’ and ‘London’ are all nouns, we are emphasizing what\nis really common between all these entities - that they are all\nthings in a broad sense. Thus grammatical categories are general\nmeanings, very general of course. When we need to make some-\nwhat ﬁner distinctions, we use the notion of sub-categorization.\nWe divide nouns into proper nouns and common nouns, verbs\ninto intransitive, transitive and bi-transitive verbs and so on, of-\nten making much ﬁner distinctions than these, leading to a more\ndetailed semantic classiﬁcation.\nIn any case, syntactic analysis\nis a kind of understanding wherein very gross, general meaning\nis captured. The syntactic structures that are obtained in such\nan analysis are representations of such very abstract and general\nmeaning. A fundamental requirement of a syntactic analyzer is\nthat it preserves and explicates as much of the meaning in the\ngiven sentence as possible but avoids distortions of meaning at all\ncosts.\nSyntax as an Independent Module\nSyntactic analysis is a step towards the ﬁnal goal of understand-\ning or generation and it is not appropriate to deal with syntax\ncompletely independent of the ﬁnal goals of NLP. What kind of\nanalysis needs to be made and what kind of structural description\nconcerned more with the mental models of the producer and the\ncomprehender than merely with the details of the code - the linear\nsequence of symbols, the message that is physically communicated\nthrough a medium from the producer to the comprehender. It is\nunfortunate that many have given too much of attention to the\ndetails of the code itself rather than to the mechanisms of produc-\ning and interpreting the codes. For example, syntacticians have\n2.1. INTRODUCTION\n93\noften spent all their life analyzing the intricacies of structure of\nsentences rather than worry about what those sentences really\nmean and how particular syntactic structures encode given mean-\nings and intentions. The central element of language is meaning\nand intention. It is unfortunate that phonology, morphology and\nsyntax have come to be considered the “core” of linguistics rather\nthan semantics. A lot of time, eﬀort and energy has been spent on\nsuperﬁcial details rather than the real, core problems and issues.\nThe available body of knowledge in linguistics and NLP today is\nthus heavily skewed and inevitably, this books reﬂects the same.\nWe will be raising some critical issues with semantics but we may\nnot be able to give satisfactory solutions in all cases.\n2.1.4\nNLP Over the Decades\nWe have seen the origins of NLP within Artiﬁcial Intelligence\nduring the mid ﬁfties. The primary aims were a) computational\nmodelling of human language production and comprehension and\nb) building intelligent systems with natural language capabilities.\nMany NLP systems were developed mainly to demonstrate ideas,\nproblems and possible solutions. They were all toy systems by to-\nday’s standards. One of the major application area was machine\ntranslation.\nOne of the important lessons learnt during the initial period\nwas that languages are richer and more complex than we may\ninitially tend to think and a much more thorough and deeper\nanalysis of all aspects of languages is essential. Evaluation of NLP\naﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It\nhelps, for example, to make clear distinctions between open ended\npure research, applied research with clear cut aims an objectives\nwith reference to speciﬁed applications and developmental activi-\nties leading to data generation, products, services etc.\n98\nCHAPTER 2. FOUNDATIONS OF NLP\n2.2\nA Layered View of Computational\nLinguistics\nLinguistics recognizes that there are several appropriate levels of\nlinguistic representation such as phonemes, morphemes, words,\nphrases, clauses, sentences and discourse segments. A word is a\nminimal meaningful unit at a particular level of abstraction while\na sentence is also a minimal meaningful unit but at a diﬀerent,\nhigher level of abstraction. Linguistic theories postulate and show\nthat certain kinds of units are appropriate while others are not.\nThus whether words can be understood in terms of lower level\nunits called morphemes would be a fundamental question in lin-\nguistic theory. Linguistics provides tools and techniques of scien-\ntiﬁc inquiry into human languages at each of these appropriate\nlevels of description.\nWe can also cut words or sentences into arbitrary parts but\nsuch parts may not have any linguistic signiﬁcance. For exam-\nple, the individual characters of which a word is made, carries no\nlinguistic signiﬁcance. The length of words expressed in terms of\nnumber of characters it includes or the length of a sentence in\nterms of the number of words in it are generally considered to be\nof no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language\nlinguistic message that is encoded as temporal sequences of sounds\nin speech and as linear sequence of symbols in the written text.\nNatural language generation aims at encoding given information,\nintentions, feelings and attitudes as linear sequences in speech or\ntext form. Translation, summarization, categorization, informa-\ntion retrieval and all other applications require understanding the\nmeaning of the text to achieve human-like performance. Seman-\ntics is thus at the very core of language technologies. In fact it\nwould be proper to view all other aspects as steps towards un-\nderstanding the meanings. If we cannot understand the meaning,\nthere is only so much we can do. Attempting any language tech-\nnology application without trying to understand meanings will be\nlike driving a car with the eyes blind-folded. You can of course\ndrive but only at a big risk.\nBut semantics is a diﬃcult area. No one knows exactly what\nwe mean by meaning. No one really knows exactly what we mean\nby understanding. Philosophers have been thinking on these top-\nics for centuries. Now that these questions have become pertinent\nto development of technologies, scientists from a variety of other\nspecializations have also been working on semantics and propos-\ning new ideas.\nA lot of new ideas have come up over the last\n60 years. But that does not mean that the problems have been\nsolved. Given the nature of the problems and issues, it would be\napt to say that we know today almost only as much as we knew\ncenturies ago. In fact many of the ideas tried out by technologists\nare superﬁcial and the real problems have not even been addressed\n216\nCHAPTER 2. FOUNDATIONS OF NLP\nproperly.\nNo wonder then, that most of the applications today do not\neven incorporate a semantics component in any serious measure.\nThere is even a claim that a lot can be done without going for\nany in-depth linguistic analysis. And practical experience does\nshow that in some tasks high performance can be achieved by",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-7",
                            "title": "Pragmatics",
                            "content": "we mean by context? What exactly do we mean by sense of a\nword? Can we derive the meaning of a sentence from the meaning\nof the words and the structure of the sentence? Can we derive\nthe meaning of a text in terms of the meaning of the sentences\nit is made up of? Suppose the answer to some of these questions\nturns out to be no. Don’t you think a whole lot of all the work\nwe are doing will become irrelevant and useless as far as NLP is\nconcerned?\n2.2.7\nPragmatics\nPragmatics is the study of the aspects of meaning and language\nuse that are dependent on the speaker, the addressee and other\nfeatures of the context of utterance, such as the following: a) The\neﬀect that the following have on the speaker’s choice of expres-\nsion and the addressee’s interpretation of an utterance: Context of\nutterance, Generally observed principles of communication, The\ngoals of the speaker b) Programmatic concerns, such as the treat-\nment of given versus new information (including presupposition),\ndeixis, speech acts (especially illocutionary acts), implicature, and\nthe relations of meaning or function between portions of discourse\nor turns of conversation.\n2.2.8\nOther Areas of Linguistics\nWe have only touched some of the areas of linguistics that are\nperhaps more directly relevant for NLP. There are several other\ninteresting and useful areas. Phonetics and phonology deal with\nthe physical and logical levels of basic sound units that make up\nthe words of a language. As such they are very much relevant for\nspeech technologies. Psycholinguistics addresses questions relat-\ning to how humans process language and use experimental meth-\nods to build and test hypotheses. Sociolinguistics is a rich ﬁeld\nconcerned with social aspects of language. Language is related\nto social status, power and politics. Historical linguistics is con-\ncerned with linguistic genetics and language change over time.\n2.3. STATISTICAL APPROACHES\n233\nLanguage policy is important as it has direct implications for the\ncosts.\nSyntax as an Independent Module\nSyntactic analysis is a step towards the ﬁnal goal of understand-\ning or generation and it is not appropriate to deal with syntax\ncompletely independent of the ﬁnal goals of NLP. What kind of\nanalysis needs to be made and what kind of structural description\nshould be produced are therefore dictated by the needs of the sub-\nsequent semantic and pragmatic processes. Autonomy of Syntax\nis to be interpreted to mean that syntax can be taken up and\nstudied as an independent component within the overall system,\nnot for its own sake, not as an end in itself.\nSome researchers have argued that human mind does not pro-\ncess sentences in separate levels such as syntax, semantics and\npragmatics and hence a good computational model must also em-\nploy an integrated holistic approach to language understanding.\nOthers have argued and even attempted to provide evidence for\nthe psychological reality of independent modules. It should be\n172\nCHAPTER 2. FOUNDATIONS OF NLP\nnoted that our current understanding of semantics and pragmat-\nics is in no way as clear and complete as our knowledge of syntax\nand hence even if integrated processing may ﬁnally turn out to be\nthe ideal approach, a modular approach may be the best for the\ntime being.\nWe would like to emphasize that integrated or modular pro-\ncessing approaches should not be confused with implementation\nstrategies. The question is not really whether syntax, semantics\nand pragmatics should all be done parallelly or one after the other\n- this is more of an implementation strategy. The real question\nis whether it is reasonable to break the task into separate and\nindependent modules. The degree of interaction between various\nmodules is the dictating factor. The proponents of the integrated\nprocessing approach believe that the diﬀerent parts interact so\nmuch and in so many complicated ways that it is not reasonable\nto break the task into separate modules. Others believe that it is\nOne should be careful not to confuse between idealization and\nproceduralization. One should not mix up the issue of procedu-\nral versus nonprocedural approach with the distinction between\n176\nCHAPTER 2. FOUNDATIONS OF NLP\ncompetence and performance.\nChomsky argues for a model of\ncompetence and against a performance model by talking about\nmemory limitations, changes in intention during speech, and even\nphysical states such as coughing, sleepiness or drunkenness. No-\nbody may want to build a model of performance that includes\ncoughing and sneezing. Idealizations are done in all approaches.\nThis is not any signiﬁcant argument against the procedural view\nat all.\nIgnoring all purely physical and psychological inﬂuences of ac-\ntual performance, the remaining knowledge of language, the char-\nacterization of the mental competence of a speaker, also has a\nprocedural part in it that has got to be dealt with explicitly.\nThe knowledge of language includes not only rules, constraints\nand principles but also the procedural aspects of what rules, con-\nstraints or principles are applied, how, when, in what order, etc.\nWe have seen that when people speak, they start from their aims\nand goals, they build up their strategies, decide the structures\nand words and construct natural language utterances. Similarly\nunderstanding language requires many steps. A procedural view\npoint is both natural and essential for NLP.\nIt should be emphasized that knowledge does not always have\nto be declarative and nonprocedural. The procedure for multiply-\ning two numbers is very much a part of the knowledge of most of\nus. There was a time in the history of AI where the arguments for\nand against procedural and declarative representations of knowl-\nedge had taken almost the shape of a controversy. It is universally\nagreed now that both kinds of representations of knowledge have\ntheir due share. There is no clash between the two, one comple-\nments the other.\n2.2.5\nSyntax: Grammars and Parsers\nLanguages exhibit complex structures and a detailed and system-\natic analysis of the structure of natural language sentences is in-\nvaluable in determining the meaning of the sentences. Form fol-\nlows function. Knowing the structure will, hopefully, help us in\nknowing the meaning. Syntax is a branch of linguistics which deals\nwith the problem of analyzing the structure of natural language\nsentences and producing structural descriptions. These structural\ndescriptions can then be analyzed further by semantic and prag-\nmatic components to obtain the meaning and the intentions be-\n2.2. COMPUTATIONAL LINGUISTICS\n167\nhind the sentence thereby capturing the communicative aspect of\nlanguage in its broadest sense.\nStudy of language can be roughly grouped into three levels -\nword level, sentence level and discourse or text level. Topics such\nas dictionaries, thesauri, WordNets, lexical semantics and mor-\nphology are primarily word level aspects of language. Syntax is\nmainly a sentence level phenomenon. Sentence level semantics is\nalso a very important aspect. Discourse analysis and pragmatics\nrequires going beyond individual sentences. The three levels are\nnot water tight compartments and this grouping is only a very\ngross classiﬁcation.\nThere are certainly important interactions\nbetween levels and the topics may span across the levels. Never-\ntheless this stratiﬁcation helps us to get a feel for where we stand.\nA lot of work has been done in Indian languages at the word level\nalthough we might still be far from perfect or even satisfactory\nlevels of performance. On the other hand, very little has been\ndone in syntax. There are hardly any computational grammars or\nparsers for Indian languages. Even such basic issues as sentence\nboundary identiﬁcation have not been addressed in any serious\nmeasure. The sentence level forms a very important barrier and\nif we do not make an all out eﬀort to pass through this stage at\n2.2. COMPUTATIONAL LINGUISTICS\n109\npragmatic knowledge is hazy.\nFrom a computational point of view, completeness and consis-\ntency are desirable. We cannot include “November” and exclude\n“December”.\nThe list of function words and other closed-class\nwords must be complete. Redundancy should be minimized. Cir-\ncular deﬁnitions must be avoided. A detailed analysis by computer\npoints to many places where such requirements have not been met\nin many good printed dictionaries. For example, in one dictionary\n’body’ is deﬁned as ’the whole of a person’ and used in a diﬀerent\nsense in deﬁning ’parliament’ as ’a law-making body’. In another\ndictionary a ’box’ is deﬁned as ’a container’ and then a ’container’\nas ’a very large, usually metal box’. Circular deﬁnitions of higher\npath lengths are very diﬃcult to detect.\nLay users as well as students of NLP often have a very sim-\nplistic view of dictionaries and lexicography. Developing lexical\nresources is considered a lower job, more akin to typing work, and\nnobody wants to work on these areas. There is always a fancy\nfor big tasks such as machine translation or speech recognition. If\nsigniﬁcant progress has to be made in any of the applications of\nNLP, lexical resources must be given great attention. One of the\npurposes of this section has been to show that there is more to it\nthan meets the eye when it comes to dictionaries.\nStructure and Organization of a Dictionary\nThe most common organization of dictionary entries is a alpha-\nbetically sorted list of entries. Such a dictionary arranged in al-\nphabetical order makes it simple and eﬃcient to look up a given\nword. Eﬃcient data structures and algorithms exist for searching\nin sorted lists of items. Data structures such as height balanced\nbinary search trees (also called AVL Trees), height balanced m-\nary trees (such as B-Trees, B+ Trees and B* trees), splay trees,\nTRIE etc. have been used. Readers may refer to any good text",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-2-subsection-8",
                            "title": "Other Areas of Linguistics",
                            "content": "artiﬁcial intelligence and linguistics. The book aims to pro-\niv\nvide a balanced treatment of linguistic, statistical and tech-\nnological material with an Indian language focus. Treatment\nis kept simple and mathematical jugglery is kept to a mini-\nmum. No background in any speciﬁc area is assumed. Style\nof writing is not very formal or terse. The focus is on the\nmajor lines of thinking, ideas and themes. Instead of giving\nready made solutions in all cases, the book raises questions\nand issues, to get budding researchers to start independent\nthinking. The bibliography is limited to readily accessible\ntext books - references to diﬃcult-to-get research articles\nand the ever changing web-sites are avoided. Language En-\ngineerings is a very active area of research and development\nand the interested reader will ﬁnd good resources on the\nweb.\nThere is enough material in the book to be used as a\ntext book for a ﬁrst course on the topic within library and\ninformation sciences, linguistics, computer science, artiﬁcial\nintelligence and other related disciplines. Parts of the book\nmay also be found useful for more advanced courses. It is\nhoped that this book will be well received. Comments and\nsuggestions to improve the book are welcome.\nA second aim of the book has been to provide some as-\nsessment of the status of language technologies in India. It\nis important to stop once in a while and take a fresh, unbi-\nased look at what all we have achieved and where we have\nnot been very successful. An analysis of failures and deﬁ-\nciencies and a feeling that we can actually do much more, is\nabsolutely essential for taking up fresh initiatives with new\nlife and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of\n88\n2.1.4\nNLP Over the Decades . . . . . . . . .\n93\n2.1.5\nLinguistics versus NLP . . . . . . . . .\n95\n2.2\nComputational Linguistics . . . . . . . . . . .\n98\n2.2.1\nDictionaries . . . . . . . . . . . . . . .\n99\n2.2.2\nThesauri and WordNets . . . . . . . . 113\n2.2.3\nMorphology . . . . . . . . . . . . . . . 120\n2.2.4\nPOS Tagging . . . . . . . . . . . . . . 162\n2.2.5\nSyntax: Grammars and Parsers . . . . 166\n2.2.6\nSemantics . . . . . . . . . . . . . . . . 215\n2.2.7\nPragmatics . . . . . . . . . . . . . . . 232\n2.2.8\nOther Areas of Linguistics . . . . . . . 232\n2.3\nStatistical Approaches . . . . . . . . . . . . . 233\n2.3.1\nCorpora . . . . . . . . . . . . . . . . . 235\n2.3.2\nStatistical Approaches to Language . . 241\n2.3.3\nMachine Learning\n. . . . . . . . . . . 245\n2.4\nIndian Language Technologies . . . . . . . . . 261\n2.4.1\nThe Text Processing Environment: . . 269\n2.4.2\nThe Alphabet . . . . . . . . . . . . . . 271\n2.4.3\nThe Script Grammar . . . . . . . . . . 273\n2.4.4\nFonts, Glyphs and Encoding Standards 277\n2.4.5\nCharacter Encoding Standards . . . . 281\n2.4.6\nRomanization . . . . . . . . . . . . . . 301\n2.4.7\nSpell Checkers\n. . . . . . . . . . . . . 304\n2.4.8\nOptical Character Recognition\n. . . . 311\n2.4.9\nLanguage Identiﬁcation\n. . . . . . . . 316\n2.4.10 Others Technologies for Indian Lan-\nguages . . . . . . . . . . . . . . . . . . 321\n2.4.11 NLP and Sanskrit\n. . . . . . . . . . . 321\n2.4.12 Epilogue . . . . . . . . . . . . . . . . . 324\n2.5\nConclusions . . . . . . . . . . . . . . . . . . . 325\n3\nAdvances in IR\n327\n3.1\nHistory of IR . . . . . . . . . . . . . . . . . . 327\n3.1.1\nFrom The Library to the Internet . . . 329\nxiv\nCONTENTS\n3.2\nBasic IR Models\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nlife and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of\nthe limited interface linguists and engineers have had in In-\ndia on topics of relevance here. Linguistics itself has changed\nv\nsubstantially over time. There have been competing theories\nand view points. Focus areas, theories or models adapted\nand view points have varied substantially even within the In-\ndian context. Generalizations and extrapolations can thus\nbe dangerous. Comments on the current state of technology\nshould be taken with care as technology keeps changing very\nfast. All views expressed are especially applicable to the In-\ndian scene. If some of the statements appear a bit negative,\nit is because the assessment being made is with respect to\nthe perfect, the ideal, with respect to what we could have\nachieved. It is not that all the good work that has been done\nis not appreciated. It should be taken in the positive sense\nthat we have the capability to do much more. We can be\nworld leaders. It is essential to have this optimism but at\nthe same time we must plan our work based on the knowl-\nedge and understanding of the ground realities. This is the\nrecipe to success. I hope this view will be appreciated and\nwell received.\nKavi Narayana Murthy\ncaitra s’ukla pratipat, paarthiva naama saMvatsara\n(chaandramaana yugaadi - 9 April 2005)\nvi\nFOREWORD\nWhen I agreed to chair on the occasion of Sarada Ran-\nganathan Memorial Lectures 2004 delivered by my former\ncolleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much\nof no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language\nand each level of linguistic description is studied more or less inde-\npendently of other levels. That is, the interactions between layers\nare presumed to be separable from the core of a given level itself so\nthat the core and the interactions with the neighboring levels can\nbe explored separately to characterize any particular level. For\nexample, Chomsky claims “autonomy of syntax” by saying that\nsyntax can be studied more or less independently of other levels of\nlinguistics. Syntax is presumed to be loosely-coupled with other\nlevels, not tightly and inseparably coupled.\nThe above stratiﬁed view of language raises certain basic ques-\ntions. For example, can we understand the meaning of a sentence\nin terms of the meaning of the words in the sentence? Or is a\nsentence an atomic unit of meaning and breaking it into words\nartiﬁcial and unhelpful? Such questions have been scientiﬁcally\n2.2. COMPUTATIONAL LINGUISTICS\n99\nexplored in great depth and detail within the Indian tradition\ndating back to several thousand years. In fact whether there are\nidentiﬁable units such as words and sentences is a question that\ndeserves careful consideration. As opposed to the stratiﬁed model,\none can posit a holistic view of language and claim that we pro-\ncess and understand language all in one go, not layer by layer. Do\nwe perform a dictionary look up and morphological analysis ﬁrst\nand then perform syntactic analysis and ﬁnally get into seman-\ntics in our minds to understand the meaning of an utterance? Or\ndo we do all these things together, in a closely integrated fashion?\nOn the one hand these are questions of implementation strategies.\nAt the same time, these are basic questions relating to layered or\naﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It\nhelps, for example, to make clear distinctions between open ended\npure research, applied research with clear cut aims an objectives\nwith reference to speciﬁed applications and developmental activi-\nties leading to data generation, products, services etc.\n98\nCHAPTER 2. FOUNDATIONS OF NLP\n2.2\nA Layered View of Computational\nLinguistics\nLinguistics recognizes that there are several appropriate levels of\nlinguistic representation such as phonemes, morphemes, words,\nphrases, clauses, sentences and discourse segments. A word is a\nminimal meaningful unit at a particular level of abstraction while\na sentence is also a minimal meaningful unit but at a diﬀerent,\nhigher level of abstraction. Linguistic theories postulate and show\nthat certain kinds of units are appropriate while others are not.\nThus whether words can be understood in terms of lower level\nunits called morphemes would be a fundamental question in lin-\nguistic theory. Linguistics provides tools and techniques of scien-\ntiﬁc inquiry into human languages at each of these appropriate\nlevels of description.\nWe can also cut words or sentences into arbitrary parts but\nsuch parts may not have any linguistic signiﬁcance. For exam-\nple, the individual characters of which a word is made, carries no\nlinguistic signiﬁcance. The length of words expressed in terms of\nnumber of characters it includes or the length of a sentence in\nterms of the number of words in it are generally considered to be\nof no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-3",
                    "title": "Statistical Approaches",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-2-section-3-subsection-1",
                            "title": "Corpora",
                            "content": "strange and bizarre constructions that may never occur in real\nusage while many important constructs we actually use regularly\nmay be ignored altogether. If a system works well on a large and\nrepresentative corpus, that would be more dependable than saying\nthat the system performs very well on a thousand diﬀerent types\nof constructs, carefully selected by expert linguists. Corpus lin-\nguistics uses corpora as the basis for all investigations, hypothesis\ngeneration, testing and validation.\nCorpora are useful, why, almost essential for developing lan-\nguage technology applications. Corpora are used to build lexical\nresources such as word lists, dictionaries, thesauri and ontolo-\ngies. Statistical techniques are available for automatic or semi-\nautomatic development of lexical databases, morphological ana-\nlyzers, POS taggers, syntactic parsers etc. Corpora are used for\ntraining in machine learning algorithms. Corpora are also used\nfor testing and validation. Many of the language technology ap-\nplications possible today owe their existence to the availability of\nlarge scale corpora, aﬀordable storage and computing power and\nthe advancements in machine learning techniques.\nIn this section we shall take a brief look at diﬀerent kinds\nof corpora and techniques for developing, managing and using\ncorpora for language engineering applications.\n2.3. STATISTICAL APPROACHES\n235\n2.3.1\nCorpora\nA corpus is a large and representative collection of linguistic data,\ncarefully collected according to prescribed criteria. It is under-\nstood that the data are in electronic and machine processable\nform. A library housing a collection of printed books is not a cor-\npus. It is not of much direct value for the language engineering\napplications - machines cannot read or process printed books eas-\nily. Linguistic data can be at various levels and we can think of\ntext corpora, speech corpora, corpora of scanned images of texts\netc. The terms “large” and “representative” are qualitative but\n35% of words used in a standard newspaper in English are not\navailable in a good printed dictionary and similarly, a large per-\ncentage of words listed in the dictionary are never used in the\nnewspaper. Systems which give good performance on one corpus\nhave often been found to fail on other corpora. Corpus provides\nan extremely useful data resource for all our investigations but\none must be careful in coming to conclusions.\nText Corpora\nTypes of Text Corpora:\nA plain text corpus is simply a collection of electronic text\n238\nCHAPTER 2. FOUNDATIONS OF NLP\ndocuments. The collection may be organized in ﬁles or folders\nand directories. Documents usually contain headers which include\nmeta-data such as title, author, publisher, year of publication, and\nother relevant ﬁelds. Several text corpora including hundreds of\nmillions of words are now available for English. Plain text corpora\nin all the major Indian languages were developed with the support\nof the Department of Electronic (now Department of Information\nTechnology), Government of India and distributed by the Central\nInstitute of Indian Languages at Mysore.\nThese corpora have\nonly about 3 Million words. Recently, larger corpora are being\ndeveloped for many Indian languages. Telugu has a text corpus\nof nearly 38 Million words as of this writing.\nA POS-tagged corpus is a text corpus where each word is\ntagged with the appropriate POS tag. We have already seen that\nwords may belong to several possible POS categories and the dic-\ntionary only lists all the possible categories. The correct POS tag\nfor a given word can only be ascertained from the context where\nit is used. We have seen how POS tagging can be performed using\ntechnologies such as HMMs. Large POS tagged corpora exist for\nEnglish but very little is available for Indian languages.\nA sense-tagged corpus is one in which every word is tagged\nwith the appropriate sense of the word. We have seen that POS\ntags can disambiguate between gross diﬀerences in meaning. To\nsuitable for NLP applications are badly required today.\nDictionaries can also be developed starting from corpora. Some\nof the best dictionaries for English have been developed using the\ncorpus based methods. Large and representative corpora are use-\nful at practically all stages of dictionary development. Corpora\nassist in extracting words, grammatical categories and features,\ncontexts, meanings and word senses, usage, etc.\nA number of\ncomputational tools exist for analyzing corpora and extracting,\nﬁltering, sorting and checking the relevant pieces of information.\nCorpora in Indian languages have started appearing. About\n3 Million word corpora are now available for the major languages.\nThese corpora are not suﬃcient for many applications. A 35 Mil-\nlion word corpus now exists for Telugu. Large scale corpora will\nhopefully become available in all the major languages of India\nsoon. There will still be challenges to be overcome before good\ndictionaries can be developed. Indian languages, especially the\nlanguages of the Dravidian family, are characterized by an ex-\ntremely rich system of morphology. What kinds of words must\n2.2. COMPUTATIONAL LINGUISTICS\n113\nbe listed in the dictionary and what kinds of word forms can be\nobtained by rules of morphology is not yet very clear. Exactly\nwhat kind of grammatical information we should provide for a\ngiven word or even the set of POS tags to use are open questions\ntoday.\n2.2.2\nThesauri and WordNets\nThesaurus\nIn very general terms, a thesaurus is a treasury or a storehouse;\nhence, a repository, especially of knowledge; often applied to a\ncomprehensive work, like a dictionary or encyclopedia.\nMore\nspeciﬁcally, a thesaurus is a book containing a classiﬁed list of\nsynonyms, organized to help you ﬁnd the word you want but can-\nnot think of.\nRoget’s thesaurus describes a thesaurus as ‘the opposite of a\ndictionary. You turn to it when you have the meaning already\nbut don’t yet have the word. It may be on the tip of your tongue,\nfor a variety of applications. Both monolingual and bilingual ap-\nplications can beneﬁt from a parallel corpus. Dictionaries, mor-\nphological analyzers, POS taggers, WSD systems, machine trans-\nlation systems can all beneﬁt from parallel corpora, especially if\nthere is a good one-to-one correspondence between the text units\nin the two languages. When such correspondences are weak, we\nprefer the term similar corpora.\nSince the units of texts in the two languages in a parallel cor-\npus may not match one to one in a very straight-forward manner,\nwe need to align the corresponding units of text, be they sentences\nor chunks or words. A parallel corpus becomes very useful once\nit is aligned.\nStatistical techniques for alignment exist but for\ngenerating high quality training corpora, some manual eﬀort is\ninevitable.\nDeveloping Text Corpora:\nText corpora can be developed by typing in printed texts,\nusing OCR or through speech recognition. OCR and speech tech-\nnologies are far from perfect, especially for Indian languages and\nthe only workable method is to key in texts as of today. Of course\none may also make use of texts already available in electronic\nforms such as newspapers. This involves downloading as well as\nformat conversion since many web pages in Indian languages to-\nday are not encoded in any standard text encoding standard such\nas ISCII or UNICODE. It can be expected that fairly large quanti-\nties of plain text corpora will become available soon for the major\nlanguages of India. A more critical question is whether these cor-\npora are balanced.\nIt is often not possible to develop corpora\nbased on pre-set criteria and pre-selected genres simply because\nnot much is available in those genres. You will ﬁnd any amount\nof material on literature but hardly any on scientiﬁc and technical\ndomains. Thus there is a tendency to put together whatever is\n240\nCHAPTER 2. FOUNDATIONS OF NLP\navailable and call it a corpus. Such corpora may not be balanced\nof speech technology is given after that. With this, it is easy\nvii\nfor KNM to lay the foundations of NLP in the next section.\nHow doing purely esoteric linguistics is diﬀerent from doing\ncomputational linguistics has been brought about by him\nclearly in a section.\nThose interested in corpora are also\ngoing to beneﬁt from this text book tremendously as there\nis a long section devoted to this area, besides introductory\nremarks in the earlier section. Under 2.4, a lot of loosely\nstrung issues have been put together in a rather well-written\nand lengthy section.\nThe third chapter deals with the latest researches in ‘In-\nformation Retrieval’, a matter so very dear to all librarians\nand information science persons who were present during the\ntalk. For those who are uninitiated, an introduction to the\nbasic model of retrieval has been presented but the advance\nlearners and persons on the job can also beneﬁt from the\ndescription of an advanced IR model, which he calls the ‘In-\ntelligent IR’. There again, KNM brings out the role played\nby linguists and semanticists.\nThe best part of the book is that it has a very good\nreading list for those interested in NLP in its Bibliography\nsection as well as a few very relevant appendices that go very\nwell with the text.\nI sincerely hope that like his lectures were well-appreciated\nby those who were fortunate enough to attend them in Ban-\ngalore, the text as put together here will also be highly ap-\npreciated by the readers of all times to come. On behalf of\nNLP community, let me thank KNM for a laudable eﬀort,\nand he deserves a word of praise also from the library and\ninformation scientists.\nUdaya Narayana Singh\nDirector, Central Institute of Indian Languages, Mysore\nMay 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-3-subsection-2",
                            "title": "Statistical Approaches to Language",
                            "content": "erate and learn languages. Language faculty was to be explored\nnot in isolation but in close relationship with other cognitive pro-\ncesses such as speech, vision and learning. Also, computer systems\nwere to be designed that could process and communicate with hu-\nman languages. Speech perception, understanding and synthesis\nwere also major areas of focus. However, early work in NLP was\nmostly limited to handling written texts and was thus largely dis-\njoint from speech technology research. Of late we can see a closer\nintegration of NLP and speech technologies. There is also a trend\ntowards multi-modal communications involving speech, language\nand gestures.\nWe begin this chapter with elements of linguistic analysis es-\nsential for NLP and take up corpus based statistical approaches in\nthe second half. The treatment is introductory. Interested readers\nwill ﬁnd pointers to more advanced and detailed material in the\nbibliography.\n2.1.3\nNLP: An AI Perspective\nThere are three major concerns in NLP:\n• Natural Language Understanding (NLU): How do we un-\nderstand what we read or hear?\n• Natural Language Generation (NLG): How do we synthesize\nNatural Language utterances to convey whatever we have\nin mind?\n• Natural Language Acquisition (NLA): How do we acquire\nor learn a language?\n2.1. INTRODUCTION\n89\nThe Producer-Comprehender Model\nWe use language to communicate information as also our ideas,\nfeelings, emotions and attitudes.\nThis communication through\nlanguage can only happen between active, cognitive processors\nsuch as human beings and, hopefully, computers. We don’t speak\nto walls, right? There can be no proper communication if one or\nboth the parties are sleeping or otherwise inattentive. We shall\nuse the term producer to refer to a person or entity that pro-\nduces speech or written text material for others to comprehend.\nSimilarly, one who listens or reads will be termed a comprehen-\nder. The producer and the comprehender must be active cognitive\nstrange and bizarre constructions that may never occur in real\nusage while many important constructs we actually use regularly\nmay be ignored altogether. If a system works well on a large and\nrepresentative corpus, that would be more dependable than saying\nthat the system performs very well on a thousand diﬀerent types\nof constructs, carefully selected by expert linguists. Corpus lin-\nguistics uses corpora as the basis for all investigations, hypothesis\ngeneration, testing and validation.\nCorpora are useful, why, almost essential for developing lan-\nguage technology applications. Corpora are used to build lexical\nresources such as word lists, dictionaries, thesauri and ontolo-\ngies. Statistical techniques are available for automatic or semi-\nautomatic development of lexical databases, morphological ana-\nlyzers, POS taggers, syntactic parsers etc. Corpora are used for\ntraining in machine learning algorithms. Corpora are also used\nfor testing and validation. Many of the language technology ap-\nplications possible today owe their existence to the availability of\nlarge scale corpora, aﬀordable storage and computing power and\nthe advancements in machine learning techniques.\nIn this section we shall take a brief look at diﬀerent kinds\nof corpora and techniques for developing, managing and using\ncorpora for language engineering applications.\n2.3. STATISTICAL APPROACHES\n235\n2.3.1\nCorpora\nA corpus is a large and representative collection of linguistic data,\ncarefully collected according to prescribed criteria. It is under-\nstood that the data are in electronic and machine processable\nform. A library housing a collection of printed books is not a cor-\npus. It is not of much direct value for the language engineering\napplications - machines cannot read or process printed books eas-\nily. Linguistic data can be at various levels and we can think of\ntext corpora, speech corpora, corpora of scanned images of texts\netc. The terms “large” and “representative” are qualitative but\nand statistical approaches to achieve the maximum possible with\nminimum eﬀort.\nThere are several ways we can integrate linguistic and statis-\ntical approaches in a hybrid architecture. Statistical methods can\nbe applied ﬁrst and linguistics used later as a ﬁlter to rule out\ncombinations that are impossible. Alternatively, linguistic meth-\nods can be applied ﬁrst and statistics used later only for rating\nand raking the possible outputs. Linguistics aims to deal with\nall and only valid constructs and it if often very diﬃcult to take\ncare of both these requirements at the same time. There may be\nseveral right answers but the wrong answers can be simply too\nmany. How do we rule out all of them? So it makes sense to\nrelax one of these constraints initially. For example, a linguistic\nmethod which captures all valid structure but does not necessar-\nily bar all invalid structures is often much easier to construct. If\ninputs can be assumed to be correct in most cases, an assumption\nthat is generally valid in many applications, we can use the lin-\nguistic module to analyze all inputs without worrying about their\nvalidity, and then apply statistical methods to rate, rank and if\nrequired ﬁlter out unacceptable combinations. Apart from these\nloosely-coupled architectures, we can also think of tightly inte-\ngrated architectures. Linguistics often forms the main device for\n2.3. STATISTICAL APPROACHES\n245\nidentifying the features to be used. Statistical methods can then\nbe used for feature weighting, dimensionality reduction, etc.\n2.3.3\nMachine Learning\nIn Machine Learning approaches, a set of training data is given\nand the machine “learns” a general rule or builds a model for\nperforming the intended task. The learning is automatic - there\nwill be no manual intervention. If the training data is good and\neﬀective learning methods are used, good models can be learnt.\nMachine learning methods are basically techniques for gener-\nysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\nhad moved from the research laboratories to the market place.\nThe strengths and weaknesses of purely statistical approaches\nhave now been clearly understood. It is now generally believed\nthat a combination of linguistic and statistical approaches may be\nessential. New application areas have emerged. At one point of\ntime NLP had become almost synonymous with Machine Transla-\ntion, especially in our country. Now Information retrieval, Infor-\nmation Extraction, Automatic Categorization, Automatic Sum-\nmarization, Text Mining have all become major application areas\nfor NLP. There is an increasing conﬂuence and synergy between\ntext and speech technologies. People have started talking about\nmulti-lingual and multi-media applications. There is an increased\nawareness of the importance of language and speech technologies\nat all levels. Many large funded research projects have been initi-\nated all over the world. In India the Government has been showing\na keen interest in developing technologies for Indian languages.\nThere are scores of universities and research organizations that\nhave been working on various aspects of language technologies.\nTeething problems have been largely overcome and attention has\nshifted to applications that can have serious impact on our society.\nWe have come a long way. However, if you look back carefully\nand analyze our achievements and failures, you will ﬁnd that al-\nmost all the critical problems have remained unsolved. We have\nnot been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.\ndiﬀerent story.\nSome researchers are concentrating on developing larger and\nbetter dictionaries, better grammars and analyzers, better rules\nfor translation, and so on. Others are trying to build large scale\nparallel corpora so that statistical machine learning techniques\ncan be employed to automatically learn such rules. Still others\nare working on developing a database of examples of translated\npieces so that new texts can be translated by analogy with already\ntranslated examples. Many others are focusing on human aided\ntranslation and translation support systems. Thus we can talk of\n1.8. MACHINE TRANSLATION\n63\nRule Based Translation, Example Based Translation, Statistical\nMachine Translation, Human Aided Machine Translation and so\non.\n• Translation is ideally a meaning preserving transformation\nfrom one language to another. This requires analysis of the\nsource language text to determine the meaning and gener-\nating appropriate natural language sentences in the target\nlanguage. Semantics is largely language independent and it\nshould be possible to represent the meaning of the source\nlanguage texts in a truely language independent meaning\nrepresentation.\nThis approach is termed the inter-lingua\napproach. The UNL project is a recent attempt in this ap-\nproach. The inter-lingua approach is theoretically neat and\nit has the practical advantage that only n analyzers and n\ngenerators would be required to build translation systems\nbetween all pairs of n languages.\nHowever, ﬁnding such\nan ideal inter-lingua representation itself has turned out to\nbe very hard. Also building analyzers and generators that\nwork with truely language independent representations is\nnot easy. In fact one may argue that the inter-lingua trans-\nlation is actually two translations, one from source language\ninto the inter-lingua and the other from the inter-lingua to\nthe target language.\n• Many systems therefore do not go for a true inter-lingua",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-3-subsection-3",
                            "title": "Machine Learning",
                            "content": "2.4. INDIAN LANGUAGE TECHNOLOGIES\n261\nan initial model and iteratively reﬁne the parameters as we get to\nsee more and more training data. The Baum-Welch algorithm is\nactually an application of the EM algorithm.\nWe avoid the details of the algorithms here. The algorithms\nare readily available and can be easily accessed by the interested\nreaders.\nOther Techniques in Machine Learning\nThere are many learning techniques and many of them have been\napplied to language technology problems. Decision Trees and De-\ncision Lists can be learnt automatically from training data. Sev-\neral techniques exist to classify objects where the decision bound-\naries are non-linear. Support Vector Machines (SVMs), which are\nbased on the principle of structural risk minimization, have been\nexplored widely in recent times.\nNeural networks and genetic\nalgorithms have been applied. Computational Learning Theory\nhas started providing answers to fundamental relations between\nthe training data size, the classiﬁcation accuracy levels and the\nassociated conﬁdence levels. Machine learning is a large and fas-\ncinating ﬁeld of study in itself. Interested readers will ﬁnd many\ngood books and other reference materials.\n2.4\nTechnology Development in Indian\nLanguages\nIndia is a land of One Billion people - about one sixth of the\nwhole world. Our civilization dates back to many thousands of\nyears. India is a land of many religions, many cultures and many\nlanguages. A life time is not suﬃcient to get even a glimpse of\neverything that is Indian.\nThe Language Scene in India:\nThe language scene in India is quite unique. Most of higher\neducation is imparted through the medium of English. Most peo-\nple prefer to send their children to English medium schools. The\nquality of books and teachers in local languages are generally con-\n262\nCHAPTER 2. FOUNDATIONS OF NLP\nsidered to be inferior, there is relatively little scientiﬁc and tech-\nnical material available in these languages and scope for gainful\ngiven set of languages.\nLanguage Identiﬁcation can be viewed\nas a generic machine learning problem, a supervised classiﬁcation\ntask in which features extracted from a training corpus are used\nfor classiﬁcation. Any of the machine learning techniques can be\nused.\nIn the last decade or so, corpus based Machine Learning ap-\nproaches have become predominant in language engineering over\nthe Knowledge Based approaches which use explicit rules hand-\ncrafted by domain experts. Recent research on Language Identi-\nﬁcation has been limited almost exclusively to Machine Learning\napproaches.\nA Machine Learning system is expected to be generic and it is\nunderstood that training is based only on the intrinsic properties\nof the data, as expressed through a set of “features”. Extraneous\nindicators such as clues from scripts or fonts used, header infor-\nmation or explicit markup tags in the document structure cannot\nbe used.\nThe basic idea is that each language uses a unique or a very\ncharacteristic alphabet, and the letters of the alphabet appear\nwith surprisingly consistent frequencies in any statistically signif-\nicant text. In addition, the frequency of occurrence of sequences\nof two, three, four and more letters are characteristically stable\nwithin, but diverse among diﬀerent natural languages. The most\nfrequent 3-grams, 4-grams etc. have been used for language iden-\ntiﬁcation. A crucial part of the recognition system is the identiﬁ-\ncation of the set of most distinctive, most frequently encountered\nsequences of characters (that is, bigrams, trigrams, etc.) that can\nbe associated with each language. Distinctiveness implies that the\nfrequency of a letter combination for a given language should be\nhigh relative to the frequency of occurrence in other languages.\nIn alphabetic writing systems such as those used for English\nand other European languages, a character is simply a letter of\nthe alphabet (or a punctuation mark, a digit or other special sym-\nthat the string “abnidella” is unlikely to be a valid English word\nwithout consulting a dictionary.\nThus machine learning is a purely data driven approach. The\ngreatest merit of this approach therefore is its generality and\nadaptability. All we need to migrate to a new language or a new\napplication is to provide appropriate training data in that lan-\nguage or for that application. The machine unlearns and relearns\n246\nCHAPTER 2. FOUNDATIONS OF NLP\nto automatically to adapt to the new situation. Migrating from\none language to another using a linguistic approach, on the other\nhand, would necessitate extensive manual exploration of the new\nlanguage structures and properties.\nMachine learning can be supervised or unsupervised. In su-\npervised learning, a set of labeled training data is given and the\nmachine learns a general decision rule which can be used for clas-\nsiﬁcation of new data items. In unsupervised learning, a set of\nunlabeled training data is given and the machine learns to group\nsimilar data items into clusters so that new data items can be\nplaced into the right clusters. The number of clusters may or may\nnot be known beforehand.\nWe describe here a selection of machine learning techniques\nbrieﬂy. The purpose is to give an exposure to the basic ideas.\nInterested readers may consult books on machine learning and\npattern classiﬁcation for an in-depth exposition.\nRegression as Classiﬁcation\nRegression analysis is a statistical technique for investigating and\nmodeling the relationship between variables in a system. When\nthere are more than two variables in the system, the term multiple\nregression is employed. Regression is often used as a modeling\ntechnique where the value of one of the selected variables, called\nthe response variable, is determined by the values of the other\nindependent variables, also called the regressors. The modeling\nprocess basically involves determining parameters of the model,\nexactly to place the book in a predeﬁned classiﬁcation scheme.\nThis takes time and eﬀort but the quality of work done would be,\nhopefully, very good. When one is faced with the problem of clas-\n48\nCHAPTER 1. THE INFORMATION AGE\nsifying a large number of documents, these same rules, guidelines\nand heuristics can be codiﬁed into computer programs and cate-\ngorization performed automatically. Hence the name knowledge\nbased approach.\nWith the increasing availability of large scale data in electronic\nform, advances in machine learning and statistical inference, there\nhas been a clear shift over the last decade or so towards automatic\nlearning from large scale data. In the Machine Learning approach,\na general inductive process (also called the learner) automatically\nbuilds a classiﬁer for a given category by observing the charac-\nteristics of a set of training documents already classiﬁed under a\nspeciﬁed set of classes. The inductive process gleans from these\nlabeled training data, the characteristics that a new unseen docu-\nment should have in order to be classiﬁed under a given category.\nThe classiﬁcation problem is thus an activity of supervised learn-\ning.\nA Machine Learning program automatically learns to distin-\nguish between diﬀerent classes or categories based on examples.\nLearning here is basically generalizing from examples. The ma-\nchine must ﬁgure out which features are more discriminative and\nwhich ones are not. Accordingly the features are weighted. All\nuseful features are considered but the more discriminative one will\ncarry higher weightage. Under suitable assumptions, it is possible\nto prove that what the machine does is about the best possible,\ngiven the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-\ngiven the features and the training data as the basis. The per-\nformance of a machine learning system can only be improved by\nusing better features or by increasing the quantity and quality of\ntraining data.\nThe aim of Automatic Text Categorization is to classify doc-\numents, typically based on the subject matter or topic, without\nany manual eﬀort. A text categorization program can automati-\ncally categorize thousands of documents in a few minutes. There\nis no way manual classiﬁcation can match that speed. In terms of\naccuracy of classiﬁcation, automatic systems can achieve accura-\ncies of 95% or even higher, depending upon the speciﬁcities of the\ntask. Manual classiﬁcation can in principle be fully correct but in\npractice one must make allowance for some errors.\nAutomated text categorization can be deﬁned as assigning\npre-deﬁned category labels to new documents based on the likeli-\nhood suggested by a training set of labeled documents. Given a\n1.7. AUTOMATIC TEXT CATEGORIZATION\n49\nset of documents with the associated category labels, the system\nlooks for discriminating features that help to set the various cate-\ngories apart. This process is called training. The system learns, so\nto say, how exactly documents within a class are similar and doc-\numents in diﬀerent classes are diﬀerent from each another. Once\nthe system has been trained, it can look at new documents unseen\nbefore and classify them into one of the set categories.\nThe similarities and diﬀerences between documents of various\nclasses are expressed in terms of features. In the simplest case,\nfeatures are the words in these documents. The assumption is that\nthere are words that occur frequently in certain classes but not\nin the others. Words like election, mandate, constituency, party,\nlegislature, parliament are more likely to occur in the political\narena than in sports. Of course there can be politics in sports,\nthere can be elections and parties in sports domain too. That is",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-4",
                    "title": "Indian Language Technologies",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-2-section-4-subsection-1",
                            "title": "The Text Processing Environment",
                            "content": "Natural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy\nUniversity of Hyderabad\nPublished By\nEss Ess Publications\nFor\nSarada Ranganathan Endowment for Library\nScience\nBangalore, INDIA\n2006\ni\nc\n⃝Kavi Narayana Murthy and Sarada Ranganathan\nEndowment for Library Science (2005)\nAll rights reserved. No part of this publication may be\nreproduced, stored in a retrieval system or transmitted, in\nany form or by any means, electronic, mechanical, photo-\ncopying, recording or otherwise without the prior written\npermission of the publisher.\nThis book has been printed from the camera-ready\ncopy prepared by the author using LATEX\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy, University of Hyderabad\nSarada Ranganathan Endowment Lecture, 24(2004)\nFirst Published 2006\nISBN 81-7000-485-3\nPrice: Rs. 850/-\nPublished by\nEss Ess Publications\n4837/24, Ansari Road, Darya Ganj, New Delhi-110 002\nTel: 001-23260807 Fax: 001-23274173\nE-mail: essess@del3.vsnl.net.in url:\nhttp//www.essessreference.com\nFor\nSarada Ranganathan Endowment for Library\nScience\n702, ‘Upstairs’, 42nd Cross, III Block, Rajajinagar,\nBangalore 560 010\nE-mail: srels@vsnl.com Tel: 080-23305109\nPrinted in India at: Printline, New Delhi 111 002\nii\nPreface\nThe contributions of Dr. S R Ranganathan to the ﬁeld of\nlibrary and information sciences is well known. Sarada Ran-\nganathan Endowment for Library Science (SRELS), founded\nby Dr Ranganathan in 1961 has been carrying out com-\nmendable work in promoting library and information sci-\nences. SRELS has been working towards improvement of\nlibrary and information services in India, training personnel\nin library and information sciences and applying research\nresults in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.\nten shapes and converting the scanned images into electronic text.\nText is much smaller in size compared to images and texts can be\neasily edited.\nThus language processing can be viewed at the levels of speech,\ntext and scanned images. NLP has traditionally focussed only on\ntext documents. While there is an increasing trend towards pro-\ncessing of multi-media documents, text processing continues to be\n2.1. INTRODUCTION\n87\nthe major focus. We shall limit ourselves mainly to processing of\ntext documents in this book.\n2.1.2\nNatural Language Processing and Com-\nputational Linguistics\nThis chapter deals with the fundamentals of language processing\nthat are essential for realizing Intelligent Information Retrieval\nas well as other applications of language and speech technologies.\nNatural Language Processing (NLP) forms the backbone of every\nhuman language technology application. NLP is concerned with\nnatural or human languages - languages which we human beings\nuse for day to day communications, as against artiﬁcial languages\nsuch as computer and robot programming languages.\nThe terms Natural Language Processing and Computational\nLinguistics have been used interchangeably and we shall do the\nsame here. There is no real diﬀerence between the two except per-\nhaps that linguists sometimes tend to prefer the latter term - in\nComputational Linguistics, Linguistics is the head of the phrase\nand Computational is only an adjectival modiﬁer. Computers, like\nmathematics, are generic and powerful tools and mere use of com-\nputers to carry out linguistic studies will not qualify as a distinct\nbranch of study in itself. Computational Linguistics or NLP is\ndiﬀerent from Linguistics in that the primary aim is to build com-\nputational models of various aspects of human language faculty.\nSuch models need to be simple, elegant and eﬃcient, yet very pre-\ncise, detailed and exhaustive. The models built must stand the\ntest of large scale real life data - language as people use it. NLP\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a\nis what we have in our ﬁnger tips. To this day, many of our tribal\nlanguages have no script. But they do have a very rich oral ‘liter-\nature’. Thus not everything we have is available in written form.\nThe greatest scholars in India have always been “illiterate” and\nthey preferred to remain so even after writing became possible.\nThe western notion of illiteracy is thus not applicable to India.\nIlliterates are not necessarily uneducated or ignorant.\nThis book, however, is focussed on computer processing of\nwritten texts. We shall therefore start with a study of our writing\nsystems and the text processing environment. Our aim here will\nbe to a get a feel for the nature of Indian languages and issues\nin technology development for Indian languages. We shall brieﬂy\nsurvey a few selected technologies.\n2.4.1\nThe Text Processing Environment:\n(Text Processing Tools)\nKEYBOARD\n(Keyboard Driver)\n(Font Encoding)\n(Character Encoding Standard)\nFILE\nPRINTER\n(Rendering Engine)\nDISPLAY\nFONTS\nThe Text Processing Environment\nFIG 2.16 The Text Processing Environment\n270\nCHAPTER 2. FOUNDATIONS OF NLP\nWhen you type in some text from a computer keyboard, a\npiece of software called the keyboard driver checks which keys are\npressed and sends out corresponding numerical codes. The text\nediting software converts these codes into a possibly diﬀerent set\nof codes corresponding to the character encoding scheme used.\nA character is a letter of the alphabet, a punctuation mark or a\nsymbol such as ‘#’ or ‘+’. For example, if ASCII is the character\nencoding scheme used, the upper case letter A is coded as the\nnumber 65.\nWhat is actually stored in a text ﬁle is just this\nnumber 65. All the software programs ‘know’ how to deal with\nthis number 65. For example, a text editor would display it as the\nletter ‘A’. A sorting program would know that ‘A’ comes before\n‘B’. If we are using the ISCII character encoding standard, the\n1991 BIS standard (IS 13194:1991), and you type the character\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nretrieval. There is an increasing cross-fertilization and integration\nof related technologies including speech.\nCurrently indexing is performed automatically on full texts.\nManual processing is slow, costly and may be inconsistent. On the\nother hand, while automatic processing can be very fast, it lacks\nthe commonsense and human judgement of manual methods and\ncan therefore be somewhat inferior in quality. The challenge today\nis to keep the superior speed factor and yet achieve near-human\nperformance through automatic methods.\nIt is clear that IR today is closely related to many other dis-\nciplines.\nIt interfaces with Database Management systems, Li-\nbrary and Information sciences, Artiﬁcial Intelligence, Natural\nLanguage Processing and Machine Learning. Database Manage-\nment systems focus on structured data stored in tables and eﬃ-\ncient processing of precisely deﬁned queries expressed in a formal\nlanguage such as SQL. The syntax and semantics of the data as\nwell as the query are clear. Recent trends towards semi-structured\ndata such as XML brings it closer to IR. Library and Information\nScience has focused on the human user aspects such as human-\ncomputer interaction, user interfaces and visualization. Eﬀective\ncategorization of human knowledge is a primary goal. Citation\nanalysis and bibliometrics are focus areas. Recent work in digital\nlibraries is bringing library science closer to computer science and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-2",
                            "title": "The Alphabet",
                            "content": "letter ‘A’ is 65 irrespective of which font is used for display. It\nis the rendering engine which actually constructs the appropriate\nimages on the computer screen or in a print-out as required.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n271\nA character encoding scheme speciﬁes the character set, im-\nposes a default collating sequence on the characters in the set and\nmaps these characters to numerical codes. Text ﬁles simply con-\ntain these codes. Only when displayed or printed by a rendering\nengine using appropriate fonts will you start seeing the content of\nsuch text ﬁles in a readable form.\nThings are diﬀerent in Indian languages. The very notion of\na character is diﬀerent. A thorough understanding of the basics\nof characters and character encoding schemes is invaluable. Let\nus start by looking at the alphabets.\n2.4.2\nThe Alphabet\nEnglish uses an alphabetic writing system. There are 26 letters\nin the alphabet and all words in English language are simply se-\nquences of these 26 letters. Knowing A-B-C-D is not enough to\nread English - you must also learn spellings. Spelling rules are\nquite arbitrary. If P-U-T is put why is B-U-T but? One letter is\nused to give diﬀerent sounds and diﬀerent letter combinations are\nused to give the same sound: (fat - fate, met - mete, kit - kite,\nput - but, case - chess - cess), (meet - meat - mete, bore - boar).\nBernard Shaw once remarked that “ﬁsh” can be spelled as “goat”\nin English, by borrowing the ’g’ for the ’f’ sound from ’rough’, ’o’\nfor the ’i’ sound from ’women’ and ’t’ for ’sh’ from ’station’!\nSpelling based alphabetic writing systems necessitate an ad-\nditional level of processing in many applications. For example, a\nText-To-Speech system must have the rules to map the spellings\nto sounds so that texts can be read out correctly.\nOn the other hand, every nursery kid in India starts by a sys-\ntematic study of phonetics, the science of sounds. The equivalent\nof the alphabet chart, called varNamaala shows a systematic and\nhigh relative to the frequency of occurrence in other languages.\nIn alphabetic writing systems such as those used for English\nand other European languages, a character is simply a letter of\nthe alphabet (or a punctuation mark, a digit or other special sym-\nbol), which is typically represented as a single byte in a character\n318\nCHAPTER 2. FOUNDATIONS OF NLP\nencoding scheme such as ASCII. Researchers dealing with such\nlanguages have naturally chosen a byte as the basic unit of text.\nFeatures such as n-grams are deﬁned in terms of bytes. Indian\nscripts are not alphabetic but rather syllabic in nature. Aksharas\nare the atomic units of writing - individual bytes have no sig-\nniﬁcance in Indian scripts. Texts are to be treated sequences of\naksharas.\nSome researchers have used lists of frequent words to distin-\nguish one language from the other. Comparing with stored lists\nof frequent words can be very eﬀective for language identiﬁca-\ntion. Our experiments using word lists with Indian Languages,\nnot described here, also conﬁrm this point. However, there are\nseveral objections to the use of lists of words, aﬃxes etc. As test\nsamples become smaller, chances of ﬁnding full words reduce. In\nsmall samples, words may be cut and storing lists of full words\nwill be of no use.\nThe most frequent words are usually closed\nclass grammatical words such as determiners, prepositions and\nconjunctions and carry little semantic information.\nSmall text\nsamples exacerbate the bursty nature of texts where such closed\nclass words surround pockets of less common words. It is these\nless common words that may in fact be more useful for language\nidentiﬁcation between certain languages than the small function\nwords. Which words to include in a word list is therefore an open\nquestion. Lastly, statistical features such as n-grams in any case\ninclude the information contained in small, frequent words, aﬃxes\netc. Given these facts and the desire to build generic, trainable\nencoding standard such as ASCII. Letters of the alphabet, along\nwith other special symbols such as punctuation marks, numerals,\nparenthesis, quote marks etc, are called characters.\nA charac-\nter encoding scheme speciﬁes the allowed set of characters and\nplaces them in a particular order. This order deﬁnes the collating\nsequence. Usually collating sequence is so selected that sorting\nin alphabetical order becomes straight forward. Each position is\ngiven a numerical code and the characters are represented inside\ncomputers by these numbers.\nThus the character ’A’ is repre-\nsented as number 65 in ASCII. Since there is a one-to-one corre-\n282\nCHAPTER 2. FOUNDATIONS OF NLP\nspondence between letters and glyphs, fonts can also use the same\nnumerical codes for placing the corresponding glyphs. Thus ’A’\ncan be represented as 65 in ASCII character encoding standard\nas also in any of the fonts. This makes the whole scheme very\nsimple and neat. The encoded text remains the same irrespective\nof which fonts are used.\nWe need a character encoding scheme for Indian languages\ntoo. But we must ﬁrst understand what we mean by a character.\nAksharas are the basic units of writing but there are too many\nof them. We cannot encode each possible akshara by giving it a\nnumerical code. We must get into more basic units and encode\nthem.\nThus aksharas will need to be composed of more basic\nunits. The script grammar we have seen above forms an excellent\nscheme to deﬁne a character encoding standard.\nAny character encoding scheme for Indian language scripts\nmust ideally deﬁne a script grammar and implement it by spec-\nifying the numerical codes for the consonants, vowels and other\nsymbols used in the grammar. In spite of the fact that the num-\nber of valid aksharas is potentially inﬁnite and practically very\nlarge, a script grammar makes it possible to encode texts using\na fairly small number of diﬀerent codes. The texts encoded in\nsuch a scheme would be a sequence of such numeric codes. The\nthis number 65. For example, a text editor would display it as the\nletter ‘A’. A sorting program would know that ‘A’ comes before\n‘B’. If we are using the ISCII character encoding standard, the\n1991 BIS standard (IS 13194:1991), and you type the character\n‘ka’, the number 179 will be stored. UNICODE is yet another\ncharacter encoding scheme - it speciﬁes how exactly texts will be\ncoded into numbers and stored in a text ﬁle. ASCII is suitable\nfor languages using the English alphabet. ISCII can be used for\nIndian Language texts, including English. UNICODE is intended\nto be useful for all languages.\nA character encoding scheme speciﬁes how texts in a given\nlanguage are encoded and stored in ﬁles. Presumably, all the text\nprocessing operations operate on this known, public, established\nstandard. Note that the emphasis is on storage representation,\nnot on how the characters appear when displayed or printed.\nWords are simply linear sequences of letters. Writing on paper\nas well as displaying on the computer screen and printing through\na computer printer are simple processes where the appropriate\nfont shapes are selected and placed one next to the other in a left\nto right fashion by a piece of software called a rendering engine.\nBy selecting appropriate fonts and style variations within that,\nwe can get the various appearances of the same letter, say ‘A’.\nIn English there is a near one to one correspondence between\nthe characters and the shapes stored in a font ﬁle.\nThus it is\nconceivable that characters and font shapes can be given the same\nnumerical encoding. Diﬀerent fonts for the same character can\nalso be given the same numerical encoding. The encoding for the\nletter ‘A’ is 65 irrespective of which font is used for display. It\nis the rendering engine which actually constructs the appropriate\nimages on the computer screen or in a print-out as required.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n271\nA character encoding scheme speciﬁes the character set, im-\nin sorted lists of items. Data structures such as height balanced\nbinary search trees (also called AVL Trees), height balanced m-\nary trees (such as B-Trees, B+ Trees and B* trees), splay trees,\nTRIE etc. have been used. Readers may refer to any good text\nbook on Data Structures and Algorithms for more details on these\ntopics.\nHowever, alphabetically sorted arrangement is neither essen-\ntial nor possible in all cases. Not all languages use an alphabetic\nsystem of writing. Languages such as Japanese and Chinese follow\nan ideographic writing system where written symbols correspond\nmore or less directly to meanings. For such languages, it would\n110\nCHAPTER 2. FOUNDATIONS OF NLP\nbe natural to classify words into semantic classes and organize\nthe dictionary accordingly. An ancient example of this is the naa-\nmaliMgaanus’aasana also known as amarakoos’a - a collection of\nnouns in Sanskrit organized into 27 major vargas or classes based\non meaning. All words relating to plants are in one group and\nwords relating to earth are in another. If you know something\nabout a word, you can locate it in the dictionary to know more\nabout it as also about many other words semantically related to\nit.\nIt is interesting to note that amarakoos’a is in verse form\nand it was expected that every student gets the whole dictionary\nby-heart!\nIn fact a dictionary organized in terms of semantic classes\ncould be called a thesaurus. We use a thesaurus for searching for\nthe most appropriate word for a given situation. The idea can be\nextended further to build networks of words inter-related along\nmany dimensions of relationships.\nSuch a structure is called a\nWordNet. We will see more on Thesauri and WordNets in later\nsections.\nThe most appropriate organization of a dictionary depends\nupon the ways we intend to use it. If you wish to look up the\ndictionary for spellings, you do not necessarily know the correct\nspellings to start with and hence simple direct search would not",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-3",
                            "title": "The Script Grammar",
                            "content": "a model of human language processing than an ineﬃcient one.\nSince computationally less powerful grammars will in general lead\nto more eﬃcient parsing, the search in NLP is for the least pow-\nerful grammar that is suﬃcient to deal eﬀectively with natural\nlanguages. We should not use a bull dozer if that could be done\n2.2. COMPUTATIONAL LINGUISTICS\n179\nwith a hand tool. We should not use context free grammars if that\ncould be done with regular grammars. Some of the early grammar\nformalisms including the transformational grammar proposed by\nNoam Chomsky and the Augmented Transition Network (ATN)\ngrammars proposed by computer scientists were computationally\nmore much more complex than needed. Many of the major gram-\nmar formalisms proposed by linguists continue to be that way even\ntoday. Positing an unnecessarily complex mechanism is unwise\nand wasteful. Computer science provides many powerful tools for\nanalyzing the formal complexity of grammars and the correspond-\ning parsing systems.\nAcademicians and researchers have a tendency to ﬁrst look for\nwhat is most interesting, rather than what is most useful. Intel-\nlectuals are motivated by love of complexity. We take pride and\nderive personal satisfaction in doing complex things.\nWe even\nhave a tendency to do simple things in a complex way. The Red\nQueen in Lewis Carrol’s Alice in Wonderland said, “I could have\ndone it in a much more complicated way”, with immense pride. A\nlot of time and eﬀort is often spent on a few rare, odd exceptional\nexamples rather than lay primary emphasis on simple, commonly\nused structures. Priorities get distorted.\nii) Universality:\nIt has been well recognized that despite superﬁcial diﬀerences,\nhuman languages share certain common underlying principles.\nLinguists are on the look out for a universal grammar - a grammar\nthat uncovers these underlying universal features. Linguists view\nuniversal grammar as an in-born biological endowment of the hu-\ngive some idea about the broad nature of issues involved in de-\nveloping and using computational grammars.\nA set of sample\nsentences is given in the Appendix and the readers are strongly\nadvised to try out each of these sentences on the grammar given\nhere. In the latter part, we shall brieﬂy summarize the salient\nfeatures of several other grammar formalisms including the Lex-\nical Functional Grammar (LFG), Generalized Phrase Structure\nGrammar (GPSG) and the Tree Adjoining Grammar (TAG). Our\naim here is not to provide an exhaustive and in-depth coverage of\ngrammar formalisms. We will be selective and very brief. Inter-\nested readers will ﬁnd a good deal of literature on all aspects of\nsyntax.\nAugmented Transition Network Grammar\nAugmented Transition Networks were developed in the early 1970s\naround the same time that linguistics was also taking a new shape\nwith Noam Chomsky leading the generative linguistics programme\nwith his Transformational Grammar (TG). While the concepts be-\nhind ATN existed for some time before, it was Woods who gave a\nformal shape to ATN. As is clear from his work, ATN came up as\nan alternative to, and improvement over, Chomsky’s TG. While\nclaiming equivalence to TG in power, Woods argued that ATN\noﬀered a number of advantages including perspicuity, suﬃcient\ngenerative power, eﬃciency of representation, ability to capture\nregularities, eﬃciency of operation and ﬂexibility for experimen-\n198\nCHAPTER 2. FOUNDATIONS OF NLP\ntation. Surprisingly, linguists are hardly aware of ATN grammars.\nWe have seen that Context Free grammars are more power-\nful than Regular Grammars. Regular grammars are equivalent\nto Simple Transition Networks (more commonly known as Finite\nState Machines or Finite State Automata).\nCFGs are in fact\nequivalent to Recursive Transition Networks, state transition net-\nworks which can call one another recursively.\nSome of the arc\nlabels can be labels of other networks.\nATNs are obtained by\n279\nFIG 2.20 Contd. Glyph to Character Mappings\nThe shapes we use in deﬁning a font should be selected based\non the simplicity of their being composed to obtain combined\nshapes for displaying full aksharas. The shapes are quite diﬀerent\nfrom script to script. The script grammar, on the other hand,\nwas based on phonetics and the aim was to develop a simple and\neﬃcient grammar to deﬁne all valid aksharas in a language inde-\npendent manner. From this, it should be clear that there may not\nbe any one-to-one correspondence between glyphs used in deﬁn-\ning fonts and the elements of which the script grammar itself was\ndeveloped. Diﬀerent scripts look diﬀerently and hence the fonts\nand the glyphs of which these fonts are made of, also diﬀer.\n280\nCHAPTER 2. FOUNDATIONS OF NLP\nIn English there is a near one to one correspondence between\nletters of the alphabet and glyphs used for rendering them. Words\nare simply sequences of letters and letters themselves are small in\nnumber. Words are written left to right by simply writing the\nletters one after the other. There is no need to compose letters\ninto any intermediate level of description. Fonts use one glyph\nper letter and there is no need for any elaborate composition. The\nglyphs are simply laid out in a line from left to right. Indian scripts\nare more complex. Glyphs must be composed in both horizontal\nan vertical direction - glyphs may need to be scaled and placed in\nvarious positions relative to each other - to the right, on the top,\nto the right-bottom, at the bottom, to the left-bottom etc. Thus\nunderstanding the concept of glyphs is very important.\nA few hundred glyphs are needed to eﬀectively render all pos-\nsible aksharas in a given Indian language. The glyphs are variable\nin size - not all characters are of the same width. Exactly what\nset of shapes to use is not ﬁxed by any standard. It is left to\nthe font designers to select the basic shape set they feel necessary\nmuch smaller than the number of words in the sentence. Finally,\nwe show how the localization of functional structure analysis to\nthe local domains of the respective clauses makes parsing in UCSG\nhighly eﬃcient on the whole.\nIn conclusion we see that the UCSG formalism has several\nmerits. Firstly, we see that the division of work into three in-\ndependent modules helps to discover, and hence exploit, what is\ncommon between a relatively free word order language like Tel-\nugu and a positional language like English. UCSG is equally well\nsuited for parsing positional languages and relatively free word or-\nder languages. In fact even cross serial dependencies as in Dutch\ncan be handled. Secondly, we get an elegant solution to the prob-\nlem of long distance dependencies. Thirdly, grammars can be writ-\nten more easily than with other grammar formalisms. Fourthly,\nwe note that eﬃcient parsers can be written. Lastly, and most\nimportantly, the division of labour into three modules that have\na very close correspondence with the three aspects of structure of\nhuman languages, will hopefully oﬀer new and useful insights into\nthe nature of our languages.\nUCSG grammars have been developed for English and a few\nIndian languages and applied for machine translation between En-\nglish and Indian languages. The UCSG system for English has\nbeen extended to a wide coverage robust partial parsing system\nby combining the basic architecture outlined above with statistical\nparsing techniques. The approach holds promise since grammars\ncan be developed without need for a parsed training corpus. See\nreferences given in the bibliography for more on UCSG.\nPartial Parsing\nAlthough a lot of work has gone into developing syntactic parsers,\nit has not been possible to achieve high performance on unre-\nstricted texts. Full syntactic parsers also have many limitations:\n1) exponential solution space with the attended computational\nineﬃciencies, 2) large and complex grammars 3) greater need for\n214\ncomplex sentences. Every elementary subtree, that is, a subtree\nthat includes just one node and its children, corresponds to one\napplication of a phrase structure rule. Thus trees are closer to\nphrase structure rules than to the structure of the sentences. We\nneed structural descriptions which separate out and vividly show\nlinear, hierarchical as well as functional structure inherent in the\ngiven sentence.\nTrees are not the most suitable structures for\ndepicting the structure of natural language sentences, although\nthey are widely used.\nWe will now brieﬂy sketch the merits and demerits of exist-\ning grammar formalisms. Linguistic grammar formalisms can be\nviewed as extensions of basic phrase structure grammars, more\nspeciﬁcally, Context Free Grammars (CFG). Let us ﬁrst under-\nstand the strengths and weaknesses of Context Free Grammars.\nThis would help us in understanding other grammar formalisms\nclearly.\nContext Free Grammars and Natural Languages\nIn the 1950s, there were major developments taking place in the\nﬁeld of computer science, including the development of high level\nprogramming languages. Grammars and parsers needed for com-\npiling these programs received a great deal of importance. In his\npapers in 1956 and 1959, Noam Chomsky laid out the foundations\nof formal properties of grammars. Chomsky classiﬁed grammars\nbased on string rewriting rules into four classes of formal com-\nplexity called Type 0 or Unrestricted Phrase Structure Grammar,\nType 1 or Context Sensitive Grammar, Type 2 or Context Free\nGrammar and Type 3 or Regular Grammar. Context Free Gram-\n2.2. COMPUTATIONAL LINGUISTICS\n183\nmars (CFG), the second least powerful in the Chomsky Hierarchy,\nwere found to be ideal for dealing with programming languages\neﬃciently. Since then a great deal of formal studies have been\nmade on CFGs within computer science and very eﬃcient pars-\ning algorithms have been developed. Linguists often fail to make",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-4",
                            "title": "Fonts, Glyphs and Encoding Standards",
                            "content": "limitations.\n• Of course if glyph encoding standards can be developed and\naccepted and used, issues of compatibility between fonts\nand character to font mappings can be taken care of once\nfor all.\n• The over all user experience depends on all the components\nof a text processing environment and their inter-compati-\nbility. Character encoding standards, fonts and rendering\nengines must all be developed carefully to provide the ap-\npropriate user experience on the whole. Quality of rendered\ntexts depend on the fonts, the glyph sets used in the font,\nthe mapping rules used and glyph selections made, and the\ndetails of the rendering engine. However, character encod-\ning is not directly concerned with the appearance of rendered\ntexts at all. Decisions on character encoding issues should\nnever be taken based solely on how rendered texts appear to\nthe eye.\nCharacter encoding has nothing to do with aes-\nthetics. All issues relating to character encoding standards\nmust be based solely on how the texts would be coded and\nhow text processing algorithms would treat these texts. Will\nthere be one or several ’spellings’ for a given word? Will a\ndefault sorting algorithm sort words in the same order as\nin a dictionary? How can we design simple, eﬃcient and\nuniform sorting methods that can sort texts in diﬀerent\nlanguages? If we carry out a type-token analysis on a cor-\npus will frequencies of words get counted correctly? What\nabout other statistical analyses such as n-grams or Markov\nanalysis? What happens when we transliterate texts from\none language to another? The ‘spelling’ of words is thus the\nonly important aspect that should dictate character encod-\ning issues.\nTo give a speciﬁc example, whether ‘kSa’ should be encoded\ndirectly in a character encoding standard such as ISCII or\nUNICODE should be decided solely on whether dictionar-\nies treat ‘kSa’ diﬀerently from the cluster formed from the\n294\nCHAPTER 2. FOUNDATIONS OF NLP\nconsonants ‘ka’ and ‘Sa’.\nand related fonts for Indian languages, UNICODE is slowly pick-\ning up and may become the dominant encoding scheme. Some\nthink switching to Unicode is inevitable. For some time at least\nISCII and UNICODE will co-exist as standard character encoding\nschemes for Indian languages. Migrating from ISCII to UNICODE\nwill anyway not be much diﬃcult.\nFrom Characters to Fonts:\nWe have seen that many commercial software packages encode\ndocuments in their own proprietary font encodings. A font en-\ncoded document is really not text at all. Fonts are made up of\nglyphs - graphic shapes which have no one-to-one correspondence\nwith any appropriate level of linguistic description. A glyph can be\na full vowel or a consonant but it can also be part of a consonant, a\nconsonant combined with a particular vowel maatra, just a vowel\nmaatra or any other arbitrary unit that is designed purely for the\nsake of graphical composition and rendering into aksharas. A dot\nthat appears in the middle of some aksharas in Telugu could be\na glyph. How can we interpret or process documents encoded as\nsequences of such arbitrary symbols? Even basic operations such\nas searching and sorting cannot be performed on such documents\nusing standard tools. Glyphs do not correspond to aksharas or to\nmore basic elements such as vowels ans consonants, glyphs can-\nnot be placed in a collating sequence that permits natural sorting.\nFont encoded documents are not texts at all. You may be able\nto type, compose and print documents using a proprietary com-\nmercial software but you cannot use such documents like normal\ntexts in other languages.\nAll texts, Indian language texts included, must be encoded in\nsome character encoding standard such as ISCII or UNICODE.\nBut unlike in the case of English, an ISCII encoded text cannot\nbe directly displayed or printed because ISCII encodes texts in\nterms of sound units, not shape units required for visual rendering.\nISCII is designed for the ears, not eyes. We have already seen that\ncoding standards such as ISCII or UNICODE. We need to grow\nbeyond using computers merely as type-writers. Of what use is\na text which is encoded in some non-standard, proprietary, secret\nsystem of codes? Vast amounts of texts in Indian languages are\nbeing developed for some purpose or the other and in most cases\nthe documents are in electronic form at some stage or the other.\nAll this eﬀort goes waste when it comes to re-using these texts for\nany research and development activity.\nThus there is a need to develop a simple, uniform and eﬃcient\nscheme for mapping ISCII encoded pages into a selected font for\nthe purposes of visualization.\nThe scheme should be provably\ncomplete and consistent. It should be easy to adapt the scheme\ndeveloped for one font for other fonts with minimal eﬀort.\nConverting from ISCII to font is best done at the level of ak-\nsharas. ISCII deﬁnes the script grammar and texts can be easily\nbroken into aksharas using this grammar. Each akshara must then\nbe mapped to the appropriate glyph sequence. Note that there\nis no grammar for fonts and hence mapping from font encoded\ndocuments back into ISCII would be a bit more complex. In the\ncase of complex scripts such as Telugu, a non-deterministic solu-\ntion requiring search may be necessary. In any case, it would be\nmuch better to map between ISCII and fonts rather than attempt\nto map directly from one font to another.\nDirect font to font\nconverters are ad-hoc solutions, not generic, one-time, re-usable\nmodules.\nThings are changing slowly now. Standard text editors are\nbecoming available. People have started demanding more. Text\nprocessing applications will get widely used. Ministry of Infor-\nmation Technology, Government of India, is a voting member of\nthe UNICODE consortium and can inﬂuence decision making in\nUNICODE. UNICODE is still in the making. It has hardly come\nto be used for Indian languages in any signiﬁcant way. This is the\nsible aksharas in a given Indian language. The glyphs are variable\nin size - not all characters are of the same width. Exactly what\nset of shapes to use is not ﬁxed by any standard. It is left to\nthe font designers to select the basic shape set they feel necessary\nto render the various aksharas eﬃciently and aesthetically. Font\ndesigners design glyphs based on the ease of composition into vari-\nous possible aksharas. Further, to make the aksharas so composed\nlook neat and aesthetic, it becomes necessary to develop several\nequivalent glyphs and use the appropriate ones based on the con-\ntext. For example, a glyph to represent a particular vowel maatra\nmay come in several shapes, sizes and locations - one for narrow\nconsonants, one for wide consonants, one for some other complex\ncombination of consonants, etc. Diﬀerent fonts for the same lan-\nguage may, and often do, diﬀer in terms of the glyph sets they\nemploy. Fonts also vary in terms of the position in the code table,\nthat is, what numerical codes are given to each of the glyphs they\nencompass. Each font uses a possibly diﬀerent set of glyphs and\npositions them in possibly diﬀerent ways in the code table. In fact\nthere is no glyph encoding standard. There is even some opposi-\ntion to the idea of developing a glyph encoding standard fearing\nthat such a standard may curtail the creativity of font designers\nin producing aesthetic fonts.\nThis total lack of standardization at the level of glyphs and\nfonts has far reaching consequences. For example, simply select-\ning a piece of text and changing the font can render the text junk.\nFonts have been treated like pieces of art and they remain propri-\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n281\netary, non-standard and incompatible with one another. Of late\nthere is some realization that standards are useful if not essential\nand there is hope that some sort of standards will develop and get\nwidely accepted. Tamil is perhaps the only language where there\nmation Technology, Government of India, is a voting member of\nthe UNICODE consortium and can inﬂuence decision making in\nUNICODE. UNICODE is still in the making. It has hardly come\nto be used for Indian languages in any signiﬁcant way. This is the\nright time to understand all the issues carefully and take the right\nsteps in the right direction.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n291\nIssues in Character Encoding Standards:\nThere is a lot of confusion regarding character encoding standards\nand their relation with fonts, keyboard drivers, rendering engines\netc.\nPeople think in terms of what they see on the computer\nscreen or on a printed paper.\nSeeing is not believing when it\ncomes to encoding standards. The reality is not what appears on\nthe screen but what is actually stored in the ﬁle. We have already\nseen that ISCII encodes sound units and thus an ISCII document\nis independent of language and script used, let alone fonts. What\nwe see on the screen is not the main concern here. We must think\nin terms of what is stored in ﬁles and how to process such text\nﬁles. The following points are included here to clearly bring out\nthe major concerns in designing character encoding standards for\nIndian languages.\n• Since character encoding schemes deal with scripts, inherent\nproperties of scripts must be part of character encoding\nstandards themselves. They cannot be relegated to the level\nof fonts or software implementations. One such issue is the\nscript grammar. There are other important issues as well.\nFor example, Telugu and Kannada scripts diﬀer from say,\nDevanagari script in terms of how consonant clusters are\nrepresented. To represent a C1C2 cluster, Devanagari uses\na half C1 and a full C2 shapes whereas Telugu and Kannada\nwould use a full C1 and a half C2 shape. The best place to\nmake this assertion is at the script level.\n• Further, although the relation between languages and scripts\nis not necessarily one-to-one and character encoding schemes",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-5",
                            "title": "Character Encoding Standards",
                            "content": "mation Technology, Government of India, is a voting member of\nthe UNICODE consortium and can inﬂuence decision making in\nUNICODE. UNICODE is still in the making. It has hardly come\nto be used for Indian languages in any signiﬁcant way. This is the\nright time to understand all the issues carefully and take the right\nsteps in the right direction.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n291\nIssues in Character Encoding Standards:\nThere is a lot of confusion regarding character encoding standards\nand their relation with fonts, keyboard drivers, rendering engines\netc.\nPeople think in terms of what they see on the computer\nscreen or on a printed paper.\nSeeing is not believing when it\ncomes to encoding standards. The reality is not what appears on\nthe screen but what is actually stored in the ﬁle. We have already\nseen that ISCII encodes sound units and thus an ISCII document\nis independent of language and script used, let alone fonts. What\nwe see on the screen is not the main concern here. We must think\nin terms of what is stored in ﬁles and how to process such text\nﬁles. The following points are included here to clearly bring out\nthe major concerns in designing character encoding standards for\nIndian languages.\n• Since character encoding schemes deal with scripts, inherent\nproperties of scripts must be part of character encoding\nstandards themselves. They cannot be relegated to the level\nof fonts or software implementations. One such issue is the\nscript grammar. There are other important issues as well.\nFor example, Telugu and Kannada scripts diﬀer from say,\nDevanagari script in terms of how consonant clusters are\nrepresented. To represent a C1C2 cluster, Devanagari uses\na half C1 and a full C2 shapes whereas Telugu and Kannada\nwould use a full C1 and a half C2 shape. The best place to\nmake this assertion is at the script level.\n• Further, although the relation between languages and scripts\nis not necessarily one-to-one and character encoding schemes\nlimitations.\n• Of course if glyph encoding standards can be developed and\naccepted and used, issues of compatibility between fonts\nand character to font mappings can be taken care of once\nfor all.\n• The over all user experience depends on all the components\nof a text processing environment and their inter-compati-\nbility. Character encoding standards, fonts and rendering\nengines must all be developed carefully to provide the ap-\npropriate user experience on the whole. Quality of rendered\ntexts depend on the fonts, the glyph sets used in the font,\nthe mapping rules used and glyph selections made, and the\ndetails of the rendering engine. However, character encod-\ning is not directly concerned with the appearance of rendered\ntexts at all. Decisions on character encoding issues should\nnever be taken based solely on how rendered texts appear to\nthe eye.\nCharacter encoding has nothing to do with aes-\nthetics. All issues relating to character encoding standards\nmust be based solely on how the texts would be coded and\nhow text processing algorithms would treat these texts. Will\nthere be one or several ’spellings’ for a given word? Will a\ndefault sorting algorithm sort words in the same order as\nin a dictionary? How can we design simple, eﬃcient and\nuniform sorting methods that can sort texts in diﬀerent\nlanguages? If we carry out a type-token analysis on a cor-\npus will frequencies of words get counted correctly? What\nabout other statistical analyses such as n-grams or Markov\nanalysis? What happens when we transliterate texts from\none language to another? The ‘spelling’ of words is thus the\nonly important aspect that should dictate character encod-\ning issues.\nTo give a speciﬁc example, whether ‘kSa’ should be encoded\ndirectly in a character encoding standard such as ISCII or\nUNICODE should be decided solely on whether dictionar-\nies treat ‘kSa’ diﬀerently from the cluster formed from the\n294\nCHAPTER 2. FOUNDATIONS OF NLP\nconsonants ‘ka’ and ‘Sa’.\nencoding standard such as ASCII. Letters of the alphabet, along\nwith other special symbols such as punctuation marks, numerals,\nparenthesis, quote marks etc, are called characters.\nA charac-\nter encoding scheme speciﬁes the allowed set of characters and\nplaces them in a particular order. This order deﬁnes the collating\nsequence. Usually collating sequence is so selected that sorting\nin alphabetical order becomes straight forward. Each position is\ngiven a numerical code and the characters are represented inside\ncomputers by these numbers.\nThus the character ’A’ is repre-\nsented as number 65 in ASCII. Since there is a one-to-one corre-\n282\nCHAPTER 2. FOUNDATIONS OF NLP\nspondence between letters and glyphs, fonts can also use the same\nnumerical codes for placing the corresponding glyphs. Thus ’A’\ncan be represented as 65 in ASCII character encoding standard\nas also in any of the fonts. This makes the whole scheme very\nsimple and neat. The encoded text remains the same irrespective\nof which fonts are used.\nWe need a character encoding scheme for Indian languages\ntoo. But we must ﬁrst understand what we mean by a character.\nAksharas are the basic units of writing but there are too many\nof them. We cannot encode each possible akshara by giving it a\nnumerical code. We must get into more basic units and encode\nthem.\nThus aksharas will need to be composed of more basic\nunits. The script grammar we have seen above forms an excellent\nscheme to deﬁne a character encoding standard.\nAny character encoding scheme for Indian language scripts\nmust ideally deﬁne a script grammar and implement it by spec-\nifying the numerical codes for the consonants, vowels and other\nsymbols used in the grammar. In spite of the fact that the num-\nber of valid aksharas is potentially inﬁnite and practically very\nlarge, a script grammar makes it possible to encode texts using\na fairly small number of diﬀerent codes. The texts encoded in\nsuch a scheme would be a sequence of such numeric codes. The\nletter ‘A’ is 65 irrespective of which font is used for display. It\nis the rendering engine which actually constructs the appropriate\nimages on the computer screen or in a print-out as required.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n271\nA character encoding scheme speciﬁes the character set, im-\nposes a default collating sequence on the characters in the set and\nmaps these characters to numerical codes. Text ﬁles simply con-\ntain these codes. Only when displayed or printed by a rendering\nengine using appropriate fonts will you start seeing the content of\nsuch text ﬁles in a readable form.\nThings are diﬀerent in Indian languages. The very notion of\na character is diﬀerent. A thorough understanding of the basics\nof characters and character encoding schemes is invaluable. Let\nus start by looking at the alphabets.\n2.4.2\nThe Alphabet\nEnglish uses an alphabetic writing system. There are 26 letters\nin the alphabet and all words in English language are simply se-\nquences of these 26 letters. Knowing A-B-C-D is not enough to\nread English - you must also learn spellings. Spelling rules are\nquite arbitrary. If P-U-T is put why is B-U-T but? One letter is\nused to give diﬀerent sounds and diﬀerent letter combinations are\nused to give the same sound: (fat - fate, met - mete, kit - kite,\nput - but, case - chess - cess), (meet - meat - mete, bore - boar).\nBernard Shaw once remarked that “ﬁsh” can be spelled as “goat”\nin English, by borrowing the ’g’ for the ’f’ sound from ’rough’, ’o’\nfor the ’i’ sound from ’women’ and ’t’ for ’sh’ from ’station’!\nSpelling based alphabetic writing systems necessitate an ad-\nditional level of processing in many applications. For example, a\nText-To-Speech system must have the rules to map the spellings\nto sounds so that texts can be read out correctly.\nOn the other hand, every nursery kid in India starts by a sys-\ntematic study of phonetics, the science of sounds. The equivalent\nof the alphabet chart, called varNamaala shows a systematic and\nvendor. All issues of standardization must be made very\ncarefully so that they are applicable across hardware and\nsoftware platforms. This is very important when develop-\ning software tools and even more so when developing fonts,\nsince there are several diﬀerent technologies used for speci-\nfying font shapes (Ex. ttf, metafont, open-type.\n• Revising the UNICODE standard is all the more critical\nsince the UNICODE consortium does not allow characters\nto be deleted or their primary properties altered as a matter\nof policy and principle. A mistake committed once will stay\nfor ever. It is better to go slow on contentious addition of\nnew symbols.\n• A character encoding scheme automatically speciﬁes the de-\nfault sorting order - words get sorted in the order of the\nnumerical codes used to represent them. Hence every care\nmust be taken to ensure that the positions alloted for the\nvarious characters and special symbols give us the correct\nsorting order. However, we need not go too far on this - uni-\nversal sorting algorithms exist and the sorting order can be\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n299\neasily speciﬁed through map ﬁles to meet our exact require-\nments even if the default sorting order is not acceptable.\n• Sounds are more basic than orthography (the writing sys-\ntem). We learn to speak words much before we start read-\ning and writing. Speaking is learnt naturally, writing - with\nconscious eﬀort. Sounds are basic, natural, intrinsic prop-\nerties of a languages. Orthography is an artifact. Not all\nare able to read and write - there are illiterates too. Or-\nthography is at best an approximate mapping of the sounds\nthat we use in a language onto a graphical format, even in\nIndian languages which are basically phonetic in nature.\nCharacter encoding schemes are best designed based on the\nstructure of sounds in a language rather than based on the\nway words are written. The varNamaala reﬂects an arrange-\nment of characters based on the nature of sounds. ISCII is",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-6",
                            "title": "Romanization",
                            "content": "2.4. INDIAN LANGUAGE TECHNOLOGIES\n303\nindividual journalists and editors. Advertisements are worse. It\ntakes a substantial amount of time, patience as well as knowledge\nof the language concerned to understand these transliterations.\nThe problem is complicated by the fact that there are many lan-\nguages and we need to perform language identiﬁcation as well.\nThere are several issues in the design of a good Romain translit-\neration scheme. Some systems are case sensitive while others are\nnot. Thus mixing of upper and lower cases has its own merits and\ndemerits. This mixing also makes the text look a bit awkward as\nupper case letters generally appear bigger. Some researchers have\nworried about storage space and suggest the use of upper case\nletters even for long vowels. The IIT(Kanpur) group has taken\nthis idea of economy of representation to the extreme and they\npropose highly unnatural mappings such as ’w’ for the ’th’ sound\nas in ’thesis’ and ’x’ for the ’d’ sound as in ’Daddy’. A few bytes\nmight be saved but the texts now look really ugly and diﬃcult to\nread. Also, keying in letters of diﬀerent cases requires frequent\nuse of SHIFT or CAPS-LOCK keys. Some use ’aa’ for long ’a’.\nThen we might expect that the same logic be universally applied\nand we use ’ii’ for the long vowel sound as in ’feet’ and ’ee’ for\nthe long vowel sound in ’ate’. However, while ’ee’ is frequently\nused in English spellings, ’ii’ is never used. Some argue that we\nmust use those sequences which are more natural and frequent in\nEnglish while others are upset that English spellings should form\nthe basis for writing Indian languages. English spelling rules are\nghastly and English phone set is diﬀerent from the set we use for\nour languages. English spellings cannot answer all our concerns.\nYet readability is important too. Any scheme that is systematic\nand uniform is more likely to be accepted and picked up easily\ncompared to ad-hoc schemes. Why should Romanization of In-\nup with a good standard, widely accepted scheme for Roman\ntransliteration. There are several schemes but most people use\nsome ad-hoc scheme of their own creation. Let us understand the\nnature of the task ﬁrst.\nIt must be recognized at the outset that English is an alpha-\nbetic writing system based on the 26 letters of the alphabet and\nspelling rules while Indian scripts are phonetically based and syl-\nlabic in nature. Even if we were to count the basic units such\nas vowels and consonants instead of full aksharas, we ﬁnd that\nthe numbers are larger than the 26 letters we have. One common\nsolution is to use diacritic marks but this scheme is inconvenient\nfor direct use on the computer. Typing in diacritic marks from\nthe computer keyboard is not convenient. Another common so-\nlution is to mix lower case and upper case letters.\nIn English\ncapital letters are used for proper names and acronyms as also to\nmark the beginning of a sentence. Here we need to use capital\nletters instead for denoting retroﬂex consonants etc. Multi-byte\nequivalents are also required as there are more symbols in Indian\nscripts than 52. We need to handle long and short vowels, aspi-\nrated and unaspirated consonants, retroﬂex sounds, etc. Based\non these ideas, several schemes have been proposed including the\niTRANS scheme and the Rice University scheme.\nThe worst part of the story is that people use completely\nad-hoc transliterations without consistently sticking to any one\nscheme. Each person seems to have his/her own standard. News-\npapers need to transliterate Indian languages into Roman on a\ndaily basis but each time it is left to the whims and fancies of\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n303\nindividual journalists and editors. Advertisements are worse. It\ntakes a substantial amount of time, patience as well as knowledge\nof the language concerned to understand these transliterations.\nThe problem is complicated by the fact that there are many lan-\nin regular use in some languages while in others they are\nnot used these days. A careful decision must accordingly\nbe taken. Roman numerals must be permitted in any case.\nFrom these discussions, it must be clear that there are con-\nfusions and unresolved issues. Users must learn to live with con-\nfusions and imperfect solutions for some time. All the teething\nproblems are being addressed and hopefully we will be able to use\nIndian languages on computers as simply and as easily as English.\n2.4.6\nRomanization\nWe have seen examples of Sanskrit, Hindi, Telugu and Kannada\nwords written using the Roman alphabet.\nThe unique multi-\nlingual environment in India makes it necessary to write one lan-\nguage using the script of another language. Sanskrit is often writ-\nten in the scripts of local languages. For example, the verses from\n302\nCHAPTER 2. FOUNDATIONS OF NLP\nbhagavadgiita may be given along with the commentaries writ-\nten in Telugu language, both of which are written in the Telugu\nscript for the beneﬁt of the Telugu knowing people. Now that\nEnglish has become a very important link language, it also be-\ncomes necessary to write Indian languages in the Roman script.\nEnglish newspapers carry advertisements of Hindi movies. The\nmovie names etc. are naturally written in the Roman script. Thus\nthere is a regular, routine need for writing Indian languages in the\nRoman script, not just for those special meta occasions when we\nare talking about languages per se. An Englishman may never\neven think of writing English in Kannada script but it is natural\nand essential to write Kannada in the Roman script.\nHowever, there seem to be no serious concerted eﬀort to come\nup with a good standard, widely accepted scheme for Roman\ntransliteration. There are several schemes but most people use\nsome ad-hoc scheme of their own creation. Let us understand the\nnature of the task ﬁrst.\nIt must be recognized at the outset that English is an alpha-\nother foreign language words into our languages. They are\nuseful for distinguishing between cases like ‘cat’, ‘caught’\nand ‘cot’ when transliterated into Indian scripts. If we have\nto accommodate these new codes at all, we must at least do\nit uniformly for all Indian languages. We must uniformly\nadd these new characters AYE and AWE into the UNI-\nCODE code table for each of the Indian languages and add\nannotations saying that these characters shall be used only\nfor transliterated foreign words and they shall never be used\nfor native words.\nWe must however be careful to understand that ordinary\nusers may not read or follow the annotations in code ta-\nbles and other documentation associated with standards.\nThey just use whatever suits them from the set of available\ncodes. Thus there is always an element of risk of proliferat-\ning multiple ‘spellings’ for words. This would surely happen\nfor quite some time, since many languages already have well\nestablished conventions. In Kannada, for example, we write\n‘kyaat’, ‘byaank’, and ‘myaap’ to depict the English words\n‘cat’, ‘bank’ and ‘map’. Diﬀerent languages have diﬀerent\nsets of conventions. Some of these have been in use for a\nlong time and well stabilized. While we may hope that in\nfuture people will shift over to the new characters provided,\nthere is no guarantee that the intended shift will take place\ncompletely. Hence decisions to add new characters must be\ntaken with utmost care. If something can be avoided, not\nadding it is the best.\n• It will be a good idea to place all special symbols in a sep-\narate code space rather than duplicate them in the code\ntable for each language. Also, the space available within\nthe code tables for speciﬁc languages may not be suﬃcient\nto incorporate all the required special symbols. Punctua-\ntion marks, diacritics and many special symbols are used\nin all or most languages and are therefore best treated in-\ndependent of any speciﬁc language. ISCII does not disturb\nour languages. English spellings cannot answer all our concerns.\nYet readability is important too. Any scheme that is systematic\nand uniform is more likely to be accepted and picked up easily\ncompared to ad-hoc schemes. Why should Romanization of In-\ndian scripts be ad-hoc just because English happens to be ad-hoc?\nIn the long run, people will ﬁnd it easier to get used to a system-\natic, uniform scheme than a hoch-poch scheme based on a foreign\nlanguage. Should English knowing people decide things for Indian\nscripts or should we keep Indians speaking Indian languages and\nknowing Indian scripts as the base? Some have proposed the use\nof special symbols such as the colon to suggest vowel lengthen-\ning but others say special symbols will interfere with processing\nof such texts. Whether we use special symbols or not, there will\nalways be a need for an escape mechanism so that literal inter-\n304\nCHAPTER 2. FOUNDATIONS OF NLP\npretations can be permitted. Some are worried about single or\neven round trip transliteration among various scripts. With the\nincreasing use of multi-lingual and multi-script documents, prob-\nlems will only increase. It is high time a well thought out standard\nis prepared and brought into widespread use.\nIn the absence of a single widely accepted and widely known\ntransliteration scheme, in this book we have avoided taking a\nstrong position based on any one ideology and used what we con-\nsider a fairly simple and straight forward scheme in this book. It\nis generally acceptable to add a ‘h’ to indicate aspirated sounds\nand use capitals for retroﬂex sounds. Long vowels have been in-\ndicated by doublets. Proper names and sentence initial words are\nnot capitalized.\n2.4.7\nSpell Checkers\nSpell checkers form one of the most basic technologies for any given\nlanguage. It may be surprising then, that good spell checkers are\nnot available for many Indian languages even today. Let us see\nwhy. We take examples from Kannada language. We start with",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-7",
                            "title": "Spell Checkers",
                            "content": "not capitalized.\n2.4.7\nSpell Checkers\nSpell checkers form one of the most basic technologies for any given\nlanguage. It may be surprising then, that good spell checkers are\nnot available for many Indian languages even today. Let us see\nwhy. We take examples from Kannada language. We start with\ngeneral issues in the design of spell checkers.\nSpelling Error Detection and Correction\nA spell checker is a computer program that deals with the detec-\ntion and correction of spelling errors in texts. An ideal detector\nshould pin point all and only those words in the text that are\nwrongly spelled. An ideal corrector should automatically correct\nall such errors. In practice, neither detection nor correction can\nbe perfect. Most practical spell checkers do not even attempt to\nautomatically correct spelling errors - instead they only oﬀer list\nof suggested alternatives for the user to choose from. The user\nhas several options: he may select one of the suggestions provided\nby the system, he may direct the program to accept the speciﬁc\ninstance or even all instances of current spelling in the text, he\nmay direct the system to add this “new” word to the custom dic-\ntionary, or he may choose to edit the word manually.\nA good spell checker must detect all or most of the spelling\nerrors and at the same time minimize false alarms. It must oﬀer\neither the single right suggestion or a small set of suggestions for\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n305\ncorrection in which the right suggestion is included. The sugges-\ntions oﬀered must be ranked and the right suggestion must occur\nin the top of the list in most cases.\nA good spell checker may be required to handle loan words,\nsplit and merged tokens, archaic usage, dialectal variations, ac-\ncepted cases of spelling variations, colloquial forms, domain spe-\nciﬁc terminology, acronyms and abbreviations, proper names etc.\nClearly no practical spell checker can be expected to handle all\nthese cases perfectly.\nsplit and merged tokens, archaic usage, dialectal variations, ac-\ncepted cases of spelling variations, colloquial forms, domain spe-\nciﬁc terminology, acronyms and abbreviations, proper names etc.\nClearly no practical spell checker can be expected to handle all\nthese cases perfectly.\nMost practical spell checkers work one word at a time and\nhence cannot even detect real word errors - mistakes that result\nin some other valid word in the language, as against non-word\nerrors - mistakes that result in an invalid word.\nFor example,\nmost spell checkers will not ﬁnd anything wrong in the sentence\n“One plus one is tow”. Catching real word errors requires context\nbased processing.\nIdeally, full syntactic parsing, semantic and\npragmatic analysis may be required. The context must somehow\nbe brought to bear to decide whether there is any spelling error\nor not. Both the textual context in which the word occurs and\nthe situational context and topic of the discourse are important.\nPractical spell checkers are far from ideal.\nDetection and correction can both be done using dictionaries\nand morphology and other linguistic tools, using statistical tech-\nniques, or using a combination of both.\nIn a dictionary based\napproach, a word not found in the dictionary is considered to\nbe due to a spelling error and other words from the dictionary\nwhich are close to the input word in terms of spelling are given\nout as suggestions for correction. In a simple statistical approach,\nthe probability of a particular alphabet/word sequence in the lan-\nguage is used instead as the basis. Interested readers may look at\na survey paper by Karen Kuckich for a variety of techniques for\ndetection and correction of spelling errors.\nA large dictionary is also not necessarily the best. A large\ndictionary includes many infrequent words which may be confused\nfor other words with a real-word spelling error. A user who wanted\nto type ‘leave’ actually types ‘lave’ by mistake but the system\nSimilarly, ‘huugalannu’ can be corrected as ’huugaLannu’. Com-\nplexity increases if there are multiple errors in a word. However,\nmultiple errors are relatively less frequent.\nThe block diagram below gives the overall structure of a typi-\ncal spell checker for a morphologically rich language such as Kan-\nnada. The techniques used here can also be applied fruitfully for\nother languages. The morphological analyzer and generator can\nbe implemented in the Network and Process model.\nAs we have seen, morphological analysis and generation are\ncomplex processes and the performance of current systems is not\nvery high. We may therefore wish to apply hybrid techniques.\nType-token analysis is performed on a large and representative\ncorpus and the most frequent word forms are identiﬁed. A suitable\nthreshold can be determined which optimizes the spell checker\nperformance in terms of both false alarms and missed detections.\nHigh frequency word forms can be directly stored in the dictionary\nand morphology applied only in those cases where the word is not\nlisted in the dictionary. There are many advanced techniques for\nspell checker design. Interested readers will ﬁnd good papers.\n310\nCHAPTER 2. FOUNDATIONS OF NLP\nInput Text\nIdentify Erroneous Part(s)\nCorrection Algorithms\nMorphological Generator\nNot OK\nGet Next Word\nMorphological Analyser\nOK\nSuggestions        for Correcting Parts\nMake Corrections\nand Correction System\nIsolated Word  Error Detection\nCorrected Text\nFIG 2.21 A Typical Dictionary-Morphology based Spell\nChecker\nThere are no bench mark data for testing and evaluating the\nperformance of spell checkers in many Indian languages.\nMost\nof the spell checkers available today have not been thoroughly\nevaluated.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n311\n2.4.8\nOptical Character Recognition\nAn Optical Character Recognition (OCR) system converts a scann-\ned image of a text document into electronic text just as if the text\nmatter was typed-in by somebody.\nScanned images are much\non everybody. That is not the point. The idea is two fold. Firstly,\n308\nCHAPTER 2. FOUNDATIONS OF NLP\na spell checker must provide options for the user to normalize the\ngiven texts as per his/her requirements. If a user wants all col-\nloquial forms to be replaced with formal varieties, a spell checker\ncan be designed to assist in the process. A user must be able to\nspecify which dialects are to be accepted and which ones are to\nbe treated as errors and substituted. Many of our languages have\na long history and there are archaic forms and modern forms. If\na user desires that he better avoid archaic forms, a spell checker\nmust assist him/her in that. Secondly, unwanted variability in the\ntext can be reduced by normalization of spellings. This will be\nvery useful for text processing applications in Indian languages.\nSpell checkers for Indian languages can and must be much more\nthan the usual spell checkers for other languages.\nSpell Checking in Morphologically Rich Languages:\nAlthough English has a rich and complex system of derivational\nmorphology, inﬂectional morphology is quite simple and straight\nforward. Most spell checkers for English therefore store the de-\nrived forms directly in the lexicon and apply rules of morphology\nonly for a few cases where the rules are simple and highly produc-\ntive. This approach is practically feasible and reasonably eﬃcient\nfor languages such as English.\nMorphological analysis and generation in Indian languages is\nquite an involved and complex task. Often there are six or seven\nlevels of aﬃxation, there being several possible aﬃxes at each level\nThus tiMduhaakiddanu ((he) had eaten) can be analyzed as\ntinnu\ni\nhaaku\ni\niru\nid\nanu\nRt:(eat)\npt-part.\nasp.aux.\npt-part.\nPerf.\nPt\nm,sl,p3\nIn languages such as Kannada, a verb root may give rise to\nseveral hundred thousands of complete words.\nDravidian lan-\nguages, especially Kannada and Telugu, are among the most com-\nplex languages of the world, comparable only to languages such\nalmost every activity in computational linguistics and NLP - word\nprocessing and text critiquing systems, spelling error detection\nand correction, grammar checking, oﬃce automation, morpholog-\nical analysis and synthesis, parsing and generation, machine trans-\nlation, question answering systems, story understanding, natural\nlanguage interfaces to databases, computer aided instruction, in-\nformation retrieval, speech synthesis, speech recognition, auto-\nmatic indexing and abstracting, concordance and other statistical\nanalyses, vocabulary studies, stylistics, psycholinguistic studies,\ntaxonomical studies etc.\nWe have seen that electronic dictionaries can be very useful\nfor people as also for NLP applications. Yet another dimension\nof the relationship between dictionaries and computers is the use\nof computers in developing dictionaries. Dictionary development\nis a huge and complex task requiring great skill and expertise on\nthe part of the lexicographer. Classical dictionaries have taken\ndecades to develop. At one point of time words used to be writ-\nten down on cards, cards sorted manually, then word frequencies\ncounted manually and duplicate cards removed and so on. Today\ncomputers provide assistance at practically every stage of dictio-\nnary development. To give an idea of this, we may start with large\nand representative collections of electronic texts, extract words\nand perform a type-token analysis (each distinct word form is a\ntype and each occurrence of a type is a token), perform morpho-\nlogical analysis to extract root words, select words, identify parts\nof speech (POS) using POS tagged corpora or through morpholog-\nical analysis, use a KWIC (Key Word in Context) Concordance\nprogram to extract sentences containing a given key word and\ndecide meanings etc.\nfrom these sentences, select examples of\nusage, and format the entries in the dictionary as required. All\nthese steps can be done semi-automatically.\nThe machine per-",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-8",
                            "title": "Optical Character Recognition",
                            "content": "jor tasks performed in preprocessing phase. Separation of text\ninto glyphs, characters, words and lines, and recognition of indi-\nvidual glyphs are tasks of the recognition stage. Post-processing\ncomprises combining the recognized glyphs into valid aksharas\nand words, spell-checking, etc. Optical character recognition is a\nvast ﬁeld and there are a large number of alternative technolo-\ngies at every level. The following paragraphs sketch some of the\nsimple techniques that have been eﬀectively used in OCR. The\ntreatment here is not intended to be comprehensive or suggestive\nof the best or recommended methods. OCR is currently an active\n312\nCHAPTER 2. FOUNDATIONS OF NLP\narea of research and the interested readers will ﬁnd a vast body\nof literature giving more technical details.\nPreprocessing Stage\nBinarization: refers to the conversion of a scanned gray level im-\nage into a two-tone or binary (pure black and white) image. A\nbinary image is appropriate for OCR work as the image docu-\nment contains only two useful classes of data — the background,\nsay the paper, and the foreground, the printed text. It is common\nto represent the background paper colour by white-coloured pixels\nand the text by black-coloured pixels. In image processing jargon,\nthe background pixels have a value of 1 and the foreground pixels\nhave a value of 0. Binarization has a signiﬁcant impact because it\nprovides input to every other stage of an OCR system. All pixels\ndarker than a threshold are mapped to pure black and the rest\nof the pixels are mapped to pure white. Several strategies can\nbe used for binarization to achieve desired performance on diﬀer-\nent types of scanned documents and scanners. Global, percentile\nbased and iterative methods have been applied to identify the best\nthreshold.\nSkew Detection and Correction: stages deal with improper\nalignment of a document while it is scanned. The usual eﬀect of\nskew could be that the lines of text or no longer horizontal but at\nevaluated.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n311\n2.4.8\nOptical Character Recognition\nAn Optical Character Recognition (OCR) system converts a scann-\ned image of a text document into electronic text just as if the text\nmatter was typed-in by somebody.\nScanned images are much\nlarger in size compared to corresponding text ﬁles.\nThe state-\nment “A picture is worth one thousand words” is literally true\nhere. Texts occupy less storage space and less network bandwidth\nwhen sent across a network. Converting images into texts makes\nit possible to edit and process the contents as normal text. OCR\nsystems are therefore very useful.\nOCR systems can be used to convert available paper docu-\nments into electronic texts without typing.\nSince OCR engine\ncan be run day and night on several computers parallelly, we can\ngenerate large scale corpora with less time and eﬀort. OCR en-\ngines can also be used for a variety of other applications. OCR\nsystems have just started appearing for Indian scripts. Most of\nthe current OCR systems for Indian languages are designed only\nfor printed texts and perform well only on reasonably good quality\ndocuments. Research work is going on for handling hand-written\ndocuments. Handling old manuscripts is more complex. The pa-\nper or other base materials used would have deteriorated, coloured\nand noisy. Also the character shapes used may be more complex\nand quite diﬀerent from the shapes used in modern digital fonts.\nSystem Overview\nAn OCR system typically contains three stages: preprocessing\nstage, recognition stage and post-processing stage. Binarization,\nseparation of image regions into textual and graphical regions,\nmulti-column detection and skew correction are some of the ma-\njor tasks performed in preprocessing phase. Separation of text\ninto glyphs, characters, words and lines, and recognition of indi-\nvidual glyphs are tasks of the recognition stage. Post-processing\ncomprises combining the recognized glyphs into valid aksharas\ninto rectangular regions using vertical and horizontal projection\nproﬁles alternately.\nThe use of horizontal and vertical projection proﬁles for all\nthe major preprocessing tasks minimizes system complexity and\nallows faster processing of documents.\nRecognition Stage\nLine, Word, Character and Glyph Separation: is a very impor-\ntant task as the recognition engine processes only one glyph at\na time.\nWord and glyph separation are the key steps.\nIn one\nrecent successful system, word segmentation has been done using\na combination of Run-Length Smearing Algorithm (RLSA) and\nConnected-Component Labelling. Words are combined into lines\nusing simple heuristics based on their locations. The performance\nof RLSA in accurately segmenting words is very high on good qual-\nity text but drops in the presence of complex layouts and tightly\npacked text that is sometimes seen in magazines. A variety of zon-\ning techniques have also be used. Words can be decomposed into\nglyphs by running the connected component labelling algorithm\nagain. The method is conceptually simple and glyph separation\ncan be very accurate.\nRecognition: There are broadly two approaches to recognition\n- template matching and classiﬁcation based on features. Direct\nmatching rarely works but reﬁned template matching algorithms\ncan actually give fairly good recognition performance. One such\ntemplate matching technique that has been used very success-\nfully is based on the notion of fringe distance where the distance\nbetween two images is a function of the distance of each black\npixel in one image to the nearest black pixel in the other image.\nA database is created from standard glyph shapes and any new\nglyph image can be matched against the templates stored in the\ndatabase and the reference template that matches best could be\n314\nCHAPTER 2. FOUNDATIONS OF NLP\ngiven out as the recognized output.\nThe two images need to be scaled and normalized before such\na comparison can be made. We may use a linear scaling algo-\nbased and iterative methods have been applied to identify the best\nthreshold.\nSkew Detection and Correction: stages deal with improper\nalignment of a document while it is scanned. The usual eﬀect of\nskew could be that the lines of text or no longer horizontal but at\nan angle, called the skew angle. Documents with skew cause line,\nword and character breaking routines to fail. Skew also causes\nreduction in recognition accuracy. Skew detection and correction\ncan be done by, say, maximizing the variance in horizontal pro-\njection proﬁle.\nText and Graphics Separation: refers to the process of identify-\ning which regions of the document image contain text and which\nregions contain pictures and other non-text information that is\nnot relevant to the OCR system. Horizontal and vertical projec-\ntion proﬁles can be used for such separation as well as for many\nother preprocessing operations (see below). A horizontal proﬁle\nis obtained by counting and plotting the number of text or black\npixels in each row of the image. A vertical proﬁle is obtained by\ncounting the black pixels in each column of the image. Horizontal\nproﬁles show distinct peaks that correspond to lines of text and\nvalleys that result from inter-line gaps. A line of text is revealed\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n313\nby a peak in the horizontal proﬁle whose width is approximately\nthe font size. A graphic object, in contrast, is much larger. The\nactual shape of the peak is also diﬀerent because of higher den-\nsity of black pixels in a graphics block. Thus, the proﬁle shapes\ndiscriminate between text and graphical blocks.\nMulti-column Text Detection: can be done using Recursive X-\nY Cuts technique. It is based on recursively splitting a document\ninto rectangular regions using vertical and horizontal projection\nproﬁles alternately.\nThe use of horizontal and vertical projection proﬁles for all\nthe major preprocessing tasks minimizes system complexity and\nallows faster processing of documents.\nRecognition Stage\ndatabase and the reference template that matches best could be\n314\nCHAPTER 2. FOUNDATIONS OF NLP\ngiven out as the recognized output.\nThe two images need to be scaled and normalized before such\na comparison can be made. We may use a linear scaling algo-\nrithm that uniformly scales all parts of the image to the required\nsize. Linear scaling is fast but suﬀers from problems with complex\nshaped glyphs at large font sizes and with small glyphs at small\nfont sizes. Non-linear normalization can improve performance by\nselectively scaling regions of low curvature. Punctuation marks,\nwhich are easily distorted because of their small sizes, are usually\nhandled separately. For example, heuristics based on location and\nstacking can be used to handle these punctuation marks.\nThere is a large variety of features that can be used to discrim-\ninate between various glyphs, a large variety of distance measures\nto compute the distance or dissimilarity and a variety of classi-\nﬁcation techniques to ﬁnally classify a given glyph assign a class\nlabel. Interested readers will ﬁnd more details in the published\npapers.\nPost-processing Stage\nAssembling Glyphs into aksharas: is much more challenging than\nit appears in the case of Indian scripts. For example, in Telugu\nscript, glyphs can be scaled and placed on top, to the left bottom,\ndirectly below, to the right bottom or to the right side of another\nbase glyph. The glyphs may not be recognized in the same order\nin which they are required to assemble aksharas correctly. Glyph\nshapes may be same but distinctions may have to be made based\non relative size and relative location. The end result of this com-\nplex assembly process is the text encoded in a suitable character\nencoding scheme such as ISCII or UNICODE.\nSpell-Checker: OCR works glyph by glyph and there can be\nerrors of omission and commission. The kinds of errors made by an\nOCR engine are quite diﬀerent from the kinds of spelling mistakes",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-9",
                            "title": "Language Identification",
                            "content": "given set of languages.\nLanguage Identiﬁcation can be viewed\nas a generic machine learning problem, a supervised classiﬁcation\ntask in which features extracted from a training corpus are used\nfor classiﬁcation. Any of the machine learning techniques can be\nused.\nIn the last decade or so, corpus based Machine Learning ap-\nproaches have become predominant in language engineering over\nthe Knowledge Based approaches which use explicit rules hand-\ncrafted by domain experts. Recent research on Language Identi-\nﬁcation has been limited almost exclusively to Machine Learning\napproaches.\nA Machine Learning system is expected to be generic and it is\nunderstood that training is based only on the intrinsic properties\nof the data, as expressed through a set of “features”. Extraneous\nindicators such as clues from scripts or fonts used, header infor-\nmation or explicit markup tags in the document structure cannot\nbe used.\nThe basic idea is that each language uses a unique or a very\ncharacteristic alphabet, and the letters of the alphabet appear\nwith surprisingly consistent frequencies in any statistically signif-\nicant text. In addition, the frequency of occurrence of sequences\nof two, three, four and more letters are characteristically stable\nwithin, but diverse among diﬀerent natural languages. The most\nfrequent 3-grams, 4-grams etc. have been used for language iden-\ntiﬁcation. A crucial part of the recognition system is the identiﬁ-\ncation of the set of most distinctive, most frequently encountered\nsequences of characters (that is, bigrams, trigrams, etc.) that can\nbe associated with each language. Distinctiveness implies that the\nfrequency of a letter combination for a given language should be\nhigh relative to the frequency of occurrence in other languages.\nIn alphabetic writing systems such as those used for English\nand other European languages, a character is simply a letter of\nthe alphabet (or a punctuation mark, a digit or other special sym-\nwords. Which words to include in a word list is therefore an open\nquestion. Lastly, statistical features such as n-grams in any case\ninclude the information contained in small, frequent words, aﬃxes\netc. Given these facts and the desire to build generic, trainable\nlanguage identiﬁcation systems, machine learning approaches that\ndepend solely on features extracted from data are preferred.\nA number of language identiﬁcation systems have been built\nfor various language groups of the world. Despite its great rele-\nvance, Language Identiﬁcation had, however remained a largely\nunexplored area for Indian languages. Recently, Multiple Linear\nRegression has been successfully applied to develop high perfor-\nmance pair-wise language identiﬁcation among the major Indian\nlanguages. An F-Measure of 98% plus has been achieved for In-\ndian languages when test samples were about 10 aksharas in size\nand the performance went up to nearly 100% when the sample size\nwas increased to about 25 aksharas. Note that aksharas are ap-\npropriate units of text for Indian scripts, not bytes. Experimental\nresults also corroborate this claim.\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n319\nTexts are treated a sequences of aksharas. The script gram-\nmar is used to segment texts into aksharas. Aksharas are smaller\nunits than full words. The number of frequently used aksharas is\nalso smaller than number of words. The number of distinct words\nis of the order of hundreds of thousands whereas aksharas in com-\nmon use are in thousands. Studies have shown that about 5000\naksharas account for more than 99% of all words in all the major\nIndian languages. Thus the size of the training corpus required\nwill be much smaller if we use akshara based features. There is\nno need to talk in terms of words or morphemes.\nMonograms, bigrams and trigrams of aksharas can be used as\nfeatures. We may also consider positional features - word initial,\nword medial and word ﬁnal n-grams may be distinguished. It is\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy\nUniversity of Hyderabad\nPublished By\nEss Ess Publications\nFor\nSarada Ranganathan Endowment for Library\nScience\nBangalore, INDIA\n2006\ni\nc\n⃝Kavi Narayana Murthy and Sarada Ranganathan\nEndowment for Library Science (2005)\nAll rights reserved. No part of this publication may be\nreproduced, stored in a retrieval system or transmitted, in\nany form or by any means, electronic, mechanical, photo-\ncopying, recording or otherwise without the prior written\npermission of the publisher.\nThis book has been printed from the camera-ready\ncopy prepared by the author using LATEX\nNatural Language Processing\nAn Information Access Perspective\nKavi Narayana Murthy, University of Hyderabad\nSarada Ranganathan Endowment Lecture, 24(2004)\nFirst Published 2006\nISBN 81-7000-485-3\nPrice: Rs. 850/-\nPublished by\nEss Ess Publications\n4837/24, Ansari Road, Darya Ganj, New Delhi-110 002\nTel: 001-23260807 Fax: 001-23274173\nE-mail: essess@del3.vsnl.net.in url:\nhttp//www.essessreference.com\nFor\nSarada Ranganathan Endowment for Library\nScience\n702, ‘Upstairs’, 42nd Cross, III Block, Rajajinagar,\nBangalore 560 010\nE-mail: srels@vsnl.com Tel: 080-23305109\nPrinted in India at: Printline, New Delhi 111 002\nii\nPreface\nThe contributions of Dr. S R Ranganathan to the ﬁeld of\nlibrary and information sciences is well known. Sarada Ran-\nganathan Endowment for Library Science (SRELS), founded\nby Dr Ranganathan in 1961 has been carrying out com-\nmendable work in promoting library and information sci-\nences. SRELS has been working towards improvement of\nlibrary and information services in India, training personnel\nin library and information sciences and applying research\nresults in related areas. The endowment organizes three se-\nries of lectures: Sarada Ranganathan Endowment Lectures,\nDr S R Ranganathan Memorial Lectures and Curzonco-\nSeshachalam Endowment Lectures. It also conducts work-\nshops, projects, consultancy work.\niteratively in case identiﬁcation fails until a solution is obtained.\nThe diﬀerences between the within-language-family and across-\nlanguage-family cases may be explored. We can see the degree of\n“closeness” between various language pairs. Hindi and Punjabi\nmay show up to be closer than, say, Oriya and Punjabi. When\nwe need to recognize languages that are closer to one another, we\nmay need more sophisticated features or larger data to get the\nsame level of performance. This idea can be extended further to\nstudy other kinds of variations among languages or language fam-\nilies as also to uncover universal, language-invariant features in a\nquantitative way.\nNote that machine learning methods are completely generic\nand hence the same program can be used for identiﬁcation be-\ntween any two languages - all that we need is suitable training\ncorpora and a few minutes of time to retrain the system on the\nnew training data. We do not need any word lists or other lin-\nguistic information about the languages being distinguished. This\ngeneric nature and adaptability is the most important merit of\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n321\nmachine learning techniques. Linguistic approaches, on the other\nhand, normally require careful linguistic study for each language\nunder consideration and hence a lot of time and eﬀort. Machine\nlearning techniques can be adapted to new languages within min-\nutes and without any manual eﬀort.\n2.4.10\nOthers Technologies for Indian Languages\nMachine translation has been taken quite seriously in India and\na lot of progress has been made. Major focus has been laid on\ntranslating between Indian languages, exploiting the commonness\nof these languages along the linguistic and cultural dimensions.\nRecently, there is also an increased emphasis on translation be-\ntween English and Indian languages. While demonstration level\nsystems have been built, there is a long way to go before machine\ntranslation can be applied in real life situations.\nthat the string “abnidella” is unlikely to be a valid English word\nwithout consulting a dictionary.\nThus machine learning is a purely data driven approach. The\ngreatest merit of this approach therefore is its generality and\nadaptability. All we need to migrate to a new language or a new\napplication is to provide appropriate training data in that lan-\nguage or for that application. The machine unlearns and relearns\n246\nCHAPTER 2. FOUNDATIONS OF NLP\nto automatically to adapt to the new situation. Migrating from\none language to another using a linguistic approach, on the other\nhand, would necessitate extensive manual exploration of the new\nlanguage structures and properties.\nMachine learning can be supervised or unsupervised. In su-\npervised learning, a set of labeled training data is given and the\nmachine learns a general decision rule which can be used for clas-\nsiﬁcation of new data items. In unsupervised learning, a set of\nunlabeled training data is given and the machine learns to group\nsimilar data items into clusters so that new data items can be\nplaced into the right clusters. The number of clusters may or may\nnot be known beforehand.\nWe describe here a selection of machine learning techniques\nbrieﬂy. The purpose is to give an exposure to the basic ideas.\nInterested readers may consult books on machine learning and\npattern classiﬁcation for an in-depth exposition.\nRegression as Classiﬁcation\nRegression analysis is a statistical technique for investigating and\nmodeling the relationship between variables in a system. When\nthere are more than two variables in the system, the term multiple\nregression is employed. Regression is often used as a modeling\ntechnique where the value of one of the selected variables, called\nthe response variable, is determined by the values of the other\nindependent variables, also called the regressors. The modeling\nprocess basically involves determining parameters of the model,",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-10",
                            "title": "Others Technologies for Indian Languages",
                            "content": "ysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\nhad moved from the research laboratories to the market place.\nThe strengths and weaknesses of purely statistical approaches\nhave now been clearly understood. It is now generally believed\nthat a combination of linguistic and statistical approaches may be\nessential. New application areas have emerged. At one point of\ntime NLP had become almost synonymous with Machine Transla-\ntion, especially in our country. Now Information retrieval, Infor-\nmation Extraction, Automatic Categorization, Automatic Sum-\nmarization, Text Mining have all become major application areas\nfor NLP. There is an increasing conﬂuence and synergy between\ntext and speech technologies. People have started talking about\nmulti-lingual and multi-media applications. There is an increased\nawareness of the importance of language and speech technologies\nat all levels. Many large funded research projects have been initi-\nated all over the world. In India the Government has been showing\na keen interest in developing technologies for Indian languages.\nThere are scores of universities and research organizations that\nhave been working on various aspects of language technologies.\nTeething problems have been largely overcome and attention has\nshifted to applications that can have serious impact on our society.\nWe have come a long way. However, if you look back carefully\nand analyze our achievements and failures, you will ﬁnd that al-\nmost all the critical problems have remained unsolved. We have\nnot been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.\nany further processing or analysis. In most cases, the electronic\nforms of the documents are never archieved.\nSpeech technologies are especially important for a country like\nIndia with many languages and high levels of illiteracy. There are\nagain certain characteristics of Indian languages which are quite\ndistinct from English. For example, stress is relatively less impor-\ntant and other prosodic features such as duration are more signiﬁ-\ncant. Aspiration is a contrastive feature. A deeper understanding\nof characteristics of our languages is essential and technology de-\nveloped for other languages cannot be simply borrowed. There is\na lot of quantitative work that needs to be done. Very little has\nbeen done so far. We do not even have large speech corpora.\nHowever, things are changing fast. There is a much higher de-\ngree of understanding and appreciation of the language technology\nissues at all levels. There is deﬁnite trend towards standardiza-\ntion. Several serious large scale eﬀorts have been initiated. There\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n267\nis a corpus of about 35 Million words for Telugu today. Auto-\nmatic text categorization systems have been developed. Language\nidentiﬁcation across Indian languages is now possible. OCR sys-\ntems have started appearing for Indian scripts. Major initiatives\nhave been taken in machine translation and speech technologies.\nSearch engines specialized for Indian languages have been devel-\noped. There is progress on Information retrieval and extraction\nsystems as well. There is hope that Indian language technologies\nwill develop very fast over the next few years.\nHowever, we should add a word of caution to our note of op-\ntimism. Whatever has been done so far in terms of technology\ndevelopment is largely ad-hoc, hoch-poch, untested, unﬁnished\nand essentially unusable.\nSpeciﬁcations are not written down.\nSystems are not designed carefully. Instead we jump to imple-\nmentation right away. Bench mark standards and standard test\nwork well for Indian languages. Richness of morphology would\ncall for special attention. Being late has one advantage - we can\ngain from others’ experiences and make a better design.\nThere is also a great need for indexing and searching pages\nnot in, but on Indian languages and Indian tradition and culture.\nMulti-lingual and multi-media extensions will become increasingly\nimportant.\nBibliography\n1. Akshar Bharati, Vineet Chaitanya and Rajeev Sangal, “Nat-\nural Language Processing: A Paninian Perspective”, Prentice-\nHall of India, 1995\n2. Andrian Akmajian, Richard A Demers and Robert M Har-\nnish, “Linguistics: An Introduction to Language and Com-\nmunication”, The MIT Press, Second Edition, 1984\n3. Baker C L, “English Syntax”, MIT Press, 1989\n4. Ben Gold and Nelson Morgan, “Speech an Audio Signal\nProcessing”,John Wiley and Sons Inc., 2002\n5. Charniak E, “Statistical Language Learning”, MIT Press,\n1993\n6. Christopher D Manning and Hinrich Shutze, “Foundations\nof Statistical Natural Language Processing”, The MIT Press,\n2000\n7. Daniel Jurafsky and James H Martin, “Speech and Lan-\nguage Processing”, Pearson Education, 2002\n8. Douglas O’Shaughnessy, “Speech Communications - Hu-\nman and Machine”, Second Edition, Universities Press, 2001\n9. D C Montgomery, E A Peck and G G Vining, “Introduction\nto Linear Regression Analysis”, John Wiley and Sons, INC.,\n2001\n10. George W Smith, “Computers and Human Language”, Ox-\nford University Press, 1991\n353\n11. Hopcroft J.E. and Ullman J.D., “Introduction to Automata\nTheory, Languages, and Computation”, Addison-Wesley,\n1979\n12. Inderjeet Mani and Mark T Maybury (Eds), “Advances in\nAutomatic Text Summarization”, the MIT Press, 1999\n13. James Allen,”Natural Language Understanding”, Second\nEdition, Pearson Education, 2003\n14. Lawrence Rabiner and Biing-Hwang Juang, “Fundamentals\nof Speech Recognition”, Pearson Education, 2003\n15. Michael G Dyer, “In-Depth Understanding”, MIT Press,1983\n16. Nigel Fabb, “Sentence Structure”, Routledge\narises. Groups trying to promote Indian language computing com-\nplain of lack of market. It is only recently that the importance\nof local language computing is getting realized more and more\nand Government of India has started promoting Indian language\ntechnologies in a big way.\nThe three language formula has worked well in some cases but\nhas not worked very well in others. Nevertheless, a large number\nof people know two, three or even more number of languages. This\nis a kind of natural multi-lingualism, very diﬀerent from the kind\nof multi-lingualism you will ﬁnd in countries that are essentially\nmono-lingual. You will ﬁnd that more or less free mixing of several\nlanguages is very common. Many documents are required to be in\nmore than one language. With so many languages in use, correctly\nidentifying the language used is itself an critical step in many\nlanguage technology applications.\nAlthough there are many languages and people from one part\nof the country sometimes feel almost like foreigners in other parts\nof the same country, it is not true that there are any serious lan-\nguage barriers. People have been travelling all over the country\nfrom ancient times and nobody has complained of language bar-\nriers. People know that everybody does not speak their language\nand there is a sense of tolerance, cooperation and understanding.\nLearn more languages. Earn more friends is the motto. Common-\nness of social culture, traditions and linguistic features also helps.\nMedia and increased travel and mixing of people have helped too.\nEnglish and Hindi act like national link languages. Thus there\nare no serious language barriers for simple day to day commu-\nnications. However, it is still diﬃcult for a vast majority of the\npeople to use computers, access information from the Internet, etc.\n264\nCHAPTER 2. FOUNDATIONS OF NLP\nTechnology for Indian languages is developing fast and hopefully\npeople will be able to overcome these problems soon.\nis what we have in our ﬁnger tips. To this day, many of our tribal\nlanguages have no script. But they do have a very rich oral ‘liter-\nature’. Thus not everything we have is available in written form.\nThe greatest scholars in India have always been “illiterate” and\nthey preferred to remain so even after writing became possible.\nThe western notion of illiteracy is thus not applicable to India.\nIlliterates are not necessarily uneducated or ignorant.\nThis book, however, is focussed on computer processing of\nwritten texts. We shall therefore start with a study of our writing\nsystems and the text processing environment. Our aim here will\nbe to a get a feel for the nature of Indian languages and issues\nin technology development for Indian languages. We shall brieﬂy\nsurvey a few selected technologies.\n2.4.1\nThe Text Processing Environment:\n(Text Processing Tools)\nKEYBOARD\n(Keyboard Driver)\n(Font Encoding)\n(Character Encoding Standard)\nFILE\nPRINTER\n(Rendering Engine)\nDISPLAY\nFONTS\nThe Text Processing Environment\nFIG 2.16 The Text Processing Environment\n270\nCHAPTER 2. FOUNDATIONS OF NLP\nWhen you type in some text from a computer keyboard, a\npiece of software called the keyboard driver checks which keys are\npressed and sends out corresponding numerical codes. The text\nediting software converts these codes into a possibly diﬀerent set\nof codes corresponding to the character encoding scheme used.\nA character is a letter of the alphabet, a punctuation mark or a\nsymbol such as ‘#’ or ‘+’. For example, if ASCII is the character\nencoding scheme used, the upper case letter A is coded as the\nnumber 65.\nWhat is actually stored in a text ﬁle is just this\nnumber 65. All the software programs ‘know’ how to deal with\nthis number 65. For example, a text editor would display it as the\nletter ‘A’. A sorting program would know that ‘A’ comes before\n‘B’. If we are using the ISCII character encoding standard, the\n1991 BIS standard (IS 13194:1991), and you type the character",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-11",
                            "title": "NLP and Sanskrit",
                            "content": "ing about. You become a bit more broad minded. Secondly, and\nmuch more importantly, knowing Sanskrit helps us to get a more\nscientiﬁc and systematic way of dealing with language at all lev-\nels. Sanskrit not only has an excellent grammar, one that is hard\nto ﬁnd fault with, there is a whole science of grammar of which\nthe Sanskrit grammar is just an instance. It is not the Sanskrit\ngrammar itself that is so very important, it is the underlying sci-\nence of grammar that is crucial. The same is true of phonetics,\nphonology, morphology and semantics. Linguists cannot aﬀord to\nbe ignorant of Sanskrit.\nA whole ocean of knowledge exists in Sanskrit on almost all\n2.4. INDIAN LANGUAGE TECHNOLOGIES\n323\naspects of language, meaning, logic and understanding. However,\nsimply knowing Sanskrit is not suﬃcient to make full use of all\nthis knowledge. Today we are unable to leverage this wealth of\nknowledge and experience not just because the number of people\nwho know Sanskrit is less. This is due largely to the very na-\nture of these traditional knowledge sources. According to Indian\ntradition, knowledge was not meant for all - in fact every eﬀort\nwas made to ensure that knowledge does not reach the hands of\nthe “un-deserving”. (If everybody is taught how to make bombs\nyou know what happens. There are no “right” hands for bombs\n- if you are “right” you will not need a bomb at all. The only\nhands interested in bombs are the “wrong hands”) It is foolish\nto publish all knowledge and then worry about it boomeranging\nback on you. Hence Gurus were extremely choosy about whom\nthey will teach and how much they will teach. Knowledge was\nmeant only for those who are extremely serious - those who con-\nsider seeking knowledge as the main goal of life and those who are\nadjudged to be capable of correct interpretation and proper appli-\ncation of that knowledge for righteous purposes. Given these, it\nwas expected that a seeker of knowledge should search for the right\ndescriptions. CFGs and trees are not the best way to do this.\nii) Relatively Free Word Order Languages\nLinear order of words and phrases in a sentence may be signiﬁ-\ncant - changing the order may render the sentence ungrammatical\nor anomalous.I drank ﬁlter coﬀee is not the same as I drank cof-\nfee ﬁlter. Grammars must therefore impose constraints on linear\npositions as required in a given language.\nA sentence is a sequence of words in English and it looks\nalmost unimaginable to view a sentence as an unordered set of\nwords. However, there are languages of the world where order of\nwords is the least important aspect of structure. All permutations\nof words in a Sanskrit sentence are grammatically valid and mean\nexactly the same thing. A sentence could be viewed as a set of\nwords, not really a sequence. It is worth noting that English was\nalso a relatively free word order language at one point of time.\nThere are also a number of human languages where there is\nconsiderable, though not unlimited scope for changing the order of\nwords in a sentence without signiﬁcantly altering its basic meaning\n(to be more precise, without altering the functional structure of\nthe sentence). Modern Indian languages are examples of this. All\nof the following Telugu sentences mean essentially the same thing\n190\nCHAPTER 2. FOUNDATIONS OF NLP\n8)\nraamuDu\nbaDiki\nsiitatoo\nveLLaaDu\nRama\nschool-to\nSita-with\nwent\n9)\nbaDiki\nraamuDu\nsiitatoo\nveLLaaDu\nschool-to\nRama\nSita-with\nwent\n10)\nsiitatoo\nraamuDu\nbaDiki\nveLLaaDu\nSita-with\nRama\nschool-to\nwent\nThe strong point of CFG is that it can eﬀectively deal with\nboth the linear and hierarchical structure inherent in human lan-\nguages. This ability to deal with linear structure comes back as a\nweakness when we have to parse sentences in relatively free word\norder languages. Here a variety of word orderings are allowed and\nlinear order is not to be considered signiﬁcant. CFGs however,\nhave no way of getting rid of their hold on linear order. A CFG\nrule such as\nysis of large scale linguistic data resources and suitable machine\nlearning techniques. It is easier to ﬁnd people who can generate\nlinguistic data than people who can provide expert knowledge.\nCommercial companies started working on NLP problems. NLP\nhad moved from the research laboratories to the market place.\nThe strengths and weaknesses of purely statistical approaches\nhave now been clearly understood. It is now generally believed\nthat a combination of linguistic and statistical approaches may be\nessential. New application areas have emerged. At one point of\ntime NLP had become almost synonymous with Machine Transla-\ntion, especially in our country. Now Information retrieval, Infor-\nmation Extraction, Automatic Categorization, Automatic Sum-\nmarization, Text Mining have all become major application areas\nfor NLP. There is an increasing conﬂuence and synergy between\ntext and speech technologies. People have started talking about\nmulti-lingual and multi-media applications. There is an increased\nawareness of the importance of language and speech technologies\nat all levels. Many large funded research projects have been initi-\nated all over the world. In India the Government has been showing\na keen interest in developing technologies for Indian languages.\nThere are scores of universities and research organizations that\nhave been working on various aspects of language technologies.\nTeething problems have been largely overcome and attention has\nshifted to applications that can have serious impact on our society.\nWe have come a long way. However, if you look back carefully\nand analyze our achievements and failures, you will ﬁnd that al-\nmost all the critical problems have remained unsolved. We have\nnot been able to build systems that understand, generate or learn\nnatural languages. It is in fact unfortunate that so much of at-\ntention is being given to very superﬁcial analysis of language and\n2.1. INTRODUCTION\n95\nthe core is almost forgotten.\ntremely systematic and scientiﬁc way of dealing with all aspects\nof language starting from the alphabet through words, phonetics,\nphonology, morphology and syntax to semantics and proper in-\nterpretation of meanings. Yet all this is designed for intelligent\nhuman beings with common-sense, not for dumb machines.\nA\nsubstantial degree of common sense and world knowledge is re-\nquired to process and understand any language for that matter\nand perhaps it is only more so in the case of Sanskrit. We have\nseen for example that compounds are very common in Sanskrit\nand interpreting compounds often requires human intelligence. It\nis also very common to leave out portions that we can ﬁll up\nourselves. Sanskrit statements are often highly elliptic. Handing\nellipses is still a very diﬃcult task for computers. Further, if one\nwere to look at Sanskrit as it is actually used, you will very of-\nten ﬁnd highly abstract and cryptic statements, calling for expert\ninterpretation and explanation from a Guru. Knowing the literal\nmeanings is of little use. If you come across the statement You\nare That what sense will you make out of this?\nYet knowing Sanskrit may perhaps help in two ways. Firstly,\nmodern Indian languages have a lot of things borrowed from San-\nskrit and partly or wholly assimilated into them. Knowing San-\nskrit helps one to get a deeper insight into the structure and\nmeaning of words and sentences in our languages. One can start\nunderstanding the beauty of language in a better way. We will\nget to know how diﬀerent languages have adapted, assimilated,\ndeveloped and grown in their own ways. We can begin to under-\nstand and appreciate even those languages which we knew noth-\ning about. You become a bit more broad minded. Secondly, and\nmuch more importantly, knowing Sanskrit helps us to get a more\nscientiﬁc and systematic way of dealing with language at all lev-\nels. Sanskrit not only has an excellent grammar, one that is hard\nin between Sanskrit and English in this regard but perhaps much\ncloser to Sanskrit than to English. Any system that heavily de-\npends on word order would be inappropriate. Morphology should\nbe the major criterion. HMMs are perhaps not the best models\nfor Indian languages.\nHMM models may not be very suitable for Indian languages.\nMorphology of Indian languages has not yet been worked out fully.\nPreliminary tag-sets have been deﬁned and used to build taggers\nfor some Indian languages but there is still a long way to go.\nThe grain size of the tag set is itself a major issue. If we include\nonly the major categories that would be too coarse and although\ntaggers may be built easily, tagged corpus so generated will be of\nvery limited use. If, on the other hand, we attempt to capture all\nthe ﬁne variations depicted in the morphology of the words, the\ntag set would become very large and POS tagging will essentially\nboil down to morphological analysis. Should we have a hundred\nthousand tags? If one were to postulate morphological analysis at\nrun time for any given application, generating and storing POS-\ntagged corpora would no longer serve any serious purpose.\nPerhaps a hierarchical design where tags are not simple atomic\nsymbols but composites indicating hierarchical reﬁnements would\nbe most appropriate for Indian languages. This would also enable\ndevelopment of more and more reﬁned tag sets and POS taggers\nin a phased manner. An an example of this concept, the tag v\nfor verb could be revised to include v-i and v-t for v-intransitive\nand v-transitive and this could further be reﬁned to show more\ndetailed sub-categorizations and so on.\n2.2.5\nSyntax: Grammars and Parsers\nLanguages exhibit complex structures and a detailed and system-\natic analysis of the structure of natural language sentences is in-\nvaluable in determining the meaning of the sentences. Form fol-\nlows function. Knowing the structure will, hopefully, help us in",
                            "children": []
                        },
                        {
                            "id": "chapter-2-section-4-subsection-12",
                            "title": "Epilogue",
                            "content": "29. What river was it caught in?\n30. Whom did you want Rama to tell to catch a ﬁsh?\n31. * In what river they ﬁshed?\n32. Whom was the ﬁsh caught by?\n33. By whom was the ﬁsh caught?\n34. Whom was the ﬁsh expected to be given to?\n35. An engineer reading the newspaper in the balcony got angry\n36. They found the answer that they were looking for\n37. Yesterday I ate a cake the likes of which I had never seen\n38. The deity in whose image we were depicted was unknown\nto many\n39. The windows broken in the scuﬄe have been replaced\n40. I saw a baby being given a bath in the open\n41. The ﬁsh that they thought you had told me not to bother\nwith was very small\n42. The horse raced past the barn fell\n360\nAppendix 3: ISCII\nCharacter Set\nCode\nCharacter Name\n161\nVowel-Modiﬁer caMdrabiMdu\n162\nVowel-Modiﬁer anusvaara\n163\nVowel-Modiﬁer visarga\n164\nVowel a\n165\nVowel aa\n166\nVowel i\n167\nVowel ii\n168\nVowel u\n169\nVowel uu\n170\nVowel R\n171\nVowel e (Southern Scripts)\n172\nVowel ee\n173\nVowel ai\n174\nVowel aye (deevanaagari Script)\n175\nVowel o (Southern Scripts)\n176\nVowel oo\n177\nVowel au\n178\nVowel awe (deevanaagari Script)\n179\nConsonant ka\n180\nConsonant kha\n181\nConsonant ga\n182\nConsonant gha\n183\nConsonant nga\n361\nCode\nCharacter Name\n184\nConsonant ca\n185\nConsonant cha\n186\nConsonant ja\n187\nConsonant jha\n188\nConsonant jnya\n189\nConsonant Ta\n190\nConsonant Tha\n191\nConsonant Da\n192\nConsonant Dha\n193\nConsonant Na\n194\nConsonant ta\n195\nConsonant tha\n196\nConsonant da\n197\nConsonant dha\n198\nConsonant na\n199\nConsonant na (Tamil)\n200\nConsonant pa\n201\nConsonant pha\n202\nConsonant ba\n203\nConsonant bha\n204\nConsonant ma\n205\nConsonant ya\n206\nConsonant jya (Assamese, Bangla, Oriya Scripts)\n207\nConsonant ra\n208\nConsonant Hard ra (Southern Scripts)\n209\nConsonant la\n210\nConsonant La\n211\nConsonant zha (Tamil, Malayalam Scripts)\n212\nConsonant va\n213\nConsonant s’a\n214\nConsonant Sa\n215\nConsonant sa\n216\nConsonant ha\n217\nConsonant INVISIBLE\n362\nCode\nCharacter Name\n218\nVowel Sign aa\n219\nVowel Sign i\n220\nVowel Sign ii\n221\nme an opportunity to sit and work peacefully. If I could\ncomplete the book in such a short period, it is because of\nthe nice academic environment at the centre and the cooper-\nix\nation and support of Prof. C N Krishnan, the director, and\nall the scientists and researchers at the centre. My special\nthanks go to Dr Sobha and Mr. Baskaran for all the lively\ndiscussions we have had on NLP.\nAlthough the idea of writing a book was there in my\nmind for a long time, ﬁnding time was not easy. I am thank-\nful to the authorities of University of Hyderabad for giving\nme leave. Whatever little I have learnt during the last 15\nyears there is due to the close interactions I have had with\nexperts from varied disciplines including linguistics, philos-\nophy, mathematics and statistics. My colleagues at the De-\npartment of Computer and Information Sciences and my\nstudents have always been a constant source of inspiration.\nNot a single day passes without some lively discussion and\ndebate on some research topic or the other in computer sci-\nence or artiﬁcial intelligence.\nThe book is based largely on my notes, slides, scrib-\nblings, papers and reports written over the last many years.\nI have been greatly beneﬁted by all the lectures and sem-\ninars I have attended and all the discussions I have had\nwith many experts from varied ﬁelds. I would have surely\nborrowed many ideas and examples from many sources. It\nwould be practically very diﬃcult to acknowledge each one\nof them separately. There are few sections that have been in-\nﬂuenced substantially from writings of others. The section\nAutomatic Summarization has drawn partly from the pa-\npers and summaries compiled by Inderjeet Mani and Mark\nT Maybury.\nParts of the section on morphology dealing\nwith English morphology have been inﬂuenced by the book\nby Andrian Akmajian, Richard A Demers and Robert M\nHarnish. Acknowledgements and credits have been included\nat appropriate places in the text.\ncryptic, they had to be explained in more detail. Commentaries\nhad to be written. Interestingly, some of the commentaries also\nrequired further elaboration and explanation and one ﬁnds several\nlayers of commentaries. The original text itself remains small and\ncan often be completely memorized. This book could be written\nin just a few pages. But that would not be very easy for most\nreaders today to read and understand. Understanding short and\ncryptic statements require more eﬀort and seriousness on the part\nof the readers. (In fact this book has actually been written point\nby point and then expanded.) It looks like we are in a situation\ntoday where we tend to write too much and then see the need for\nproducing summaries.\nWe ﬁnd a plethora of ideas on summarization in the mi-\nimaaMsa and other s’aastras. We can learn a lot about proper\nway of organizing and structuring our thoughts and hence the\ndocuments we create. One of the basic requirements for any co-\nhesive piece of writing, however small or big it may be, is that\nit should be possible to express its purport in just one sentence\n(eekavaakyata). If you cannot express the gist in one sentence, ei-\nther you have not understood the writing properly or the writing\nitself is incoherent. Ramayana, which runs into 24,000 s’lokas, has\nbeen summarized into a few pages, into one page, into one small\nparagraph and into a single statement.\n1.7. AUTOMATIC TEXT CATEGORIZATION\n47\n1.7\nAutomatic Text Categorization\n1.7.1\nWhy Text Categorization?\nOver the past decade, there has been an explosion in the avail-\nability of electronic information. As the availability information\nincreases, the inability of people to assimilate and proﬁtably uti-\nlize such large amounts of information becomes more and more\nevident. One of the most successful paradigm for organizing this\nmass of information, making it comprehensible to people, is per-\nhaps by categorizing the diﬀerent documents according to their\nsubject matter or topic.\nMay 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having\ngiven me an opportunity to present my views to such an\naugust audience.\nThe book has expanded from the top-\nics covered in the lectures, based on my own experience in\nteaching and research in this ﬁeld over the last many years.\nI am grateful to SRELS for coming forward to publish this\nbook.\nMy special thanks go to Prof.\nA Neelameghan upon\nwhose invitation and inspiration I undertook the job of writ-\ning this book. Although the idea of writing a book on NLP\nwas there in me for a long time, it is only due to his encour-\nagement that this book has taken shape so soon.\nProf.\nUdaya Narayana Singh, director CIIL, Mysore\nwas kind enough to inaugurate my lectures at SRELS. His\nspeeches are always as inspiring as they are informative. I\nam also grateful to him for writing an excellent foreword to\nthis book.\nA number of my colleagues, friends and students, com-\ning from diverse backgrounds including linguistics, computer\nscience, artiﬁcial intelligence, mathematics and engineering,\nhave taken pains to go through the drafts and have made\nvery valuable comments and suggestions. My sincere thanks\ngo to G Uma Maheshwara Rao, S Rajendran, S Durga Bha-\nvani, L Sobha, S Baskaran, Ramesh Kumar, G Bharadwaja\nKumar and Hla Hla Htay.\nI sincerely acknowledge the AU-KBC Research Centre, a\nsmall but very active privately funded research organization\nin Chennai, for inviting me as a visiting scientist and giving\nme an opportunity to sit and work peacefully. If I could\ncomplete the book in such a short period, it is because of\nthe nice academic environment at the centre and the cooper-\nix\nation and support of Prof. C N Krishnan, the director, and\nall the scientists and researchers at the centre. My special\nearly 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-\nsible for computers to demonstrate in-depth understanding and\nanswer complex questions in a human-like manner. Let us look at\nsamples of actual transcripts from BORIS:\nRichard hadn’t heard from his college roommate Paul\nfor years.\nRichard\nhad borrowed\nmoney from\nPaul,\nwhich was\nnever paid back,\nbut now he had no idea\nwhere to find his old friend. When a letter finally\narrived from San Francisco,\nRichard was anxious to\nfind out how Paul was.\nQ What happened to Richard at home?\nA Richard got a letter from Paul\nQ Who is Paul?\nA Richard’s friend\nQ Did Richard want to see Paul?\nA Yes. Richard wanted to know how Paul was\nQ Had Paul lent money to Richard?\nA Yes. Paul had lent money to Richard\nQ Why didn’t Richard pay Paul back?\nA Richard did not know where Paul was\nQ How did Richard feel when the letter appeared?\nA Richard felt glad because Paul and he were friends\nFIG 1.5 Sample Transcripts from BORIS\n1.3. QUESTION ANSWERING SYSTEMS\n25\nThis sample transcript vividly brings out the depth of under-\nstanding by BORIS. It is not straight forward to answer questions\nsuch as what happened to somebody somewhere. Many things\nmight have happened, some of them may be explicitly given to\nus in the story while some we can infer from our world knowl-\nedge of what “typically” happens in given kinds of situations.\nBut everything that happened or could have happened would not\nbe interesting. Something unusual, something special, something\nworth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-2-section-5",
                    "title": "Conclusions",
                    "content": null,
                    "children": []
                }
            ]
        },
        {
            "id": "chapter-3",
            "title": "Advances in IR",
            "content": null,
            "children": [
                {
                    "id": "chapter-3-section-1",
                    "title": "History of IR",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-3-section-1-subsection-1",
                            "title": "From The Library to the Internet",
                            "content": "We\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\ntion from anywhere in the world. We are no longer conﬁned\nto one oﬃce or one library. Today we live in the information\nage.\nWe have billions of pages of material on the Internet to-\nday. With the Digital Library initiatives whole libraries are\ngetting converted to electronic form. Trillions of bytes of\nelectronic data are getting generated everyday. But merely\nhaving some data or information somewhere is of no use.\nWhat is really required is an easy way to access relevant,\ntimely, useful, and authentic information in a well presented\nmanner. How do we know which documents are relevant,\nauthentic and dependable?\nHow do we ﬁsh out what we\nare looking for in this vast ocean of web pages? How do we\ncategorize, classify, index and structure these largely unor-\nganized collections of electronic documents on the Internet\nso that they become more easily accessible and hence more\nuseful?\nTechnology enables us to create, store and process large\nvolumes of information at great speed.\nBut the speed at\nwhich we human beings can read and understand documents\nremains the same, irrespective of technological advances.\nWe still take minutes to browse a page, possibly hours to\nread carefully and understand the content and may be days,\nweeks or even months to chew and fully digest the purport of\na serious piece of writing. Thus we are in a situation today\nwhere we have more information than we can handle. Tech-\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nof speech technology is given after that. With this, it is easy\nvii\nfor KNM to lay the foundations of NLP in the next section.\nHow doing purely esoteric linguistics is diﬀerent from doing\ncomputational linguistics has been brought about by him\nclearly in a section.\nThose interested in corpora are also\ngoing to beneﬁt from this text book tremendously as there\nis a long section devoted to this area, besides introductory\nremarks in the earlier section. Under 2.4, a lot of loosely\nstrung issues have been put together in a rather well-written\nand lengthy section.\nThe third chapter deals with the latest researches in ‘In-\nformation Retrieval’, a matter so very dear to all librarians\nand information science persons who were present during the\ntalk. For those who are uninitiated, an introduction to the\nbasic model of retrieval has been presented but the advance\nlearners and persons on the job can also beneﬁt from the\ndescription of an advanced IR model, which he calls the ‘In-\ntelligent IR’. There again, KNM brings out the role played\nby linguists and semanticists.\nThe best part of the book is that it has a very good\nreading list for those interested in NLP in its Bibliography\nsection as well as a few very relevant appendices that go very\nwell with the text.\nI sincerely hope that like his lectures were well-appreciated\nby those who were fortunate enough to attend them in Ban-\ngalore, the text as put together here will also be highly ap-\npreciated by the readers of all times to come. On behalf of\nNLP community, let me thank KNM for a laudable eﬀort,\nand he deserves a word of praise also from the library and\ninformation scientists.\nUdaya Narayana Singh\nDirector, Central Institute of Indian Languages, Mysore\nMay 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having\nMay 31, 2005\nviii\nAcknowledgements\nThe origin of this book can be traced to the Sarada Ran-\nganathan Endowment Lectures I gave at SRELS (Sarada\nRanganathan Endowment for Library Science, Bangalore)\nduring August 2004. I am indebted to SRELS for having\ngiven me an opportunity to present my views to such an\naugust audience.\nThe book has expanded from the top-\nics covered in the lectures, based on my own experience in\nteaching and research in this ﬁeld over the last many years.\nI am grateful to SRELS for coming forward to publish this\nbook.\nMy special thanks go to Prof.\nA Neelameghan upon\nwhose invitation and inspiration I undertook the job of writ-\ning this book. Although the idea of writing a book on NLP\nwas there in me for a long time, it is only due to his encour-\nagement that this book has taken shape so soon.\nProf.\nUdaya Narayana Singh, director CIIL, Mysore\nwas kind enough to inaugurate my lectures at SRELS. His\nspeeches are always as inspiring as they are informative. I\nam also grateful to him for writing an excellent foreword to\nthis book.\nA number of my colleagues, friends and students, com-\ning from diverse backgrounds including linguistics, computer\nscience, artiﬁcial intelligence, mathematics and engineering,\nhave taken pains to go through the drafts and have made\nvery valuable comments and suggestions. My sincere thanks\ngo to G Uma Maheshwara Rao, S Rajendran, S Durga Bha-\nvani, L Sobha, S Baskaran, Ramesh Kumar, G Bharadwaja\nKumar and Hla Hla Htay.\nI sincerely acknowledge the AU-KBC Research Centre, a\nsmall but very active privately funded research organization\nin Chennai, for inviting me as a visiting scientist and giving\nme an opportunity to sit and work peacefully. If I could\ncomplete the book in such a short period, it is because of\nthe nice academic environment at the centre and the cooper-\nix\nation and support of Prof. C N Krishnan, the director, and\nall the scientists and researchers at the centre. My special\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\ncolleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much\ntime and care for the newcomers in the ﬁeld, in the midst\nof numerous academic commitments. KNM has had proba-\nbly a thought of doing something like this, useful for both\nlinguistics as well as AI students for a long time, and this\ncomes now as his wish-fulﬁllment. The perspective has, of\ncourse, been made very clear from the author’s end, in case\nwe want to know for sure about his angle of looking into it,\nand that is stated in the sub-title : ‘The Information Access\nApproach’.\nIt is, therefore, not surprising that KNM would start\nfrom the information science knowledge explosion, and sit-\nuate the text in that context. In fact, he begins with QA-\nsystem under which he deals with ELIZA and early NLP\nsystems. This is followed by a lucid introduction to the two\ngreatest utility works being done world-wide on information\nsciences, namely, ‘Information Retrieval’ and ‘Information\nExtraction’ systems. The other advantages of the latest de-\nvelopments in machine understanding of human language\ntexts are creation of automatic summary as well as auto-\ncategorization of text-types and sub-types.\nIt is obvious that many readers of this book would be\ninterested in getting to know more about Machine Trans-\nlation (MT) as well as Machine-Aided Translation (MAT),\nand the author has devoted a whole section on that area. He\nhas identiﬁed the challenges before the machine translation\nenthusiasts as they exist today. An introduction to the ﬁeld\nof speech technology is given after that. With this, it is easy\nvii\nfor KNM to lay the foundations of NLP in the next section.\nHow doing purely esoteric linguistics is diﬀerent from doing\ncomputational linguistics has been brought about by him\nclearly in a section.",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-2",
                    "title": "Basic IR Models",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-3-section-2-subsection-1",
                            "title": "IR Models",
                            "content": "exclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models\nAn IR model must specify four things:\n• Document Representation\n• Query Representation\n• Deﬁnition of Relevance\n• Matching/Retrieval Function\nThe simplest way one can think of matching documents and\nquery is to perform direct keyword search - search for the given\nquery string verbatim in the documents and return the documents\nthat contain the given query string. This is too rigid. Computer\nwill not match Computers or even computer if the search is case\nsensitive. So some kind of soft match may be introduced. Even\nwith that extension the search is too rigid.\nSuppose you want\ndocuments containing the words Sun, Moon, orbit, eclipse but not\n3.2. BASIC IR MODELS\n331\nnecessarily all at one place and not necessarily exactly in the order\ngiven. It would then be helpful to consider documents and queries\nas unordered collections of words. This is called that bag-of-words\nrepresentation. A bag is an unordered collection of items just like\na set, only multiple occurrences are allowed. Some early systems\nused pre-speciﬁed sets of index terms. Current systems tend to\nprefer full text indexing. It is also possible to exploit structure\nof documents and meta-data ( such as URL, title, anchor string,\nauthor and other meta tags, font sizes, capitalization and position\nin the document). Thus the HTML/XML structure in hypertext\ndocuments could be made use of.\nThe Boolean Retrieval Model\nThe Boolean model is based on set theoretic principles. Here doc-\numents are treated as sets of keywords.\nNote that a set is an\nunordered collection of items. Frequency of occurrence is not rel-\nevant. In a set an item is either present or absent. Items cannot\nrepeat. Queries are treated as Boolean expressions of keywords,\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nthe correct sense of the word can we be sure that we are looking\nat the right document. We may try to take into account the order\nof words in the query. A full syntactic analysis may be carried\n3.3. TOWARDS INTELLIGENT IR\n339\nout. We may try to adapt to the user based on direct or indirect\nfeedback. We may try to take into account the authority of the\nsource.\nWe have already seen that many of the NLP tasks are inher-\nently diﬃcult in themselves and performance of current systems\nis limited. Thus full syntactic analysis is not only time consuming\nbut also limited in performance - current grammars and parsers\nfail on a signiﬁcant percentage of cases. Unrestricted WSD sys-\ntems are still at a research level. The real challenge, therefore, is\nto integrate the best of available NLP technologies with IR models\nwithout sacriﬁcing the simplicity and eﬃciency of IR models. We\nwill explore below some the ideas and techniques that have been\nused to build better IR systems.\n3.3.1\nImproving User Queries - Relevance Feed-\nback\nA big question is how do we assess the relevance of a retrieved\ndocument for a given query? Relevance is a subjective judgment\nand may include being on the proper subject, being timely (re-\ncent information), being authoritative (from a trusted source),\nsatisfying the goals of the user and his/her intended use of the\ninformation (information need), etc. Relevance is not an yes/no\nquestion, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\ndatabase or an index of web pages. The task of creating an in-\ndex is made complicated by the large size of the data as also the\nhighly dynamic nature of the web, lack of any centralized con-\ntrol and the heterogeneity of the document types and formats.\nToday IR means retrieving relevant documents from very large\ncollections such as from the web. Full text indexing is taken for\ngranted.\n330\nCHAPTER 3. ADVANCES IN IR\n3.2\nBasic IR Models\nIn ad-hoc retrieval, the most common view of IR, an unaided\nuser expresses his need through a short query. The IR system\nmatches the query against the documents in the collection and\nreturns the documents that match. The returned documents may\nbe given in a ranked order.\nThe document collection is ﬁxed\nand a one time indexing on the whole collection is performed to\ncreate and store an index. Matching is performed on the index,\nnot on the original documents. Each time a user issues a query, a\nmatch operation is performed and the results returned. Document\nFiltering provides an alternative view. Here the query may be\nconsidered ﬁxed and a stream of documents need to be checked.\nThe query expresses what exactly a particular user wants or does\nnot want. Documents are ﬁltered out accordingly. There is no\nscope to perform indexing of the whole collection of documents\nsince the collection is not available beforehand and documents\nkeep coming. Here the decision is usually Boolean - include or\nexclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\nprovides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers\nand industrial partners. It is based on the Resource Description\nFramework (RDF), which integrates a variety of applications us-\ning XML for syntax and URIs for naming. See Scientiﬁc American\nMay 2001 issue for an interesting article by Tim Berners-Lee.\n3.3.7\nInformation Retrieval is Diﬃcult\nIR is an inherently diﬃcult task. Can you search millions of doc-\numents and accurately suggest the most relevant documents for a\ngiven query in a fraction of a second? In IR we are asking com-\nputers to do what we human beings cannot do. And we want IR\nto be fully automatic - there is no scope for human intervention.\nWhat makes IR diﬃcult? There are three major issues:\n• Understanding user needs: Understanding exactly what the\nuser is looking for is not easy. Key words do not tell us\nwhat is the purpose of the current search, what all the user\nalready knows, what all he has already searched or what\nlevel of abstraction would suit his level of knowledge and\nexpertise. Social and cultural contexts are important. It\nlooks strange that we set forth on a grand searching and\nretrieval operation without understanding exactly what we\nare looking for!\n• Understanding the Documents: Unless you know exactly\nwhat the documents contain, what they pertain to and what\n3.3. TOWARDS INTELLIGENT IR\n351\nall things they include, for whom it is written, what back-\nground is assumed etc., how can we say which documents\nwill suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-2",
                            "title": "Term Weighting: tf-idf",
                            "content": "Another formula that has been proposed to reﬁne the raw tf is\n0.5 + (0.5 ∗tf)/(maxtf).\nSince the total number of documents in a collection may be\nlarge, it is usual to squash the raw idf value also by taking loga-\nrithms.\nThe product of Term Frequency and Inverse Document Fre-\nquency, abbreviated as tf.idf, is one of most basic term weighing\nschemes. In fact tf.idf is a class of term weighting schemes. There\nare several combinations, some of the commonly used ones are\nshown below:\n3.2. BASIC IR MODELS\n335\nTerm Frequency\nIDF\nNormal-\n-ization.\nn: natural: tf\nn:natural: idf:\nN\nni\nn:no norm.\nl: log: 1 + log(tf)\nt:log( N\nni )\nc:cosine\na: augmented: 0.5 +\n0.5∗tf\nmax tf\nmax tf: freq. of the most\nfrequent term in the doc.\nVarious schemes of tf.idf weighting are denoted using the la-\nbels in the above table. Thus ltc.ann refers to logarithmic terms\nfrequency, logarithmic idf and cosine normalization for the docu-\nments and augmented term frequency, plain idf and no normal-\nization for the query terms. Weights for terms that do not occur\nin a given document can be taken as zero.\nThese tf.idf schemes consider both the local and global eﬀects\nof term frequencies. They are simple and eﬀective, they are widely\nused.\nHowever, such schemes for term weighting are ad hoc and lack\na proper mathematical basis. Schemes based on term distribution\nprobability models have been proposed. Some of the commonly\nused schemes are based on 1) Poisson distribution, 2) two-Poisson\nmodel and 3) Katz’s K-Mixture model.\nThe terms in the query are usually taken to be equally weighted\nbut it is conceivable that users specify the relative weights of the\nquery terms. There will still be no logical connectives and the\nmatching algorithms will continue to be statistical. It is not easy\nfor users to think of appropriate ways of weighting the query terms\nand hence this idea is diﬃcult to use.\n3.2.3\nSimilarity Measures\nHow do we quantify the similarity between two vectors? We have\nwith their root/stem forms\n• Chunking - grouping together words into phrases\nThese pre-processing steps help to obtain more discriminative\nfeatures and/or to reduce the number of features. It may be noted\nthat these methods are to a large extent language speciﬁc.\n1.7.4\nFeature Weighting\nNumerical weights need to be computed for the index terms before\nmachine learning techniques can be applied. Here are some of the\nbasic ideas for term weighting:\n• Term Attributes:\nAttributes of the terms such as their\nsyntactic categories can be used to weight the terms.\n• Text attributes:\nThe number of terms in a text, the length\nof the text etc. can be used.\n52\nCHAPTER 1. THE INFORMATION AGE\n• Relation between the term and the text:\nRelative frequency\nof the term in the text, location of the term in the text,\nrelationship with other terms in the text etc.\n• Relation to corpus:\nRelation between the term and the\ndocument corpus or some other reference corpus can also\nbe used.\n• Expert Knowledge:\nExpert knowledge is a potential source\nbut is rarely used.\nThe most common approach is to consider the frequency of\noccurrence of terms in a given document in relation to their fre-\nquencies of occurrence in other documents in the collection. This\nscheme is known as the tf-idf scheme. Here is how tf-idf weights\ncan be computed for given terms:\n• Term Frequency:\nWords that occur more frequently in a\ngiven category are likely to be more signiﬁcant to the spec-\niﬁed category and are thus given higher weightage. Since\nthe occurrence of a rare term in a short text is more sig-\nniﬁcant than its occurrence in a long text, log of the term\nfrequency is used to reduce the importance of raw term\nfrequencies in those collections that have a wide range of\ntext lengths.\nAnaphoric references and synonyms reduce\nthe true term frequency. In morphologically rich languages,\npoor morphological analysis or stemming also adds to this\neﬀect.\n• Inverse Document Frequency: Terms that occur in almost\nfrequencies in those collections that have a wide range of\ntext lengths.\nAnaphoric references and synonyms reduce\nthe true term frequency. In morphologically rich languages,\npoor morphological analysis or stemming also adds to this\neﬀect.\n• Inverse Document Frequency: Terms that occur in almost\nall documents are useless for classiﬁcation. Therefore, terms\nthat occur in smaller number of documents are given higher\nweightage.\n• Inverse Category Frequency: Inverse Category Frequency\ncould be more appropriate than inverse document frequency\nsince the distribution of documents into categories may be\nskewed. A log can again be taken to weigh this factor down\nso that it does not become over-dominating.\n• Product of tf and idf:\nTerm frequency and Inverse Doc-\nument Frequency are inter-related. Terms that occur fre-\nquently in a particular class but not very frequently in other\n1.7. AUTOMATIC TEXT CATEGORIZATION\n53\nclasses are the most signiﬁcant. Hence a product of tf and\nidf is often used.\n• Length Normalization:\nLong and verbose texts usually use\nthe same terms repeatedly. As a result, the term frequency\nfactors are large for long texts and small for short ones, ob-\nscuring the real term importance. Term frequencies can be\nnormalized for length of texts by dividing them by the total\nword count in the document, or better still, by the frequency\nof the most frequently occurring term in the document.\n• Cosine Normalization The directions of the feature vectors\nrather than their lengths are considered to be better in-\ndicators of the various classes.\nIn cosine normalization,\neach term weight is divided by a factor representing the Eu-\nclidean vector length. Thus all vectors become unity length\nvectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives\nterm attribute 51\nterm frequency 52, 333\nterm weighting 333\ntext attribute 51\ntext categorization 6, 47\napproaches to 47\ncategory pivoted 49\ndocument pivoted 49\nhard 50\nmulti-label 50\nranking 50\ntext classiﬁcation 50, 53\ntext clustering 50, 53\ntext mining 94\ntext representation 50\ntext summarization 6, 39\napproaches to 41\nevaluation of 45\nIndian tradition 45\nrelation to IE 44\ntext tiling 344\ntext-to-speech - see speech syn-\nthesis\ntf-idf 51\ntf - see term frequency\ntheories of meaning 217\nIndian 218\nthesaurus 113\ntree adjoining grammar 205\ntrigrams 317\ntroponym 118\nTTS - see speech synthesis\nTuring test 27\ntype-token analysis 142, 237\nUCSG - see universal clause struc-\nture grammar\nunderstanding in context 20\nungrammaticality 20\nUNICODE 270, 287\nuniversal clause structure gram-\nmar 209\nunsupervised learning 246, 253\nvarNamaala 272\nvector space model 30\nViterbi algorithm 260\nword 121\ndeﬁnition 121\nformation 145\nhow many 140\nproperties 138\nstructure 143\nword group 134\nwordnet 113\nword sense disambiguation 56,\n226, 343\nworld knowledge 16, 26\nWSD - see word sense disam-\nbiguation\nyoogyata 209\n369\nFor More Information Visit\n202.41.85.68\n370\nAbout the Author\nKavi Narayana Murthy obtained his bachelors degree in Me-\nchanical Engineering from Bangalore University in the year 1983,\nMaster of Technology in Artiﬁcial Intelligence and Robotics from\nUniversity of Hyderabad in the year 1988, and PhD in Computer\nScience from University of Hyderabad in the year 1996. He has\nover 5 years of experience in reputed private and public industries\nand over 15 years of post graduate teaching and research expe-\nrience at the department of computer and information sciences,\nUniversity of Hyderabad. His research interests include Natural\nLanguage Processing and Speech Recognition.\nHe has over 40\npublications.\nHe has traveled widely and delivered lectures in\nmany prestigious academic institutions as well as industry. He\nknows several languages. His general interests include Sanskrit\nCricket.\nOne cannot be so sure that the document is about football\nif the word goal appears frequently because the word goal can\nappear in many diﬀerent kinds of documents with diﬀerent mean-\nings. Similarly, if some words, say, important, however, also, re-\n334\nCHAPTER 3. ADVANCES IN IR\ncently, although, change etc. occur frequently in some document,\nwe may not be able to say much about the semantic content of\nthe document. In fact these words may occur frequently in many\ndocuments on a variety of topics. Hence it makes sense to look\nat the number of diﬀerent documents in the collection in which\na particular term appears. If a term occurs in many, or almost\nall the documents, it is less useful as an indicator of any particu-\nlar topic. The Inverse Document Frequency (IDF) is the ratio of\nthe total number of documents in the collection to the number of\ndocuments in the collection in which the speciﬁed term occurs.\nThe term frequency is often dampened - √tf or 1 + log(tf) is\nused. This is because a document containing a term more often\nis more signiﬁcant but may not be by as much as the frequency\nitself suggests. If a term occurs once in d1 and thrice in d2, d2 is\nsurely more signiﬁcant than d1 for the given term but may not be\nreally three times as signiﬁcant. Also, there are length eﬀects. A\nlarge document and a small document should not be treated on\npar. The occurrence of a rare term in a small document is perhaps\nmore signiﬁcant than the occurrence of the same term in a large\ndocument.\nThus the tf values are often normalized for length\nby dividing them by the size of the document or, perhaps better\nstill, by the frequency of the most frequent word in the document.\nAnother formula that has been proposed to reﬁne the raw tf is\n0.5 + (0.5 ∗tf)/(maxtf).\nSince the total number of documents in a collection may be\nlarge, it is usual to squash the raw idf value also by taking loga-\nrithms.\nThe product of Term Frequency and Inverse Document Fre-",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-3",
                            "title": "Similarity Measures",
                            "content": "matching algorithms will continue to be statistical. It is not easy\nfor users to think of appropriate ways of weighting the query terms\nand hence this idea is diﬃcult to use.\n3.2.3\nSimilarity Measures\nHow do we quantify the similarity between two vectors? We have\nseen that directions between the two vectors can be taken as a\nmeasure of similarity between the two vectors.\n(Sometimes it\nis more convenient to talk in terms of dissimilarity rather than\nsimilarity. Greater the distance between two quantities, higher is\nthe dissimilarity. Hence the name Distance Measures.) It is usual\nto measure the Cosine of the angle between the two vectors rather\nthan the angle itself. The cosine will be 1 when the two vectors\n336\nCHAPTER 3. ADVANCES IN IR\ncoincide and 0 when they are completely disjoint (orthogonal).\nThe cosine measure is given by:\ncosine(q, d) =\nPn\ni=1 qi∗di\n√Pn\ni=1 q2\ni\n∗√Pn\ni=1 d2\ni\nIt is appropriate to normalize the lengths of all vectors to\n1. Otherwise, longer vectors (corresponding to longer documents)\nwould have an unfair advantage and the corresponding documents\ntend to get ranked higher than the shorter documents.\nIf the\nvector lengths are all normalized to 1, the denominator becomes 1\nand so the cosine is simply the dot product of the two vectors. It\ncan be shown that the cosine measure and the Euclidean distance\nproduce identical rankings.\n3.2.4\nThe Probability Ranking Principle\nIf the user does not ﬁnd many relevant documents in the ﬁrst\npage of returned results, he is often willing to look at the next\npage, thereby trading Precision for Recall. The Probability Rank-\ning Principle states that ranking documents in order of decreasing\nprobability of relevance is optimal. Retrieval is viewed as a greedy\nsearch where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the\naccurately within the given texts.\n• A Text Summarization system produces abstracts\nor summaries of documents so that we can select ap-\npropriate documents by looking at the summaries in-\nstead of the whole documents.\nThe challenge is to\nunderstand the essence of a document and produce\nappropriate summaries.\n• A Categorization system classiﬁes and groups to-\ngether similar documents so that it becomes so much\neasier to locate relevant documents. The big question\nis how to quantify similarity. Computers can only work\nwith quantiﬁed data, qualitative reasoning is best done\nby human beings.\n1.3. QUESTION ANSWERING SYSTEMS\n7\n• By embedding an Automatic Translation compo-\nnent, we may be able to handle multi-lingual and/or\ncross-lingual information processing. For example, a\nHindi query can be translated into English, the web\nsearched for relevant documents, the retrieved docu-\nments summarized and the summaries translated back\ninto Hindi for presentation to the user.\nAll these technologies are closely inter-related. For ex-\nample, we may retrieve relevant documents using IR and\nthen summarize the retrieved documents. Text Categoriza-\ntion and Summarization can be used prior to indexing or\nsearching.\nEach one of these technologies is a big ﬁeld in itself and\nall of them are very active areas of research today. There\nare many common threads and cross fertilization of ideas\ntakes place regularly across these areas.\nWe shall take a\nlook at some of these technologies brieﬂy in this book. Our\naim would be to give basic ideas behind these technologies.\nReaders interested in more detailed or more technical mate-\nrial will ﬁnd useful pointers in the Bibliography.\nThroughout, we will see examples of the need for a more\nthorough and detailed linguistic analysis to overcome the\ncurrent problems and challenges. Chapter Two will lead us\nthrough foundations of Natural Language Processing. We\nwill get back to more on Information Retrieval in Chapter\nThree.\n1.3\nvectors.\n1.7.5\nText Classiﬁcation and Clustering\nOnce texts are represented in terms of features and the weights\nof the features are computed, any of the standard classiﬁcation\nand clustering techniques can be applied. The section 2.3.3 gives\na brief description of some of these techniques.\nIn particular,\nthere we will show how the Bayesian Learning approach can be\napplied to the task of automatic text categorization. Models are\nbuilt from labelled training data and then applied to classify new\nunseen documents. When there is no labeled training data set\navailable, it is also possible to automatically cluster or group to-\ngether similar documents. A suitable measure of similarity must\nbe deﬁned. Often the term distance is used as a measure of dis-\nsimilarity. Clustering techniques work by attempting to reduce\nthe intra-cluster distances while maximizing the inter-cluster dis-\ntances. See section 2.3.3 for more on this.\nA number of automatic text categorization systems have been\ndeveloped and put to use for English and other major languages of\nthe world. Work on Indian languages has started only recently. A\ncategorization system developed recently by the author could clas-\nsify News Articles in Telugu into broad categories such as Sports,\nPolitics, Economics and Business and Cinema with nearly 95% ac-\ncuracy. The system was trained on a preclassiﬁed set of about 600\n54\nCHAPTER 1. THE INFORMATION AGE\ndocuments and tested on about 200 previously unseen documents.\nDeveloping automatic text categorization systems requires a\nlarge amounts of pre-classiﬁed training data.\nAlso, the perfor-\nmance of the system may deteriorate if the classes considered are\nﬁne grained or there is a lot of overlap. Performance also drops\ndown when the distribution of training documents in various cate-\ngories is non-uniform and highly skewed. To classify library books\ninto the standard categories, for example, would require a much\nclassiﬁcation.\nThe prior probabilities of each category Prior(Ci) are evalu-\nated as the ratios of the number of documents in category Ci to\nthe number of documents in the total collection.\n2.3. STATISTICAL APPROACHES\n253\nFinally, the posterior probabilities of each category are calcu-\nlated by adding the log likelihoods to the log priors.\nP(Ci|dj) = log(P(dj|Ci)) + log(Prior(Ci))\n(2.10)\nA test document is assigned the category with the maximum\nposterior probability.\nTo minimize misclassiﬁcation errors due\nto narrow diﬀerences, a threshold value can be used to include\na reject option. Performance can then be measured in terms of\nPrecision, and Recall. In order to capture the Precision-Recall\ntrade-oﬀin a single quantity, a combined measure such as the\nF-measure can be used.\nUnsupervised Learning: Clustering\nWhen we have training data where the instances are not labelled\nfor their correct class, we can still try to group the instances based\non their similarities and dissimilarities. Similarity or dissimilarity\nis measured in terms of features and a suitable distance measure.\nWhat we do is to group together instances that are similar and\nhence close to one another. The idea is to group instances into\nclusters such that the intra-cluster distances are smaller and the\ninter-cluster distances are larger.\nThis is kind of unsupervised\nlearning is called clustering.\nThere are several interesting clustering algorithms but we shall\nsketch only one of them here - the K-Means Clustering Algorithm.\nThe idea is simple. Let us say we have set of instances and we wish\nto group them into K classes. K is assumed to be known. We start\nby arbitrarily picking up K items from the given set of instances\nand treating them as the representatives of the K clusters to be\ndiscovered. Now the distances of each of the instances to these\nK cluster centres are computed and each instance is assigned to\nthe cluster it is closest to. Once this is done, the centroids of the\nslot fillers in output templates\nA single scoring measure known as the F-measure is also used\nto rank IE systems. It is an approximation to the weighted geo-\nmetric mean of recall and precision:\nF = (β + 1)PR\nβP + R\n(1.1)\n38\nCHAPTER 1. THE INFORMATION AGE\nwhere P is precision, R is recall, and β is a parameter en-\ncoding the relative importance of recall and precision. When we\ngive equal weightage to precision and recall, F-measure will be\n2PR/(P + R).\n1.5.3\nArchitecture of an IE System\nHobbs proposed a generic architecture for an IE system.\nThe\nHobbs system consists of the following ten modules:\n• Text Zoner - turns a text into a set of segments.\n• Preprocessor - turns a text or text segment into a se-\nquence of sentences, each of which is a sequence of lexical\nitems.\n• A Filter - turns a sequence of sentences into a smaller set\nof sentences by ﬁltering out irrelevant ones.\n• A Preparser - takes a sequence of lexical items and tries\nto identify reliably determinable small-scale structures.\n• A Parser - takes a set of lexical items (words and phrases)\nand outputs a set of parse-tree fragments, which may or\nmay not be complete.\n• Fragment Combiner - attempts to combine parse-tree or\nlogical-form fragments into a structure of the same type for\nthe whole sentence.\n• A Semantic Interpreter - generates semantic structures\nor logical forms from parse-tree fragments.\n• A Lexical Disambiguator - reduces the ambiguity of the\npredicates in the logical form fragments.\n• A Coreference Resolver - identiﬁes diﬀerent descriptions\nof the same entity in diﬀerent parts of a text.\n• A Template Generator - ﬁlls the IE templates from the\nsemantic structures.\nThe SIFT system developed by BBN, the LOLITA system de-\nveloped by University of Durham, The SRA IE system, the Pinoc-\nchio IE Toolkit, The CICERO system, genescene and Proteus-BIO\n1.6. AUTOMATIC SUMMARIZATION\n39\nare some of the major systems developed for Information Extrac-\ntion in the last few years.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-4",
                            "title": "The Probability Ranking Principle",
                            "content": "into a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nAlso we must not allow people to deliberately manipulate these\nranks by artiﬁcially creating a large number of dummy documents\nand make them all point to a given document to increase its rank.\nGoogle works by looking at all the documents that point to a\ngiven document and weight these pointers by the ranks of those\ndocuments. Thus a document gets a high rank if it has many links\ncoming from high ranked documents.\nOf course this deﬁnition\nis recursive but there is a simple iterative algorithm that keeps\nupdating the page ranks. It is not easy to fool the system for\nlong - even if dummy documents are created to enhance the page\nrank of some document, those dummy documents would soon get\nvery low ranks as no other high ranked documents would be really\nlinking to them. The web may appear to be a totally unorganized\nand uncontrolled mess but simple ideas like this can bring some\norder to the chaos.\nThe Google Page Rank (PR) computation formula is given\nbelow:\nPR(A) = (1 −d) + d(PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))\nwhere Tis are citations of document A and C(T) gives the total\nnumber of outgoing links from document T. Here d is a factor that\nspeciﬁes the relative importance of the current document to the\ndocuments which cite it.\n3.3.3\nRole of Linguistics\nStop Words\nWords that seem to be useless for searching are termed stop words.\nThese are mainly grammatical or function words such as articles,\nprepositions and conjunctions.\nWords such as the, from, could\nhave important grammatical function in sentences but are unlikely\nto help us in retrieving relevant documents using the bag-of-words\nrepresentation. Function words may be extracted from an elec-\n342\nCHAPTER 3. ADVANCES IN IR\nthe classiﬁcation performance.\nNote that each time we need to classify a given object, the k\nnearest neighbours will have to be computed. This involves com-\nputing the distances to all the objects in the collection. Therefore\nthis technique is an instance based classiﬁcation technique.\n250\nCHAPTER 2. FOUNDATIONS OF NLP\nNearest neighbour classiﬁcation techniques work quite well in\nmany application areas and are widely used.\nBayesian Learning Theory\nBayesian Learning is a probabilistic approach to inference based\non the assumption that the quantities of interest are governed by\nprobability distributions and the optimal decision can be made by\nreasoning about these probabilities together with observed data.\nIn corpus based approaches, probabilities are estimated by\ncounting the frequencies of favourable cases and all possible cases\nand taking the ratio of the two.\nFor example, the number of\ndocuments belong to a given category divided by the total number\nof documents would give the probability that a document belongs\nto the speciﬁed category. Thus the probabilities we are talking\nhere are empirical probabilities.\nP(X) denotes the probability of some event X. P(X, Y ) de-\nnotes the joint probability of both the events X and Y occurring\ntogether. P(X|Y ) denotes the probability of X given that Y has\nalready occurred. It is thus a conditional probability. P(X|Y )\ncan be computed as P(X,Y )\nP(Y ) . Similarly, P(Y |X) is P(Y,X)\nP(X) .\nSince P(X, Y ) and P(Y, X) are the same, we can combine these\nlast two equations to get\nP(X|Y ) = P(Y |X)∗P(X)\nP(Y )\nThis is known as Bayes Theorem. To understand Bayes The-\norem let us replace X by “disease” and Y by “symptoms”. Then\nwe get:\nP(disease|symptoms) = P(symptoms|disease)∗P(disease)\nP(symptoms)\nWe are interested in computing the probability that a patient\nhas a particular disease given the symptoms he has. For exam-\nple, we may wish to compute the probability that a patient has\nsearch where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the\nviolation of this assumption is when there are duplicates in the\ncollection. If an ambiguous word such as capital is included in the\nquery, an optimal system may be expected to retrieve and present\nthe documents so that the user sees this ambiguity but the PRP\nprinciple would give documents that are maximally relevant for\neither of the two senses of the ambiguous word. Further, relevance\nis diﬃcult to quantify and measure accurately. At best we make\ngood estimates. It may be worth looking at the variance of these\nestimates and prefer those decisions with lower variance.\n3.2.5\nPerformance Evaluation\nAs we have already seen, Precision is the percentage of relevant\ndocuments in the returned set and Recall is the percentage of\nall relevant documents in the collection that is in the returned\nset. However, most IR systems produce a ranked list of returned\n3.2. BASIC IR MODELS\n337\ndocuments and a case where the last three of the ten documents\nreturned are relevant cannot be equated with the case where the\nﬁrst three out of ten are relevant. Most users scan the returned\ndocuments from top to bottom and would like to see many relevant\ndocuments right at the top. Thus by measuring the precision at\nseveral initial segments of the ranked list, one may obtain a good\nimpression of how well the system ranks relevant documents. We\nmay therefore measure the Precision at speciﬁed cutoﬀlevels such\nas 5, 10, 20 or 100 from the top. By considering all the documents\nabove a relevant document and computing Precision and then by\naveraging all such Precision values for each of the relevant docu-\nment retrieved, one gets an uninterpolated average Precision. This\naverage Precision will be 1 if all the relevant documents are at the\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nInstead of interactively improving the query, one may assume\nthat the top few (say, 20) hits are all actually relevant and au-\ntomatically improve the query without asking users to judge any\nretrieved documents. This technique is termed Pseudo-relevance\nFeedback.\n3.3.2\nPage Ranking\nRetrieving the documents that best match the given query does\nnot necessarily give us the best overall performance. There are a\nvery large number of documents on the Internet and too many of\nthem may match equally well. Simply listing a large number of\nrelevant documents is not very good. How do we help the users\nthen? We should use not only the terms in the documents and\nqueries for matching and ranking the documents, but also some\ngeneral measure of goodness of various documents. It would be\nnice if we could somehow compute authenticity or dependability\nof documents but there is no simple way to do that. What search\nengines such as Google do is to instead use the popularity of the\ndocuments as a measure of goodness. If many people are look-\ning at a document perhaps there is something important or useful\nabout it. But how do we ﬁnd out who is looking at which docu-\nment and for what purpose? One thing that is clearly computable\nfrom the web of documents on the Internet is the hyperlink struc-\nture that links up various documents. We could look at the links\n3.3. TOWARDS INTELLIGENT IR\n341\ngoing out from a given document but perhaps the links coming\ninto a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-2-subsection-5",
                            "title": "Performance Evaluation",
                            "content": "things than we expect of them.\nIn terms of speed, automated systems are far superior to man-\nual methods in most tasks. Computers can process thousands,\nlakhs or even millions of documents in the time it takes us to\ngo through just a few pages. In terms of accuracy, automated\nmethods give comparable or superior performance today in cer-\ntain areas such as Information Retrieval, and Text Categorizations\nwhen the data is very large. On the other hand, people are often\n1.11. SHAPE OF THINGS TO COME\n81\nmuch better compared to machines when it comes to tasks such\nas translation or question answering. But we must remember that\neven when automated systems give good performance, their scope\nis often limited and they may be brittle - even a small change\nin the conditions may bring down the performance drastically.\nFor example, today’s speech recognition systems are not robust\nagainst such simple looking variations as change of microphone\nor even moving the head a little away from the microphone while\nspeaking. Human beings are generally more robust than machines\nin most cases.\nThe question therefore is not whether people are better or\nmachines are better. Each of them have their own strengths and\nweaknesses. The challenge is to properly understand the strengths\nand weaknesses of the two and build man-machine synergies that\ncan produce the best results with minimal eﬀort.\n1.11\nShape of Things to Come\nSo far we have seen several interesting and useful applications of\nhuman language technologies. Our treatment has been introduc-\ntory and informal. The focus has been on what kinds of things\ncan be and have been done. In the process we have got some feel\nfor the need for thorough linguistic and statistical analysis of lan-\nguage, techniques for representing and reasoning with knowledge,\ncomputational methodologies for making the machines learn from\ndata, and large scale linguistic data resources that permit eﬀective\nlearning and generalization by machines. We have made progress\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\nLanguage Understanding. However, some researchers in the IR\nﬁeld have traditionally considered it neither absolutely essential\nnor always highly beneﬁcial to carry out in depth linguistic anal-\nysis of documents or user queries. The challenge they have set for\nthemselves is to achieve high levels of retrieval performance with-\n34\nCHAPTER 1. THE INFORMATION AGE\nout recourse to Natural Language Processing (NLP) in any great\nmeasure. Empirical studies of the use of advanced NLP techniques\nhave also given mixed results - in some cases there was some im-\nprovement in performance while in other cases either there was no\nsigniﬁcant improvement or there was actually a small reduction in\nperformance. This could be for various reasons including possibly\nthe kind of linguistic analysis that was carried out and the speciﬁc\nIR tasks and evaluation methods employed.\nPart of the reason for diﬀerences of opinion on the role of\nNLP is the criteria for success. What is it that we want in the\nend? Is the performance measured in terms of Precision, Recall\nor whatever the only criterion for success? Or are we looking for\nintelligent IR systems, intelligent IE systems and so on? In the\nlong run, speciﬁc applications such as IR, IE, Categorization and\nSummarization must be viewed in the context of intelligent pro-\ncessing of human languages by machines. If we can move towards\nmachines that are capable of human-like understanding, generat-\ning and learning natural languages, we can not only get better\nIR systems but also many other applications that have not taken\noﬀyet. Automatic Programming has long remained unsuccess-\nful. If one day we could tell computers instead of program them,\nthe whole world of software engineering will change dramatically.\nResearch should not be constrained too much by forces of pro-\nfessionalism. Asking the right questions is more important than\nbeing successful all the times.\nThe bag of words representation is too crude. Documents are\neven incorporate a semantics component in any serious measure.\nThere is even a claim that a lot can be done without going for\nany in-depth linguistic analysis. And practical experience does\nshow that in some tasks high performance can be achieved by\nonly a very superﬁcial analysis provided large scale training data\nis available and the right methods are used for learning from this\ndata. For example, text categorization systems achieve 95% plus\nperformance by using raw words as features, without need for any\ndictionary or morphological analysis, let alone syntax or seman-\ntics. Human beings are also far from perfect and in some tasks\nit is possible to achieve performance comparable to or even some\nwhat better than human performance without need for in-depth\nlinguistic analysis.\nIt would be wrong, however, to conclude that machines have\nreached or are approaching human levels of intelligence. Given\nsuitable, data, sets of features to use, a method to compute and\nweigh the features and a classiﬁcation method, automatic tech-\nniques exist for performing classiﬁcation in an optimal way as\ndeﬁned by some given criteria. Human beings can also do this,\nalthough they may take more time than computers or commit\nmistakes while computing, but they can also do much more. Ma-\nchine learning today is largely restricted to generalization from\nexamples. But we human beings can ’learn’ even new methods of\nmachine learning. We learn to identify discriminative features, we\nlearn to weigh them and we learn the optimization criteria and\nmethods. There is no comparison between machines and human\nbeings. High performance is achievable only in some restricted\ntasks under suitable assumptions. This does not mean that ma-\nchines can understand the meaning of texts nor does it mean that\nunderstanding meanings is not necessary.\nIn the section on syntax, we said form follows function and\nhence knowing the structure greatly facilitates understanding the\nValues of the decision variable for the two classes are chosen sym-\nmetrically around zero and the parameters are estimated from the\ntraining data. A test sample can then be classiﬁed as belonging to\nclass C1 or C2 depending upon whether the value of the decision\nvariable is positive or negative. It is possible to reject a point if\nthe value of the decision variable is too close to zero, say, closer\nthan a speciﬁed threshold.\nClassiﬁcation performance can be speciﬁed in terms of Pre-\ncision and Recall, or using some combined measure such as the\nF-Measure:\nRecall =\nOk\nTotal ∗100\n(2.6)\nPrecision =\nOk\nTotal −Unknown ∗100\n(2.7)\nF = 2PR\nP + R\n(2.8)\nwhere Ok is the number of test samples that are correctly classi-\nﬁed, Unknown is the number of test samples that are not classiﬁed\nand Total is the total size of the test data. There is usually a trade\noﬀbetween Precision and Recall and a single combined measure is\ntherefore useful for comparison. F-Measure is one such measure.\nThe deﬁnition shown here gives equal weightage for Precision and\nRecall.\nWe have outlined a general method for supervised two-class\nclassiﬁcation using Multiple Linear Regression.\nThe method is\nconceptually simple and based on sound theoretical foundations.\nThe method is symmetric in the features. Although matrix inver-\nsion is required for estimating the values of the parameters, once\n2.3. STATISTICAL APPROACHES\n249\nthe model is built classifying objects is very eﬃcient - only com-\nputation of the linear regression equation and checking the sign\nof the decision variable are required. The technique is thus highly\nsuitable for two-class classiﬁcation problems with a reasonably\nsmall number of features.\nTechniques also exist for validating the adequacy of the model\nfor a given problem and for evaluating the relative signiﬁcance of\nthe various features (which can be used for feature selection).\nWe will illustrate the use of regression as a classiﬁcation tool",
                            "children": []
                        }
                    ]
                },
                {
                    "id": "chapter-3-section-3",
                    "title": "Towards Intelligent IR",
                    "content": null,
                    "children": [
                        {
                            "id": "chapter-3-section-3-subsection-1",
                            "title": "Improving User Queries - Relevance Feedback",
                            "content": "question, we may have to deal with degrees of relevance. No single\ndocument may be very relevant but a set of them could together\nbe. Evaluating the relevance of a document for a given query is\nthus a very complex task and we need to make some simplifying\nassumptions in practice. For example, we may use the simplest\nnotion of relevance that the query string appears verbatim in the\ndocument. A slightly less strict notion could be that the words in\nthe query appear frequently in the document, in any order (bag\nof words).\nInstead of looking for improving the performance of retrieval\nfor given queries, we may invert the problem and see how the\nquery itself may be improved so that performance is maximized.\nThis makes sense because it is not always very easy for a user\nto specify exactly what he wants as a query. An ideal query is\none that expresses the user’s requirements precisely in relation to\nthe documents in the collection.\nBut ﬁnding an ideal query is\ndiﬃcult unless we already know exactly what the documents in\n340\nCHAPTER 3. ADVANCES IN IR\nthe collection contain. We may therefore start with a good guess\nand hope to improve after we see some results. We make the as-\nsumption that documents which are relevant to a given query are\nsimilar. Therefore a query can be improved by making it closer\nto relevant documents retrieved and farther from the irrelevant\ndocuments retrieved. Thus by looking at the returned results and\njudging them as relevant or otherwise, we may obtain an improved\nquery. More terms from the relevant documents retrieved can be\nadded and weights for the terms can also be adjusted based on\nthe frequency of occurrence of those terms in the relevant and ir-\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nInstead of interactively improving the query, one may assume\nthat the top few (say, 20) hits are all actually relevant and au-\ntomatically improve the query without asking users to judge any\nretrieved documents. This technique is termed Pseudo-relevance\nFeedback.\n3.3.2\nPage Ranking\nRetrieving the documents that best match the given query does\nnot necessarily give us the best overall performance. There are a\nvery large number of documents on the Internet and too many of\nthem may match equally well. Simply listing a large number of\nrelevant documents is not very good. How do we help the users\nthen? We should use not only the terms in the documents and\nqueries for matching and ranking the documents, but also some\ngeneral measure of goodness of various documents. It would be\nnice if we could somehow compute authenticity or dependability\nof documents but there is no simple way to do that. What search\nengines such as Google do is to instead use the popularity of the\ndocuments as a measure of goodness. If many people are look-\ning at a document perhaps there is something important or useful\nabout it. But how do we ﬁnd out who is looking at which docu-\nment and for what purpose? One thing that is clearly computable\nfrom the web of documents on the Internet is the hyperlink struc-\nture that links up various documents. We could look at the links\n3.3. TOWARDS INTELLIGENT IR\n341\ngoing out from a given document but perhaps the links coming\ninto a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nrate it is doing now, the amount of user time spent in locating\nrelevant information will also keep growing. Hence we need to\nlook for alternative architectures for searching and information\nretrieval from the web.\nMeta search engines have several advantages over direct use\nof search engines. Unlike search engines, meta search engines can\nreside locally and can be customized or adapted to the needs of\na speciﬁc user or a user group.\nA meta search engine can be\nlinked to a local database so that web search can be replaced with\nlocal search in some situations and frequently downloaded pages\ncan be locally archived. Advanced techniques for automatically\nbuilding and adapting user models are of great interest. It has\nbeen seen that users generally restrict themselves to a particular\nsubset of topics when they initiate a web search.\nThis can be\nused to construct a user proﬁle which should be extremely use-\nful in search optimization. More detailed understanding of the\nuser’s needs becomes possible as the meta search engine is local\nand customizable. For example, you may specify whether you are\nseriously looking for an answer to a very speciﬁc question or you\nare generally exploring what all is available. Search engines treat\neach query as a fresh task and they have no idea of what you have\nalready searched, what all you already know and what exactly\nyou are now looking for. Meta search engines can be customized\nto work in the background mode so that user’s time in waiting\nfor results can be minimized. A meta search engine can moni-\ntor the network bandwidth dynamically and adjust the ﬁring of\nthe various search engines accordingly. Users need to learn a sin-\ngle query interface since the system automatically formulates the\n348\nCHAPTER 3. ADVANCES IN IR\nuser’s query as required by various search engines. The system\ncan also learn statistical models of search engines and the pages\nthey index so that it becomes possible to choose the right search\nto go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant\nto the query. As we have already seen, the most common approach\nis to view documents and queries are bags of words and represent\nthem as feature vectors where each feature corresponds to one\nword. Similarity between feature vectors is quantiﬁed in terms\nof the orientations of these vectors. Performance is measured in\nterms of Precision and Recall.\nIntelligent IR, on the other hand, requires that we consider the\nsyntax as well as semantics of documents and queries, we adapt\nto users based on direct or indirect feedback and learning, and we\ntake care of authority and dependability of documents. An ideal\ninformation retrieval system is one that can perform like a human\nassistant. This is really the software grand challenge. Obviously,\nwe are far from such an ideal.\nLet us now see what kinds of\nimprovements and enhancements can be or have been made in\nthe ﬁeld of information retrieval.\nMere presence or absence of keywords is clearly too naive a\nview of a document. Can we say I have a bad head ache and Now\nI am free from head ache mean the same thing and both match\nthe query head ache equally well? Is India beat Australia same as\nAustralia beat India? Can we equate I like Govinda’s movies and\nI like Govinda’s movies as much as I would like a burning stove if\nI were sitting on it? Most current IR systems continue to use the\nbag-of-words representation while examples like this clearly show\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nunchangeable. Meaning depends upon the reader’s interest, back-\nground, purpose, attitudes etc. Relevance is not a simple yes-no\nquestion. We can therefore think of degree of relevance. No sin-\ngle document may be highly relevant but a combination of two or\nmore may be.\nClearly, such a strong view of relevance is subjective and im-\npracticable for automatic evaluation. Hence in practice a weaker\ndeﬁnition involving only the topical relevance is used as an indica-\ntor of potentially useful documents. Topical relevance is necessary\nbut not suﬃcient. It is easier to deal with and often a major con-\ntributor to total relevance. Topical relevance is more closely tied\nup with the document itself and not so much to idiosyncrasies\nof individual users. The relevance of documents is measured by\nsimply counting the proportion of terms in the query which are\nfound in the documents retrieved. The retrieved documents are\nranked accordingly.\nWhat we have seen in this section is the bare-bones descrip-\ntion of a modern IR system. It is deﬁcient in many ways and a\nlarge number of ideas have been proposed and used to go beyond\nthis primitive design. Chapter Three of the book is devoted to ad-\ndressing these concerns in some detail and Chapter Two provides\nthe required background in Natural Language Processing.\n1.4. INFORMATION RETRIEVAL\n33\n1.4.6\nChallenges in Information Retrieval\nIn his Turing Award lecture, Jim Grey deﬁned the Software Grand\nChallenge as a software that could answer questions as eﬀectively\nas an educated person. Answering questions, or even just retriev-\ning relevant documents from which we can hopefully ﬁnd answers\nto our speciﬁc questions, is not easy. There are three steps in the\nprocess and each one is a challenge - 1) understand exactly what\nthe user wants 2) understand the contents of the documents so\nyou know which document is relevant for what, and 3) develop\nautomatic methods for matching the user requirements with the",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-2",
                            "title": "Page Ranking",
                            "content": "into a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nAlso we must not allow people to deliberately manipulate these\nranks by artiﬁcially creating a large number of dummy documents\nand make them all point to a given document to increase its rank.\nGoogle works by looking at all the documents that point to a\ngiven document and weight these pointers by the ranks of those\ndocuments. Thus a document gets a high rank if it has many links\ncoming from high ranked documents.\nOf course this deﬁnition\nis recursive but there is a simple iterative algorithm that keeps\nupdating the page ranks. It is not easy to fool the system for\nlong - even if dummy documents are created to enhance the page\nrank of some document, those dummy documents would soon get\nvery low ranks as no other high ranked documents would be really\nlinking to them. The web may appear to be a totally unorganized\nand uncontrolled mess but simple ideas like this can bring some\norder to the chaos.\nThe Google Page Rank (PR) computation formula is given\nbelow:\nPR(A) = (1 −d) + d(PR(T1)/C(T1) + ... + PR(Tn)/C(Tn))\nwhere Tis are citations of document A and C(T) gives the total\nnumber of outgoing links from document T. Here d is a factor that\nspeciﬁes the relative importance of the current document to the\ndocuments which cite it.\n3.3.3\nRole of Linguistics\nStop Words\nWords that seem to be useless for searching are termed stop words.\nThese are mainly grammatical or function words such as articles,\nprepositions and conjunctions.\nWords such as the, from, could\nhave important grammatical function in sentences but are unlikely\nto help us in retrieving relevant documents using the bag-of-words\nrepresentation. Function words may be extracted from an elec-\n342\nCHAPTER 3. ADVANCES IN IR\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\nrelevant documents. Usually performance increases substantially\nwith just one iteration but if required the process can be iterated\nupon. This process is called Relevance Feedback. Which terms\nto add and how exactly to adjust the weights are the important\nissues.\nInstead of interactively improving the query, one may assume\nthat the top few (say, 20) hits are all actually relevant and au-\ntomatically improve the query without asking users to judge any\nretrieved documents. This technique is termed Pseudo-relevance\nFeedback.\n3.3.2\nPage Ranking\nRetrieving the documents that best match the given query does\nnot necessarily give us the best overall performance. There are a\nvery large number of documents on the Internet and too many of\nthem may match equally well. Simply listing a large number of\nrelevant documents is not very good. How do we help the users\nthen? We should use not only the terms in the documents and\nqueries for matching and ranking the documents, but also some\ngeneral measure of goodness of various documents. It would be\nnice if we could somehow compute authenticity or dependability\nof documents but there is no simple way to do that. What search\nengines such as Google do is to instead use the popularity of the\ndocuments as a measure of goodness. If many people are look-\ning at a document perhaps there is something important or useful\nabout it. But how do we ﬁnd out who is looking at which docu-\nment and for what purpose? One thing that is clearly computable\nfrom the web of documents on the Internet is the hyperlink struc-\nture that links up various documents. We could look at the links\n3.3. TOWARDS INTELLIGENT IR\n341\ngoing out from a given document but perhaps the links coming\ninto a given documents from other documents is a better measure\nof its importance. Thus the rank of a page goes high if it is pointed\nto by many documents. Of course the documents that point to a\ngiven document themselves may or may not be very important.\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\ndatabase or an index of web pages. The task of creating an in-\ndex is made complicated by the large size of the data as also the\nhighly dynamic nature of the web, lack of any centralized con-\ntrol and the heterogeneity of the document types and formats.\nToday IR means retrieving relevant documents from very large\ncollections such as from the web. Full text indexing is taken for\ngranted.\n330\nCHAPTER 3. ADVANCES IN IR\n3.2\nBasic IR Models\nIn ad-hoc retrieval, the most common view of IR, an unaided\nuser expresses his need through a short query. The IR system\nmatches the query against the documents in the collection and\nreturns the documents that match. The returned documents may\nbe given in a ranked order.\nThe document collection is ﬁxed\nand a one time indexing on the whole collection is performed to\ncreate and store an index. Matching is performed on the index,\nnot on the original documents. Each time a user issues a query, a\nmatch operation is performed and the results returned. Document\nFiltering provides an alternative view. Here the query may be\nconsidered ﬁxed and a stream of documents need to be checked.\nThe query expresses what exactly a particular user wants or does\nnot want. Documents are ﬁltered out accordingly. There is no\nscope to perform indexing of the whole collection of documents\nsince the collection is not available beforehand and documents\nkeep coming. Here the decision is usually Boolean - include or\nexclude a document. Ranking is also conceivable. Routing is also\nsimilar to ﬁltering except that the documents ﬁltered in may also\nhave to be routed to diﬀerent agencies. If nothing is mentioned,\nad-hoc retrieval is usually taken as the the default mode.\n3.2.1\nIR Models\nsearch where at each step we try to identify the most relevant\ndocument yet to be retrieved and ﬁnally rank the obtained doc-\numents in decreasing order of relevance. This assumes that the\ndocuments are independent. A clear but extreme example of the\nviolation of this assumption is when there are duplicates in the\ncollection. If an ambiguous word such as capital is included in the\nquery, an optimal system may be expected to retrieve and present\nthe documents so that the user sees this ambiguity but the PRP\nprinciple would give documents that are maximally relevant for\neither of the two senses of the ambiguous word. Further, relevance\nis diﬃcult to quantify and measure accurately. At best we make\ngood estimates. It may be worth looking at the variance of these\nestimates and prefer those decisions with lower variance.\n3.2.5\nPerformance Evaluation\nAs we have already seen, Precision is the percentage of relevant\ndocuments in the returned set and Recall is the percentage of\nall relevant documents in the collection that is in the returned\nset. However, most IR systems produce a ranked list of returned\n3.2. BASIC IR MODELS\n337\ndocuments and a case where the last three of the ten documents\nreturned are relevant cannot be equated with the case where the\nﬁrst three out of ten are relevant. Most users scan the returned\ndocuments from top to bottom and would like to see many relevant\ndocuments right at the top. Thus by measuring the precision at\nseveral initial segments of the ranked list, one may obtain a good\nimpression of how well the system ranks relevant documents. We\nmay therefore measure the Precision at speciﬁed cutoﬀlevels such\nas 5, 10, 20 or 100 from the top. By considering all the documents\nabove a relevant document and computing Precision and then by\naveraging all such Precision values for each of the relevant docu-\nment retrieved, one gets an uninterpolated average Precision. This\naverage Precision will be 1 if all the relevant documents are at the",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-3",
                            "title": "Role of Linguistics",
                            "content": "concerned with social aspects of language. Language is related\nto social status, power and politics. Historical linguistics is con-\ncerned with linguistic genetics and language change over time.\n2.3. STATISTICAL APPROACHES\n233\nLanguage policy is important as it has direct implications for the\npeople. Should the primary education be in the mother tongue\nor in some other language? The three language formula in our\ncountry is a example of language policy implementation. Teach-\ning language is another aspect that is closely related other areas\nof language and linguistics. Language teaching requires method-\nologies and techniques diﬀerent from those required for teaching,\nsay, mathematics or science. There are even language games that\ncan promote eﬀective language learning. To know the importance\nof language try to spend one day, just one day, without speaking,\nlistening, reading or writing! That would be extremely diﬃcult.\n2.3\nCorpus Based and Statistical Ap-\nproaches\nAt one point of time, experts used to specify what is right or ac-\nceptable and what is not. Grammars used to be prescriptive in\nnature and students could be punished for not following the rules.\nThis strictness was considered essential for maintaining the purity\nand standards. After all language is for communication and we\ncannot eﬀectively communicate if we all do not follow a set of com-\nmonly accepted protocols and standards. Looseness and lightness\nof thought about language is really more dangerous than careless\nuse of language. We often hear people say that grammar is not\nimportant and as long as they can communicate with others, that\nis good enough. But one must remember that we cannot commu-\nnicate eﬀectively unless we take language a bit more seriously. A\nlarge number of day to day problems at home or oﬃce can ac-\ntually be traced to communication problems arising from laxity\nin the use of language. Carelessness about language is harmful.\nlife and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of\nthe limited interface linguists and engineers have had in In-\ndia on topics of relevance here. Linguistics itself has changed\nv\nsubstantially over time. There have been competing theories\nand view points. Focus areas, theories or models adapted\nand view points have varied substantially even within the In-\ndian context. Generalizations and extrapolations can thus\nbe dangerous. Comments on the current state of technology\nshould be taken with care as technology keeps changing very\nfast. All views expressed are especially applicable to the In-\ndian scene. If some of the statements appear a bit negative,\nit is because the assessment being made is with respect to\nthe perfect, the ideal, with respect to what we could have\nachieved. It is not that all the good work that has been done\nis not appreciated. It should be taken in the positive sense\nthat we have the capability to do much more. We can be\nworld leaders. It is essential to have this optimism but at\nthe same time we must plan our work based on the knowl-\nedge and understanding of the ground realities. This is the\nrecipe to success. I hope this view will be appreciated and\nwell received.\nKavi Narayana Murthy\ncaitra s’ukla pratipat, paarthiva naama saMvatsara\n(chaandramaana yugaadi - 9 April 2005)\nvi\nFOREWORD\nWhen I agreed to chair on the occasion of Sarada Ran-\nganathan Memorial Lectures 2004 delivered by my former\ncolleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much\n97\nhome the problem by taking a down to earth example here, the\nnature of diﬀerence in the orientation of linguistics and NLP is\nthe same at all levels of abstraction.\nNLP forces linguistic theories, models, ideas and hypotheses\nto be tested on large scale, real world data. This helps to validate\nthe theories, ﬁnd places where they fail, pick up counter examples\nand reﬁne the theory or model accordingly. The NLP approach\nalso makes available large scale data which can be used to build\ntheories and models. Manual methods are tedious, time consum-\ning and hence limited to small data sets. They are also prone to\nhuman biases and misinterpretations. Testing on large scale real\nworld data often throws out new insights.\nAny aspect of human language processing can be modeled and\ntested in NLP as long as the data and rules to handle all cases\nare covered exhaustively in detail and in precise enough terms for\na computer to follow. Priorities for development may be dictated\nby concerns of practical need etc. but there is really no area of\nlinguistics that NLP cannot touch.\nNLP attaches great importance to computational eﬃciency in\nterms of memory space and computation time.\nLinguistics at-\ntaches great importance to simplicity, elegance and economy of\nexpression but eﬃciency in terms of computational space and time\nis usually not a major concern except in certain areas such as psy-\ncholinguistics. The fundamental assumption in AI is that human\nbeings are intelligent and doing things in a simpler, more elegant,\nfaster and more eﬃcient way is an important element of human\nintelligence. In this sense, if an NLP approach shows a much more\neﬃcient method of solving a particular problem, linguists cannot\naﬀord to ignore the implications. After all, linguistics is all about\nhuman language faculty and how it intelligently handles language.\nApplication orientation also often helps in ﬁxing priorities and\nplans of action in a more systematic and justiﬁable manner. It\nof no consequence in many areas of linguistics. However, quan-\ntities such as these can have considerable statistical signiﬁcance\nand statistical models of language often exploit such quantities as\nuseful features.\nModern linguists have also taken a layered view of language\nand each level of linguistic description is studied more or less inde-\npendently of other levels. That is, the interactions between layers\nare presumed to be separable from the core of a given level itself so\nthat the core and the interactions with the neighboring levels can\nbe explored separately to characterize any particular level. For\nexample, Chomsky claims “autonomy of syntax” by saying that\nsyntax can be studied more or less independently of other levels of\nlinguistics. Syntax is presumed to be loosely-coupled with other\nlevels, not tightly and inseparably coupled.\nThe above stratiﬁed view of language raises certain basic ques-\ntions. For example, can we understand the meaning of a sentence\nin terms of the meaning of the words in the sentence? Or is a\nsentence an atomic unit of meaning and breaking it into words\nartiﬁcial and unhelpful? Such questions have been scientiﬁcally\n2.2. COMPUTATIONAL LINGUISTICS\n99\nexplored in great depth and detail within the Indian tradition\ndating back to several thousand years. In fact whether there are\nidentiﬁable units such as words and sentences is a question that\ndeserves careful consideration. As opposed to the stratiﬁed model,\none can posit a holistic view of language and claim that we pro-\ncess and understand language all in one go, not layer by layer. Do\nwe perform a dictionary look up and morphological analysis ﬁrst\nand then perform syntactic analysis and ﬁnally get into seman-\ntics in our minds to understand the meaning of an utterance? Or\ndo we do all these things together, in a closely integrated fashion?\nOn the one hand these are questions of implementation strategies.\nAt the same time, these are basic questions relating to layered or\nguages express more of syntax in morphology than other\nlanguages, e.g., verb arguments are incorporated into the\nverb.\nThis classiﬁcation is quite artiﬁcial.\nReal languages rarely\nfall cleanly into one of the above classes, e.g., even Mandarin\n2.2. COMPUTATIONAL LINGUISTICS\n137\nhas a few suﬃxes. Dravidian languages are inﬂectional as well\nas agglutinative. Moreover, this classiﬁcation mixes the aspect of\nwhat is expressed morphologically and the means for expressing\nit.\nThere is a lot more to it than meets the eye:\nBy now you must have understood that even a simple looking\ntask such as breaking sentences into words is really a very complex\ntask. It is easy to build demonstration systems to give a false\nimpression of progress and success. You might have heard people\nclaim that they have solved all problems. Researchers must learn\nto see through the hype and propaganda and get straight to the\nground realities. NLP is not easy. NLP should not be compared\nwith telephones or cars. NLP requires working with meanings,\nNLP requires working with complex issues of human cognition.\nNLP can only be compared with other similar tasks that are also\nlinked to human cognition. Have we been able to understand how\nwe learn? Have we been able to understand how we understand\nspeech? Have we been able to understand how we can recognize\na person by his/her voice, hand-writing or a momentary glance\nat his/her face? May be but in a very limited way. NLP is no\ndiﬀerent. Fantastic results should not be expected. Do not fall\nfor false claims.\nIt is also not a very good idea for every young researcher\nto jump into machine translation, information extraction, speech\nrecognition or other such big tasks. We need to do a lot of ground\nwork before we can see real successes in such large tasks. The\ncomputational dictionaries and morphological analyzers we have\ntoday for Indian languages are all far from perfect, far from ade-\nquate. It takes many years of real hard work to prepare a good",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-4",
                            "title": "Latent Semantic Indexing",
                            "content": "improved, then the role of NLP could be better appreciated. User\nperspectives matter a lot. Many users may be happy that they\nare able to get something useful from the Internet and not both-\nered too much about the low precision and recall of the current\nsystems. As user expectations grow, and IR is applied for more\nserious and highly speciﬁc retrieval tasks, the need for moving\ntowards more intelligent models would be felt more and more.\n3.3.4\nLatent Semantic Indexing\nThe bag-of-words representation treats words and phrases as iso-\nlated occurrences. In fact the co-occurrence of terms has a sig-\nniﬁcant implication for the semantic content of documents. Co-\noccurrence also implicitly disambiguates words. For example, if\nthe word bank occurs frequently with words such as cheque, with-\ndraw, deposit, balance, interest, etc. then it indicates the ﬁnancial\ninstitution sense, not the bank of a river. Latent Semantic Index-\ning (LSI) is a technique by which co-occurring terms are grouped\ntogether and treated similarly.\nLSI actually projects from the\noriginal n-dimensional vector space with n diﬀerent terms onto a\nlower dimensional space such that co-occurring terms fall along\nsame dimensions and non-co-occurring terms are projected onto\ndiﬀerent dimensions. The idea is that the latent, that is hidden,\n’true’ semantic space of possibilities is obtained from the surface\nrepresentation where each term is treated as a separate entity and\nthe semantic relationships between the terms are implicit. In the\nlatent semantic space, a query and a document can have high sim-\nilarity even if they do not share any terms, as long as the terms\nare semantically similar according to the co-occurrence analysis.\nLSI can thus be viewed as a similarity metric and an alternative\nto word overlap measures such as tf.idf. The latent semantic space\nhas fewer dimensions and thus LSI can also be looked upon as a\ndimensionality reduction technique.\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE\nentity) is a possible feature. Features are given numerical values.\nThus feature vectors can be geometrically visualized as points in\nn-dimensional space or as vectors connecting the origin to these\npoints. The spatial similarity between such vectors is used as a\nmetaphor to deﬁne the similarity between documents and queries.\nRetrieved documents are ranked based on the degree of similarity.\nTECHNOLOGY\nLANGUAGE\nQUERY (\"LANGUAGE TECHNOLOGY\")\nDOC−1\nDOC−2\nDOC−3\nFIG 3.1 The Vector Space Model in Two Dimensions\nThere are several issues in the design of a vector space model.\nHow do we determine which terms in a given document are impor-\ntant? What about word sense? Should we take the surface form\nof words or the root forms? What deﬁnition of word do we use?\nHow do we take care of multi-token words, compounds, idioms\nand phrases? What is the relationship between the importance of\na given term for a given document in relation to its importance\nfor other documents in the collection or for the whole collection?\nHow exactly do we compute the similarity between a query and\na document? In the case of a large hyperlinked and highly dy-\nnamic collection such as the world wide web, what exactly is the\ncollection and what are the eﬀects of the hyperlinks, formatting\ninformation and meta data available? There are no perfect an-\nswers to all the questions but we will be able to give some ideas\nas we proceed.\n3.2. BASIC IR MODELS\n333\nVector space models based on the bag-of-words representa-\ntion ignores syntax (word order, phrase structure, proximity, and\nmore) and semantics (word senses, scope of quantiﬁers and nega-\ntion, anaphoric references, synonymy and other kinds of relation-\nships between words and between words and concepts). “Restau-\nrant” will not match with “cafe”. “PRC” will not match with\n“China”.\nBut “Bat” as in cricket may match with the mam-\nmal called bat. “Apple” will treat the apple fruit and the Apple\ncomputer without distinction. Vector space models lack the tight\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-5",
                            "title": "Meta Search Engines",
                            "content": "year it was realized that no search engine could possibly index all\nthe information on the web and be reasonably eﬃcient at the same\ntime. The web is extremely large. To get an idea, consider these\n1996 ﬁgures for the number of pages indexed by various search\nengines: Excite - 1.5 Million pages, Lycos - 19 Million unique\nURLs including ftps and gopher and Altavista - 166 Million pages.\nThese look like toys by today’s standards. Yet no search engine\nindexes all the available web pages. As the web keeps growing\nfaster and larger this need would only become more and more\napparent. While interacting with search engines, it was realized\nthat the user who had a varied set of interests started gradually\ntranscending a single search engine and was forced to submit and\nresubmit his queries to several search engines. This was a major\nbottleneck in World Wide Web searching and from this need for\n3.3. TOWARDS INTELLIGENT IR\n347\nreducing the amount of user time spent in submitting queries to\nmultiple search engines arose the concept of a meta search engine.\nA meta search engine takes a user query and submits the\nquery to many public domain search engines either in parallel\nor sequentially, collects results returned by each search engine\nand returns the compiled result to the user. Apart from this, a\nmeta search engine may provide other facilities such as removing\nduplicates and ﬁltering the results. Meta search engines are not\nintended to replace search engines - search engines are the service\nproviders to the meta search engines. Search engines are likely to\nbe increasingly used by meta search engines rather than by the\nend users directly. If Internet keeps growing at the phenomenal\nrate it is doing now, the amount of user time spent in locating\nrelevant information will also keep growing. Hence we need to\nlook for alternative architectures for searching and information\nretrieval from the web.\nMeta search engines have several advantages over direct use\ngle query interface since the system automatically formulates the\n348\nCHAPTER 3. ADVANCES IN IR\nuser’s query as required by various search engines. The system\ncan also learn statistical models of search engines and the pages\nthey index so that it becomes possible to choose the right search\nengines for the right query. Such models can be used to predict,\nfor example, the expected results for a given search. The meta\nsearch engine can “know”, even before searching, that there will\nbe too many hits and the query needs to be tightened up or that\nthere may not be many relevant documents. Overall performance\ncan also be optimized by choosing the search engines and ﬁring\nthem judiciously.\nUsers spend a lot of time trying to locate what they want from\nthe Internet. Even with technology improvements and increasing\nbandwidths, eﬀective bandwidth is getting oﬀset by increasing\nresources and the number of users. On the whole availability of\nsuﬃcient bandwidth will remain an imminent problem in many\nparts of the world. Meta search engines oﬀer one good solution to\nsaving user time.\nThere are several meta search engines in use. The Metacrawler\nand the Savvy search (http://www.savvy.com) are two such search\nservices on the web. The Metacrawler utilizes several search en-\ngines to which it submits its queries. The service removes dupli-\ncates from the results returned and presents it to the user in a\nclick-able format. The All in One Page (http://www.allone.com)\nis a meta search service which provides results from multiple re-\nsource applications which include gopher, Archie, HTTP and oth-\ners.\nHere we shall brieﬂy look at a Meta Search Engine called\nPersonal Search Assistant (PSA) developed by us sometime back.\nThe PSA system has been implemented using CGI scripts written\nin Perl which run under the Apache Web Server. PSA is modular.\nLet us take a quick look at each of the major modules:\n• The Collector Module submits queries to several search en-\nrate it is doing now, the amount of user time spent in locating\nrelevant information will also keep growing. Hence we need to\nlook for alternative architectures for searching and information\nretrieval from the web.\nMeta search engines have several advantages over direct use\nof search engines. Unlike search engines, meta search engines can\nreside locally and can be customized or adapted to the needs of\na speciﬁc user or a user group.\nA meta search engine can be\nlinked to a local database so that web search can be replaced with\nlocal search in some situations and frequently downloaded pages\ncan be locally archived. Advanced techniques for automatically\nbuilding and adapting user models are of great interest. It has\nbeen seen that users generally restrict themselves to a particular\nsubset of topics when they initiate a web search.\nThis can be\nused to construct a user proﬁle which should be extremely use-\nful in search optimization. More detailed understanding of the\nuser’s needs becomes possible as the meta search engine is local\nand customizable. For example, you may specify whether you are\nseriously looking for an answer to a very speciﬁc question or you\nare generally exploring what all is available. Search engines treat\neach query as a fresh task and they have no idea of what you have\nalready searched, what all you already know and what exactly\nyou are now looking for. Meta search engines can be customized\nto work in the background mode so that user’s time in waiting\nfor results can be minimized. A meta search engine can moni-\ntor the network bandwidth dynamically and adjust the ﬁring of\nthe various search engines accordingly. Users need to learn a sin-\ngle query interface since the system automatically formulates the\n348\nCHAPTER 3. ADVANCES IN IR\nuser’s query as required by various search engines. The system\ncan also learn statistical models of search engines and the pages\nthey index so that it becomes possible to choose the right search\nquestions requires a good deal of intelligence.\n• Browsers allow us to interactively navigate through a\nweb of inter-connected documents, physically located\nin diﬀerent computers spread across the globe. Here\nthe role of the computer is limited to locating and\ntaking you to the documents you ask for. You start\nby specifying a URL (Uniform Resource Locator) - an\naddress of a website or a speciﬁc document you are\nlooking for. You decide which links to explore further\nand which documents to read or download and save\nfor future use.\n• Search Engines and Information Retrieval (IR)\nSystems accept a query from the user and attempt to\n1.2. TECHNOLOGY FOR ACCESSING INFO\n5\nretrieve those documents in the collection that seem to\nmatch the user’s needs as expressed in his/her query.\nThe role of the machine is limited to drawing the user’s\nattention to documents that are potentially relevant\nto his/her needs. The challenge is to understand the\nuser’s speciﬁc requirements and locate documents that\nare, hopefully, the most relevant. Search engines have\nbecome an integral part of our everyday use of com-\nputers.\n• There are many search engines but no single search\nengine is good enough in all situations. Meta Search\nEngines combine the best of several Search Engines.\nThey provide a common user interface, format queries\nas required for various search engines, ﬁre the search\nengines serially or in parallel either as a foreground\nor as a background process, collect and collate results,\nstore results in a local database for reuse, personalize\nand adapt to individual user’s needs etc. Search en-\ngines are general purpose solutions while meta search\nengines can reside on your computer and they can be\npersonalized to suit your needs. The challenge is to\nbuild user models as well as to decide which search\nengines to ﬁre for what kind of queries. Collating re-\ntrieved results, removing duplicates etc. also require\nsubstantial amounts of intelligence.\n. . . . . . . . . . . . . . . . 330\n3.2.1\nIR Models . . . . . . . . . . . . . . . . 330\n3.2.2\nTerm Weighting: tf-idf . . . . . . . . . 333\n3.2.3\nSimilarity Measures\n. . . . . . . . . . 335\n3.2.4\nThe Probability Ranking Principle . . 336\n3.2.5\nPerformance Evaluation . . . . . . . . 336\n3.3\nTowards Intelligent IR . . . . . . . . . . . . . 338\n3.3.1\nImproving User Queries - Relevance\nFeedback\n. . . . . . . . . . . . . . . . 339\n3.3.2\nPage Ranking . . . . . . . . . . . . . . 340\n3.3.3\nRole of Linguistics . . . . . . . . . . . 341\n3.3.4\nLatent Semantic Indexing . . . . . . . 345\n3.3.5\nMeta Search Engines . . . . . . . . . . 346\n3.3.6\nSemantic Web\n. . . . . . . . . . . . . 349\n3.3.7\nInformation Retrieval is Diﬃcult . . . 350\n3.3.8\nConclusions . . . . . . . . . . . . . . . 351\nBibliography\n353\nAppendix 1: C5 Tag Set\n355\nAppendix 2: Sample Sentences\n359\nAppendix 3: ISCII Character Set\n361\nIndex\n365\nChapter 1\nThe Age of\nInformation and\nTechnology\n1.1\nThe Information Age\nModern technology enables us to store and process not only\ntext and speech but also images (line drawings, half-tone\npictures, colour photographs, animations and scanned pages\ncontaining handwritten or printed text, tables, pictures, etc),\nvideo, music, structured databases and presentation materi-\nals (slides, brochures, hand-outs, pamphlets) in various in-\nteresting combinations. Even the relatively small and eco-\nnomical hand held devices such as PDAs and cell phones\nnow support voice, text, static images and video. Some cell\nphones come with a built-in camera while others include a\nmusic player. There is a conﬂuence of information technolo-\ngies, communication technologies and entertainment.\nWe\nroutinely create, store, maintain, process and disseminate\nlarge amounts of information whether it is for serious study\nor for routine oﬃce work or just for fun. The boundaries of\nspace and time have melted away. We can access informa-\n1\n2\nCHAPTER 1. THE INFORMATION AGE",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-6",
                            "title": "Semantic Web",
                            "content": "provides a common framework that allows data to be shared and\nreused across application, enterprise, and community boundaries.\nIt is a collaborative eﬀort led by W3C (World Wide Web Con-\nsortium) with participation from a large number of researchers\nand industrial partners. It is based on the Resource Description\nFramework (RDF), which integrates a variety of applications us-\ning XML for syntax and URIs for naming. See Scientiﬁc American\nMay 2001 issue for an interesting article by Tim Berners-Lee.\n3.3.7\nInformation Retrieval is Diﬃcult\nIR is an inherently diﬃcult task. Can you search millions of doc-\numents and accurately suggest the most relevant documents for a\ngiven query in a fraction of a second? In IR we are asking com-\nputers to do what we human beings cannot do. And we want IR\nto be fully automatic - there is no scope for human intervention.\nWhat makes IR diﬃcult? There are three major issues:\n• Understanding user needs: Understanding exactly what the\nuser is looking for is not easy. Key words do not tell us\nwhat is the purpose of the current search, what all the user\nalready knows, what all he has already searched or what\nlevel of abstraction would suit his level of knowledge and\nexpertise. Social and cultural contexts are important. It\nlooks strange that we set forth on a grand searching and\nretrieval operation without understanding exactly what we\nare looking for!\n• Understanding the Documents: Unless you know exactly\nwhat the documents contain, what they pertain to and what\n3.3. TOWARDS INTELLIGENT IR\n351\nall things they include, for whom it is written, what back-\nground is assumed etc., how can we say which documents\nwill suit a given user’s needs at a given point of time? We\nhave already seen that understanding the meanings and in-\ntentions in a document is extremely diﬃcult.\nSo we go\nahead and search documents without knowing what exactly\nthey contain?\n• Eﬃcient Indexing ad Searching: Computationally eﬃcient\nnologies believe (and wish to believe) that the solution to\nthis information overload problem lies in more technology.\nWe can develop technology that helps us to access relevant\npieces of information from large collections of electronic doc-\numents so that we can take informed decisions within con-\nstraints of time. We can develop technology that helps us\nto locate, retrieve, categorize, summarize and translate the\nmaterial we are interested in.\nThe ﬁeld of Information Retrieval (IR) is all about locat-\n1.2. TECHNOLOGY FOR ACCESSING INFO\n3\ning relevant document from a large collection of documents\n- one of the most basic requirements in all walks of life to-\nday. At one point of time, the scope of IR was essentially\none library but today we need to look at a world-wide in-\nterconnected mesh of billions of electronic documents. In\nthis book we will explore this fascinating ﬁeld of informa-\ntion retrieval in relation to other closely related areas. We\nlimit ourselves to processing of text documents, leaving aside\npictures, sounds, etc.\nProcessing texts requires linguistic\nand statistical analysis of natural languages and we include\na chapter on foundations of Natural Language Processing\n(NLP).\nIt will be understood throughout this book that by docu-\nments we mean documents stored in electronic form in com-\nputer memory, not printed books or hand-written manu-\nscripts. Also, we are looking for automatic or semi-automatic\nmeans of storing, processing and retrieving such documents,\nnot the entirely manual methods. Manual methods can pre-\nsumably give better quality results but they are tedious,\ntime consuming and often error-prone and inconsistent. Nec-\nessary human expertise for manual processing is often not\navailable or is too costly. As the amount of available infor-\nmation and the desire to quickly retrieve relevant documents\nincrease, manual methods give way to automatic methods\nbased on technology. In most situations manual methods\nare simply not practicable.\n1.2\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nretrieval. There is an increasing cross-fertilization and integration\nof related technologies including speech.\nCurrently indexing is performed automatically on full texts.\nManual processing is slow, costly and may be inconsistent. On the\nother hand, while automatic processing can be very fast, it lacks\nthe commonsense and human judgement of manual methods and\ncan therefore be somewhat inferior in quality. The challenge today\nis to keep the superior speed factor and yet achieve near-human\nperformance through automatic methods.\nIt is clear that IR today is closely related to many other dis-\nciplines.\nIt interfaces with Database Management systems, Li-\nbrary and Information sciences, Artiﬁcial Intelligence, Natural\nLanguage Processing and Machine Learning. Database Manage-\nment systems focus on structured data stored in tables and eﬃ-\ncient processing of precisely deﬁned queries expressed in a formal\nlanguage such as SQL. The syntax and semantics of the data as\nwell as the query are clear. Recent trends towards semi-structured\ndata such as XML brings it closer to IR. Library and Information\nScience has focused on the human user aspects such as human-\ncomputer interaction, user interfaces and visualization. Eﬀective\ncategorization of human knowledge is a primary goal. Citation\nanalysis and bibliometrics are focus areas. Recent work in digital\nlibraries is bringing library science closer to computer science and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nden structure of natural language texts, if not into the meaning\nand intentions, it would be possible to cull out useful pieces of\ninformation and present them in suitably structured ways. Let\nus consider newspaper articles about earthquakes. If a computer\ncould read through such documents and tell us the place, the time,\nthe magnitude of the quake on Richter scale, extent of damage to\nlife and property etc. in a tabular form, it will open up a whole\nrange of possibilities that would otherwise require manually read-\ning all those thousands of documents.\nWhile Information Retrieval is concerned with location and re-\ntrieval of relevant documents, Information Extraction is all about\neliciting relevant pieces of information from given documents and\nimposing a desired structure on them. The task is more complex\nsince it requires, by deﬁnition, a thorough and detailed analysis of\nthe full texts. There are many interesting and useful applications.\nSystems have been built to extract structured information from\nresume submitted by job seekers. There are systems to populate a\nrelational databases from classiﬁed advertisements and brochures\nof electronic components. Here is another example:\nInput to the system:\n“14 April- A bomb went oﬀnear a communication tower in\nDelhi leaving a large part of city without energy. According to of-\nﬁcial sources, the bomb allegedly detonated by Pakistan militants,\nblew up a communication tower in northwestern part of Delhi at\n06:50 P.M.”\n36\nCHAPTER 1. THE INFORMATION AGE\nOutput of the system:\nIncident type:\nBombing\nDate:\nApril 14\nLocation:\nDelhi\nAlleged Perpetrator:\nPakistan militants\nTarget:\nCommunication Tower\nTABLE 1.2 Template Structure\nInformation extraction (IE) is a term which has come to be\napplied to the activity of automatically extracting pre-speciﬁed\nkinds of entities, events and relationships. The goal is identiﬁca-\ntion of instances of a particular class of events or relationships in a",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-7",
                            "title": "Information Retrieval is Difficult",
                            "content": "to go, and how to get there. We start by recapitulating what we\nhave already seen.\nLet us deﬁne a typical IR task: given a corpus of textual\nnatural-language documents and a user query in the form of a\ntextual string, to ﬁnd a ranked set of documents that are relevant\nto the query. As we have already seen, the most common approach\nis to view documents and queries are bags of words and represent\nthem as feature vectors where each feature corresponds to one\nword. Similarity between feature vectors is quantiﬁed in terms\nof the orientations of these vectors. Performance is measured in\nterms of Precision and Recall.\nIntelligent IR, on the other hand, requires that we consider the\nsyntax as well as semantics of documents and queries, we adapt\nto users based on direct or indirect feedback and learning, and we\ntake care of authority and dependability of documents. An ideal\ninformation retrieval system is one that can perform like a human\nassistant. This is really the software grand challenge. Obviously,\nwe are far from such an ideal.\nLet us now see what kinds of\nimprovements and enhancements can be or have been made in\nthe ﬁeld of information retrieval.\nMere presence or absence of keywords is clearly too naive a\nview of a document. Can we say I have a bad head ache and Now\nI am free from head ache mean the same thing and both match\nthe query head ache equally well? Is India beat Australia same as\nAustralia beat India? Can we equate I like Govinda’s movies and\nI like Govinda’s movies as much as I would like a burning stove if\nI were sitting on it? Most current IR systems continue to use the\nbag-of-words representation while examples like this clearly show\nthe inadequacies of such a representation.\nThere are several way we may think for improving this basic\nmodel of IR. We can try to take into account the meaning of the\nwords used. Match may mean a cricket match or a matrimonial\nmatch or a match box and only when we are able to disambiguate\nof documents. The idea is to retrieve documents which are rele-\nvant to a user at a given point of time for a particular purpose.\nThe document collection may be very large. Thus IR deals with\neﬃcient storage, indexing and matching techniques.\nIn this chapter we will look at some the recent advances in\nthe ﬁeld of Information Retrieval. We begin with a brief historical\nsketch and then give the basic IR model. Then we look at various\ndirections of development of the IR ﬁeld. The idea of Intelligent\nInformation Retrieval will be introduced and the need for deeper\nlinguistic analysis will be highlighted.\n3.1\nA Brief History of Information Re-\ntrieval\nInitial explorations of text retrieval systems from small collections\nof scientiﬁc abstracts, legal and business documents etc. began\nin the nineteen sixties and seventies. Foundations of Boolean and\nvector-space models were developed. During these early days, doc-\numents were studied and brief descriptors or lists of index terms\nwere manually prepared for each document.\n327\n328\nCHAPTER 3. ADVANCES IN IR\nDuring the eighties, large scale document databases started\nappearing.\nLexis-Nexis, Dialog and MEDLINE are noteworthy\nexamples of such databases. The need for eﬃcient retrieval from\nlarge collections was increasingly felt. This gave a big push to IR\nresearch and development.\nDuring the nineties, focus shifted to searching FTP-able doc-\numents on Internet (example: Archie, WAIS) and searching the\nWorld Wide Web (example: Lycos, Yahoo, Altavista). Organized\ncompetitions such as NIST TREC were held. Recommender sys-\ntems (example: Ringo, Amazon, NetPerceptions) appeared. Au-\ntomated text categorization and clustering systems were devel-\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\noped. Large scale information extraction systems started appear-\ning in the 2000’s. Google’s innovating ideas such as the back link\nbased page ranking are noteworthy. Current interests include re-\ntrieval from rich media documents and cross-lingual information\nretrieval. There is an increasing cross-fertilization and integration\nof related technologies including speech.\nCurrently indexing is performed automatically on full texts.\nManual processing is slow, costly and may be inconsistent. On the\nother hand, while automatic processing can be very fast, it lacks\nthe commonsense and human judgement of manual methods and\ncan therefore be somewhat inferior in quality. The challenge today\nis to keep the superior speed factor and yet achieve near-human\nperformance through automatic methods.\nIt is clear that IR today is closely related to many other dis-\nciplines.\nIt interfaces with Database Management systems, Li-\nbrary and Information sciences, Artiﬁcial Intelligence, Natural\nLanguage Processing and Machine Learning. Database Manage-\nment systems focus on structured data stored in tables and eﬃ-\ncient processing of precisely deﬁned queries expressed in a formal\nlanguage such as SQL. The syntax and semantics of the data as\nwell as the query are clear. Recent trends towards semi-structured\ndata such as XML brings it closer to IR. Library and Information\nScience has focused on the human user aspects such as human-\ncomputer interaction, user interfaces and visualization. Eﬀective\ncategorization of human knowledge is a primary goal. Citation\nanalysis and bibliometrics are focus areas. Recent work in digital\nlibraries is bringing library science closer to computer science and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\ninformation retrieval.AI has focussed on representation of knowl-\nedge, inference and intelligent action. Recent work on web ontolo-\n3.1. HISTORY OF IR\n329\ngies and intelligent information agents brings it closer to IR. NLP\nis essential to move from superﬁcial treatment of documents and\nqueries to deeper, meaning based approaches. Word Sense Disam-\nbiguation has a direct impact. We have seen that IR systems can\nbe viewed as a kind of natural language Question Answering sys-\ntems. Supervised and unsupervised learning techniques help us to\ngroup together similar documents and thereby facilitate eﬀective\nIR. Further developments in IR will clearly be related develop-\nments in all these various disciplines.\n3.1.1\nFrom The Library to the Internet\nWithin the domain of the library, one of the main tasks is the\nmanual classiﬁcation of books according to a speciﬁed classiﬁca-\ntion system so that relevant books can be easily accessed by users.\nOften only limited parts of the documents such as titles, front and\nback matter, table of contents etc. are used for the purpose. Full\ntext indexing is not feasible in manual methods. Human beings\nhave commonsense and manual classiﬁcation can be claimed to be\nsuperior in quality. However, to err is human and human errors do\ncreep in at times. Automated systems can guarantee consistency.\nThe arrival of the World Wide Web around 1993 spurred a\nsubstantial increase in the already immense world of Internet. The\nsize and scope of the Internet today has reached mind boggling\nlevels. As the volume of information available on the World Wide\nWeb went on increasing, the need for eﬃcient ways of locating req-\nuisite information on the web was increasingly felt and this gave\nrise to the ﬁrst search engine Yahoo (http://www.yahoo.com) and\nlater many others such as Altavista, Infoseek, Excite, Hotbot, Ly-\ncos, Webcrawler and Google. Search engines follow the standard\ntechnique of using a spider or crawler to create and update a\nadvanced techniques we will be looking at in Chapter Three.\n1.4. INFORMATION RETRIEVAL\n29\n1.4.1\nIR Deﬁned\nInformation Retrieval is a vast ﬁeld concerned with the storage\nand retrieval of documents. Documents may include texts, im-\nages, video, speech, music and web pages in various combinations.\nProcessing and retrieval of images, video, speech, music and rich\nmulti-media documents has become an increasingly active area of\nresearch in recent times. Nonetheless, text remains the most ba-\nsic, ubiquitous and the most widely used medium of representing\nand communicating information. Text documents take much less\nspace to store and less network bandwidth to move across than\npictures. After all, a picture is worth a thousand words! Here we\nwill mainly look at the issues relating to storage and subsequent\nretrieval of text documents in response to user queries.\nOf particular interest will be what is termed ad hoc retrieval.\nHere an unaided user formulates a query and requests for relevant\ndocuments. The system searches the collection and returns a pos-\nsibly ordered set of potentially useful documents. For example,\nyou may go to a search engine, type “computer” and press the\nsearch button. The search engine comes back and says it found\nso many million web pages that seem to be relevant to your query\nand it also displays the links to the top few that it thinks are the\nmost relevant. A good system will be expected to return most of\nthe relevant documents in the collection and few irrelevant docu-\nments.\nA Document is a unit of text that is indexed and retrieved.\nA document may be an article, a research paper, a whole book,\na chapter or section from a book, or even single sentences. In a\ntraditional library setting, a document may be a book while on the\nInternet a document may be just a single web page. A Collection\nis a set of such documents. A Query refers to a short formulation\nof a user request in a suitable format.\nSEARCH ENGINE\nCRAWLER\nINDEX\nDOCUMENT\nREPOSITORY\nQUERY",
                            "children": []
                        },
                        {
                            "id": "chapter-3-section-3-subsection-8",
                            "title": "Conclusions",
                            "content": "life and vigour.\nAll disciplines make such an honest self-\nassessment at periodic intervals. In this spirit, I have taken\nthe liberty of freely expressing my views and opinions, es-\npecially with regard to the status and progress of NLP in\nIndia. My views on linguistics must be viewed in terms of\nthe limited interface linguists and engineers have had in In-\ndia on topics of relevance here. Linguistics itself has changed\nv\nsubstantially over time. There have been competing theories\nand view points. Focus areas, theories or models adapted\nand view points have varied substantially even within the In-\ndian context. Generalizations and extrapolations can thus\nbe dangerous. Comments on the current state of technology\nshould be taken with care as technology keeps changing very\nfast. All views expressed are especially applicable to the In-\ndian scene. If some of the statements appear a bit negative,\nit is because the assessment being made is with respect to\nthe perfect, the ideal, with respect to what we could have\nachieved. It is not that all the good work that has been done\nis not appreciated. It should be taken in the positive sense\nthat we have the capability to do much more. We can be\nworld leaders. It is essential to have this optimism but at\nthe same time we must plan our work based on the knowl-\nedge and understanding of the ground realities. This is the\nrecipe to success. I hope this view will be appreciated and\nwell received.\nKavi Narayana Murthy\ncaitra s’ukla pratipat, paarthiva naama saMvatsara\n(chaandramaana yugaadi - 9 April 2005)\nvi\nFOREWORD\nWhen I agreed to chair on the occasion of Sarada Ran-\nganathan Memorial Lectures 2004 delivered by my former\ncolleague from Hyderabad University days, Professor Kavi\nNarayana Murthy, little did I know that this was going to\nbecome a part of a much-needed text in Natural Language\nProcessing (NLP), which many of us have been asking one\nanother to write but none was willing to spend so much\nsize reduction results in the essentials being retained and less im-\nportant stuﬀeliminated. One way to view this is to say that noise\nis reduced. Eliminating or reducing noise can also increase the\naccuracy in many applications.\nWe are all familiar with summarization in our daily life. We\noften scan news headlines before or instead of reading the full\nnews. Teachers produce outlines of notes and course materials to\nstudents. Minutes of a meeting summarize what all transpired in\nthe meeting. Previews of movies give a quick advance view of the\nshape of things to come in the movie. Synopses of TV serials are\ntelecast to let the users get a feel for the serials. Book reviews\nare summaries as retold by the reviewer. Newspapers and maga-\nzines publish Radio and TV guides for the coming week or month.\nResumes and Obituaries are biographical summaries. Novels are\noften speciﬁcally abridged for, say, children to read. Weather fore-\ncasts and Market report bulletins are summaries too. Sound Bites\nconsolidate on going debate on a particular issue. Chronologies\nand gists of history are a kind of summary.\nSummarization is\nitself not new. Automatic techniques for summarization are.\nAlta-Vista Discovery uses Inxight’s summarizer for ﬁltering\nweb based IR. Orcale’s Context (data mining of text databases),\nMicrosoft Word’s AutoSummarize, British Telecom’s ProSum are\n1.6. AUTOMATIC SUMMARIZATION\n41\nsome of the commercial implementations.\n1.6.2\nApproaches to Automatic Summarization\nSummarization can be viewed as a reductive transformation of\nsource text to summary text through content reduction by selec-\ntion and / or generalization of what is important in the source.\nText Summarization has been deﬁned as the process of distilling\nthe most important information from a source(s) to produce an\nabridged version for a particular user(s) and task(s). The primary\naim is to automatically produce a gist. There is no manual in-\ntervention. The appropriateness of the summary so produced is\nearly 1980s Michael Dyer was working on the ideas of in-depth\n24\nCHAPTER 1. THE INFORMATION AGE\nunderstanding as part of his doctoral work at MIT. He built a\nsystem called BORIS (Better Organized Reasoning and Inference\nSystem). BORIS showed that under certain assumptions, it is pos-\nsible for computers to demonstrate in-depth understanding and\nanswer complex questions in a human-like manner. Let us look at\nsamples of actual transcripts from BORIS:\nRichard hadn’t heard from his college roommate Paul\nfor years.\nRichard\nhad borrowed\nmoney from\nPaul,\nwhich was\nnever paid back,\nbut now he had no idea\nwhere to find his old friend. When a letter finally\narrived from San Francisco,\nRichard was anxious to\nfind out how Paul was.\nQ What happened to Richard at home?\nA Richard got a letter from Paul\nQ Who is Paul?\nA Richard’s friend\nQ Did Richard want to see Paul?\nA Yes. Richard wanted to know how Paul was\nQ Had Paul lent money to Richard?\nA Yes. Paul had lent money to Richard\nQ Why didn’t Richard pay Paul back?\nA Richard did not know where Paul was\nQ How did Richard feel when the letter appeared?\nA Richard felt glad because Paul and he were friends\nFIG 1.5 Sample Transcripts from BORIS\n1.3. QUESTION ANSWERING SYSTEMS\n25\nThis sample transcript vividly brings out the depth of under-\nstanding by BORIS. It is not straight forward to answer questions\nsuch as what happened to somebody somewhere. Many things\nmight have happened, some of them may be explicitly given to\nus in the story while some we can infer from our world knowl-\nedge of what “typically” happens in given kinds of situations.\nBut everything that happened or could have happened would not\nbe interesting. Something unusual, something special, something\nworth noting is what is expected out of such a question. Notice\nalso that the story never says explicitly that Paul wrote the letter.\nSince we are talking of Paul and nobody else, we can infer that it\nmust be Paul who must have written the letter. To say Paul was\nif we read the sentence “John bought a television” we understand\nmany things - that John probably bought the TV in a store, there\nwas another person in the store, John gave him money, that per-\nson gave John the TV, the person who sold the TV no longer\npossesses that TV, John bought the TV in order to watch the\nshows probably for his own enjoyment, John will plug in the TV\nat home, and so on. Although we can never be sure about such\ninferences, understanding texts necessarily involves a large num-\nber of such inferences. We will have to make such assumptions\nand inferences, even if some of them need to be changed later\non in the light of new information coming in. Note that our un-\nderstanding of the sentence is going to be same or similar if the\nsentence were to read “A TV set was sold to John”. CD theory\nproposed that any sentence in any language can be broken down\nand expressed in terms of eleven primitive acts. Each of these acts\n22\nCHAPTER 1. THE INFORMATION AGE\nhad an associated set of cases such as the actor, the recipient and\nthe instrument. These case-frames encapsulated expectations for\nthe conceptualizations being built. Such expectations are useful\nfor word sense disambiguation, pronoun reference etc. too. The\nsentence above could be conceptualized in Cd as\n(ATRANS\nACTOR\nJohn\nOBJECT\nmoney\nFROM\nJohn\nTO\nindividual)\n(ATRANS\nACTOR\nindividual\nOBJECT\nTV\nFROM\nindividual\nTO\nJohn)\nHere ATRANS stands for transfer of possession. The other\nprimitive acts in CD theory were PROPEL (the application of\nphysical force - throwing, hitting, falling, pulling, kicking, etc.),\nPTRANS (transfer of physical location - driving, ﬂying, taking a\nbus, walking, etc.), INGEST (AN organism taken something from\noutside environment and makes it internal - breathing, eating,\nsmoking, etc.), EXPEL (opposite of INGEST - sweating, crying,\ndefecating, spitting, etc.), MTRANS (transfer of mental infor-\nmation from one individual to another - speaking, reading, etc.),\n29. What river was it caught in?\n30. Whom did you want Rama to tell to catch a ﬁsh?\n31. * In what river they ﬁshed?\n32. Whom was the ﬁsh caught by?\n33. By whom was the ﬁsh caught?\n34. Whom was the ﬁsh expected to be given to?\n35. An engineer reading the newspaper in the balcony got angry\n36. They found the answer that they were looking for\n37. Yesterday I ate a cake the likes of which I had never seen\n38. The deity in whose image we were depicted was unknown\nto many\n39. The windows broken in the scuﬄe have been replaced\n40. I saw a baby being given a bath in the open\n41. The ﬁsh that they thought you had told me not to bother\nwith was very small\n42. The horse raced past the barn fell\n360\nAppendix 3: ISCII\nCharacter Set\nCode\nCharacter Name\n161\nVowel-Modiﬁer caMdrabiMdu\n162\nVowel-Modiﬁer anusvaara\n163\nVowel-Modiﬁer visarga\n164\nVowel a\n165\nVowel aa\n166\nVowel i\n167\nVowel ii\n168\nVowel u\n169\nVowel uu\n170\nVowel R\n171\nVowel e (Southern Scripts)\n172\nVowel ee\n173\nVowel ai\n174\nVowel aye (deevanaagari Script)\n175\nVowel o (Southern Scripts)\n176\nVowel oo\n177\nVowel au\n178\nVowel awe (deevanaagari Script)\n179\nConsonant ka\n180\nConsonant kha\n181\nConsonant ga\n182\nConsonant gha\n183\nConsonant nga\n361\nCode\nCharacter Name\n184\nConsonant ca\n185\nConsonant cha\n186\nConsonant ja\n187\nConsonant jha\n188\nConsonant jnya\n189\nConsonant Ta\n190\nConsonant Tha\n191\nConsonant Da\n192\nConsonant Dha\n193\nConsonant Na\n194\nConsonant ta\n195\nConsonant tha\n196\nConsonant da\n197\nConsonant dha\n198\nConsonant na\n199\nConsonant na (Tamil)\n200\nConsonant pa\n201\nConsonant pha\n202\nConsonant ba\n203\nConsonant bha\n204\nConsonant ma\n205\nConsonant ya\n206\nConsonant jya (Assamese, Bangla, Oriya Scripts)\n207\nConsonant ra\n208\nConsonant Hard ra (Southern Scripts)\n209\nConsonant la\n210\nConsonant La\n211\nConsonant zha (Tamil, Malayalam Scripts)\n212\nConsonant va\n213\nConsonant s’a\n214\nConsonant Sa\n215\nConsonant sa\n216\nConsonant ha\n217\nConsonant INVISIBLE\n362\nCode\nCharacter Name\n218\nVowel Sign aa\n219\nVowel Sign i\n220\nVowel Sign ii\n221",
                            "children": []
                        }
                    ]
                }
            ]
        },
        {
            "id": "chapter-4",
            "title": "Appendices",
            "content": null,
            "children": [
                {
                    "id": "chapter-4-section-1",
                    "title": "C5 Tag Set",
                    "content": null,
                    "children": []
                },
                {
                    "id": "chapter-4-section-2",
                    "title": "Sample Sentences",
                    "content": null,
                    "children": []
                },
                {
                    "id": "chapter-4-section-3",
                    "title": "ISCII Character Set",
                    "content": null,
                    "children": []
                }
            ]
        }
    ]
}